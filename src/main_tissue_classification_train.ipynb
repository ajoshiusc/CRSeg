{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python -c \"import monai\" || pip install -q \"monai-weekly[nibabel, tqdm]\"\n",
    "!python -c \"import matplotlib\" || pip install -q matplotlib\n",
    "!python -c \"import onnxruntime\" || pip install -q onnxruntime\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MONAI version: 1.3.2\n",
      "Numpy version: 1.26.2\n",
      "Pytorch version: 2.3.1+cu121\n",
      "MONAI flags: HAS_EXT = False, USE_COMPILED = False, USE_META_DICT = False\n",
      "MONAI rev id: 59a7211070538586369afd4a01eca0a7fe2e742e\n",
      "MONAI __file__: /home1/<username>/.local/lib/python3.9/site-packages/monai/__init__.py\n",
      "\n",
      "Optional dependencies:\n",
      "Pytorch Ignite version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "ITK version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "Nibabel version: 5.2.1\n",
      "scikit-image version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "scipy version: 1.11.4\n",
      "Pillow version: 10.1.0\n",
      "Tensorboard version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "gdown version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "TorchVision version: 0.18.1+cu121\n",
      "tqdm version: 4.66.2\n",
      "lmdb version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "psutil version: 5.9.1\n",
      "pandas version: 2.1.4\n",
      "einops version: 0.8.0\n",
      "transformers version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "mlflow version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "pynrrd version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "clearml version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "\n",
      "For details about installing the optional dependencies, please visit:\n",
      "    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from monai.apps import DecathlonDataset\n",
    "from monai.config import print_config\n",
    "from monai.data import DataLoader, decollate_batch, CacheDataset, Dataset\n",
    "from monai.handlers.utils import from_engine\n",
    "from monai.losses import DiceLoss\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.networks.nets import SegResNet\n",
    "from myutils import ConvertToMultiChannelHeadRecod\n",
    "from monai.transforms import (\n",
    "    Activations,\n",
    "    Activationsd,\n",
    "    AsDiscrete,\n",
    "    AsDiscreted,\n",
    "    Compose,\n",
    "    Invertd,\n",
    "    LoadImaged,\n",
    "    MapTransform,\n",
    "    NormalizeIntensityd,\n",
    "    Orientationd,\n",
    "    RandFlipd,\n",
    "    RandScaleIntensityd,\n",
    "    RandShiftIntensityd,\n",
    "    RandSpatialCropd,\n",
    "    Resized,\n",
    "    Spacingd,\n",
    "    EnsureTyped,\n",
    "    EnsureChannelFirstd,\n",
    ")\n",
    "from monai.utils import set_determinism\n",
    "import onnxruntime\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "\n",
    "print_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_determinism(seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a new transform to convert brain tumor labels\n",
    "\n",
    "Here we convert the multi-classes labels into multi-labels segmentation task in One-Hot format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup transforms for training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = Compose(\n",
    "    [\n",
    "        # load 4 Nifti images and stack them together\n",
    "        LoadImaged(keys=[\"image\", \"label\"]),\n",
    "        ConvertToMultiChannelHeadRecod(keys=\"label\"),\n",
    "        EnsureChannelFirstd(keys=[\"image\"]),\n",
    "        EnsureTyped(keys=[\"image\", \"label\"]),\n",
    "        Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
    "        #Spacingd(\n",
    "        #    keys=[\"image\", \"label\"],\n",
    "        #    pixdim=(2.0, 2.0, 2.0),\n",
    "        #    mode=(\"bilinear\", \"nearest\"),\n",
    "        #),\n",
    "        Resized(keys=[\"image\", \"label\"], spatial_size=[96, 96, 96], mode=(\"trilinear\", \"nearest\")),\n",
    "       # RandSpatialCropd(keys=[\"image\", \"label\"], roi_size=[224, 224, 144], random_size=False),\n",
    "        RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=0),\n",
    "        RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=1),\n",
    "        RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=2),\n",
    "        NormalizeIntensityd(keys=\"image\", nonzero=True, channel_wise=True),\n",
    "        RandScaleIntensityd(keys=\"image\", factors=0.1, prob=1.0),\n",
    "        RandShiftIntensityd(keys=\"image\", offsets=0.1, prob=1.0),\n",
    "    ]\n",
    ")\n",
    "val_transform = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"image\", \"label\"]),\n",
    "        ConvertToMultiChannelHeadRecod(keys=\"label\"),\n",
    "        EnsureChannelFirstd(keys=[\"image\"]),\n",
    "        EnsureTyped(keys=[\"image\", \"label\"]),\n",
    "        Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
    "        #Spacingd(\n",
    "        #    keys=[\"image\", \"label\"],\n",
    "        #    pixdim=(2.0, 2.0, 2.0),\n",
    "        #    mode=(\"bilinear\", \"nearest\"),\n",
    "        #),\n",
    "        Resized(keys=[\"image\", \"label\"], spatial_size=[96, 96, 96], mode=(\"trilinear\", \"nearest\")),\n",
    "        NormalizeIntensityd(keys=\"image\", nonzero=True, channel_wise=True),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 455/455 [00:00<00:00, 343424.21it/s]\n"
     ]
    }
   ],
   "source": [
    "headreco_dir = '/project/ajoshi_27/headreco_out/'\n",
    "root_dir = '/project/ajoshi_1183/Projects/CRSeg/models'\n",
    "mode = 'train'\n",
    "\n",
    "# Read the list of subjects\n",
    "with open(mode+'.txt', 'r') as myfile:\n",
    "    sub_lst = myfile.read().splitlines()\n",
    "\n",
    "train_images = list()\n",
    "train_labels = list()\n",
    "for subname in tqdm(sub_lst):\n",
    "\n",
    "    subdir = os.path.join(headreco_dir, 'm2m_'+subname)\n",
    "    train_images.append(os.path.join(subdir, 'T1fs_conform.nii.gz'))\n",
    "    train_labels.append(os.path.join(subdir, subname + '_masks_contr.nii.gz'))\n",
    "\n",
    "\n",
    "#train_images = sorted(glob.glob(os.path.join(data_dir, \"imagesTr\", \"*.nii.gz\")))\n",
    "#train_labels = sorted(glob.glob(os.path.join(data_dir, \"labelsTr\", \"*.nii.gz\")))\n",
    "\n",
    "data_dicts = [{\"image\": image_name, \"label\": label_name} for image_name, label_name in zip(train_images, train_labels)]\n",
    "train_files, val_files = data_dicts[:-9], data_dicts[-9:]\n",
    "\n",
    "# train_ds = CacheDataset(data=train_files, transform=train_transform, cache_rate=1.0, num_workers=4)\n",
    "train_ds = Dataset(data=train_files, transform=train_transform)\n",
    "\n",
    "# use batch_size=2 to load images and use RandCropByPosNegLabeld\n",
    "# to generate 2 x 4 images for network training\n",
    "train_loader = DataLoader(train_ds, batch_size=2, shuffle=True, num_workers=4)\n",
    "\n",
    "# val_ds = CacheDataset(data=val_files, transform=val_transform, cache_rate=1.0, num_workers=4)\n",
    "val_ds = Dataset(data=val_files, transform=val_transform)\n",
    "val_loader = DataLoader(val_ds, batch_size=2, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check data shape and visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image shape: torch.Size([1, 96, 96, 96])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1728x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label shape: torch.Size([3, 96, 96, 96])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1296x432 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "val_data_example = val_ds[2]\n",
    "print(f\"image shape: {val_data_example['image'].shape}\")\n",
    "\n",
    "plt.figure(\"image\", (24, 6))\n",
    "for i in range(1):\n",
    "    plt.subplot(1, 1, i + 1)\n",
    "    plt.title(f\"image channel {i}\")\n",
    "    plt.imshow(val_data_example[\"image\"][i, :, :, 48].detach().cpu(), cmap=\"gray\")\n",
    "plt.show()\n",
    "# also visualize the 3 channels label corresponding to this image\n",
    "print(f\"label shape: {val_data_example['label'].shape}\")\n",
    "plt.figure(\"label\", (18, 6))\n",
    "for i in range(3):\n",
    "    plt.subplot(1, 3, i + 1)\n",
    "    plt.title(f\"label channel {i}\")\n",
    "    plt.imshow(val_data_example[\"label\"][i, :, :, 48].detach().cpu())\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model, Loss, Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 300\n",
    "val_interval = 5\n",
    "VAL_AMP = True\n",
    "\n",
    "# standard PyTorch program style: create SegResNet, DiceLoss and Adam optimizer\n",
    "device = torch.device(\"cuda:0\")\n",
    "model = SegResNet(\n",
    "    blocks_down=[1, 2, 2, 4],\n",
    "    blocks_up=[1, 1, 1],\n",
    "    init_filters=16,\n",
    "    in_channels=1,\n",
    "    out_channels=3,\n",
    "    dropout_prob=0.2,\n",
    ").to(device)\n",
    "loss_function = DiceLoss(smooth_nr=0, smooth_dr=1e-5, squared_pred=True, to_onehot_y=False, sigmoid=True)\n",
    "optimizer = torch.optim.Adam(model.parameters(), 1e-4, weight_decay=1e-5)\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=max_epochs)\n",
    "\n",
    "dice_metric = DiceMetric(include_background=True, reduction=\"mean\")\n",
    "dice_metric_batch = DiceMetric(include_background=True, reduction=\"mean_batch\")\n",
    "\n",
    "post_trans = Compose([Activations(sigmoid=True), AsDiscrete(threshold=0.5)])\n",
    "\n",
    "\n",
    "# define inference method\n",
    "def inference(input):\n",
    "    def _compute(input):\n",
    "        return sliding_window_inference(\n",
    "            inputs=input,\n",
    "            roi_size=(96, 96, 96),\n",
    "            sw_batch_size=1,\n",
    "            predictor=model,\n",
    "            overlap=0.5,\n",
    "        )\n",
    "\n",
    "    if VAL_AMP:\n",
    "        with torch.cuda.amp.autocast():\n",
    "            return _compute(input)\n",
    "    else:\n",
    "        return _compute(input)\n",
    "\n",
    "\n",
    "# use amp to accelerate training\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "# enable cuDNN benchmark\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute a typical PyTorch training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "epoch 1/300\n",
      "1/223, train_loss: 0.8541, step time: 8.7929\n",
      "2/223, train_loss: 0.8403, step time: 0.0999\n",
      "3/223, train_loss: 0.8228, step time: 0.0990\n",
      "4/223, train_loss: 0.7878, step time: 0.1012\n",
      "5/223, train_loss: 0.7748, step time: 0.1029\n",
      "6/223, train_loss: 0.7840, step time: 0.1126\n",
      "7/223, train_loss: 0.7869, step time: 0.1137\n",
      "8/223, train_loss: 0.8063, step time: 0.0999\n",
      "9/223, train_loss: 0.7896, step time: 0.1011\n",
      "10/223, train_loss: 0.7924, step time: 0.1113\n",
      "11/223, train_loss: 0.7598, step time: 0.1067\n",
      "12/223, train_loss: 0.7602, step time: 0.0996\n",
      "13/223, train_loss: 0.7520, step time: 0.1107\n",
      "14/223, train_loss: 0.7874, step time: 0.0997\n",
      "15/223, train_loss: 0.7925, step time: 0.1155\n",
      "16/223, train_loss: 0.7521, step time: 0.1003\n",
      "17/223, train_loss: 0.7722, step time: 0.0999\n",
      "18/223, train_loss: 0.7816, step time: 0.0994\n",
      "19/223, train_loss: 0.8010, step time: 0.1018\n",
      "20/223, train_loss: 0.7621, step time: 0.0994\n",
      "21/223, train_loss: 0.7761, step time: 0.0999\n",
      "22/223, train_loss: 0.7626, step time: 0.1320\n",
      "23/223, train_loss: 0.7790, step time: 0.0998\n",
      "24/223, train_loss: 0.7589, step time: 0.0992\n",
      "25/223, train_loss: 0.7727, step time: 0.1002\n",
      "26/223, train_loss: 0.7749, step time: 0.0998\n",
      "27/223, train_loss: 0.7616, step time: 0.1002\n",
      "28/223, train_loss: 0.7711, step time: 0.1070\n",
      "29/223, train_loss: 0.7361, step time: 0.0996\n",
      "30/223, train_loss: 0.7564, step time: 0.1001\n",
      "31/223, train_loss: 0.7501, step time: 0.0996\n",
      "32/223, train_loss: 0.7385, step time: 0.0994\n",
      "33/223, train_loss: 0.7393, step time: 0.0996\n",
      "34/223, train_loss: 0.7388, step time: 0.1237\n",
      "35/223, train_loss: 0.7247, step time: 0.1003\n",
      "36/223, train_loss: 0.7232, step time: 0.1003\n",
      "37/223, train_loss: 0.7220, step time: 0.1151\n",
      "38/223, train_loss: 0.7347, step time: 0.1098\n",
      "39/223, train_loss: 0.7537, step time: 0.1001\n",
      "40/223, train_loss: 0.7572, step time: 0.1023\n",
      "41/223, train_loss: 0.7517, step time: 0.0999\n",
      "42/223, train_loss: 0.7464, step time: 0.1220\n",
      "43/223, train_loss: 0.7427, step time: 0.1280\n",
      "44/223, train_loss: 0.7198, step time: 0.1085\n",
      "45/223, train_loss: 0.7265, step time: 0.1051\n",
      "46/223, train_loss: 0.7330, step time: 0.1001\n",
      "47/223, train_loss: 0.7202, step time: 0.1030\n",
      "48/223, train_loss: 0.7440, step time: 0.1138\n",
      "49/223, train_loss: 0.7339, step time: 0.1085\n",
      "50/223, train_loss: 0.7106, step time: 0.1127\n",
      "51/223, train_loss: 0.7284, step time: 0.0992\n",
      "52/223, train_loss: 0.7320, step time: 0.0998\n",
      "53/223, train_loss: 0.7495, step time: 0.1005\n",
      "54/223, train_loss: 0.7372, step time: 0.1056\n",
      "55/223, train_loss: 0.7330, step time: 0.1038\n",
      "56/223, train_loss: 0.7448, step time: 0.0995\n",
      "57/223, train_loss: 0.7334, step time: 0.1007\n",
      "58/223, train_loss: 0.7327, step time: 0.1057\n",
      "59/223, train_loss: 0.7495, step time: 0.1129\n",
      "60/223, train_loss: 0.7265, step time: 0.0999\n",
      "61/223, train_loss: 0.7087, step time: 0.1008\n",
      "62/223, train_loss: 0.7315, step time: 0.1006\n",
      "63/223, train_loss: 0.7323, step time: 0.1003\n",
      "64/223, train_loss: 0.7232, step time: 0.0999\n",
      "65/223, train_loss: 0.7311, step time: 0.0995\n",
      "66/223, train_loss: 0.7151, step time: 0.1009\n",
      "67/223, train_loss: 0.7134, step time: 0.1031\n",
      "68/223, train_loss: 0.7178, step time: 0.1070\n",
      "69/223, train_loss: 0.7110, step time: 0.1036\n",
      "70/223, train_loss: 0.7001, step time: 0.1005\n",
      "71/223, train_loss: 0.7341, step time: 0.1165\n",
      "72/223, train_loss: 0.7373, step time: 0.1000\n",
      "73/223, train_loss: 0.7222, step time: 0.1105\n",
      "74/223, train_loss: 0.6933, step time: 0.1019\n",
      "75/223, train_loss: 0.7129, step time: 0.1003\n",
      "76/223, train_loss: 0.7469, step time: 0.1175\n",
      "77/223, train_loss: 0.7325, step time: 0.1010\n",
      "78/223, train_loss: 0.7440, step time: 0.1097\n",
      "79/223, train_loss: 0.7203, step time: 0.1000\n",
      "80/223, train_loss: 0.7234, step time: 0.1000\n",
      "81/223, train_loss: 0.7418, step time: 0.1050\n",
      "82/223, train_loss: 0.7153, step time: 0.1015\n",
      "83/223, train_loss: 0.7309, step time: 0.0991\n",
      "84/223, train_loss: 0.7059, step time: 0.1214\n",
      "85/223, train_loss: 0.7138, step time: 0.1075\n",
      "86/223, train_loss: 0.6999, step time: 0.1046\n",
      "87/223, train_loss: 0.7038, step time: 0.1000\n",
      "88/223, train_loss: 0.7062, step time: 0.0986\n",
      "89/223, train_loss: 0.7076, step time: 0.1012\n",
      "90/223, train_loss: 0.7094, step time: 0.1000\n",
      "91/223, train_loss: 0.6845, step time: 0.0991\n",
      "92/223, train_loss: 0.7200, step time: 0.1002\n",
      "93/223, train_loss: 0.7212, step time: 0.1064\n",
      "94/223, train_loss: 0.7018, step time: 0.1120\n",
      "95/223, train_loss: 0.7022, step time: 0.1148\n",
      "96/223, train_loss: 0.6992, step time: 0.1025\n",
      "97/223, train_loss: 0.7025, step time: 0.1012\n",
      "98/223, train_loss: 0.7259, step time: 0.1007\n",
      "99/223, train_loss: 0.7090, step time: 0.0997\n",
      "100/223, train_loss: 0.6988, step time: 0.1118\n",
      "101/223, train_loss: 0.7078, step time: 0.1080\n",
      "102/223, train_loss: 0.7018, step time: 0.1122\n",
      "103/223, train_loss: 0.7008, step time: 0.1002\n",
      "104/223, train_loss: 0.7146, step time: 0.0996\n",
      "105/223, train_loss: 0.6937, step time: 0.1053\n",
      "106/223, train_loss: 0.6798, step time: 0.1049\n",
      "107/223, train_loss: 0.6977, step time: 0.1114\n",
      "108/223, train_loss: 0.7086, step time: 0.1136\n",
      "109/223, train_loss: 0.7167, step time: 0.1349\n",
      "110/223, train_loss: 0.6901, step time: 0.1056\n",
      "111/223, train_loss: 0.7174, step time: 0.1110\n",
      "112/223, train_loss: 0.7176, step time: 0.1085\n",
      "113/223, train_loss: 0.7082, step time: 0.0997\n",
      "114/223, train_loss: 0.6921, step time: 0.1075\n",
      "115/223, train_loss: 0.7187, step time: 0.1012\n",
      "116/223, train_loss: 0.7044, step time: 0.1050\n",
      "117/223, train_loss: 0.7118, step time: 0.0994\n",
      "118/223, train_loss: 0.6997, step time: 0.0996\n",
      "119/223, train_loss: 0.7082, step time: 0.1002\n",
      "120/223, train_loss: 0.7002, step time: 0.1075\n",
      "121/223, train_loss: 0.6965, step time: 0.1232\n",
      "122/223, train_loss: 0.6849, step time: 0.1050\n",
      "123/223, train_loss: 0.6826, step time: 0.1050\n",
      "124/223, train_loss: 0.6867, step time: 0.1209\n",
      "125/223, train_loss: 0.6955, step time: 0.1005\n",
      "126/223, train_loss: 0.6854, step time: 0.1088\n",
      "127/223, train_loss: 0.6829, step time: 0.1096\n",
      "128/223, train_loss: 0.7033, step time: 0.1001\n",
      "129/223, train_loss: 0.6969, step time: 0.1050\n",
      "130/223, train_loss: 0.6836, step time: 0.1039\n",
      "131/223, train_loss: 0.6843, step time: 0.1100\n",
      "132/223, train_loss: 0.6946, step time: 0.1262\n",
      "133/223, train_loss: 0.6988, step time: 0.1118\n",
      "134/223, train_loss: 0.7047, step time: 0.1087\n",
      "135/223, train_loss: 0.6550, step time: 0.1018\n",
      "136/223, train_loss: 0.6751, step time: 0.1206\n",
      "137/223, train_loss: 0.6790, step time: 0.1228\n",
      "138/223, train_loss: 0.6880, step time: 0.1093\n",
      "139/223, train_loss: 0.7065, step time: 0.1005\n",
      "140/223, train_loss: 0.7118, step time: 0.1226\n",
      "141/223, train_loss: 0.6909, step time: 0.1006\n",
      "142/223, train_loss: 0.6759, step time: 0.1248\n",
      "143/223, train_loss: 0.6777, step time: 0.1047\n",
      "144/223, train_loss: 0.7034, step time: 0.1002\n",
      "145/223, train_loss: 0.6954, step time: 0.1004\n",
      "146/223, train_loss: 0.6992, step time: 0.0998\n",
      "147/223, train_loss: 0.6959, step time: 0.1052\n",
      "148/223, train_loss: 0.6969, step time: 0.1008\n",
      "149/223, train_loss: 0.6903, step time: 0.1012\n",
      "150/223, train_loss: 0.7152, step time: 0.1014\n",
      "151/223, train_loss: 0.6782, step time: 0.1003\n",
      "152/223, train_loss: 0.6590, step time: 0.1009\n",
      "153/223, train_loss: 0.6876, step time: 0.1039\n",
      "154/223, train_loss: 0.7022, step time: 0.1095\n",
      "155/223, train_loss: 0.7014, step time: 0.0999\n",
      "156/223, train_loss: 0.6884, step time: 0.1003\n",
      "157/223, train_loss: 0.6772, step time: 0.1054\n",
      "158/223, train_loss: 0.6703, step time: 0.1087\n",
      "159/223, train_loss: 0.6743, step time: 0.1020\n",
      "160/223, train_loss: 0.6829, step time: 0.1002\n",
      "161/223, train_loss: 0.6618, step time: 0.1111\n",
      "162/223, train_loss: 0.6811, step time: 0.1124\n",
      "163/223, train_loss: 0.7108, step time: 0.0997\n",
      "164/223, train_loss: 0.6909, step time: 0.1008\n",
      "165/223, train_loss: 0.6527, step time: 0.1019\n",
      "166/223, train_loss: 0.7015, step time: 0.1055\n",
      "167/223, train_loss: 0.6962, step time: 0.1102\n",
      "168/223, train_loss: 0.6777, step time: 0.1005\n",
      "169/223, train_loss: 0.6968, step time: 0.1003\n",
      "170/223, train_loss: 0.6738, step time: 0.1003\n",
      "171/223, train_loss: 0.6778, step time: 0.1190\n",
      "172/223, train_loss: 0.6798, step time: 0.1015\n",
      "173/223, train_loss: 0.6732, step time: 0.1032\n",
      "174/223, train_loss: 0.6809, step time: 0.1175\n",
      "175/223, train_loss: 0.6789, step time: 0.1075\n",
      "176/223, train_loss: 0.6712, step time: 0.1123\n",
      "177/223, train_loss: 0.6755, step time: 0.1161\n",
      "178/223, train_loss: 0.6737, step time: 0.1082\n",
      "179/223, train_loss: 0.6799, step time: 0.0995\n",
      "180/223, train_loss: 0.6950, step time: 0.1002\n",
      "181/223, train_loss: 0.6599, step time: 0.1025\n",
      "182/223, train_loss: 0.6742, step time: 0.1134\n",
      "183/223, train_loss: 0.6702, step time: 0.1084\n",
      "184/223, train_loss: 0.6956, step time: 0.1028\n",
      "185/223, train_loss: 0.6886, step time: 0.1061\n",
      "186/223, train_loss: 0.6945, step time: 0.1009\n",
      "187/223, train_loss: 0.6842, step time: 0.1174\n",
      "188/223, train_loss: 0.6875, step time: 0.0997\n",
      "189/223, train_loss: 0.6597, step time: 0.1051\n",
      "190/223, train_loss: 0.6586, step time: 0.1044\n",
      "191/223, train_loss: 0.6639, step time: 0.1047\n",
      "192/223, train_loss: 0.6644, step time: 0.1205\n",
      "193/223, train_loss: 0.6757, step time: 0.1048\n",
      "194/223, train_loss: 0.6687, step time: 0.1030\n",
      "195/223, train_loss: 0.6735, step time: 0.1039\n",
      "196/223, train_loss: 0.6936, step time: 0.1026\n",
      "197/223, train_loss: 0.6648, step time: 0.1199\n",
      "198/223, train_loss: 0.6662, step time: 0.1001\n",
      "199/223, train_loss: 0.6595, step time: 0.0996\n",
      "200/223, train_loss: 0.6693, step time: 0.0996\n",
      "201/223, train_loss: 0.6482, step time: 0.1141\n",
      "202/223, train_loss: 0.6431, step time: 0.1050\n",
      "203/223, train_loss: 0.6625, step time: 0.1003\n",
      "204/223, train_loss: 0.6789, step time: 0.0988\n",
      "205/223, train_loss: 0.6683, step time: 0.1129\n",
      "206/223, train_loss: 0.6567, step time: 0.1037\n",
      "207/223, train_loss: 0.6897, step time: 0.1002\n",
      "208/223, train_loss: 0.6673, step time: 0.1003\n",
      "209/223, train_loss: 0.6626, step time: 0.1009\n",
      "210/223, train_loss: 0.6491, step time: 0.1003\n",
      "211/223, train_loss: 0.6647, step time: 0.1103\n",
      "212/223, train_loss: 0.6714, step time: 0.1105\n",
      "213/223, train_loss: 0.6504, step time: 0.0988\n",
      "214/223, train_loss: 0.6742, step time: 0.1111\n",
      "215/223, train_loss: 0.6809, step time: 0.1018\n",
      "216/223, train_loss: 0.6561, step time: 0.1244\n",
      "217/223, train_loss: 0.6311, step time: 0.1155\n",
      "218/223, train_loss: 0.6804, step time: 0.1002\n",
      "219/223, train_loss: 0.6646, step time: 0.1080\n",
      "220/223, train_loss: 0.6707, step time: 0.1094\n",
      "221/223, train_loss: 0.6657, step time: 0.1047\n",
      "222/223, train_loss: 0.6475, step time: 0.1006\n",
      "223/223, train_loss: 0.6826, step time: 0.0997\n",
      "epoch 1 average loss: 0.7092\n",
      "time consuming of epoch 1 is: 99.6007\n",
      "----------\n",
      "epoch 2/300\n",
      "1/223, train_loss: 0.6639, step time: 0.1021\n",
      "2/223, train_loss: 0.6570, step time: 0.1001\n",
      "3/223, train_loss: 0.6499, step time: 0.0992\n",
      "4/223, train_loss: 0.6667, step time: 0.1032\n",
      "5/223, train_loss: 0.6585, step time: 0.1023\n",
      "6/223, train_loss: 0.6688, step time: 0.1011\n",
      "7/223, train_loss: 0.6613, step time: 0.0997\n",
      "8/223, train_loss: 0.6606, step time: 0.1009\n",
      "9/223, train_loss: 0.6710, step time: 0.1009\n",
      "10/223, train_loss: 0.6500, step time: 0.1054\n",
      "11/223, train_loss: 0.6622, step time: 0.1157\n",
      "12/223, train_loss: 0.6480, step time: 0.1237\n",
      "13/223, train_loss: 0.6569, step time: 0.1150\n",
      "14/223, train_loss: 0.6792, step time: 0.1096\n",
      "15/223, train_loss: 0.6631, step time: 0.1000\n",
      "16/223, train_loss: 0.6785, step time: 0.0999\n",
      "17/223, train_loss: 0.6341, step time: 0.1046\n",
      "18/223, train_loss: 0.6759, step time: 0.1059\n",
      "19/223, train_loss: 0.6595, step time: 0.1053\n",
      "20/223, train_loss: 0.6961, step time: 0.1007\n",
      "21/223, train_loss: 0.6763, step time: 0.1128\n",
      "22/223, train_loss: 0.6495, step time: 0.1097\n",
      "23/223, train_loss: 0.6589, step time: 0.1001\n",
      "24/223, train_loss: 0.6599, step time: 0.1067\n",
      "25/223, train_loss: 0.6481, step time: 0.1005\n",
      "26/223, train_loss: 0.6504, step time: 0.1141\n",
      "27/223, train_loss: 0.6591, step time: 0.1079\n",
      "28/223, train_loss: 0.6543, step time: 0.1170\n",
      "29/223, train_loss: 0.6625, step time: 0.1149\n",
      "30/223, train_loss: 0.6378, step time: 0.1028\n",
      "31/223, train_loss: 0.6779, step time: 0.1104\n",
      "32/223, train_loss: 0.6504, step time: 0.1146\n",
      "33/223, train_loss: 0.6473, step time: 0.1115\n",
      "34/223, train_loss: 0.6238, step time: 0.1120\n",
      "35/223, train_loss: 0.6597, step time: 0.0991\n",
      "36/223, train_loss: 0.6559, step time: 0.1050\n",
      "37/223, train_loss: 0.6612, step time: 0.1081\n",
      "38/223, train_loss: 0.6362, step time: 0.1176\n",
      "39/223, train_loss: 0.6240, step time: 0.1124\n",
      "40/223, train_loss: 0.6697, step time: 0.1332\n",
      "41/223, train_loss: 0.6387, step time: 0.1099\n",
      "42/223, train_loss: 0.6500, step time: 0.1170\n",
      "43/223, train_loss: 0.6349, step time: 0.1072\n",
      "44/223, train_loss: 0.6461, step time: 0.1247\n",
      "45/223, train_loss: 0.6285, step time: 0.1063\n",
      "46/223, train_loss: 0.6339, step time: 0.0999\n",
      "47/223, train_loss: 0.6586, step time: 0.1105\n",
      "48/223, train_loss: 0.6454, step time: 0.1117\n",
      "49/223, train_loss: 0.6392, step time: 0.1061\n",
      "50/223, train_loss: 0.6654, step time: 0.1021\n",
      "51/223, train_loss: 0.6480, step time: 0.1124\n",
      "52/223, train_loss: 0.6540, step time: 0.1130\n",
      "53/223, train_loss: 0.6449, step time: 0.1076\n",
      "54/223, train_loss: 0.6590, step time: 0.1128\n",
      "55/223, train_loss: 0.6503, step time: 0.1124\n",
      "56/223, train_loss: 0.6648, step time: 0.1003\n",
      "57/223, train_loss: 0.6559, step time: 0.1099\n",
      "58/223, train_loss: 0.6559, step time: 0.1219\n",
      "59/223, train_loss: 0.6384, step time: 0.1046\n",
      "60/223, train_loss: 0.6397, step time: 0.1010\n",
      "61/223, train_loss: 0.6609, step time: 0.1103\n",
      "62/223, train_loss: 0.6435, step time: 0.1021\n",
      "63/223, train_loss: 0.6593, step time: 0.1000\n",
      "64/223, train_loss: 0.6685, step time: 0.1064\n",
      "65/223, train_loss: 0.6784, step time: 0.1086\n",
      "66/223, train_loss: 0.6397, step time: 0.1024\n",
      "67/223, train_loss: 0.6725, step time: 0.1094\n",
      "68/223, train_loss: 0.6318, step time: 0.1103\n",
      "69/223, train_loss: 0.6415, step time: 0.1006\n",
      "70/223, train_loss: 0.6499, step time: 0.1198\n",
      "71/223, train_loss: 0.6570, step time: 0.1055\n",
      "72/223, train_loss: 0.6619, step time: 0.1130\n",
      "73/223, train_loss: 0.6543, step time: 0.1007\n",
      "74/223, train_loss: 0.6052, step time: 0.1136\n",
      "75/223, train_loss: 0.6524, step time: 0.1088\n",
      "76/223, train_loss: 0.6415, step time: 0.1082\n",
      "77/223, train_loss: 0.6382, step time: 0.1009\n",
      "78/223, train_loss: 0.6540, step time: 0.1090\n",
      "79/223, train_loss: 0.6207, step time: 0.1062\n",
      "80/223, train_loss: 0.6120, step time: 0.1005\n",
      "81/223, train_loss: 0.6211, step time: 0.1000\n",
      "82/223, train_loss: 0.6267, step time: 0.1162\n",
      "83/223, train_loss: 0.6105, step time: 0.1023\n",
      "84/223, train_loss: 0.6416, step time: 0.1177\n",
      "85/223, train_loss: 0.6551, step time: 0.1021\n",
      "86/223, train_loss: 0.6295, step time: 0.1107\n",
      "87/223, train_loss: 0.6449, step time: 0.1129\n",
      "88/223, train_loss: 0.6226, step time: 0.1011\n",
      "89/223, train_loss: 0.6468, step time: 0.1020\n",
      "90/223, train_loss: 0.6338, step time: 0.1102\n",
      "91/223, train_loss: 0.6317, step time: 0.1002\n",
      "92/223, train_loss: 0.6365, step time: 0.0998\n",
      "93/223, train_loss: 0.6303, step time: 0.1185\n",
      "94/223, train_loss: 0.6345, step time: 0.1116\n",
      "95/223, train_loss: 0.6261, step time: 0.0999\n",
      "96/223, train_loss: 0.6335, step time: 0.1000\n",
      "97/223, train_loss: 0.6388, step time: 0.1063\n",
      "98/223, train_loss: 0.6050, step time: 0.1000\n",
      "99/223, train_loss: 0.6232, step time: 0.1001\n",
      "100/223, train_loss: 0.6572, step time: 0.1001\n",
      "101/223, train_loss: 0.6260, step time: 0.1001\n",
      "102/223, train_loss: 0.6323, step time: 0.1000\n",
      "103/223, train_loss: 0.6265, step time: 0.1038\n",
      "104/223, train_loss: 0.6430, step time: 0.1142\n",
      "105/223, train_loss: 0.6279, step time: 0.1003\n",
      "106/223, train_loss: 0.6125, step time: 0.1041\n",
      "107/223, train_loss: 0.6273, step time: 0.1005\n",
      "108/223, train_loss: 0.6368, step time: 0.1025\n",
      "109/223, train_loss: 0.6140, step time: 0.1194\n",
      "110/223, train_loss: 0.6604, step time: 0.1066\n",
      "111/223, train_loss: 0.6577, step time: 0.1127\n",
      "112/223, train_loss: 0.6298, step time: 0.1082\n",
      "113/223, train_loss: 0.6436, step time: 0.1199\n",
      "114/223, train_loss: 0.6491, step time: 0.1011\n",
      "115/223, train_loss: 0.6322, step time: 0.1004\n",
      "116/223, train_loss: 0.6231, step time: 0.1014\n",
      "117/223, train_loss: 0.6265, step time: 0.0998\n",
      "118/223, train_loss: 0.6172, step time: 0.1173\n",
      "119/223, train_loss: 0.6678, step time: 0.1105\n",
      "120/223, train_loss: 0.6105, step time: 0.1064\n",
      "121/223, train_loss: 0.6292, step time: 0.1167\n",
      "122/223, train_loss: 0.6233, step time: 0.1007\n",
      "123/223, train_loss: 0.5973, step time: 0.1009\n",
      "124/223, train_loss: 0.6178, step time: 0.0999\n",
      "125/223, train_loss: 0.6343, step time: 0.1179\n",
      "126/223, train_loss: 0.6254, step time: 0.1001\n",
      "127/223, train_loss: 0.6251, step time: 0.1007\n",
      "128/223, train_loss: 0.6319, step time: 0.1005\n",
      "129/223, train_loss: 0.6616, step time: 0.1067\n",
      "130/223, train_loss: 0.6185, step time: 0.1092\n",
      "131/223, train_loss: 0.6216, step time: 0.1053\n",
      "132/223, train_loss: 0.6383, step time: 0.0997\n",
      "133/223, train_loss: 0.6294, step time: 0.1079\n",
      "134/223, train_loss: 0.6016, step time: 0.1202\n",
      "135/223, train_loss: 0.6211, step time: 0.1165\n",
      "136/223, train_loss: 0.6049, step time: 0.1001\n",
      "137/223, train_loss: 0.6453, step time: 0.1014\n",
      "138/223, train_loss: 0.6244, step time: 0.1003\n",
      "139/223, train_loss: 0.6003, step time: 0.1007\n",
      "140/223, train_loss: 0.6665, step time: 0.1040\n",
      "141/223, train_loss: 0.6323, step time: 0.1177\n",
      "142/223, train_loss: 0.6175, step time: 0.1170\n",
      "143/223, train_loss: 0.6327, step time: 0.1175\n",
      "144/223, train_loss: 0.6074, step time: 0.1003\n",
      "145/223, train_loss: 0.6185, step time: 0.1093\n",
      "146/223, train_loss: 0.6257, step time: 0.0998\n",
      "147/223, train_loss: 0.5901, step time: 0.1161\n",
      "148/223, train_loss: 0.6166, step time: 0.1019\n",
      "149/223, train_loss: 0.6418, step time: 0.0999\n",
      "150/223, train_loss: 0.6495, step time: 0.1087\n",
      "151/223, train_loss: 0.6203, step time: 0.1151\n",
      "152/223, train_loss: 0.6317, step time: 0.1184\n",
      "153/223, train_loss: 0.6257, step time: 0.1191\n",
      "154/223, train_loss: 0.6030, step time: 0.1146\n",
      "155/223, train_loss: 0.6435, step time: 0.1024\n",
      "156/223, train_loss: 0.6167, step time: 0.0995\n",
      "157/223, train_loss: 0.6146, step time: 0.1090\n",
      "158/223, train_loss: 0.5936, step time: 0.1054\n",
      "159/223, train_loss: 0.6369, step time: 0.1026\n",
      "160/223, train_loss: 0.6152, step time: 0.1012\n",
      "161/223, train_loss: 0.6209, step time: 0.1004\n",
      "162/223, train_loss: 0.6356, step time: 0.1132\n",
      "163/223, train_loss: 0.6222, step time: 0.1007\n",
      "164/223, train_loss: 0.6126, step time: 0.1000\n",
      "165/223, train_loss: 0.6245, step time: 0.1024\n",
      "166/223, train_loss: 0.5982, step time: 0.1004\n",
      "167/223, train_loss: 0.6212, step time: 0.1053\n",
      "168/223, train_loss: 0.6207, step time: 0.1006\n",
      "169/223, train_loss: 0.6222, step time: 0.1001\n",
      "170/223, train_loss: 0.6079, step time: 0.0996\n",
      "171/223, train_loss: 0.6301, step time: 0.1003\n",
      "172/223, train_loss: 0.6090, step time: 0.1220\n",
      "173/223, train_loss: 0.6000, step time: 0.0998\n",
      "174/223, train_loss: 0.6208, step time: 0.1009\n",
      "175/223, train_loss: 0.5930, step time: 0.1055\n",
      "176/223, train_loss: 0.5869, step time: 0.1161\n",
      "177/223, train_loss: 0.6213, step time: 0.1004\n",
      "178/223, train_loss: 0.6063, step time: 0.1150\n",
      "179/223, train_loss: 0.6020, step time: 0.1010\n",
      "180/223, train_loss: 0.6108, step time: 0.1116\n",
      "181/223, train_loss: 0.5960, step time: 0.1035\n",
      "182/223, train_loss: 0.5974, step time: 0.1075\n",
      "183/223, train_loss: 0.6272, step time: 0.1022\n",
      "184/223, train_loss: 0.5966, step time: 0.0994\n",
      "185/223, train_loss: 0.6251, step time: 0.0994\n",
      "186/223, train_loss: 0.5893, step time: 0.1076\n",
      "187/223, train_loss: 0.6200, step time: 0.1310\n",
      "188/223, train_loss: 0.6170, step time: 0.1004\n",
      "189/223, train_loss: 0.6073, step time: 0.1124\n",
      "190/223, train_loss: 0.6089, step time: 0.1053\n",
      "191/223, train_loss: 0.5996, step time: 0.1044\n",
      "192/223, train_loss: 0.6043, step time: 0.0997\n",
      "193/223, train_loss: 0.5996, step time: 0.0995\n",
      "194/223, train_loss: 0.5786, step time: 0.1077\n",
      "195/223, train_loss: 0.5875, step time: 0.0994\n",
      "196/223, train_loss: 0.5909, step time: 0.1001\n",
      "197/223, train_loss: 0.6157, step time: 0.1029\n",
      "198/223, train_loss: 0.6180, step time: 0.1208\n",
      "199/223, train_loss: 0.6186, step time: 0.0988\n",
      "200/223, train_loss: 0.6176, step time: 0.1166\n",
      "201/223, train_loss: 0.5871, step time: 0.0986\n",
      "202/223, train_loss: 0.5695, step time: 0.1066\n",
      "203/223, train_loss: 0.6006, step time: 0.1111\n",
      "204/223, train_loss: 0.6350, step time: 0.1134\n",
      "205/223, train_loss: 0.5924, step time: 0.0996\n",
      "206/223, train_loss: 0.6106, step time: 0.1126\n",
      "207/223, train_loss: 0.5891, step time: 0.1354\n",
      "208/223, train_loss: 0.5854, step time: 0.1142\n",
      "209/223, train_loss: 0.6115, step time: 0.0996\n",
      "210/223, train_loss: 0.6249, step time: 0.1013\n",
      "211/223, train_loss: 0.6128, step time: 0.0995\n",
      "212/223, train_loss: 0.6109, step time: 0.1183\n",
      "213/223, train_loss: 0.6278, step time: 0.1104\n",
      "214/223, train_loss: 0.5759, step time: 0.1092\n",
      "215/223, train_loss: 0.5934, step time: 0.1006\n",
      "216/223, train_loss: 0.6067, step time: 0.1011\n",
      "217/223, train_loss: 0.6034, step time: 0.1051\n",
      "218/223, train_loss: 0.5854, step time: 0.1023\n",
      "219/223, train_loss: 0.6195, step time: 0.1005\n",
      "220/223, train_loss: 0.6113, step time: 0.0997\n",
      "221/223, train_loss: 0.5735, step time: 0.0998\n",
      "222/223, train_loss: 0.5821, step time: 0.0991\n",
      "223/223, train_loss: 0.6316, step time: 0.0990\n",
      "epoch 2 average loss: 0.6310\n",
      "time consuming of epoch 2 is: 84.6359\n",
      "----------\n",
      "epoch 3/300\n",
      "1/223, train_loss: 0.5949, step time: 0.1053\n",
      "2/223, train_loss: 0.6029, step time: 0.1112\n",
      "3/223, train_loss: 0.5996, step time: 0.1263\n",
      "4/223, train_loss: 0.5821, step time: 0.1265\n",
      "5/223, train_loss: 0.5936, step time: 0.1053\n",
      "6/223, train_loss: 0.6150, step time: 0.0997\n",
      "7/223, train_loss: 0.5789, step time: 0.0998\n",
      "8/223, train_loss: 0.5867, step time: 0.1273\n",
      "9/223, train_loss: 0.6052, step time: 0.1112\n",
      "10/223, train_loss: 0.6147, step time: 0.1001\n",
      "11/223, train_loss: 0.6106, step time: 0.1010\n",
      "12/223, train_loss: 0.5911, step time: 0.1086\n",
      "13/223, train_loss: 0.6122, step time: 0.1026\n",
      "14/223, train_loss: 0.5646, step time: 0.1002\n",
      "15/223, train_loss: 0.6172, step time: 0.1189\n",
      "16/223, train_loss: 0.5824, step time: 0.1095\n",
      "17/223, train_loss: 0.5951, step time: 0.1140\n",
      "18/223, train_loss: 0.5946, step time: 0.1198\n",
      "19/223, train_loss: 0.5879, step time: 0.1088\n",
      "20/223, train_loss: 0.5686, step time: 0.1095\n",
      "21/223, train_loss: 0.5871, step time: 0.1003\n",
      "22/223, train_loss: 0.6027, step time: 0.0996\n",
      "23/223, train_loss: 0.5703, step time: 0.0999\n",
      "24/223, train_loss: 0.6092, step time: 0.1000\n",
      "25/223, train_loss: 0.5975, step time: 0.1073\n",
      "26/223, train_loss: 0.5864, step time: 0.0992\n",
      "27/223, train_loss: 0.5755, step time: 0.0999\n",
      "28/223, train_loss: 0.6408, step time: 0.1215\n",
      "29/223, train_loss: 0.5832, step time: 0.1175\n",
      "30/223, train_loss: 0.5782, step time: 0.1101\n",
      "31/223, train_loss: 0.5860, step time: 0.1068\n",
      "32/223, train_loss: 0.5760, step time: 0.1076\n",
      "33/223, train_loss: 0.5828, step time: 0.1026\n",
      "34/223, train_loss: 0.6179, step time: 0.1039\n",
      "35/223, train_loss: 0.5702, step time: 0.1000\n",
      "36/223, train_loss: 0.5783, step time: 0.1013\n",
      "37/223, train_loss: 0.5743, step time: 0.0998\n",
      "38/223, train_loss: 0.5738, step time: 0.1004\n",
      "39/223, train_loss: 0.5958, step time: 0.1001\n",
      "40/223, train_loss: 0.5797, step time: 0.1074\n",
      "41/223, train_loss: 0.6006, step time: 0.1006\n",
      "42/223, train_loss: 0.5699, step time: 0.0998\n",
      "43/223, train_loss: 0.6028, step time: 0.1001\n",
      "44/223, train_loss: 0.5863, step time: 0.1199\n",
      "45/223, train_loss: 0.5946, step time: 0.1013\n",
      "46/223, train_loss: 0.5927, step time: 0.0993\n",
      "47/223, train_loss: 0.5783, step time: 0.1001\n",
      "48/223, train_loss: 0.5753, step time: 0.1240\n",
      "49/223, train_loss: 0.6049, step time: 0.1053\n",
      "50/223, train_loss: 0.5895, step time: 0.1210\n",
      "51/223, train_loss: 0.5854, step time: 0.1367\n",
      "52/223, train_loss: 0.5919, step time: 0.1106\n",
      "53/223, train_loss: 0.5872, step time: 0.1072\n",
      "54/223, train_loss: 0.5852, step time: 0.1082\n",
      "55/223, train_loss: 0.5806, step time: 0.1273\n",
      "56/223, train_loss: 0.5883, step time: 0.1008\n",
      "57/223, train_loss: 0.5751, step time: 0.1093\n",
      "58/223, train_loss: 0.5778, step time: 0.1198\n",
      "59/223, train_loss: 0.5796, step time: 0.1246\n",
      "60/223, train_loss: 0.6008, step time: 0.1170\n",
      "61/223, train_loss: 0.5570, step time: 0.1131\n",
      "62/223, train_loss: 0.6017, step time: 0.1053\n",
      "63/223, train_loss: 0.5964, step time: 0.0999\n",
      "64/223, train_loss: 0.6187, step time: 0.1016\n",
      "65/223, train_loss: 0.5954, step time: 0.1132\n",
      "66/223, train_loss: 0.5857, step time: 0.1018\n",
      "67/223, train_loss: 0.5952, step time: 0.1172\n",
      "68/223, train_loss: 0.5911, step time: 0.1030\n",
      "69/223, train_loss: 0.5646, step time: 0.1174\n",
      "70/223, train_loss: 0.6151, step time: 0.1059\n",
      "71/223, train_loss: 0.5918, step time: 0.1006\n",
      "72/223, train_loss: 0.5760, step time: 0.1014\n",
      "73/223, train_loss: 0.5805, step time: 0.1153\n",
      "74/223, train_loss: 0.5976, step time: 0.1274\n",
      "75/223, train_loss: 0.5489, step time: 0.1000\n",
      "76/223, train_loss: 0.5682, step time: 0.1115\n",
      "77/223, train_loss: 0.5520, step time: 0.1001\n",
      "78/223, train_loss: 0.5786, step time: 0.0995\n",
      "79/223, train_loss: 0.5869, step time: 0.1008\n",
      "80/223, train_loss: 0.5945, step time: 0.1135\n",
      "81/223, train_loss: 0.5492, step time: 0.1076\n",
      "82/223, train_loss: 0.5455, step time: 0.1112\n",
      "83/223, train_loss: 0.5520, step time: 0.1112\n",
      "84/223, train_loss: 0.5649, step time: 0.1166\n",
      "85/223, train_loss: 0.5617, step time: 0.1003\n",
      "86/223, train_loss: 0.5881, step time: 0.1200\n",
      "87/223, train_loss: 0.5658, step time: 0.1085\n",
      "88/223, train_loss: 0.5969, step time: 0.1216\n",
      "89/223, train_loss: 0.5783, step time: 0.1171\n",
      "90/223, train_loss: 0.5962, step time: 0.1157\n",
      "91/223, train_loss: 0.5789, step time: 0.1160\n",
      "92/223, train_loss: 0.6002, step time: 0.1289\n",
      "93/223, train_loss: 0.5516, step time: 0.1029\n",
      "94/223, train_loss: 0.5602, step time: 0.0993\n",
      "95/223, train_loss: 0.6060, step time: 0.1003\n",
      "96/223, train_loss: 0.5577, step time: 0.1005\n",
      "97/223, train_loss: 0.5908, step time: 0.1134\n",
      "98/223, train_loss: 0.5360, step time: 0.1155\n",
      "99/223, train_loss: 0.5731, step time: 0.1322\n",
      "100/223, train_loss: 0.5655, step time: 0.1006\n",
      "101/223, train_loss: 0.5883, step time: 0.1140\n",
      "102/223, train_loss: 0.5566, step time: 0.1106\n",
      "103/223, train_loss: 0.5734, step time: 0.0996\n",
      "104/223, train_loss: 0.5850, step time: 0.1078\n",
      "105/223, train_loss: 0.5512, step time: 0.1050\n",
      "106/223, train_loss: 0.5584, step time: 0.1198\n",
      "107/223, train_loss: 0.5739, step time: 0.0991\n",
      "108/223, train_loss: 0.5568, step time: 0.0990\n",
      "109/223, train_loss: 0.5629, step time: 0.1003\n",
      "110/223, train_loss: 0.5602, step time: 0.1015\n",
      "111/223, train_loss: 0.5799, step time: 0.0983\n",
      "112/223, train_loss: 0.5717, step time: 0.1047\n",
      "113/223, train_loss: 0.5825, step time: 0.1088\n",
      "114/223, train_loss: 0.5686, step time: 0.1125\n",
      "115/223, train_loss: 0.5868, step time: 0.1009\n",
      "116/223, train_loss: 0.5578, step time: 0.0996\n",
      "117/223, train_loss: 0.5989, step time: 0.1184\n",
      "118/223, train_loss: 0.5717, step time: 0.1005\n",
      "119/223, train_loss: 0.5890, step time: 0.1005\n",
      "120/223, train_loss: 0.5633, step time: 0.1008\n",
      "121/223, train_loss: 0.5711, step time: 0.1147\n",
      "122/223, train_loss: 0.5570, step time: 0.1006\n",
      "123/223, train_loss: 0.5536, step time: 0.1007\n",
      "124/223, train_loss: 0.5862, step time: 0.1007\n",
      "125/223, train_loss: 0.5822, step time: 0.1031\n",
      "126/223, train_loss: 0.5408, step time: 0.1010\n",
      "127/223, train_loss: 0.5793, step time: 0.1266\n",
      "128/223, train_loss: 0.5391, step time: 0.1184\n",
      "129/223, train_loss: 0.5797, step time: 0.1092\n",
      "130/223, train_loss: 0.5607, step time: 0.1059\n",
      "131/223, train_loss: 0.5753, step time: 0.1110\n",
      "132/223, train_loss: 0.5603, step time: 0.1029\n",
      "133/223, train_loss: 0.5559, step time: 0.1085\n",
      "134/223, train_loss: 0.5497, step time: 0.1191\n",
      "135/223, train_loss: 0.5751, step time: 0.1115\n",
      "136/223, train_loss: 0.5528, step time: 0.1001\n",
      "137/223, train_loss: 0.5690, step time: 0.1095\n",
      "138/223, train_loss: 0.5646, step time: 0.1244\n",
      "139/223, train_loss: 0.5594, step time: 0.1096\n",
      "140/223, train_loss: 0.5430, step time: 0.1013\n",
      "141/223, train_loss: 0.5624, step time: 0.1124\n",
      "142/223, train_loss: 0.5518, step time: 0.1255\n",
      "143/223, train_loss: 0.5477, step time: 0.1105\n",
      "144/223, train_loss: 0.5816, step time: 0.1159\n",
      "145/223, train_loss: 0.5570, step time: 0.1033\n",
      "146/223, train_loss: 0.5915, step time: 0.1141\n",
      "147/223, train_loss: 0.5455, step time: 0.1088\n",
      "148/223, train_loss: 0.5858, step time: 0.1072\n",
      "149/223, train_loss: 0.5897, step time: 0.1008\n",
      "150/223, train_loss: 0.5794, step time: 0.1076\n",
      "151/223, train_loss: 0.5599, step time: 0.1095\n",
      "152/223, train_loss: 0.5756, step time: 0.0994\n",
      "153/223, train_loss: 0.5652, step time: 0.1135\n",
      "154/223, train_loss: 0.5703, step time: 0.1064\n",
      "155/223, train_loss: 0.5543, step time: 0.1102\n",
      "156/223, train_loss: 0.5408, step time: 0.1170\n",
      "157/223, train_loss: 0.5729, step time: 0.1212\n",
      "158/223, train_loss: 0.5729, step time: 0.1040\n",
      "159/223, train_loss: 0.5551, step time: 0.1061\n",
      "160/223, train_loss: 0.5529, step time: 0.0993\n",
      "161/223, train_loss: 0.5684, step time: 0.1063\n",
      "162/223, train_loss: 0.5459, step time: 0.1137\n",
      "163/223, train_loss: 0.5527, step time: 0.1055\n",
      "164/223, train_loss: 0.5642, step time: 0.1140\n",
      "165/223, train_loss: 0.5447, step time: 0.1073\n",
      "166/223, train_loss: 0.5668, step time: 0.0985\n",
      "167/223, train_loss: 0.5511, step time: 0.0993\n",
      "168/223, train_loss: 0.5399, step time: 0.0987\n",
      "169/223, train_loss: 0.5483, step time: 0.1131\n",
      "170/223, train_loss: 0.5410, step time: 0.1057\n",
      "171/223, train_loss: 0.5205, step time: 0.1165\n",
      "172/223, train_loss: 0.5379, step time: 0.1080\n",
      "173/223, train_loss: 0.5616, step time: 0.1012\n",
      "174/223, train_loss: 0.5407, step time: 0.1005\n",
      "175/223, train_loss: 0.5650, step time: 0.1103\n",
      "176/223, train_loss: 0.5544, step time: 0.1029\n",
      "177/223, train_loss: 0.5460, step time: 0.1002\n",
      "178/223, train_loss: 0.5657, step time: 0.0999\n",
      "179/223, train_loss: 0.5582, step time: 0.1008\n",
      "180/223, train_loss: 0.5567, step time: 0.1091\n",
      "181/223, train_loss: 0.5459, step time: 0.1101\n",
      "182/223, train_loss: 0.5322, step time: 0.0992\n",
      "183/223, train_loss: 0.5353, step time: 0.1171\n",
      "184/223, train_loss: 0.5804, step time: 0.1107\n",
      "185/223, train_loss: 0.5535, step time: 0.1132\n",
      "186/223, train_loss: 0.5718, step time: 0.0992\n",
      "187/223, train_loss: 0.5262, step time: 0.1042\n",
      "188/223, train_loss: 0.5468, step time: 0.1105\n",
      "189/223, train_loss: 0.5585, step time: 0.1112\n",
      "190/223, train_loss: 0.5361, step time: 0.0986\n",
      "191/223, train_loss: 0.5411, step time: 0.1008\n",
      "192/223, train_loss: 0.5815, step time: 0.1007\n",
      "193/223, train_loss: 0.5497, step time: 0.1260\n",
      "194/223, train_loss: 0.5440, step time: 0.1359\n",
      "195/223, train_loss: 0.5643, step time: 0.1000\n",
      "196/223, train_loss: 0.5568, step time: 0.1000\n",
      "197/223, train_loss: 0.5355, step time: 0.1080\n",
      "198/223, train_loss: 0.5502, step time: 0.1211\n",
      "199/223, train_loss: 0.5395, step time: 0.1251\n",
      "200/223, train_loss: 0.5545, step time: 0.1042\n",
      "201/223, train_loss: 0.5204, step time: 0.1141\n",
      "202/223, train_loss: 0.5526, step time: 0.1168\n",
      "203/223, train_loss: 0.5552, step time: 0.1188\n",
      "204/223, train_loss: 0.5331, step time: 0.1100\n",
      "205/223, train_loss: 0.5491, step time: 0.1069\n",
      "206/223, train_loss: 0.5408, step time: 0.1071\n",
      "207/223, train_loss: 0.5314, step time: 0.1131\n",
      "208/223, train_loss: 0.5526, step time: 0.1128\n",
      "209/223, train_loss: 0.5383, step time: 0.1154\n",
      "210/223, train_loss: 0.5378, step time: 0.1144\n",
      "211/223, train_loss: 0.5471, step time: 0.1079\n",
      "212/223, train_loss: 0.5426, step time: 0.1242\n",
      "213/223, train_loss: 0.5420, step time: 0.1051\n",
      "214/223, train_loss: 0.5433, step time: 0.1178\n",
      "215/223, train_loss: 0.5319, step time: 0.1096\n",
      "216/223, train_loss: 0.5640, step time: 0.1071\n",
      "217/223, train_loss: 0.5116, step time: 0.1055\n",
      "218/223, train_loss: 0.5296, step time: 0.1176\n",
      "219/223, train_loss: 0.5463, step time: 0.1103\n",
      "220/223, train_loss: 0.5679, step time: 0.1185\n",
      "221/223, train_loss: 0.5602, step time: 0.0989\n",
      "222/223, train_loss: 0.5526, step time: 0.0994\n",
      "223/223, train_loss: 0.5723, step time: 0.0996\n",
      "epoch 3 average loss: 0.5699\n",
      "time consuming of epoch 3 is: 86.0820\n",
      "----------\n",
      "epoch 4/300\n",
      "1/223, train_loss: 0.5525, step time: 0.1003\n",
      "2/223, train_loss: 0.5429, step time: 0.1081\n",
      "3/223, train_loss: 0.5536, step time: 0.1157\n",
      "4/223, train_loss: 0.5428, step time: 0.1108\n",
      "5/223, train_loss: 0.5399, step time: 0.1195\n",
      "6/223, train_loss: 0.5480, step time: 0.1263\n",
      "7/223, train_loss: 0.5047, step time: 0.1219\n",
      "8/223, train_loss: 0.5721, step time: 0.1244\n",
      "9/223, train_loss: 0.5485, step time: 0.1064\n",
      "10/223, train_loss: 0.5291, step time: 0.1156\n",
      "11/223, train_loss: 0.5396, step time: 0.1134\n",
      "12/223, train_loss: 0.5534, step time: 0.1061\n",
      "13/223, train_loss: 0.5443, step time: 0.1101\n",
      "14/223, train_loss: 0.5451, step time: 0.1090\n",
      "15/223, train_loss: 0.5539, step time: 0.1213\n",
      "16/223, train_loss: 0.5377, step time: 0.1102\n",
      "17/223, train_loss: 0.5538, step time: 0.1169\n",
      "18/223, train_loss: 0.5592, step time: 0.1004\n",
      "19/223, train_loss: 0.5473, step time: 0.1002\n",
      "20/223, train_loss: 0.5227, step time: 0.1015\n",
      "21/223, train_loss: 0.5592, step time: 0.1010\n",
      "22/223, train_loss: 0.5492, step time: 0.1175\n",
      "23/223, train_loss: 0.5225, step time: 0.1151\n",
      "24/223, train_loss: 0.5455, step time: 0.1035\n",
      "25/223, train_loss: 0.5733, step time: 0.1064\n",
      "26/223, train_loss: 0.5458, step time: 0.1101\n",
      "27/223, train_loss: 0.5146, step time: 0.1000\n",
      "28/223, train_loss: 0.5408, step time: 0.1000\n",
      "29/223, train_loss: 0.5110, step time: 0.1016\n",
      "30/223, train_loss: 0.5261, step time: 0.1166\n",
      "31/223, train_loss: 0.5425, step time: 0.1007\n",
      "32/223, train_loss: 0.5167, step time: 0.1217\n",
      "33/223, train_loss: 0.5350, step time: 0.1036\n",
      "34/223, train_loss: 0.5110, step time: 0.1096\n",
      "35/223, train_loss: 0.5600, step time: 0.1088\n",
      "36/223, train_loss: 0.5218, step time: 0.1127\n",
      "37/223, train_loss: 0.5059, step time: 0.1180\n",
      "38/223, train_loss: 0.5206, step time: 0.1275\n",
      "39/223, train_loss: 0.5490, step time: 0.1124\n",
      "40/223, train_loss: 0.5257, step time: 0.1052\n",
      "41/223, train_loss: 0.5255, step time: 0.1221\n",
      "42/223, train_loss: 0.5146, step time: 0.1124\n",
      "43/223, train_loss: 0.5353, step time: 0.1055\n",
      "44/223, train_loss: 0.5297, step time: 0.1081\n",
      "45/223, train_loss: 0.5106, step time: 0.1023\n",
      "46/223, train_loss: 0.5314, step time: 0.1204\n",
      "47/223, train_loss: 0.5380, step time: 0.1112\n",
      "48/223, train_loss: 0.5278, step time: 0.1082\n",
      "49/223, train_loss: 0.5392, step time: 0.1004\n",
      "50/223, train_loss: 0.5504, step time: 0.1054\n",
      "51/223, train_loss: 0.5218, step time: 0.1101\n",
      "52/223, train_loss: 0.5235, step time: 0.1009\n",
      "53/223, train_loss: 0.5078, step time: 0.1176\n",
      "54/223, train_loss: 0.5251, step time: 0.1164\n",
      "55/223, train_loss: 0.5365, step time: 0.1089\n",
      "56/223, train_loss: 0.5246, step time: 0.1245\n",
      "57/223, train_loss: 0.5246, step time: 0.1159\n",
      "58/223, train_loss: 0.5264, step time: 0.1090\n",
      "59/223, train_loss: 0.5090, step time: 0.1078\n",
      "60/223, train_loss: 0.5241, step time: 0.1138\n",
      "61/223, train_loss: 0.5116, step time: 0.1189\n",
      "62/223, train_loss: 0.5264, step time: 0.1154\n",
      "63/223, train_loss: 0.5166, step time: 0.1173\n",
      "64/223, train_loss: 0.5067, step time: 0.1138\n",
      "65/223, train_loss: 0.5224, step time: 0.0992\n",
      "66/223, train_loss: 0.5119, step time: 0.1176\n",
      "67/223, train_loss: 0.5233, step time: 0.1110\n",
      "68/223, train_loss: 0.5214, step time: 0.1042\n",
      "69/223, train_loss: 0.5376, step time: 0.1063\n",
      "70/223, train_loss: 0.5295, step time: 0.1106\n",
      "71/223, train_loss: 0.5147, step time: 0.1095\n",
      "72/223, train_loss: 0.5150, step time: 0.1077\n",
      "73/223, train_loss: 0.5153, step time: 0.1126\n",
      "74/223, train_loss: 0.5151, step time: 0.1024\n",
      "75/223, train_loss: 0.5366, step time: 0.1131\n",
      "76/223, train_loss: 0.5224, step time: 0.1164\n",
      "77/223, train_loss: 0.5421, step time: 0.1225\n",
      "78/223, train_loss: 0.5330, step time: 0.1102\n",
      "79/223, train_loss: 0.5284, step time: 0.1142\n",
      "80/223, train_loss: 0.5356, step time: 0.1053\n",
      "81/223, train_loss: 0.5477, step time: 0.1007\n",
      "82/223, train_loss: 0.5322, step time: 0.1118\n",
      "83/223, train_loss: 0.5239, step time: 0.1075\n",
      "84/223, train_loss: 0.5217, step time: 0.1003\n",
      "85/223, train_loss: 0.5319, step time: 0.1060\n",
      "86/223, train_loss: 0.4991, step time: 0.1098\n",
      "87/223, train_loss: 0.5285, step time: 0.1047\n",
      "88/223, train_loss: 0.5118, step time: 0.1128\n",
      "89/223, train_loss: 0.5106, step time: 0.1116\n",
      "90/223, train_loss: 0.5042, step time: 0.1000\n",
      "91/223, train_loss: 0.4823, step time: 0.1073\n",
      "92/223, train_loss: 0.5102, step time: 0.1063\n",
      "93/223, train_loss: 0.5169, step time: 0.1002\n",
      "94/223, train_loss: 0.5264, step time: 0.1003\n",
      "95/223, train_loss: 0.5086, step time: 0.1119\n",
      "96/223, train_loss: 0.5181, step time: 0.0989\n",
      "97/223, train_loss: 0.5247, step time: 0.0999\n",
      "98/223, train_loss: 0.5209, step time: 0.0994\n",
      "99/223, train_loss: 0.5335, step time: 0.1189\n",
      "100/223, train_loss: 0.4996, step time: 0.1079\n",
      "101/223, train_loss: 0.5091, step time: 0.0994\n",
      "102/223, train_loss: 0.5268, step time: 0.0996\n",
      "103/223, train_loss: 0.5185, step time: 0.1059\n",
      "104/223, train_loss: 0.5199, step time: 0.1086\n",
      "105/223, train_loss: 0.5097, step time: 0.1052\n",
      "106/223, train_loss: 0.5163, step time: 0.1157\n",
      "107/223, train_loss: 0.5011, step time: 0.1039\n",
      "108/223, train_loss: 0.5087, step time: 0.1081\n",
      "109/223, train_loss: 0.5044, step time: 0.1205\n",
      "110/223, train_loss: 0.4986, step time: 0.1036\n",
      "111/223, train_loss: 0.5099, step time: 0.1005\n",
      "112/223, train_loss: 0.4920, step time: 0.0994\n",
      "113/223, train_loss: 0.5044, step time: 0.1142\n",
      "114/223, train_loss: 0.5031, step time: 0.1008\n",
      "115/223, train_loss: 0.5074, step time: 0.1043\n",
      "116/223, train_loss: 0.4927, step time: 0.1086\n",
      "117/223, train_loss: 0.5020, step time: 0.1064\n",
      "118/223, train_loss: 0.5446, step time: 0.1092\n",
      "119/223, train_loss: 0.5053, step time: 0.1049\n",
      "120/223, train_loss: 0.5375, step time: 0.1116\n",
      "121/223, train_loss: 0.4934, step time: 0.1144\n",
      "122/223, train_loss: 0.5176, step time: 0.1266\n",
      "123/223, train_loss: 0.4923, step time: 0.1219\n",
      "124/223, train_loss: 0.5156, step time: 0.1012\n",
      "125/223, train_loss: 0.5055, step time: 0.1055\n",
      "126/223, train_loss: 0.5047, step time: 0.1134\n",
      "127/223, train_loss: 0.4870, step time: 0.1077\n",
      "128/223, train_loss: 0.5303, step time: 0.1216\n",
      "129/223, train_loss: 0.5158, step time: 0.1302\n",
      "130/223, train_loss: 0.5154, step time: 0.1198\n",
      "131/223, train_loss: 0.5350, step time: 0.1005\n",
      "132/223, train_loss: 0.5081, step time: 0.1068\n",
      "133/223, train_loss: 0.5125, step time: 0.1071\n",
      "134/223, train_loss: 0.5415, step time: 0.1264\n",
      "135/223, train_loss: 0.5032, step time: 0.1149\n",
      "136/223, train_loss: 0.5081, step time: 0.1087\n",
      "137/223, train_loss: 0.5291, step time: 0.1135\n",
      "138/223, train_loss: 0.5137, step time: 0.1123\n",
      "139/223, train_loss: 0.5016, step time: 0.1150\n",
      "140/223, train_loss: 0.4858, step time: 0.1389\n",
      "141/223, train_loss: 0.5073, step time: 0.1233\n",
      "142/223, train_loss: 0.5321, step time: 0.1105\n",
      "143/223, train_loss: 0.4991, step time: 0.1182\n",
      "144/223, train_loss: 0.4878, step time: 0.1206\n",
      "145/223, train_loss: 0.5267, step time: 0.1104\n",
      "146/223, train_loss: 0.4939, step time: 0.1181\n",
      "147/223, train_loss: 0.5180, step time: 0.1022\n",
      "148/223, train_loss: 0.5096, step time: 0.1082\n",
      "149/223, train_loss: 0.4688, step time: 0.1082\n",
      "150/223, train_loss: 0.4910, step time: 0.1137\n",
      "151/223, train_loss: 0.5120, step time: 0.1150\n",
      "152/223, train_loss: 0.4811, step time: 0.1104\n",
      "153/223, train_loss: 0.5141, step time: 0.1164\n",
      "154/223, train_loss: 0.5129, step time: 0.1069\n",
      "155/223, train_loss: 0.4928, step time: 0.1084\n",
      "156/223, train_loss: 0.4916, step time: 0.1015\n",
      "157/223, train_loss: 0.4841, step time: 0.1037\n",
      "158/223, train_loss: 0.4902, step time: 0.1040\n",
      "159/223, train_loss: 0.5021, step time: 0.1020\n",
      "160/223, train_loss: 0.5098, step time: 0.1085\n",
      "161/223, train_loss: 0.5004, step time: 0.1069\n",
      "162/223, train_loss: 0.4931, step time: 0.1005\n",
      "163/223, train_loss: 0.5315, step time: 0.1106\n",
      "164/223, train_loss: 0.5049, step time: 0.1127\n",
      "165/223, train_loss: 0.4952, step time: 0.1129\n",
      "166/223, train_loss: 0.5215, step time: 0.1002\n",
      "167/223, train_loss: 0.5077, step time: 0.1040\n",
      "168/223, train_loss: 0.5007, step time: 0.1310\n",
      "169/223, train_loss: 0.5182, step time: 0.1101\n",
      "170/223, train_loss: 0.5260, step time: 0.1001\n",
      "171/223, train_loss: 0.4930, step time: 0.1102\n",
      "172/223, train_loss: 0.5223, step time: 0.1022\n",
      "173/223, train_loss: 0.5233, step time: 0.1009\n",
      "174/223, train_loss: 0.5064, step time: 0.1002\n",
      "175/223, train_loss: 0.5009, step time: 0.1070\n",
      "176/223, train_loss: 0.5319, step time: 0.1059\n",
      "177/223, train_loss: 0.5047, step time: 0.1089\n",
      "178/223, train_loss: 0.5021, step time: 0.1109\n",
      "179/223, train_loss: 0.5097, step time: 0.1118\n",
      "180/223, train_loss: 0.4934, step time: 0.1037\n",
      "181/223, train_loss: 0.5125, step time: 0.1004\n",
      "182/223, train_loss: 0.5008, step time: 0.0998\n",
      "183/223, train_loss: 0.4915, step time: 0.1026\n",
      "184/223, train_loss: 0.4842, step time: 0.1123\n",
      "185/223, train_loss: 0.5080, step time: 0.1143\n",
      "186/223, train_loss: 0.5299, step time: 0.1002\n",
      "187/223, train_loss: 0.4724, step time: 0.1050\n",
      "188/223, train_loss: 0.5094, step time: 0.1002\n",
      "189/223, train_loss: 0.5069, step time: 0.1021\n",
      "190/223, train_loss: 0.4855, step time: 0.1004\n",
      "191/223, train_loss: 0.5347, step time: 0.1124\n",
      "192/223, train_loss: 0.5008, step time: 0.1003\n",
      "193/223, train_loss: 0.5184, step time: 0.1350\n",
      "194/223, train_loss: 0.4784, step time: 0.1001\n",
      "195/223, train_loss: 0.5008, step time: 0.1139\n",
      "196/223, train_loss: 0.4877, step time: 0.1006\n",
      "197/223, train_loss: 0.4700, step time: 0.1002\n",
      "198/223, train_loss: 0.4829, step time: 0.1006\n",
      "199/223, train_loss: 0.4818, step time: 0.1043\n",
      "200/223, train_loss: 0.4815, step time: 0.1049\n",
      "201/223, train_loss: 0.4848, step time: 0.0997\n",
      "202/223, train_loss: 0.4694, step time: 0.1078\n",
      "203/223, train_loss: 0.5069, step time: 0.1003\n",
      "204/223, train_loss: 0.4941, step time: 0.1047\n",
      "205/223, train_loss: 0.5057, step time: 0.0994\n",
      "206/223, train_loss: 0.4919, step time: 0.1037\n",
      "207/223, train_loss: 0.5124, step time: 0.1203\n",
      "208/223, train_loss: 0.5001, step time: 0.1099\n",
      "209/223, train_loss: 0.5140, step time: 0.0994\n",
      "210/223, train_loss: 0.4918, step time: 0.0998\n",
      "211/223, train_loss: 0.4941, step time: 0.1080\n",
      "212/223, train_loss: 0.4950, step time: 0.1080\n",
      "213/223, train_loss: 0.5192, step time: 0.1160\n",
      "214/223, train_loss: 0.5055, step time: 0.1004\n",
      "215/223, train_loss: 0.4750, step time: 0.1157\n",
      "216/223, train_loss: 0.4883, step time: 0.1059\n",
      "217/223, train_loss: 0.4975, step time: 0.1024\n",
      "218/223, train_loss: 0.4985, step time: 0.1001\n",
      "219/223, train_loss: 0.4802, step time: 0.0992\n",
      "220/223, train_loss: 0.4857, step time: 0.0998\n",
      "221/223, train_loss: 0.5069, step time: 0.0997\n",
      "222/223, train_loss: 0.4810, step time: 0.1002\n",
      "223/223, train_loss: 0.4872, step time: 0.0988\n",
      "epoch 4 average loss: 0.5151\n",
      "time consuming of epoch 4 is: 84.3845\n",
      "----------\n",
      "epoch 5/300\n",
      "1/223, train_loss: 0.4791, step time: 0.1177\n",
      "2/223, train_loss: 0.4855, step time: 0.1116\n",
      "3/223, train_loss: 0.4802, step time: 0.1145\n",
      "4/223, train_loss: 0.4907, step time: 0.1286\n",
      "5/223, train_loss: 0.5227, step time: 0.1004\n",
      "6/223, train_loss: 0.4697, step time: 0.0998\n",
      "7/223, train_loss: 0.4928, step time: 0.1134\n",
      "8/223, train_loss: 0.4773, step time: 0.1213\n",
      "9/223, train_loss: 0.4804, step time: 0.1137\n",
      "10/223, train_loss: 0.4800, step time: 0.1020\n",
      "11/223, train_loss: 0.4882, step time: 0.1527\n",
      "12/223, train_loss: 0.4782, step time: 0.1126\n",
      "13/223, train_loss: 0.4822, step time: 0.1149\n",
      "14/223, train_loss: 0.4819, step time: 0.0993\n",
      "15/223, train_loss: 0.4780, step time: 0.1000\n",
      "16/223, train_loss: 0.4842, step time: 0.1150\n",
      "17/223, train_loss: 0.4853, step time: 0.1068\n",
      "18/223, train_loss: 0.4758, step time: 0.1071\n",
      "19/223, train_loss: 0.4685, step time: 0.1077\n",
      "20/223, train_loss: 0.4876, step time: 0.1104\n",
      "21/223, train_loss: 0.4594, step time: 0.1031\n",
      "22/223, train_loss: 0.4716, step time: 0.1136\n",
      "23/223, train_loss: 0.4749, step time: 0.1004\n",
      "24/223, train_loss: 0.4859, step time: 0.1012\n",
      "25/223, train_loss: 0.4995, step time: 0.1086\n",
      "26/223, train_loss: 0.4650, step time: 0.1092\n",
      "27/223, train_loss: 0.4718, step time: 0.1048\n",
      "28/223, train_loss: 0.4747, step time: 0.1072\n",
      "29/223, train_loss: 0.4738, step time: 0.1037\n",
      "30/223, train_loss: 0.4834, step time: 0.1004\n",
      "31/223, train_loss: 0.4799, step time: 0.1004\n",
      "32/223, train_loss: 0.4732, step time: 0.0997\n",
      "33/223, train_loss: 0.4758, step time: 0.1009\n",
      "34/223, train_loss: 0.4888, step time: 0.1007\n",
      "35/223, train_loss: 0.4920, step time: 0.1079\n",
      "36/223, train_loss: 0.4713, step time: 0.1105\n",
      "37/223, train_loss: 0.4780, step time: 0.1028\n",
      "38/223, train_loss: 0.4626, step time: 0.1204\n",
      "39/223, train_loss: 0.4651, step time: 0.1110\n",
      "40/223, train_loss: 0.4626, step time: 0.1065\n",
      "41/223, train_loss: 0.4854, step time: 0.1151\n",
      "42/223, train_loss: 0.4958, step time: 0.1083\n",
      "43/223, train_loss: 0.4607, step time: 0.1280\n",
      "44/223, train_loss: 0.4734, step time: 0.1051\n",
      "45/223, train_loss: 0.4687, step time: 0.1107\n",
      "46/223, train_loss: 0.4745, step time: 0.1063\n",
      "47/223, train_loss: 0.4684, step time: 0.1071\n",
      "48/223, train_loss: 0.4908, step time: 0.1150\n",
      "49/223, train_loss: 0.4748, step time: 0.1173\n",
      "50/223, train_loss: 0.4712, step time: 0.1368\n",
      "51/223, train_loss: 0.4829, step time: 0.1155\n",
      "52/223, train_loss: 0.4658, step time: 0.1036\n",
      "53/223, train_loss: 0.4736, step time: 0.1054\n",
      "54/223, train_loss: 0.4893, step time: 0.1001\n",
      "55/223, train_loss: 0.4803, step time: 0.1004\n",
      "56/223, train_loss: 0.4718, step time: 0.1013\n",
      "57/223, train_loss: 0.4753, step time: 0.1010\n",
      "58/223, train_loss: 0.4690, step time: 0.1024\n",
      "59/223, train_loss: 0.4630, step time: 0.1150\n",
      "60/223, train_loss: 0.4525, step time: 0.1001\n",
      "61/223, train_loss: 0.4589, step time: 0.1152\n",
      "62/223, train_loss: 0.4691, step time: 0.1110\n",
      "63/223, train_loss: 0.4778, step time: 0.0996\n",
      "64/223, train_loss: 0.4586, step time: 0.1011\n",
      "65/223, train_loss: 0.4798, step time: 0.1112\n",
      "66/223, train_loss: 0.4702, step time: 0.1062\n",
      "67/223, train_loss: 0.4719, step time: 0.1049\n",
      "68/223, train_loss: 0.4609, step time: 0.1049\n",
      "69/223, train_loss: 0.4574, step time: 0.1084\n",
      "70/223, train_loss: 0.4492, step time: 0.1132\n",
      "71/223, train_loss: 0.4717, step time: 0.1176\n",
      "72/223, train_loss: 0.4688, step time: 0.1090\n",
      "73/223, train_loss: 0.4703, step time: 0.1142\n",
      "74/223, train_loss: 0.4712, step time: 0.1135\n",
      "75/223, train_loss: 0.4600, step time: 0.1117\n",
      "76/223, train_loss: 0.4729, step time: 0.0998\n",
      "77/223, train_loss: 0.4661, step time: 0.1005\n",
      "78/223, train_loss: 0.4911, step time: 0.1279\n",
      "79/223, train_loss: 0.4598, step time: 0.1096\n",
      "80/223, train_loss: 0.4628, step time: 0.1001\n",
      "81/223, train_loss: 0.4793, step time: 0.0998\n",
      "82/223, train_loss: 0.4565, step time: 0.1075\n",
      "83/223, train_loss: 0.4759, step time: 0.1067\n",
      "84/223, train_loss: 0.4602, step time: 0.1092\n",
      "85/223, train_loss: 0.4680, step time: 0.1084\n",
      "86/223, train_loss: 0.4584, step time: 0.1142\n",
      "87/223, train_loss: 0.4620, step time: 0.1123\n",
      "88/223, train_loss: 0.4751, step time: 0.1063\n",
      "89/223, train_loss: 0.4640, step time: 0.0997\n",
      "90/223, train_loss: 0.4727, step time: 0.0996\n",
      "91/223, train_loss: 0.4676, step time: 0.0991\n",
      "92/223, train_loss: 0.4704, step time: 0.1002\n",
      "93/223, train_loss: 0.5485, step time: 0.1081\n",
      "94/223, train_loss: 0.4568, step time: 0.1094\n",
      "95/223, train_loss: 0.4726, step time: 0.1176\n",
      "96/223, train_loss: 0.4543, step time: 0.1007\n",
      "97/223, train_loss: 0.4587, step time: 0.1008\n",
      "98/223, train_loss: 0.4440, step time: 0.1020\n",
      "99/223, train_loss: 0.4639, step time: 0.1147\n",
      "100/223, train_loss: 0.4701, step time: 0.1000\n",
      "101/223, train_loss: 0.4692, step time: 0.1078\n",
      "102/223, train_loss: 0.4544, step time: 0.1043\n",
      "103/223, train_loss: 0.4459, step time: 0.1200\n",
      "104/223, train_loss: 0.4743, step time: 0.0997\n",
      "105/223, train_loss: 0.4423, step time: 0.1003\n",
      "106/223, train_loss: 0.4680, step time: 0.1073\n",
      "107/223, train_loss: 0.4918, step time: 0.1059\n",
      "108/223, train_loss: 0.4500, step time: 0.1193\n",
      "109/223, train_loss: 0.4618, step time: 0.1105\n",
      "110/223, train_loss: 0.4800, step time: 0.0997\n",
      "111/223, train_loss: 0.4569, step time: 0.1052\n",
      "112/223, train_loss: 0.4506, step time: 0.1159\n",
      "113/223, train_loss: 0.4758, step time: 0.1038\n",
      "114/223, train_loss: 0.4548, step time: 0.1003\n",
      "115/223, train_loss: 0.4303, step time: 0.1045\n",
      "116/223, train_loss: 0.4604, step time: 0.1186\n",
      "117/223, train_loss: 0.4796, step time: 0.1008\n",
      "118/223, train_loss: 0.4500, step time: 0.0999\n",
      "119/223, train_loss: 0.4407, step time: 0.1273\n",
      "120/223, train_loss: 0.4508, step time: 0.1002\n",
      "121/223, train_loss: 0.4518, step time: 0.1002\n",
      "122/223, train_loss: 0.4539, step time: 0.1012\n",
      "123/223, train_loss: 0.4605, step time: 0.1051\n",
      "124/223, train_loss: 0.4633, step time: 0.1198\n",
      "125/223, train_loss: 0.4703, step time: 0.1122\n",
      "126/223, train_loss: 0.4390, step time: 0.1093\n",
      "127/223, train_loss: 0.4615, step time: 0.1002\n",
      "128/223, train_loss: 0.4455, step time: 0.1000\n",
      "129/223, train_loss: 0.4664, step time: 0.1006\n",
      "130/223, train_loss: 0.4317, step time: 0.1081\n",
      "131/223, train_loss: 0.4634, step time: 0.1135\n",
      "132/223, train_loss: 0.4352, step time: 0.1156\n",
      "133/223, train_loss: 0.4524, step time: 0.1098\n",
      "134/223, train_loss: 0.4307, step time: 0.1197\n",
      "135/223, train_loss: 0.4458, step time: 0.0997\n",
      "136/223, train_loss: 0.4615, step time: 0.1134\n",
      "137/223, train_loss: 0.4516, step time: 0.1018\n",
      "138/223, train_loss: 0.4708, step time: 0.1019\n",
      "139/223, train_loss: 0.4595, step time: 0.1078\n",
      "140/223, train_loss: 0.4694, step time: 0.1004\n",
      "141/223, train_loss: 0.4685, step time: 0.0998\n",
      "142/223, train_loss: 0.4557, step time: 0.1000\n",
      "143/223, train_loss: 0.4578, step time: 0.1009\n",
      "144/223, train_loss: 0.4341, step time: 0.1051\n",
      "145/223, train_loss: 0.4425, step time: 0.1069\n",
      "146/223, train_loss: 0.4730, step time: 0.1023\n",
      "147/223, train_loss: 0.4762, step time: 0.1089\n",
      "148/223, train_loss: 0.4396, step time: 0.1134\n",
      "149/223, train_loss: 0.4428, step time: 0.1025\n",
      "150/223, train_loss: 0.4491, step time: 0.1010\n",
      "151/223, train_loss: 0.4370, step time: 0.1096\n",
      "152/223, train_loss: 0.4484, step time: 0.1130\n",
      "153/223, train_loss: 0.4599, step time: 0.1002\n",
      "154/223, train_loss: 0.4543, step time: 0.0996\n",
      "155/223, train_loss: 0.4552, step time: 0.1183\n",
      "156/223, train_loss: 0.4331, step time: 0.1051\n",
      "157/223, train_loss: 0.4593, step time: 0.1015\n",
      "158/223, train_loss: 0.4473, step time: 0.1245\n",
      "159/223, train_loss: 0.4460, step time: 0.1101\n",
      "160/223, train_loss: 0.4479, step time: 0.1131\n",
      "161/223, train_loss: 0.4557, step time: 0.1007\n",
      "162/223, train_loss: 0.4444, step time: 0.1003\n",
      "163/223, train_loss: 0.4573, step time: 0.1089\n",
      "164/223, train_loss: 0.4650, step time: 0.1283\n",
      "165/223, train_loss: 0.4243, step time: 0.1014\n",
      "166/223, train_loss: 0.4419, step time: 0.1009\n",
      "167/223, train_loss: 0.4308, step time: 0.1122\n",
      "168/223, train_loss: 0.4250, step time: 0.1050\n",
      "169/223, train_loss: 0.4488, step time: 0.1003\n",
      "170/223, train_loss: 0.4351, step time: 0.1005\n",
      "171/223, train_loss: 0.4289, step time: 0.1166\n",
      "172/223, train_loss: 0.4311, step time: 0.1109\n",
      "173/223, train_loss: 0.4524, step time: 0.1021\n",
      "174/223, train_loss: 0.4516, step time: 0.1002\n",
      "175/223, train_loss: 0.4321, step time: 0.1087\n",
      "176/223, train_loss: 0.4257, step time: 0.1096\n",
      "177/223, train_loss: 0.4616, step time: 0.0998\n",
      "178/223, train_loss: 0.4346, step time: 0.1020\n",
      "179/223, train_loss: 0.4596, step time: 0.1124\n",
      "180/223, train_loss: 0.4507, step time: 0.1129\n",
      "181/223, train_loss: 0.4501, step time: 0.0994\n",
      "182/223, train_loss: 0.4422, step time: 0.0993\n",
      "183/223, train_loss: 0.4539, step time: 0.1222\n",
      "184/223, train_loss: 0.4528, step time: 0.1014\n",
      "185/223, train_loss: 0.4593, step time: 0.0999\n",
      "186/223, train_loss: 0.4312, step time: 0.1146\n",
      "187/223, train_loss: 0.4357, step time: 0.1195\n",
      "188/223, train_loss: 0.4246, step time: 0.1128\n",
      "189/223, train_loss: 0.4274, step time: 0.1001\n",
      "190/223, train_loss: 0.4368, step time: 0.1184\n",
      "191/223, train_loss: 0.4275, step time: 0.1092\n",
      "192/223, train_loss: 0.4347, step time: 0.1007\n",
      "193/223, train_loss: 0.4453, step time: 0.1064\n",
      "194/223, train_loss: 0.4369, step time: 0.1111\n",
      "195/223, train_loss: 0.4410, step time: 0.1095\n",
      "196/223, train_loss: 0.4658, step time: 0.1143\n",
      "197/223, train_loss: 0.4340, step time: 0.1025\n",
      "198/223, train_loss: 0.4212, step time: 0.1012\n",
      "199/223, train_loss: 0.4312, step time: 0.1054\n",
      "200/223, train_loss: 0.4243, step time: 0.1101\n",
      "201/223, train_loss: 0.4284, step time: 0.1056\n",
      "202/223, train_loss: 0.4417, step time: 0.1017\n",
      "203/223, train_loss: 0.4322, step time: 0.1125\n",
      "204/223, train_loss: 0.4108, step time: 0.1270\n",
      "205/223, train_loss: 0.4493, step time: 0.1074\n",
      "206/223, train_loss: 0.4249, step time: 0.1044\n",
      "207/223, train_loss: 0.4259, step time: 0.1128\n",
      "208/223, train_loss: 0.4261, step time: 0.1082\n",
      "209/223, train_loss: 0.4374, step time: 0.1005\n",
      "210/223, train_loss: 0.4452, step time: 0.1020\n",
      "211/223, train_loss: 0.4475, step time: 0.1293\n",
      "212/223, train_loss: 0.4277, step time: 0.1027\n",
      "213/223, train_loss: 0.4431, step time: 0.1104\n",
      "214/223, train_loss: 0.4427, step time: 0.1115\n",
      "215/223, train_loss: 0.4344, step time: 0.1023\n",
      "216/223, train_loss: 0.4257, step time: 0.1146\n",
      "217/223, train_loss: 0.4480, step time: 0.0998\n",
      "218/223, train_loss: 0.4349, step time: 0.0998\n",
      "219/223, train_loss: 0.4313, step time: 0.1096\n",
      "220/223, train_loss: 0.4315, step time: 0.0995\n",
      "221/223, train_loss: 0.4360, step time: 0.0993\n",
      "222/223, train_loss: 0.4277, step time: 0.1002\n",
      "223/223, train_loss: 0.4272, step time: 0.1003\n",
      "epoch 5 average loss: 0.4588\n",
      "saved new best metric model\n",
      "current epoch: 5 current mean dice: 0.6299 tc: 0.7075 wt: 0.5913 et: 0.5908\n",
      "best mean dice: 0.6299 at epoch: 5\n",
      "time consuming of epoch 5 is: 93.5210\n",
      "----------\n",
      "epoch 6/300\n",
      "1/223, train_loss: 0.4326, step time: 0.1072\n",
      "2/223, train_loss: 0.4342, step time: 0.1024\n",
      "3/223, train_loss: 0.4323, step time: 0.1007\n",
      "4/223, train_loss: 0.4497, step time: 0.0998\n",
      "5/223, train_loss: 0.4462, step time: 0.1077\n",
      "6/223, train_loss: 0.4451, step time: 0.1112\n",
      "7/223, train_loss: 0.4370, step time: 0.1197\n",
      "8/223, train_loss: 0.5020, step time: 0.1001\n",
      "9/223, train_loss: 0.4525, step time: 0.1213\n",
      "10/223, train_loss: 0.4213, step time: 0.1116\n",
      "11/223, train_loss: 0.4368, step time: 0.1105\n",
      "12/223, train_loss: 0.4077, step time: 0.1135\n",
      "13/223, train_loss: 0.4272, step time: 0.1001\n",
      "14/223, train_loss: 0.4379, step time: 0.0995\n",
      "15/223, train_loss: 0.4284, step time: 0.1000\n",
      "16/223, train_loss: 0.4366, step time: 0.1009\n",
      "17/223, train_loss: 0.4048, step time: 0.1001\n",
      "18/223, train_loss: 0.4210, step time: 0.1001\n",
      "19/223, train_loss: 0.4278, step time: 0.1136\n",
      "20/223, train_loss: 0.4354, step time: 0.1264\n",
      "21/223, train_loss: 0.4154, step time: 0.0987\n",
      "22/223, train_loss: 0.4242, step time: 0.0989\n",
      "23/223, train_loss: 0.4103, step time: 0.1006\n",
      "24/223, train_loss: 0.4278, step time: 0.0999\n",
      "25/223, train_loss: 0.4217, step time: 0.0989\n",
      "26/223, train_loss: 0.4100, step time: 0.1088\n",
      "27/223, train_loss: 0.4172, step time: 0.1218\n",
      "28/223, train_loss: 0.4196, step time: 0.1039\n",
      "29/223, train_loss: 0.4447, step time: 0.1004\n",
      "30/223, train_loss: 0.4128, step time: 0.0999\n",
      "31/223, train_loss: 0.4231, step time: 0.1153\n",
      "32/223, train_loss: 0.4182, step time: 0.1093\n",
      "33/223, train_loss: 0.4035, step time: 0.1043\n",
      "34/223, train_loss: 0.4172, step time: 0.1001\n",
      "35/223, train_loss: 0.4181, step time: 0.0994\n",
      "36/223, train_loss: 0.4163, step time: 0.1006\n",
      "37/223, train_loss: 0.4168, step time: 0.1062\n",
      "38/223, train_loss: 0.4157, step time: 0.1003\n",
      "39/223, train_loss: 0.4199, step time: 0.1097\n",
      "40/223, train_loss: 0.4343, step time: 0.1050\n",
      "41/223, train_loss: 0.4157, step time: 0.1003\n",
      "42/223, train_loss: 0.4155, step time: 0.1025\n",
      "43/223, train_loss: 0.4263, step time: 0.1148\n",
      "44/223, train_loss: 0.4283, step time: 0.1007\n",
      "45/223, train_loss: 0.4245, step time: 0.1001\n",
      "46/223, train_loss: 0.4140, step time: 0.1002\n",
      "47/223, train_loss: 0.4419, step time: 0.0999\n",
      "48/223, train_loss: 0.4310, step time: 0.1005\n",
      "49/223, train_loss: 0.4171, step time: 0.0999\n",
      "50/223, train_loss: 0.4261, step time: 0.1251\n",
      "51/223, train_loss: 0.3988, step time: 0.1039\n",
      "52/223, train_loss: 0.4219, step time: 0.1101\n",
      "53/223, train_loss: 0.4165, step time: 0.1000\n",
      "54/223, train_loss: 0.4223, step time: 0.1138\n",
      "55/223, train_loss: 0.4108, step time: 0.1086\n",
      "56/223, train_loss: 0.4086, step time: 0.1112\n",
      "57/223, train_loss: 0.4051, step time: 0.1176\n",
      "58/223, train_loss: 0.4236, step time: 0.1120\n",
      "59/223, train_loss: 0.4295, step time: 0.1124\n",
      "60/223, train_loss: 0.4099, step time: 0.0997\n",
      "61/223, train_loss: 0.4375, step time: 0.0997\n",
      "62/223, train_loss: 0.4072, step time: 0.1086\n",
      "63/223, train_loss: 0.4044, step time: 0.1000\n",
      "64/223, train_loss: 0.4015, step time: 0.1001\n",
      "65/223, train_loss: 0.4135, step time: 0.1023\n",
      "66/223, train_loss: 0.4173, step time: 0.1003\n",
      "67/223, train_loss: 0.4110, step time: 0.1062\n",
      "68/223, train_loss: 0.4137, step time: 0.1003\n",
      "69/223, train_loss: 0.4143, step time: 0.1133\n",
      "70/223, train_loss: 0.4268, step time: 0.1112\n",
      "71/223, train_loss: 0.3929, step time: 0.1000\n",
      "72/223, train_loss: 0.4062, step time: 0.1034\n",
      "73/223, train_loss: 0.4192, step time: 0.1078\n",
      "74/223, train_loss: 0.4051, step time: 0.1005\n",
      "75/223, train_loss: 0.4112, step time: 0.1420\n",
      "76/223, train_loss: 0.4118, step time: 0.1058\n",
      "77/223, train_loss: 0.4128, step time: 0.0999\n",
      "78/223, train_loss: 0.4082, step time: 0.1107\n",
      "79/223, train_loss: 0.4076, step time: 0.1172\n",
      "80/223, train_loss: 0.4189, step time: 0.1174\n",
      "81/223, train_loss: 0.4115, step time: 0.1220\n",
      "82/223, train_loss: 0.4254, step time: 0.1081\n",
      "83/223, train_loss: 0.4040, step time: 0.1057\n",
      "84/223, train_loss: 0.4427, step time: 0.1221\n",
      "85/223, train_loss: 0.4032, step time: 0.1094\n",
      "86/223, train_loss: 0.4128, step time: 0.1105\n",
      "87/223, train_loss: 0.4052, step time: 0.1087\n",
      "88/223, train_loss: 0.4250, step time: 0.0996\n",
      "89/223, train_loss: 0.4007, step time: 0.1003\n",
      "90/223, train_loss: 0.4189, step time: 0.1007\n",
      "91/223, train_loss: 0.4012, step time: 0.1007\n",
      "92/223, train_loss: 0.4081, step time: 0.0994\n",
      "93/223, train_loss: 0.4244, step time: 0.1009\n",
      "94/223, train_loss: 0.4038, step time: 0.1001\n",
      "95/223, train_loss: 0.4067, step time: 0.0997\n",
      "96/223, train_loss: 0.4122, step time: 0.1002\n",
      "97/223, train_loss: 0.4035, step time: 0.1070\n",
      "98/223, train_loss: 0.4247, step time: 0.1032\n",
      "99/223, train_loss: 0.4049, step time: 0.1024\n",
      "100/223, train_loss: 0.4225, step time: 0.1011\n",
      "101/223, train_loss: 0.4095, step time: 0.1141\n",
      "102/223, train_loss: 0.3969, step time: 0.1017\n",
      "103/223, train_loss: 0.4268, step time: 0.1138\n",
      "104/223, train_loss: 0.4224, step time: 0.1247\n",
      "105/223, train_loss: 0.4063, step time: 0.1034\n",
      "106/223, train_loss: 0.4352, step time: 0.1002\n",
      "107/223, train_loss: 0.3963, step time: 0.1003\n",
      "108/223, train_loss: 0.4160, step time: 0.1001\n",
      "109/223, train_loss: 0.4125, step time: 0.1041\n",
      "110/223, train_loss: 0.4315, step time: 0.1001\n",
      "111/223, train_loss: 0.3947, step time: 0.0998\n",
      "112/223, train_loss: 0.3893, step time: 0.0994\n",
      "113/223, train_loss: 0.4057, step time: 0.1064\n",
      "114/223, train_loss: 0.4292, step time: 0.1134\n",
      "115/223, train_loss: 0.4143, step time: 0.1018\n",
      "116/223, train_loss: 0.3925, step time: 0.0998\n",
      "117/223, train_loss: 0.3995, step time: 0.1109\n",
      "118/223, train_loss: 0.4045, step time: 0.1011\n",
      "119/223, train_loss: 0.4026, step time: 0.1065\n",
      "120/223, train_loss: 0.3818, step time: 0.1001\n",
      "121/223, train_loss: 0.4109, step time: 0.0998\n",
      "122/223, train_loss: 0.4084, step time: 0.1050\n",
      "123/223, train_loss: 0.4109, step time: 0.1218\n",
      "124/223, train_loss: 0.3984, step time: 0.0985\n",
      "125/223, train_loss: 0.3990, step time: 0.1000\n",
      "126/223, train_loss: 0.3955, step time: 0.1079\n",
      "127/223, train_loss: 0.4096, step time: 0.1108\n",
      "128/223, train_loss: 0.4125, step time: 0.0999\n",
      "129/223, train_loss: 0.3791, step time: 0.1102\n",
      "130/223, train_loss: 0.3847, step time: 0.1143\n",
      "131/223, train_loss: 0.4128, step time: 0.1171\n",
      "132/223, train_loss: 0.4165, step time: 0.1007\n",
      "133/223, train_loss: 0.4065, step time: 0.1101\n",
      "134/223, train_loss: 0.4055, step time: 0.1121\n",
      "135/223, train_loss: 0.3941, step time: 0.0998\n",
      "136/223, train_loss: 0.3922, step time: 0.0996\n",
      "137/223, train_loss: 0.3979, step time: 0.1115\n",
      "138/223, train_loss: 0.4012, step time: 0.1005\n",
      "139/223, train_loss: 0.3919, step time: 0.1143\n",
      "140/223, train_loss: 0.4036, step time: 0.1005\n",
      "141/223, train_loss: 0.3998, step time: 0.1010\n",
      "142/223, train_loss: 0.4003, step time: 0.1101\n",
      "143/223, train_loss: 0.3954, step time: 0.1055\n",
      "144/223, train_loss: 0.4010, step time: 0.1161\n",
      "145/223, train_loss: 0.3888, step time: 0.1068\n",
      "146/223, train_loss: 0.3912, step time: 0.1001\n",
      "147/223, train_loss: 0.3806, step time: 0.1245\n",
      "148/223, train_loss: 0.3965, step time: 0.1056\n",
      "149/223, train_loss: 0.4030, step time: 0.1178\n",
      "150/223, train_loss: 0.4064, step time: 0.1007\n",
      "151/223, train_loss: 0.3898, step time: 0.1083\n",
      "152/223, train_loss: 0.3883, step time: 0.1096\n",
      "153/223, train_loss: 0.3971, step time: 0.1098\n",
      "154/223, train_loss: 0.4033, step time: 0.1121\n",
      "155/223, train_loss: 0.4119, step time: 0.1085\n",
      "156/223, train_loss: 0.4096, step time: 0.1043\n",
      "157/223, train_loss: 0.3782, step time: 0.1063\n",
      "158/223, train_loss: 0.3813, step time: 0.1001\n",
      "159/223, train_loss: 0.4050, step time: 0.1124\n",
      "160/223, train_loss: 0.3967, step time: 0.1008\n",
      "161/223, train_loss: 0.3834, step time: 0.1029\n",
      "162/223, train_loss: 0.3934, step time: 0.1501\n",
      "163/223, train_loss: 0.4005, step time: 0.1072\n",
      "164/223, train_loss: 0.3823, step time: 0.1116\n",
      "165/223, train_loss: 0.3737, step time: 0.1102\n",
      "166/223, train_loss: 0.3964, step time: 0.1103\n",
      "167/223, train_loss: 0.3981, step time: 0.1028\n",
      "168/223, train_loss: 0.3762, step time: 0.1058\n",
      "169/223, train_loss: 0.4179, step time: 0.1124\n",
      "170/223, train_loss: 0.3826, step time: 0.1226\n",
      "171/223, train_loss: 0.3998, step time: 0.0998\n",
      "172/223, train_loss: 0.4040, step time: 0.1003\n",
      "173/223, train_loss: 0.3809, step time: 0.1209\n",
      "174/223, train_loss: 0.3940, step time: 0.1156\n",
      "175/223, train_loss: 0.4032, step time: 0.1047\n",
      "176/223, train_loss: 0.4007, step time: 0.1000\n",
      "177/223, train_loss: 0.3973, step time: 0.1108\n",
      "178/223, train_loss: 0.3855, step time: 0.0995\n",
      "179/223, train_loss: 0.4060, step time: 0.1085\n",
      "180/223, train_loss: 0.3846, step time: 0.1012\n",
      "181/223, train_loss: 0.3791, step time: 0.1003\n",
      "182/223, train_loss: 0.3816, step time: 0.1051\n",
      "183/223, train_loss: 0.3901, step time: 0.1002\n",
      "184/223, train_loss: 0.3901, step time: 0.1003\n",
      "185/223, train_loss: 0.3818, step time: 0.1020\n",
      "186/223, train_loss: 0.3960, step time: 0.1206\n",
      "187/223, train_loss: 0.3727, step time: 0.1176\n",
      "188/223, train_loss: 0.3899, step time: 0.1158\n",
      "189/223, train_loss: 0.3832, step time: 0.1155\n",
      "190/223, train_loss: 0.3800, step time: 0.1001\n",
      "191/223, train_loss: 0.3767, step time: 0.1022\n",
      "192/223, train_loss: 0.3825, step time: 0.1005\n",
      "193/223, train_loss: 0.3889, step time: 0.1071\n",
      "194/223, train_loss: 0.3898, step time: 0.1150\n",
      "195/223, train_loss: 0.3645, step time: 0.1007\n",
      "196/223, train_loss: 0.3864, step time: 0.1000\n",
      "197/223, train_loss: 0.3746, step time: 0.1040\n",
      "198/223, train_loss: 0.3936, step time: 0.1006\n",
      "199/223, train_loss: 0.3819, step time: 0.1077\n",
      "200/223, train_loss: 0.3804, step time: 0.1010\n",
      "201/223, train_loss: 0.3766, step time: 0.1060\n",
      "202/223, train_loss: 0.3843, step time: 0.1002\n",
      "203/223, train_loss: 0.4010, step time: 0.1023\n",
      "204/223, train_loss: 0.3844, step time: 0.1009\n",
      "205/223, train_loss: 0.3773, step time: 0.1008\n",
      "206/223, train_loss: 0.3787, step time: 0.1002\n",
      "207/223, train_loss: 0.3702, step time: 0.1001\n",
      "208/223, train_loss: 0.3725, step time: 0.0996\n",
      "209/223, train_loss: 0.4011, step time: 0.1081\n",
      "210/223, train_loss: 0.3754, step time: 0.1023\n",
      "211/223, train_loss: 0.3705, step time: 0.1004\n",
      "212/223, train_loss: 0.3810, step time: 0.1008\n",
      "213/223, train_loss: 0.3864, step time: 0.1045\n",
      "214/223, train_loss: 0.4001, step time: 0.1000\n",
      "215/223, train_loss: 0.3704, step time: 0.1018\n",
      "216/223, train_loss: 0.3648, step time: 0.1010\n",
      "217/223, train_loss: 0.3721, step time: 0.1004\n",
      "218/223, train_loss: 0.3756, step time: 0.1000\n",
      "219/223, train_loss: 0.3997, step time: 0.1003\n",
      "220/223, train_loss: 0.3921, step time: 0.1002\n",
      "221/223, train_loss: 0.3879, step time: 0.0993\n",
      "222/223, train_loss: 0.3803, step time: 0.0993\n",
      "223/223, train_loss: 0.3719, step time: 0.0998\n",
      "epoch 6 average loss: 0.4054\n",
      "time consuming of epoch 6 is: 85.2512\n",
      "----------\n",
      "epoch 7/300\n",
      "1/223, train_loss: 0.3649, step time: 0.1007\n",
      "2/223, train_loss: 0.3801, step time: 0.0999\n",
      "3/223, train_loss: 0.3878, step time: 0.1135\n",
      "4/223, train_loss: 0.3711, step time: 0.1111\n",
      "5/223, train_loss: 0.3807, step time: 0.1251\n",
      "6/223, train_loss: 0.3922, step time: 0.1124\n",
      "7/223, train_loss: 0.3714, step time: 0.1236\n",
      "8/223, train_loss: 0.3804, step time: 0.0998\n",
      "9/223, train_loss: 0.3916, step time: 0.1149\n",
      "10/223, train_loss: 0.3587, step time: 0.1102\n",
      "11/223, train_loss: 0.3782, step time: 0.1130\n",
      "12/223, train_loss: 0.3782, step time: 0.1104\n",
      "13/223, train_loss: 0.4021, step time: 0.1136\n",
      "14/223, train_loss: 0.3920, step time: 0.1220\n",
      "15/223, train_loss: 0.3619, step time: 0.1051\n",
      "16/223, train_loss: 0.3631, step time: 0.1159\n",
      "17/223, train_loss: 0.3654, step time: 0.1157\n",
      "18/223, train_loss: 0.3625, step time: 0.0998\n",
      "19/223, train_loss: 0.3879, step time: 0.1173\n",
      "20/223, train_loss: 0.3763, step time: 0.1033\n",
      "21/223, train_loss: 0.3865, step time: 0.1160\n",
      "22/223, train_loss: 0.4071, step time: 0.1125\n",
      "23/223, train_loss: 0.3829, step time: 0.1181\n",
      "24/223, train_loss: 0.3716, step time: 0.1187\n",
      "25/223, train_loss: 0.3925, step time: 0.1208\n",
      "26/223, train_loss: 0.3892, step time: 0.1150\n",
      "27/223, train_loss: 0.3619, step time: 0.1295\n",
      "28/223, train_loss: 0.3681, step time: 0.1014\n",
      "29/223, train_loss: 0.3759, step time: 0.1179\n",
      "30/223, train_loss: 0.3800, step time: 0.1139\n",
      "31/223, train_loss: 0.3637, step time: 0.1246\n",
      "32/223, train_loss: 0.3753, step time: 0.1171\n",
      "33/223, train_loss: 0.3781, step time: 0.1208\n",
      "34/223, train_loss: 0.3767, step time: 0.1211\n",
      "35/223, train_loss: 0.3722, step time: 0.1045\n",
      "36/223, train_loss: 0.3712, step time: 0.1002\n",
      "37/223, train_loss: 0.3589, step time: 0.0999\n",
      "38/223, train_loss: 0.3780, step time: 0.0996\n",
      "39/223, train_loss: 0.3628, step time: 0.1019\n",
      "40/223, train_loss: 0.3696, step time: 0.1047\n",
      "41/223, train_loss: 0.3723, step time: 0.1109\n",
      "42/223, train_loss: 0.3926, step time: 0.1030\n",
      "43/223, train_loss: 0.3663, step time: 0.1007\n",
      "44/223, train_loss: 0.3607, step time: 0.1083\n",
      "45/223, train_loss: 0.3746, step time: 0.1184\n",
      "46/223, train_loss: 0.3747, step time: 0.1134\n",
      "47/223, train_loss: 0.3839, step time: 0.1055\n",
      "48/223, train_loss: 0.3631, step time: 0.1063\n",
      "49/223, train_loss: 0.3801, step time: 0.1052\n",
      "50/223, train_loss: 0.3576, step time: 0.1049\n",
      "51/223, train_loss: 0.3581, step time: 0.1003\n",
      "52/223, train_loss: 0.3766, step time: 0.1022\n",
      "53/223, train_loss: 0.3641, step time: 0.1020\n",
      "54/223, train_loss: 0.3786, step time: 0.1021\n",
      "55/223, train_loss: 0.3884, step time: 0.1007\n",
      "56/223, train_loss: 0.3843, step time: 0.1010\n",
      "57/223, train_loss: 0.3540, step time: 0.1176\n",
      "58/223, train_loss: 0.3689, step time: 0.1340\n",
      "59/223, train_loss: 0.3489, step time: 0.1006\n",
      "60/223, train_loss: 0.3726, step time: 0.1000\n",
      "61/223, train_loss: 0.3625, step time: 0.1143\n",
      "62/223, train_loss: 0.3549, step time: 0.1002\n",
      "63/223, train_loss: 0.3695, step time: 0.1047\n",
      "64/223, train_loss: 0.3763, step time: 0.1006\n",
      "65/223, train_loss: 0.3742, step time: 0.1031\n",
      "66/223, train_loss: 0.3677, step time: 0.1046\n",
      "67/223, train_loss: 0.3774, step time: 0.1073\n",
      "68/223, train_loss: 0.3785, step time: 0.1010\n",
      "69/223, train_loss: 0.3619, step time: 0.1103\n",
      "70/223, train_loss: 0.3715, step time: 0.1105\n",
      "71/223, train_loss: 0.3515, step time: 0.1019\n",
      "72/223, train_loss: 0.3533, step time: 0.0999\n",
      "73/223, train_loss: 0.3657, step time: 0.1055\n",
      "74/223, train_loss: 0.3644, step time: 0.1095\n",
      "75/223, train_loss: 0.3517, step time: 0.1149\n",
      "76/223, train_loss: 0.3725, step time: 0.1119\n",
      "77/223, train_loss: 0.3587, step time: 0.1015\n",
      "78/223, train_loss: 0.3696, step time: 0.1004\n",
      "79/223, train_loss: 0.3665, step time: 0.0995\n",
      "80/223, train_loss: 0.3562, step time: 0.1007\n",
      "81/223, train_loss: 0.3601, step time: 0.1059\n",
      "82/223, train_loss: 0.3650, step time: 0.0999\n",
      "83/223, train_loss: 0.3547, step time: 0.1123\n",
      "84/223, train_loss: 0.3636, step time: 0.1070\n",
      "85/223, train_loss: 0.3563, step time: 0.1094\n",
      "86/223, train_loss: 0.3772, step time: 0.1101\n",
      "87/223, train_loss: 0.3655, step time: 0.1077\n",
      "88/223, train_loss: 0.3655, step time: 0.1052\n",
      "89/223, train_loss: 0.3680, step time: 0.1040\n",
      "90/223, train_loss: 0.3661, step time: 0.1003\n",
      "91/223, train_loss: 0.3642, step time: 0.1006\n",
      "92/223, train_loss: 0.3467, step time: 0.1020\n",
      "93/223, train_loss: 0.3554, step time: 0.1007\n",
      "94/223, train_loss: 0.3466, step time: 0.1051\n",
      "95/223, train_loss: 0.3634, step time: 0.1178\n",
      "96/223, train_loss: 0.3753, step time: 0.0998\n",
      "97/223, train_loss: 0.3574, step time: 0.1099\n",
      "98/223, train_loss: 0.3659, step time: 0.1094\n",
      "99/223, train_loss: 0.3580, step time: 0.1210\n",
      "100/223, train_loss: 0.3440, step time: 0.1003\n",
      "101/223, train_loss: 0.3509, step time: 0.1004\n",
      "102/223, train_loss: 0.3610, step time: 0.1186\n",
      "103/223, train_loss: 0.3559, step time: 0.1116\n",
      "104/223, train_loss: 0.3483, step time: 0.1022\n",
      "105/223, train_loss: 0.3611, step time: 0.1041\n",
      "106/223, train_loss: 0.3607, step time: 0.1286\n",
      "107/223, train_loss: 0.3505, step time: 0.1371\n",
      "108/223, train_loss: 0.3821, step time: 0.0999\n",
      "109/223, train_loss: 0.3557, step time: 0.1195\n",
      "110/223, train_loss: 0.3689, step time: 0.1053\n",
      "111/223, train_loss: 0.3438, step time: 0.1106\n",
      "112/223, train_loss: 0.3631, step time: 0.1006\n",
      "113/223, train_loss: 0.3506, step time: 0.1055\n",
      "114/223, train_loss: 0.3591, step time: 0.1011\n",
      "115/223, train_loss: 0.3712, step time: 0.1001\n",
      "116/223, train_loss: 0.3519, step time: 0.1022\n",
      "117/223, train_loss: 0.3646, step time: 0.1233\n",
      "118/223, train_loss: 0.3545, step time: 0.1002\n",
      "119/223, train_loss: 0.3478, step time: 0.1008\n",
      "120/223, train_loss: 0.3500, step time: 0.1020\n",
      "121/223, train_loss: 0.3512, step time: 0.1003\n",
      "122/223, train_loss: 0.3679, step time: 0.1012\n",
      "123/223, train_loss: 0.3338, step time: 0.1141\n",
      "124/223, train_loss: 0.3416, step time: 0.1003\n",
      "125/223, train_loss: 0.3652, step time: 0.1004\n",
      "126/223, train_loss: 0.3475, step time: 0.1119\n",
      "127/223, train_loss: 0.3572, step time: 0.1002\n",
      "128/223, train_loss: 0.3582, step time: 0.1002\n",
      "129/223, train_loss: 0.3484, step time: 0.1077\n",
      "130/223, train_loss: 0.3478, step time: 0.1003\n",
      "131/223, train_loss: 0.3540, step time: 0.1041\n",
      "132/223, train_loss: 0.3452, step time: 0.1010\n",
      "133/223, train_loss: 0.3719, step time: 0.1075\n",
      "134/223, train_loss: 0.3535, step time: 0.1003\n",
      "135/223, train_loss: 0.3532, step time: 0.1004\n",
      "136/223, train_loss: 0.3579, step time: 0.1075\n",
      "137/223, train_loss: 0.3587, step time: 0.1109\n",
      "138/223, train_loss: 0.3467, step time: 0.1056\n",
      "139/223, train_loss: 0.3396, step time: 0.1000\n",
      "140/223, train_loss: 0.3403, step time: 0.1004\n",
      "141/223, train_loss: 0.3650, step time: 0.1120\n",
      "142/223, train_loss: 0.3464, step time: 0.0994\n",
      "143/223, train_loss: 0.3611, step time: 0.1014\n",
      "144/223, train_loss: 0.3447, step time: 0.1013\n",
      "145/223, train_loss: 0.3445, step time: 0.1040\n",
      "146/223, train_loss: 0.3649, step time: 0.0989\n",
      "147/223, train_loss: 0.3359, step time: 0.0990\n",
      "148/223, train_loss: 0.3505, step time: 0.1025\n",
      "149/223, train_loss: 0.3500, step time: 0.1205\n",
      "150/223, train_loss: 0.3654, step time: 0.1187\n",
      "151/223, train_loss: 0.3462, step time: 0.1055\n",
      "152/223, train_loss: 0.3478, step time: 0.1068\n",
      "153/223, train_loss: 0.3370, step time: 0.1025\n",
      "154/223, train_loss: 0.3572, step time: 0.1054\n",
      "155/223, train_loss: 0.3407, step time: 0.1000\n",
      "156/223, train_loss: 0.3529, step time: 0.1001\n",
      "157/223, train_loss: 0.3582, step time: 0.0994\n",
      "158/223, train_loss: 0.3475, step time: 0.1073\n",
      "159/223, train_loss: 0.3388, step time: 0.1120\n",
      "160/223, train_loss: 0.3342, step time: 0.1142\n",
      "161/223, train_loss: 0.3502, step time: 0.1147\n",
      "162/223, train_loss: 0.3414, step time: 0.1093\n",
      "163/223, train_loss: 0.3373, step time: 0.1167\n",
      "164/223, train_loss: 0.3544, step time: 0.1056\n",
      "165/223, train_loss: 0.3429, step time: 0.0999\n",
      "166/223, train_loss: 0.3472, step time: 0.1169\n",
      "167/223, train_loss: 0.3404, step time: 0.1166\n",
      "168/223, train_loss: 0.3354, step time: 0.1126\n",
      "169/223, train_loss: 0.3552, step time: 0.1006\n",
      "170/223, train_loss: 0.3504, step time: 0.1115\n",
      "171/223, train_loss: 0.3418, step time: 0.1468\n",
      "172/223, train_loss: 0.3356, step time: 0.1127\n",
      "173/223, train_loss: 0.3409, step time: 0.1125\n",
      "174/223, train_loss: 0.3335, step time: 0.1001\n",
      "175/223, train_loss: 0.3388, step time: 0.1306\n",
      "176/223, train_loss: 0.3541, step time: 0.1052\n",
      "177/223, train_loss: 0.3405, step time: 0.0999\n",
      "178/223, train_loss: 0.3373, step time: 0.1059\n",
      "179/223, train_loss: 0.3384, step time: 0.1092\n",
      "180/223, train_loss: 0.3479, step time: 0.1280\n",
      "181/223, train_loss: 0.3309, step time: 0.1072\n",
      "182/223, train_loss: 0.3525, step time: 0.1043\n",
      "183/223, train_loss: 0.3437, step time: 0.1022\n",
      "184/223, train_loss: 0.3525, step time: 0.1002\n",
      "185/223, train_loss: 0.3469, step time: 0.1140\n",
      "186/223, train_loss: 0.3317, step time: 0.1139\n",
      "187/223, train_loss: 0.3385, step time: 0.1067\n",
      "188/223, train_loss: 0.3606, step time: 0.1138\n",
      "189/223, train_loss: 0.3369, step time: 0.1120\n",
      "190/223, train_loss: 0.3522, step time: 0.1189\n",
      "191/223, train_loss: 0.3538, step time: 0.1028\n",
      "192/223, train_loss: 0.3408, step time: 0.1137\n",
      "193/223, train_loss: 0.3520, step time: 0.0996\n",
      "194/223, train_loss: 0.3337, step time: 0.1187\n",
      "195/223, train_loss: 0.3519, step time: 0.1139\n",
      "196/223, train_loss: 0.3571, step time: 0.1366\n",
      "197/223, train_loss: 0.3373, step time: 0.1048\n",
      "198/223, train_loss: 0.3374, step time: 0.1107\n",
      "199/223, train_loss: 0.3651, step time: 0.1010\n",
      "200/223, train_loss: 0.3441, step time: 0.1053\n",
      "201/223, train_loss: 0.3322, step time: 0.1168\n",
      "202/223, train_loss: 0.3304, step time: 0.1075\n",
      "203/223, train_loss: 0.3316, step time: 0.1008\n",
      "204/223, train_loss: 0.3378, step time: 0.1011\n",
      "205/223, train_loss: 0.3461, step time: 0.1061\n",
      "206/223, train_loss: 0.3326, step time: 0.1011\n",
      "207/223, train_loss: 0.3553, step time: 0.1175\n",
      "208/223, train_loss: 0.3321, step time: 0.1043\n",
      "209/223, train_loss: 0.3299, step time: 0.1063\n",
      "210/223, train_loss: 0.3418, step time: 0.1152\n",
      "211/223, train_loss: 0.3373, step time: 0.1056\n",
      "212/223, train_loss: 0.3316, step time: 0.1079\n",
      "213/223, train_loss: 0.3274, step time: 0.1144\n",
      "214/223, train_loss: 0.4299, step time: 0.1083\n",
      "215/223, train_loss: 0.3503, step time: 0.1003\n",
      "216/223, train_loss: 0.3669, step time: 0.0996\n",
      "217/223, train_loss: 0.3368, step time: 0.0993\n",
      "218/223, train_loss: 0.3448, step time: 0.0992\n",
      "219/223, train_loss: 0.3385, step time: 0.0991\n",
      "220/223, train_loss: 0.3228, step time: 0.0998\n",
      "221/223, train_loss: 0.3424, step time: 0.1003\n",
      "222/223, train_loss: 0.3301, step time: 0.0997\n",
      "223/223, train_loss: 0.3402, step time: 0.0989\n",
      "epoch 7 average loss: 0.3580\n",
      "time consuming of epoch 7 is: 85.9653\n",
      "----------\n",
      "epoch 8/300\n",
      "1/223, train_loss: 0.3248, step time: 0.1009\n",
      "2/223, train_loss: 0.3467, step time: 0.1075\n",
      "3/223, train_loss: 0.3531, step time: 0.1141\n",
      "4/223, train_loss: 0.3310, step time: 0.1197\n",
      "5/223, train_loss: 0.3395, step time: 0.1124\n",
      "6/223, train_loss: 0.3383, step time: 0.1211\n",
      "7/223, train_loss: 0.3292, step time: 0.1070\n",
      "8/223, train_loss: 0.3313, step time: 0.1082\n",
      "9/223, train_loss: 0.3272, step time: 0.1076\n",
      "10/223, train_loss: 0.3308, step time: 0.1052\n",
      "11/223, train_loss: 0.3312, step time: 0.1127\n",
      "12/223, train_loss: 0.3236, step time: 0.1186\n",
      "13/223, train_loss: 0.3316, step time: 0.0998\n",
      "14/223, train_loss: 0.3265, step time: 0.1111\n",
      "15/223, train_loss: 0.3410, step time: 0.1009\n",
      "16/223, train_loss: 0.3449, step time: 0.1323\n",
      "17/223, train_loss: 0.3395, step time: 0.1092\n",
      "18/223, train_loss: 0.3357, step time: 0.1154\n",
      "19/223, train_loss: 0.3511, step time: 0.1106\n",
      "20/223, train_loss: 0.3244, step time: 0.1213\n",
      "21/223, train_loss: 0.3203, step time: 0.1398\n",
      "22/223, train_loss: 0.3476, step time: 0.1110\n",
      "23/223, train_loss: 0.3394, step time: 0.1113\n",
      "24/223, train_loss: 0.3209, step time: 0.1085\n",
      "25/223, train_loss: 0.3447, step time: 0.1111\n",
      "26/223, train_loss: 0.3583, step time: 0.1087\n",
      "27/223, train_loss: 0.3232, step time: 0.1143\n",
      "28/223, train_loss: 0.3285, step time: 0.1055\n",
      "29/223, train_loss: 0.3248, step time: 0.1294\n",
      "30/223, train_loss: 0.3409, step time: 0.1046\n",
      "31/223, train_loss: 0.3273, step time: 0.1016\n",
      "32/223, train_loss: 0.3308, step time: 0.1189\n",
      "33/223, train_loss: 0.3405, step time: 0.1056\n",
      "34/223, train_loss: 0.3415, step time: 0.1168\n",
      "35/223, train_loss: 0.3147, step time: 0.1048\n",
      "36/223, train_loss: 0.3303, step time: 0.1087\n",
      "37/223, train_loss: 0.3262, step time: 0.1008\n",
      "38/223, train_loss: 0.3326, step time: 0.1009\n",
      "39/223, train_loss: 0.3288, step time: 0.1001\n",
      "40/223, train_loss: 0.3235, step time: 0.1011\n",
      "41/223, train_loss: 0.3282, step time: 0.1288\n",
      "42/223, train_loss: 0.3218, step time: 0.1147\n",
      "43/223, train_loss: 0.3183, step time: 0.1127\n",
      "44/223, train_loss: 0.3362, step time: 0.1084\n",
      "45/223, train_loss: 0.3223, step time: 0.1263\n",
      "46/223, train_loss: 0.3165, step time: 0.1103\n",
      "47/223, train_loss: 0.3468, step time: 0.0994\n",
      "48/223, train_loss: 0.3166, step time: 0.0993\n",
      "49/223, train_loss: 0.3441, step time: 0.1058\n",
      "50/223, train_loss: 0.3201, step time: 0.1130\n",
      "51/223, train_loss: 0.3169, step time: 0.1061\n",
      "52/223, train_loss: 0.3293, step time: 0.0990\n",
      "53/223, train_loss: 0.3175, step time: 0.0994\n",
      "54/223, train_loss: 0.3487, step time: 0.1144\n",
      "55/223, train_loss: 0.3290, step time: 0.1005\n",
      "56/223, train_loss: 0.3290, step time: 0.1106\n",
      "57/223, train_loss: 0.3197, step time: 0.0999\n",
      "58/223, train_loss: 0.3298, step time: 0.1024\n",
      "59/223, train_loss: 0.3277, step time: 0.1095\n",
      "60/223, train_loss: 0.3191, step time: 0.0996\n",
      "61/223, train_loss: 0.3256, step time: 0.0985\n",
      "62/223, train_loss: 0.3304, step time: 0.1035\n",
      "63/223, train_loss: 0.3189, step time: 0.1199\n",
      "64/223, train_loss: 0.3164, step time: 0.1105\n",
      "65/223, train_loss: 0.3267, step time: 0.0991\n",
      "66/223, train_loss: 0.3314, step time: 0.1098\n",
      "67/223, train_loss: 0.3159, step time: 0.1089\n",
      "68/223, train_loss: 0.3354, step time: 0.1192\n",
      "69/223, train_loss: 0.3360, step time: 0.1058\n",
      "70/223, train_loss: 0.3339, step time: 0.1056\n",
      "71/223, train_loss: 0.3202, step time: 0.1000\n",
      "72/223, train_loss: 0.3257, step time: 0.1035\n",
      "73/223, train_loss: 0.3148, step time: 0.1002\n",
      "74/223, train_loss: 0.3182, step time: 0.1148\n",
      "75/223, train_loss: 0.3378, step time: 0.1188\n",
      "76/223, train_loss: 0.3295, step time: 0.1070\n",
      "77/223, train_loss: 0.3294, step time: 0.1001\n",
      "78/223, train_loss: 0.3338, step time: 0.1000\n",
      "79/223, train_loss: 0.3174, step time: 0.1005\n",
      "80/223, train_loss: 0.3298, step time: 0.1003\n",
      "81/223, train_loss: 0.3168, step time: 0.1007\n",
      "82/223, train_loss: 0.3295, step time: 0.1000\n",
      "83/223, train_loss: 0.3211, step time: 0.0996\n",
      "84/223, train_loss: 0.3273, step time: 0.0997\n",
      "85/223, train_loss: 0.3236, step time: 0.1158\n",
      "86/223, train_loss: 0.3288, step time: 0.1003\n",
      "87/223, train_loss: 0.3161, step time: 0.1065\n",
      "88/223, train_loss: 0.3098, step time: 0.1037\n",
      "89/223, train_loss: 0.3199, step time: 0.1057\n",
      "90/223, train_loss: 0.3085, step time: 0.1039\n",
      "91/223, train_loss: 0.3221, step time: 0.1004\n",
      "92/223, train_loss: 0.3268, step time: 0.1000\n",
      "93/223, train_loss: 0.3164, step time: 0.1150\n",
      "94/223, train_loss: 0.3199, step time: 0.1017\n",
      "95/223, train_loss: 0.3210, step time: 0.1189\n",
      "96/223, train_loss: 0.3166, step time: 0.1075\n",
      "97/223, train_loss: 0.3128, step time: 0.1079\n",
      "98/223, train_loss: 0.3068, step time: 0.1135\n",
      "99/223, train_loss: 0.3156, step time: 0.1038\n",
      "100/223, train_loss: 0.3235, step time: 0.1010\n",
      "101/223, train_loss: 0.3185, step time: 0.1029\n",
      "102/223, train_loss: 0.4325, step time: 0.1218\n",
      "103/223, train_loss: 0.3421, step time: 0.1064\n",
      "104/223, train_loss: 0.3085, step time: 0.1004\n",
      "105/223, train_loss: 0.3185, step time: 0.0994\n",
      "106/223, train_loss: 0.3348, step time: 0.1029\n",
      "107/223, train_loss: 0.3056, step time: 0.1022\n",
      "108/223, train_loss: 0.3308, step time: 0.1004\n",
      "109/223, train_loss: 0.3306, step time: 0.0998\n",
      "110/223, train_loss: 0.3123, step time: 0.0996\n",
      "111/223, train_loss: 0.3155, step time: 0.1000\n",
      "112/223, train_loss: 0.3162, step time: 0.0992\n",
      "113/223, train_loss: 0.3184, step time: 0.1063\n",
      "114/223, train_loss: 0.3380, step time: 0.1164\n",
      "115/223, train_loss: 0.3265, step time: 0.1007\n",
      "116/223, train_loss: 0.3266, step time: 0.0996\n",
      "117/223, train_loss: 0.3086, step time: 0.1000\n",
      "118/223, train_loss: 0.3116, step time: 0.1196\n",
      "119/223, train_loss: 0.3164, step time: 0.1000\n",
      "120/223, train_loss: 0.3120, step time: 0.1163\n",
      "121/223, train_loss: 0.3103, step time: 0.1098\n",
      "122/223, train_loss: 0.3254, step time: 0.1143\n",
      "123/223, train_loss: 0.3207, step time: 0.1186\n",
      "124/223, train_loss: 0.3214, step time: 0.1002\n",
      "125/223, train_loss: 0.3122, step time: 0.0997\n",
      "126/223, train_loss: 0.3300, step time: 0.1007\n",
      "127/223, train_loss: 0.3264, step time: 0.1012\n",
      "128/223, train_loss: 0.3184, step time: 0.0998\n",
      "129/223, train_loss: 0.3103, step time: 0.0999\n",
      "130/223, train_loss: 0.3184, step time: 0.1207\n",
      "131/223, train_loss: 0.3069, step time: 0.1016\n",
      "132/223, train_loss: 0.3050, step time: 0.1095\n",
      "133/223, train_loss: 0.3130, step time: 0.1057\n",
      "134/223, train_loss: 0.3147, step time: 0.1138\n",
      "135/223, train_loss: 0.3164, step time: 0.1002\n",
      "136/223, train_loss: 0.3178, step time: 0.1001\n",
      "137/223, train_loss: 0.3007, step time: 0.0999\n",
      "138/223, train_loss: 0.3296, step time: 0.1021\n",
      "139/223, train_loss: 0.3245, step time: 0.1208\n",
      "140/223, train_loss: 0.3291, step time: 0.0998\n",
      "141/223, train_loss: 0.3017, step time: 0.1012\n",
      "142/223, train_loss: 0.3135, step time: 0.1032\n",
      "143/223, train_loss: 0.3301, step time: 0.1004\n",
      "144/223, train_loss: 0.3066, step time: 0.1049\n",
      "145/223, train_loss: 0.3016, step time: 0.1024\n",
      "146/223, train_loss: 0.3186, step time: 0.0990\n",
      "147/223, train_loss: 0.3135, step time: 0.0984\n",
      "148/223, train_loss: 0.3226, step time: 0.0992\n",
      "149/223, train_loss: 0.3185, step time: 0.0983\n",
      "150/223, train_loss: 0.3233, step time: 0.1201\n",
      "151/223, train_loss: 0.3282, step time: 0.1059\n",
      "152/223, train_loss: 0.3068, step time: 0.1133\n",
      "153/223, train_loss: 0.3166, step time: 0.1178\n",
      "154/223, train_loss: 0.3060, step time: 0.1085\n",
      "155/223, train_loss: 0.3097, step time: 0.1012\n",
      "156/223, train_loss: 0.3080, step time: 0.1115\n",
      "157/223, train_loss: 0.3065, step time: 0.1092\n",
      "158/223, train_loss: 0.2997, step time: 0.1153\n",
      "159/223, train_loss: 0.3127, step time: 0.1016\n",
      "160/223, train_loss: 0.3061, step time: 0.1016\n",
      "161/223, train_loss: 0.3061, step time: 0.1079\n",
      "162/223, train_loss: 0.3082, step time: 0.1168\n",
      "163/223, train_loss: 0.3212, step time: 0.1165\n",
      "164/223, train_loss: 0.3083, step time: 0.1098\n",
      "165/223, train_loss: 0.3161, step time: 0.1061\n",
      "166/223, train_loss: 0.3043, step time: 0.1073\n",
      "167/223, train_loss: 0.3096, step time: 0.1042\n",
      "168/223, train_loss: 0.3157, step time: 0.1044\n",
      "169/223, train_loss: 0.3019, step time: 0.1026\n",
      "170/223, train_loss: 0.3084, step time: 0.1185\n",
      "171/223, train_loss: 0.3083, step time: 0.1187\n",
      "172/223, train_loss: 0.3008, step time: 0.1262\n",
      "173/223, train_loss: 0.3016, step time: 0.0999\n",
      "174/223, train_loss: 0.3143, step time: 0.0997\n",
      "175/223, train_loss: 0.3090, step time: 0.1019\n",
      "176/223, train_loss: 0.3069, step time: 0.1027\n",
      "177/223, train_loss: 0.3078, step time: 0.1044\n",
      "178/223, train_loss: 0.3191, step time: 0.1225\n",
      "179/223, train_loss: 0.3038, step time: 0.1111\n",
      "180/223, train_loss: 0.3161, step time: 0.1004\n",
      "181/223, train_loss: 0.3134, step time: 0.1004\n",
      "182/223, train_loss: 0.2853, step time: 0.1079\n",
      "183/223, train_loss: 0.3021, step time: 0.1138\n",
      "184/223, train_loss: 0.3018, step time: 0.1122\n",
      "185/223, train_loss: 0.2973, step time: 0.1080\n",
      "186/223, train_loss: 0.3010, step time: 0.0996\n",
      "187/223, train_loss: 0.2996, step time: 0.1060\n",
      "188/223, train_loss: 0.3138, step time: 0.1242\n",
      "189/223, train_loss: 0.3044, step time: 0.1017\n",
      "190/223, train_loss: 0.3168, step time: 0.1020\n",
      "191/223, train_loss: 0.2972, step time: 0.1004\n",
      "192/223, train_loss: 0.3161, step time: 0.1003\n",
      "193/223, train_loss: 0.3063, step time: 0.1081\n",
      "194/223, train_loss: 0.3160, step time: 0.1112\n",
      "195/223, train_loss: 0.3083, step time: 0.1203\n",
      "196/223, train_loss: 0.3139, step time: 0.1035\n",
      "197/223, train_loss: 0.2970, step time: 0.1168\n",
      "198/223, train_loss: 0.3054, step time: 0.1049\n",
      "199/223, train_loss: 0.3175, step time: 0.1030\n",
      "200/223, train_loss: 0.2916, step time: 0.1000\n",
      "201/223, train_loss: 0.3013, step time: 0.1047\n",
      "202/223, train_loss: 0.3136, step time: 0.0999\n",
      "203/223, train_loss: 0.3041, step time: 0.1318\n",
      "204/223, train_loss: 0.3196, step time: 0.1328\n",
      "205/223, train_loss: 0.3131, step time: 0.1209\n",
      "206/223, train_loss: 0.2970, step time: 0.1002\n",
      "207/223, train_loss: 0.3053, step time: 0.1001\n",
      "208/223, train_loss: 0.3049, step time: 0.0996\n",
      "209/223, train_loss: 0.3083, step time: 0.1027\n",
      "210/223, train_loss: 0.3031, step time: 0.1005\n",
      "211/223, train_loss: 0.3004, step time: 0.1025\n",
      "212/223, train_loss: 0.3246, step time: 0.1076\n",
      "213/223, train_loss: 0.3045, step time: 0.1010\n",
      "214/223, train_loss: 0.2968, step time: 0.1060\n",
      "215/223, train_loss: 0.3066, step time: 0.0993\n",
      "216/223, train_loss: 0.3055, step time: 0.1016\n",
      "217/223, train_loss: 0.2960, step time: 0.1248\n",
      "218/223, train_loss: 0.2957, step time: 0.1000\n",
      "219/223, train_loss: 0.3036, step time: 0.0997\n",
      "220/223, train_loss: 0.2954, step time: 0.0994\n",
      "221/223, train_loss: 0.3082, step time: 0.1013\n",
      "222/223, train_loss: 0.3050, step time: 0.0992\n",
      "223/223, train_loss: 0.3030, step time: 0.0993\n",
      "epoch 8 average loss: 0.3194\n",
      "time consuming of epoch 8 is: 87.0802\n",
      "----------\n",
      "epoch 9/300\n",
      "1/223, train_loss: 0.3045, step time: 0.1013\n",
      "2/223, train_loss: 0.3024, step time: 0.1186\n",
      "3/223, train_loss: 0.2983, step time: 0.1178\n",
      "4/223, train_loss: 0.2960, step time: 0.1191\n",
      "5/223, train_loss: 0.3066, step time: 0.1101\n",
      "6/223, train_loss: 0.2899, step time: 0.1354\n",
      "7/223, train_loss: 0.2998, step time: 0.1005\n",
      "8/223, train_loss: 0.3030, step time: 0.1003\n",
      "9/223, train_loss: 0.2957, step time: 0.1000\n",
      "10/223, train_loss: 0.2897, step time: 0.1116\n",
      "11/223, train_loss: 0.2913, step time: 0.1148\n",
      "12/223, train_loss: 0.3149, step time: 0.1000\n",
      "13/223, train_loss: 0.2849, step time: 0.1092\n",
      "14/223, train_loss: 0.2932, step time: 0.0999\n",
      "15/223, train_loss: 0.3084, step time: 0.1086\n",
      "16/223, train_loss: 0.2934, step time: 0.1272\n",
      "17/223, train_loss: 0.3131, step time: 0.1000\n",
      "18/223, train_loss: 0.2917, step time: 0.0999\n",
      "19/223, train_loss: 0.3019, step time: 0.1008\n",
      "20/223, train_loss: 0.2924, step time: 0.0997\n",
      "21/223, train_loss: 0.2985, step time: 0.1003\n",
      "22/223, train_loss: 0.2968, step time: 0.0994\n",
      "23/223, train_loss: 0.3017, step time: 0.1006\n",
      "24/223, train_loss: 0.2873, step time: 0.1002\n",
      "25/223, train_loss: 0.2956, step time: 0.1013\n",
      "26/223, train_loss: 0.2989, step time: 0.0998\n",
      "27/223, train_loss: 0.2983, step time: 0.0999\n",
      "28/223, train_loss: 0.2962, step time: 0.0999\n",
      "29/223, train_loss: 0.2939, step time: 0.1108\n",
      "30/223, train_loss: 0.2954, step time: 0.1070\n",
      "31/223, train_loss: 0.2940, step time: 0.1056\n",
      "32/223, train_loss: 0.2903, step time: 0.1002\n",
      "33/223, train_loss: 0.2771, step time: 0.1039\n",
      "34/223, train_loss: 0.3013, step time: 0.1000\n",
      "35/223, train_loss: 0.2986, step time: 0.1097\n",
      "36/223, train_loss: 0.3034, step time: 0.1193\n",
      "37/223, train_loss: 0.3061, step time: 0.1176\n",
      "38/223, train_loss: 0.3226, step time: 0.1115\n",
      "39/223, train_loss: 0.2959, step time: 0.1192\n",
      "40/223, train_loss: 0.3012, step time: 0.1254\n",
      "41/223, train_loss: 0.2929, step time: 0.1108\n",
      "42/223, train_loss: 0.3097, step time: 0.1160\n",
      "43/223, train_loss: 0.2901, step time: 0.1196\n",
      "44/223, train_loss: 0.2830, step time: 0.1125\n",
      "45/223, train_loss: 0.2831, step time: 0.1004\n",
      "46/223, train_loss: 0.3003, step time: 0.1042\n",
      "47/223, train_loss: 0.2956, step time: 0.1007\n",
      "48/223, train_loss: 0.2983, step time: 0.0999\n",
      "49/223, train_loss: 0.3103, step time: 0.1186\n",
      "50/223, train_loss: 0.2886, step time: 0.1063\n",
      "51/223, train_loss: 0.3072, step time: 0.1224\n",
      "52/223, train_loss: 0.2963, step time: 0.1054\n",
      "53/223, train_loss: 0.2967, step time: 0.1034\n",
      "54/223, train_loss: 0.2841, step time: 0.1005\n",
      "55/223, train_loss: 0.2963, step time: 0.1003\n",
      "56/223, train_loss: 0.2972, step time: 0.1022\n",
      "57/223, train_loss: 0.2890, step time: 0.1068\n",
      "58/223, train_loss: 0.2935, step time: 0.1168\n",
      "59/223, train_loss: 0.2989, step time: 0.1148\n",
      "60/223, train_loss: 0.2893, step time: 0.1192\n",
      "61/223, train_loss: 0.2814, step time: 0.0998\n",
      "62/223, train_loss: 0.2879, step time: 0.1114\n",
      "63/223, train_loss: 0.3057, step time: 0.1009\n",
      "64/223, train_loss: 0.2919, step time: 0.1009\n",
      "65/223, train_loss: 0.2863, step time: 0.1009\n",
      "66/223, train_loss: 0.2898, step time: 0.1186\n",
      "67/223, train_loss: 0.2883, step time: 0.1044\n",
      "68/223, train_loss: 0.2916, step time: 0.0990\n",
      "69/223, train_loss: 0.2918, step time: 0.1148\n",
      "70/223, train_loss: 0.2931, step time: 0.1033\n",
      "71/223, train_loss: 0.2833, step time: 0.1008\n",
      "72/223, train_loss: 0.2960, step time: 0.1144\n",
      "73/223, train_loss: 0.3040, step time: 0.1131\n",
      "74/223, train_loss: 0.2746, step time: 0.1004\n",
      "75/223, train_loss: 0.3008, step time: 0.1148\n",
      "76/223, train_loss: 0.2949, step time: 0.1097\n",
      "77/223, train_loss: 0.2867, step time: 0.1055\n",
      "78/223, train_loss: 0.2855, step time: 0.1086\n",
      "79/223, train_loss: 0.3066, step time: 0.1098\n",
      "80/223, train_loss: 0.2999, step time: 0.1037\n",
      "81/223, train_loss: 0.2977, step time: 0.1146\n",
      "82/223, train_loss: 0.2842, step time: 0.0993\n",
      "83/223, train_loss: 0.2877, step time: 0.1090\n",
      "84/223, train_loss: 0.2905, step time: 0.1001\n",
      "85/223, train_loss: 0.2955, step time: 0.1008\n",
      "86/223, train_loss: 0.2863, step time: 0.1181\n",
      "87/223, train_loss: 0.2859, step time: 0.1125\n",
      "88/223, train_loss: 0.2865, step time: 0.0990\n",
      "89/223, train_loss: 0.2912, step time: 0.0998\n",
      "90/223, train_loss: 0.2994, step time: 0.1088\n",
      "91/223, train_loss: 0.2887, step time: 0.1149\n",
      "92/223, train_loss: 0.2784, step time: 0.1049\n",
      "93/223, train_loss: 0.3031, step time: 0.1135\n",
      "94/223, train_loss: 0.2817, step time: 0.1149\n",
      "95/223, train_loss: 0.2871, step time: 0.1078\n",
      "96/223, train_loss: 0.2998, step time: 0.1087\n",
      "97/223, train_loss: 0.2921, step time: 0.0998\n",
      "98/223, train_loss: 0.2840, step time: 0.1003\n",
      "99/223, train_loss: 0.2814, step time: 0.0993\n",
      "100/223, train_loss: 0.2896, step time: 0.1055\n",
      "101/223, train_loss: 0.2799, step time: 0.1150\n",
      "102/223, train_loss: 0.2759, step time: 0.1147\n",
      "103/223, train_loss: 0.2829, step time: 0.1085\n",
      "104/223, train_loss: 0.2872, step time: 0.1000\n",
      "105/223, train_loss: 0.2831, step time: 0.1057\n",
      "106/223, train_loss: 0.2868, step time: 0.1166\n",
      "107/223, train_loss: 0.2750, step time: 0.1090\n",
      "108/223, train_loss: 0.3061, step time: 0.1004\n",
      "109/223, train_loss: 0.3045, step time: 0.1023\n",
      "110/223, train_loss: 0.2771, step time: 0.1051\n",
      "111/223, train_loss: 0.2822, step time: 0.0999\n",
      "112/223, train_loss: 0.2937, step time: 0.1003\n",
      "113/223, train_loss: 0.2916, step time: 0.1107\n",
      "114/223, train_loss: 0.3056, step time: 0.1161\n",
      "115/223, train_loss: 0.2891, step time: 0.1070\n",
      "116/223, train_loss: 0.2908, step time: 0.0997\n",
      "117/223, train_loss: 0.2870, step time: 0.1050\n",
      "118/223, train_loss: 0.2775, step time: 0.0995\n",
      "119/223, train_loss: 0.2848, step time: 0.1174\n",
      "120/223, train_loss: 0.2927, step time: 0.1015\n",
      "121/223, train_loss: 0.2914, step time: 0.1195\n",
      "122/223, train_loss: 0.2994, step time: 0.1072\n",
      "123/223, train_loss: 0.2920, step time: 0.0999\n",
      "124/223, train_loss: 0.2888, step time: 0.0998\n",
      "125/223, train_loss: 0.2853, step time: 0.1052\n",
      "126/223, train_loss: 0.2855, step time: 0.1109\n",
      "127/223, train_loss: 0.2864, step time: 0.1000\n",
      "128/223, train_loss: 0.3043, step time: 0.0997\n",
      "129/223, train_loss: 0.2720, step time: 0.1138\n",
      "130/223, train_loss: 0.2821, step time: 0.1270\n",
      "131/223, train_loss: 0.4120, step time: 0.1013\n",
      "132/223, train_loss: 0.2841, step time: 0.0993\n",
      "133/223, train_loss: 0.2740, step time: 0.1113\n",
      "134/223, train_loss: 0.2778, step time: 0.1092\n",
      "135/223, train_loss: 0.2919, step time: 0.1114\n",
      "136/223, train_loss: 0.2749, step time: 0.1001\n",
      "137/223, train_loss: 0.2985, step time: 0.1173\n",
      "138/223, train_loss: 0.2872, step time: 0.1125\n",
      "139/223, train_loss: 0.2901, step time: 0.1010\n",
      "140/223, train_loss: 0.2775, step time: 0.0998\n",
      "141/223, train_loss: 0.2865, step time: 0.1059\n",
      "142/223, train_loss: 0.2807, step time: 0.1054\n",
      "143/223, train_loss: 0.2834, step time: 0.0995\n",
      "144/223, train_loss: 0.2664, step time: 0.1001\n",
      "145/223, train_loss: 0.2842, step time: 0.1103\n",
      "146/223, train_loss: 0.2871, step time: 0.1188\n",
      "147/223, train_loss: 0.2752, step time: 0.0997\n",
      "148/223, train_loss: 0.2792, step time: 0.0999\n",
      "149/223, train_loss: 0.2987, step time: 0.1071\n",
      "150/223, train_loss: 0.2944, step time: 0.1176\n",
      "151/223, train_loss: 0.2840, step time: 0.1000\n",
      "152/223, train_loss: 0.2777, step time: 0.1009\n",
      "153/223, train_loss: 0.2773, step time: 0.1093\n",
      "154/223, train_loss: 0.2898, step time: 0.1042\n",
      "155/223, train_loss: 0.2740, step time: 0.1093\n",
      "156/223, train_loss: 0.2841, step time: 0.1026\n",
      "157/223, train_loss: 0.2802, step time: 0.1001\n",
      "158/223, train_loss: 0.2884, step time: 0.1146\n",
      "159/223, train_loss: 0.2909, step time: 0.1160\n",
      "160/223, train_loss: 0.2830, step time: 0.1152\n",
      "161/223, train_loss: 0.2803, step time: 0.1101\n",
      "162/223, train_loss: 0.2805, step time: 0.0998\n",
      "163/223, train_loss: 0.2714, step time: 0.1049\n",
      "164/223, train_loss: 0.2861, step time: 0.1008\n",
      "165/223, train_loss: 0.2748, step time: 0.1151\n",
      "166/223, train_loss: 0.2856, step time: 0.0988\n",
      "167/223, train_loss: 0.2920, step time: 0.1159\n",
      "168/223, train_loss: 0.2775, step time: 0.0990\n",
      "169/223, train_loss: 0.2930, step time: 0.1161\n",
      "170/223, train_loss: 0.2851, step time: 0.1004\n",
      "171/223, train_loss: 0.2866, step time: 0.1073\n",
      "172/223, train_loss: 0.2882, step time: 0.1138\n",
      "173/223, train_loss: 0.2765, step time: 0.1130\n",
      "174/223, train_loss: 0.2871, step time: 0.0985\n",
      "175/223, train_loss: 0.2834, step time: 0.1152\n",
      "176/223, train_loss: 0.2814, step time: 0.1171\n",
      "177/223, train_loss: 0.2831, step time: 0.1003\n",
      "178/223, train_loss: 0.2833, step time: 0.0994\n",
      "179/223, train_loss: 0.2805, step time: 0.1141\n",
      "180/223, train_loss: 0.2804, step time: 0.1047\n",
      "181/223, train_loss: 0.2917, step time: 0.1007\n",
      "182/223, train_loss: 0.2797, step time: 0.0999\n",
      "183/223, train_loss: 0.2759, step time: 0.1035\n",
      "184/223, train_loss: 0.2756, step time: 0.0997\n",
      "185/223, train_loss: 0.2798, step time: 0.1008\n",
      "186/223, train_loss: 0.2730, step time: 0.1012\n",
      "187/223, train_loss: 0.2868, step time: 0.0998\n",
      "188/223, train_loss: 0.2760, step time: 0.1039\n",
      "189/223, train_loss: 0.2842, step time: 0.1000\n",
      "190/223, train_loss: 0.2687, step time: 0.0996\n",
      "191/223, train_loss: 0.2786, step time: 0.0993\n",
      "192/223, train_loss: 0.2720, step time: 0.1045\n",
      "193/223, train_loss: 0.2804, step time: 0.0997\n",
      "194/223, train_loss: 0.2808, step time: 0.0994\n",
      "195/223, train_loss: 0.2772, step time: 0.1001\n",
      "196/223, train_loss: 0.2813, step time: 0.1142\n",
      "197/223, train_loss: 0.2839, step time: 0.0991\n",
      "198/223, train_loss: 0.2850, step time: 0.0991\n",
      "199/223, train_loss: 0.2735, step time: 0.0989\n",
      "200/223, train_loss: 0.2776, step time: 0.1003\n",
      "201/223, train_loss: 0.2795, step time: 0.0996\n",
      "202/223, train_loss: 0.2796, step time: 0.0992\n",
      "203/223, train_loss: 0.2707, step time: 0.0995\n",
      "204/223, train_loss: 0.2726, step time: 0.1048\n",
      "205/223, train_loss: 0.2742, step time: 0.0996\n",
      "206/223, train_loss: 0.2931, step time: 0.0994\n",
      "207/223, train_loss: 0.2821, step time: 0.0999\n",
      "208/223, train_loss: 0.2800, step time: 0.1079\n",
      "209/223, train_loss: 0.2746, step time: 0.1106\n",
      "210/223, train_loss: 0.2851, step time: 0.1076\n",
      "211/223, train_loss: 0.2768, step time: 0.1033\n",
      "212/223, train_loss: 0.2760, step time: 0.1006\n",
      "213/223, train_loss: 0.2748, step time: 0.1228\n",
      "214/223, train_loss: 0.2851, step time: 0.1050\n",
      "215/223, train_loss: 0.2793, step time: 0.1057\n",
      "216/223, train_loss: 0.2559, step time: 0.1049\n",
      "217/223, train_loss: 0.2660, step time: 0.1073\n",
      "218/223, train_loss: 0.2836, step time: 0.1286\n",
      "219/223, train_loss: 0.2780, step time: 0.1273\n",
      "220/223, train_loss: 0.2664, step time: 0.1001\n",
      "221/223, train_loss: 0.2618, step time: 0.0997\n",
      "222/223, train_loss: 0.2653, step time: 0.0993\n",
      "223/223, train_loss: 0.2669, step time: 0.0998\n",
      "epoch 9 average loss: 0.2883\n",
      "time consuming of epoch 9 is: 89.6312\n",
      "----------\n",
      "epoch 10/300\n",
      "1/223, train_loss: 0.2680, step time: 0.1002\n",
      "2/223, train_loss: 0.2781, step time: 0.1072\n",
      "3/223, train_loss: 0.2801, step time: 0.1145\n",
      "4/223, train_loss: 0.2798, step time: 0.1059\n",
      "5/223, train_loss: 0.2715, step time: 0.1106\n",
      "6/223, train_loss: 0.2646, step time: 0.0998\n",
      "7/223, train_loss: 0.2695, step time: 0.1102\n",
      "8/223, train_loss: 0.2815, step time: 0.1008\n",
      "9/223, train_loss: 0.2749, step time: 0.1020\n",
      "10/223, train_loss: 0.2843, step time: 0.0995\n",
      "11/223, train_loss: 0.2811, step time: 0.1014\n",
      "12/223, train_loss: 0.2778, step time: 0.1056\n",
      "13/223, train_loss: 0.2813, step time: 0.1005\n",
      "14/223, train_loss: 0.2702, step time: 0.1000\n",
      "15/223, train_loss: 0.2790, step time: 0.1273\n",
      "16/223, train_loss: 0.2688, step time: 0.1069\n",
      "17/223, train_loss: 0.2633, step time: 0.1102\n",
      "18/223, train_loss: 0.2630, step time: 0.1004\n",
      "19/223, train_loss: 0.2640, step time: 0.1001\n",
      "20/223, train_loss: 0.2594, step time: 0.1138\n",
      "21/223, train_loss: 0.2737, step time: 0.1064\n",
      "22/223, train_loss: 0.2880, step time: 0.1158\n",
      "23/223, train_loss: 0.2723, step time: 0.1158\n",
      "24/223, train_loss: 0.2674, step time: 0.1033\n",
      "25/223, train_loss: 0.2719, step time: 0.1143\n",
      "26/223, train_loss: 0.2735, step time: 0.0996\n",
      "27/223, train_loss: 0.2675, step time: 0.1118\n",
      "28/223, train_loss: 0.2549, step time: 0.1055\n",
      "29/223, train_loss: 0.2698, step time: 0.1128\n",
      "30/223, train_loss: 0.2626, step time: 0.1045\n",
      "31/223, train_loss: 0.2623, step time: 0.1053\n",
      "32/223, train_loss: 0.2709, step time: 0.1005\n",
      "33/223, train_loss: 0.2708, step time: 0.1020\n",
      "34/223, train_loss: 0.2886, step time: 0.1016\n",
      "35/223, train_loss: 0.2783, step time: 0.1004\n",
      "36/223, train_loss: 0.2690, step time: 0.1140\n",
      "37/223, train_loss: 0.2718, step time: 0.1077\n",
      "38/223, train_loss: 0.2616, step time: 0.1005\n",
      "39/223, train_loss: 0.2571, step time: 0.1011\n",
      "40/223, train_loss: 0.2669, step time: 0.0998\n",
      "41/223, train_loss: 0.2851, step time: 0.1134\n",
      "42/223, train_loss: 0.2691, step time: 0.1024\n",
      "43/223, train_loss: 0.2678, step time: 0.1005\n",
      "44/223, train_loss: 0.2580, step time: 0.1004\n",
      "45/223, train_loss: 0.2830, step time: 0.1131\n",
      "46/223, train_loss: 0.2750, step time: 0.1198\n",
      "47/223, train_loss: 0.2612, step time: 0.0999\n",
      "48/223, train_loss: 0.2715, step time: 0.1011\n",
      "49/223, train_loss: 0.2731, step time: 0.0998\n",
      "50/223, train_loss: 0.2712, step time: 0.0996\n",
      "51/223, train_loss: 0.2691, step time: 0.1005\n",
      "52/223, train_loss: 0.2838, step time: 0.1008\n",
      "53/223, train_loss: 0.2630, step time: 0.1130\n",
      "54/223, train_loss: 0.2633, step time: 0.1159\n",
      "55/223, train_loss: 0.2641, step time: 0.0999\n",
      "56/223, train_loss: 0.2642, step time: 0.0999\n",
      "57/223, train_loss: 0.2602, step time: 0.1128\n",
      "58/223, train_loss: 0.2775, step time: 0.1171\n",
      "59/223, train_loss: 0.2696, step time: 0.1199\n",
      "60/223, train_loss: 0.2584, step time: 0.1015\n",
      "61/223, train_loss: 0.2634, step time: 0.1268\n",
      "62/223, train_loss: 0.2712, step time: 0.1001\n",
      "63/223, train_loss: 0.2620, step time: 0.0996\n",
      "64/223, train_loss: 0.2761, step time: 0.1103\n",
      "65/223, train_loss: 0.2705, step time: 0.1007\n",
      "66/223, train_loss: 0.2646, step time: 0.1000\n",
      "67/223, train_loss: 0.2564, step time: 0.1003\n",
      "68/223, train_loss: 0.2597, step time: 0.1002\n",
      "69/223, train_loss: 0.2786, step time: 0.1275\n",
      "70/223, train_loss: 0.2537, step time: 0.1072\n",
      "71/223, train_loss: 0.2568, step time: 0.1000\n",
      "72/223, train_loss: 0.2630, step time: 0.1004\n",
      "73/223, train_loss: 0.2623, step time: 0.1055\n",
      "74/223, train_loss: 0.2727, step time: 0.1489\n",
      "75/223, train_loss: 0.2687, step time: 0.0998\n",
      "76/223, train_loss: 0.2524, step time: 0.0998\n",
      "77/223, train_loss: 0.2593, step time: 0.1004\n",
      "78/223, train_loss: 0.2665, step time: 0.1013\n",
      "79/223, train_loss: 0.2454, step time: 0.1011\n",
      "80/223, train_loss: 0.2836, step time: 0.1039\n",
      "81/223, train_loss: 0.2487, step time: 0.1393\n",
      "82/223, train_loss: 0.2664, step time: 0.1131\n",
      "83/223, train_loss: 0.2549, step time: 0.1030\n",
      "84/223, train_loss: 0.2687, step time: 0.1000\n",
      "85/223, train_loss: 0.2853, step time: 0.1112\n",
      "86/223, train_loss: 0.2702, step time: 0.1013\n",
      "87/223, train_loss: 0.2627, step time: 0.0999\n",
      "88/223, train_loss: 0.2545, step time: 0.1015\n",
      "89/223, train_loss: 0.2644, step time: 0.1062\n",
      "90/223, train_loss: 0.2709, step time: 0.1150\n",
      "91/223, train_loss: 0.2753, step time: 0.1106\n",
      "92/223, train_loss: 0.2662, step time: 0.1068\n",
      "93/223, train_loss: 0.2683, step time: 0.1102\n",
      "94/223, train_loss: 0.2671, step time: 0.1037\n",
      "95/223, train_loss: 0.2590, step time: 0.1231\n",
      "96/223, train_loss: 0.2449, step time: 0.1187\n",
      "97/223, train_loss: 0.2622, step time: 0.1082\n",
      "98/223, train_loss: 0.2634, step time: 0.1146\n",
      "99/223, train_loss: 0.2576, step time: 0.1002\n",
      "100/223, train_loss: 0.2709, step time: 0.1001\n",
      "101/223, train_loss: 0.2548, step time: 0.1086\n",
      "102/223, train_loss: 0.2685, step time: 0.1093\n",
      "103/223, train_loss: 0.2596, step time: 0.1012\n",
      "104/223, train_loss: 0.2504, step time: 0.1101\n",
      "105/223, train_loss: 0.2539, step time: 0.1045\n",
      "106/223, train_loss: 0.2650, step time: 0.1118\n",
      "107/223, train_loss: 0.2641, step time: 0.1085\n",
      "108/223, train_loss: 0.2585, step time: 0.1167\n",
      "109/223, train_loss: 0.2590, step time: 0.1002\n",
      "110/223, train_loss: 0.2574, step time: 0.1223\n",
      "111/223, train_loss: 0.2736, step time: 0.1116\n",
      "112/223, train_loss: 0.2441, step time: 0.1017\n",
      "113/223, train_loss: 0.2499, step time: 0.1007\n",
      "114/223, train_loss: 0.2718, step time: 0.0998\n",
      "115/223, train_loss: 0.2508, step time: 0.1006\n",
      "116/223, train_loss: 0.2447, step time: 0.1039\n",
      "117/223, train_loss: 0.2585, step time: 0.1002\n",
      "118/223, train_loss: 0.2506, step time: 0.0998\n",
      "119/223, train_loss: 0.2473, step time: 0.1005\n",
      "120/223, train_loss: 0.2777, step time: 0.1006\n",
      "121/223, train_loss: 0.2743, step time: 0.1077\n",
      "122/223, train_loss: 0.2816, step time: 0.1257\n",
      "123/223, train_loss: 0.2518, step time: 0.1253\n",
      "124/223, train_loss: 0.2505, step time: 0.1138\n",
      "125/223, train_loss: 0.2462, step time: 0.1018\n",
      "126/223, train_loss: 0.2526, step time: 0.1004\n",
      "127/223, train_loss: 0.2598, step time: 0.1008\n",
      "128/223, train_loss: 0.2726, step time: 0.1049\n",
      "129/223, train_loss: 0.2583, step time: 0.1161\n",
      "130/223, train_loss: 0.2593, step time: 0.1016\n",
      "131/223, train_loss: 0.2450, step time: 0.1284\n",
      "132/223, train_loss: 0.2686, step time: 0.1105\n",
      "133/223, train_loss: 0.2473, step time: 0.1029\n",
      "134/223, train_loss: 0.2430, step time: 0.1023\n",
      "135/223, train_loss: 0.2587, step time: 0.1117\n",
      "136/223, train_loss: 0.2562, step time: 0.1000\n",
      "137/223, train_loss: 0.2556, step time: 0.1087\n",
      "138/223, train_loss: 0.2552, step time: 0.0997\n",
      "139/223, train_loss: 0.2560, step time: 0.1020\n",
      "140/223, train_loss: 0.2560, step time: 0.1044\n",
      "141/223, train_loss: 0.2622, step time: 0.1057\n",
      "142/223, train_loss: 0.2684, step time: 0.1054\n",
      "143/223, train_loss: 0.2580, step time: 0.1102\n",
      "144/223, train_loss: 0.2618, step time: 0.1004\n",
      "145/223, train_loss: 0.2443, step time: 0.1098\n",
      "146/223, train_loss: 0.2544, step time: 0.1002\n",
      "147/223, train_loss: 0.2399, step time: 0.1015\n",
      "148/223, train_loss: 0.2518, step time: 0.1021\n",
      "149/223, train_loss: 0.2517, step time: 0.0999\n",
      "150/223, train_loss: 0.2651, step time: 0.1007\n",
      "151/223, train_loss: 0.2519, step time: 0.1058\n",
      "152/223, train_loss: 0.2436, step time: 0.1088\n",
      "153/223, train_loss: 0.2582, step time: 0.1014\n",
      "154/223, train_loss: 0.2634, step time: 0.1083\n",
      "155/223, train_loss: 0.2470, step time: 0.1008\n",
      "156/223, train_loss: 0.2780, step time: 0.1012\n",
      "157/223, train_loss: 0.2534, step time: 0.1043\n",
      "158/223, train_loss: 0.3854, step time: 0.1172\n",
      "159/223, train_loss: 0.2483, step time: 0.1133\n",
      "160/223, train_loss: 0.2496, step time: 0.1003\n",
      "161/223, train_loss: 0.2597, step time: 0.1010\n",
      "162/223, train_loss: 0.2553, step time: 0.0998\n",
      "163/223, train_loss: 0.2372, step time: 0.1060\n",
      "164/223, train_loss: 0.2467, step time: 0.1004\n",
      "165/223, train_loss: 0.2562, step time: 0.0997\n",
      "166/223, train_loss: 0.2522, step time: 0.0988\n",
      "167/223, train_loss: 0.2530, step time: 0.0995\n",
      "168/223, train_loss: 0.2719, step time: 0.1007\n",
      "169/223, train_loss: 0.2639, step time: 0.0994\n",
      "170/223, train_loss: 0.2771, step time: 0.0998\n",
      "171/223, train_loss: 0.2494, step time: 0.0996\n",
      "172/223, train_loss: 0.2452, step time: 0.1047\n",
      "173/223, train_loss: 0.2650, step time: 0.1065\n",
      "174/223, train_loss: 0.2611, step time: 0.1126\n",
      "175/223, train_loss: 0.2467, step time: 0.0998\n",
      "176/223, train_loss: 0.2679, step time: 0.1090\n",
      "177/223, train_loss: 0.2488, step time: 0.1155\n",
      "178/223, train_loss: 0.2335, step time: 0.1077\n",
      "179/223, train_loss: 0.2656, step time: 0.1242\n",
      "180/223, train_loss: 0.2689, step time: 0.1127\n",
      "181/223, train_loss: 0.2532, step time: 0.1021\n",
      "182/223, train_loss: 0.2407, step time: 0.1061\n",
      "183/223, train_loss: 0.2424, step time: 0.1207\n",
      "184/223, train_loss: 0.2572, step time: 0.1229\n",
      "185/223, train_loss: 0.2687, step time: 0.1015\n",
      "186/223, train_loss: 0.2409, step time: 0.1002\n",
      "187/223, train_loss: 0.2554, step time: 0.1013\n",
      "188/223, train_loss: 0.2505, step time: 0.1014\n",
      "189/223, train_loss: 0.2516, step time: 0.1194\n",
      "190/223, train_loss: 0.2628, step time: 0.1017\n",
      "191/223, train_loss: 0.2539, step time: 0.1218\n",
      "192/223, train_loss: 0.2553, step time: 0.1197\n",
      "193/223, train_loss: 0.2549, step time: 0.1055\n",
      "194/223, train_loss: 0.2416, step time: 0.1105\n",
      "195/223, train_loss: 0.2441, step time: 0.1239\n",
      "196/223, train_loss: 0.2483, step time: 0.1065\n",
      "197/223, train_loss: 0.2479, step time: 0.1018\n",
      "198/223, train_loss: 0.2572, step time: 0.1073\n",
      "199/223, train_loss: 0.2499, step time: 0.1261\n",
      "200/223, train_loss: 0.2432, step time: 0.1054\n",
      "201/223, train_loss: 0.2516, step time: 0.1180\n",
      "202/223, train_loss: 0.2509, step time: 0.1044\n",
      "203/223, train_loss: 0.2469, step time: 0.1052\n",
      "204/223, train_loss: 0.2567, step time: 0.0999\n",
      "205/223, train_loss: 0.2369, step time: 0.1007\n",
      "206/223, train_loss: 0.2374, step time: 0.1003\n",
      "207/223, train_loss: 0.2489, step time: 0.1003\n",
      "208/223, train_loss: 0.2399, step time: 0.1010\n",
      "209/223, train_loss: 0.2666, step time: 0.0987\n",
      "210/223, train_loss: 0.2431, step time: 0.1104\n",
      "211/223, train_loss: 0.2394, step time: 0.1157\n",
      "212/223, train_loss: 0.2377, step time: 0.1015\n",
      "213/223, train_loss: 0.2638, step time: 0.0999\n",
      "214/223, train_loss: 0.2483, step time: 0.1246\n",
      "215/223, train_loss: 0.2502, step time: 0.1277\n",
      "216/223, train_loss: 0.2471, step time: 0.0999\n",
      "217/223, train_loss: 0.2487, step time: 0.0987\n",
      "218/223, train_loss: 0.2562, step time: 0.0987\n",
      "219/223, train_loss: 0.2496, step time: 0.0987\n",
      "220/223, train_loss: 0.2516, step time: 0.0994\n",
      "221/223, train_loss: 0.2490, step time: 0.0995\n",
      "222/223, train_loss: 0.2462, step time: 0.0991\n",
      "223/223, train_loss: 0.2436, step time: 0.0997\n",
      "epoch 10 average loss: 0.2611\n",
      "saved new best metric model\n",
      "current epoch: 10 current mean dice: 0.7534 tc: 0.8664 wt: 0.7487 et: 0.6451\n",
      "best mean dice: 0.7534 at epoch: 10\n",
      "time consuming of epoch 10 is: 90.1614\n",
      "----------\n",
      "epoch 11/300\n",
      "1/223, train_loss: 0.2414, step time: 0.1087\n",
      "2/223, train_loss: 0.2443, step time: 0.1069\n",
      "3/223, train_loss: 0.2389, step time: 0.0992\n",
      "4/223, train_loss: 0.2468, step time: 0.0994\n",
      "5/223, train_loss: 0.2400, step time: 0.1117\n",
      "6/223, train_loss: 0.2406, step time: 0.1040\n",
      "7/223, train_loss: 0.2407, step time: 0.1100\n",
      "8/223, train_loss: 0.2527, step time: 0.1236\n",
      "9/223, train_loss: 0.2642, step time: 0.0999\n",
      "10/223, train_loss: 0.2438, step time: 0.1079\n",
      "11/223, train_loss: 0.2650, step time: 0.1013\n",
      "12/223, train_loss: 0.2555, step time: 0.1000\n",
      "13/223, train_loss: 0.2351, step time: 0.1393\n",
      "14/223, train_loss: 0.2445, step time: 0.1074\n",
      "15/223, train_loss: 0.2565, step time: 0.1029\n",
      "16/223, train_loss: 0.2443, step time: 0.1022\n",
      "17/223, train_loss: 0.2412, step time: 0.1100\n",
      "18/223, train_loss: 0.2474, step time: 0.1140\n",
      "19/223, train_loss: 0.2406, step time: 0.1115\n",
      "20/223, train_loss: 0.2294, step time: 0.1006\n",
      "21/223, train_loss: 0.2604, step time: 0.1134\n",
      "22/223, train_loss: 0.2669, step time: 0.1117\n",
      "23/223, train_loss: 0.2533, step time: 0.1116\n",
      "24/223, train_loss: 0.2487, step time: 0.1137\n",
      "25/223, train_loss: 0.2653, step time: 0.1133\n",
      "26/223, train_loss: 0.2669, step time: 0.1164\n",
      "27/223, train_loss: 0.2433, step time: 0.1021\n",
      "28/223, train_loss: 0.2293, step time: 0.1011\n",
      "29/223, train_loss: 0.2495, step time: 0.1077\n",
      "30/223, train_loss: 0.2331, step time: 0.1438\n",
      "31/223, train_loss: 0.2445, step time: 0.1182\n",
      "32/223, train_loss: 0.2440, step time: 0.0999\n",
      "33/223, train_loss: 0.2500, step time: 0.1273\n",
      "34/223, train_loss: 0.2578, step time: 0.1148\n",
      "35/223, train_loss: 0.2504, step time: 0.1129\n",
      "36/223, train_loss: 0.2359, step time: 0.0999\n",
      "37/223, train_loss: 0.2358, step time: 0.1114\n",
      "38/223, train_loss: 0.2563, step time: 0.1009\n",
      "39/223, train_loss: 0.2392, step time: 0.1138\n",
      "40/223, train_loss: 0.2468, step time: 0.1029\n",
      "41/223, train_loss: 0.2390, step time: 0.1182\n",
      "42/223, train_loss: 0.2518, step time: 0.1081\n",
      "43/223, train_loss: 0.2388, step time: 0.1005\n",
      "44/223, train_loss: 0.2448, step time: 0.1000\n",
      "45/223, train_loss: 0.2536, step time: 0.1004\n",
      "46/223, train_loss: 0.2331, step time: 0.1000\n",
      "47/223, train_loss: 0.2447, step time: 0.1000\n",
      "48/223, train_loss: 0.2388, step time: 0.1007\n",
      "49/223, train_loss: 0.2358, step time: 0.1092\n",
      "50/223, train_loss: 0.2488, step time: 0.1052\n",
      "51/223, train_loss: 0.2488, step time: 0.1032\n",
      "52/223, train_loss: 0.2396, step time: 0.1008\n",
      "53/223, train_loss: 0.2467, step time: 0.1258\n",
      "54/223, train_loss: 0.2445, step time: 0.0996\n",
      "55/223, train_loss: 0.2512, step time: 0.0998\n",
      "56/223, train_loss: 0.2323, step time: 0.1036\n",
      "57/223, train_loss: 0.2402, step time: 0.1088\n",
      "58/223, train_loss: 0.2486, step time: 0.1002\n",
      "59/223, train_loss: 0.2496, step time: 0.0994\n",
      "60/223, train_loss: 0.2467, step time: 0.1046\n",
      "61/223, train_loss: 0.2400, step time: 0.1455\n",
      "62/223, train_loss: 0.2357, step time: 0.1115\n",
      "63/223, train_loss: 0.2559, step time: 0.1007\n",
      "64/223, train_loss: 0.2400, step time: 0.0999\n",
      "65/223, train_loss: 0.2397, step time: 0.1123\n",
      "66/223, train_loss: 0.2624, step time: 0.0998\n",
      "67/223, train_loss: 0.2442, step time: 0.1001\n",
      "68/223, train_loss: 0.2304, step time: 0.1033\n",
      "69/223, train_loss: 0.2344, step time: 0.1003\n",
      "70/223, train_loss: 0.2276, step time: 0.1010\n",
      "71/223, train_loss: 0.2337, step time: 0.0998\n",
      "72/223, train_loss: 0.2428, step time: 0.1038\n",
      "73/223, train_loss: 0.2390, step time: 0.1159\n",
      "74/223, train_loss: 0.2337, step time: 0.1001\n",
      "75/223, train_loss: 0.2498, step time: 0.0998\n",
      "76/223, train_loss: 0.2450, step time: 0.1483\n",
      "77/223, train_loss: 0.2501, step time: 0.1052\n",
      "78/223, train_loss: 0.2390, step time: 0.1016\n",
      "79/223, train_loss: 0.2583, step time: 0.1081\n",
      "80/223, train_loss: 0.2411, step time: 0.1006\n",
      "81/223, train_loss: 0.2400, step time: 0.1055\n",
      "82/223, train_loss: 0.2481, step time: 0.1653\n",
      "83/223, train_loss: 0.2517, step time: 0.1126\n",
      "84/223, train_loss: 0.2518, step time: 0.1232\n",
      "85/223, train_loss: 0.2420, step time: 0.1118\n",
      "86/223, train_loss: 0.2631, step time: 0.1236\n",
      "87/223, train_loss: 0.2342, step time: 0.1263\n",
      "88/223, train_loss: 0.2651, step time: 0.1019\n",
      "89/223, train_loss: 0.2384, step time: 0.1107\n",
      "90/223, train_loss: 0.2581, step time: 0.1262\n",
      "91/223, train_loss: 0.2301, step time: 0.1162\n",
      "92/223, train_loss: 0.2333, step time: 0.1085\n",
      "93/223, train_loss: 0.2370, step time: 0.1047\n",
      "94/223, train_loss: 0.2470, step time: 0.1001\n",
      "95/223, train_loss: 0.2408, step time: 0.1001\n",
      "96/223, train_loss: 0.2489, step time: 0.1016\n",
      "97/223, train_loss: 0.2332, step time: 0.1048\n",
      "98/223, train_loss: 0.2459, step time: 0.0994\n",
      "99/223, train_loss: 0.2512, step time: 0.0996\n",
      "100/223, train_loss: 0.2299, step time: 0.1048\n",
      "101/223, train_loss: 0.2384, step time: 0.1571\n",
      "102/223, train_loss: 0.2328, step time: 0.1268\n",
      "103/223, train_loss: 0.2234, step time: 0.1008\n",
      "104/223, train_loss: 0.2304, step time: 0.1007\n",
      "105/223, train_loss: 0.2338, step time: 0.1002\n",
      "106/223, train_loss: 0.2346, step time: 0.1001\n",
      "107/223, train_loss: 0.2375, step time: 0.1010\n",
      "108/223, train_loss: 0.2387, step time: 0.1011\n",
      "109/223, train_loss: 0.2447, step time: 0.1004\n",
      "110/223, train_loss: 0.2341, step time: 0.1007\n",
      "111/223, train_loss: 0.2291, step time: 0.1002\n",
      "112/223, train_loss: 0.2464, step time: 0.1031\n",
      "113/223, train_loss: 0.2303, step time: 0.1095\n",
      "114/223, train_loss: 0.2400, step time: 0.1221\n",
      "115/223, train_loss: 0.2549, step time: 0.1060\n",
      "116/223, train_loss: 0.2519, step time: 0.1067\n",
      "117/223, train_loss: 0.2387, step time: 0.1019\n",
      "118/223, train_loss: 0.2500, step time: 0.1194\n",
      "119/223, train_loss: 0.2478, step time: 0.1009\n",
      "120/223, train_loss: 0.2466, step time: 0.0999\n",
      "121/223, train_loss: 0.2335, step time: 0.1089\n",
      "122/223, train_loss: 0.2364, step time: 0.1113\n",
      "123/223, train_loss: 0.2222, step time: 0.1013\n",
      "124/223, train_loss: 0.2530, step time: 0.1002\n",
      "125/223, train_loss: 0.2385, step time: 0.1403\n",
      "126/223, train_loss: 0.2437, step time: 0.1198\n",
      "127/223, train_loss: 0.2574, step time: 0.1020\n",
      "128/223, train_loss: 0.2398, step time: 0.0998\n",
      "129/223, train_loss: 0.2417, step time: 0.1061\n",
      "130/223, train_loss: 0.2416, step time: 0.1001\n",
      "131/223, train_loss: 0.2321, step time: 0.1000\n",
      "132/223, train_loss: 0.2389, step time: 0.1056\n",
      "133/223, train_loss: 0.2422, step time: 0.0994\n",
      "134/223, train_loss: 0.2346, step time: 0.0996\n",
      "135/223, train_loss: 0.2408, step time: 0.1306\n",
      "136/223, train_loss: 0.2504, step time: 0.1004\n",
      "137/223, train_loss: 0.2412, step time: 0.1029\n",
      "138/223, train_loss: 0.2477, step time: 0.0992\n",
      "139/223, train_loss: 0.2431, step time: 0.1011\n",
      "140/223, train_loss: 0.2409, step time: 0.1064\n",
      "141/223, train_loss: 0.2400, step time: 0.1324\n",
      "142/223, train_loss: 0.2359, step time: 0.1073\n",
      "143/223, train_loss: 0.2441, step time: 0.1001\n",
      "144/223, train_loss: 0.2404, step time: 0.1011\n",
      "145/223, train_loss: 0.2458, step time: 0.1042\n",
      "146/223, train_loss: 0.2474, step time: 0.1131\n",
      "147/223, train_loss: 0.2344, step time: 0.1000\n",
      "148/223, train_loss: 0.2260, step time: 0.0993\n",
      "149/223, train_loss: 0.2399, step time: 0.1041\n",
      "150/223, train_loss: 0.2400, step time: 0.1068\n",
      "151/223, train_loss: 0.2258, step time: 0.1260\n",
      "152/223, train_loss: 0.2518, step time: 0.1179\n",
      "153/223, train_loss: 0.2330, step time: 0.1160\n",
      "154/223, train_loss: 0.2304, step time: 0.1065\n",
      "155/223, train_loss: 0.2464, step time: 0.1325\n",
      "156/223, train_loss: 0.2417, step time: 0.1098\n",
      "157/223, train_loss: 0.2374, step time: 0.1063\n",
      "158/223, train_loss: 0.2391, step time: 0.1391\n",
      "159/223, train_loss: 0.3748, step time: 0.1080\n",
      "160/223, train_loss: 0.2244, step time: 0.1194\n",
      "161/223, train_loss: 0.2349, step time: 0.1113\n",
      "162/223, train_loss: 0.2313, step time: 0.1177\n",
      "163/223, train_loss: 0.2258, step time: 0.1087\n",
      "164/223, train_loss: 0.2432, step time: 0.1246\n",
      "165/223, train_loss: 0.2128, step time: 0.1014\n",
      "166/223, train_loss: 0.2346, step time: 0.1051\n",
      "167/223, train_loss: 0.2228, step time: 0.1064\n",
      "168/223, train_loss: 0.2395, step time: 0.1006\n",
      "169/223, train_loss: 0.2227, step time: 0.0993\n",
      "170/223, train_loss: 0.2362, step time: 0.0988\n",
      "171/223, train_loss: 0.2252, step time: 0.0995\n",
      "172/223, train_loss: 0.2309, step time: 0.0998\n",
      "173/223, train_loss: 0.2422, step time: 0.1063\n",
      "174/223, train_loss: 0.2427, step time: 0.1183\n",
      "175/223, train_loss: 0.2246, step time: 0.1196\n",
      "176/223, train_loss: 0.2291, step time: 0.1130\n",
      "177/223, train_loss: 0.2237, step time: 0.1146\n",
      "178/223, train_loss: 0.2282, step time: 0.1008\n",
      "179/223, train_loss: 0.2251, step time: 0.1313\n",
      "180/223, train_loss: 0.2270, step time: 0.1030\n",
      "181/223, train_loss: 0.2330, step time: 0.1000\n",
      "182/223, train_loss: 0.2259, step time: 0.0994\n",
      "183/223, train_loss: 0.2391, step time: 0.1005\n",
      "184/223, train_loss: 0.2374, step time: 0.1140\n",
      "185/223, train_loss: 0.2417, step time: 0.1152\n",
      "186/223, train_loss: 0.2340, step time: 0.1163\n",
      "187/223, train_loss: 0.2415, step time: 0.1086\n",
      "188/223, train_loss: 0.2306, step time: 0.1286\n",
      "189/223, train_loss: 0.2306, step time: 0.1013\n",
      "190/223, train_loss: 0.2302, step time: 0.1033\n",
      "191/223, train_loss: 0.2341, step time: 0.1185\n",
      "192/223, train_loss: 0.2179, step time: 0.1006\n",
      "193/223, train_loss: 0.2268, step time: 0.1095\n",
      "194/223, train_loss: 0.2199, step time: 0.1187\n",
      "195/223, train_loss: 0.2373, step time: 0.1167\n",
      "196/223, train_loss: 0.2411, step time: 0.1103\n",
      "197/223, train_loss: 0.2253, step time: 0.1112\n",
      "198/223, train_loss: 0.2341, step time: 0.1005\n",
      "199/223, train_loss: 0.2256, step time: 0.1009\n",
      "200/223, train_loss: 0.2237, step time: 0.1066\n",
      "201/223, train_loss: 0.2340, step time: 0.1134\n",
      "202/223, train_loss: 0.2359, step time: 0.1010\n",
      "203/223, train_loss: 0.2281, step time: 0.0996\n",
      "204/223, train_loss: 0.2256, step time: 0.1090\n",
      "205/223, train_loss: 0.2262, step time: 0.1177\n",
      "206/223, train_loss: 0.2367, step time: 0.1173\n",
      "207/223, train_loss: 0.2281, step time: 0.1006\n",
      "208/223, train_loss: 0.2345, step time: 0.1116\n",
      "209/223, train_loss: 0.2276, step time: 0.1202\n",
      "210/223, train_loss: 0.2216, step time: 0.1096\n",
      "211/223, train_loss: 0.2172, step time: 0.1101\n",
      "212/223, train_loss: 0.2227, step time: 0.1057\n",
      "213/223, train_loss: 0.2174, step time: 0.0988\n",
      "214/223, train_loss: 0.2396, step time: 0.0989\n",
      "215/223, train_loss: 0.2256, step time: 0.0982\n",
      "216/223, train_loss: 0.2323, step time: 0.1023\n",
      "217/223, train_loss: 0.2293, step time: 0.1001\n",
      "218/223, train_loss: 0.2335, step time: 0.1036\n",
      "219/223, train_loss: 0.2233, step time: 0.0997\n",
      "220/223, train_loss: 0.2253, step time: 0.1034\n",
      "221/223, train_loss: 0.2300, step time: 0.0992\n",
      "222/223, train_loss: 0.2332, step time: 0.0991\n",
      "223/223, train_loss: 0.2324, step time: 0.0996\n",
      "epoch 11 average loss: 0.2399\n",
      "time consuming of epoch 11 is: 87.7724\n",
      "----------\n",
      "epoch 12/300\n",
      "1/223, train_loss: 0.2455, step time: 0.1017\n",
      "2/223, train_loss: 0.2305, step time: 0.1143\n",
      "3/223, train_loss: 0.2296, step time: 0.1026\n",
      "4/223, train_loss: 0.2259, step time: 0.1080\n",
      "5/223, train_loss: 0.2408, step time: 0.1005\n",
      "6/223, train_loss: 0.2209, step time: 0.1009\n",
      "7/223, train_loss: 0.2161, step time: 0.1083\n",
      "8/223, train_loss: 0.2149, step time: 0.1165\n",
      "9/223, train_loss: 0.2227, step time: 0.1017\n",
      "10/223, train_loss: 0.2276, step time: 0.1006\n",
      "11/223, train_loss: 0.2248, step time: 0.1116\n",
      "12/223, train_loss: 0.2247, step time: 0.1219\n",
      "13/223, train_loss: 0.2586, step time: 0.1108\n",
      "14/223, train_loss: 0.2287, step time: 0.1152\n",
      "15/223, train_loss: 0.2106, step time: 0.1118\n",
      "16/223, train_loss: 0.2168, step time: 0.1243\n",
      "17/223, train_loss: 0.2243, step time: 0.1111\n",
      "18/223, train_loss: 0.2163, step time: 0.1126\n",
      "19/223, train_loss: 0.2369, step time: 0.1363\n",
      "20/223, train_loss: 0.2244, step time: 0.1392\n",
      "21/223, train_loss: 0.2243, step time: 0.1000\n",
      "22/223, train_loss: 0.2443, step time: 0.0997\n",
      "23/223, train_loss: 0.2285, step time: 0.1006\n",
      "24/223, train_loss: 0.2151, step time: 0.0998\n",
      "25/223, train_loss: 0.2392, step time: 0.0996\n",
      "26/223, train_loss: 0.2463, step time: 0.1008\n",
      "27/223, train_loss: 0.2179, step time: 0.0999\n",
      "28/223, train_loss: 0.2196, step time: 0.0998\n",
      "29/223, train_loss: 0.2353, step time: 0.1010\n",
      "30/223, train_loss: 0.2271, step time: 0.1006\n",
      "31/223, train_loss: 0.2262, step time: 0.1063\n",
      "32/223, train_loss: 0.2338, step time: 0.0998\n",
      "33/223, train_loss: 0.2332, step time: 0.1025\n",
      "34/223, train_loss: 0.2257, step time: 0.1006\n",
      "35/223, train_loss: 0.2222, step time: 0.1041\n",
      "36/223, train_loss: 0.2400, step time: 0.1006\n",
      "37/223, train_loss: 0.2212, step time: 0.0998\n",
      "38/223, train_loss: 0.2207, step time: 0.1004\n",
      "39/223, train_loss: 0.2307, step time: 0.1149\n",
      "40/223, train_loss: 0.2162, step time: 0.1177\n",
      "41/223, train_loss: 0.2347, step time: 0.1019\n",
      "42/223, train_loss: 0.2299, step time: 0.1127\n",
      "43/223, train_loss: 0.2176, step time: 0.1042\n",
      "44/223, train_loss: 0.2297, step time: 0.0992\n",
      "45/223, train_loss: 0.2233, step time: 0.1011\n",
      "46/223, train_loss: 0.2187, step time: 0.1154\n",
      "47/223, train_loss: 0.2453, step time: 0.1123\n",
      "48/223, train_loss: 0.2218, step time: 0.1072\n",
      "49/223, train_loss: 0.2170, step time: 0.1058\n",
      "50/223, train_loss: 0.2395, step time: 0.1114\n",
      "51/223, train_loss: 0.2224, step time: 0.1281\n",
      "52/223, train_loss: 0.2348, step time: 0.1008\n",
      "53/223, train_loss: 0.2345, step time: 0.0999\n",
      "54/223, train_loss: 0.2106, step time: 0.1028\n",
      "55/223, train_loss: 0.2277, step time: 0.1006\n",
      "56/223, train_loss: 0.2164, step time: 0.1152\n",
      "57/223, train_loss: 0.2153, step time: 0.1004\n",
      "58/223, train_loss: 0.2184, step time: 0.1160\n",
      "59/223, train_loss: 0.2198, step time: 0.1089\n",
      "60/223, train_loss: 0.2176, step time: 0.1144\n",
      "61/223, train_loss: 0.2220, step time: 0.1052\n",
      "62/223, train_loss: 0.2235, step time: 0.1238\n",
      "63/223, train_loss: 0.2212, step time: 0.1192\n",
      "64/223, train_loss: 0.2218, step time: 0.1023\n",
      "65/223, train_loss: 0.2247, step time: 0.1214\n",
      "66/223, train_loss: 0.2298, step time: 0.1168\n",
      "67/223, train_loss: 0.2070, step time: 0.1100\n",
      "68/223, train_loss: 0.2324, step time: 0.1180\n",
      "69/223, train_loss: 0.2181, step time: 0.1222\n",
      "70/223, train_loss: 0.2120, step time: 0.1005\n",
      "71/223, train_loss: 0.2099, step time: 0.1013\n",
      "72/223, train_loss: 0.2325, step time: 0.1039\n",
      "73/223, train_loss: 0.2247, step time: 0.1076\n",
      "74/223, train_loss: 0.2289, step time: 0.1034\n",
      "75/223, train_loss: 0.2304, step time: 0.1007\n",
      "76/223, train_loss: 0.2218, step time: 0.1151\n",
      "77/223, train_loss: 0.2128, step time: 0.1009\n",
      "78/223, train_loss: 0.2291, step time: 0.1065\n",
      "79/223, train_loss: 0.2148, step time: 0.1274\n",
      "80/223, train_loss: 0.2178, step time: 0.1000\n",
      "81/223, train_loss: 0.2331, step time: 0.1017\n",
      "82/223, train_loss: 0.2168, step time: 0.1005\n",
      "83/223, train_loss: 0.2247, step time: 0.1137\n",
      "84/223, train_loss: 0.2150, step time: 0.1131\n",
      "85/223, train_loss: 0.2470, step time: 0.1301\n",
      "86/223, train_loss: 0.2402, step time: 0.1053\n",
      "87/223, train_loss: 0.2320, step time: 0.0996\n",
      "88/223, train_loss: 0.2122, step time: 0.1080\n",
      "89/223, train_loss: 0.2394, step time: 0.1097\n",
      "90/223, train_loss: 0.2256, step time: 0.1006\n",
      "91/223, train_loss: 0.2131, step time: 0.1001\n",
      "92/223, train_loss: 0.2335, step time: 0.1093\n",
      "93/223, train_loss: 0.2388, step time: 0.1142\n",
      "94/223, train_loss: 0.2179, step time: 0.1125\n",
      "95/223, train_loss: 0.2233, step time: 0.1030\n",
      "96/223, train_loss: 0.2196, step time: 0.1063\n",
      "97/223, train_loss: 0.2279, step time: 0.1001\n",
      "98/223, train_loss: 0.2392, step time: 0.1043\n",
      "99/223, train_loss: 0.2216, step time: 0.1001\n",
      "100/223, train_loss: 0.2219, step time: 0.1112\n",
      "101/223, train_loss: 0.2271, step time: 0.1059\n",
      "102/223, train_loss: 0.2124, step time: 0.1040\n",
      "103/223, train_loss: 0.2180, step time: 0.1139\n",
      "104/223, train_loss: 0.2186, step time: 0.1033\n",
      "105/223, train_loss: 0.2234, step time: 0.0999\n",
      "106/223, train_loss: 0.2234, step time: 0.1050\n",
      "107/223, train_loss: 0.2197, step time: 0.1000\n",
      "108/223, train_loss: 0.2246, step time: 0.1151\n",
      "109/223, train_loss: 0.2154, step time: 0.1111\n",
      "110/223, train_loss: 0.2138, step time: 0.1024\n",
      "111/223, train_loss: 0.2216, step time: 0.0994\n",
      "112/223, train_loss: 0.2156, step time: 0.1028\n",
      "113/223, train_loss: 0.2055, step time: 0.1009\n",
      "114/223, train_loss: 0.2355, step time: 0.1162\n",
      "115/223, train_loss: 0.2125, step time: 0.1186\n",
      "116/223, train_loss: 0.2132, step time: 0.1006\n",
      "117/223, train_loss: 0.2053, step time: 0.1095\n",
      "118/223, train_loss: 0.2204, step time: 0.1163\n",
      "119/223, train_loss: 0.2194, step time: 0.1323\n",
      "120/223, train_loss: 0.2188, step time: 0.1005\n",
      "121/223, train_loss: 0.2178, step time: 0.1004\n",
      "122/223, train_loss: 0.2343, step time: 0.1006\n",
      "123/223, train_loss: 0.2266, step time: 0.1001\n",
      "124/223, train_loss: 0.2063, step time: 0.1042\n",
      "125/223, train_loss: 0.2104, step time: 0.0993\n",
      "126/223, train_loss: 0.2205, step time: 0.1001\n",
      "127/223, train_loss: 0.2080, step time: 0.1006\n",
      "128/223, train_loss: 0.2125, step time: 0.1004\n",
      "129/223, train_loss: 0.2151, step time: 0.1009\n",
      "130/223, train_loss: 0.2223, step time: 0.0999\n",
      "131/223, train_loss: 0.2241, step time: 0.1002\n",
      "132/223, train_loss: 0.2146, step time: 0.1036\n",
      "133/223, train_loss: 0.2275, step time: 0.1066\n",
      "134/223, train_loss: 0.2222, step time: 0.1160\n",
      "135/223, train_loss: 0.2118, step time: 0.1072\n",
      "136/223, train_loss: 0.2175, step time: 0.1038\n",
      "137/223, train_loss: 0.2167, step time: 0.1007\n",
      "138/223, train_loss: 0.2179, step time: 0.1097\n",
      "139/223, train_loss: 0.2133, step time: 0.1287\n",
      "140/223, train_loss: 0.2280, step time: 0.1113\n",
      "141/223, train_loss: 0.2289, step time: 0.0994\n",
      "142/223, train_loss: 0.2220, step time: 0.1035\n",
      "143/223, train_loss: 0.2192, step time: 0.1081\n",
      "144/223, train_loss: 0.2166, step time: 0.1146\n",
      "145/223, train_loss: 0.2197, step time: 0.0992\n",
      "146/223, train_loss: 0.2185, step time: 0.1055\n",
      "147/223, train_loss: 0.2200, step time: 0.1007\n",
      "148/223, train_loss: 0.2130, step time: 0.1052\n",
      "149/223, train_loss: 0.2328, step time: 0.1134\n",
      "150/223, train_loss: 0.2211, step time: 0.1003\n",
      "151/223, train_loss: 0.2407, step time: 0.1005\n",
      "152/223, train_loss: 0.2133, step time: 0.1195\n",
      "153/223, train_loss: 0.2205, step time: 0.0996\n",
      "154/223, train_loss: 0.2109, step time: 0.1008\n",
      "155/223, train_loss: 0.2194, step time: 0.1006\n",
      "156/223, train_loss: 0.2075, step time: 0.1000\n",
      "157/223, train_loss: 0.2079, step time: 0.1103\n",
      "158/223, train_loss: 0.2140, step time: 0.1015\n",
      "159/223, train_loss: 0.2301, step time: 0.1028\n",
      "160/223, train_loss: 0.2228, step time: 0.1139\n",
      "161/223, train_loss: 0.2318, step time: 0.1192\n",
      "162/223, train_loss: 0.2150, step time: 0.1005\n",
      "163/223, train_loss: 0.2184, step time: 0.1033\n",
      "164/223, train_loss: 0.2145, step time: 0.1090\n",
      "165/223, train_loss: 0.2167, step time: 0.1048\n",
      "166/223, train_loss: 0.2269, step time: 0.0995\n",
      "167/223, train_loss: 0.2157, step time: 0.1251\n",
      "168/223, train_loss: 0.2136, step time: 0.1113\n",
      "169/223, train_loss: 0.2281, step time: 0.1104\n",
      "170/223, train_loss: 0.2052, step time: 0.1133\n",
      "171/223, train_loss: 0.2210, step time: 0.1056\n",
      "172/223, train_loss: 0.2133, step time: 0.1142\n",
      "173/223, train_loss: 0.2159, step time: 0.1115\n",
      "174/223, train_loss: 0.2211, step time: 0.1050\n",
      "175/223, train_loss: 0.2090, step time: 0.1076\n",
      "176/223, train_loss: 0.2207, step time: 0.1221\n",
      "177/223, train_loss: 0.2131, step time: 0.1002\n",
      "178/223, train_loss: 0.2211, step time: 0.1139\n",
      "179/223, train_loss: 0.2216, step time: 0.1174\n",
      "180/223, train_loss: 0.2146, step time: 0.1001\n",
      "181/223, train_loss: 0.2087, step time: 0.0997\n",
      "182/223, train_loss: 0.2164, step time: 0.1119\n",
      "183/223, train_loss: 0.2219, step time: 0.1078\n",
      "184/223, train_loss: 0.2044, step time: 0.1002\n",
      "185/223, train_loss: 0.2152, step time: 0.1005\n",
      "186/223, train_loss: 0.2076, step time: 0.1229\n",
      "187/223, train_loss: 0.2300, step time: 0.1040\n",
      "188/223, train_loss: 0.2295, step time: 0.1009\n",
      "189/223, train_loss: 0.2272, step time: 0.1000\n",
      "190/223, train_loss: 0.2176, step time: 0.1206\n",
      "191/223, train_loss: 0.2303, step time: 0.1122\n",
      "192/223, train_loss: 0.2162, step time: 0.1026\n",
      "193/223, train_loss: 0.2169, step time: 0.1011\n",
      "194/223, train_loss: 0.2104, step time: 0.1191\n",
      "195/223, train_loss: 0.2155, step time: 0.1185\n",
      "196/223, train_loss: 0.2284, step time: 0.1107\n",
      "197/223, train_loss: 0.2186, step time: 0.1015\n",
      "198/223, train_loss: 0.2010, step time: 0.1210\n",
      "199/223, train_loss: 0.2172, step time: 0.1135\n",
      "200/223, train_loss: 0.3781, step time: 0.1170\n",
      "201/223, train_loss: 0.2338, step time: 0.1211\n",
      "202/223, train_loss: 0.2287, step time: 0.1115\n",
      "203/223, train_loss: 0.2261, step time: 0.1008\n",
      "204/223, train_loss: 0.2133, step time: 0.1104\n",
      "205/223, train_loss: 0.2317, step time: 0.1001\n",
      "206/223, train_loss: 0.2131, step time: 0.1134\n",
      "207/223, train_loss: 0.2234, step time: 0.1064\n",
      "208/223, train_loss: 0.2311, step time: 0.1097\n",
      "209/223, train_loss: 0.2217, step time: 0.0996\n",
      "210/223, train_loss: 0.2107, step time: 0.1003\n",
      "211/223, train_loss: 0.2253, step time: 0.1111\n",
      "212/223, train_loss: 0.2021, step time: 0.1155\n",
      "213/223, train_loss: 0.2388, step time: 0.1132\n",
      "214/223, train_loss: 0.2062, step time: 0.1060\n",
      "215/223, train_loss: 0.2067, step time: 0.1066\n",
      "216/223, train_loss: 0.2184, step time: 0.1266\n",
      "217/223, train_loss: 0.2137, step time: 0.1064\n",
      "218/223, train_loss: 0.2182, step time: 0.1104\n",
      "219/223, train_loss: 0.2216, step time: 0.1000\n",
      "220/223, train_loss: 0.2180, step time: 0.0983\n",
      "221/223, train_loss: 0.2128, step time: 0.0990\n",
      "222/223, train_loss: 0.2346, step time: 0.1000\n",
      "223/223, train_loss: 0.2139, step time: 0.1003\n",
      "epoch 12 average loss: 0.2226\n",
      "time consuming of epoch 12 is: 86.7277\n",
      "----------\n",
      "epoch 13/300\n",
      "1/223, train_loss: 0.2123, step time: 0.1034\n",
      "2/223, train_loss: 0.2259, step time: 0.1130\n",
      "3/223, train_loss: 0.2099, step time: 0.1025\n",
      "4/223, train_loss: 0.2127, step time: 0.1164\n",
      "5/223, train_loss: 0.2029, step time: 0.1193\n",
      "6/223, train_loss: 0.2202, step time: 0.1000\n",
      "7/223, train_loss: 0.2144, step time: 0.1265\n",
      "8/223, train_loss: 0.2212, step time: 0.1144\n",
      "9/223, train_loss: 0.2247, step time: 0.1133\n",
      "10/223, train_loss: 0.2180, step time: 0.1005\n",
      "11/223, train_loss: 0.2213, step time: 0.1103\n",
      "12/223, train_loss: 0.2151, step time: 0.1150\n",
      "13/223, train_loss: 0.2149, step time: 0.1040\n",
      "14/223, train_loss: 0.2124, step time: 0.1196\n",
      "15/223, train_loss: 0.2066, step time: 0.1089\n",
      "16/223, train_loss: 0.2247, step time: 0.1059\n",
      "17/223, train_loss: 0.3757, step time: 0.1005\n",
      "18/223, train_loss: 0.2267, step time: 0.1004\n",
      "19/223, train_loss: 0.1987, step time: 0.1145\n",
      "20/223, train_loss: 0.2164, step time: 0.1134\n",
      "21/223, train_loss: 0.2009, step time: 0.0998\n",
      "22/223, train_loss: 0.2122, step time: 0.1012\n",
      "23/223, train_loss: 0.2131, step time: 0.1004\n",
      "24/223, train_loss: 0.2182, step time: 0.0997\n",
      "25/223, train_loss: 0.2089, step time: 0.1050\n",
      "26/223, train_loss: 0.2150, step time: 0.1009\n",
      "27/223, train_loss: 0.2130, step time: 0.1003\n",
      "28/223, train_loss: 0.2124, step time: 0.1003\n",
      "29/223, train_loss: 0.2211, step time: 0.1140\n",
      "30/223, train_loss: 0.2021, step time: 0.0995\n",
      "31/223, train_loss: 0.2228, step time: 0.0999\n",
      "32/223, train_loss: 0.2136, step time: 0.0999\n",
      "33/223, train_loss: 0.2245, step time: 0.1027\n",
      "34/223, train_loss: 0.2044, step time: 0.0996\n",
      "35/223, train_loss: 0.2020, step time: 0.0990\n",
      "36/223, train_loss: 0.2183, step time: 0.0998\n",
      "37/223, train_loss: 0.2175, step time: 0.0997\n",
      "38/223, train_loss: 0.2200, step time: 0.0995\n",
      "39/223, train_loss: 0.2344, step time: 0.1000\n",
      "40/223, train_loss: 0.2086, step time: 0.1006\n",
      "41/223, train_loss: 0.2148, step time: 0.1039\n",
      "42/223, train_loss: 0.2088, step time: 0.1128\n",
      "43/223, train_loss: 0.2227, step time: 0.0999\n",
      "44/223, train_loss: 0.2082, step time: 0.1024\n",
      "45/223, train_loss: 0.2060, step time: 0.1008\n",
      "46/223, train_loss: 0.2407, step time: 0.1060\n",
      "47/223, train_loss: 0.2050, step time: 0.1003\n",
      "48/223, train_loss: 0.2150, step time: 0.1003\n",
      "49/223, train_loss: 0.2056, step time: 0.1041\n",
      "50/223, train_loss: 0.2076, step time: 0.1071\n",
      "51/223, train_loss: 0.2114, step time: 0.1060\n",
      "52/223, train_loss: 0.1999, step time: 0.1042\n",
      "53/223, train_loss: 0.2085, step time: 0.0996\n",
      "54/223, train_loss: 0.1976, step time: 0.1177\n",
      "55/223, train_loss: 0.2158, step time: 0.1179\n",
      "56/223, train_loss: 0.2078, step time: 0.1236\n",
      "57/223, train_loss: 0.2137, step time: 0.1205\n",
      "58/223, train_loss: 0.2164, step time: 0.1049\n",
      "59/223, train_loss: 0.2032, step time: 0.1105\n",
      "60/223, train_loss: 0.2242, step time: 0.1134\n",
      "61/223, train_loss: 0.2153, step time: 0.1050\n",
      "62/223, train_loss: 0.2347, step time: 0.1099\n",
      "63/223, train_loss: 0.1979, step time: 0.1087\n",
      "64/223, train_loss: 0.2292, step time: 0.1198\n",
      "65/223, train_loss: 0.2042, step time: 0.1064\n",
      "66/223, train_loss: 0.2200, step time: 0.1098\n",
      "67/223, train_loss: 0.2111, step time: 0.1094\n",
      "68/223, train_loss: 0.2262, step time: 0.1128\n",
      "69/223, train_loss: 0.2096, step time: 0.0985\n",
      "70/223, train_loss: 0.2118, step time: 0.1046\n",
      "71/223, train_loss: 0.2101, step time: 0.1005\n",
      "72/223, train_loss: 0.2170, step time: 0.0997\n",
      "73/223, train_loss: 0.2139, step time: 0.1025\n",
      "74/223, train_loss: 0.2155, step time: 0.1034\n",
      "75/223, train_loss: 0.2055, step time: 0.1030\n",
      "76/223, train_loss: 0.2187, step time: 0.0997\n",
      "77/223, train_loss: 0.2176, step time: 0.1125\n",
      "78/223, train_loss: 0.2062, step time: 0.1232\n",
      "79/223, train_loss: 0.2199, step time: 0.1058\n",
      "80/223, train_loss: 0.2125, step time: 0.1007\n",
      "81/223, train_loss: 0.2102, step time: 0.1000\n",
      "82/223, train_loss: 0.2048, step time: 0.1072\n",
      "83/223, train_loss: 0.1980, step time: 0.1087\n",
      "84/223, train_loss: 0.2212, step time: 0.1006\n",
      "85/223, train_loss: 0.2137, step time: 0.1004\n",
      "86/223, train_loss: 0.2150, step time: 0.1176\n",
      "87/223, train_loss: 0.2266, step time: 0.1175\n",
      "88/223, train_loss: 0.2350, step time: 0.1066\n",
      "89/223, train_loss: 0.2095, step time: 0.1113\n",
      "90/223, train_loss: 0.2061, step time: 0.1282\n",
      "91/223, train_loss: 0.2034, step time: 0.1001\n",
      "92/223, train_loss: 0.2160, step time: 0.0995\n",
      "93/223, train_loss: 0.2158, step time: 0.1088\n",
      "94/223, train_loss: 0.2156, step time: 0.1113\n",
      "95/223, train_loss: 0.2014, step time: 0.1162\n",
      "96/223, train_loss: 0.2306, step time: 0.1136\n",
      "97/223, train_loss: 0.1945, step time: 0.1006\n",
      "98/223, train_loss: 0.2187, step time: 0.1084\n",
      "99/223, train_loss: 0.2051, step time: 0.1055\n",
      "100/223, train_loss: 0.2034, step time: 0.1152\n",
      "101/223, train_loss: 0.2041, step time: 0.1050\n",
      "102/223, train_loss: 0.2018, step time: 0.1237\n",
      "103/223, train_loss: 0.2154, step time: 0.1107\n",
      "104/223, train_loss: 0.2028, step time: 0.0989\n",
      "105/223, train_loss: 0.2145, step time: 0.1050\n",
      "106/223, train_loss: 0.1998, step time: 0.1054\n",
      "107/223, train_loss: 0.2320, step time: 0.0998\n",
      "108/223, train_loss: 0.2197, step time: 0.1001\n",
      "109/223, train_loss: 0.1989, step time: 0.1011\n",
      "110/223, train_loss: 0.2277, step time: 0.1028\n",
      "111/223, train_loss: 0.2198, step time: 0.1040\n",
      "112/223, train_loss: 0.2205, step time: 0.1155\n",
      "113/223, train_loss: 0.2272, step time: 0.0995\n",
      "114/223, train_loss: 0.2227, step time: 0.1140\n",
      "115/223, train_loss: 0.2044, step time: 0.1060\n",
      "116/223, train_loss: 0.2033, step time: 0.1274\n",
      "117/223, train_loss: 0.2059, step time: 0.1252\n",
      "118/223, train_loss: 0.2256, step time: 0.1000\n",
      "119/223, train_loss: 0.2137, step time: 0.0999\n",
      "120/223, train_loss: 0.2151, step time: 0.1000\n",
      "121/223, train_loss: 0.2002, step time: 0.1014\n",
      "122/223, train_loss: 0.2210, step time: 0.1027\n",
      "123/223, train_loss: 0.2008, step time: 0.1099\n",
      "124/223, train_loss: 0.1926, step time: 0.1013\n",
      "125/223, train_loss: 0.2025, step time: 0.1000\n",
      "126/223, train_loss: 0.2115, step time: 0.1031\n",
      "127/223, train_loss: 0.2121, step time: 0.1005\n",
      "128/223, train_loss: 0.2065, step time: 0.1002\n",
      "129/223, train_loss: 0.1986, step time: 0.1047\n",
      "130/223, train_loss: 0.2196, step time: 0.1074\n",
      "131/223, train_loss: 0.2258, step time: 0.1457\n",
      "132/223, train_loss: 0.2172, step time: 0.1096\n",
      "133/223, train_loss: 0.2220, step time: 0.1016\n",
      "134/223, train_loss: 0.2098, step time: 0.0999\n",
      "135/223, train_loss: 0.2089, step time: 0.1025\n",
      "136/223, train_loss: 0.2059, step time: 0.0997\n",
      "137/223, train_loss: 0.1918, step time: 0.1044\n",
      "138/223, train_loss: 0.2197, step time: 0.1465\n",
      "139/223, train_loss: 0.2126, step time: 0.1029\n",
      "140/223, train_loss: 0.1842, step time: 0.1003\n",
      "141/223, train_loss: 0.2288, step time: 0.1030\n",
      "142/223, train_loss: 0.1935, step time: 0.1232\n",
      "143/223, train_loss: 0.2118, step time: 0.1055\n",
      "144/223, train_loss: 0.2171, step time: 0.1006\n",
      "145/223, train_loss: 0.2051, step time: 0.1044\n",
      "146/223, train_loss: 0.1987, step time: 0.1171\n",
      "147/223, train_loss: 0.2093, step time: 0.1258\n",
      "148/223, train_loss: 0.2113, step time: 0.1107\n",
      "149/223, train_loss: 0.2265, step time: 0.1060\n",
      "150/223, train_loss: 0.1938, step time: 0.1013\n",
      "151/223, train_loss: 0.2300, step time: 0.1002\n",
      "152/223, train_loss: 0.1959, step time: 0.1005\n",
      "153/223, train_loss: 0.2031, step time: 0.1060\n",
      "154/223, train_loss: 0.2104, step time: 0.1052\n",
      "155/223, train_loss: 0.2088, step time: 0.1587\n",
      "156/223, train_loss: 0.2032, step time: 0.1260\n",
      "157/223, train_loss: 0.2193, step time: 0.1241\n",
      "158/223, train_loss: 0.2012, step time: 0.1229\n",
      "159/223, train_loss: 0.1987, step time: 0.1095\n",
      "160/223, train_loss: 0.2094, step time: 0.1059\n",
      "161/223, train_loss: 0.2111, step time: 0.1166\n",
      "162/223, train_loss: 0.1928, step time: 0.1000\n",
      "163/223, train_loss: 0.2007, step time: 0.1018\n",
      "164/223, train_loss: 0.1988, step time: 0.1014\n",
      "165/223, train_loss: 0.1934, step time: 0.1185\n",
      "166/223, train_loss: 0.2185, step time: 0.1097\n",
      "167/223, train_loss: 0.1939, step time: 0.1000\n",
      "168/223, train_loss: 0.2080, step time: 0.1009\n",
      "169/223, train_loss: 0.2003, step time: 0.1017\n",
      "170/223, train_loss: 0.2025, step time: 0.1158\n",
      "171/223, train_loss: 0.2014, step time: 0.1006\n",
      "172/223, train_loss: 0.2037, step time: 0.1331\n",
      "173/223, train_loss: 0.2308, step time: 0.1044\n",
      "174/223, train_loss: 0.1963, step time: 0.0988\n",
      "175/223, train_loss: 0.1982, step time: 0.1085\n",
      "176/223, train_loss: 0.2169, step time: 0.1149\n",
      "177/223, train_loss: 0.1881, step time: 0.1065\n",
      "178/223, train_loss: 0.2012, step time: 0.0990\n",
      "179/223, train_loss: 0.2005, step time: 0.1010\n",
      "180/223, train_loss: 0.2266, step time: 0.1049\n",
      "181/223, train_loss: 0.2242, step time: 0.0998\n",
      "182/223, train_loss: 0.1921, step time: 0.1099\n",
      "183/223, train_loss: 0.2162, step time: 0.1020\n",
      "184/223, train_loss: 0.2179, step time: 0.1052\n",
      "185/223, train_loss: 0.2012, step time: 0.1014\n",
      "186/223, train_loss: 0.2095, step time: 0.1033\n",
      "187/223, train_loss: 0.2050, step time: 0.1014\n",
      "188/223, train_loss: 0.2146, step time: 0.0998\n",
      "189/223, train_loss: 0.1964, step time: 0.1005\n",
      "190/223, train_loss: 0.2102, step time: 0.0995\n",
      "191/223, train_loss: 0.2055, step time: 0.1000\n",
      "192/223, train_loss: 0.1988, step time: 0.1128\n",
      "193/223, train_loss: 0.2047, step time: 0.1002\n",
      "194/223, train_loss: 0.2001, step time: 0.1074\n",
      "195/223, train_loss: 0.2118, step time: 0.1001\n",
      "196/223, train_loss: 0.2102, step time: 0.1000\n",
      "197/223, train_loss: 0.2062, step time: 0.1152\n",
      "198/223, train_loss: 0.2067, step time: 0.1301\n",
      "199/223, train_loss: 0.2032, step time: 0.0990\n",
      "200/223, train_loss: 0.2146, step time: 0.1221\n",
      "201/223, train_loss: 0.2112, step time: 0.1052\n",
      "202/223, train_loss: 0.1948, step time: 0.1078\n",
      "203/223, train_loss: 0.2091, step time: 0.1240\n",
      "204/223, train_loss: 0.2033, step time: 0.1123\n",
      "205/223, train_loss: 0.2046, step time: 0.1129\n",
      "206/223, train_loss: 0.1915, step time: 0.1158\n",
      "207/223, train_loss: 0.2007, step time: 0.1008\n",
      "208/223, train_loss: 0.1918, step time: 0.1040\n",
      "209/223, train_loss: 0.2215, step time: 0.1333\n",
      "210/223, train_loss: 0.2166, step time: 0.1002\n",
      "211/223, train_loss: 0.1840, step time: 0.1045\n",
      "212/223, train_loss: 0.2267, step time: 0.1213\n",
      "213/223, train_loss: 0.2029, step time: 0.1329\n",
      "214/223, train_loss: 0.2182, step time: 0.1082\n",
      "215/223, train_loss: 0.2079, step time: 0.1090\n",
      "216/223, train_loss: 0.2014, step time: 0.1050\n",
      "217/223, train_loss: 0.2072, step time: 0.0997\n",
      "218/223, train_loss: 0.1987, step time: 0.1005\n",
      "219/223, train_loss: 0.2139, step time: 0.0997\n",
      "220/223, train_loss: 0.1986, step time: 0.1004\n",
      "221/223, train_loss: 0.2154, step time: 0.0996\n",
      "222/223, train_loss: 0.1978, step time: 0.1002\n",
      "223/223, train_loss: 0.2021, step time: 0.0996\n",
      "epoch 13 average loss: 0.2112\n",
      "time consuming of epoch 13 is: 88.8030\n",
      "----------\n",
      "epoch 14/300\n",
      "1/223, train_loss: 0.1905, step time: 0.1171\n",
      "2/223, train_loss: 0.2085, step time: 0.1024\n",
      "3/223, train_loss: 0.2098, step time: 0.1167\n",
      "4/223, train_loss: 0.2050, step time: 0.1128\n",
      "5/223, train_loss: 0.2064, step time: 0.1041\n",
      "6/223, train_loss: 0.2115, step time: 0.1171\n",
      "7/223, train_loss: 0.2041, step time: 0.1127\n",
      "8/223, train_loss: 0.2213, step time: 0.1187\n",
      "9/223, train_loss: 0.2228, step time: 0.1141\n",
      "10/223, train_loss: 0.1995, step time: 0.1129\n",
      "11/223, train_loss: 0.2078, step time: 0.1107\n",
      "12/223, train_loss: 0.2048, step time: 0.1145\n",
      "13/223, train_loss: 0.1893, step time: 0.1151\n",
      "14/223, train_loss: 0.1956, step time: 0.1051\n",
      "15/223, train_loss: 0.1897, step time: 0.1004\n",
      "16/223, train_loss: 0.2142, step time: 0.1040\n",
      "17/223, train_loss: 0.1886, step time: 0.1171\n",
      "18/223, train_loss: 0.2065, step time: 0.1152\n",
      "19/223, train_loss: 0.2041, step time: 0.1060\n",
      "20/223, train_loss: 0.2154, step time: 0.1057\n",
      "21/223, train_loss: 0.2039, step time: 0.1122\n",
      "22/223, train_loss: 0.1910, step time: 0.1054\n",
      "23/223, train_loss: 0.2146, step time: 0.1007\n",
      "24/223, train_loss: 0.2041, step time: 0.1001\n",
      "25/223, train_loss: 0.2022, step time: 0.0996\n",
      "26/223, train_loss: 0.2088, step time: 0.1062\n",
      "27/223, train_loss: 0.2181, step time: 0.1197\n",
      "28/223, train_loss: 0.2102, step time: 0.1211\n",
      "29/223, train_loss: 0.2063, step time: 0.1188\n",
      "30/223, train_loss: 0.1937, step time: 0.1010\n",
      "31/223, train_loss: 0.2077, step time: 0.0991\n",
      "32/223, train_loss: 0.2097, step time: 0.0993\n",
      "33/223, train_loss: 0.2142, step time: 0.0984\n",
      "34/223, train_loss: 0.2131, step time: 0.1045\n",
      "35/223, train_loss: 0.1992, step time: 0.1002\n",
      "36/223, train_loss: 0.1942, step time: 0.1096\n",
      "37/223, train_loss: 0.2043, step time: 0.1038\n",
      "38/223, train_loss: 0.2090, step time: 0.1003\n",
      "39/223, train_loss: 0.2085, step time: 0.1058\n",
      "40/223, train_loss: 0.1996, step time: 0.1105\n",
      "41/223, train_loss: 0.1923, step time: 0.1072\n",
      "42/223, train_loss: 0.2043, step time: 0.1017\n",
      "43/223, train_loss: 0.2005, step time: 0.1082\n",
      "44/223, train_loss: 0.2023, step time: 0.1041\n",
      "45/223, train_loss: 0.1959, step time: 0.1084\n",
      "46/223, train_loss: 0.1974, step time: 0.1186\n",
      "47/223, train_loss: 0.2101, step time: 0.1033\n",
      "48/223, train_loss: 0.2081, step time: 0.1264\n",
      "49/223, train_loss: 0.1977, step time: 0.1002\n",
      "50/223, train_loss: 0.2090, step time: 0.1038\n",
      "51/223, train_loss: 0.1972, step time: 0.0998\n",
      "52/223, train_loss: 0.2052, step time: 0.1068\n",
      "53/223, train_loss: 0.1998, step time: 0.1057\n",
      "54/223, train_loss: 0.2057, step time: 0.1112\n",
      "55/223, train_loss: 0.1898, step time: 0.1007\n",
      "56/223, train_loss: 0.2109, step time: 0.0998\n",
      "57/223, train_loss: 0.1911, step time: 0.1173\n",
      "58/223, train_loss: 0.1887, step time: 0.1234\n",
      "59/223, train_loss: 0.2040, step time: 0.1099\n",
      "60/223, train_loss: 0.1966, step time: 0.1176\n",
      "61/223, train_loss: 0.2006, step time: 0.1184\n",
      "62/223, train_loss: 0.2034, step time: 0.1004\n",
      "63/223, train_loss: 0.2041, step time: 0.1145\n",
      "64/223, train_loss: 0.1870, step time: 0.1158\n",
      "65/223, train_loss: 0.1816, step time: 0.1002\n",
      "66/223, train_loss: 0.1848, step time: 0.0995\n",
      "67/223, train_loss: 0.1979, step time: 0.1005\n",
      "68/223, train_loss: 0.2039, step time: 0.1005\n",
      "69/223, train_loss: 0.1953, step time: 0.1090\n",
      "70/223, train_loss: 0.2209, step time: 0.1046\n",
      "71/223, train_loss: 0.2151, step time: 0.1008\n",
      "72/223, train_loss: 0.2065, step time: 0.1004\n",
      "73/223, train_loss: 0.1894, step time: 0.0996\n",
      "74/223, train_loss: 0.2123, step time: 0.0997\n",
      "75/223, train_loss: 0.2203, step time: 0.1003\n",
      "76/223, train_loss: 0.1993, step time: 0.1143\n",
      "77/223, train_loss: 0.1952, step time: 0.1060\n",
      "78/223, train_loss: 0.1968, step time: 0.1114\n",
      "79/223, train_loss: 0.2046, step time: 0.1019\n",
      "80/223, train_loss: 0.1999, step time: 0.1040\n",
      "81/223, train_loss: 0.2012, step time: 0.1297\n",
      "82/223, train_loss: 0.2150, step time: 0.1135\n",
      "83/223, train_loss: 0.1916, step time: 0.1071\n",
      "84/223, train_loss: 0.2121, step time: 0.1002\n",
      "85/223, train_loss: 0.1860, step time: 0.1331\n",
      "86/223, train_loss: 0.1996, step time: 0.1066\n",
      "87/223, train_loss: 0.1986, step time: 0.1148\n",
      "88/223, train_loss: 0.1865, step time: 0.1054\n",
      "89/223, train_loss: 0.2004, step time: 0.1186\n",
      "90/223, train_loss: 0.1881, step time: 0.1192\n",
      "91/223, train_loss: 0.1986, step time: 0.1023\n",
      "92/223, train_loss: 0.2053, step time: 0.1004\n",
      "93/223, train_loss: 0.2040, step time: 0.1134\n",
      "94/223, train_loss: 0.1830, step time: 0.1006\n",
      "95/223, train_loss: 0.1954, step time: 0.1010\n",
      "96/223, train_loss: 0.2057, step time: 0.1063\n",
      "97/223, train_loss: 0.1923, step time: 0.1091\n",
      "98/223, train_loss: 0.2043, step time: 0.1144\n",
      "99/223, train_loss: 0.2144, step time: 0.1050\n",
      "100/223, train_loss: 0.2088, step time: 0.1147\n",
      "101/223, train_loss: 0.1864, step time: 0.1222\n",
      "102/223, train_loss: 0.1908, step time: 0.1015\n",
      "103/223, train_loss: 0.2144, step time: 0.1023\n",
      "104/223, train_loss: 0.2098, step time: 0.1005\n",
      "105/223, train_loss: 0.2104, step time: 0.1000\n",
      "106/223, train_loss: 0.1993, step time: 0.1256\n",
      "107/223, train_loss: 0.1970, step time: 0.1161\n",
      "108/223, train_loss: 0.2152, step time: 0.1003\n",
      "109/223, train_loss: 0.2057, step time: 0.1112\n",
      "110/223, train_loss: 0.1830, step time: 0.1047\n",
      "111/223, train_loss: 0.2211, step time: 0.1103\n",
      "112/223, train_loss: 0.2011, step time: 0.0997\n",
      "113/223, train_loss: 0.1923, step time: 0.1019\n",
      "114/223, train_loss: 0.2021, step time: 0.1102\n",
      "115/223, train_loss: 0.2139, step time: 0.1005\n",
      "116/223, train_loss: 0.2005, step time: 0.1007\n",
      "117/223, train_loss: 0.1800, step time: 0.0992\n",
      "118/223, train_loss: 0.1851, step time: 0.1009\n",
      "119/223, train_loss: 0.2124, step time: 0.1001\n",
      "120/223, train_loss: 0.2119, step time: 0.1003\n",
      "121/223, train_loss: 0.2033, step time: 0.1005\n",
      "122/223, train_loss: 0.2120, step time: 0.1058\n",
      "123/223, train_loss: 0.2000, step time: 0.0999\n",
      "124/223, train_loss: 0.1930, step time: 0.1006\n",
      "125/223, train_loss: 0.1883, step time: 0.1019\n",
      "126/223, train_loss: 0.2125, step time: 0.1003\n",
      "127/223, train_loss: 0.1859, step time: 0.1008\n",
      "128/223, train_loss: 0.2074, step time: 0.1002\n",
      "129/223, train_loss: 0.1847, step time: 0.1086\n",
      "130/223, train_loss: 0.1935, step time: 0.1000\n",
      "131/223, train_loss: 0.2120, step time: 0.1007\n",
      "132/223, train_loss: 0.2081, step time: 0.0997\n",
      "133/223, train_loss: 0.2054, step time: 0.1077\n",
      "134/223, train_loss: 0.2003, step time: 0.1001\n",
      "135/223, train_loss: 0.1988, step time: 0.1064\n",
      "136/223, train_loss: 0.1940, step time: 0.1102\n",
      "137/223, train_loss: 0.1914, step time: 0.0988\n",
      "138/223, train_loss: 0.2106, step time: 0.1159\n",
      "139/223, train_loss: 0.1914, step time: 0.0998\n",
      "140/223, train_loss: 0.2242, step time: 0.0999\n",
      "141/223, train_loss: 0.1937, step time: 0.1132\n",
      "142/223, train_loss: 0.1926, step time: 0.0988\n",
      "143/223, train_loss: 0.2033, step time: 0.1055\n",
      "144/223, train_loss: 0.2309, step time: 0.1003\n",
      "145/223, train_loss: 0.1988, step time: 0.1007\n",
      "146/223, train_loss: 0.2006, step time: 0.1050\n",
      "147/223, train_loss: 0.1978, step time: 0.1001\n",
      "148/223, train_loss: 0.1962, step time: 0.1409\n",
      "149/223, train_loss: 0.1976, step time: 0.1151\n",
      "150/223, train_loss: 0.1906, step time: 0.1103\n",
      "151/223, train_loss: 0.2016, step time: 0.1001\n",
      "152/223, train_loss: 0.1855, step time: 0.0999\n",
      "153/223, train_loss: 0.1991, step time: 0.1019\n",
      "154/223, train_loss: 0.1893, step time: 0.1050\n",
      "155/223, train_loss: 0.1965, step time: 0.1054\n",
      "156/223, train_loss: 0.1992, step time: 0.1089\n",
      "157/223, train_loss: 0.1933, step time: 0.1114\n",
      "158/223, train_loss: 0.1941, step time: 0.1035\n",
      "159/223, train_loss: 0.1881, step time: 0.1004\n",
      "160/223, train_loss: 0.1809, step time: 0.1038\n",
      "161/223, train_loss: 0.1934, step time: 0.1005\n",
      "162/223, train_loss: 0.1938, step time: 0.1182\n",
      "163/223, train_loss: 0.1905, step time: 0.1049\n",
      "164/223, train_loss: 0.1958, step time: 0.1014\n",
      "165/223, train_loss: 0.2047, step time: 0.1075\n",
      "166/223, train_loss: 0.2089, step time: 0.1157\n",
      "167/223, train_loss: 0.2027, step time: 0.1002\n",
      "168/223, train_loss: 0.1930, step time: 0.1007\n",
      "169/223, train_loss: 0.2086, step time: 0.1209\n",
      "170/223, train_loss: 0.1866, step time: 0.1056\n",
      "171/223, train_loss: 0.2000, step time: 0.1229\n",
      "172/223, train_loss: 0.2026, step time: 0.1104\n",
      "173/223, train_loss: 0.2097, step time: 0.1010\n",
      "174/223, train_loss: 0.2018, step time: 0.1002\n",
      "175/223, train_loss: 0.2145, step time: 0.0997\n",
      "176/223, train_loss: 0.2008, step time: 0.1020\n",
      "177/223, train_loss: 0.2160, step time: 0.1048\n",
      "178/223, train_loss: 0.2136, step time: 0.1070\n",
      "179/223, train_loss: 0.2051, step time: 0.1082\n",
      "180/223, train_loss: 0.1840, step time: 0.1369\n",
      "181/223, train_loss: 0.2090, step time: 0.1141\n",
      "182/223, train_loss: 0.1805, step time: 0.1034\n",
      "183/223, train_loss: 0.3584, step time: 0.1074\n",
      "184/223, train_loss: 0.2018, step time: 0.1002\n",
      "185/223, train_loss: 0.2025, step time: 0.1179\n",
      "186/223, train_loss: 0.2129, step time: 0.1162\n",
      "187/223, train_loss: 0.2159, step time: 0.1081\n",
      "188/223, train_loss: 0.2016, step time: 0.1085\n",
      "189/223, train_loss: 0.2062, step time: 0.1222\n",
      "190/223, train_loss: 0.2004, step time: 0.1143\n",
      "191/223, train_loss: 0.1997, step time: 0.1114\n",
      "192/223, train_loss: 0.2005, step time: 0.1217\n",
      "193/223, train_loss: 0.2115, step time: 0.1001\n",
      "194/223, train_loss: 0.2036, step time: 0.1173\n",
      "195/223, train_loss: 0.1990, step time: 0.0999\n",
      "196/223, train_loss: 0.2090, step time: 0.0997\n",
      "197/223, train_loss: 0.1898, step time: 0.1085\n",
      "198/223, train_loss: 0.1946, step time: 0.1052\n",
      "199/223, train_loss: 0.1971, step time: 0.0995\n",
      "200/223, train_loss: 0.1900, step time: 0.0998\n",
      "201/223, train_loss: 0.2172, step time: 0.1034\n",
      "202/223, train_loss: 0.2076, step time: 0.1109\n",
      "203/223, train_loss: 0.1912, step time: 0.1095\n",
      "204/223, train_loss: 0.1932, step time: 0.1004\n",
      "205/223, train_loss: 0.1978, step time: 0.1046\n",
      "206/223, train_loss: 0.2168, step time: 0.1032\n",
      "207/223, train_loss: 0.2005, step time: 0.1012\n",
      "208/223, train_loss: 0.1974, step time: 0.1240\n",
      "209/223, train_loss: 0.2002, step time: 0.1004\n",
      "210/223, train_loss: 0.2082, step time: 0.1158\n",
      "211/223, train_loss: 0.2030, step time: 0.1027\n",
      "212/223, train_loss: 0.2013, step time: 0.1007\n",
      "213/223, train_loss: 0.1978, step time: 0.1082\n",
      "214/223, train_loss: 0.1914, step time: 0.1012\n",
      "215/223, train_loss: 0.2049, step time: 0.1122\n",
      "216/223, train_loss: 0.1940, step time: 0.1038\n",
      "217/223, train_loss: 0.1970, step time: 0.1003\n",
      "218/223, train_loss: 0.2016, step time: 0.0992\n",
      "219/223, train_loss: 0.1867, step time: 0.1003\n",
      "220/223, train_loss: 0.2036, step time: 0.0992\n",
      "221/223, train_loss: 0.1954, step time: 0.0998\n",
      "222/223, train_loss: 0.1988, step time: 0.0997\n",
      "223/223, train_loss: 0.2026, step time: 0.0997\n",
      "epoch 14 average loss: 0.2019\n",
      "time consuming of epoch 14 is: 88.3366\n",
      "----------\n",
      "epoch 15/300\n",
      "1/223, train_loss: 0.1954, step time: 0.1009\n",
      "2/223, train_loss: 0.2027, step time: 0.1003\n",
      "3/223, train_loss: 0.2107, step time: 0.1000\n",
      "4/223, train_loss: 0.1952, step time: 0.1099\n",
      "5/223, train_loss: 0.1981, step time: 0.1006\n",
      "6/223, train_loss: 0.1911, step time: 0.1109\n",
      "7/223, train_loss: 0.2062, step time: 0.1160\n",
      "8/223, train_loss: 0.1802, step time: 0.1150\n",
      "9/223, train_loss: 0.1817, step time: 0.1085\n",
      "10/223, train_loss: 0.1835, step time: 0.1002\n",
      "11/223, train_loss: 0.2085, step time: 0.1013\n",
      "12/223, train_loss: 0.2011, step time: 0.0997\n",
      "13/223, train_loss: 0.1991, step time: 0.1124\n",
      "14/223, train_loss: 0.2000, step time: 0.1079\n",
      "15/223, train_loss: 0.2096, step time: 0.0998\n",
      "16/223, train_loss: 0.1876, step time: 0.1007\n",
      "17/223, train_loss: 0.1853, step time: 0.1145\n",
      "18/223, train_loss: 0.2097, step time: 0.1072\n",
      "19/223, train_loss: 0.2137, step time: 0.1070\n",
      "20/223, train_loss: 0.2016, step time: 0.1004\n",
      "21/223, train_loss: 0.2132, step time: 0.1141\n",
      "22/223, train_loss: 0.1798, step time: 0.1010\n",
      "23/223, train_loss: 0.1899, step time: 0.1001\n",
      "24/223, train_loss: 0.2193, step time: 0.0999\n",
      "25/223, train_loss: 0.1924, step time: 0.1138\n",
      "26/223, train_loss: 0.2071, step time: 0.1006\n",
      "27/223, train_loss: 0.2005, step time: 0.1005\n",
      "28/223, train_loss: 0.2005, step time: 0.1004\n",
      "29/223, train_loss: 0.1970, step time: 0.1056\n",
      "30/223, train_loss: 0.1871, step time: 0.1048\n",
      "31/223, train_loss: 0.1949, step time: 0.1197\n",
      "32/223, train_loss: 0.1954, step time: 0.1266\n",
      "33/223, train_loss: 0.2030, step time: 0.1112\n",
      "34/223, train_loss: 0.1961, step time: 0.1130\n",
      "35/223, train_loss: 0.1873, step time: 0.1180\n",
      "36/223, train_loss: 0.1957, step time: 0.1126\n",
      "37/223, train_loss: 0.1764, step time: 0.1002\n",
      "38/223, train_loss: 0.2112, step time: 0.1131\n",
      "39/223, train_loss: 0.1804, step time: 0.1115\n",
      "40/223, train_loss: 0.1984, step time: 0.1048\n",
      "41/223, train_loss: 0.1781, step time: 0.0998\n",
      "42/223, train_loss: 0.1785, step time: 0.1012\n",
      "43/223, train_loss: 0.2044, step time: 0.1058\n",
      "44/223, train_loss: 0.1703, step time: 0.1005\n",
      "45/223, train_loss: 0.1861, step time: 0.1008\n",
      "46/223, train_loss: 0.2036, step time: 0.1117\n",
      "47/223, train_loss: 0.1839, step time: 0.1110\n",
      "48/223, train_loss: 0.2058, step time: 0.1215\n",
      "49/223, train_loss: 0.2272, step time: 0.1011\n",
      "50/223, train_loss: 0.2080, step time: 0.1179\n",
      "51/223, train_loss: 0.1935, step time: 0.1028\n",
      "52/223, train_loss: 0.1817, step time: 0.1041\n",
      "53/223, train_loss: 0.1856, step time: 0.1192\n",
      "54/223, train_loss: 0.1854, step time: 0.1131\n",
      "55/223, train_loss: 0.1904, step time: 0.0998\n",
      "56/223, train_loss: 0.2159, step time: 0.1013\n",
      "57/223, train_loss: 0.1957, step time: 0.1006\n",
      "58/223, train_loss: 0.1996, step time: 0.1055\n",
      "59/223, train_loss: 0.1925, step time: 0.1021\n",
      "60/223, train_loss: 0.1986, step time: 0.1093\n",
      "61/223, train_loss: 0.1941, step time: 0.1146\n",
      "62/223, train_loss: 0.1926, step time: 0.1103\n",
      "63/223, train_loss: 0.2112, step time: 0.1208\n",
      "64/223, train_loss: 0.2176, step time: 0.1244\n",
      "65/223, train_loss: 0.1974, step time: 0.1175\n",
      "66/223, train_loss: 0.1873, step time: 0.1133\n",
      "67/223, train_loss: 0.1882, step time: 0.1003\n",
      "68/223, train_loss: 0.1869, step time: 0.1007\n",
      "69/223, train_loss: 0.1876, step time: 0.1054\n",
      "70/223, train_loss: 0.1972, step time: 0.1237\n",
      "71/223, train_loss: 0.1875, step time: 0.1229\n",
      "72/223, train_loss: 0.1936, step time: 0.1180\n",
      "73/223, train_loss: 0.1839, step time: 0.1000\n",
      "74/223, train_loss: 0.1885, step time: 0.0993\n",
      "75/223, train_loss: 0.2031, step time: 0.1006\n",
      "76/223, train_loss: 0.1869, step time: 0.0993\n",
      "77/223, train_loss: 0.1877, step time: 0.1089\n",
      "78/223, train_loss: 0.1833, step time: 0.1150\n",
      "79/223, train_loss: 0.1890, step time: 0.1342\n",
      "80/223, train_loss: 0.1885, step time: 0.1196\n",
      "81/223, train_loss: 0.1933, step time: 0.1004\n",
      "82/223, train_loss: 0.2047, step time: 0.1022\n",
      "83/223, train_loss: 0.1947, step time: 0.1005\n",
      "84/223, train_loss: 0.1912, step time: 0.1004\n",
      "85/223, train_loss: 0.1727, step time: 0.1050\n",
      "86/223, train_loss: 0.2091, step time: 0.1000\n",
      "87/223, train_loss: 0.1754, step time: 0.0993\n",
      "88/223, train_loss: 0.2023, step time: 0.1142\n",
      "89/223, train_loss: 0.1883, step time: 0.1286\n",
      "90/223, train_loss: 0.2005, step time: 0.1098\n",
      "91/223, train_loss: 0.1829, step time: 0.1011\n",
      "92/223, train_loss: 0.1757, step time: 0.1009\n",
      "93/223, train_loss: 0.2096, step time: 0.1069\n",
      "94/223, train_loss: 0.2100, step time: 0.1205\n",
      "95/223, train_loss: 0.1966, step time: 0.1172\n",
      "96/223, train_loss: 0.1866, step time: 0.1133\n",
      "97/223, train_loss: 0.1863, step time: 0.1007\n",
      "98/223, train_loss: 0.1773, step time: 0.1001\n",
      "99/223, train_loss: 0.2022, step time: 0.1075\n",
      "100/223, train_loss: 0.2110, step time: 0.1242\n",
      "101/223, train_loss: 0.1848, step time: 0.1133\n",
      "102/223, train_loss: 0.1903, step time: 0.1121\n",
      "103/223, train_loss: 0.1843, step time: 0.1113\n",
      "104/223, train_loss: 0.1832, step time: 0.1223\n",
      "105/223, train_loss: 0.1932, step time: 0.1242\n",
      "106/223, train_loss: 0.1765, step time: 0.1092\n",
      "107/223, train_loss: 0.1910, step time: 0.1322\n",
      "108/223, train_loss: 0.1851, step time: 0.1291\n",
      "109/223, train_loss: 0.2201, step time: 0.1073\n",
      "110/223, train_loss: 0.2048, step time: 0.1161\n",
      "111/223, train_loss: 0.1775, step time: 0.1226\n",
      "112/223, train_loss: 0.1997, step time: 0.1095\n",
      "113/223, train_loss: 0.1834, step time: 0.1112\n",
      "114/223, train_loss: 0.1703, step time: 0.1124\n",
      "115/223, train_loss: 0.1921, step time: 0.1007\n",
      "116/223, train_loss: 0.1860, step time: 0.1002\n",
      "117/223, train_loss: 0.1730, step time: 0.1023\n",
      "118/223, train_loss: 0.1766, step time: 0.1248\n",
      "119/223, train_loss: 0.2057, step time: 0.1089\n",
      "120/223, train_loss: 0.1896, step time: 0.1103\n",
      "121/223, train_loss: 0.1800, step time: 0.0998\n",
      "122/223, train_loss: 0.1992, step time: 0.1091\n",
      "123/223, train_loss: 0.1747, step time: 0.1045\n",
      "124/223, train_loss: 0.1938, step time: 0.1003\n",
      "125/223, train_loss: 0.1965, step time: 0.1003\n",
      "126/223, train_loss: 0.1968, step time: 0.1094\n",
      "127/223, train_loss: 0.1902, step time: 0.1228\n",
      "128/223, train_loss: 0.1941, step time: 0.0995\n",
      "129/223, train_loss: 0.1803, step time: 0.1091\n",
      "130/223, train_loss: 0.1869, step time: 0.1088\n",
      "131/223, train_loss: 0.1913, step time: 0.1002\n",
      "132/223, train_loss: 0.1992, step time: 0.1162\n",
      "133/223, train_loss: 0.2045, step time: 0.1003\n",
      "134/223, train_loss: 0.2028, step time: 0.1335\n",
      "135/223, train_loss: 0.1876, step time: 0.1001\n",
      "136/223, train_loss: 0.1970, step time: 0.1002\n",
      "137/223, train_loss: 0.1918, step time: 0.1052\n",
      "138/223, train_loss: 0.1763, step time: 0.1312\n",
      "139/223, train_loss: 0.1923, step time: 0.1232\n",
      "140/223, train_loss: 0.2041, step time: 0.1101\n",
      "141/223, train_loss: 0.1864, step time: 0.1061\n",
      "142/223, train_loss: 0.1772, step time: 0.1002\n",
      "143/223, train_loss: 0.2082, step time: 0.1158\n",
      "144/223, train_loss: 0.1969, step time: 0.1006\n",
      "145/223, train_loss: 0.1937, step time: 0.1008\n",
      "146/223, train_loss: 0.1930, step time: 0.1190\n",
      "147/223, train_loss: 0.1805, step time: 0.1132\n",
      "148/223, train_loss: 0.1807, step time: 0.1007\n",
      "149/223, train_loss: 0.1856, step time: 0.1002\n",
      "150/223, train_loss: 0.1999, step time: 0.1000\n",
      "151/223, train_loss: 0.1941, step time: 0.1001\n",
      "152/223, train_loss: 0.1959, step time: 0.1014\n",
      "153/223, train_loss: 0.2026, step time: 0.1006\n",
      "154/223, train_loss: 0.2211, step time: 0.1238\n",
      "155/223, train_loss: 0.1865, step time: 0.1083\n",
      "156/223, train_loss: 0.1937, step time: 0.1002\n",
      "157/223, train_loss: 0.1821, step time: 0.1122\n",
      "158/223, train_loss: 0.1724, step time: 0.1094\n",
      "159/223, train_loss: 0.1900, step time: 0.1006\n",
      "160/223, train_loss: 0.1759, step time: 0.1183\n",
      "161/223, train_loss: 0.1805, step time: 0.1052\n",
      "162/223, train_loss: 0.1847, step time: 0.1004\n",
      "163/223, train_loss: 0.1952, step time: 0.1114\n",
      "164/223, train_loss: 0.1982, step time: 0.1026\n",
      "165/223, train_loss: 0.1907, step time: 0.1009\n",
      "166/223, train_loss: 0.1841, step time: 0.1355\n",
      "167/223, train_loss: 0.1977, step time: 0.1047\n",
      "168/223, train_loss: 0.2029, step time: 0.1001\n",
      "169/223, train_loss: 0.2035, step time: 0.1011\n",
      "170/223, train_loss: 0.1801, step time: 0.1049\n",
      "171/223, train_loss: 0.1781, step time: 0.1156\n",
      "172/223, train_loss: 0.2056, step time: 0.1130\n",
      "173/223, train_loss: 0.1917, step time: 0.1099\n",
      "174/223, train_loss: 0.1857, step time: 0.1030\n",
      "175/223, train_loss: 0.2007, step time: 0.1168\n",
      "176/223, train_loss: 0.2026, step time: 0.1249\n",
      "177/223, train_loss: 0.2017, step time: 0.1019\n",
      "178/223, train_loss: 0.2007, step time: 0.1033\n",
      "179/223, train_loss: 0.1931, step time: 0.1196\n",
      "180/223, train_loss: 0.1870, step time: 0.1170\n",
      "181/223, train_loss: 0.1875, step time: 0.1244\n",
      "182/223, train_loss: 0.1779, step time: 0.1154\n",
      "183/223, train_loss: 0.1968, step time: 0.0997\n",
      "184/223, train_loss: 0.2019, step time: 0.1030\n",
      "185/223, train_loss: 0.2026, step time: 0.1032\n",
      "186/223, train_loss: 0.1761, step time: 0.1165\n",
      "187/223, train_loss: 0.1879, step time: 0.1101\n",
      "188/223, train_loss: 0.1978, step time: 0.1012\n",
      "189/223, train_loss: 0.1766, step time: 0.1179\n",
      "190/223, train_loss: 0.1955, step time: 0.0998\n",
      "191/223, train_loss: 0.1912, step time: 0.1005\n",
      "192/223, train_loss: 0.1897, step time: 0.1004\n",
      "193/223, train_loss: 0.1911, step time: 0.1040\n",
      "194/223, train_loss: 0.1975, step time: 0.1232\n",
      "195/223, train_loss: 0.1816, step time: 0.1120\n",
      "196/223, train_loss: 0.1939, step time: 0.1018\n",
      "197/223, train_loss: 0.1964, step time: 0.1004\n",
      "198/223, train_loss: 0.1887, step time: 0.1123\n",
      "199/223, train_loss: 0.1886, step time: 0.1341\n",
      "200/223, train_loss: 0.2042, step time: 0.1183\n",
      "201/223, train_loss: 0.1778, step time: 0.1222\n",
      "202/223, train_loss: 0.1781, step time: 0.1108\n",
      "203/223, train_loss: 0.2010, step time: 0.1128\n",
      "204/223, train_loss: 0.1891, step time: 0.1408\n",
      "205/223, train_loss: 0.1981, step time: 0.1002\n",
      "206/223, train_loss: 0.3587, step time: 0.1093\n",
      "207/223, train_loss: 0.1956, step time: 0.1115\n",
      "208/223, train_loss: 0.1827, step time: 0.1195\n",
      "209/223, train_loss: 0.1862, step time: 0.1076\n",
      "210/223, train_loss: 0.1950, step time: 0.1150\n",
      "211/223, train_loss: 0.1996, step time: 0.1118\n",
      "212/223, train_loss: 0.1792, step time: 0.1018\n",
      "213/223, train_loss: 0.2116, step time: 0.0999\n",
      "214/223, train_loss: 0.1945, step time: 0.1154\n",
      "215/223, train_loss: 0.1933, step time: 0.1000\n",
      "216/223, train_loss: 0.1836, step time: 0.1010\n",
      "217/223, train_loss: 0.1822, step time: 0.1091\n",
      "218/223, train_loss: 0.1807, step time: 0.0992\n",
      "219/223, train_loss: 0.1835, step time: 0.0996\n",
      "220/223, train_loss: 0.1859, step time: 0.0994\n",
      "221/223, train_loss: 0.1799, step time: 0.1039\n",
      "222/223, train_loss: 0.1755, step time: 0.0992\n",
      "223/223, train_loss: 0.1841, step time: 0.0996\n",
      "epoch 15 average loss: 0.1933\n",
      "saved new best metric model\n",
      "current epoch: 15 current mean dice: 0.7756 tc: 0.8765 wt: 0.7903 et: 0.6599\n",
      "best mean dice: 0.7756 at epoch: 15\n",
      "time consuming of epoch 15 is: 90.5757\n",
      "----------\n",
      "epoch 16/300\n",
      "1/223, train_loss: 0.1803, step time: 0.1015\n",
      "2/223, train_loss: 0.1849, step time: 0.1179\n",
      "3/223, train_loss: 0.1858, step time: 0.1262\n",
      "4/223, train_loss: 0.1804, step time: 0.1063\n",
      "5/223, train_loss: 0.1917, step time: 0.1006\n",
      "6/223, train_loss: 0.2119, step time: 0.1141\n",
      "7/223, train_loss: 0.1904, step time: 0.1003\n",
      "8/223, train_loss: 0.1833, step time: 0.1007\n",
      "9/223, train_loss: 0.1858, step time: 0.1130\n",
      "10/223, train_loss: 0.1678, step time: 0.1109\n",
      "11/223, train_loss: 0.3533, step time: 0.1022\n",
      "12/223, train_loss: 0.1956, step time: 0.1104\n",
      "13/223, train_loss: 0.1938, step time: 0.1080\n",
      "14/223, train_loss: 0.1848, step time: 0.1097\n",
      "15/223, train_loss: 0.1848, step time: 0.1135\n",
      "16/223, train_loss: 0.1993, step time: 0.1338\n",
      "17/223, train_loss: 0.1849, step time: 0.1002\n",
      "18/223, train_loss: 0.1734, step time: 0.1003\n",
      "19/223, train_loss: 0.1828, step time: 0.1004\n",
      "20/223, train_loss: 0.2050, step time: 0.1001\n",
      "21/223, train_loss: 0.1837, step time: 0.1040\n",
      "22/223, train_loss: 0.1877, step time: 0.1135\n",
      "23/223, train_loss: 0.1920, step time: 0.1040\n",
      "24/223, train_loss: 0.2016, step time: 0.0995\n",
      "25/223, train_loss: 0.1789, step time: 0.1184\n",
      "26/223, train_loss: 0.1843, step time: 0.1307\n",
      "27/223, train_loss: 0.1734, step time: 0.1074\n",
      "28/223, train_loss: 0.1858, step time: 0.1154\n",
      "29/223, train_loss: 0.1840, step time: 0.1085\n",
      "30/223, train_loss: 0.1862, step time: 0.1222\n",
      "31/223, train_loss: 0.1958, step time: 0.1054\n",
      "32/223, train_loss: 0.2018, step time: 0.1170\n",
      "33/223, train_loss: 0.1784, step time: 0.1214\n",
      "34/223, train_loss: 0.1743, step time: 0.1227\n",
      "35/223, train_loss: 0.1780, step time: 0.1237\n",
      "36/223, train_loss: 0.1864, step time: 0.1045\n",
      "37/223, train_loss: 0.1953, step time: 0.1083\n",
      "38/223, train_loss: 0.2016, step time: 0.1141\n",
      "39/223, train_loss: 0.1889, step time: 0.1056\n",
      "40/223, train_loss: 0.2083, step time: 0.1005\n",
      "41/223, train_loss: 0.1864, step time: 0.1050\n",
      "42/223, train_loss: 0.1841, step time: 0.1440\n",
      "43/223, train_loss: 0.1806, step time: 0.1059\n",
      "44/223, train_loss: 0.1777, step time: 0.1105\n",
      "45/223, train_loss: 0.1870, step time: 0.1059\n",
      "46/223, train_loss: 0.1843, step time: 0.1003\n",
      "47/223, train_loss: 0.1828, step time: 0.1004\n",
      "48/223, train_loss: 0.2019, step time: 0.1013\n",
      "49/223, train_loss: 0.1894, step time: 0.1002\n",
      "50/223, train_loss: 0.1933, step time: 0.1401\n",
      "51/223, train_loss: 0.1776, step time: 0.1006\n",
      "52/223, train_loss: 0.1754, step time: 0.1014\n",
      "53/223, train_loss: 0.1724, step time: 0.1063\n",
      "54/223, train_loss: 0.1876, step time: 0.1005\n",
      "55/223, train_loss: 0.1882, step time: 0.1337\n",
      "56/223, train_loss: 0.1956, step time: 0.1091\n",
      "57/223, train_loss: 0.1907, step time: 0.1001\n",
      "58/223, train_loss: 0.2026, step time: 0.1100\n",
      "59/223, train_loss: 0.1948, step time: 0.1185\n",
      "60/223, train_loss: 0.1818, step time: 0.1355\n",
      "61/223, train_loss: 0.1622, step time: 0.1011\n",
      "62/223, train_loss: 0.1966, step time: 0.1005\n",
      "63/223, train_loss: 0.2077, step time: 0.1076\n",
      "64/223, train_loss: 0.1844, step time: 0.1001\n",
      "65/223, train_loss: 0.1935, step time: 0.1111\n",
      "66/223, train_loss: 0.1927, step time: 0.1256\n",
      "67/223, train_loss: 0.1927, step time: 0.1047\n",
      "68/223, train_loss: 0.1844, step time: 0.0998\n",
      "69/223, train_loss: 0.1757, step time: 0.1045\n",
      "70/223, train_loss: 0.1863, step time: 0.1167\n",
      "71/223, train_loss: 0.1868, step time: 0.1013\n",
      "72/223, train_loss: 0.1961, step time: 0.1099\n",
      "73/223, train_loss: 0.1824, step time: 0.1115\n",
      "74/223, train_loss: 0.1894, step time: 0.1068\n",
      "75/223, train_loss: 0.1804, step time: 0.1015\n",
      "76/223, train_loss: 0.2002, step time: 0.1070\n",
      "77/223, train_loss: 0.1841, step time: 0.1368\n",
      "78/223, train_loss: 0.1799, step time: 0.1348\n",
      "79/223, train_loss: 0.1811, step time: 0.0997\n",
      "80/223, train_loss: 0.1726, step time: 0.1000\n",
      "81/223, train_loss: 0.1811, step time: 0.1154\n",
      "82/223, train_loss: 0.1869, step time: 0.1247\n",
      "83/223, train_loss: 0.1773, step time: 0.1189\n",
      "84/223, train_loss: 0.2235, step time: 0.1002\n",
      "85/223, train_loss: 0.1926, step time: 0.1334\n",
      "86/223, train_loss: 0.1996, step time: 0.1006\n",
      "87/223, train_loss: 0.1785, step time: 0.1030\n",
      "88/223, train_loss: 0.1794, step time: 0.1000\n",
      "89/223, train_loss: 0.1825, step time: 0.1100\n",
      "90/223, train_loss: 0.1833, step time: 0.1076\n",
      "91/223, train_loss: 0.1787, step time: 0.1099\n",
      "92/223, train_loss: 0.1872, step time: 0.1010\n",
      "93/223, train_loss: 0.1702, step time: 0.1147\n",
      "94/223, train_loss: 0.1736, step time: 0.0998\n",
      "95/223, train_loss: 0.1823, step time: 0.0998\n",
      "96/223, train_loss: 0.1834, step time: 0.1170\n",
      "97/223, train_loss: 0.1894, step time: 0.1380\n",
      "98/223, train_loss: 0.2024, step time: 0.1022\n",
      "99/223, train_loss: 0.1933, step time: 0.1067\n",
      "100/223, train_loss: 0.1928, step time: 0.1020\n",
      "101/223, train_loss: 0.1765, step time: 0.1133\n",
      "102/223, train_loss: 0.1812, step time: 0.1000\n",
      "103/223, train_loss: 0.1905, step time: 0.1091\n",
      "104/223, train_loss: 0.1949, step time: 0.1002\n",
      "105/223, train_loss: 0.1675, step time: 0.1128\n",
      "106/223, train_loss: 0.1946, step time: 0.1098\n",
      "107/223, train_loss: 0.1729, step time: 0.1280\n",
      "108/223, train_loss: 0.1801, step time: 0.1100\n",
      "109/223, train_loss: 0.1870, step time: 0.1007\n",
      "110/223, train_loss: 0.1830, step time: 0.1002\n",
      "111/223, train_loss: 0.1761, step time: 0.1009\n",
      "112/223, train_loss: 0.1849, step time: 0.1003\n",
      "113/223, train_loss: 0.1750, step time: 0.1160\n",
      "114/223, train_loss: 0.1724, step time: 0.1143\n",
      "115/223, train_loss: 0.2006, step time: 0.1122\n",
      "116/223, train_loss: 0.1699, step time: 0.1043\n",
      "117/223, train_loss: 0.1721, step time: 0.1037\n",
      "118/223, train_loss: 0.1966, step time: 0.0995\n",
      "119/223, train_loss: 0.1851, step time: 0.1220\n",
      "120/223, train_loss: 0.1944, step time: 0.1032\n",
      "121/223, train_loss: 0.1741, step time: 0.1151\n",
      "122/223, train_loss: 0.2080, step time: 0.1089\n",
      "123/223, train_loss: 0.1801, step time: 0.1043\n",
      "124/223, train_loss: 0.1730, step time: 0.0998\n",
      "125/223, train_loss: 0.1794, step time: 0.1120\n",
      "126/223, train_loss: 0.1692, step time: 0.0999\n",
      "127/223, train_loss: 0.1856, step time: 0.0998\n",
      "128/223, train_loss: 0.2025, step time: 0.1070\n",
      "129/223, train_loss: 0.1898, step time: 0.1139\n",
      "130/223, train_loss: 0.1867, step time: 0.1206\n",
      "131/223, train_loss: 0.1796, step time: 0.1150\n",
      "132/223, train_loss: 0.1814, step time: 0.1006\n",
      "133/223, train_loss: 0.1949, step time: 0.0987\n",
      "134/223, train_loss: 0.1904, step time: 0.0992\n",
      "135/223, train_loss: 0.1934, step time: 0.0995\n",
      "136/223, train_loss: 0.1951, step time: 0.1033\n",
      "137/223, train_loss: 0.1937, step time: 0.0992\n",
      "138/223, train_loss: 0.1824, step time: 0.0988\n",
      "139/223, train_loss: 0.1679, step time: 0.1002\n",
      "140/223, train_loss: 0.1776, step time: 0.1005\n",
      "141/223, train_loss: 0.1677, step time: 0.1004\n",
      "142/223, train_loss: 0.1908, step time: 0.0997\n",
      "143/223, train_loss: 0.1891, step time: 0.1005\n",
      "144/223, train_loss: 0.1768, step time: 0.1015\n",
      "145/223, train_loss: 0.1822, step time: 0.1081\n",
      "146/223, train_loss: 0.1974, step time: 0.1004\n",
      "147/223, train_loss: 0.1879, step time: 0.1006\n",
      "148/223, train_loss: 0.1764, step time: 0.1394\n",
      "149/223, train_loss: 0.1862, step time: 0.1088\n",
      "150/223, train_loss: 0.1919, step time: 0.1026\n",
      "151/223, train_loss: 0.1627, step time: 0.0998\n",
      "152/223, train_loss: 0.1873, step time: 0.1041\n",
      "153/223, train_loss: 0.1854, step time: 0.1303\n",
      "154/223, train_loss: 0.1823, step time: 0.1005\n",
      "155/223, train_loss: 0.1936, step time: 0.1000\n",
      "156/223, train_loss: 0.1917, step time: 0.1091\n",
      "157/223, train_loss: 0.1840, step time: 0.1013\n",
      "158/223, train_loss: 0.1873, step time: 0.1075\n",
      "159/223, train_loss: 0.1796, step time: 0.0995\n",
      "160/223, train_loss: 0.1905, step time: 0.1149\n",
      "161/223, train_loss: 0.1722, step time: 0.1002\n",
      "162/223, train_loss: 0.1790, step time: 0.1066\n",
      "163/223, train_loss: 0.1875, step time: 0.1004\n",
      "164/223, train_loss: 0.1723, step time: 0.1000\n",
      "165/223, train_loss: 0.1741, step time: 0.0995\n",
      "166/223, train_loss: 0.1866, step time: 0.0997\n",
      "167/223, train_loss: 0.1781, step time: 0.1136\n",
      "168/223, train_loss: 0.1852, step time: 0.1196\n",
      "169/223, train_loss: 0.1858, step time: 0.1103\n",
      "170/223, train_loss: 0.1899, step time: 0.0998\n",
      "171/223, train_loss: 0.1987, step time: 0.1214\n",
      "172/223, train_loss: 0.1986, step time: 0.1014\n",
      "173/223, train_loss: 0.1915, step time: 0.1092\n",
      "174/223, train_loss: 0.1781, step time: 0.1006\n",
      "175/223, train_loss: 0.1811, step time: 0.1050\n",
      "176/223, train_loss: 0.1934, step time: 0.1151\n",
      "177/223, train_loss: 0.1889, step time: 0.0994\n",
      "178/223, train_loss: 0.1855, step time: 0.0997\n",
      "179/223, train_loss: 0.1824, step time: 0.0998\n",
      "180/223, train_loss: 0.1792, step time: 0.1049\n",
      "181/223, train_loss: 0.1989, step time: 0.1130\n",
      "182/223, train_loss: 0.1682, step time: 0.1002\n",
      "183/223, train_loss: 0.1839, step time: 0.0997\n",
      "184/223, train_loss: 0.1853, step time: 0.1050\n",
      "185/223, train_loss: 0.1926, step time: 0.1004\n",
      "186/223, train_loss: 0.2054, step time: 0.1083\n",
      "187/223, train_loss: 0.1980, step time: 0.1050\n",
      "188/223, train_loss: 0.1753, step time: 0.1093\n",
      "189/223, train_loss: 0.1875, step time: 0.1171\n",
      "190/223, train_loss: 0.1975, step time: 0.1174\n",
      "191/223, train_loss: 0.1892, step time: 0.1060\n",
      "192/223, train_loss: 0.1746, step time: 0.1272\n",
      "193/223, train_loss: 0.1944, step time: 0.1084\n",
      "194/223, train_loss: 0.1795, step time: 0.1007\n",
      "195/223, train_loss: 0.1859, step time: 0.1017\n",
      "196/223, train_loss: 0.1605, step time: 0.1012\n",
      "197/223, train_loss: 0.1966, step time: 0.0995\n",
      "198/223, train_loss: 0.1756, step time: 0.1011\n",
      "199/223, train_loss: 0.2001, step time: 0.1004\n",
      "200/223, train_loss: 0.1718, step time: 0.1019\n",
      "201/223, train_loss: 0.1933, step time: 0.1000\n",
      "202/223, train_loss: 0.1666, step time: 0.1036\n",
      "203/223, train_loss: 0.1932, step time: 0.1146\n",
      "204/223, train_loss: 0.1943, step time: 0.1184\n",
      "205/223, train_loss: 0.1758, step time: 0.1062\n",
      "206/223, train_loss: 0.1879, step time: 0.1241\n",
      "207/223, train_loss: 0.1707, step time: 0.1135\n",
      "208/223, train_loss: 0.1716, step time: 0.1132\n",
      "209/223, train_loss: 0.1773, step time: 0.0992\n",
      "210/223, train_loss: 0.2019, step time: 0.1022\n",
      "211/223, train_loss: 0.1758, step time: 0.1096\n",
      "212/223, train_loss: 0.1738, step time: 0.1073\n",
      "213/223, train_loss: 0.1795, step time: 0.1220\n",
      "214/223, train_loss: 0.1876, step time: 0.1118\n",
      "215/223, train_loss: 0.1993, step time: 0.1015\n",
      "216/223, train_loss: 0.1977, step time: 0.0999\n",
      "217/223, train_loss: 0.1744, step time: 0.1081\n",
      "218/223, train_loss: 0.2030, step time: 0.1114\n",
      "219/223, train_loss: 0.1743, step time: 0.1016\n",
      "220/223, train_loss: 0.1860, step time: 0.0994\n",
      "221/223, train_loss: 0.1957, step time: 0.0996\n",
      "222/223, train_loss: 0.1756, step time: 0.0995\n",
      "223/223, train_loss: 0.1758, step time: 0.1000\n",
      "epoch 16 average loss: 0.1865\n",
      "time consuming of epoch 16 is: 88.4039\n",
      "----------\n",
      "epoch 17/300\n",
      "1/223, train_loss: 0.2024, step time: 0.1015\n",
      "2/223, train_loss: 0.1711, step time: 0.0993\n",
      "3/223, train_loss: 0.1773, step time: 0.1002\n",
      "4/223, train_loss: 0.1832, step time: 0.1080\n",
      "5/223, train_loss: 0.1991, step time: 0.0998\n",
      "6/223, train_loss: 0.1830, step time: 0.1031\n",
      "7/223, train_loss: 0.1968, step time: 0.1058\n",
      "8/223, train_loss: 0.1803, step time: 0.1003\n",
      "9/223, train_loss: 0.1786, step time: 0.1002\n",
      "10/223, train_loss: 0.1935, step time: 0.1000\n",
      "11/223, train_loss: 0.1600, step time: 0.1002\n",
      "12/223, train_loss: 0.1720, step time: 0.1062\n",
      "13/223, train_loss: 0.1734, step time: 0.1040\n",
      "14/223, train_loss: 0.1755, step time: 0.1090\n",
      "15/223, train_loss: 0.1787, step time: 0.1120\n",
      "16/223, train_loss: 0.1755, step time: 0.1011\n",
      "17/223, train_loss: 0.2002, step time: 0.1163\n",
      "18/223, train_loss: 0.1999, step time: 0.1023\n",
      "19/223, train_loss: 0.1832, step time: 0.1203\n",
      "20/223, train_loss: 0.1859, step time: 0.1190\n",
      "21/223, train_loss: 0.1774, step time: 0.1126\n",
      "22/223, train_loss: 0.1659, step time: 0.1293\n",
      "23/223, train_loss: 0.1712, step time: 0.1052\n",
      "24/223, train_loss: 0.1812, step time: 0.1177\n",
      "25/223, train_loss: 0.1880, step time: 0.1112\n",
      "26/223, train_loss: 0.1771, step time: 0.1203\n",
      "27/223, train_loss: 0.1986, step time: 0.1086\n",
      "28/223, train_loss: 0.1929, step time: 0.1111\n",
      "29/223, train_loss: 0.1869, step time: 0.1090\n",
      "30/223, train_loss: 0.1927, step time: 0.1102\n",
      "31/223, train_loss: 0.1879, step time: 0.1001\n",
      "32/223, train_loss: 0.3591, step time: 0.1169\n",
      "33/223, train_loss: 0.1677, step time: 0.0997\n",
      "34/223, train_loss: 0.1812, step time: 0.1196\n",
      "35/223, train_loss: 0.1836, step time: 0.0996\n",
      "36/223, train_loss: 0.2130, step time: 0.1053\n",
      "37/223, train_loss: 0.1822, step time: 0.1043\n",
      "38/223, train_loss: 0.1736, step time: 0.1000\n",
      "39/223, train_loss: 0.1933, step time: 0.1095\n",
      "40/223, train_loss: 0.1811, step time: 0.1216\n",
      "41/223, train_loss: 0.1953, step time: 0.1076\n",
      "42/223, train_loss: 0.1850, step time: 0.1029\n",
      "43/223, train_loss: 0.1704, step time: 0.1099\n",
      "44/223, train_loss: 0.1895, step time: 0.1071\n",
      "45/223, train_loss: 0.1763, step time: 0.1060\n",
      "46/223, train_loss: 0.1713, step time: 0.1003\n",
      "47/223, train_loss: 0.1803, step time: 0.1225\n",
      "48/223, train_loss: 0.1854, step time: 0.1233\n",
      "49/223, train_loss: 0.1903, step time: 0.1010\n",
      "50/223, train_loss: 0.1654, step time: 0.1013\n",
      "51/223, train_loss: 0.1921, step time: 0.1074\n",
      "52/223, train_loss: 0.1789, step time: 0.1048\n",
      "53/223, train_loss: 0.1743, step time: 0.1024\n",
      "54/223, train_loss: 0.1866, step time: 0.1057\n",
      "55/223, train_loss: 0.1846, step time: 0.1159\n",
      "56/223, train_loss: 0.1883, step time: 0.1196\n",
      "57/223, train_loss: 0.1802, step time: 0.1085\n",
      "58/223, train_loss: 0.1767, step time: 0.1110\n",
      "59/223, train_loss: 0.1844, step time: 0.1126\n",
      "60/223, train_loss: 0.2005, step time: 0.1083\n",
      "61/223, train_loss: 0.1878, step time: 0.1059\n",
      "62/223, train_loss: 0.2074, step time: 0.1101\n",
      "63/223, train_loss: 0.2083, step time: 0.1085\n",
      "64/223, train_loss: 0.1781, step time: 0.1001\n",
      "65/223, train_loss: 0.1784, step time: 0.1102\n",
      "66/223, train_loss: 0.1725, step time: 0.1177\n",
      "67/223, train_loss: 0.1733, step time: 0.1267\n",
      "68/223, train_loss: 0.1772, step time: 0.1114\n",
      "69/223, train_loss: 0.1717, step time: 0.1079\n",
      "70/223, train_loss: 0.1829, step time: 0.1216\n",
      "71/223, train_loss: 0.1838, step time: 0.1318\n",
      "72/223, train_loss: 0.1676, step time: 0.1076\n",
      "73/223, train_loss: 0.1716, step time: 0.1051\n",
      "74/223, train_loss: 0.1920, step time: 0.1004\n",
      "75/223, train_loss: 0.1759, step time: 0.1112\n",
      "76/223, train_loss: 0.1953, step time: 0.1108\n",
      "77/223, train_loss: 0.1839, step time: 0.1131\n",
      "78/223, train_loss: 0.1779, step time: 0.1037\n",
      "79/223, train_loss: 0.1781, step time: 0.1012\n",
      "80/223, train_loss: 0.1841, step time: 0.1001\n",
      "81/223, train_loss: 0.1752, step time: 0.0999\n",
      "82/223, train_loss: 0.1814, step time: 0.1047\n",
      "83/223, train_loss: 0.1597, step time: 0.1108\n",
      "84/223, train_loss: 0.1768, step time: 0.1000\n",
      "85/223, train_loss: 0.1773, step time: 0.1052\n",
      "86/223, train_loss: 0.1902, step time: 0.1151\n",
      "87/223, train_loss: 0.1713, step time: 0.1103\n",
      "88/223, train_loss: 0.1824, step time: 0.1153\n",
      "89/223, train_loss: 0.1806, step time: 0.1116\n",
      "90/223, train_loss: 0.1625, step time: 0.1149\n",
      "91/223, train_loss: 0.1669, step time: 0.0996\n",
      "92/223, train_loss: 0.1765, step time: 0.1007\n",
      "93/223, train_loss: 0.1999, step time: 0.1008\n",
      "94/223, train_loss: 0.1727, step time: 0.0999\n",
      "95/223, train_loss: 0.1857, step time: 0.1170\n",
      "96/223, train_loss: 0.1826, step time: 0.1110\n",
      "97/223, train_loss: 0.1856, step time: 0.1038\n",
      "98/223, train_loss: 0.1870, step time: 0.1006\n",
      "99/223, train_loss: 0.1790, step time: 0.1000\n",
      "100/223, train_loss: 0.1717, step time: 0.1233\n",
      "101/223, train_loss: 0.1788, step time: 0.1210\n",
      "102/223, train_loss: 0.1704, step time: 0.1149\n",
      "103/223, train_loss: 0.1891, step time: 0.0999\n",
      "104/223, train_loss: 0.1817, step time: 0.1013\n",
      "105/223, train_loss: 0.1863, step time: 0.1010\n",
      "106/223, train_loss: 0.1716, step time: 0.1000\n",
      "107/223, train_loss: 0.1684, step time: 0.1028\n",
      "108/223, train_loss: 0.1946, step time: 0.1156\n",
      "109/223, train_loss: 0.1816, step time: 0.0996\n",
      "110/223, train_loss: 0.1879, step time: 0.1065\n",
      "111/223, train_loss: 0.1791, step time: 0.0998\n",
      "112/223, train_loss: 0.1735, step time: 0.1076\n",
      "113/223, train_loss: 0.1898, step time: 0.1213\n",
      "114/223, train_loss: 0.1795, step time: 0.1387\n",
      "115/223, train_loss: 0.1641, step time: 0.1096\n",
      "116/223, train_loss: 0.1863, step time: 0.1002\n",
      "117/223, train_loss: 0.1810, step time: 0.1131\n",
      "118/223, train_loss: 0.2050, step time: 0.1337\n",
      "119/223, train_loss: 0.1741, step time: 0.1278\n",
      "120/223, train_loss: 0.1743, step time: 0.1091\n",
      "121/223, train_loss: 0.1912, step time: 0.1002\n",
      "122/223, train_loss: 0.1933, step time: 0.1080\n",
      "123/223, train_loss: 0.1797, step time: 0.1050\n",
      "124/223, train_loss: 0.1935, step time: 0.1007\n",
      "125/223, train_loss: 0.1727, step time: 0.1140\n",
      "126/223, train_loss: 0.1610, step time: 0.0994\n",
      "127/223, train_loss: 0.1869, step time: 0.1112\n",
      "128/223, train_loss: 0.2118, step time: 0.1002\n",
      "129/223, train_loss: 0.1759, step time: 0.1003\n",
      "130/223, train_loss: 0.1994, step time: 0.1009\n",
      "131/223, train_loss: 0.1935, step time: 0.0999\n",
      "132/223, train_loss: 0.1895, step time: 0.1000\n",
      "133/223, train_loss: 0.1836, step time: 0.1065\n",
      "134/223, train_loss: 0.2012, step time: 0.1221\n",
      "135/223, train_loss: 0.1956, step time: 0.1031\n",
      "136/223, train_loss: 0.1973, step time: 0.1109\n",
      "137/223, train_loss: 0.1685, step time: 0.1043\n",
      "138/223, train_loss: 0.2020, step time: 0.1005\n",
      "139/223, train_loss: 0.1781, step time: 0.0995\n",
      "140/223, train_loss: 0.1761, step time: 0.1032\n",
      "141/223, train_loss: 0.2065, step time: 0.0995\n",
      "142/223, train_loss: 0.1746, step time: 0.1001\n",
      "143/223, train_loss: 0.1952, step time: 0.1028\n",
      "144/223, train_loss: 0.1788, step time: 0.1000\n",
      "145/223, train_loss: 0.1734, step time: 0.1009\n",
      "146/223, train_loss: 0.1849, step time: 0.1006\n",
      "147/223, train_loss: 0.1772, step time: 0.1007\n",
      "148/223, train_loss: 0.1823, step time: 0.1020\n",
      "149/223, train_loss: 0.1834, step time: 0.1041\n",
      "150/223, train_loss: 0.1829, step time: 0.1022\n",
      "151/223, train_loss: 0.1879, step time: 0.1009\n",
      "152/223, train_loss: 0.1884, step time: 0.1114\n",
      "153/223, train_loss: 0.1818, step time: 0.1020\n",
      "154/223, train_loss: 0.1722, step time: 0.1006\n",
      "155/223, train_loss: 0.1935, step time: 0.1065\n",
      "156/223, train_loss: 0.1773, step time: 0.1007\n",
      "157/223, train_loss: 0.1789, step time: 0.1080\n",
      "158/223, train_loss: 0.1775, step time: 0.1100\n",
      "159/223, train_loss: 0.1758, step time: 0.0997\n",
      "160/223, train_loss: 0.1756, step time: 0.1042\n",
      "161/223, train_loss: 0.1880, step time: 0.1014\n",
      "162/223, train_loss: 0.1716, step time: 0.1000\n",
      "163/223, train_loss: 0.1652, step time: 0.1006\n",
      "164/223, train_loss: 0.1978, step time: 0.1215\n",
      "165/223, train_loss: 0.1842, step time: 0.1020\n",
      "166/223, train_loss: 0.1893, step time: 0.1003\n",
      "167/223, train_loss: 0.1840, step time: 0.1161\n",
      "168/223, train_loss: 0.1927, step time: 0.1007\n",
      "169/223, train_loss: 0.1887, step time: 0.1002\n",
      "170/223, train_loss: 0.1753, step time: 0.1037\n",
      "171/223, train_loss: 0.1698, step time: 0.1100\n",
      "172/223, train_loss: 0.1736, step time: 0.1045\n",
      "173/223, train_loss: 0.1836, step time: 0.1001\n",
      "174/223, train_loss: 0.1884, step time: 0.1100\n",
      "175/223, train_loss: 0.1857, step time: 0.0997\n",
      "176/223, train_loss: 0.1835, step time: 0.1378\n",
      "177/223, train_loss: 0.1666, step time: 0.0985\n",
      "178/223, train_loss: 0.1668, step time: 0.1043\n",
      "179/223, train_loss: 0.2054, step time: 0.1009\n",
      "180/223, train_loss: 0.1679, step time: 0.0991\n",
      "181/223, train_loss: 0.1778, step time: 0.1057\n",
      "182/223, train_loss: 0.1883, step time: 0.1004\n",
      "183/223, train_loss: 0.1769, step time: 0.1006\n",
      "184/223, train_loss: 0.1875, step time: 0.1048\n",
      "185/223, train_loss: 0.1690, step time: 0.1211\n",
      "186/223, train_loss: 0.1780, step time: 0.1153\n",
      "187/223, train_loss: 0.1873, step time: 0.0987\n",
      "188/223, train_loss: 0.1986, step time: 0.1127\n",
      "189/223, train_loss: 0.1894, step time: 0.1028\n",
      "190/223, train_loss: 0.1643, step time: 0.1006\n",
      "191/223, train_loss: 0.1712, step time: 0.1056\n",
      "192/223, train_loss: 0.1768, step time: 0.1294\n",
      "193/223, train_loss: 0.1891, step time: 0.1141\n",
      "194/223, train_loss: 0.1809, step time: 0.1152\n",
      "195/223, train_loss: 0.1775, step time: 0.1132\n",
      "196/223, train_loss: 0.1866, step time: 0.1082\n",
      "197/223, train_loss: 0.2008, step time: 0.1272\n",
      "198/223, train_loss: 0.1674, step time: 0.1351\n",
      "199/223, train_loss: 0.1739, step time: 0.1084\n",
      "200/223, train_loss: 0.1733, step time: 0.0997\n",
      "201/223, train_loss: 0.1660, step time: 0.1117\n",
      "202/223, train_loss: 0.1782, step time: 0.1006\n",
      "203/223, train_loss: 0.1971, step time: 0.1168\n",
      "204/223, train_loss: 0.1837, step time: 0.1167\n",
      "205/223, train_loss: 0.1855, step time: 0.1169\n",
      "206/223, train_loss: 0.1842, step time: 0.1110\n",
      "207/223, train_loss: 0.1701, step time: 0.1075\n",
      "208/223, train_loss: 0.1679, step time: 0.1115\n",
      "209/223, train_loss: 0.1662, step time: 0.1107\n",
      "210/223, train_loss: 0.1666, step time: 0.1039\n",
      "211/223, train_loss: 0.1714, step time: 0.1161\n",
      "212/223, train_loss: 0.1739, step time: 0.1081\n",
      "213/223, train_loss: 0.1666, step time: 0.1103\n",
      "214/223, train_loss: 0.1756, step time: 0.1008\n",
      "215/223, train_loss: 0.2034, step time: 0.1271\n",
      "216/223, train_loss: 0.1686, step time: 0.1132\n",
      "217/223, train_loss: 0.1951, step time: 0.1069\n",
      "218/223, train_loss: 0.1776, step time: 0.1036\n",
      "219/223, train_loss: 0.2043, step time: 0.1003\n",
      "220/223, train_loss: 0.1800, step time: 0.1154\n",
      "221/223, train_loss: 0.1766, step time: 0.1001\n",
      "222/223, train_loss: 0.1787, step time: 0.0998\n",
      "223/223, train_loss: 0.1741, step time: 0.0998\n",
      "epoch 17 average loss: 0.1828\n",
      "time consuming of epoch 17 is: 85.5399\n",
      "----------\n",
      "epoch 18/300\n",
      "1/223, train_loss: 0.1664, step time: 0.1010\n",
      "2/223, train_loss: 0.1645, step time: 0.0987\n",
      "3/223, train_loss: 0.1763, step time: 0.0993\n",
      "4/223, train_loss: 0.1779, step time: 0.1005\n",
      "5/223, train_loss: 0.1590, step time: 0.1413\n",
      "6/223, train_loss: 0.1732, step time: 0.1004\n",
      "7/223, train_loss: 0.1860, step time: 0.0989\n",
      "8/223, train_loss: 0.1809, step time: 0.0994\n",
      "9/223, train_loss: 0.1907, step time: 0.1051\n",
      "10/223, train_loss: 0.1764, step time: 0.1011\n",
      "11/223, train_loss: 0.1734, step time: 0.1014\n",
      "12/223, train_loss: 0.1619, step time: 0.1057\n",
      "13/223, train_loss: 0.1860, step time: 0.1000\n",
      "14/223, train_loss: 0.1605, step time: 0.1035\n",
      "15/223, train_loss: 0.1945, step time: 0.1081\n",
      "16/223, train_loss: 0.1794, step time: 0.1159\n",
      "17/223, train_loss: 0.1816, step time: 0.1090\n",
      "18/223, train_loss: 0.1876, step time: 0.1230\n",
      "19/223, train_loss: 0.1685, step time: 0.1107\n",
      "20/223, train_loss: 0.1903, step time: 0.1024\n",
      "21/223, train_loss: 0.1725, step time: 0.0998\n",
      "22/223, train_loss: 0.1749, step time: 0.1047\n",
      "23/223, train_loss: 0.1730, step time: 0.1103\n",
      "24/223, train_loss: 0.1873, step time: 0.1008\n",
      "25/223, train_loss: 0.1756, step time: 0.1010\n",
      "26/223, train_loss: 0.1733, step time: 0.1002\n",
      "27/223, train_loss: 0.1777, step time: 0.1246\n",
      "28/223, train_loss: 0.1683, step time: 0.1134\n",
      "29/223, train_loss: 0.1874, step time: 0.1083\n",
      "30/223, train_loss: 0.3442, step time: 0.1002\n",
      "31/223, train_loss: 0.1633, step time: 0.1086\n",
      "32/223, train_loss: 0.1942, step time: 0.1015\n",
      "33/223, train_loss: 0.1674, step time: 0.1001\n",
      "34/223, train_loss: 0.1781, step time: 0.0999\n",
      "35/223, train_loss: 0.1806, step time: 0.1008\n",
      "36/223, train_loss: 0.1951, step time: 0.1012\n",
      "37/223, train_loss: 0.1780, step time: 0.0997\n",
      "38/223, train_loss: 0.1762, step time: 0.1105\n",
      "39/223, train_loss: 0.1921, step time: 0.1004\n",
      "40/223, train_loss: 0.1833, step time: 0.0999\n",
      "41/223, train_loss: 0.1783, step time: 0.1209\n",
      "42/223, train_loss: 0.1800, step time: 0.1059\n",
      "43/223, train_loss: 0.1689, step time: 0.0991\n",
      "44/223, train_loss: 0.1804, step time: 0.0990\n",
      "45/223, train_loss: 0.1697, step time: 0.1145\n",
      "46/223, train_loss: 0.2072, step time: 0.1005\n",
      "47/223, train_loss: 0.1784, step time: 0.1003\n",
      "48/223, train_loss: 0.1999, step time: 0.0995\n",
      "49/223, train_loss: 0.1864, step time: 0.1038\n",
      "50/223, train_loss: 0.1723, step time: 0.1116\n",
      "51/223, train_loss: 0.1658, step time: 0.1007\n",
      "52/223, train_loss: 0.1745, step time: 0.1004\n",
      "53/223, train_loss: 0.1796, step time: 0.0996\n",
      "54/223, train_loss: 0.1632, step time: 0.1003\n",
      "55/223, train_loss: 0.1683, step time: 0.0996\n",
      "56/223, train_loss: 0.1756, step time: 0.1006\n",
      "57/223, train_loss: 0.1650, step time: 0.1005\n",
      "58/223, train_loss: 0.1769, step time: 0.1002\n",
      "59/223, train_loss: 0.1940, step time: 0.0999\n",
      "60/223, train_loss: 0.1784, step time: 0.1000\n",
      "61/223, train_loss: 0.1716, step time: 0.1011\n",
      "62/223, train_loss: 0.1697, step time: 0.1010\n",
      "63/223, train_loss: 0.1558, step time: 0.0998\n",
      "64/223, train_loss: 0.1749, step time: 0.1008\n",
      "65/223, train_loss: 0.1810, step time: 0.1044\n",
      "66/223, train_loss: 0.1805, step time: 0.1056\n",
      "67/223, train_loss: 0.1662, step time: 0.0991\n",
      "68/223, train_loss: 0.1774, step time: 0.0999\n",
      "69/223, train_loss: 0.1704, step time: 0.1069\n",
      "70/223, train_loss: 0.1767, step time: 0.1303\n",
      "71/223, train_loss: 0.1744, step time: 0.1098\n",
      "72/223, train_loss: 0.1732, step time: 0.1005\n",
      "73/223, train_loss: 0.1853, step time: 0.1065\n",
      "74/223, train_loss: 0.1914, step time: 0.1145\n",
      "75/223, train_loss: 0.1789, step time: 0.1185\n",
      "76/223, train_loss: 0.1776, step time: 0.1131\n",
      "77/223, train_loss: 0.1786, step time: 0.1066\n",
      "78/223, train_loss: 0.1693, step time: 0.1099\n",
      "79/223, train_loss: 0.1633, step time: 0.1151\n",
      "80/223, train_loss: 0.1602, step time: 0.1005\n",
      "81/223, train_loss: 0.2001, step time: 0.1026\n",
      "82/223, train_loss: 0.1652, step time: 0.1167\n",
      "83/223, train_loss: 0.1830, step time: 0.1005\n",
      "84/223, train_loss: 0.1818, step time: 0.1006\n",
      "85/223, train_loss: 0.1645, step time: 0.1126\n",
      "86/223, train_loss: 0.1745, step time: 0.1090\n",
      "87/223, train_loss: 0.1614, step time: 0.1186\n",
      "88/223, train_loss: 0.1760, step time: 0.1068\n",
      "89/223, train_loss: 0.1785, step time: 0.1198\n",
      "90/223, train_loss: 0.1905, step time: 0.1021\n",
      "91/223, train_loss: 0.1876, step time: 0.1239\n",
      "92/223, train_loss: 0.1686, step time: 0.1008\n",
      "93/223, train_loss: 0.1795, step time: 0.1216\n",
      "94/223, train_loss: 0.1775, step time: 0.1172\n",
      "95/223, train_loss: 0.1649, step time: 0.1002\n",
      "96/223, train_loss: 0.1746, step time: 0.1000\n",
      "97/223, train_loss: 0.1756, step time: 0.1161\n",
      "98/223, train_loss: 0.1725, step time: 0.1155\n",
      "99/223, train_loss: 0.1907, step time: 0.1265\n",
      "100/223, train_loss: 0.1795, step time: 0.1099\n",
      "101/223, train_loss: 0.1749, step time: 0.1118\n",
      "102/223, train_loss: 0.1854, step time: 0.1208\n",
      "103/223, train_loss: 0.1769, step time: 0.1258\n",
      "104/223, train_loss: 0.1666, step time: 0.1058\n",
      "105/223, train_loss: 0.1705, step time: 0.1165\n",
      "106/223, train_loss: 0.1894, step time: 0.1121\n",
      "107/223, train_loss: 0.1737, step time: 0.1225\n",
      "108/223, train_loss: 0.1778, step time: 0.1141\n",
      "109/223, train_loss: 0.1766, step time: 0.1005\n",
      "110/223, train_loss: 0.1774, step time: 0.1006\n",
      "111/223, train_loss: 0.1778, step time: 0.1063\n",
      "112/223, train_loss: 0.1715, step time: 0.1069\n",
      "113/223, train_loss: 0.1740, step time: 0.1073\n",
      "114/223, train_loss: 0.1777, step time: 0.1242\n",
      "115/223, train_loss: 0.1628, step time: 0.1099\n",
      "116/223, train_loss: 0.1868, step time: 0.0997\n",
      "117/223, train_loss: 0.1835, step time: 0.1081\n",
      "118/223, train_loss: 0.1850, step time: 0.1155\n",
      "119/223, train_loss: 0.1633, step time: 0.1061\n",
      "120/223, train_loss: 0.1832, step time: 0.1086\n",
      "121/223, train_loss: 0.1854, step time: 0.1006\n",
      "122/223, train_loss: 0.1739, step time: 0.0999\n",
      "123/223, train_loss: 0.1596, step time: 0.1137\n",
      "124/223, train_loss: 0.1903, step time: 0.1001\n",
      "125/223, train_loss: 0.1788, step time: 0.1051\n",
      "126/223, train_loss: 0.1878, step time: 0.1018\n",
      "127/223, train_loss: 0.1758, step time: 0.1191\n",
      "128/223, train_loss: 0.1853, step time: 0.0999\n",
      "129/223, train_loss: 0.1665, step time: 0.1107\n",
      "130/223, train_loss: 0.2006, step time: 0.1219\n",
      "131/223, train_loss: 0.1793, step time: 0.1441\n",
      "132/223, train_loss: 0.1835, step time: 0.1000\n",
      "133/223, train_loss: 0.1708, step time: 0.1178\n",
      "134/223, train_loss: 0.1971, step time: 0.1036\n",
      "135/223, train_loss: 0.1691, step time: 0.1025\n",
      "136/223, train_loss: 0.1663, step time: 0.1001\n",
      "137/223, train_loss: 0.1843, step time: 0.1233\n",
      "138/223, train_loss: 0.1814, step time: 0.1044\n",
      "139/223, train_loss: 0.1694, step time: 0.1002\n",
      "140/223, train_loss: 0.1765, step time: 0.1020\n",
      "141/223, train_loss: 0.1926, step time: 0.1121\n",
      "142/223, train_loss: 0.1924, step time: 0.1057\n",
      "143/223, train_loss: 0.1755, step time: 0.1132\n",
      "144/223, train_loss: 0.1814, step time: 0.1073\n",
      "145/223, train_loss: 0.1938, step time: 0.1327\n",
      "146/223, train_loss: 0.1749, step time: 0.1157\n",
      "147/223, train_loss: 0.1727, step time: 0.1070\n",
      "148/223, train_loss: 0.1654, step time: 0.1019\n",
      "149/223, train_loss: 0.1754, step time: 0.1042\n",
      "150/223, train_loss: 0.1731, step time: 0.1003\n",
      "151/223, train_loss: 0.1702, step time: 0.1002\n",
      "152/223, train_loss: 0.1749, step time: 0.1006\n",
      "153/223, train_loss: 0.1856, step time: 0.1038\n",
      "154/223, train_loss: 0.1768, step time: 0.1005\n",
      "155/223, train_loss: 0.1803, step time: 0.1011\n",
      "156/223, train_loss: 0.2071, step time: 0.1001\n",
      "157/223, train_loss: 0.1945, step time: 0.1123\n",
      "158/223, train_loss: 0.1803, step time: 0.1203\n",
      "159/223, train_loss: 0.1805, step time: 0.1133\n",
      "160/223, train_loss: 0.1756, step time: 0.1163\n",
      "161/223, train_loss: 0.1791, step time: 0.1108\n",
      "162/223, train_loss: 0.1576, step time: 0.0999\n",
      "163/223, train_loss: 0.1698, step time: 0.0997\n",
      "164/223, train_loss: 0.1638, step time: 0.0998\n",
      "165/223, train_loss: 0.1757, step time: 0.1290\n",
      "166/223, train_loss: 0.1598, step time: 0.1062\n",
      "167/223, train_loss: 0.1878, step time: 0.1000\n",
      "168/223, train_loss: 0.1533, step time: 0.0999\n",
      "169/223, train_loss: 0.1723, step time: 0.1001\n",
      "170/223, train_loss: 0.1902, step time: 0.1040\n",
      "171/223, train_loss: 0.1693, step time: 0.1081\n",
      "172/223, train_loss: 0.1577, step time: 0.1024\n",
      "173/223, train_loss: 0.1724, step time: 0.1033\n",
      "174/223, train_loss: 0.1715, step time: 0.1002\n",
      "175/223, train_loss: 0.1924, step time: 0.1002\n",
      "176/223, train_loss: 0.1725, step time: 0.1017\n",
      "177/223, train_loss: 0.1766, step time: 0.1124\n",
      "178/223, train_loss: 0.1833, step time: 0.1070\n",
      "179/223, train_loss: 0.1639, step time: 0.1208\n",
      "180/223, train_loss: 0.1549, step time: 0.1080\n",
      "181/223, train_loss: 0.1668, step time: 0.1136\n",
      "182/223, train_loss: 0.1787, step time: 0.1165\n",
      "183/223, train_loss: 0.1698, step time: 0.1119\n",
      "184/223, train_loss: 0.1595, step time: 0.1001\n",
      "185/223, train_loss: 0.1798, step time: 0.1099\n",
      "186/223, train_loss: 0.1710, step time: 0.1152\n",
      "187/223, train_loss: 0.1732, step time: 0.1095\n",
      "188/223, train_loss: 0.1647, step time: 0.1198\n",
      "189/223, train_loss: 0.1869, step time: 0.1123\n",
      "190/223, train_loss: 0.1740, step time: 0.1181\n",
      "191/223, train_loss: 0.1732, step time: 0.0995\n",
      "192/223, train_loss: 0.1770, step time: 0.1175\n",
      "193/223, train_loss: 0.1925, step time: 0.1180\n",
      "194/223, train_loss: 0.1854, step time: 0.1002\n",
      "195/223, train_loss: 0.1863, step time: 0.1249\n",
      "196/223, train_loss: 0.1621, step time: 0.0992\n",
      "197/223, train_loss: 0.1820, step time: 0.1122\n",
      "198/223, train_loss: 0.1913, step time: 0.1094\n",
      "199/223, train_loss: 0.1685, step time: 0.0988\n",
      "200/223, train_loss: 0.1683, step time: 0.0983\n",
      "201/223, train_loss: 0.1638, step time: 0.0998\n",
      "202/223, train_loss: 0.1897, step time: 0.1173\n",
      "203/223, train_loss: 0.1901, step time: 0.1091\n",
      "204/223, train_loss: 0.1753, step time: 0.0997\n",
      "205/223, train_loss: 0.1800, step time: 0.1028\n",
      "206/223, train_loss: 0.1872, step time: 0.0999\n",
      "207/223, train_loss: 0.1642, step time: 0.1075\n",
      "208/223, train_loss: 0.1816, step time: 0.1124\n",
      "209/223, train_loss: 0.1691, step time: 0.0997\n",
      "210/223, train_loss: 0.1764, step time: 0.1007\n",
      "211/223, train_loss: 0.1782, step time: 0.1159\n",
      "212/223, train_loss: 0.1955, step time: 0.1086\n",
      "213/223, train_loss: 0.1850, step time: 0.1154\n",
      "214/223, train_loss: 0.1953, step time: 0.1211\n",
      "215/223, train_loss: 0.1721, step time: 0.1089\n",
      "216/223, train_loss: 0.1759, step time: 0.1163\n",
      "217/223, train_loss: 0.1940, step time: 0.1018\n",
      "218/223, train_loss: 0.1761, step time: 0.1002\n",
      "219/223, train_loss: 0.1871, step time: 0.1001\n",
      "220/223, train_loss: 0.1640, step time: 0.1004\n",
      "221/223, train_loss: 0.1575, step time: 0.1000\n",
      "222/223, train_loss: 0.1806, step time: 0.1001\n",
      "223/223, train_loss: 0.1622, step time: 0.0994\n",
      "epoch 18 average loss: 0.1778\n",
      "time consuming of epoch 18 is: 85.8305\n",
      "----------\n",
      "epoch 19/300\n",
      "1/223, train_loss: 0.1811, step time: 0.1054\n",
      "2/223, train_loss: 0.1837, step time: 0.1003\n",
      "3/223, train_loss: 0.1585, step time: 0.1173\n",
      "4/223, train_loss: 0.1829, step time: 0.1076\n",
      "5/223, train_loss: 0.1881, step time: 0.1015\n",
      "6/223, train_loss: 0.1717, step time: 0.0997\n",
      "7/223, train_loss: 0.1725, step time: 0.1208\n",
      "8/223, train_loss: 0.1808, step time: 0.1129\n",
      "9/223, train_loss: 0.1870, step time: 0.1011\n",
      "10/223, train_loss: 0.1885, step time: 0.1037\n",
      "11/223, train_loss: 0.1736, step time: 0.1174\n",
      "12/223, train_loss: 0.1652, step time: 0.1071\n",
      "13/223, train_loss: 0.1777, step time: 0.1118\n",
      "14/223, train_loss: 0.1655, step time: 0.1073\n",
      "15/223, train_loss: 0.1513, step time: 0.1016\n",
      "16/223, train_loss: 0.1688, step time: 0.1283\n",
      "17/223, train_loss: 0.1701, step time: 0.1118\n",
      "18/223, train_loss: 0.1556, step time: 0.1100\n",
      "19/223, train_loss: 0.1760, step time: 0.1092\n",
      "20/223, train_loss: 0.1712, step time: 0.1105\n",
      "21/223, train_loss: 0.1928, step time: 0.1209\n",
      "22/223, train_loss: 0.1734, step time: 0.1126\n",
      "23/223, train_loss: 0.1686, step time: 0.1148\n",
      "24/223, train_loss: 0.1516, step time: 0.1128\n",
      "25/223, train_loss: 0.1692, step time: 0.0998\n",
      "26/223, train_loss: 0.1870, step time: 0.1020\n",
      "27/223, train_loss: 0.1743, step time: 0.1008\n",
      "28/223, train_loss: 0.1807, step time: 0.1270\n",
      "29/223, train_loss: 0.1829, step time: 0.1134\n",
      "30/223, train_loss: 0.1605, step time: 0.1031\n",
      "31/223, train_loss: 0.1472, step time: 0.1003\n",
      "32/223, train_loss: 0.1812, step time: 0.1009\n",
      "33/223, train_loss: 0.1658, step time: 0.1058\n",
      "34/223, train_loss: 0.1850, step time: 0.0998\n",
      "35/223, train_loss: 0.1637, step time: 0.1004\n",
      "36/223, train_loss: 0.1703, step time: 0.1049\n",
      "37/223, train_loss: 0.1759, step time: 0.1261\n",
      "38/223, train_loss: 0.1683, step time: 0.1023\n",
      "39/223, train_loss: 0.1708, step time: 0.1003\n",
      "40/223, train_loss: 0.1862, step time: 0.1011\n",
      "41/223, train_loss: 0.1656, step time: 0.1049\n",
      "42/223, train_loss: 0.1713, step time: 0.1019\n",
      "43/223, train_loss: 0.1658, step time: 0.1098\n",
      "44/223, train_loss: 0.1692, step time: 0.1018\n",
      "45/223, train_loss: 0.1667, step time: 0.1055\n",
      "46/223, train_loss: 0.1692, step time: 0.1165\n",
      "47/223, train_loss: 0.1627, step time: 0.1111\n",
      "48/223, train_loss: 0.1827, step time: 0.1027\n",
      "49/223, train_loss: 0.1834, step time: 0.1127\n",
      "50/223, train_loss: 0.1844, step time: 0.1024\n",
      "51/223, train_loss: 0.1610, step time: 0.1109\n",
      "52/223, train_loss: 0.1787, step time: 0.1310\n",
      "53/223, train_loss: 0.1729, step time: 0.1101\n",
      "54/223, train_loss: 0.1827, step time: 0.0990\n",
      "55/223, train_loss: 0.1679, step time: 0.0985\n",
      "56/223, train_loss: 0.1739, step time: 0.1090\n",
      "57/223, train_loss: 0.1821, step time: 0.1128\n",
      "58/223, train_loss: 0.1781, step time: 0.1002\n",
      "59/223, train_loss: 0.1800, step time: 0.1064\n",
      "60/223, train_loss: 0.1934, step time: 0.1128\n",
      "61/223, train_loss: 0.1844, step time: 0.0992\n",
      "62/223, train_loss: 0.1619, step time: 0.0998\n",
      "63/223, train_loss: 0.1589, step time: 0.0999\n",
      "64/223, train_loss: 0.1684, step time: 0.1373\n",
      "65/223, train_loss: 0.1901, step time: 0.1121\n",
      "66/223, train_loss: 0.1731, step time: 0.1249\n",
      "67/223, train_loss: 0.1701, step time: 0.1076\n",
      "68/223, train_loss: 0.1631, step time: 0.0991\n",
      "69/223, train_loss: 0.1710, step time: 0.1101\n",
      "70/223, train_loss: 0.1579, step time: 0.1113\n",
      "71/223, train_loss: 0.1599, step time: 0.1033\n",
      "72/223, train_loss: 0.1713, step time: 0.1042\n",
      "73/223, train_loss: 0.1625, step time: 0.1006\n",
      "74/223, train_loss: 0.1704, step time: 0.1064\n",
      "75/223, train_loss: 0.1642, step time: 0.1046\n",
      "76/223, train_loss: 0.1620, step time: 0.1132\n",
      "77/223, train_loss: 0.1629, step time: 0.1010\n",
      "78/223, train_loss: 0.1678, step time: 0.0995\n",
      "79/223, train_loss: 0.1798, step time: 0.0997\n",
      "80/223, train_loss: 0.1650, step time: 0.1013\n",
      "81/223, train_loss: 0.1707, step time: 0.0997\n",
      "82/223, train_loss: 0.1607, step time: 0.0991\n",
      "83/223, train_loss: 0.1728, step time: 0.0997\n",
      "84/223, train_loss: 0.1976, step time: 0.1015\n",
      "85/223, train_loss: 0.1956, step time: 0.0991\n",
      "86/223, train_loss: 0.1656, step time: 0.0996\n",
      "87/223, train_loss: 0.1742, step time: 0.1040\n",
      "88/223, train_loss: 0.1759, step time: 0.1004\n",
      "89/223, train_loss: 0.1674, step time: 0.1149\n",
      "90/223, train_loss: 0.1886, step time: 0.1241\n",
      "91/223, train_loss: 0.1718, step time: 0.1058\n",
      "92/223, train_loss: 0.1793, step time: 0.1110\n",
      "93/223, train_loss: 0.1822, step time: 0.1055\n",
      "94/223, train_loss: 0.1729, step time: 0.1005\n",
      "95/223, train_loss: 0.1851, step time: 0.1105\n",
      "96/223, train_loss: 0.1619, step time: 0.1050\n",
      "97/223, train_loss: 0.1636, step time: 0.1046\n",
      "98/223, train_loss: 0.1693, step time: 0.1006\n",
      "99/223, train_loss: 0.1762, step time: 0.1036\n",
      "100/223, train_loss: 0.1690, step time: 0.1009\n",
      "101/223, train_loss: 0.1628, step time: 0.1007\n",
      "102/223, train_loss: 0.1694, step time: 0.1085\n",
      "103/223, train_loss: 0.1628, step time: 0.1043\n",
      "104/223, train_loss: 0.1722, step time: 0.1146\n",
      "105/223, train_loss: 0.1731, step time: 0.1117\n",
      "106/223, train_loss: 0.1683, step time: 0.1011\n",
      "107/223, train_loss: 0.1715, step time: 0.0996\n",
      "108/223, train_loss: 0.1972, step time: 0.1046\n",
      "109/223, train_loss: 0.1697, step time: 0.1237\n",
      "110/223, train_loss: 0.1696, step time: 0.1020\n",
      "111/223, train_loss: 0.1737, step time: 0.1006\n",
      "112/223, train_loss: 0.1631, step time: 0.1101\n",
      "113/223, train_loss: 0.1809, step time: 0.1011\n",
      "114/223, train_loss: 0.1865, step time: 0.1068\n",
      "115/223, train_loss: 0.1580, step time: 0.1045\n",
      "116/223, train_loss: 0.1619, step time: 0.0999\n",
      "117/223, train_loss: 0.1991, step time: 0.1203\n",
      "118/223, train_loss: 0.1482, step time: 0.1189\n",
      "119/223, train_loss: 0.1714, step time: 0.1256\n",
      "120/223, train_loss: 0.1709, step time: 0.1002\n",
      "121/223, train_loss: 0.1738, step time: 0.1113\n",
      "122/223, train_loss: 0.1690, step time: 0.1021\n",
      "123/223, train_loss: 0.1851, step time: 0.1130\n",
      "124/223, train_loss: 0.1771, step time: 0.1006\n",
      "125/223, train_loss: 0.1721, step time: 0.0998\n",
      "126/223, train_loss: 0.1586, step time: 0.1012\n",
      "127/223, train_loss: 0.1787, step time: 0.1003\n",
      "128/223, train_loss: 0.1762, step time: 0.1039\n",
      "129/223, train_loss: 0.1580, step time: 0.1088\n",
      "130/223, train_loss: 0.1773, step time: 0.1098\n",
      "131/223, train_loss: 0.1602, step time: 0.1010\n",
      "132/223, train_loss: 0.1798, step time: 0.0999\n",
      "133/223, train_loss: 0.1703, step time: 0.1156\n",
      "134/223, train_loss: 0.1735, step time: 0.1082\n",
      "135/223, train_loss: 0.1887, step time: 0.1109\n",
      "136/223, train_loss: 0.1698, step time: 0.1026\n",
      "137/223, train_loss: 0.1670, step time: 0.1089\n",
      "138/223, train_loss: 0.1899, step time: 0.1149\n",
      "139/223, train_loss: 0.1781, step time: 0.1013\n",
      "140/223, train_loss: 0.1657, step time: 0.1012\n",
      "141/223, train_loss: 0.1773, step time: 0.1089\n",
      "142/223, train_loss: 0.1869, step time: 0.1033\n",
      "143/223, train_loss: 0.1726, step time: 0.1153\n",
      "144/223, train_loss: 0.1710, step time: 0.1126\n",
      "145/223, train_loss: 0.1767, step time: 0.1298\n",
      "146/223, train_loss: 0.1681, step time: 0.1121\n",
      "147/223, train_loss: 0.1748, step time: 0.1009\n",
      "148/223, train_loss: 0.1854, step time: 0.1002\n",
      "149/223, train_loss: 0.1605, step time: 0.1000\n",
      "150/223, train_loss: 0.1679, step time: 0.1066\n",
      "151/223, train_loss: 0.1728, step time: 0.1158\n",
      "152/223, train_loss: 0.1662, step time: 0.1064\n",
      "153/223, train_loss: 0.1831, step time: 0.1101\n",
      "154/223, train_loss: 0.1795, step time: 0.1043\n",
      "155/223, train_loss: 0.1774, step time: 0.1135\n",
      "156/223, train_loss: 0.1619, step time: 0.1089\n",
      "157/223, train_loss: 0.1701, step time: 0.1116\n",
      "158/223, train_loss: 0.1648, step time: 0.0999\n",
      "159/223, train_loss: 0.1712, step time: 0.1124\n",
      "160/223, train_loss: 0.1689, step time: 0.1050\n",
      "161/223, train_loss: 0.1733, step time: 0.0996\n",
      "162/223, train_loss: 0.1722, step time: 0.1001\n",
      "163/223, train_loss: 0.1779, step time: 0.1010\n",
      "164/223, train_loss: 0.1725, step time: 0.1150\n",
      "165/223, train_loss: 0.1801, step time: 0.1205\n",
      "166/223, train_loss: 0.1676, step time: 0.1259\n",
      "167/223, train_loss: 0.1712, step time: 0.1101\n",
      "168/223, train_loss: 0.1688, step time: 0.1067\n",
      "169/223, train_loss: 0.1661, step time: 0.1173\n",
      "170/223, train_loss: 0.1834, step time: 0.1206\n",
      "171/223, train_loss: 0.1747, step time: 0.1000\n",
      "172/223, train_loss: 0.1631, step time: 0.0999\n",
      "173/223, train_loss: 0.1541, step time: 0.1079\n",
      "174/223, train_loss: 0.1700, step time: 0.1192\n",
      "175/223, train_loss: 0.1648, step time: 0.1093\n",
      "176/223, train_loss: 0.1718, step time: 0.1044\n",
      "177/223, train_loss: 0.1651, step time: 0.1067\n",
      "178/223, train_loss: 0.1665, step time: 0.0999\n",
      "179/223, train_loss: 0.1748, step time: 0.1187\n",
      "180/223, train_loss: 0.3503, step time: 0.1111\n",
      "181/223, train_loss: 0.1752, step time: 0.1067\n",
      "182/223, train_loss: 0.1501, step time: 0.1158\n",
      "183/223, train_loss: 0.1710, step time: 0.1282\n",
      "184/223, train_loss: 0.1619, step time: 0.1169\n",
      "185/223, train_loss: 0.1685, step time: 0.1134\n",
      "186/223, train_loss: 0.1541, step time: 0.1000\n",
      "187/223, train_loss: 0.1774, step time: 0.1001\n",
      "188/223, train_loss: 0.1686, step time: 0.1002\n",
      "189/223, train_loss: 0.1719, step time: 0.1156\n",
      "190/223, train_loss: 0.1615, step time: 0.1082\n",
      "191/223, train_loss: 0.1697, step time: 0.1249\n",
      "192/223, train_loss: 0.1565, step time: 0.1151\n",
      "193/223, train_loss: 0.1729, step time: 0.1003\n",
      "194/223, train_loss: 0.1754, step time: 0.1006\n",
      "195/223, train_loss: 0.1662, step time: 0.1060\n",
      "196/223, train_loss: 0.2020, step time: 0.1010\n",
      "197/223, train_loss: 0.1762, step time: 0.1105\n",
      "198/223, train_loss: 0.1710, step time: 0.1016\n",
      "199/223, train_loss: 0.1575, step time: 0.0998\n",
      "200/223, train_loss: 0.1704, step time: 0.1005\n",
      "201/223, train_loss: 0.1608, step time: 0.1073\n",
      "202/223, train_loss: 0.1513, step time: 0.1158\n",
      "203/223, train_loss: 0.1774, step time: 0.1009\n",
      "204/223, train_loss: 0.1789, step time: 0.1067\n",
      "205/223, train_loss: 0.1603, step time: 0.1085\n",
      "206/223, train_loss: 0.1596, step time: 0.1003\n",
      "207/223, train_loss: 0.1847, step time: 0.0996\n",
      "208/223, train_loss: 0.1580, step time: 0.1044\n",
      "209/223, train_loss: 0.1775, step time: 0.1183\n",
      "210/223, train_loss: 0.1574, step time: 0.1094\n",
      "211/223, train_loss: 0.1836, step time: 0.0998\n",
      "212/223, train_loss: 0.1637, step time: 0.1004\n",
      "213/223, train_loss: 0.1752, step time: 0.1135\n",
      "214/223, train_loss: 0.1836, step time: 0.1004\n",
      "215/223, train_loss: 0.1740, step time: 0.1011\n",
      "216/223, train_loss: 0.1803, step time: 0.1113\n",
      "217/223, train_loss: 0.1700, step time: 0.1000\n",
      "218/223, train_loss: 0.1692, step time: 0.0995\n",
      "219/223, train_loss: 0.1901, step time: 0.1000\n",
      "220/223, train_loss: 0.1791, step time: 0.1001\n",
      "221/223, train_loss: 0.1716, step time: 0.0992\n",
      "222/223, train_loss: 0.1675, step time: 0.0993\n",
      "223/223, train_loss: 0.1606, step time: 0.0997\n",
      "epoch 19 average loss: 0.1728\n",
      "time consuming of epoch 19 is: 86.8848\n",
      "----------\n",
      "epoch 20/300\n",
      "1/223, train_loss: 0.1656, step time: 0.1100\n",
      "2/223, train_loss: 0.1690, step time: 0.1062\n",
      "3/223, train_loss: 0.1691, step time: 0.1053\n",
      "4/223, train_loss: 0.1765, step time: 0.1006\n",
      "5/223, train_loss: 0.1664, step time: 0.1236\n",
      "6/223, train_loss: 0.1914, step time: 0.1067\n",
      "7/223, train_loss: 0.1597, step time: 0.1141\n",
      "8/223, train_loss: 0.1882, step time: 0.1085\n",
      "9/223, train_loss: 0.1701, step time: 0.1119\n",
      "10/223, train_loss: 0.1705, step time: 0.1096\n",
      "11/223, train_loss: 0.1615, step time: 0.1100\n",
      "12/223, train_loss: 0.1622, step time: 0.1114\n",
      "13/223, train_loss: 0.1564, step time: 0.1121\n",
      "14/223, train_loss: 0.1715, step time: 0.1003\n",
      "15/223, train_loss: 0.1821, step time: 0.1181\n",
      "16/223, train_loss: 0.1846, step time: 0.1092\n",
      "17/223, train_loss: 0.1691, step time: 0.1028\n",
      "18/223, train_loss: 0.1594, step time: 0.1440\n",
      "19/223, train_loss: 0.1577, step time: 0.1153\n",
      "20/223, train_loss: 0.1729, step time: 0.0998\n",
      "21/223, train_loss: 0.1840, step time: 0.1140\n",
      "22/223, train_loss: 0.1654, step time: 0.1107\n",
      "23/223, train_loss: 0.1749, step time: 0.1385\n",
      "24/223, train_loss: 0.1739, step time: 0.1085\n",
      "25/223, train_loss: 0.1585, step time: 0.1025\n",
      "26/223, train_loss: 0.1701, step time: 0.1136\n",
      "27/223, train_loss: 0.1645, step time: 0.1018\n",
      "28/223, train_loss: 0.1846, step time: 0.1137\n",
      "29/223, train_loss: 0.1558, step time: 0.1162\n",
      "30/223, train_loss: 0.1668, step time: 0.1059\n",
      "31/223, train_loss: 0.1665, step time: 0.1002\n",
      "32/223, train_loss: 0.1729, step time: 0.1006\n",
      "33/223, train_loss: 0.1460, step time: 0.1075\n",
      "34/223, train_loss: 0.1714, step time: 0.1141\n",
      "35/223, train_loss: 0.1871, step time: 0.1220\n",
      "36/223, train_loss: 0.1545, step time: 0.1137\n",
      "37/223, train_loss: 0.1596, step time: 0.1176\n",
      "38/223, train_loss: 0.1643, step time: 0.1092\n",
      "39/223, train_loss: 0.1789, step time: 0.1183\n",
      "40/223, train_loss: 0.1870, step time: 0.1405\n",
      "41/223, train_loss: 0.1794, step time: 0.1087\n",
      "42/223, train_loss: 0.1660, step time: 0.1015\n",
      "43/223, train_loss: 0.1782, step time: 0.1209\n",
      "44/223, train_loss: 0.1743, step time: 0.1006\n",
      "45/223, train_loss: 0.1672, step time: 0.1208\n",
      "46/223, train_loss: 0.1609, step time: 0.1307\n",
      "47/223, train_loss: 0.1801, step time: 0.1091\n",
      "48/223, train_loss: 0.1733, step time: 0.1132\n",
      "49/223, train_loss: 0.1701, step time: 0.1146\n",
      "50/223, train_loss: 0.1561, step time: 0.1016\n",
      "51/223, train_loss: 0.1768, step time: 0.1215\n",
      "52/223, train_loss: 0.1874, step time: 0.1146\n",
      "53/223, train_loss: 0.1655, step time: 0.1113\n",
      "54/223, train_loss: 0.1679, step time: 0.1091\n",
      "55/223, train_loss: 0.1652, step time: 0.1274\n",
      "56/223, train_loss: 0.1928, step time: 0.1075\n",
      "57/223, train_loss: 0.1582, step time: 0.1083\n",
      "58/223, train_loss: 0.1890, step time: 0.1027\n",
      "59/223, train_loss: 0.1639, step time: 0.1235\n",
      "60/223, train_loss: 0.1682, step time: 0.1107\n",
      "61/223, train_loss: 0.1646, step time: 0.1018\n",
      "62/223, train_loss: 0.1634, step time: 0.0996\n",
      "63/223, train_loss: 0.1471, step time: 0.0987\n",
      "64/223, train_loss: 0.1825, step time: 0.1016\n",
      "65/223, train_loss: 0.1667, step time: 0.1121\n",
      "66/223, train_loss: 0.1752, step time: 0.1220\n",
      "67/223, train_loss: 0.1867, step time: 0.1065\n",
      "68/223, train_loss: 0.1774, step time: 0.0988\n",
      "69/223, train_loss: 0.1777, step time: 0.1467\n",
      "70/223, train_loss: 0.1720, step time: 0.1019\n",
      "71/223, train_loss: 0.1638, step time: 0.1002\n",
      "72/223, train_loss: 0.1651, step time: 0.0990\n",
      "73/223, train_loss: 0.1708, step time: 0.1179\n",
      "74/223, train_loss: 0.1631, step time: 0.1037\n",
      "75/223, train_loss: 0.1673, step time: 0.1130\n",
      "76/223, train_loss: 0.1862, step time: 0.1062\n",
      "77/223, train_loss: 0.1682, step time: 0.1382\n",
      "78/223, train_loss: 0.1742, step time: 0.1223\n",
      "79/223, train_loss: 0.1637, step time: 0.1002\n",
      "80/223, train_loss: 0.1651, step time: 0.0994\n",
      "81/223, train_loss: 0.1836, step time: 0.1054\n",
      "82/223, train_loss: 0.1534, step time: 0.1245\n",
      "83/223, train_loss: 0.1541, step time: 0.1143\n",
      "84/223, train_loss: 0.1783, step time: 0.1005\n",
      "85/223, train_loss: 0.1756, step time: 0.1086\n",
      "86/223, train_loss: 0.1777, step time: 0.1051\n",
      "87/223, train_loss: 0.1591, step time: 0.1001\n",
      "88/223, train_loss: 0.1677, step time: 0.1011\n",
      "89/223, train_loss: 0.1627, step time: 0.1130\n",
      "90/223, train_loss: 0.1749, step time: 0.1063\n",
      "91/223, train_loss: 0.1519, step time: 0.1125\n",
      "92/223, train_loss: 0.1679, step time: 0.1011\n",
      "93/223, train_loss: 0.1615, step time: 0.1124\n",
      "94/223, train_loss: 0.1809, step time: 0.1117\n",
      "95/223, train_loss: 0.1714, step time: 0.1087\n",
      "96/223, train_loss: 0.1750, step time: 0.1064\n",
      "97/223, train_loss: 0.1751, step time: 0.1120\n",
      "98/223, train_loss: 0.1595, step time: 0.0993\n",
      "99/223, train_loss: 0.1638, step time: 0.0998\n",
      "100/223, train_loss: 0.1714, step time: 0.1064\n",
      "101/223, train_loss: 0.1749, step time: 0.1117\n",
      "102/223, train_loss: 0.1571, step time: 0.1253\n",
      "103/223, train_loss: 0.1522, step time: 0.1265\n",
      "104/223, train_loss: 0.1630, step time: 0.1052\n",
      "105/223, train_loss: 0.1469, step time: 0.1135\n",
      "106/223, train_loss: 0.1648, step time: 0.1088\n",
      "107/223, train_loss: 0.1973, step time: 0.1186\n",
      "108/223, train_loss: 0.1917, step time: 0.1012\n",
      "109/223, train_loss: 0.1633, step time: 0.1103\n",
      "110/223, train_loss: 0.1596, step time: 0.1162\n",
      "111/223, train_loss: 0.1695, step time: 0.1225\n",
      "112/223, train_loss: 0.1704, step time: 0.1176\n",
      "113/223, train_loss: 0.1690, step time: 0.0996\n",
      "114/223, train_loss: 0.1667, step time: 0.1084\n",
      "115/223, train_loss: 0.1758, step time: 0.1057\n",
      "116/223, train_loss: 0.1556, step time: 0.1262\n",
      "117/223, train_loss: 0.1796, step time: 0.0998\n",
      "118/223, train_loss: 0.1946, step time: 0.1045\n",
      "119/223, train_loss: 0.1738, step time: 0.0996\n",
      "120/223, train_loss: 0.1702, step time: 0.1011\n",
      "121/223, train_loss: 0.1923, step time: 0.1171\n",
      "122/223, train_loss: 0.1574, step time: 0.1105\n",
      "123/223, train_loss: 0.1647, step time: 0.1168\n",
      "124/223, train_loss: 0.1826, step time: 0.1089\n",
      "125/223, train_loss: 0.1600, step time: 0.1144\n",
      "126/223, train_loss: 0.1729, step time: 0.1125\n",
      "127/223, train_loss: 0.1508, step time: 0.1274\n",
      "128/223, train_loss: 0.1603, step time: 0.1138\n",
      "129/223, train_loss: 0.1831, step time: 0.1227\n",
      "130/223, train_loss: 0.1683, step time: 0.1105\n",
      "131/223, train_loss: 0.1669, step time: 0.1326\n",
      "132/223, train_loss: 0.1830, step time: 0.1043\n",
      "133/223, train_loss: 0.1946, step time: 0.1001\n",
      "134/223, train_loss: 0.1743, step time: 0.1149\n",
      "135/223, train_loss: 0.1645, step time: 0.1048\n",
      "136/223, train_loss: 0.1464, step time: 0.1059\n",
      "137/223, train_loss: 0.1845, step time: 0.1040\n",
      "138/223, train_loss: 0.1635, step time: 0.1014\n",
      "139/223, train_loss: 0.1657, step time: 0.0999\n",
      "140/223, train_loss: 0.1621, step time: 0.1147\n",
      "141/223, train_loss: 0.1631, step time: 0.1133\n",
      "142/223, train_loss: 0.1631, step time: 0.1108\n",
      "143/223, train_loss: 0.1778, step time: 0.1082\n",
      "144/223, train_loss: 0.1672, step time: 0.1043\n",
      "145/223, train_loss: 0.1601, step time: 0.1167\n",
      "146/223, train_loss: 0.1794, step time: 0.1349\n",
      "147/223, train_loss: 0.1870, step time: 0.0991\n",
      "148/223, train_loss: 0.1841, step time: 0.1067\n",
      "149/223, train_loss: 0.1547, step time: 0.1058\n",
      "150/223, train_loss: 0.1737, step time: 0.0990\n",
      "151/223, train_loss: 0.1862, step time: 0.0987\n",
      "152/223, train_loss: 0.1673, step time: 0.0990\n",
      "153/223, train_loss: 0.1578, step time: 0.0989\n",
      "154/223, train_loss: 0.1646, step time: 0.0990\n",
      "155/223, train_loss: 0.1645, step time: 0.0991\n",
      "156/223, train_loss: 0.1890, step time: 0.0992\n",
      "157/223, train_loss: 0.1748, step time: 0.1009\n",
      "158/223, train_loss: 0.1594, step time: 0.1151\n",
      "159/223, train_loss: 0.1630, step time: 0.1060\n",
      "160/223, train_loss: 0.1730, step time: 0.1146\n",
      "161/223, train_loss: 0.1708, step time: 0.1100\n",
      "162/223, train_loss: 0.1725, step time: 0.1135\n",
      "163/223, train_loss: 0.1718, step time: 0.1001\n",
      "164/223, train_loss: 0.1576, step time: 0.1046\n",
      "165/223, train_loss: 0.1631, step time: 0.1144\n",
      "166/223, train_loss: 0.1744, step time: 0.1223\n",
      "167/223, train_loss: 0.1532, step time: 0.1095\n",
      "168/223, train_loss: 0.1595, step time: 0.1000\n",
      "169/223, train_loss: 0.1625, step time: 0.1003\n",
      "170/223, train_loss: 0.1755, step time: 0.0993\n",
      "171/223, train_loss: 0.1885, step time: 0.1002\n",
      "172/223, train_loss: 0.1865, step time: 0.1092\n",
      "173/223, train_loss: 0.1638, step time: 0.1003\n",
      "174/223, train_loss: 0.1919, step time: 0.1004\n",
      "175/223, train_loss: 0.1740, step time: 0.1013\n",
      "176/223, train_loss: 0.1727, step time: 0.1061\n",
      "177/223, train_loss: 0.1642, step time: 0.1096\n",
      "178/223, train_loss: 0.1821, step time: 0.1092\n",
      "179/223, train_loss: 0.1820, step time: 0.1248\n",
      "180/223, train_loss: 0.1697, step time: 0.0999\n",
      "181/223, train_loss: 0.1761, step time: 0.1033\n",
      "182/223, train_loss: 0.1697, step time: 0.1298\n",
      "183/223, train_loss: 0.1563, step time: 0.1010\n",
      "184/223, train_loss: 0.1816, step time: 0.1006\n",
      "185/223, train_loss: 0.1711, step time: 0.0991\n",
      "186/223, train_loss: 0.1694, step time: 0.0980\n",
      "187/223, train_loss: 0.1654, step time: 0.0995\n",
      "188/223, train_loss: 0.1782, step time: 0.1099\n",
      "189/223, train_loss: 0.1686, step time: 0.1002\n",
      "190/223, train_loss: 0.1653, step time: 0.0989\n",
      "191/223, train_loss: 0.1849, step time: 0.0992\n",
      "192/223, train_loss: 0.1715, step time: 0.0998\n",
      "193/223, train_loss: 0.1640, step time: 0.0998\n",
      "194/223, train_loss: 0.1613, step time: 0.0995\n",
      "195/223, train_loss: 0.1699, step time: 0.0997\n",
      "196/223, train_loss: 0.1652, step time: 0.1078\n",
      "197/223, train_loss: 0.1666, step time: 0.0998\n",
      "198/223, train_loss: 0.1617, step time: 0.0996\n",
      "199/223, train_loss: 0.1634, step time: 0.0990\n",
      "200/223, train_loss: 0.1687, step time: 0.0995\n",
      "201/223, train_loss: 0.1611, step time: 0.0998\n",
      "202/223, train_loss: 0.1702, step time: 0.0997\n",
      "203/223, train_loss: 0.1601, step time: 0.1001\n",
      "204/223, train_loss: 0.1710, step time: 0.1022\n",
      "205/223, train_loss: 0.1691, step time: 0.1007\n",
      "206/223, train_loss: 0.1620, step time: 0.1001\n",
      "207/223, train_loss: 0.1810, step time: 0.1004\n",
      "208/223, train_loss: 0.1693, step time: 0.1005\n",
      "209/223, train_loss: 0.1593, step time: 0.0989\n",
      "210/223, train_loss: 0.3376, step time: 0.1064\n",
      "211/223, train_loss: 0.1641, step time: 0.1153\n",
      "212/223, train_loss: 0.1692, step time: 0.0995\n",
      "213/223, train_loss: 0.1742, step time: 0.1019\n",
      "214/223, train_loss: 0.1655, step time: 0.0989\n",
      "215/223, train_loss: 0.1648, step time: 0.0990\n",
      "216/223, train_loss: 0.1546, step time: 0.1057\n",
      "217/223, train_loss: 0.1539, step time: 0.0994\n",
      "218/223, train_loss: 0.1706, step time: 0.1000\n",
      "219/223, train_loss: 0.1663, step time: 0.1009\n",
      "220/223, train_loss: 0.1631, step time: 0.1059\n",
      "221/223, train_loss: 0.1805, step time: 0.0997\n",
      "222/223, train_loss: 0.1576, step time: 0.1002\n",
      "223/223, train_loss: 0.1617, step time: 0.0997\n",
      "epoch 20 average loss: 0.1704\n",
      "saved new best metric model\n",
      "current epoch: 20 current mean dice: 0.7903 tc: 0.8860 wt: 0.8072 et: 0.6777\n",
      "best mean dice: 0.7903 at epoch: 20\n",
      "time consuming of epoch 20 is: 96.9132\n",
      "----------\n",
      "epoch 21/300\n",
      "1/223, train_loss: 0.1751, step time: 0.1056\n",
      "2/223, train_loss: 0.1801, step time: 0.0991\n",
      "3/223, train_loss: 0.1650, step time: 0.0990\n",
      "4/223, train_loss: 0.1727, step time: 0.0984\n",
      "5/223, train_loss: 0.1597, step time: 0.1179\n",
      "6/223, train_loss: 0.1825, step time: 0.1121\n",
      "7/223, train_loss: 0.1821, step time: 0.1000\n",
      "8/223, train_loss: 0.1723, step time: 0.1104\n",
      "9/223, train_loss: 0.1874, step time: 0.1030\n",
      "10/223, train_loss: 0.1613, step time: 0.1148\n",
      "11/223, train_loss: 0.1726, step time: 0.1152\n",
      "12/223, train_loss: 0.1721, step time: 0.1004\n",
      "13/223, train_loss: 0.1857, step time: 0.1113\n",
      "14/223, train_loss: 0.1720, step time: 0.1060\n",
      "15/223, train_loss: 0.1739, step time: 0.1040\n",
      "16/223, train_loss: 0.1711, step time: 0.1102\n",
      "17/223, train_loss: 0.1694, step time: 0.1161\n",
      "18/223, train_loss: 0.1522, step time: 0.1001\n",
      "19/223, train_loss: 0.1624, step time: 0.1021\n",
      "20/223, train_loss: 0.1675, step time: 0.1050\n",
      "21/223, train_loss: 0.1562, step time: 0.1207\n",
      "22/223, train_loss: 0.1521, step time: 0.1008\n",
      "23/223, train_loss: 0.1421, step time: 0.1002\n",
      "24/223, train_loss: 0.1718, step time: 0.1017\n",
      "25/223, train_loss: 0.1622, step time: 0.1091\n",
      "26/223, train_loss: 0.1678, step time: 0.1038\n",
      "27/223, train_loss: 0.1533, step time: 0.1034\n",
      "28/223, train_loss: 0.1671, step time: 0.1136\n",
      "29/223, train_loss: 0.1707, step time: 0.1022\n",
      "30/223, train_loss: 0.1922, step time: 0.1002\n",
      "31/223, train_loss: 0.1690, step time: 0.1015\n",
      "32/223, train_loss: 0.1769, step time: 0.1007\n",
      "33/223, train_loss: 0.1552, step time: 0.1034\n",
      "34/223, train_loss: 0.1671, step time: 0.1088\n",
      "35/223, train_loss: 0.1621, step time: 0.1020\n",
      "36/223, train_loss: 0.1644, step time: 0.1282\n",
      "37/223, train_loss: 0.1586, step time: 0.1192\n",
      "38/223, train_loss: 0.1608, step time: 0.1079\n",
      "39/223, train_loss: 0.1752, step time: 0.1064\n",
      "40/223, train_loss: 0.1776, step time: 0.1024\n",
      "41/223, train_loss: 0.1670, step time: 0.1163\n",
      "42/223, train_loss: 0.1912, step time: 0.1116\n",
      "43/223, train_loss: 0.1640, step time: 0.1029\n",
      "44/223, train_loss: 0.1731, step time: 0.1214\n",
      "45/223, train_loss: 0.1806, step time: 0.1173\n",
      "46/223, train_loss: 0.1545, step time: 0.1083\n",
      "47/223, train_loss: 0.1672, step time: 0.1005\n",
      "48/223, train_loss: 0.1712, step time: 0.1052\n",
      "49/223, train_loss: 0.1670, step time: 0.1040\n",
      "50/223, train_loss: 0.1696, step time: 0.1259\n",
      "51/223, train_loss: 0.1607, step time: 0.1019\n",
      "52/223, train_loss: 0.1542, step time: 0.1344\n",
      "53/223, train_loss: 0.1628, step time: 0.1177\n",
      "54/223, train_loss: 0.1708, step time: 0.1182\n",
      "55/223, train_loss: 0.1508, step time: 0.1139\n",
      "56/223, train_loss: 0.1521, step time: 0.1341\n",
      "57/223, train_loss: 0.1572, step time: 0.1089\n",
      "58/223, train_loss: 0.1649, step time: 0.1133\n",
      "59/223, train_loss: 0.1643, step time: 0.0996\n",
      "60/223, train_loss: 0.1560, step time: 0.1002\n",
      "61/223, train_loss: 0.1762, step time: 0.1003\n",
      "62/223, train_loss: 0.1809, step time: 0.1060\n",
      "63/223, train_loss: 0.1747, step time: 0.1042\n",
      "64/223, train_loss: 0.1476, step time: 0.0999\n",
      "65/223, train_loss: 0.1692, step time: 0.1129\n",
      "66/223, train_loss: 0.1611, step time: 0.1082\n",
      "67/223, train_loss: 0.1507, step time: 0.1063\n",
      "68/223, train_loss: 0.1552, step time: 0.1132\n",
      "69/223, train_loss: 0.1734, step time: 0.1278\n",
      "70/223, train_loss: 0.1706, step time: 0.1117\n",
      "71/223, train_loss: 0.1695, step time: 0.1118\n",
      "72/223, train_loss: 0.1647, step time: 0.1086\n",
      "73/223, train_loss: 0.1506, step time: 0.1020\n",
      "74/223, train_loss: 0.1605, step time: 0.1101\n",
      "75/223, train_loss: 0.1827, step time: 0.1164\n",
      "76/223, train_loss: 0.1766, step time: 0.1079\n",
      "77/223, train_loss: 0.1705, step time: 0.1004\n",
      "78/223, train_loss: 0.1661, step time: 0.1007\n",
      "79/223, train_loss: 0.1755, step time: 0.1065\n",
      "80/223, train_loss: 0.1550, step time: 0.1266\n",
      "81/223, train_loss: 0.1565, step time: 0.1077\n",
      "82/223, train_loss: 0.1671, step time: 0.1127\n",
      "83/223, train_loss: 0.1868, step time: 0.1096\n",
      "84/223, train_loss: 0.1716, step time: 0.1007\n",
      "85/223, train_loss: 0.1667, step time: 0.1086\n",
      "86/223, train_loss: 0.1513, step time: 0.1125\n",
      "87/223, train_loss: 0.1720, step time: 0.1099\n",
      "88/223, train_loss: 0.1659, step time: 0.1035\n",
      "89/223, train_loss: 0.1729, step time: 0.1080\n",
      "90/223, train_loss: 0.1618, step time: 0.1076\n",
      "91/223, train_loss: 0.1687, step time: 0.1000\n",
      "92/223, train_loss: 0.1608, step time: 0.1004\n",
      "93/223, train_loss: 0.1647, step time: 0.1138\n",
      "94/223, train_loss: 0.1523, step time: 0.1039\n",
      "95/223, train_loss: 0.1635, step time: 0.1117\n",
      "96/223, train_loss: 0.1764, step time: 0.1150\n",
      "97/223, train_loss: 0.1532, step time: 0.1014\n",
      "98/223, train_loss: 0.1550, step time: 0.1081\n",
      "99/223, train_loss: 0.1656, step time: 0.1153\n",
      "100/223, train_loss: 0.3427, step time: 0.1103\n",
      "101/223, train_loss: 0.1567, step time: 0.1072\n",
      "102/223, train_loss: 0.1707, step time: 0.1133\n",
      "103/223, train_loss: 0.1490, step time: 0.1051\n",
      "104/223, train_loss: 0.1515, step time: 0.1004\n",
      "105/223, train_loss: 0.1900, step time: 0.1004\n",
      "106/223, train_loss: 0.1740, step time: 0.1089\n",
      "107/223, train_loss: 0.1573, step time: 0.1007\n",
      "108/223, train_loss: 0.1719, step time: 0.1010\n",
      "109/223, train_loss: 0.1677, step time: 0.1008\n",
      "110/223, train_loss: 0.1607, step time: 0.1168\n",
      "111/223, train_loss: 0.1757, step time: 0.1103\n",
      "112/223, train_loss: 0.1520, step time: 0.1026\n",
      "113/223, train_loss: 0.1476, step time: 0.1007\n",
      "114/223, train_loss: 0.1712, step time: 0.1196\n",
      "115/223, train_loss: 0.1667, step time: 0.1224\n",
      "116/223, train_loss: 0.1478, step time: 0.1204\n",
      "117/223, train_loss: 0.1518, step time: 0.0993\n",
      "118/223, train_loss: 0.1933, step time: 0.1028\n",
      "119/223, train_loss: 0.1676, step time: 0.1263\n",
      "120/223, train_loss: 0.1674, step time: 0.1118\n",
      "121/223, train_loss: 0.1736, step time: 0.1131\n",
      "122/223, train_loss: 0.1602, step time: 0.1053\n",
      "123/223, train_loss: 0.1612, step time: 0.1172\n",
      "124/223, train_loss: 0.1668, step time: 0.0997\n",
      "125/223, train_loss: 0.1582, step time: 0.1147\n",
      "126/223, train_loss: 0.1475, step time: 0.1032\n",
      "127/223, train_loss: 0.1686, step time: 0.1081\n",
      "128/223, train_loss: 0.1565, step time: 0.1004\n",
      "129/223, train_loss: 0.1728, step time: 0.1018\n",
      "130/223, train_loss: 0.1699, step time: 0.1338\n",
      "131/223, train_loss: 0.1720, step time: 0.1004\n",
      "132/223, train_loss: 0.1520, step time: 0.1009\n",
      "133/223, train_loss: 0.1776, step time: 0.1002\n",
      "134/223, train_loss: 0.1836, step time: 0.1164\n",
      "135/223, train_loss: 0.1570, step time: 0.1004\n",
      "136/223, train_loss: 0.1504, step time: 0.0999\n",
      "137/223, train_loss: 0.1785, step time: 0.1173\n",
      "138/223, train_loss: 0.1515, step time: 0.1087\n",
      "139/223, train_loss: 0.1646, step time: 0.1070\n",
      "140/223, train_loss: 0.1637, step time: 0.1415\n",
      "141/223, train_loss: 0.1626, step time: 0.1100\n",
      "142/223, train_loss: 0.1575, step time: 0.1112\n",
      "143/223, train_loss: 0.1634, step time: 0.1010\n",
      "144/223, train_loss: 0.1602, step time: 0.1001\n",
      "145/223, train_loss: 0.1717, step time: 0.1159\n",
      "146/223, train_loss: 0.1788, step time: 0.1101\n",
      "147/223, train_loss: 0.1792, step time: 0.1107\n",
      "148/223, train_loss: 0.1478, step time: 0.1006\n",
      "149/223, train_loss: 0.1634, step time: 0.1009\n",
      "150/223, train_loss: 0.1585, step time: 0.1123\n",
      "151/223, train_loss: 0.1605, step time: 0.1000\n",
      "152/223, train_loss: 0.1610, step time: 0.1195\n",
      "153/223, train_loss: 0.1638, step time: 0.0998\n",
      "154/223, train_loss: 0.1629, step time: 0.1037\n",
      "155/223, train_loss: 0.1722, step time: 0.1130\n",
      "156/223, train_loss: 0.1579, step time: 0.1009\n",
      "157/223, train_loss: 0.1772, step time: 0.1039\n",
      "158/223, train_loss: 0.1691, step time: 0.1027\n",
      "159/223, train_loss: 0.1527, step time: 0.1033\n",
      "160/223, train_loss: 0.1693, step time: 0.1112\n",
      "161/223, train_loss: 0.1985, step time: 0.1004\n",
      "162/223, train_loss: 0.1552, step time: 0.1008\n",
      "163/223, train_loss: 0.1658, step time: 0.1010\n",
      "164/223, train_loss: 0.1588, step time: 0.1015\n",
      "165/223, train_loss: 0.1666, step time: 0.1007\n",
      "166/223, train_loss: 0.1803, step time: 0.1236\n",
      "167/223, train_loss: 0.1757, step time: 0.1207\n",
      "168/223, train_loss: 0.1570, step time: 0.1082\n",
      "169/223, train_loss: 0.1605, step time: 0.1183\n",
      "170/223, train_loss: 0.1661, step time: 0.1001\n",
      "171/223, train_loss: 0.1780, step time: 0.1009\n",
      "172/223, train_loss: 0.1660, step time: 0.1577\n",
      "173/223, train_loss: 0.1585, step time: 0.1002\n",
      "174/223, train_loss: 0.1627, step time: 0.1112\n",
      "175/223, train_loss: 0.1768, step time: 0.1051\n",
      "176/223, train_loss: 0.1542, step time: 0.1183\n",
      "177/223, train_loss: 0.1626, step time: 0.1136\n",
      "178/223, train_loss: 0.1838, step time: 0.1195\n",
      "179/223, train_loss: 0.1740, step time: 0.1013\n",
      "180/223, train_loss: 0.1496, step time: 0.1010\n",
      "181/223, train_loss: 0.1679, step time: 0.1007\n",
      "182/223, train_loss: 0.1659, step time: 0.1018\n",
      "183/223, train_loss: 0.1625, step time: 0.1003\n",
      "184/223, train_loss: 0.1712, step time: 0.1011\n",
      "185/223, train_loss: 0.1567, step time: 0.1008\n",
      "186/223, train_loss: 0.1608, step time: 0.1071\n",
      "187/223, train_loss: 0.1652, step time: 0.1021\n",
      "188/223, train_loss: 0.1906, step time: 0.1015\n",
      "189/223, train_loss: 0.1493, step time: 0.1139\n",
      "190/223, train_loss: 0.1722, step time: 0.1093\n",
      "191/223, train_loss: 0.1852, step time: 0.1022\n",
      "192/223, train_loss: 0.1806, step time: 0.1160\n",
      "193/223, train_loss: 0.1753, step time: 0.1084\n",
      "194/223, train_loss: 0.1647, step time: 0.1171\n",
      "195/223, train_loss: 0.1743, step time: 0.1014\n",
      "196/223, train_loss: 0.1680, step time: 0.1063\n",
      "197/223, train_loss: 0.1716, step time: 0.1070\n",
      "198/223, train_loss: 0.1687, step time: 0.1003\n",
      "199/223, train_loss: 0.1506, step time: 0.1001\n",
      "200/223, train_loss: 0.1754, step time: 0.1028\n",
      "201/223, train_loss: 0.1645, step time: 0.1099\n",
      "202/223, train_loss: 0.1669, step time: 0.1119\n",
      "203/223, train_loss: 0.1549, step time: 0.0999\n",
      "204/223, train_loss: 0.1566, step time: 0.1010\n",
      "205/223, train_loss: 0.1622, step time: 0.1051\n",
      "206/223, train_loss: 0.1542, step time: 0.1125\n",
      "207/223, train_loss: 0.1471, step time: 0.1005\n",
      "208/223, train_loss: 0.1641, step time: 0.1000\n",
      "209/223, train_loss: 0.1467, step time: 0.1011\n",
      "210/223, train_loss: 0.1720, step time: 0.1019\n",
      "211/223, train_loss: 0.1926, step time: 0.1145\n",
      "212/223, train_loss: 0.1729, step time: 0.0999\n",
      "213/223, train_loss: 0.1574, step time: 0.1006\n",
      "214/223, train_loss: 0.1798, step time: 0.1125\n",
      "215/223, train_loss: 0.1642, step time: 0.1001\n",
      "216/223, train_loss: 0.1540, step time: 0.1057\n",
      "217/223, train_loss: 0.1524, step time: 0.0998\n",
      "218/223, train_loss: 0.1534, step time: 0.1006\n",
      "219/223, train_loss: 0.1573, step time: 0.1002\n",
      "220/223, train_loss: 0.1706, step time: 0.0997\n",
      "221/223, train_loss: 0.1635, step time: 0.1001\n",
      "222/223, train_loss: 0.1653, step time: 0.0990\n",
      "223/223, train_loss: 0.1559, step time: 0.0985\n",
      "epoch 21 average loss: 0.1667\n",
      "time consuming of epoch 21 is: 85.6107\n",
      "----------\n",
      "epoch 22/300\n",
      "1/223, train_loss: 0.1746, step time: 0.1064\n",
      "2/223, train_loss: 0.1646, step time: 0.1038\n",
      "3/223, train_loss: 0.1581, step time: 0.1131\n",
      "4/223, train_loss: 0.1588, step time: 0.1104\n",
      "5/223, train_loss: 0.1697, step time: 0.1036\n",
      "6/223, train_loss: 0.1706, step time: 0.1004\n",
      "7/223, train_loss: 0.1834, step time: 0.1080\n",
      "8/223, train_loss: 0.1713, step time: 0.1005\n",
      "9/223, train_loss: 0.1518, step time: 0.1224\n",
      "10/223, train_loss: 0.1545, step time: 0.1076\n",
      "11/223, train_loss: 0.1727, step time: 0.1010\n",
      "12/223, train_loss: 0.1790, step time: 0.1005\n",
      "13/223, train_loss: 0.1565, step time: 0.1054\n",
      "14/223, train_loss: 0.1696, step time: 0.1532\n",
      "15/223, train_loss: 0.1660, step time: 0.1053\n",
      "16/223, train_loss: 0.1655, step time: 0.1126\n",
      "17/223, train_loss: 0.1753, step time: 0.1138\n",
      "18/223, train_loss: 0.1682, step time: 0.1127\n",
      "19/223, train_loss: 0.1590, step time: 0.1005\n",
      "20/223, train_loss: 0.1787, step time: 0.1094\n",
      "21/223, train_loss: 0.1726, step time: 0.1105\n",
      "22/223, train_loss: 0.1512, step time: 0.1026\n",
      "23/223, train_loss: 0.1797, step time: 0.1008\n",
      "24/223, train_loss: 0.1650, step time: 0.1036\n",
      "25/223, train_loss: 0.1697, step time: 0.1087\n",
      "26/223, train_loss: 0.1778, step time: 0.1206\n",
      "27/223, train_loss: 0.1654, step time: 0.1004\n",
      "28/223, train_loss: 0.1555, step time: 0.0996\n",
      "29/223, train_loss: 0.1539, step time: 0.1051\n",
      "30/223, train_loss: 0.3521, step time: 0.1317\n",
      "31/223, train_loss: 0.1515, step time: 0.1117\n",
      "32/223, train_loss: 0.1755, step time: 0.1005\n",
      "33/223, train_loss: 0.1652, step time: 0.1151\n",
      "34/223, train_loss: 0.1661, step time: 0.1166\n",
      "35/223, train_loss: 0.1695, step time: 0.1056\n",
      "36/223, train_loss: 0.1676, step time: 0.1002\n",
      "37/223, train_loss: 0.1732, step time: 0.1112\n",
      "38/223, train_loss: 0.1817, step time: 0.1143\n",
      "39/223, train_loss: 0.1558, step time: 0.1161\n",
      "40/223, train_loss: 0.1648, step time: 0.1100\n",
      "41/223, train_loss: 0.1536, step time: 0.1150\n",
      "42/223, train_loss: 0.1534, step time: 0.1369\n",
      "43/223, train_loss: 0.1848, step time: 0.1050\n",
      "44/223, train_loss: 0.1689, step time: 0.1195\n",
      "45/223, train_loss: 0.1644, step time: 0.1044\n",
      "46/223, train_loss: 0.1580, step time: 0.1197\n",
      "47/223, train_loss: 0.1524, step time: 0.1001\n",
      "48/223, train_loss: 0.2047, step time: 0.1080\n",
      "49/223, train_loss: 0.1740, step time: 0.1183\n",
      "50/223, train_loss: 0.1837, step time: 0.1094\n",
      "51/223, train_loss: 0.1572, step time: 0.1061\n",
      "52/223, train_loss: 0.1637, step time: 0.1161\n",
      "53/223, train_loss: 0.1583, step time: 0.1118\n",
      "54/223, train_loss: 0.1596, step time: 0.1135\n",
      "55/223, train_loss: 0.1606, step time: 0.1059\n",
      "56/223, train_loss: 0.2127, step time: 0.1002\n",
      "57/223, train_loss: 0.1710, step time: 0.1109\n",
      "58/223, train_loss: 0.1603, step time: 0.1210\n",
      "59/223, train_loss: 0.1860, step time: 0.1031\n",
      "60/223, train_loss: 0.1703, step time: 0.1017\n",
      "61/223, train_loss: 0.1721, step time: 0.1117\n",
      "62/223, train_loss: 0.1685, step time: 0.1099\n",
      "63/223, train_loss: 0.1575, step time: 0.1073\n",
      "64/223, train_loss: 0.1692, step time: 0.1273\n",
      "65/223, train_loss: 0.1725, step time: 0.1042\n",
      "66/223, train_loss: 0.1688, step time: 0.1003\n",
      "67/223, train_loss: 0.1782, step time: 0.1006\n",
      "68/223, train_loss: 0.1725, step time: 0.1103\n",
      "69/223, train_loss: 0.1763, step time: 0.1153\n",
      "70/223, train_loss: 0.1792, step time: 0.1193\n",
      "71/223, train_loss: 0.1812, step time: 0.1193\n",
      "72/223, train_loss: 0.1624, step time: 0.0995\n",
      "73/223, train_loss: 0.1650, step time: 0.1071\n",
      "74/223, train_loss: 0.1879, step time: 0.1013\n",
      "75/223, train_loss: 0.1639, step time: 0.1173\n",
      "76/223, train_loss: 0.1609, step time: 0.1069\n",
      "77/223, train_loss: 0.1666, step time: 0.1129\n",
      "78/223, train_loss: 0.1764, step time: 0.1214\n",
      "79/223, train_loss: 0.1593, step time: 0.1149\n",
      "80/223, train_loss: 0.1711, step time: 0.1214\n",
      "81/223, train_loss: 0.1600, step time: 0.1138\n",
      "82/223, train_loss: 0.1516, step time: 0.1106\n",
      "83/223, train_loss: 0.1605, step time: 0.1015\n",
      "84/223, train_loss: 0.1560, step time: 0.1184\n",
      "85/223, train_loss: 0.1559, step time: 0.1181\n",
      "86/223, train_loss: 0.1805, step time: 0.1000\n",
      "87/223, train_loss: 0.1636, step time: 0.1016\n",
      "88/223, train_loss: 0.1513, step time: 0.1071\n",
      "89/223, train_loss: 0.1547, step time: 0.1218\n",
      "90/223, train_loss: 0.1736, step time: 0.1206\n",
      "91/223, train_loss: 0.1687, step time: 0.1047\n",
      "92/223, train_loss: 0.1726, step time: 0.0998\n",
      "93/223, train_loss: 0.1586, step time: 0.1146\n",
      "94/223, train_loss: 0.1692, step time: 0.1011\n",
      "95/223, train_loss: 0.1645, step time: 0.1057\n",
      "96/223, train_loss: 0.1698, step time: 0.1124\n",
      "97/223, train_loss: 0.1859, step time: 0.1020\n",
      "98/223, train_loss: 0.1688, step time: 0.1168\n",
      "99/223, train_loss: 0.1649, step time: 0.1136\n",
      "100/223, train_loss: 0.1638, step time: 0.1059\n",
      "101/223, train_loss: 0.1709, step time: 0.1060\n",
      "102/223, train_loss: 0.1429, step time: 0.0999\n",
      "103/223, train_loss: 0.1549, step time: 0.1062\n",
      "104/223, train_loss: 0.1604, step time: 0.0999\n",
      "105/223, train_loss: 0.1563, step time: 0.1147\n",
      "106/223, train_loss: 0.1607, step time: 0.1186\n",
      "107/223, train_loss: 0.1593, step time: 0.1036\n",
      "108/223, train_loss: 0.1596, step time: 0.1073\n",
      "109/223, train_loss: 0.1558, step time: 0.1076\n",
      "110/223, train_loss: 0.1817, step time: 0.1018\n",
      "111/223, train_loss: 0.1539, step time: 0.0996\n",
      "112/223, train_loss: 0.1524, step time: 0.1005\n",
      "113/223, train_loss: 0.1682, step time: 0.1004\n",
      "114/223, train_loss: 0.1513, step time: 0.1000\n",
      "115/223, train_loss: 0.1677, step time: 0.1211\n",
      "116/223, train_loss: 0.1769, step time: 0.1072\n",
      "117/223, train_loss: 0.1747, step time: 0.1019\n",
      "118/223, train_loss: 0.1719, step time: 0.1121\n",
      "119/223, train_loss: 0.1649, step time: 0.1174\n",
      "120/223, train_loss: 0.1764, step time: 0.1129\n",
      "121/223, train_loss: 0.1569, step time: 0.1112\n",
      "122/223, train_loss: 0.1574, step time: 0.1001\n",
      "123/223, train_loss: 0.1512, step time: 0.1144\n",
      "124/223, train_loss: 0.1608, step time: 0.1034\n",
      "125/223, train_loss: 0.1576, step time: 0.1095\n",
      "126/223, train_loss: 0.1619, step time: 0.0991\n",
      "127/223, train_loss: 0.1641, step time: 0.1182\n",
      "128/223, train_loss: 0.1427, step time: 0.1015\n",
      "129/223, train_loss: 0.1740, step time: 0.1197\n",
      "130/223, train_loss: 0.1639, step time: 0.1040\n",
      "131/223, train_loss: 0.1674, step time: 0.1050\n",
      "132/223, train_loss: 0.1906, step time: 0.1187\n",
      "133/223, train_loss: 0.1659, step time: 0.1104\n",
      "134/223, train_loss: 0.1626, step time: 0.0999\n",
      "135/223, train_loss: 0.1557, step time: 0.1121\n",
      "136/223, train_loss: 0.1784, step time: 0.1225\n",
      "137/223, train_loss: 0.1551, step time: 0.1045\n",
      "138/223, train_loss: 0.1702, step time: 0.1218\n",
      "139/223, train_loss: 0.1681, step time: 0.1126\n",
      "140/223, train_loss: 0.1510, step time: 0.1142\n",
      "141/223, train_loss: 0.1863, step time: 0.0992\n",
      "142/223, train_loss: 0.1638, step time: 0.1094\n",
      "143/223, train_loss: 0.1627, step time: 0.1082\n",
      "144/223, train_loss: 0.1620, step time: 0.1185\n",
      "145/223, train_loss: 0.1697, step time: 0.1003\n",
      "146/223, train_loss: 0.1486, step time: 0.1017\n",
      "147/223, train_loss: 0.1774, step time: 0.1003\n",
      "148/223, train_loss: 0.1479, step time: 0.1231\n",
      "149/223, train_loss: 0.1729, step time: 0.1009\n",
      "150/223, train_loss: 0.1493, step time: 0.1015\n",
      "151/223, train_loss: 0.1562, step time: 0.1001\n",
      "152/223, train_loss: 0.1693, step time: 0.1411\n",
      "153/223, train_loss: 0.1552, step time: 0.1016\n",
      "154/223, train_loss: 0.1443, step time: 0.1018\n",
      "155/223, train_loss: 0.1576, step time: 0.1001\n",
      "156/223, train_loss: 0.1688, step time: 0.1002\n",
      "157/223, train_loss: 0.1670, step time: 0.0995\n",
      "158/223, train_loss: 0.1540, step time: 0.0995\n",
      "159/223, train_loss: 0.1745, step time: 0.0997\n",
      "160/223, train_loss: 0.1615, step time: 0.1004\n",
      "161/223, train_loss: 0.1652, step time: 0.1015\n",
      "162/223, train_loss: 0.1642, step time: 0.1238\n",
      "163/223, train_loss: 0.1535, step time: 0.1135\n",
      "164/223, train_loss: 0.1570, step time: 0.1008\n",
      "165/223, train_loss: 0.1547, step time: 0.1057\n",
      "166/223, train_loss: 0.1612, step time: 0.1013\n",
      "167/223, train_loss: 0.1657, step time: 0.1177\n",
      "168/223, train_loss: 0.1798, step time: 0.1002\n",
      "169/223, train_loss: 0.1810, step time: 0.1005\n",
      "170/223, train_loss: 0.1526, step time: 0.1002\n",
      "171/223, train_loss: 0.1549, step time: 0.0999\n",
      "172/223, train_loss: 0.1801, step time: 0.1290\n",
      "173/223, train_loss: 0.1597, step time: 0.1077\n",
      "174/223, train_loss: 0.1610, step time: 0.1003\n",
      "175/223, train_loss: 0.1660, step time: 0.1087\n",
      "176/223, train_loss: 0.1521, step time: 0.1164\n",
      "177/223, train_loss: 0.1562, step time: 0.1024\n",
      "178/223, train_loss: 0.1588, step time: 0.1006\n",
      "179/223, train_loss: 0.1568, step time: 0.1125\n",
      "180/223, train_loss: 0.1715, step time: 0.1060\n",
      "181/223, train_loss: 0.1602, step time: 0.1077\n",
      "182/223, train_loss: 0.1483, step time: 0.0998\n",
      "183/223, train_loss: 0.1665, step time: 0.0990\n",
      "184/223, train_loss: 0.1652, step time: 0.1077\n",
      "185/223, train_loss: 0.1565, step time: 0.0995\n",
      "186/223, train_loss: 0.1612, step time: 0.1066\n",
      "187/223, train_loss: 0.1554, step time: 0.1067\n",
      "188/223, train_loss: 0.1562, step time: 0.1009\n",
      "189/223, train_loss: 0.1692, step time: 0.0994\n",
      "190/223, train_loss: 0.1555, step time: 0.1120\n",
      "191/223, train_loss: 0.1667, step time: 0.1000\n",
      "192/223, train_loss: 0.1625, step time: 0.0996\n",
      "193/223, train_loss: 0.1613, step time: 0.0998\n",
      "194/223, train_loss: 0.1706, step time: 0.1068\n",
      "195/223, train_loss: 0.1614, step time: 0.0998\n",
      "196/223, train_loss: 0.1584, step time: 0.1043\n",
      "197/223, train_loss: 0.1399, step time: 0.1002\n",
      "198/223, train_loss: 0.1576, step time: 0.1098\n",
      "199/223, train_loss: 0.1461, step time: 0.1012\n",
      "200/223, train_loss: 0.1586, step time: 0.1125\n",
      "201/223, train_loss: 0.1612, step time: 0.1037\n",
      "202/223, train_loss: 0.1503, step time: 0.1338\n",
      "203/223, train_loss: 0.1708, step time: 0.0989\n",
      "204/223, train_loss: 0.1473, step time: 0.1022\n",
      "205/223, train_loss: 0.1600, step time: 0.1129\n",
      "206/223, train_loss: 0.1572, step time: 0.1005\n",
      "207/223, train_loss: 0.1450, step time: 0.1107\n",
      "208/223, train_loss: 0.1555, step time: 0.1096\n",
      "209/223, train_loss: 0.1442, step time: 0.1021\n",
      "210/223, train_loss: 0.1576, step time: 0.1115\n",
      "211/223, train_loss: 0.1728, step time: 0.1211\n",
      "212/223, train_loss: 0.1871, step time: 0.1005\n",
      "213/223, train_loss: 0.1705, step time: 0.1004\n",
      "214/223, train_loss: 0.1735, step time: 0.1003\n",
      "215/223, train_loss: 0.1769, step time: 0.1072\n",
      "216/223, train_loss: 0.1898, step time: 0.1027\n",
      "217/223, train_loss: 0.1622, step time: 0.1035\n",
      "218/223, train_loss: 0.1713, step time: 0.1169\n",
      "219/223, train_loss: 0.1636, step time: 0.1001\n",
      "220/223, train_loss: 0.1465, step time: 0.0987\n",
      "221/223, train_loss: 0.1764, step time: 0.0989\n",
      "222/223, train_loss: 0.1727, step time: 0.0998\n",
      "223/223, train_loss: 0.1745, step time: 0.0998\n",
      "epoch 22 average loss: 0.1657\n",
      "time consuming of epoch 22 is: 86.1392\n",
      "----------\n",
      "epoch 23/300\n",
      "1/223, train_loss: 0.1699, step time: 0.1144\n",
      "2/223, train_loss: 0.1647, step time: 0.1259\n",
      "3/223, train_loss: 0.1553, step time: 0.1085\n",
      "4/223, train_loss: 0.1622, step time: 0.1077\n",
      "5/223, train_loss: 0.1648, step time: 0.1243\n",
      "6/223, train_loss: 0.1570, step time: 0.1114\n",
      "7/223, train_loss: 0.1577, step time: 0.1138\n",
      "8/223, train_loss: 0.1680, step time: 0.1153\n",
      "9/223, train_loss: 0.1462, step time: 0.0998\n",
      "10/223, train_loss: 0.1473, step time: 0.1142\n",
      "11/223, train_loss: 0.1723, step time: 0.0998\n",
      "12/223, train_loss: 0.1613, step time: 0.1151\n",
      "13/223, train_loss: 0.1650, step time: 0.1058\n",
      "14/223, train_loss: 0.1732, step time: 0.1126\n",
      "15/223, train_loss: 0.1581, step time: 0.1005\n",
      "16/223, train_loss: 0.1603, step time: 0.1112\n",
      "17/223, train_loss: 0.1664, step time: 0.1108\n",
      "18/223, train_loss: 0.1607, step time: 0.1106\n",
      "19/223, train_loss: 0.1811, step time: 0.1098\n",
      "20/223, train_loss: 0.1634, step time: 0.1109\n",
      "21/223, train_loss: 0.1812, step time: 0.1011\n",
      "22/223, train_loss: 0.1816, step time: 0.1164\n",
      "23/223, train_loss: 0.1581, step time: 0.1094\n",
      "24/223, train_loss: 0.1765, step time: 0.1073\n",
      "25/223, train_loss: 0.1846, step time: 0.1013\n",
      "26/223, train_loss: 0.1612, step time: 0.1293\n",
      "27/223, train_loss: 0.1744, step time: 0.1282\n",
      "28/223, train_loss: 0.1670, step time: 0.1078\n",
      "29/223, train_loss: 0.1735, step time: 0.1021\n",
      "30/223, train_loss: 0.1574, step time: 0.1099\n",
      "31/223, train_loss: 0.1662, step time: 0.0997\n",
      "32/223, train_loss: 0.1774, step time: 0.0982\n",
      "33/223, train_loss: 0.1571, step time: 0.0990\n",
      "34/223, train_loss: 0.1714, step time: 0.1091\n",
      "35/223, train_loss: 0.1563, step time: 0.1127\n",
      "36/223, train_loss: 0.1607, step time: 0.1153\n",
      "37/223, train_loss: 0.1439, step time: 0.1051\n",
      "38/223, train_loss: 0.1714, step time: 0.1029\n",
      "39/223, train_loss: 0.1898, step time: 0.1008\n",
      "40/223, train_loss: 0.1578, step time: 0.1203\n",
      "41/223, train_loss: 0.1616, step time: 0.1081\n",
      "42/223, train_loss: 0.1572, step time: 0.1175\n",
      "43/223, train_loss: 0.1706, step time: 0.1411\n",
      "44/223, train_loss: 0.1681, step time: 0.1076\n",
      "45/223, train_loss: 0.1598, step time: 0.1009\n",
      "46/223, train_loss: 0.1675, step time: 0.0995\n",
      "47/223, train_loss: 0.1636, step time: 0.1299\n",
      "48/223, train_loss: 0.1935, step time: 0.1017\n",
      "49/223, train_loss: 0.1535, step time: 0.1142\n",
      "50/223, train_loss: 0.1663, step time: 0.1007\n",
      "51/223, train_loss: 0.1686, step time: 0.1483\n",
      "52/223, train_loss: 0.1551, step time: 0.1116\n",
      "53/223, train_loss: 0.1614, step time: 0.1103\n",
      "54/223, train_loss: 0.1782, step time: 0.1097\n",
      "55/223, train_loss: 0.1543, step time: 0.1014\n",
      "56/223, train_loss: 0.1622, step time: 0.1312\n",
      "57/223, train_loss: 0.1680, step time: 0.1255\n",
      "58/223, train_loss: 0.1755, step time: 0.1181\n",
      "59/223, train_loss: 0.1551, step time: 0.1066\n",
      "60/223, train_loss: 0.1477, step time: 0.1243\n",
      "61/223, train_loss: 0.1534, step time: 0.1029\n",
      "62/223, train_loss: 0.1518, step time: 0.1001\n",
      "63/223, train_loss: 0.1668, step time: 0.1073\n",
      "64/223, train_loss: 0.1558, step time: 0.1072\n",
      "65/223, train_loss: 0.1572, step time: 0.1000\n",
      "66/223, train_loss: 0.1628, step time: 0.1006\n",
      "67/223, train_loss: 0.1616, step time: 0.1194\n",
      "68/223, train_loss: 0.1780, step time: 0.1167\n",
      "69/223, train_loss: 0.1588, step time: 0.1134\n",
      "70/223, train_loss: 0.1618, step time: 0.1118\n",
      "71/223, train_loss: 0.1606, step time: 0.1104\n",
      "72/223, train_loss: 0.1638, step time: 0.1103\n",
      "73/223, train_loss: 0.1478, step time: 0.1053\n",
      "74/223, train_loss: 0.1642, step time: 0.0999\n",
      "75/223, train_loss: 0.1640, step time: 0.1034\n",
      "76/223, train_loss: 0.1652, step time: 0.1145\n",
      "77/223, train_loss: 0.1483, step time: 0.1008\n",
      "78/223, train_loss: 0.1606, step time: 0.1033\n",
      "79/223, train_loss: 0.1646, step time: 0.1054\n",
      "80/223, train_loss: 0.1529, step time: 0.1144\n",
      "81/223, train_loss: 0.1614, step time: 0.1311\n",
      "82/223, train_loss: 0.1544, step time: 0.1043\n",
      "83/223, train_loss: 0.1531, step time: 0.1131\n",
      "84/223, train_loss: 0.1640, step time: 0.1002\n",
      "85/223, train_loss: 0.1818, step time: 0.1158\n",
      "86/223, train_loss: 0.1657, step time: 0.1060\n",
      "87/223, train_loss: 0.1511, step time: 0.1005\n",
      "88/223, train_loss: 0.3439, step time: 0.1167\n",
      "89/223, train_loss: 0.1694, step time: 0.1123\n",
      "90/223, train_loss: 0.1878, step time: 0.1139\n",
      "91/223, train_loss: 0.1544, step time: 0.1080\n",
      "92/223, train_loss: 0.1635, step time: 0.0994\n",
      "93/223, train_loss: 0.1689, step time: 0.1074\n",
      "94/223, train_loss: 0.1877, step time: 0.1306\n",
      "95/223, train_loss: 0.1755, step time: 0.0993\n",
      "96/223, train_loss: 0.1673, step time: 0.0999\n",
      "97/223, train_loss: 0.1493, step time: 0.1000\n",
      "98/223, train_loss: 0.1531, step time: 0.1075\n",
      "99/223, train_loss: 0.1693, step time: 0.1067\n",
      "100/223, train_loss: 0.1515, step time: 0.1002\n",
      "101/223, train_loss: 0.1398, step time: 0.1010\n",
      "102/223, train_loss: 0.1646, step time: 0.1019\n",
      "103/223, train_loss: 0.1602, step time: 0.1004\n",
      "104/223, train_loss: 0.1613, step time: 0.1006\n",
      "105/223, train_loss: 0.1662, step time: 0.1044\n",
      "106/223, train_loss: 0.1571, step time: 0.1164\n",
      "107/223, train_loss: 0.1598, step time: 0.1075\n",
      "108/223, train_loss: 0.1473, step time: 0.1223\n",
      "109/223, train_loss: 0.1675, step time: 0.1101\n",
      "110/223, train_loss: 0.1515, step time: 0.1175\n",
      "111/223, train_loss: 0.1487, step time: 0.0994\n",
      "112/223, train_loss: 0.1445, step time: 0.1000\n",
      "113/223, train_loss: 0.1532, step time: 0.0997\n",
      "114/223, train_loss: 0.1600, step time: 0.1051\n",
      "115/223, train_loss: 0.1556, step time: 0.1000\n",
      "116/223, train_loss: 0.1658, step time: 0.1002\n",
      "117/223, train_loss: 0.1593, step time: 0.1000\n",
      "118/223, train_loss: 0.1583, step time: 0.1010\n",
      "119/223, train_loss: 0.1765, step time: 0.0997\n",
      "120/223, train_loss: 0.1571, step time: 0.0990\n",
      "121/223, train_loss: 0.1562, step time: 0.1006\n",
      "122/223, train_loss: 0.1670, step time: 0.1140\n",
      "123/223, train_loss: 0.1569, step time: 0.1092\n",
      "124/223, train_loss: 0.1671, step time: 0.1136\n",
      "125/223, train_loss: 0.1603, step time: 0.1004\n",
      "126/223, train_loss: 0.1522, step time: 0.0992\n",
      "127/223, train_loss: 0.1603, step time: 0.0989\n",
      "128/223, train_loss: 0.1651, step time: 0.0996\n",
      "129/223, train_loss: 0.1692, step time: 0.1001\n",
      "130/223, train_loss: 0.1764, step time: 0.0996\n",
      "131/223, train_loss: 0.1560, step time: 0.1062\n",
      "132/223, train_loss: 0.1487, step time: 0.1004\n",
      "133/223, train_loss: 0.1466, step time: 0.0994\n",
      "134/223, train_loss: 0.1698, step time: 0.1095\n",
      "135/223, train_loss: 0.1555, step time: 0.0991\n",
      "136/223, train_loss: 0.1510, step time: 0.1200\n",
      "137/223, train_loss: 0.1568, step time: 0.0992\n",
      "138/223, train_loss: 0.1564, step time: 0.1156\n",
      "139/223, train_loss: 0.1658, step time: 0.1071\n",
      "140/223, train_loss: 0.1671, step time: 0.1088\n",
      "141/223, train_loss: 0.1735, step time: 0.1060\n",
      "142/223, train_loss: 0.1623, step time: 0.1070\n",
      "143/223, train_loss: 0.1630, step time: 0.1030\n",
      "144/223, train_loss: 0.1542, step time: 0.1070\n",
      "145/223, train_loss: 0.1695, step time: 0.1190\n",
      "146/223, train_loss: 0.1572, step time: 0.1073\n",
      "147/223, train_loss: 0.1500, step time: 0.1128\n",
      "148/223, train_loss: 0.1598, step time: 0.1197\n",
      "149/223, train_loss: 0.1632, step time: 0.1159\n",
      "150/223, train_loss: 0.1726, step time: 0.1086\n",
      "151/223, train_loss: 0.1618, step time: 0.1070\n",
      "152/223, train_loss: 0.1671, step time: 0.1226\n",
      "153/223, train_loss: 0.1653, step time: 0.1043\n",
      "154/223, train_loss: 0.1525, step time: 0.1087\n",
      "155/223, train_loss: 0.1519, step time: 0.1097\n",
      "156/223, train_loss: 0.1671, step time: 0.1207\n",
      "157/223, train_loss: 0.1748, step time: 0.1115\n",
      "158/223, train_loss: 0.1560, step time: 0.1006\n",
      "159/223, train_loss: 0.1595, step time: 0.1011\n",
      "160/223, train_loss: 0.1681, step time: 0.1007\n",
      "161/223, train_loss: 0.1482, step time: 0.1013\n",
      "162/223, train_loss: 0.1813, step time: 0.1200\n",
      "163/223, train_loss: 0.1719, step time: 0.1215\n",
      "164/223, train_loss: 0.1523, step time: 0.1105\n",
      "165/223, train_loss: 0.1576, step time: 0.1053\n",
      "166/223, train_loss: 0.1621, step time: 0.1012\n",
      "167/223, train_loss: 0.1509, step time: 0.1089\n",
      "168/223, train_loss: 0.1627, step time: 0.1010\n",
      "169/223, train_loss: 0.1658, step time: 0.1079\n",
      "170/223, train_loss: 0.1571, step time: 0.1081\n",
      "171/223, train_loss: 0.1732, step time: 0.1076\n",
      "172/223, train_loss: 0.1689, step time: 0.1202\n",
      "173/223, train_loss: 0.1568, step time: 0.1038\n",
      "174/223, train_loss: 0.1447, step time: 0.1006\n",
      "175/223, train_loss: 0.1653, step time: 0.1009\n",
      "176/223, train_loss: 0.1695, step time: 0.1176\n",
      "177/223, train_loss: 0.1692, step time: 0.1271\n",
      "178/223, train_loss: 0.1699, step time: 0.1000\n",
      "179/223, train_loss: 0.1653, step time: 0.1008\n",
      "180/223, train_loss: 0.1535, step time: 0.1007\n",
      "181/223, train_loss: 0.1690, step time: 0.1251\n",
      "182/223, train_loss: 0.1617, step time: 0.0994\n",
      "183/223, train_loss: 0.1611, step time: 0.0990\n",
      "184/223, train_loss: 0.1629, step time: 0.0994\n",
      "185/223, train_loss: 0.1529, step time: 0.1063\n",
      "186/223, train_loss: 0.1478, step time: 0.1003\n",
      "187/223, train_loss: 0.1560, step time: 0.1028\n",
      "188/223, train_loss: 0.1645, step time: 0.1117\n",
      "189/223, train_loss: 0.1559, step time: 0.1106\n",
      "190/223, train_loss: 0.1646, step time: 0.1261\n",
      "191/223, train_loss: 0.1598, step time: 0.1275\n",
      "192/223, train_loss: 0.1520, step time: 0.1091\n",
      "193/223, train_loss: 0.1838, step time: 0.1041\n",
      "194/223, train_loss: 0.1740, step time: 0.1080\n",
      "195/223, train_loss: 0.1546, step time: 0.1003\n",
      "196/223, train_loss: 0.1681, step time: 0.1250\n",
      "197/223, train_loss: 0.1599, step time: 0.1177\n",
      "198/223, train_loss: 0.1473, step time: 0.1055\n",
      "199/223, train_loss: 0.1488, step time: 0.1006\n",
      "200/223, train_loss: 0.1720, step time: 0.1096\n",
      "201/223, train_loss: 0.1580, step time: 0.1034\n",
      "202/223, train_loss: 0.1568, step time: 0.1053\n",
      "203/223, train_loss: 0.1511, step time: 0.1366\n",
      "204/223, train_loss: 0.1628, step time: 0.1116\n",
      "205/223, train_loss: 0.1631, step time: 0.0999\n",
      "206/223, train_loss: 0.1639, step time: 0.1139\n",
      "207/223, train_loss: 0.1443, step time: 0.1051\n",
      "208/223, train_loss: 0.1567, step time: 0.0992\n",
      "209/223, train_loss: 0.1562, step time: 0.1041\n",
      "210/223, train_loss: 0.1529, step time: 0.1115\n",
      "211/223, train_loss: 0.1667, step time: 0.1128\n",
      "212/223, train_loss: 0.1724, step time: 0.1161\n",
      "213/223, train_loss: 0.1476, step time: 0.0998\n",
      "214/223, train_loss: 0.1603, step time: 0.1058\n",
      "215/223, train_loss: 0.1741, step time: 0.1113\n",
      "216/223, train_loss: 0.1503, step time: 0.1111\n",
      "217/223, train_loss: 0.1740, step time: 0.1138\n",
      "218/223, train_loss: 0.1735, step time: 0.1002\n",
      "219/223, train_loss: 0.1570, step time: 0.1004\n",
      "220/223, train_loss: 0.1581, step time: 0.1002\n",
      "221/223, train_loss: 0.1700, step time: 0.1003\n",
      "222/223, train_loss: 0.1545, step time: 0.0999\n",
      "223/223, train_loss: 0.1544, step time: 0.0994\n",
      "epoch 23 average loss: 0.1629\n",
      "time consuming of epoch 23 is: 87.5382\n",
      "----------\n",
      "epoch 24/300\n",
      "1/223, train_loss: 0.1575, step time: 0.1051\n",
      "2/223, train_loss: 0.1812, step time: 0.1035\n",
      "3/223, train_loss: 0.1766, step time: 0.1054\n",
      "4/223, train_loss: 0.1474, step time: 0.1199\n",
      "5/223, train_loss: 0.1625, step time: 0.1009\n",
      "6/223, train_loss: 0.1651, step time: 0.0998\n",
      "7/223, train_loss: 0.1961, step time: 0.1010\n",
      "8/223, train_loss: 0.1509, step time: 0.1005\n",
      "9/223, train_loss: 0.1460, step time: 0.1118\n",
      "10/223, train_loss: 0.1696, step time: 0.0998\n",
      "11/223, train_loss: 0.1707, step time: 0.1000\n",
      "12/223, train_loss: 0.1503, step time: 0.1006\n",
      "13/223, train_loss: 0.1649, step time: 0.1210\n",
      "14/223, train_loss: 0.1714, step time: 0.1001\n",
      "15/223, train_loss: 0.1570, step time: 0.1019\n",
      "16/223, train_loss: 0.1609, step time: 0.1015\n",
      "17/223, train_loss: 0.1646, step time: 0.1002\n",
      "18/223, train_loss: 0.1731, step time: 0.1016\n",
      "19/223, train_loss: 0.1767, step time: 0.1003\n",
      "20/223, train_loss: 0.1531, step time: 0.1021\n",
      "21/223, train_loss: 0.1606, step time: 0.0993\n",
      "22/223, train_loss: 0.1528, step time: 0.0992\n",
      "23/223, train_loss: 0.1569, step time: 0.0993\n",
      "24/223, train_loss: 0.1652, step time: 0.1132\n",
      "25/223, train_loss: 0.1539, step time: 0.1280\n",
      "26/223, train_loss: 0.1690, step time: 0.1005\n",
      "27/223, train_loss: 0.1721, step time: 0.1003\n",
      "28/223, train_loss: 0.1602, step time: 0.1111\n",
      "29/223, train_loss: 0.1425, step time: 0.1105\n",
      "30/223, train_loss: 0.1581, step time: 0.1086\n",
      "31/223, train_loss: 0.1562, step time: 0.1142\n",
      "32/223, train_loss: 0.1732, step time: 0.1119\n",
      "33/223, train_loss: 0.1512, step time: 0.1026\n",
      "34/223, train_loss: 0.1621, step time: 0.1100\n",
      "35/223, train_loss: 0.1519, step time: 0.1147\n",
      "36/223, train_loss: 0.1528, step time: 0.1219\n",
      "37/223, train_loss: 0.1578, step time: 0.1124\n",
      "38/223, train_loss: 0.1660, step time: 0.1186\n",
      "39/223, train_loss: 0.1518, step time: 0.1038\n",
      "40/223, train_loss: 0.1552, step time: 0.1157\n",
      "41/223, train_loss: 0.1588, step time: 0.1038\n",
      "42/223, train_loss: 0.1605, step time: 0.0989\n",
      "43/223, train_loss: 0.1696, step time: 0.1002\n",
      "44/223, train_loss: 0.1707, step time: 0.1202\n",
      "45/223, train_loss: 0.1516, step time: 0.1011\n",
      "46/223, train_loss: 0.1552, step time: 0.1111\n",
      "47/223, train_loss: 0.1552, step time: 0.1022\n",
      "48/223, train_loss: 0.1372, step time: 0.0995\n",
      "49/223, train_loss: 0.1593, step time: 0.1206\n",
      "50/223, train_loss: 0.1581, step time: 0.1016\n",
      "51/223, train_loss: 0.1464, step time: 0.1002\n",
      "52/223, train_loss: 0.1586, step time: 0.1001\n",
      "53/223, train_loss: 0.1545, step time: 0.1222\n",
      "54/223, train_loss: 0.1634, step time: 0.1001\n",
      "55/223, train_loss: 0.1576, step time: 0.1116\n",
      "56/223, train_loss: 0.1632, step time: 0.1063\n",
      "57/223, train_loss: 0.1512, step time: 0.1042\n",
      "58/223, train_loss: 0.1750, step time: 0.1268\n",
      "59/223, train_loss: 0.1516, step time: 0.1198\n",
      "60/223, train_loss: 0.1584, step time: 0.1055\n",
      "61/223, train_loss: 0.1567, step time: 0.1337\n",
      "62/223, train_loss: 0.1678, step time: 0.1087\n",
      "63/223, train_loss: 0.1493, step time: 0.1434\n",
      "64/223, train_loss: 0.1607, step time: 0.1136\n",
      "65/223, train_loss: 0.1397, step time: 0.1010\n",
      "66/223, train_loss: 0.1681, step time: 0.1129\n",
      "67/223, train_loss: 0.1599, step time: 0.1071\n",
      "68/223, train_loss: 0.1737, step time: 0.1333\n",
      "69/223, train_loss: 0.1565, step time: 0.1076\n",
      "70/223, train_loss: 0.1670, step time: 0.1084\n",
      "71/223, train_loss: 0.1629, step time: 0.1078\n",
      "72/223, train_loss: 0.1695, step time: 0.1111\n",
      "73/223, train_loss: 0.1655, step time: 0.1108\n",
      "74/223, train_loss: 0.1542, step time: 0.1161\n",
      "75/223, train_loss: 0.1698, step time: 0.1503\n",
      "76/223, train_loss: 0.1706, step time: 0.1048\n",
      "77/223, train_loss: 0.1687, step time: 0.1075\n",
      "78/223, train_loss: 0.1635, step time: 0.1062\n",
      "79/223, train_loss: 0.1548, step time: 0.1015\n",
      "80/223, train_loss: 0.1527, step time: 0.1007\n",
      "81/223, train_loss: 0.1620, step time: 0.1002\n",
      "82/223, train_loss: 0.1492, step time: 0.0993\n",
      "83/223, train_loss: 0.1591, step time: 0.0997\n",
      "84/223, train_loss: 0.1704, step time: 0.1056\n",
      "85/223, train_loss: 0.1699, step time: 0.1079\n",
      "86/223, train_loss: 0.1639, step time: 0.1156\n",
      "87/223, train_loss: 0.1730, step time: 0.1253\n",
      "88/223, train_loss: 0.1474, step time: 0.0994\n",
      "89/223, train_loss: 0.1541, step time: 0.1010\n",
      "90/223, train_loss: 0.1620, step time: 0.0994\n",
      "91/223, train_loss: 0.1611, step time: 0.1014\n",
      "92/223, train_loss: 0.1690, step time: 0.1683\n",
      "93/223, train_loss: 0.1408, step time: 0.1113\n",
      "94/223, train_loss: 0.1561, step time: 0.1153\n",
      "95/223, train_loss: 0.1638, step time: 0.1083\n",
      "96/223, train_loss: 0.1489, step time: 0.1248\n",
      "97/223, train_loss: 0.1666, step time: 0.1120\n",
      "98/223, train_loss: 0.1555, step time: 0.1188\n",
      "99/223, train_loss: 0.1485, step time: 0.1064\n",
      "100/223, train_loss: 0.1559, step time: 0.1179\n",
      "101/223, train_loss: 0.1622, step time: 0.1098\n",
      "102/223, train_loss: 0.1709, step time: 0.1166\n",
      "103/223, train_loss: 0.1560, step time: 0.1077\n",
      "104/223, train_loss: 0.1654, step time: 0.1244\n",
      "105/223, train_loss: 0.1745, step time: 0.1102\n",
      "106/223, train_loss: 0.1642, step time: 0.1003\n",
      "107/223, train_loss: 0.1531, step time: 0.1002\n",
      "108/223, train_loss: 0.1609, step time: 0.1002\n",
      "109/223, train_loss: 0.1566, step time: 0.1086\n",
      "110/223, train_loss: 0.1520, step time: 0.1151\n",
      "111/223, train_loss: 0.1449, step time: 0.1162\n",
      "112/223, train_loss: 0.1545, step time: 0.0994\n",
      "113/223, train_loss: 0.1522, step time: 0.1097\n",
      "114/223, train_loss: 0.1654, step time: 0.0999\n",
      "115/223, train_loss: 0.1647, step time: 0.1023\n",
      "116/223, train_loss: 0.1710, step time: 0.1076\n",
      "117/223, train_loss: 0.1779, step time: 0.1133\n",
      "118/223, train_loss: 0.1614, step time: 0.1042\n",
      "119/223, train_loss: 0.1574, step time: 0.1063\n",
      "120/223, train_loss: 0.1655, step time: 0.1083\n",
      "121/223, train_loss: 0.1506, step time: 0.1097\n",
      "122/223, train_loss: 0.1565, step time: 0.1004\n",
      "123/223, train_loss: 0.1421, step time: 0.0996\n",
      "124/223, train_loss: 0.1678, step time: 0.1034\n",
      "125/223, train_loss: 0.1659, step time: 0.0999\n",
      "126/223, train_loss: 0.1607, step time: 0.0998\n",
      "127/223, train_loss: 0.1583, step time: 0.1019\n",
      "128/223, train_loss: 0.1777, step time: 0.1159\n",
      "129/223, train_loss: 0.1536, step time: 0.1057\n",
      "130/223, train_loss: 0.1688, step time: 0.1258\n",
      "131/223, train_loss: 0.1478, step time: 0.1262\n",
      "132/223, train_loss: 0.1696, step time: 0.1156\n",
      "133/223, train_loss: 0.1674, step time: 0.1107\n",
      "134/223, train_loss: 0.1748, step time: 0.1267\n",
      "135/223, train_loss: 0.1670, step time: 0.1184\n",
      "136/223, train_loss: 0.1727, step time: 0.1006\n",
      "137/223, train_loss: 0.3451, step time: 0.1113\n",
      "138/223, train_loss: 0.1542, step time: 0.1106\n",
      "139/223, train_loss: 0.1728, step time: 0.1211\n",
      "140/223, train_loss: 0.1799, step time: 0.1056\n",
      "141/223, train_loss: 0.1601, step time: 0.1058\n",
      "142/223, train_loss: 0.1583, step time: 0.1006\n",
      "143/223, train_loss: 0.1679, step time: 0.1185\n",
      "144/223, train_loss: 0.1482, step time: 0.1112\n",
      "145/223, train_loss: 0.1505, step time: 0.1181\n",
      "146/223, train_loss: 0.1712, step time: 0.1164\n",
      "147/223, train_loss: 0.1530, step time: 0.1029\n",
      "148/223, train_loss: 0.1557, step time: 0.1174\n",
      "149/223, train_loss: 0.1617, step time: 0.0996\n",
      "150/223, train_loss: 0.1513, step time: 0.1004\n",
      "151/223, train_loss: 0.1563, step time: 0.1005\n",
      "152/223, train_loss: 0.1636, step time: 0.1014\n",
      "153/223, train_loss: 0.1670, step time: 0.0996\n",
      "154/223, train_loss: 0.1503, step time: 0.1002\n",
      "155/223, train_loss: 0.1591, step time: 0.1007\n",
      "156/223, train_loss: 0.1442, step time: 0.1067\n",
      "157/223, train_loss: 0.1543, step time: 0.1002\n",
      "158/223, train_loss: 0.1545, step time: 0.0994\n",
      "159/223, train_loss: 0.1612, step time: 0.1005\n",
      "160/223, train_loss: 0.1712, step time: 0.1013\n",
      "161/223, train_loss: 0.1612, step time: 0.0998\n",
      "162/223, train_loss: 0.1489, step time: 0.1004\n",
      "163/223, train_loss: 0.1542, step time: 0.1006\n",
      "164/223, train_loss: 0.1546, step time: 0.1137\n",
      "165/223, train_loss: 0.1515, step time: 0.1141\n",
      "166/223, train_loss: 0.1483, step time: 0.1130\n",
      "167/223, train_loss: 0.1574, step time: 0.1150\n",
      "168/223, train_loss: 0.1586, step time: 0.1221\n",
      "169/223, train_loss: 0.1543, step time: 0.1007\n",
      "170/223, train_loss: 0.1625, step time: 0.1008\n",
      "171/223, train_loss: 0.1535, step time: 0.0995\n",
      "172/223, train_loss: 0.1735, step time: 0.1000\n",
      "173/223, train_loss: 0.1724, step time: 0.1207\n",
      "174/223, train_loss: 0.1682, step time: 0.1037\n",
      "175/223, train_loss: 0.1645, step time: 0.1117\n",
      "176/223, train_loss: 0.1491, step time: 0.1006\n",
      "177/223, train_loss: 0.1438, step time: 0.1111\n",
      "178/223, train_loss: 0.1798, step time: 0.1044\n",
      "179/223, train_loss: 0.1525, step time: 0.0997\n",
      "180/223, train_loss: 0.1738, step time: 0.1102\n",
      "181/223, train_loss: 0.1386, step time: 0.1090\n",
      "182/223, train_loss: 0.1405, step time: 0.1006\n",
      "183/223, train_loss: 0.1506, step time: 0.1007\n",
      "184/223, train_loss: 0.1592, step time: 0.1064\n",
      "185/223, train_loss: 0.1561, step time: 0.0999\n",
      "186/223, train_loss: 0.1439, step time: 0.1202\n",
      "187/223, train_loss: 0.1584, step time: 0.1001\n",
      "188/223, train_loss: 0.1569, step time: 0.1079\n",
      "189/223, train_loss: 0.1596, step time: 0.1120\n",
      "190/223, train_loss: 0.1621, step time: 0.1097\n",
      "191/223, train_loss: 0.1666, step time: 0.1211\n",
      "192/223, train_loss: 0.1852, step time: 0.1089\n",
      "193/223, train_loss: 0.1646, step time: 0.1182\n",
      "194/223, train_loss: 0.1603, step time: 0.1004\n",
      "195/223, train_loss: 0.1697, step time: 0.0999\n",
      "196/223, train_loss: 0.1571, step time: 0.1020\n",
      "197/223, train_loss: 0.1502, step time: 0.1163\n",
      "198/223, train_loss: 0.1471, step time: 0.1068\n",
      "199/223, train_loss: 0.1573, step time: 0.1049\n",
      "200/223, train_loss: 0.1755, step time: 0.1003\n",
      "201/223, train_loss: 0.1690, step time: 0.1258\n",
      "202/223, train_loss: 0.1446, step time: 0.1097\n",
      "203/223, train_loss: 0.1681, step time: 0.1024\n",
      "204/223, train_loss: 0.1638, step time: 0.1003\n",
      "205/223, train_loss: 0.1580, step time: 0.1189\n",
      "206/223, train_loss: 0.1538, step time: 0.1106\n",
      "207/223, train_loss: 0.1587, step time: 0.1204\n",
      "208/223, train_loss: 0.1431, step time: 0.1068\n",
      "209/223, train_loss: 0.1484, step time: 0.1023\n",
      "210/223, train_loss: 0.1413, step time: 0.1025\n",
      "211/223, train_loss: 0.1413, step time: 0.1090\n",
      "212/223, train_loss: 0.1693, step time: 0.1151\n",
      "213/223, train_loss: 0.1443, step time: 0.1273\n",
      "214/223, train_loss: 0.1503, step time: 0.1067\n",
      "215/223, train_loss: 0.1829, step time: 0.1056\n",
      "216/223, train_loss: 0.1558, step time: 0.1005\n",
      "217/223, train_loss: 0.1556, step time: 0.1085\n",
      "218/223, train_loss: 0.1593, step time: 0.1207\n",
      "219/223, train_loss: 0.1689, step time: 0.1208\n",
      "220/223, train_loss: 0.1608, step time: 0.1204\n",
      "221/223, train_loss: 0.1464, step time: 0.0998\n",
      "222/223, train_loss: 0.1738, step time: 0.0996\n",
      "223/223, train_loss: 0.1586, step time: 0.0994\n",
      "epoch 24 average loss: 0.1607\n",
      "time consuming of epoch 24 is: 90.9416\n",
      "----------\n",
      "epoch 25/300\n",
      "1/223, train_loss: 0.1640, step time: 0.1019\n",
      "2/223, train_loss: 0.1599, step time: 0.1119\n",
      "3/223, train_loss: 0.1498, step time: 0.1062\n",
      "4/223, train_loss: 0.1541, step time: 0.1219\n",
      "5/223, train_loss: 0.1536, step time: 0.1168\n",
      "6/223, train_loss: 0.1700, step time: 0.1139\n",
      "7/223, train_loss: 0.1605, step time: 0.1292\n",
      "8/223, train_loss: 0.1610, step time: 0.1117\n",
      "9/223, train_loss: 0.1546, step time: 0.1192\n",
      "10/223, train_loss: 0.1619, step time: 0.1168\n",
      "11/223, train_loss: 0.1494, step time: 0.1148\n",
      "12/223, train_loss: 0.1472, step time: 0.1001\n",
      "13/223, train_loss: 0.1520, step time: 0.1260\n",
      "14/223, train_loss: 0.1473, step time: 0.1022\n",
      "15/223, train_loss: 0.1590, step time: 0.1116\n",
      "16/223, train_loss: 0.1552, step time: 0.1001\n",
      "17/223, train_loss: 0.1648, step time: 0.1124\n",
      "18/223, train_loss: 0.1590, step time: 0.1148\n",
      "19/223, train_loss: 0.1505, step time: 0.1016\n",
      "20/223, train_loss: 0.1472, step time: 0.1020\n",
      "21/223, train_loss: 0.1818, step time: 0.1152\n",
      "22/223, train_loss: 0.1585, step time: 0.1004\n",
      "23/223, train_loss: 0.1680, step time: 0.1004\n",
      "24/223, train_loss: 0.1514, step time: 0.1057\n",
      "25/223, train_loss: 0.1553, step time: 0.1210\n",
      "26/223, train_loss: 0.1676, step time: 0.1225\n",
      "27/223, train_loss: 0.1548, step time: 0.1049\n",
      "28/223, train_loss: 0.1686, step time: 0.1034\n",
      "29/223, train_loss: 0.1689, step time: 0.1033\n",
      "30/223, train_loss: 0.1462, step time: 0.1064\n",
      "31/223, train_loss: 0.1558, step time: 0.1058\n",
      "32/223, train_loss: 0.1537, step time: 0.1011\n",
      "33/223, train_loss: 0.1601, step time: 0.1005\n",
      "34/223, train_loss: 0.1515, step time: 0.1074\n",
      "35/223, train_loss: 0.1718, step time: 0.1161\n",
      "36/223, train_loss: 0.1570, step time: 0.1066\n",
      "37/223, train_loss: 0.1563, step time: 0.1160\n",
      "38/223, train_loss: 0.1675, step time: 0.1069\n",
      "39/223, train_loss: 0.1686, step time: 0.1157\n",
      "40/223, train_loss: 0.1661, step time: 0.1188\n",
      "41/223, train_loss: 0.1590, step time: 0.1161\n",
      "42/223, train_loss: 0.1625, step time: 0.1005\n",
      "43/223, train_loss: 0.1507, step time: 0.1003\n",
      "44/223, train_loss: 0.1514, step time: 0.1001\n",
      "45/223, train_loss: 0.1438, step time: 0.1164\n",
      "46/223, train_loss: 0.1664, step time: 0.1184\n",
      "47/223, train_loss: 0.1736, step time: 0.1207\n",
      "48/223, train_loss: 0.1582, step time: 0.0999\n",
      "49/223, train_loss: 0.1422, step time: 0.1005\n",
      "50/223, train_loss: 0.1664, step time: 0.1013\n",
      "51/223, train_loss: 0.1873, step time: 0.1003\n",
      "52/223, train_loss: 0.1704, step time: 0.1006\n",
      "53/223, train_loss: 0.1741, step time: 0.0993\n",
      "54/223, train_loss: 0.1666, step time: 0.1002\n",
      "55/223, train_loss: 0.1484, step time: 0.1010\n",
      "56/223, train_loss: 0.1799, step time: 0.1417\n",
      "57/223, train_loss: 0.1606, step time: 0.1041\n",
      "58/223, train_loss: 0.1776, step time: 0.1068\n",
      "59/223, train_loss: 0.1698, step time: 0.1006\n",
      "60/223, train_loss: 0.1514, step time: 0.1002\n",
      "61/223, train_loss: 0.1451, step time: 0.1092\n",
      "62/223, train_loss: 0.1788, step time: 0.1238\n",
      "63/223, train_loss: 0.1448, step time: 0.1130\n",
      "64/223, train_loss: 0.1529, step time: 0.1210\n",
      "65/223, train_loss: 0.1480, step time: 0.1044\n",
      "66/223, train_loss: 0.1531, step time: 0.1099\n",
      "67/223, train_loss: 0.1644, step time: 0.1087\n",
      "68/223, train_loss: 0.1548, step time: 0.1174\n",
      "69/223, train_loss: 0.1548, step time: 0.1054\n",
      "70/223, train_loss: 0.1556, step time: 0.1165\n",
      "71/223, train_loss: 0.1367, step time: 0.1007\n",
      "72/223, train_loss: 0.1664, step time: 0.1008\n",
      "73/223, train_loss: 0.1600, step time: 0.1233\n",
      "74/223, train_loss: 0.1724, step time: 0.0994\n",
      "75/223, train_loss: 0.1509, step time: 0.0995\n",
      "76/223, train_loss: 0.1587, step time: 0.1044\n",
      "77/223, train_loss: 0.1320, step time: 0.1097\n",
      "78/223, train_loss: 0.1528, step time: 0.1157\n",
      "79/223, train_loss: 0.1437, step time: 0.1005\n",
      "80/223, train_loss: 0.1546, step time: 0.1011\n",
      "81/223, train_loss: 0.1474, step time: 0.1143\n",
      "82/223, train_loss: 0.1495, step time: 0.1003\n",
      "83/223, train_loss: 0.1501, step time: 0.1124\n",
      "84/223, train_loss: 0.1817, step time: 0.1013\n",
      "85/223, train_loss: 0.1462, step time: 0.1540\n",
      "86/223, train_loss: 0.1460, step time: 0.1049\n",
      "87/223, train_loss: 0.1613, step time: 0.1133\n",
      "88/223, train_loss: 0.1499, step time: 0.1010\n",
      "89/223, train_loss: 0.1591, step time: 0.1050\n",
      "90/223, train_loss: 0.1542, step time: 0.1275\n",
      "91/223, train_loss: 0.1546, step time: 0.1322\n",
      "92/223, train_loss: 0.1644, step time: 0.1132\n",
      "93/223, train_loss: 0.1642, step time: 0.1084\n",
      "94/223, train_loss: 0.1445, step time: 0.1006\n",
      "95/223, train_loss: 0.1535, step time: 0.1171\n",
      "96/223, train_loss: 0.1770, step time: 0.1049\n",
      "97/223, train_loss: 0.1508, step time: 0.1030\n",
      "98/223, train_loss: 0.1679, step time: 0.1123\n",
      "99/223, train_loss: 0.1584, step time: 0.1087\n",
      "100/223, train_loss: 0.1355, step time: 0.1085\n",
      "101/223, train_loss: 0.1644, step time: 0.1060\n",
      "102/223, train_loss: 0.1586, step time: 0.1040\n",
      "103/223, train_loss: 0.1884, step time: 0.1002\n",
      "104/223, train_loss: 0.1503, step time: 0.1225\n",
      "105/223, train_loss: 0.1607, step time: 0.1203\n",
      "106/223, train_loss: 0.1622, step time: 0.1026\n",
      "107/223, train_loss: 0.1534, step time: 0.1033\n",
      "108/223, train_loss: 0.1736, step time: 0.1187\n",
      "109/223, train_loss: 0.1580, step time: 0.1100\n",
      "110/223, train_loss: 0.1713, step time: 0.1022\n",
      "111/223, train_loss: 0.1756, step time: 0.1172\n",
      "112/223, train_loss: 0.1604, step time: 0.0997\n",
      "113/223, train_loss: 0.1748, step time: 0.1097\n",
      "114/223, train_loss: 0.1484, step time: 0.1239\n",
      "115/223, train_loss: 0.1734, step time: 0.1060\n",
      "116/223, train_loss: 0.1509, step time: 0.1026\n",
      "117/223, train_loss: 0.1694, step time: 0.1129\n",
      "118/223, train_loss: 0.1545, step time: 0.1087\n",
      "119/223, train_loss: 0.1669, step time: 0.0995\n",
      "120/223, train_loss: 0.1616, step time: 0.0988\n",
      "121/223, train_loss: 0.1663, step time: 0.1180\n",
      "122/223, train_loss: 0.1694, step time: 0.1113\n",
      "123/223, train_loss: 0.1734, step time: 0.1033\n",
      "124/223, train_loss: 0.1609, step time: 0.1010\n",
      "125/223, train_loss: 0.1625, step time: 0.1106\n",
      "126/223, train_loss: 0.1683, step time: 0.1000\n",
      "127/223, train_loss: 0.1508, step time: 0.1010\n",
      "128/223, train_loss: 0.1707, step time: 0.1202\n",
      "129/223, train_loss: 0.1680, step time: 0.1060\n",
      "130/223, train_loss: 0.1506, step time: 0.0999\n",
      "131/223, train_loss: 0.1625, step time: 0.1004\n",
      "132/223, train_loss: 0.1811, step time: 0.1090\n",
      "133/223, train_loss: 0.1570, step time: 0.1007\n",
      "134/223, train_loss: 0.1554, step time: 0.1125\n",
      "135/223, train_loss: 0.1730, step time: 0.1149\n",
      "136/223, train_loss: 0.1620, step time: 0.1006\n",
      "137/223, train_loss: 0.1523, step time: 0.1189\n",
      "138/223, train_loss: 0.1693, step time: 0.1183\n",
      "139/223, train_loss: 0.1546, step time: 0.1149\n",
      "140/223, train_loss: 0.1470, step time: 0.1073\n",
      "141/223, train_loss: 0.1378, step time: 0.1042\n",
      "142/223, train_loss: 0.1703, step time: 0.1199\n",
      "143/223, train_loss: 0.1390, step time: 0.1098\n",
      "144/223, train_loss: 0.1523, step time: 0.1244\n",
      "145/223, train_loss: 0.1526, step time: 0.1112\n",
      "146/223, train_loss: 0.1410, step time: 0.1005\n",
      "147/223, train_loss: 0.1698, step time: 0.1000\n",
      "148/223, train_loss: 0.1711, step time: 0.1377\n",
      "149/223, train_loss: 0.1515, step time: 0.1021\n",
      "150/223, train_loss: 0.1563, step time: 0.1001\n",
      "151/223, train_loss: 0.1588, step time: 0.1018\n",
      "152/223, train_loss: 0.1638, step time: 0.1166\n",
      "153/223, train_loss: 0.1477, step time: 0.1033\n",
      "154/223, train_loss: 0.1676, step time: 0.1356\n",
      "155/223, train_loss: 0.1728, step time: 0.1340\n",
      "156/223, train_loss: 0.1781, step time: 0.1189\n",
      "157/223, train_loss: 0.1603, step time: 0.1146\n",
      "158/223, train_loss: 0.1751, step time: 0.1330\n",
      "159/223, train_loss: 0.1593, step time: 0.1198\n",
      "160/223, train_loss: 0.1628, step time: 0.0999\n",
      "161/223, train_loss: 0.1417, step time: 0.1178\n",
      "162/223, train_loss: 0.1519, step time: 0.1003\n",
      "163/223, train_loss: 0.1533, step time: 0.1000\n",
      "164/223, train_loss: 0.1404, step time: 0.1033\n",
      "165/223, train_loss: 0.1573, step time: 0.1243\n",
      "166/223, train_loss: 0.1429, step time: 0.1102\n",
      "167/223, train_loss: 0.1534, step time: 0.1088\n",
      "168/223, train_loss: 0.1659, step time: 0.1079\n",
      "169/223, train_loss: 0.1528, step time: 0.1133\n",
      "170/223, train_loss: 0.1579, step time: 0.1231\n",
      "171/223, train_loss: 0.1569, step time: 0.1082\n",
      "172/223, train_loss: 0.1452, step time: 0.1438\n",
      "173/223, train_loss: 0.1720, step time: 0.1176\n",
      "174/223, train_loss: 0.1469, step time: 0.1013\n",
      "175/223, train_loss: 0.1446, step time: 0.1013\n",
      "176/223, train_loss: 0.1640, step time: 0.1126\n",
      "177/223, train_loss: 0.1428, step time: 0.1058\n",
      "178/223, train_loss: 0.1602, step time: 0.1160\n",
      "179/223, train_loss: 0.1488, step time: 0.1130\n",
      "180/223, train_loss: 0.1488, step time: 0.1016\n",
      "181/223, train_loss: 0.1598, step time: 0.1207\n",
      "182/223, train_loss: 0.1575, step time: 0.1012\n",
      "183/223, train_loss: 0.1587, step time: 0.1002\n",
      "184/223, train_loss: 0.1627, step time: 0.1016\n",
      "185/223, train_loss: 0.1651, step time: 0.1065\n",
      "186/223, train_loss: 0.1861, step time: 0.1152\n",
      "187/223, train_loss: 0.1474, step time: 0.1163\n",
      "188/223, train_loss: 0.1413, step time: 0.1056\n",
      "189/223, train_loss: 0.1479, step time: 0.1000\n",
      "190/223, train_loss: 0.1404, step time: 0.1001\n",
      "191/223, train_loss: 0.1594, step time: 0.1004\n",
      "192/223, train_loss: 0.1693, step time: 0.1017\n",
      "193/223, train_loss: 0.1671, step time: 0.1152\n",
      "194/223, train_loss: 0.1799, step time: 0.1003\n",
      "195/223, train_loss: 0.1601, step time: 0.1096\n",
      "196/223, train_loss: 0.1607, step time: 0.1009\n",
      "197/223, train_loss: 0.1652, step time: 0.1007\n",
      "198/223, train_loss: 0.1616, step time: 0.1002\n",
      "199/223, train_loss: 0.1712, step time: 0.1218\n",
      "200/223, train_loss: 0.1395, step time: 0.1187\n",
      "201/223, train_loss: 0.1386, step time: 0.1037\n",
      "202/223, train_loss: 0.1544, step time: 0.1083\n",
      "203/223, train_loss: 0.1621, step time: 0.1113\n",
      "204/223, train_loss: 0.1426, step time: 0.1002\n",
      "205/223, train_loss: 0.1440, step time: 0.1140\n",
      "206/223, train_loss: 0.1443, step time: 0.1045\n",
      "207/223, train_loss: 0.1646, step time: 0.1002\n",
      "208/223, train_loss: 0.1592, step time: 0.1020\n",
      "209/223, train_loss: 0.1550, step time: 0.1122\n",
      "210/223, train_loss: 0.1524, step time: 0.1209\n",
      "211/223, train_loss: 0.1816, step time: 0.1058\n",
      "212/223, train_loss: 0.1608, step time: 0.1001\n",
      "213/223, train_loss: 0.1587, step time: 0.1006\n",
      "214/223, train_loss: 0.3330, step time: 0.0998\n",
      "215/223, train_loss: 0.1608, step time: 0.1233\n",
      "216/223, train_loss: 0.1604, step time: 0.1269\n",
      "217/223, train_loss: 0.1635, step time: 0.1016\n",
      "218/223, train_loss: 0.1454, step time: 0.1026\n",
      "219/223, train_loss: 0.1453, step time: 0.1009\n",
      "220/223, train_loss: 0.1669, step time: 0.1007\n",
      "221/223, train_loss: 0.1546, step time: 0.0987\n",
      "222/223, train_loss: 0.1618, step time: 0.0989\n",
      "223/223, train_loss: 0.1480, step time: 0.0997\n",
      "epoch 25 average loss: 0.1593\n",
      "saved new best metric model\n",
      "current epoch: 25 current mean dice: 0.8103 tc: 0.8958 wt: 0.8276 et: 0.7075\n",
      "best mean dice: 0.8103 at epoch: 25\n",
      "time consuming of epoch 25 is: 88.7730\n",
      "----------\n",
      "epoch 26/300\n",
      "1/223, train_loss: 0.1626, step time: 0.1008\n",
      "2/223, train_loss: 0.1440, step time: 0.1100\n",
      "3/223, train_loss: 0.1508, step time: 0.1166\n",
      "4/223, train_loss: 0.1519, step time: 0.1076\n",
      "5/223, train_loss: 0.1765, step time: 0.1074\n",
      "6/223, train_loss: 0.1550, step time: 0.1071\n",
      "7/223, train_loss: 0.1585, step time: 0.1069\n",
      "8/223, train_loss: 0.1575, step time: 0.1109\n",
      "9/223, train_loss: 0.1737, step time: 0.1007\n",
      "10/223, train_loss: 0.1626, step time: 0.1005\n",
      "11/223, train_loss: 0.1653, step time: 0.1000\n",
      "12/223, train_loss: 0.1593, step time: 0.1009\n",
      "13/223, train_loss: 0.1476, step time: 0.1034\n",
      "14/223, train_loss: 0.1476, step time: 0.1031\n",
      "15/223, train_loss: 0.1514, step time: 0.0995\n",
      "16/223, train_loss: 0.1428, step time: 0.0994\n",
      "17/223, train_loss: 0.1592, step time: 0.1013\n",
      "18/223, train_loss: 0.1482, step time: 0.0995\n",
      "19/223, train_loss: 0.1457, step time: 0.0995\n",
      "20/223, train_loss: 0.1592, step time: 0.1011\n",
      "21/223, train_loss: 0.1669, step time: 0.0999\n",
      "22/223, train_loss: 0.1597, step time: 0.1044\n",
      "23/223, train_loss: 0.1621, step time: 0.1005\n",
      "24/223, train_loss: 0.1555, step time: 0.1005\n",
      "25/223, train_loss: 0.1605, step time: 0.1160\n",
      "26/223, train_loss: 0.1493, step time: 0.1054\n",
      "27/223, train_loss: 0.1596, step time: 0.1001\n",
      "28/223, train_loss: 0.1607, step time: 0.0989\n",
      "29/223, train_loss: 0.1687, step time: 0.1013\n",
      "30/223, train_loss: 0.1610, step time: 0.1182\n",
      "31/223, train_loss: 0.1472, step time: 0.1145\n",
      "32/223, train_loss: 0.1596, step time: 0.1000\n",
      "33/223, train_loss: 0.1513, step time: 0.1003\n",
      "34/223, train_loss: 0.1514, step time: 0.1097\n",
      "35/223, train_loss: 0.1748, step time: 0.0997\n",
      "36/223, train_loss: 0.1639, step time: 0.0996\n",
      "37/223, train_loss: 0.1356, step time: 0.1152\n",
      "38/223, train_loss: 0.1470, step time: 0.1056\n",
      "39/223, train_loss: 0.1579, step time: 0.1001\n",
      "40/223, train_loss: 0.1525, step time: 0.1045\n",
      "41/223, train_loss: 0.1735, step time: 0.1005\n",
      "42/223, train_loss: 0.1764, step time: 0.1060\n",
      "43/223, train_loss: 0.1450, step time: 0.1008\n",
      "44/223, train_loss: 0.1604, step time: 0.0998\n",
      "45/223, train_loss: 0.1435, step time: 0.1072\n",
      "46/223, train_loss: 0.1505, step time: 0.1111\n",
      "47/223, train_loss: 0.1591, step time: 0.0999\n",
      "48/223, train_loss: 0.1440, step time: 0.1036\n",
      "49/223, train_loss: 0.1549, step time: 0.1159\n",
      "50/223, train_loss: 0.3443, step time: 0.1063\n",
      "51/223, train_loss: 0.1713, step time: 0.1030\n",
      "52/223, train_loss: 0.1575, step time: 0.1172\n",
      "53/223, train_loss: 0.1476, step time: 0.1082\n",
      "54/223, train_loss: 0.1659, step time: 0.1160\n",
      "55/223, train_loss: 0.1522, step time: 0.1034\n",
      "56/223, train_loss: 0.1582, step time: 0.1050\n",
      "57/223, train_loss: 0.1569, step time: 0.0999\n",
      "58/223, train_loss: 0.1502, step time: 0.1113\n",
      "59/223, train_loss: 0.1422, step time: 0.0999\n",
      "60/223, train_loss: 0.1466, step time: 0.1007\n",
      "61/223, train_loss: 0.1446, step time: 0.1157\n",
      "62/223, train_loss: 0.1526, step time: 0.1178\n",
      "63/223, train_loss: 0.1545, step time: 0.1197\n",
      "64/223, train_loss: 0.1567, step time: 0.1003\n",
      "65/223, train_loss: 0.1507, step time: 0.1122\n",
      "66/223, train_loss: 0.1417, step time: 0.1003\n",
      "67/223, train_loss: 0.1557, step time: 0.1002\n",
      "68/223, train_loss: 0.1500, step time: 0.0997\n",
      "69/223, train_loss: 0.1496, step time: 0.0999\n",
      "70/223, train_loss: 0.1602, step time: 0.1039\n",
      "71/223, train_loss: 0.1465, step time: 0.1001\n",
      "72/223, train_loss: 0.1564, step time: 0.1009\n",
      "73/223, train_loss: 0.1546, step time: 0.1083\n",
      "74/223, train_loss: 0.1689, step time: 0.1122\n",
      "75/223, train_loss: 0.1639, step time: 0.1106\n",
      "76/223, train_loss: 0.1678, step time: 0.1248\n",
      "77/223, train_loss: 0.1595, step time: 0.1062\n",
      "78/223, train_loss: 0.1647, step time: 0.1002\n",
      "79/223, train_loss: 0.1545, step time: 0.1002\n",
      "80/223, train_loss: 0.1559, step time: 0.1002\n",
      "81/223, train_loss: 0.1665, step time: 0.1115\n",
      "82/223, train_loss: 0.1732, step time: 0.1082\n",
      "83/223, train_loss: 0.1667, step time: 0.1110\n",
      "84/223, train_loss: 0.1571, step time: 0.1305\n",
      "85/223, train_loss: 0.1537, step time: 0.1065\n",
      "86/223, train_loss: 0.1672, step time: 0.1221\n",
      "87/223, train_loss: 0.1613, step time: 0.1173\n",
      "88/223, train_loss: 0.1509, step time: 0.1091\n",
      "89/223, train_loss: 0.1739, step time: 0.1054\n",
      "90/223, train_loss: 0.1774, step time: 0.0989\n",
      "91/223, train_loss: 0.1538, step time: 0.0988\n",
      "92/223, train_loss: 0.1459, step time: 0.0992\n",
      "93/223, train_loss: 0.1583, step time: 0.0987\n",
      "94/223, train_loss: 0.1942, step time: 0.1084\n",
      "95/223, train_loss: 0.1421, step time: 0.1137\n",
      "96/223, train_loss: 0.1527, step time: 0.1071\n",
      "97/223, train_loss: 0.1479, step time: 0.1095\n",
      "98/223, train_loss: 0.1403, step time: 0.1181\n",
      "99/223, train_loss: 0.1689, step time: 0.1235\n",
      "100/223, train_loss: 0.1394, step time: 0.1208\n",
      "101/223, train_loss: 0.1453, step time: 0.1096\n",
      "102/223, train_loss: 0.1540, step time: 0.1131\n",
      "103/223, train_loss: 0.1574, step time: 0.0991\n",
      "104/223, train_loss: 0.1625, step time: 0.0995\n",
      "105/223, train_loss: 0.1567, step time: 0.1070\n",
      "106/223, train_loss: 0.1536, step time: 0.1187\n",
      "107/223, train_loss: 0.1665, step time: 0.1176\n",
      "108/223, train_loss: 0.1339, step time: 0.1036\n",
      "109/223, train_loss: 0.1499, step time: 0.1254\n",
      "110/223, train_loss: 0.1651, step time: 0.1125\n",
      "111/223, train_loss: 0.1378, step time: 0.1096\n",
      "112/223, train_loss: 0.1491, step time: 0.1041\n",
      "113/223, train_loss: 0.1818, step time: 0.1086\n",
      "114/223, train_loss: 0.1493, step time: 0.1176\n",
      "115/223, train_loss: 0.1673, step time: 0.1134\n",
      "116/223, train_loss: 0.1471, step time: 0.0990\n",
      "117/223, train_loss: 0.1669, step time: 0.1062\n",
      "118/223, train_loss: 0.1750, step time: 0.1176\n",
      "119/223, train_loss: 0.1374, step time: 0.1146\n",
      "120/223, train_loss: 0.1595, step time: 0.1237\n",
      "121/223, train_loss: 0.1564, step time: 0.1110\n",
      "122/223, train_loss: 0.1691, step time: 0.0998\n",
      "123/223, train_loss: 0.1623, step time: 0.1003\n",
      "124/223, train_loss: 0.1643, step time: 0.1003\n",
      "125/223, train_loss: 0.1512, step time: 0.1099\n",
      "126/223, train_loss: 0.1457, step time: 0.1075\n",
      "127/223, train_loss: 0.1615, step time: 0.1001\n",
      "128/223, train_loss: 0.1333, step time: 0.1001\n",
      "129/223, train_loss: 0.1465, step time: 0.1062\n",
      "130/223, train_loss: 0.1585, step time: 0.0998\n",
      "131/223, train_loss: 0.1671, step time: 0.1056\n",
      "132/223, train_loss: 0.1482, step time: 0.1002\n",
      "133/223, train_loss: 0.1434, step time: 0.1039\n",
      "134/223, train_loss: 0.1583, step time: 0.1054\n",
      "135/223, train_loss: 0.1554, step time: 0.1002\n",
      "136/223, train_loss: 0.1704, step time: 0.1004\n",
      "137/223, train_loss: 0.1516, step time: 0.0996\n",
      "138/223, train_loss: 0.1523, step time: 0.1141\n",
      "139/223, train_loss: 0.1421, step time: 0.1004\n",
      "140/223, train_loss: 0.1420, step time: 0.1011\n",
      "141/223, train_loss: 0.1534, step time: 0.1003\n",
      "142/223, train_loss: 0.1478, step time: 0.1001\n",
      "143/223, train_loss: 0.1463, step time: 0.1000\n",
      "144/223, train_loss: 0.1518, step time: 0.0999\n",
      "145/223, train_loss: 0.1678, step time: 0.1006\n",
      "146/223, train_loss: 0.1629, step time: 0.0998\n",
      "147/223, train_loss: 0.1649, step time: 0.0994\n",
      "148/223, train_loss: 0.1534, step time: 0.1001\n",
      "149/223, train_loss: 0.1558, step time: 0.1080\n",
      "150/223, train_loss: 0.1499, step time: 0.1144\n",
      "151/223, train_loss: 0.1688, step time: 0.1132\n",
      "152/223, train_loss: 0.1604, step time: 0.1086\n",
      "153/223, train_loss: 0.1440, step time: 0.1131\n",
      "154/223, train_loss: 0.1439, step time: 0.1105\n",
      "155/223, train_loss: 0.1631, step time: 0.1095\n",
      "156/223, train_loss: 0.1658, step time: 0.1217\n",
      "157/223, train_loss: 0.1476, step time: 0.1027\n",
      "158/223, train_loss: 0.1658, step time: 0.1101\n",
      "159/223, train_loss: 0.1432, step time: 0.1097\n",
      "160/223, train_loss: 0.1421, step time: 0.1085\n",
      "161/223, train_loss: 0.1587, step time: 0.1127\n",
      "162/223, train_loss: 0.1662, step time: 0.1179\n",
      "163/223, train_loss: 0.1324, step time: 0.1004\n",
      "164/223, train_loss: 0.1527, step time: 0.1001\n",
      "165/223, train_loss: 0.1426, step time: 0.1228\n",
      "166/223, train_loss: 0.1546, step time: 0.1005\n",
      "167/223, train_loss: 0.1386, step time: 0.1145\n",
      "168/223, train_loss: 0.1513, step time: 0.1289\n",
      "169/223, train_loss: 0.1563, step time: 0.1122\n",
      "170/223, train_loss: 0.1436, step time: 0.1237\n",
      "171/223, train_loss: 0.1361, step time: 0.1094\n",
      "172/223, train_loss: 0.1680, step time: 0.1067\n",
      "173/223, train_loss: 0.1410, step time: 0.1050\n",
      "174/223, train_loss: 0.1397, step time: 0.1001\n",
      "175/223, train_loss: 0.1538, step time: 0.1005\n",
      "176/223, train_loss: 0.1522, step time: 0.0996\n",
      "177/223, train_loss: 0.1503, step time: 0.1054\n",
      "178/223, train_loss: 0.1597, step time: 0.1050\n",
      "179/223, train_loss: 0.1524, step time: 0.1043\n",
      "180/223, train_loss: 0.1769, step time: 0.1144\n",
      "181/223, train_loss: 0.1524, step time: 0.1003\n",
      "182/223, train_loss: 0.1551, step time: 0.1051\n",
      "183/223, train_loss: 0.1383, step time: 0.1096\n",
      "184/223, train_loss: 0.1490, step time: 0.1078\n",
      "185/223, train_loss: 0.1728, step time: 0.1009\n",
      "186/223, train_loss: 0.1601, step time: 0.1042\n",
      "187/223, train_loss: 0.1693, step time: 0.1006\n",
      "188/223, train_loss: 0.1601, step time: 0.1007\n",
      "189/223, train_loss: 0.1534, step time: 0.1007\n",
      "190/223, train_loss: 0.1541, step time: 0.1221\n",
      "191/223, train_loss: 0.1436, step time: 0.1095\n",
      "192/223, train_loss: 0.1590, step time: 0.0998\n",
      "193/223, train_loss: 0.1677, step time: 0.1094\n",
      "194/223, train_loss: 0.1589, step time: 0.1140\n",
      "195/223, train_loss: 0.1669, step time: 0.1002\n",
      "196/223, train_loss: 0.1766, step time: 0.1000\n",
      "197/223, train_loss: 0.1575, step time: 0.0999\n",
      "198/223, train_loss: 0.1592, step time: 0.1120\n",
      "199/223, train_loss: 0.1617, step time: 0.1069\n",
      "200/223, train_loss: 0.1640, step time: 0.1006\n",
      "201/223, train_loss: 0.1511, step time: 0.1001\n",
      "202/223, train_loss: 0.1642, step time: 0.1000\n",
      "203/223, train_loss: 0.1557, step time: 0.1057\n",
      "204/223, train_loss: 0.1442, step time: 0.1103\n",
      "205/223, train_loss: 0.1491, step time: 0.1027\n",
      "206/223, train_loss: 0.1461, step time: 0.1228\n",
      "207/223, train_loss: 0.1532, step time: 0.1133\n",
      "208/223, train_loss: 0.1458, step time: 0.1089\n",
      "209/223, train_loss: 0.1438, step time: 0.1014\n",
      "210/223, train_loss: 0.1405, step time: 0.1302\n",
      "211/223, train_loss: 0.1425, step time: 0.1257\n",
      "212/223, train_loss: 0.1574, step time: 0.1003\n",
      "213/223, train_loss: 0.1666, step time: 0.0996\n",
      "214/223, train_loss: 0.1753, step time: 0.1175\n",
      "215/223, train_loss: 0.1607, step time: 0.1003\n",
      "216/223, train_loss: 0.1550, step time: 0.0999\n",
      "217/223, train_loss: 0.1605, step time: 0.1001\n",
      "218/223, train_loss: 0.1422, step time: 0.1031\n",
      "219/223, train_loss: 0.1478, step time: 0.1075\n",
      "220/223, train_loss: 0.1416, step time: 0.1010\n",
      "221/223, train_loss: 0.1669, step time: 0.1000\n",
      "222/223, train_loss: 0.1532, step time: 0.0986\n",
      "223/223, train_loss: 0.1615, step time: 0.0995\n",
      "epoch 26 average loss: 0.1565\n",
      "time consuming of epoch 26 is: 85.2582\n",
      "----------\n",
      "epoch 27/300\n",
      "1/223, train_loss: 0.1493, step time: 0.1011\n",
      "2/223, train_loss: 0.1593, step time: 0.1011\n",
      "3/223, train_loss: 0.1690, step time: 0.1000\n",
      "4/223, train_loss: 0.1669, step time: 0.1005\n",
      "5/223, train_loss: 0.1592, step time: 0.1146\n",
      "6/223, train_loss: 0.1572, step time: 0.1087\n",
      "7/223, train_loss: 0.1558, step time: 0.0998\n",
      "8/223, train_loss: 0.1483, step time: 0.1009\n",
      "9/223, train_loss: 0.1595, step time: 0.1072\n",
      "10/223, train_loss: 0.1784, step time: 0.1055\n",
      "11/223, train_loss: 0.1529, step time: 0.1006\n",
      "12/223, train_loss: 0.1494, step time: 0.0999\n",
      "13/223, train_loss: 0.1408, step time: 0.1117\n",
      "14/223, train_loss: 0.1840, step time: 0.1126\n",
      "15/223, train_loss: 0.1440, step time: 0.0997\n",
      "16/223, train_loss: 0.1747, step time: 0.0999\n",
      "17/223, train_loss: 0.1488, step time: 0.1000\n",
      "18/223, train_loss: 0.1509, step time: 0.1049\n",
      "19/223, train_loss: 0.1336, step time: 0.1079\n",
      "20/223, train_loss: 0.1460, step time: 0.1138\n",
      "21/223, train_loss: 0.1397, step time: 0.1066\n",
      "22/223, train_loss: 0.1445, step time: 0.0997\n",
      "23/223, train_loss: 0.1560, step time: 0.0996\n",
      "24/223, train_loss: 0.1414, step time: 0.1128\n",
      "25/223, train_loss: 0.1376, step time: 0.1009\n",
      "26/223, train_loss: 0.1644, step time: 0.1054\n",
      "27/223, train_loss: 0.1687, step time: 0.1106\n",
      "28/223, train_loss: 0.1600, step time: 0.1000\n",
      "29/223, train_loss: 0.1571, step time: 0.1027\n",
      "30/223, train_loss: 0.1554, step time: 0.1027\n",
      "31/223, train_loss: 0.1518, step time: 0.1114\n",
      "32/223, train_loss: 0.1477, step time: 0.1088\n",
      "33/223, train_loss: 0.1538, step time: 0.0992\n",
      "34/223, train_loss: 0.1442, step time: 0.1098\n",
      "35/223, train_loss: 0.1752, step time: 0.1081\n",
      "36/223, train_loss: 0.1590, step time: 0.1078\n",
      "37/223, train_loss: 0.1423, step time: 0.1032\n",
      "38/223, train_loss: 0.1701, step time: 0.1078\n",
      "39/223, train_loss: 0.1557, step time: 0.1109\n",
      "40/223, train_loss: 0.1522, step time: 0.1054\n",
      "41/223, train_loss: 0.1450, step time: 0.1009\n",
      "42/223, train_loss: 0.1692, step time: 0.1008\n",
      "43/223, train_loss: 0.1517, step time: 0.1004\n",
      "44/223, train_loss: 0.1492, step time: 0.1011\n",
      "45/223, train_loss: 0.1539, step time: 0.1008\n",
      "46/223, train_loss: 0.1521, step time: 0.1111\n",
      "47/223, train_loss: 0.1503, step time: 0.1087\n",
      "48/223, train_loss: 0.1516, step time: 0.1026\n",
      "49/223, train_loss: 0.1474, step time: 0.1157\n",
      "50/223, train_loss: 0.1562, step time: 0.1156\n",
      "51/223, train_loss: 0.1340, step time: 0.1238\n",
      "52/223, train_loss: 0.1519, step time: 0.1036\n",
      "53/223, train_loss: 0.1432, step time: 0.1055\n",
      "54/223, train_loss: 0.1573, step time: 0.1095\n",
      "55/223, train_loss: 0.1523, step time: 0.1085\n",
      "56/223, train_loss: 0.1640, step time: 0.1048\n",
      "57/223, train_loss: 0.1455, step time: 0.1010\n",
      "58/223, train_loss: 0.1634, step time: 0.1040\n",
      "59/223, train_loss: 0.1373, step time: 0.1023\n",
      "60/223, train_loss: 0.1526, step time: 0.1015\n",
      "61/223, train_loss: 0.1418, step time: 0.1064\n",
      "62/223, train_loss: 0.1562, step time: 0.1139\n",
      "63/223, train_loss: 0.1691, step time: 0.1222\n",
      "64/223, train_loss: 0.1789, step time: 0.0994\n",
      "65/223, train_loss: 0.1575, step time: 0.1118\n",
      "66/223, train_loss: 0.1413, step time: 0.1002\n",
      "67/223, train_loss: 0.1472, step time: 0.1076\n",
      "68/223, train_loss: 0.1590, step time: 0.1120\n",
      "69/223, train_loss: 0.1655, step time: 0.1126\n",
      "70/223, train_loss: 0.1412, step time: 0.1006\n",
      "71/223, train_loss: 0.1454, step time: 0.1005\n",
      "72/223, train_loss: 0.1792, step time: 0.1011\n",
      "73/223, train_loss: 0.1402, step time: 0.1137\n",
      "74/223, train_loss: 0.1452, step time: 0.1030\n",
      "75/223, train_loss: 0.1608, step time: 0.1211\n",
      "76/223, train_loss: 0.1455, step time: 0.1009\n",
      "77/223, train_loss: 0.1606, step time: 0.1140\n",
      "78/223, train_loss: 0.1393, step time: 0.1056\n",
      "79/223, train_loss: 0.1614, step time: 0.1188\n",
      "80/223, train_loss: 0.1535, step time: 0.1128\n",
      "81/223, train_loss: 0.1569, step time: 0.1050\n",
      "82/223, train_loss: 0.1625, step time: 0.1082\n",
      "83/223, train_loss: 0.1439, step time: 0.1140\n",
      "84/223, train_loss: 0.1538, step time: 0.1115\n",
      "85/223, train_loss: 0.1618, step time: 0.1040\n",
      "86/223, train_loss: 0.1510, step time: 0.1054\n",
      "87/223, train_loss: 0.1650, step time: 0.0995\n",
      "88/223, train_loss: 0.1551, step time: 0.1181\n",
      "89/223, train_loss: 0.1422, step time: 0.1219\n",
      "90/223, train_loss: 0.1620, step time: 0.1040\n",
      "91/223, train_loss: 0.1543, step time: 0.1003\n",
      "92/223, train_loss: 0.1746, step time: 0.0998\n",
      "93/223, train_loss: 0.1419, step time: 0.1171\n",
      "94/223, train_loss: 0.1540, step time: 0.1194\n",
      "95/223, train_loss: 0.1713, step time: 0.1302\n",
      "96/223, train_loss: 0.1746, step time: 0.0998\n",
      "97/223, train_loss: 0.1434, step time: 0.1119\n",
      "98/223, train_loss: 0.1494, step time: 0.1086\n",
      "99/223, train_loss: 0.1682, step time: 0.1299\n",
      "100/223, train_loss: 0.1480, step time: 0.1283\n",
      "101/223, train_loss: 0.1482, step time: 0.1166\n",
      "102/223, train_loss: 0.1426, step time: 0.1031\n",
      "103/223, train_loss: 0.1713, step time: 0.1230\n",
      "104/223, train_loss: 0.1720, step time: 0.1225\n",
      "105/223, train_loss: 0.1585, step time: 0.1220\n",
      "106/223, train_loss: 0.1598, step time: 0.1449\n",
      "107/223, train_loss: 0.1569, step time: 0.0998\n",
      "108/223, train_loss: 0.1415, step time: 0.1028\n",
      "109/223, train_loss: 0.1443, step time: 0.1101\n",
      "110/223, train_loss: 0.1528, step time: 0.1117\n",
      "111/223, train_loss: 0.1413, step time: 0.1094\n",
      "112/223, train_loss: 0.1467, step time: 0.1133\n",
      "113/223, train_loss: 0.1558, step time: 0.1114\n",
      "114/223, train_loss: 0.1571, step time: 0.1165\n",
      "115/223, train_loss: 0.1606, step time: 0.1169\n",
      "116/223, train_loss: 0.1425, step time: 0.1046\n",
      "117/223, train_loss: 0.1645, step time: 0.1123\n",
      "118/223, train_loss: 0.3495, step time: 0.1120\n",
      "119/223, train_loss: 0.1599, step time: 0.1222\n",
      "120/223, train_loss: 0.1538, step time: 0.1000\n",
      "121/223, train_loss: 0.1576, step time: 0.1197\n",
      "122/223, train_loss: 0.1613, step time: 0.1100\n",
      "123/223, train_loss: 0.1466, step time: 0.1092\n",
      "124/223, train_loss: 0.1746, step time: 0.1193\n",
      "125/223, train_loss: 0.1417, step time: 0.1116\n",
      "126/223, train_loss: 0.1487, step time: 0.1106\n",
      "127/223, train_loss: 0.1544, step time: 0.1182\n",
      "128/223, train_loss: 0.1364, step time: 0.1000\n",
      "129/223, train_loss: 0.1678, step time: 0.1032\n",
      "130/223, train_loss: 0.1550, step time: 0.1098\n",
      "131/223, train_loss: 0.1663, step time: 0.1030\n",
      "132/223, train_loss: 0.1589, step time: 0.1092\n",
      "133/223, train_loss: 0.1571, step time: 0.1111\n",
      "134/223, train_loss: 0.1642, step time: 0.1175\n",
      "135/223, train_loss: 0.1473, step time: 0.1044\n",
      "136/223, train_loss: 0.1359, step time: 0.1053\n",
      "137/223, train_loss: 0.1562, step time: 0.1012\n",
      "138/223, train_loss: 0.1551, step time: 0.1092\n",
      "139/223, train_loss: 0.1442, step time: 0.1045\n",
      "140/223, train_loss: 0.1588, step time: 0.1255\n",
      "141/223, train_loss: 0.1491, step time: 0.1130\n",
      "142/223, train_loss: 0.1588, step time: 0.1181\n",
      "143/223, train_loss: 0.1448, step time: 0.1050\n",
      "144/223, train_loss: 0.1738, step time: 0.1147\n",
      "145/223, train_loss: 0.1428, step time: 0.1028\n",
      "146/223, train_loss: 0.1356, step time: 0.1008\n",
      "147/223, train_loss: 0.1472, step time: 0.1000\n",
      "148/223, train_loss: 0.1560, step time: 0.1061\n",
      "149/223, train_loss: 0.1394, step time: 0.1059\n",
      "150/223, train_loss: 0.1656, step time: 0.1157\n",
      "151/223, train_loss: 0.1517, step time: 0.1006\n",
      "152/223, train_loss: 0.1523, step time: 0.1000\n",
      "153/223, train_loss: 0.1709, step time: 0.1005\n",
      "154/223, train_loss: 0.1547, step time: 0.1000\n",
      "155/223, train_loss: 0.1535, step time: 0.1001\n",
      "156/223, train_loss: 0.1664, step time: 0.0997\n",
      "157/223, train_loss: 0.1596, step time: 0.1051\n",
      "158/223, train_loss: 0.1585, step time: 0.1049\n",
      "159/223, train_loss: 0.1639, step time: 0.1080\n",
      "160/223, train_loss: 0.1501, step time: 0.1113\n",
      "161/223, train_loss: 0.1486, step time: 0.1109\n",
      "162/223, train_loss: 0.1711, step time: 0.1046\n",
      "163/223, train_loss: 0.1557, step time: 0.1438\n",
      "164/223, train_loss: 0.1570, step time: 0.1040\n",
      "165/223, train_loss: 0.1611, step time: 0.1001\n",
      "166/223, train_loss: 0.1536, step time: 0.0995\n",
      "167/223, train_loss: 0.1541, step time: 0.1162\n",
      "168/223, train_loss: 0.1691, step time: 0.1013\n",
      "169/223, train_loss: 0.1558, step time: 0.1019\n",
      "170/223, train_loss: 0.1375, step time: 0.0999\n",
      "171/223, train_loss: 0.1653, step time: 0.0997\n",
      "172/223, train_loss: 0.1634, step time: 0.1049\n",
      "173/223, train_loss: 0.1429, step time: 0.1079\n",
      "174/223, train_loss: 0.1560, step time: 0.1188\n",
      "175/223, train_loss: 0.1461, step time: 0.1305\n",
      "176/223, train_loss: 0.1517, step time: 0.1034\n",
      "177/223, train_loss: 0.1579, step time: 0.1121\n",
      "178/223, train_loss: 0.1577, step time: 0.1003\n",
      "179/223, train_loss: 0.1543, step time: 0.1000\n",
      "180/223, train_loss: 0.1485, step time: 0.1053\n",
      "181/223, train_loss: 0.1609, step time: 0.1187\n",
      "182/223, train_loss: 0.1488, step time: 0.1197\n",
      "183/223, train_loss: 0.1418, step time: 0.1132\n",
      "184/223, train_loss: 0.1492, step time: 0.1091\n",
      "185/223, train_loss: 0.1488, step time: 0.1063\n",
      "186/223, train_loss: 0.1439, step time: 0.1016\n",
      "187/223, train_loss: 0.1761, step time: 0.1368\n",
      "188/223, train_loss: 0.1617, step time: 0.0993\n",
      "189/223, train_loss: 0.1632, step time: 0.1104\n",
      "190/223, train_loss: 0.1502, step time: 0.1142\n",
      "191/223, train_loss: 0.1566, step time: 0.1239\n",
      "192/223, train_loss: 0.1548, step time: 0.1279\n",
      "193/223, train_loss: 0.1510, step time: 0.1053\n",
      "194/223, train_loss: 0.1639, step time: 0.1037\n",
      "195/223, train_loss: 0.1599, step time: 0.1034\n",
      "196/223, train_loss: 0.1454, step time: 0.1055\n",
      "197/223, train_loss: 0.1521, step time: 0.1116\n",
      "198/223, train_loss: 0.1517, step time: 0.1021\n",
      "199/223, train_loss: 0.1670, step time: 0.0998\n",
      "200/223, train_loss: 0.1614, step time: 0.1005\n",
      "201/223, train_loss: 0.1418, step time: 0.1023\n",
      "202/223, train_loss: 0.1518, step time: 0.1137\n",
      "203/223, train_loss: 0.1386, step time: 0.1017\n",
      "204/223, train_loss: 0.1487, step time: 0.1009\n",
      "205/223, train_loss: 0.1565, step time: 0.1132\n",
      "206/223, train_loss: 0.1523, step time: 0.1019\n",
      "207/223, train_loss: 0.1389, step time: 0.1318\n",
      "208/223, train_loss: 0.1542, step time: 0.1019\n",
      "209/223, train_loss: 0.1481, step time: 0.1090\n",
      "210/223, train_loss: 0.1541, step time: 0.1190\n",
      "211/223, train_loss: 0.1470, step time: 0.1271\n",
      "212/223, train_loss: 0.1609, step time: 0.1051\n",
      "213/223, train_loss: 0.1461, step time: 0.1228\n",
      "214/223, train_loss: 0.1388, step time: 0.1199\n",
      "215/223, train_loss: 0.1525, step time: 0.1046\n",
      "216/223, train_loss: 0.1529, step time: 0.1020\n",
      "217/223, train_loss: 0.1335, step time: 0.1070\n",
      "218/223, train_loss: 0.1588, step time: 0.1220\n",
      "219/223, train_loss: 0.1521, step time: 0.1072\n",
      "220/223, train_loss: 0.1588, step time: 0.0993\n",
      "221/223, train_loss: 0.1410, step time: 0.0992\n",
      "222/223, train_loss: 0.1426, step time: 0.0995\n",
      "223/223, train_loss: 0.1391, step time: 0.0992\n",
      "epoch 27 average loss: 0.1549\n",
      "time consuming of epoch 27 is: 84.6184\n",
      "----------\n",
      "epoch 28/300\n",
      "1/223, train_loss: 0.1390, step time: 0.1089\n",
      "2/223, train_loss: 0.1451, step time: 0.1362\n",
      "3/223, train_loss: 0.1571, step time: 0.1190\n",
      "4/223, train_loss: 0.1580, step time: 0.1220\n",
      "5/223, train_loss: 0.1430, step time: 0.1171\n",
      "6/223, train_loss: 0.1474, step time: 0.1088\n",
      "7/223, train_loss: 0.1664, step time: 0.1166\n",
      "8/223, train_loss: 0.1391, step time: 0.1160\n",
      "9/223, train_loss: 0.1545, step time: 0.1008\n",
      "10/223, train_loss: 0.1573, step time: 0.1081\n",
      "11/223, train_loss: 0.1515, step time: 0.1075\n",
      "12/223, train_loss: 0.1554, step time: 0.1184\n",
      "13/223, train_loss: 0.1493, step time: 0.1085\n",
      "14/223, train_loss: 0.1420, step time: 0.1057\n",
      "15/223, train_loss: 0.1606, step time: 0.1007\n",
      "16/223, train_loss: 0.1373, step time: 0.1004\n",
      "17/223, train_loss: 0.1399, step time: 0.1090\n",
      "18/223, train_loss: 0.1680, step time: 0.1154\n",
      "19/223, train_loss: 0.1430, step time: 0.1056\n",
      "20/223, train_loss: 0.1621, step time: 0.1115\n",
      "21/223, train_loss: 0.1592, step time: 0.1149\n",
      "22/223, train_loss: 0.1625, step time: 0.1004\n",
      "23/223, train_loss: 0.1633, step time: 0.1003\n",
      "24/223, train_loss: 0.1446, step time: 0.1003\n",
      "25/223, train_loss: 0.3359, step time: 0.1113\n",
      "26/223, train_loss: 0.1516, step time: 0.1003\n",
      "27/223, train_loss: 0.1442, step time: 0.1002\n",
      "28/223, train_loss: 0.1499, step time: 0.1092\n",
      "29/223, train_loss: 0.1410, step time: 0.1107\n",
      "30/223, train_loss: 0.1493, step time: 0.1007\n",
      "31/223, train_loss: 0.1402, step time: 0.1004\n",
      "32/223, train_loss: 0.1404, step time: 0.1027\n",
      "33/223, train_loss: 0.1422, step time: 0.1053\n",
      "34/223, train_loss: 0.1600, step time: 0.1307\n",
      "35/223, train_loss: 0.1468, step time: 0.1191\n",
      "36/223, train_loss: 0.1545, step time: 0.1212\n",
      "37/223, train_loss: 0.1662, step time: 0.1042\n",
      "38/223, train_loss: 0.1536, step time: 0.1124\n",
      "39/223, train_loss: 0.1424, step time: 0.1113\n",
      "40/223, train_loss: 0.1572, step time: 0.1066\n",
      "41/223, train_loss: 0.1527, step time: 0.1139\n",
      "42/223, train_loss: 0.1418, step time: 0.1402\n",
      "43/223, train_loss: 0.1455, step time: 0.1180\n",
      "44/223, train_loss: 0.1455, step time: 0.1149\n",
      "45/223, train_loss: 0.1617, step time: 0.1001\n",
      "46/223, train_loss: 0.1422, step time: 0.1035\n",
      "47/223, train_loss: 0.1529, step time: 0.1248\n",
      "48/223, train_loss: 0.1507, step time: 0.1238\n",
      "49/223, train_loss: 0.1413, step time: 0.1232\n",
      "50/223, train_loss: 0.1425, step time: 0.1091\n",
      "51/223, train_loss: 0.1568, step time: 0.1217\n",
      "52/223, train_loss: 0.1537, step time: 0.1160\n",
      "53/223, train_loss: 0.1554, step time: 0.1072\n",
      "54/223, train_loss: 0.1434, step time: 0.1078\n",
      "55/223, train_loss: 0.1345, step time: 0.1240\n",
      "56/223, train_loss: 0.1605, step time: 0.1173\n",
      "57/223, train_loss: 0.1410, step time: 0.1168\n",
      "58/223, train_loss: 0.1523, step time: 0.1037\n",
      "59/223, train_loss: 0.1501, step time: 0.1208\n",
      "60/223, train_loss: 0.1600, step time: 0.1199\n",
      "61/223, train_loss: 0.1586, step time: 0.1005\n",
      "62/223, train_loss: 0.1601, step time: 0.1027\n",
      "63/223, train_loss: 0.1620, step time: 0.1132\n",
      "64/223, train_loss: 0.1535, step time: 0.1062\n",
      "65/223, train_loss: 0.1477, step time: 0.1131\n",
      "66/223, train_loss: 0.1414, step time: 0.1098\n",
      "67/223, train_loss: 0.1646, step time: 0.1060\n",
      "68/223, train_loss: 0.1518, step time: 0.1015\n",
      "69/223, train_loss: 0.1700, step time: 0.1328\n",
      "70/223, train_loss: 0.1731, step time: 0.1002\n",
      "71/223, train_loss: 0.1540, step time: 0.1101\n",
      "72/223, train_loss: 0.1437, step time: 0.1157\n",
      "73/223, train_loss: 0.1385, step time: 0.1143\n",
      "74/223, train_loss: 0.1462, step time: 0.1145\n",
      "75/223, train_loss: 0.1586, step time: 0.1216\n",
      "76/223, train_loss: 0.1589, step time: 0.0996\n",
      "77/223, train_loss: 0.1598, step time: 0.1209\n",
      "78/223, train_loss: 0.1452, step time: 0.1141\n",
      "79/223, train_loss: 0.1461, step time: 0.1163\n",
      "80/223, train_loss: 0.1775, step time: 0.1226\n",
      "81/223, train_loss: 0.1513, step time: 0.1017\n",
      "82/223, train_loss: 0.1404, step time: 0.1048\n",
      "83/223, train_loss: 0.1576, step time: 0.1005\n",
      "84/223, train_loss: 0.1656, step time: 0.1002\n",
      "85/223, train_loss: 0.1455, step time: 0.1156\n",
      "86/223, train_loss: 0.1595, step time: 0.1051\n",
      "87/223, train_loss: 0.1493, step time: 0.1007\n",
      "88/223, train_loss: 0.1501, step time: 0.1003\n",
      "89/223, train_loss: 0.1911, step time: 0.1019\n",
      "90/223, train_loss: 0.1639, step time: 0.1000\n",
      "91/223, train_loss: 0.1332, step time: 0.0996\n",
      "92/223, train_loss: 0.1522, step time: 0.1009\n",
      "93/223, train_loss: 0.1381, step time: 0.1099\n",
      "94/223, train_loss: 0.1654, step time: 0.1165\n",
      "95/223, train_loss: 0.1553, step time: 0.1141\n",
      "96/223, train_loss: 0.1766, step time: 0.1052\n",
      "97/223, train_loss: 0.1676, step time: 0.1200\n",
      "98/223, train_loss: 0.1407, step time: 0.1084\n",
      "99/223, train_loss: 0.1555, step time: 0.1028\n",
      "100/223, train_loss: 0.1479, step time: 0.1051\n",
      "101/223, train_loss: 0.1487, step time: 0.1057\n",
      "102/223, train_loss: 0.1461, step time: 0.1001\n",
      "103/223, train_loss: 0.1610, step time: 0.1112\n",
      "104/223, train_loss: 0.1438, step time: 0.1030\n",
      "105/223, train_loss: 0.1552, step time: 0.1154\n",
      "106/223, train_loss: 0.1513, step time: 0.1373\n",
      "107/223, train_loss: 0.1646, step time: 0.1036\n",
      "108/223, train_loss: 0.1545, step time: 0.0996\n",
      "109/223, train_loss: 0.1598, step time: 0.1111\n",
      "110/223, train_loss: 0.1572, step time: 0.1153\n",
      "111/223, train_loss: 0.1553, step time: 0.0995\n",
      "112/223, train_loss: 0.1420, step time: 0.1284\n",
      "113/223, train_loss: 0.1625, step time: 0.1057\n",
      "114/223, train_loss: 0.1573, step time: 0.1111\n",
      "115/223, train_loss: 0.1777, step time: 0.1105\n",
      "116/223, train_loss: 0.1509, step time: 0.1018\n",
      "117/223, train_loss: 0.1576, step time: 0.1023\n",
      "118/223, train_loss: 0.1481, step time: 0.1025\n",
      "119/223, train_loss: 0.1590, step time: 0.0986\n",
      "120/223, train_loss: 0.1509, step time: 0.1014\n",
      "121/223, train_loss: 0.1472, step time: 0.1016\n",
      "122/223, train_loss: 0.1341, step time: 0.1038\n",
      "123/223, train_loss: 0.1677, step time: 0.1038\n",
      "124/223, train_loss: 0.1397, step time: 0.0990\n",
      "125/223, train_loss: 0.1450, step time: 0.1064\n",
      "126/223, train_loss: 0.1578, step time: 0.1184\n",
      "127/223, train_loss: 0.1555, step time: 0.1154\n",
      "128/223, train_loss: 0.1484, step time: 0.1171\n",
      "129/223, train_loss: 0.1550, step time: 0.1132\n",
      "130/223, train_loss: 0.1562, step time: 0.1007\n",
      "131/223, train_loss: 0.1580, step time: 0.1051\n",
      "132/223, train_loss: 0.1561, step time: 0.1139\n",
      "133/223, train_loss: 0.1584, step time: 0.1141\n",
      "134/223, train_loss: 0.1828, step time: 0.1091\n",
      "135/223, train_loss: 0.1618, step time: 0.1064\n",
      "136/223, train_loss: 0.1501, step time: 0.1144\n",
      "137/223, train_loss: 0.1538, step time: 0.1055\n",
      "138/223, train_loss: 0.1597, step time: 0.1118\n",
      "139/223, train_loss: 0.1708, step time: 0.1221\n",
      "140/223, train_loss: 0.1576, step time: 0.1116\n",
      "141/223, train_loss: 0.1490, step time: 0.1176\n",
      "142/223, train_loss: 0.1503, step time: 0.1080\n",
      "143/223, train_loss: 0.1469, step time: 0.1158\n",
      "144/223, train_loss: 0.1591, step time: 0.1005\n",
      "145/223, train_loss: 0.1813, step time: 0.1084\n",
      "146/223, train_loss: 0.1482, step time: 0.1002\n",
      "147/223, train_loss: 0.1572, step time: 0.1063\n",
      "148/223, train_loss: 0.1567, step time: 0.1045\n",
      "149/223, train_loss: 0.1458, step time: 0.1029\n",
      "150/223, train_loss: 0.1490, step time: 0.1447\n",
      "151/223, train_loss: 0.1322, step time: 0.1019\n",
      "152/223, train_loss: 0.1638, step time: 0.0985\n",
      "153/223, train_loss: 0.1534, step time: 0.1011\n",
      "154/223, train_loss: 0.1505, step time: 0.1004\n",
      "155/223, train_loss: 0.1508, step time: 0.1056\n",
      "156/223, train_loss: 0.1438, step time: 0.0995\n",
      "157/223, train_loss: 0.1539, step time: 0.1159\n",
      "158/223, train_loss: 0.1653, step time: 0.1088\n",
      "159/223, train_loss: 0.1569, step time: 0.1083\n",
      "160/223, train_loss: 0.1581, step time: 0.0996\n",
      "161/223, train_loss: 0.1571, step time: 0.1121\n",
      "162/223, train_loss: 0.1358, step time: 0.1021\n",
      "163/223, train_loss: 0.1489, step time: 0.1015\n",
      "164/223, train_loss: 0.1498, step time: 0.1167\n",
      "165/223, train_loss: 0.1456, step time: 0.1030\n",
      "166/223, train_loss: 0.1429, step time: 0.1059\n",
      "167/223, train_loss: 0.1618, step time: 0.1012\n",
      "168/223, train_loss: 0.1643, step time: 0.0999\n",
      "169/223, train_loss: 0.1658, step time: 0.1006\n",
      "170/223, train_loss: 0.1447, step time: 0.1233\n",
      "171/223, train_loss: 0.1729, step time: 0.1087\n",
      "172/223, train_loss: 0.1503, step time: 0.1006\n",
      "173/223, train_loss: 0.1645, step time: 0.1110\n",
      "174/223, train_loss: 0.1449, step time: 0.1005\n",
      "175/223, train_loss: 0.1454, step time: 0.1005\n",
      "176/223, train_loss: 0.1457, step time: 0.1010\n",
      "177/223, train_loss: 0.1476, step time: 0.1067\n",
      "178/223, train_loss: 0.1634, step time: 0.1001\n",
      "179/223, train_loss: 0.1424, step time: 0.1001\n",
      "180/223, train_loss: 0.1493, step time: 0.1005\n",
      "181/223, train_loss: 0.1381, step time: 0.1016\n",
      "182/223, train_loss: 0.1334, step time: 0.1025\n",
      "183/223, train_loss: 0.1559, step time: 0.1009\n",
      "184/223, train_loss: 0.1705, step time: 0.1018\n",
      "185/223, train_loss: 0.1399, step time: 0.1011\n",
      "186/223, train_loss: 0.1454, step time: 0.1005\n",
      "187/223, train_loss: 0.1547, step time: 0.1007\n",
      "188/223, train_loss: 0.1396, step time: 0.1003\n",
      "189/223, train_loss: 0.1438, step time: 0.1062\n",
      "190/223, train_loss: 0.1527, step time: 0.1082\n",
      "191/223, train_loss: 0.1535, step time: 0.1000\n",
      "192/223, train_loss: 0.1548, step time: 0.0995\n",
      "193/223, train_loss: 0.1430, step time: 0.1105\n",
      "194/223, train_loss: 0.1476, step time: 0.1058\n",
      "195/223, train_loss: 0.1450, step time: 0.1211\n",
      "196/223, train_loss: 0.1313, step time: 0.1189\n",
      "197/223, train_loss: 0.1414, step time: 0.1011\n",
      "198/223, train_loss: 0.1536, step time: 0.1033\n",
      "199/223, train_loss: 0.1439, step time: 0.1102\n",
      "200/223, train_loss: 0.1386, step time: 0.1004\n",
      "201/223, train_loss: 0.1432, step time: 0.1199\n",
      "202/223, train_loss: 0.1468, step time: 0.1087\n",
      "203/223, train_loss: 0.1493, step time: 0.1148\n",
      "204/223, train_loss: 0.1434, step time: 0.1034\n",
      "205/223, train_loss: 0.1416, step time: 0.1098\n",
      "206/223, train_loss: 0.1466, step time: 0.1167\n",
      "207/223, train_loss: 0.1432, step time: 0.1127\n",
      "208/223, train_loss: 0.1459, step time: 0.1087\n",
      "209/223, train_loss: 0.1604, step time: 0.1091\n",
      "210/223, train_loss: 0.1555, step time: 0.1143\n",
      "211/223, train_loss: 0.1388, step time: 0.1125\n",
      "212/223, train_loss: 0.1513, step time: 0.1018\n",
      "213/223, train_loss: 0.1367, step time: 0.1168\n",
      "214/223, train_loss: 0.1388, step time: 0.1002\n",
      "215/223, train_loss: 0.1304, step time: 0.1231\n",
      "216/223, train_loss: 0.1323, step time: 0.1150\n",
      "217/223, train_loss: 0.1528, step time: 0.1123\n",
      "218/223, train_loss: 0.1658, step time: 0.1154\n",
      "219/223, train_loss: 0.1478, step time: 0.1056\n",
      "220/223, train_loss: 0.1387, step time: 0.1192\n",
      "221/223, train_loss: 0.1637, step time: 0.0992\n",
      "222/223, train_loss: 0.1423, step time: 0.0994\n",
      "223/223, train_loss: 0.1309, step time: 0.0998\n",
      "epoch 28 average loss: 0.1526\n",
      "time consuming of epoch 28 is: 86.1633\n",
      "----------\n",
      "epoch 29/300\n",
      "1/223, train_loss: 0.1441, step time: 0.1103\n",
      "2/223, train_loss: 0.1471, step time: 0.1136\n",
      "3/223, train_loss: 0.1465, step time: 0.1071\n",
      "4/223, train_loss: 0.1466, step time: 0.1094\n",
      "5/223, train_loss: 0.1436, step time: 0.1034\n",
      "6/223, train_loss: 0.1470, step time: 0.1171\n",
      "7/223, train_loss: 0.1540, step time: 0.1268\n",
      "8/223, train_loss: 0.1296, step time: 0.1089\n",
      "9/223, train_loss: 0.1540, step time: 0.1136\n",
      "10/223, train_loss: 0.1756, step time: 0.1202\n",
      "11/223, train_loss: 0.1572, step time: 0.1169\n",
      "12/223, train_loss: 0.1482, step time: 0.1101\n",
      "13/223, train_loss: 0.1458, step time: 0.1151\n",
      "14/223, train_loss: 0.1579, step time: 0.1002\n",
      "15/223, train_loss: 0.1480, step time: 0.1132\n",
      "16/223, train_loss: 0.1464, step time: 0.1016\n",
      "17/223, train_loss: 0.1454, step time: 0.1002\n",
      "18/223, train_loss: 0.1437, step time: 0.1375\n",
      "19/223, train_loss: 0.1424, step time: 0.1131\n",
      "20/223, train_loss: 0.1724, step time: 0.1025\n",
      "21/223, train_loss: 0.1547, step time: 0.1033\n",
      "22/223, train_loss: 0.1588, step time: 0.1065\n",
      "23/223, train_loss: 0.1621, step time: 0.1119\n",
      "24/223, train_loss: 0.1362, step time: 0.1168\n",
      "25/223, train_loss: 0.1427, step time: 0.1122\n",
      "26/223, train_loss: 0.1581, step time: 0.1108\n",
      "27/223, train_loss: 0.1356, step time: 0.1151\n",
      "28/223, train_loss: 0.1454, step time: 0.1064\n",
      "29/223, train_loss: 0.1482, step time: 0.1173\n",
      "30/223, train_loss: 0.1563, step time: 0.1144\n",
      "31/223, train_loss: 0.1479, step time: 0.1122\n",
      "32/223, train_loss: 0.1501, step time: 0.1137\n",
      "33/223, train_loss: 0.1540, step time: 0.1164\n",
      "34/223, train_loss: 0.1602, step time: 0.1194\n",
      "35/223, train_loss: 0.1543, step time: 0.1136\n",
      "36/223, train_loss: 0.1468, step time: 0.1001\n",
      "37/223, train_loss: 0.1562, step time: 0.1021\n",
      "38/223, train_loss: 0.1353, step time: 0.1097\n",
      "39/223, train_loss: 0.1311, step time: 0.1060\n",
      "40/223, train_loss: 0.1518, step time: 0.1106\n",
      "41/223, train_loss: 0.1528, step time: 0.1130\n",
      "42/223, train_loss: 0.1525, step time: 0.1232\n",
      "43/223, train_loss: 0.1620, step time: 0.1007\n",
      "44/223, train_loss: 0.1562, step time: 0.1007\n",
      "45/223, train_loss: 0.1657, step time: 0.1016\n",
      "46/223, train_loss: 0.1462, step time: 0.1032\n",
      "47/223, train_loss: 0.1447, step time: 0.0999\n",
      "48/223, train_loss: 0.1462, step time: 0.1000\n",
      "49/223, train_loss: 0.1427, step time: 0.1167\n",
      "50/223, train_loss: 0.1635, step time: 0.1227\n",
      "51/223, train_loss: 0.1420, step time: 0.1098\n",
      "52/223, train_loss: 0.1331, step time: 0.1057\n",
      "53/223, train_loss: 0.1314, step time: 0.1112\n",
      "54/223, train_loss: 0.1431, step time: 0.1063\n",
      "55/223, train_loss: 0.1489, step time: 0.1013\n",
      "56/223, train_loss: 0.1671, step time: 0.1002\n",
      "57/223, train_loss: 0.1617, step time: 0.1123\n",
      "58/223, train_loss: 0.1565, step time: 0.1050\n",
      "59/223, train_loss: 0.1487, step time: 0.1001\n",
      "60/223, train_loss: 0.1524, step time: 0.1071\n",
      "61/223, train_loss: 0.1452, step time: 0.0997\n",
      "62/223, train_loss: 0.1637, step time: 0.1068\n",
      "63/223, train_loss: 0.1578, step time: 0.1208\n",
      "64/223, train_loss: 0.1502, step time: 0.1124\n",
      "65/223, train_loss: 0.1609, step time: 0.1109\n",
      "66/223, train_loss: 0.1570, step time: 0.1148\n",
      "67/223, train_loss: 0.1495, step time: 0.1015\n",
      "68/223, train_loss: 0.1529, step time: 0.1086\n",
      "69/223, train_loss: 0.1554, step time: 0.1109\n",
      "70/223, train_loss: 0.1464, step time: 0.1069\n",
      "71/223, train_loss: 0.1548, step time: 0.1253\n",
      "72/223, train_loss: 0.1459, step time: 0.0995\n",
      "73/223, train_loss: 0.1570, step time: 0.1111\n",
      "74/223, train_loss: 0.1545, step time: 0.1166\n",
      "75/223, train_loss: 0.1484, step time: 0.1144\n",
      "76/223, train_loss: 0.1598, step time: 0.1235\n",
      "77/223, train_loss: 0.1569, step time: 0.1092\n",
      "78/223, train_loss: 0.1454, step time: 0.1069\n",
      "79/223, train_loss: 0.1513, step time: 0.1005\n",
      "80/223, train_loss: 0.1702, step time: 0.1025\n",
      "81/223, train_loss: 0.1429, step time: 0.1066\n",
      "82/223, train_loss: 0.1822, step time: 0.1151\n",
      "83/223, train_loss: 0.1470, step time: 0.1020\n",
      "84/223, train_loss: 0.1478, step time: 0.1090\n",
      "85/223, train_loss: 0.1451, step time: 0.1081\n",
      "86/223, train_loss: 0.1432, step time: 0.1137\n",
      "87/223, train_loss: 0.1419, step time: 0.1131\n",
      "88/223, train_loss: 0.1732, step time: 0.1460\n",
      "89/223, train_loss: 0.1369, step time: 0.1099\n",
      "90/223, train_loss: 0.1678, step time: 0.1068\n",
      "91/223, train_loss: 0.1483, step time: 0.1016\n",
      "92/223, train_loss: 0.1435, step time: 0.1052\n",
      "93/223, train_loss: 0.1543, step time: 0.1042\n",
      "94/223, train_loss: 0.1413, step time: 0.1043\n",
      "95/223, train_loss: 0.1707, step time: 0.1133\n",
      "96/223, train_loss: 0.1401, step time: 0.1354\n",
      "97/223, train_loss: 0.1646, step time: 0.1021\n",
      "98/223, train_loss: 0.1619, step time: 0.1161\n",
      "99/223, train_loss: 0.1506, step time: 0.1018\n",
      "100/223, train_loss: 0.1393, step time: 0.1002\n",
      "101/223, train_loss: 0.1607, step time: 0.1105\n",
      "102/223, train_loss: 0.1613, step time: 0.1132\n",
      "103/223, train_loss: 0.1652, step time: 0.1328\n",
      "104/223, train_loss: 0.1439, step time: 0.1327\n",
      "105/223, train_loss: 0.1590, step time: 0.1086\n",
      "106/223, train_loss: 0.1503, step time: 0.1163\n",
      "107/223, train_loss: 0.1531, step time: 0.1135\n",
      "108/223, train_loss: 0.1494, step time: 0.1047\n",
      "109/223, train_loss: 0.1584, step time: 0.1068\n",
      "110/223, train_loss: 0.1352, step time: 0.1142\n",
      "111/223, train_loss: 0.1494, step time: 0.1074\n",
      "112/223, train_loss: 0.1571, step time: 0.1413\n",
      "113/223, train_loss: 0.1615, step time: 0.1237\n",
      "114/223, train_loss: 0.1509, step time: 0.1080\n",
      "115/223, train_loss: 0.1600, step time: 0.1128\n",
      "116/223, train_loss: 0.1553, step time: 0.1011\n",
      "117/223, train_loss: 0.1411, step time: 0.1000\n",
      "118/223, train_loss: 0.1460, step time: 0.1008\n",
      "119/223, train_loss: 0.1610, step time: 0.1001\n",
      "120/223, train_loss: 0.1428, step time: 0.1010\n",
      "121/223, train_loss: 0.1520, step time: 0.1004\n",
      "122/223, train_loss: 0.1598, step time: 0.1008\n",
      "123/223, train_loss: 0.1395, step time: 0.0994\n",
      "124/223, train_loss: 0.1429, step time: 0.1062\n",
      "125/223, train_loss: 0.1436, step time: 0.1161\n",
      "126/223, train_loss: 0.1451, step time: 0.1094\n",
      "127/223, train_loss: 0.1473, step time: 0.1150\n",
      "128/223, train_loss: 0.1478, step time: 0.1043\n",
      "129/223, train_loss: 0.1533, step time: 0.1096\n",
      "130/223, train_loss: 0.1354, step time: 0.1001\n",
      "131/223, train_loss: 0.1558, step time: 0.1007\n",
      "132/223, train_loss: 0.1442, step time: 0.1008\n",
      "133/223, train_loss: 0.1350, step time: 0.1134\n",
      "134/223, train_loss: 0.1580, step time: 0.1004\n",
      "135/223, train_loss: 0.1400, step time: 0.1000\n",
      "136/223, train_loss: 0.1526, step time: 0.1006\n",
      "137/223, train_loss: 0.1442, step time: 0.1024\n",
      "138/223, train_loss: 0.1412, step time: 0.1088\n",
      "139/223, train_loss: 0.1562, step time: 0.1025\n",
      "140/223, train_loss: 0.1465, step time: 0.1366\n",
      "141/223, train_loss: 0.1372, step time: 0.1047\n",
      "142/223, train_loss: 0.1466, step time: 0.0994\n",
      "143/223, train_loss: 0.1431, step time: 0.1117\n",
      "144/223, train_loss: 0.1409, step time: 0.1135\n",
      "145/223, train_loss: 0.1389, step time: 0.1118\n",
      "146/223, train_loss: 0.1460, step time: 0.1059\n",
      "147/223, train_loss: 0.1564, step time: 0.1061\n",
      "148/223, train_loss: 0.1530, step time: 0.1134\n",
      "149/223, train_loss: 0.1526, step time: 0.0996\n",
      "150/223, train_loss: 0.1547, step time: 0.1112\n",
      "151/223, train_loss: 0.1500, step time: 0.1343\n",
      "152/223, train_loss: 0.1252, step time: 0.1134\n",
      "153/223, train_loss: 0.1410, step time: 0.1284\n",
      "154/223, train_loss: 0.1720, step time: 0.1037\n",
      "155/223, train_loss: 0.1362, step time: 0.1095\n",
      "156/223, train_loss: 0.1523, step time: 0.1309\n",
      "157/223, train_loss: 0.1393, step time: 0.1021\n",
      "158/223, train_loss: 0.1377, step time: 0.1052\n",
      "159/223, train_loss: 0.1480, step time: 0.1114\n",
      "160/223, train_loss: 0.1505, step time: 0.1172\n",
      "161/223, train_loss: 0.1577, step time: 0.1295\n",
      "162/223, train_loss: 0.1652, step time: 0.1087\n",
      "163/223, train_loss: 0.1438, step time: 0.1016\n",
      "164/223, train_loss: 0.1476, step time: 0.1264\n",
      "165/223, train_loss: 0.1619, step time: 0.1161\n",
      "166/223, train_loss: 0.1435, step time: 0.1282\n",
      "167/223, train_loss: 0.1421, step time: 0.1045\n",
      "168/223, train_loss: 0.1429, step time: 0.0991\n",
      "169/223, train_loss: 0.1514, step time: 0.1138\n",
      "170/223, train_loss: 0.1567, step time: 0.1179\n",
      "171/223, train_loss: 0.1307, step time: 0.1155\n",
      "172/223, train_loss: 0.1491, step time: 0.1189\n",
      "173/223, train_loss: 0.1521, step time: 0.1131\n",
      "174/223, train_loss: 0.1708, step time: 0.1085\n",
      "175/223, train_loss: 0.1484, step time: 0.1110\n",
      "176/223, train_loss: 0.1523, step time: 0.1257\n",
      "177/223, train_loss: 0.1677, step time: 0.1032\n",
      "178/223, train_loss: 0.1550, step time: 0.1085\n",
      "179/223, train_loss: 0.1492, step time: 0.0983\n",
      "180/223, train_loss: 0.1379, step time: 0.0996\n",
      "181/223, train_loss: 0.1437, step time: 0.0994\n",
      "182/223, train_loss: 0.1381, step time: 0.1110\n",
      "183/223, train_loss: 0.1376, step time: 0.1120\n",
      "184/223, train_loss: 0.1404, step time: 0.1181\n",
      "185/223, train_loss: 0.1521, step time: 0.0992\n",
      "186/223, train_loss: 0.1490, step time: 0.1344\n",
      "187/223, train_loss: 0.1425, step time: 0.1269\n",
      "188/223, train_loss: 0.1487, step time: 0.0995\n",
      "189/223, train_loss: 0.1548, step time: 0.1139\n",
      "190/223, train_loss: 0.1442, step time: 0.1057\n",
      "191/223, train_loss: 0.1619, step time: 0.1009\n",
      "192/223, train_loss: 0.1744, step time: 0.1029\n",
      "193/223, train_loss: 0.1432, step time: 0.1031\n",
      "194/223, train_loss: 0.1488, step time: 0.1220\n",
      "195/223, train_loss: 0.1439, step time: 0.1058\n",
      "196/223, train_loss: 0.1472, step time: 0.1267\n",
      "197/223, train_loss: 0.1582, step time: 0.1011\n",
      "198/223, train_loss: 0.1291, step time: 0.1218\n",
      "199/223, train_loss: 0.1548, step time: 0.1158\n",
      "200/223, train_loss: 0.1576, step time: 0.0987\n",
      "201/223, train_loss: 0.1564, step time: 0.0993\n",
      "202/223, train_loss: 0.1529, step time: 0.1120\n",
      "203/223, train_loss: 0.1524, step time: 0.1022\n",
      "204/223, train_loss: 0.1654, step time: 0.1017\n",
      "205/223, train_loss: 0.1607, step time: 0.1032\n",
      "206/223, train_loss: 0.1457, step time: 0.1157\n",
      "207/223, train_loss: 0.3311, step time: 0.1097\n",
      "208/223, train_loss: 0.1510, step time: 0.1123\n",
      "209/223, train_loss: 0.1708, step time: 0.1138\n",
      "210/223, train_loss: 0.1716, step time: 0.1026\n",
      "211/223, train_loss: 0.1729, step time: 0.1153\n",
      "212/223, train_loss: 0.1265, step time: 0.1140\n",
      "213/223, train_loss: 0.1441, step time: 0.1036\n",
      "214/223, train_loss: 0.1552, step time: 0.1002\n",
      "215/223, train_loss: 0.1556, step time: 0.1001\n",
      "216/223, train_loss: 0.1453, step time: 0.1003\n",
      "217/223, train_loss: 0.1405, step time: 0.1011\n",
      "218/223, train_loss: 0.1423, step time: 0.0997\n",
      "219/223, train_loss: 0.1492, step time: 0.1003\n",
      "220/223, train_loss: 0.1662, step time: 0.0999\n",
      "221/223, train_loss: 0.1552, step time: 0.1009\n",
      "222/223, train_loss: 0.1473, step time: 0.0991\n",
      "223/223, train_loss: 0.1472, step time: 0.0992\n",
      "epoch 29 average loss: 0.1513\n",
      "time consuming of epoch 29 is: 85.6542\n",
      "----------\n",
      "epoch 30/300\n",
      "1/223, train_loss: 0.1575, step time: 0.1009\n",
      "2/223, train_loss: 0.1457, step time: 0.0990\n",
      "3/223, train_loss: 0.1445, step time: 0.1076\n",
      "4/223, train_loss: 0.1347, step time: 0.0999\n",
      "5/223, train_loss: 0.1560, step time: 0.1083\n",
      "6/223, train_loss: 0.1547, step time: 0.0994\n",
      "7/223, train_loss: 0.1371, step time: 0.1125\n",
      "8/223, train_loss: 0.3326, step time: 0.1210\n",
      "9/223, train_loss: 0.1437, step time: 0.1105\n",
      "10/223, train_loss: 0.1482, step time: 0.1173\n",
      "11/223, train_loss: 0.1408, step time: 0.1095\n",
      "12/223, train_loss: 0.1822, step time: 0.1000\n",
      "13/223, train_loss: 0.1411, step time: 0.1072\n",
      "14/223, train_loss: 0.1570, step time: 0.1172\n",
      "15/223, train_loss: 0.1590, step time: 0.1255\n",
      "16/223, train_loss: 0.1414, step time: 0.1019\n",
      "17/223, train_loss: 0.1578, step time: 0.1286\n",
      "18/223, train_loss: 0.1455, step time: 0.1052\n",
      "19/223, train_loss: 0.1501, step time: 0.1055\n",
      "20/223, train_loss: 0.1357, step time: 0.1003\n",
      "21/223, train_loss: 0.1475, step time: 0.1180\n",
      "22/223, train_loss: 0.1511, step time: 0.1615\n",
      "23/223, train_loss: 0.1528, step time: 0.1183\n",
      "24/223, train_loss: 0.1517, step time: 0.1006\n",
      "25/223, train_loss: 0.1379, step time: 0.1070\n",
      "26/223, train_loss: 0.1488, step time: 0.1140\n",
      "27/223, train_loss: 0.1513, step time: 0.1014\n",
      "28/223, train_loss: 0.1421, step time: 0.1009\n",
      "29/223, train_loss: 0.1635, step time: 0.1062\n",
      "30/223, train_loss: 0.1470, step time: 0.1004\n",
      "31/223, train_loss: 0.1351, step time: 0.1020\n",
      "32/223, train_loss: 0.1651, step time: 0.1072\n",
      "33/223, train_loss: 0.1499, step time: 0.1236\n",
      "34/223, train_loss: 0.1397, step time: 0.1086\n",
      "35/223, train_loss: 0.1518, step time: 0.1125\n",
      "36/223, train_loss: 0.1397, step time: 0.1002\n",
      "37/223, train_loss: 0.1471, step time: 0.1140\n",
      "38/223, train_loss: 0.1441, step time: 0.1017\n",
      "39/223, train_loss: 0.1414, step time: 0.1074\n",
      "40/223, train_loss: 0.1409, step time: 0.1000\n",
      "41/223, train_loss: 0.1567, step time: 0.1159\n",
      "42/223, train_loss: 0.1552, step time: 0.1080\n",
      "43/223, train_loss: 0.1383, step time: 0.1000\n",
      "44/223, train_loss: 0.1646, step time: 0.1004\n",
      "45/223, train_loss: 0.1516, step time: 0.1058\n",
      "46/223, train_loss: 0.1402, step time: 0.1121\n",
      "47/223, train_loss: 0.1461, step time: 0.0997\n",
      "48/223, train_loss: 0.1472, step time: 0.1004\n",
      "49/223, train_loss: 0.1428, step time: 0.1050\n",
      "50/223, train_loss: 0.1459, step time: 0.1300\n",
      "51/223, train_loss: 0.1394, step time: 0.1056\n",
      "52/223, train_loss: 0.1416, step time: 0.1008\n",
      "53/223, train_loss: 0.1363, step time: 0.1171\n",
      "54/223, train_loss: 0.1390, step time: 0.1048\n",
      "55/223, train_loss: 0.1639, step time: 0.1038\n",
      "56/223, train_loss: 0.1484, step time: 0.1216\n",
      "57/223, train_loss: 0.1620, step time: 0.1047\n",
      "58/223, train_loss: 0.1497, step time: 0.1008\n",
      "59/223, train_loss: 0.1330, step time: 0.1012\n",
      "60/223, train_loss: 0.1516, step time: 0.1687\n",
      "61/223, train_loss: 0.1515, step time: 0.1007\n",
      "62/223, train_loss: 0.1598, step time: 0.1012\n",
      "63/223, train_loss: 0.1539, step time: 0.1079\n",
      "64/223, train_loss: 0.1283, step time: 0.1009\n",
      "65/223, train_loss: 0.1406, step time: 0.1292\n",
      "66/223, train_loss: 0.1454, step time: 0.1056\n",
      "67/223, train_loss: 0.1467, step time: 0.1008\n",
      "68/223, train_loss: 0.1703, step time: 0.1046\n",
      "69/223, train_loss: 0.1428, step time: 0.1163\n",
      "70/223, train_loss: 0.1454, step time: 0.1279\n",
      "71/223, train_loss: 0.1472, step time: 0.1098\n",
      "72/223, train_loss: 0.1516, step time: 0.1221\n",
      "73/223, train_loss: 0.1441, step time: 0.1074\n",
      "74/223, train_loss: 0.1459, step time: 0.1061\n",
      "75/223, train_loss: 0.1326, step time: 0.1000\n",
      "76/223, train_loss: 0.1586, step time: 0.1047\n",
      "77/223, train_loss: 0.1417, step time: 0.1263\n",
      "78/223, train_loss: 0.1416, step time: 0.1052\n",
      "79/223, train_loss: 0.1485, step time: 0.1074\n",
      "80/223, train_loss: 0.1333, step time: 0.1233\n",
      "81/223, train_loss: 0.1457, step time: 0.1001\n",
      "82/223, train_loss: 0.1302, step time: 0.1193\n",
      "83/223, train_loss: 0.1472, step time: 0.1083\n",
      "84/223, train_loss: 0.1414, step time: 0.1173\n",
      "85/223, train_loss: 0.1581, step time: 0.1052\n",
      "86/223, train_loss: 0.1461, step time: 0.1150\n",
      "87/223, train_loss: 0.1374, step time: 0.1103\n",
      "88/223, train_loss: 0.1252, step time: 0.0999\n",
      "89/223, train_loss: 0.1388, step time: 0.1127\n",
      "90/223, train_loss: 0.1488, step time: 0.1086\n",
      "91/223, train_loss: 0.1373, step time: 0.1250\n",
      "92/223, train_loss: 0.1516, step time: 0.0997\n",
      "93/223, train_loss: 0.1378, step time: 0.1015\n",
      "94/223, train_loss: 0.1589, step time: 0.1116\n",
      "95/223, train_loss: 0.1439, step time: 0.1003\n",
      "96/223, train_loss: 0.1288, step time: 0.1005\n",
      "97/223, train_loss: 0.1400, step time: 0.1121\n",
      "98/223, train_loss: 0.1288, step time: 0.1139\n",
      "99/223, train_loss: 0.1393, step time: 0.1007\n",
      "100/223, train_loss: 0.1343, step time: 0.1179\n",
      "101/223, train_loss: 0.1436, step time: 0.1178\n",
      "102/223, train_loss: 0.1692, step time: 0.1073\n",
      "103/223, train_loss: 0.1449, step time: 0.1012\n",
      "104/223, train_loss: 0.1371, step time: 0.1007\n",
      "105/223, train_loss: 0.1373, step time: 0.1168\n",
      "106/223, train_loss: 0.1448, step time: 0.1007\n",
      "107/223, train_loss: 0.1353, step time: 0.1296\n",
      "108/223, train_loss: 0.1429, step time: 0.1003\n",
      "109/223, train_loss: 0.1354, step time: 0.1000\n",
      "110/223, train_loss: 0.1553, step time: 0.1010\n",
      "111/223, train_loss: 0.1485, step time: 0.1007\n",
      "112/223, train_loss: 0.1518, step time: 0.1004\n",
      "113/223, train_loss: 0.1410, step time: 0.1117\n",
      "114/223, train_loss: 0.1513, step time: 0.1164\n",
      "115/223, train_loss: 0.1505, step time: 0.1004\n",
      "116/223, train_loss: 0.1415, step time: 0.0997\n",
      "117/223, train_loss: 0.1418, step time: 0.1334\n",
      "118/223, train_loss: 0.1473, step time: 0.1226\n",
      "119/223, train_loss: 0.1507, step time: 0.1002\n",
      "120/223, train_loss: 0.1420, step time: 0.1004\n",
      "121/223, train_loss: 0.1465, step time: 0.1132\n",
      "122/223, train_loss: 0.1343, step time: 0.1121\n",
      "123/223, train_loss: 0.1543, step time: 0.1228\n",
      "124/223, train_loss: 0.1480, step time: 0.1001\n",
      "125/223, train_loss: 0.1403, step time: 0.1008\n",
      "126/223, train_loss: 0.1524, step time: 0.1017\n",
      "127/223, train_loss: 0.1424, step time: 0.1008\n",
      "128/223, train_loss: 0.1404, step time: 0.0996\n",
      "129/223, train_loss: 0.1574, step time: 0.1001\n",
      "130/223, train_loss: 0.1692, step time: 0.1150\n",
      "131/223, train_loss: 0.1474, step time: 0.1007\n",
      "132/223, train_loss: 0.1384, step time: 0.1004\n",
      "133/223, train_loss: 0.1460, step time: 0.1094\n",
      "134/223, train_loss: 0.1466, step time: 0.1124\n",
      "135/223, train_loss: 0.1512, step time: 0.1111\n",
      "136/223, train_loss: 0.1480, step time: 0.1143\n",
      "137/223, train_loss: 0.1419, step time: 0.1190\n",
      "138/223, train_loss: 0.1658, step time: 0.1227\n",
      "139/223, train_loss: 0.1429, step time: 0.1505\n",
      "140/223, train_loss: 0.1562, step time: 0.1048\n",
      "141/223, train_loss: 0.1313, step time: 0.1083\n",
      "142/223, train_loss: 0.1599, step time: 0.1125\n",
      "143/223, train_loss: 0.1429, step time: 0.1146\n",
      "144/223, train_loss: 0.1556, step time: 0.1092\n",
      "145/223, train_loss: 0.1496, step time: 0.1147\n",
      "146/223, train_loss: 0.1596, step time: 0.1090\n",
      "147/223, train_loss: 0.1368, step time: 0.1051\n",
      "148/223, train_loss: 0.1290, step time: 0.1006\n",
      "149/223, train_loss: 0.1439, step time: 0.1378\n",
      "150/223, train_loss: 0.1495, step time: 0.1107\n",
      "151/223, train_loss: 0.1362, step time: 0.1007\n",
      "152/223, train_loss: 0.1503, step time: 0.1006\n",
      "153/223, train_loss: 0.1774, step time: 0.1003\n",
      "154/223, train_loss: 0.1424, step time: 0.0998\n",
      "155/223, train_loss: 0.1467, step time: 0.1016\n",
      "156/223, train_loss: 0.1574, step time: 0.1039\n",
      "157/223, train_loss: 0.1684, step time: 0.1151\n",
      "158/223, train_loss: 0.1474, step time: 0.1018\n",
      "159/223, train_loss: 0.1503, step time: 0.1119\n",
      "160/223, train_loss: 0.1479, step time: 0.1068\n",
      "161/223, train_loss: 0.1389, step time: 0.1048\n",
      "162/223, train_loss: 0.1712, step time: 0.1000\n",
      "163/223, train_loss: 0.1393, step time: 0.1008\n",
      "164/223, train_loss: 0.1490, step time: 0.1038\n",
      "165/223, train_loss: 0.1443, step time: 0.1347\n",
      "166/223, train_loss: 0.1670, step time: 0.1146\n",
      "167/223, train_loss: 0.1410, step time: 0.1017\n",
      "168/223, train_loss: 0.1525, step time: 0.1236\n",
      "169/223, train_loss: 0.1496, step time: 0.1074\n",
      "170/223, train_loss: 0.1397, step time: 0.1076\n",
      "171/223, train_loss: 0.1421, step time: 0.1078\n",
      "172/223, train_loss: 0.1384, step time: 0.1369\n",
      "173/223, train_loss: 0.1366, step time: 0.1081\n",
      "174/223, train_loss: 0.1458, step time: 0.1124\n",
      "175/223, train_loss: 0.1351, step time: 0.1256\n",
      "176/223, train_loss: 0.1311, step time: 0.1173\n",
      "177/223, train_loss: 0.1645, step time: 0.1154\n",
      "178/223, train_loss: 0.1536, step time: 0.1102\n",
      "179/223, train_loss: 0.1564, step time: 0.1221\n",
      "180/223, train_loss: 0.1582, step time: 0.1011\n",
      "181/223, train_loss: 0.1585, step time: 0.1009\n",
      "182/223, train_loss: 0.1472, step time: 0.1006\n",
      "183/223, train_loss: 0.1402, step time: 0.0999\n",
      "184/223, train_loss: 0.1463, step time: 0.1039\n",
      "185/223, train_loss: 0.1574, step time: 0.1110\n",
      "186/223, train_loss: 0.1401, step time: 0.0997\n",
      "187/223, train_loss: 0.1449, step time: 0.0996\n",
      "188/223, train_loss: 0.1498, step time: 0.1295\n",
      "189/223, train_loss: 0.1447, step time: 0.1172\n",
      "190/223, train_loss: 0.1493, step time: 0.1004\n",
      "191/223, train_loss: 0.1541, step time: 0.1009\n",
      "192/223, train_loss: 0.1576, step time: 0.1029\n",
      "193/223, train_loss: 0.1382, step time: 0.1039\n",
      "194/223, train_loss: 0.1471, step time: 0.1064\n",
      "195/223, train_loss: 0.1445, step time: 0.1002\n",
      "196/223, train_loss: 0.1371, step time: 0.0998\n",
      "197/223, train_loss: 0.1585, step time: 0.1102\n",
      "198/223, train_loss: 0.1679, step time: 0.1009\n",
      "199/223, train_loss: 0.1343, step time: 0.1009\n",
      "200/223, train_loss: 0.1521, step time: 0.1125\n",
      "201/223, train_loss: 0.1440, step time: 0.1136\n",
      "202/223, train_loss: 0.1358, step time: 0.1008\n",
      "203/223, train_loss: 0.1590, step time: 0.1034\n",
      "204/223, train_loss: 0.1704, step time: 0.1091\n",
      "205/223, train_loss: 0.1424, step time: 0.1144\n",
      "206/223, train_loss: 0.1428, step time: 0.1344\n",
      "207/223, train_loss: 0.1337, step time: 0.1171\n",
      "208/223, train_loss: 0.1598, step time: 0.1056\n",
      "209/223, train_loss: 0.1528, step time: 0.1151\n",
      "210/223, train_loss: 0.1557, step time: 0.1318\n",
      "211/223, train_loss: 0.1481, step time: 0.1243\n",
      "212/223, train_loss: 0.1564, step time: 0.1007\n",
      "213/223, train_loss: 0.1528, step time: 0.1137\n",
      "214/223, train_loss: 0.1426, step time: 0.1001\n",
      "215/223, train_loss: 0.1339, step time: 0.1003\n",
      "216/223, train_loss: 0.1490, step time: 0.0999\n",
      "217/223, train_loss: 0.1473, step time: 0.1015\n",
      "218/223, train_loss: 0.1477, step time: 0.0999\n",
      "219/223, train_loss: 0.1554, step time: 0.0995\n",
      "220/223, train_loss: 0.1334, step time: 0.0994\n",
      "221/223, train_loss: 0.1531, step time: 0.0985\n",
      "222/223, train_loss: 0.1357, step time: 0.0994\n",
      "223/223, train_loss: 0.1473, step time: 0.1001\n",
      "epoch 30 average loss: 0.1480\n",
      "saved new best metric model\n",
      "current epoch: 30 current mean dice: 0.8166 tc: 0.9001 wt: 0.8335 et: 0.7164\n",
      "best mean dice: 0.8166 at epoch: 30\n",
      "time consuming of epoch 30 is: 88.5998\n",
      "----------\n",
      "epoch 31/300\n",
      "1/223, train_loss: 0.1535, step time: 0.1114\n",
      "2/223, train_loss: 0.1638, step time: 0.1190\n",
      "3/223, train_loss: 0.1423, step time: 0.1296\n",
      "4/223, train_loss: 0.1535, step time: 0.1115\n",
      "5/223, train_loss: 0.1417, step time: 0.0999\n",
      "6/223, train_loss: 0.1448, step time: 0.1054\n",
      "7/223, train_loss: 0.1459, step time: 0.1016\n",
      "8/223, train_loss: 0.1513, step time: 0.1157\n",
      "9/223, train_loss: 0.1499, step time: 0.1046\n",
      "10/223, train_loss: 0.1330, step time: 0.0997\n",
      "11/223, train_loss: 0.1571, step time: 0.1003\n",
      "12/223, train_loss: 0.1521, step time: 0.1012\n",
      "13/223, train_loss: 0.1541, step time: 0.1304\n",
      "14/223, train_loss: 0.1310, step time: 0.1130\n",
      "15/223, train_loss: 0.1326, step time: 0.1119\n",
      "16/223, train_loss: 0.1526, step time: 0.1076\n",
      "17/223, train_loss: 0.1350, step time: 0.1247\n",
      "18/223, train_loss: 0.1514, step time: 0.1028\n",
      "19/223, train_loss: 0.1521, step time: 0.1006\n",
      "20/223, train_loss: 0.1292, step time: 0.1007\n",
      "21/223, train_loss: 0.1348, step time: 0.1065\n",
      "22/223, train_loss: 0.1502, step time: 0.1003\n",
      "23/223, train_loss: 0.1565, step time: 0.1006\n",
      "24/223, train_loss: 0.1336, step time: 0.1199\n",
      "25/223, train_loss: 0.1393, step time: 0.1068\n",
      "26/223, train_loss: 0.1569, step time: 0.1104\n",
      "27/223, train_loss: 0.1570, step time: 0.1120\n",
      "28/223, train_loss: 0.1622, step time: 0.1129\n",
      "29/223, train_loss: 0.1459, step time: 0.1174\n",
      "30/223, train_loss: 0.1452, step time: 0.1091\n",
      "31/223, train_loss: 0.1327, step time: 0.1131\n",
      "32/223, train_loss: 0.1344, step time: 0.1246\n",
      "33/223, train_loss: 0.1449, step time: 0.1003\n",
      "34/223, train_loss: 0.1590, step time: 0.1174\n",
      "35/223, train_loss: 0.1297, step time: 0.1138\n",
      "36/223, train_loss: 0.1381, step time: 0.1017\n",
      "37/223, train_loss: 0.1510, step time: 0.1047\n",
      "38/223, train_loss: 0.1437, step time: 0.1171\n",
      "39/223, train_loss: 0.1462, step time: 0.1104\n",
      "40/223, train_loss: 0.1308, step time: 0.1148\n",
      "41/223, train_loss: 0.1434, step time: 0.1013\n",
      "42/223, train_loss: 0.1562, step time: 0.1055\n",
      "43/223, train_loss: 0.1550, step time: 0.1126\n",
      "44/223, train_loss: 0.1509, step time: 0.1212\n",
      "45/223, train_loss: 0.1457, step time: 0.1128\n",
      "46/223, train_loss: 0.1432, step time: 0.1000\n",
      "47/223, train_loss: 0.1550, step time: 0.1240\n",
      "48/223, train_loss: 0.1472, step time: 0.1166\n",
      "49/223, train_loss: 0.1424, step time: 0.1178\n",
      "50/223, train_loss: 0.1249, step time: 0.1172\n",
      "51/223, train_loss: 0.1386, step time: 0.1135\n",
      "52/223, train_loss: 0.1451, step time: 0.1108\n",
      "53/223, train_loss: 0.1503, step time: 0.1083\n",
      "54/223, train_loss: 0.1394, step time: 0.1216\n",
      "55/223, train_loss: 0.1573, step time: 0.1132\n",
      "56/223, train_loss: 0.1585, step time: 0.1160\n",
      "57/223, train_loss: 0.1546, step time: 0.1214\n",
      "58/223, train_loss: 0.1544, step time: 0.1002\n",
      "59/223, train_loss: 0.1626, step time: 0.1029\n",
      "60/223, train_loss: 0.1312, step time: 0.1123\n",
      "61/223, train_loss: 0.1470, step time: 0.1138\n",
      "62/223, train_loss: 0.1518, step time: 0.1006\n",
      "63/223, train_loss: 0.1375, step time: 0.1145\n",
      "64/223, train_loss: 0.1376, step time: 0.1138\n",
      "65/223, train_loss: 0.1570, step time: 0.1161\n",
      "66/223, train_loss: 0.1262, step time: 0.1180\n",
      "67/223, train_loss: 0.1427, step time: 0.1106\n",
      "68/223, train_loss: 0.1260, step time: 0.0984\n",
      "69/223, train_loss: 0.1460, step time: 0.1043\n",
      "70/223, train_loss: 0.1447, step time: 0.1041\n",
      "71/223, train_loss: 0.1358, step time: 0.1047\n",
      "72/223, train_loss: 0.1594, step time: 0.1032\n",
      "73/223, train_loss: 0.1481, step time: 0.1112\n",
      "74/223, train_loss: 0.1449, step time: 0.1049\n",
      "75/223, train_loss: 0.1360, step time: 0.1216\n",
      "76/223, train_loss: 0.1364, step time: 0.1191\n",
      "77/223, train_loss: 0.1417, step time: 0.1041\n",
      "78/223, train_loss: 0.1462, step time: 0.1086\n",
      "79/223, train_loss: 0.1391, step time: 0.1050\n",
      "80/223, train_loss: 0.1465, step time: 0.1112\n",
      "81/223, train_loss: 0.1472, step time: 0.1011\n",
      "82/223, train_loss: 0.1344, step time: 0.1022\n",
      "83/223, train_loss: 0.1352, step time: 0.1010\n",
      "84/223, train_loss: 0.1542, step time: 0.1007\n",
      "85/223, train_loss: 0.1402, step time: 0.1330\n",
      "86/223, train_loss: 0.1403, step time: 0.1135\n",
      "87/223, train_loss: 0.1615, step time: 0.1044\n",
      "88/223, train_loss: 0.1692, step time: 0.1005\n",
      "89/223, train_loss: 0.1447, step time: 0.1107\n",
      "90/223, train_loss: 0.1517, step time: 0.1014\n",
      "91/223, train_loss: 0.1417, step time: 0.1007\n",
      "92/223, train_loss: 0.1617, step time: 0.1013\n",
      "93/223, train_loss: 0.1424, step time: 0.1091\n",
      "94/223, train_loss: 0.1505, step time: 0.1008\n",
      "95/223, train_loss: 0.1579, step time: 0.1008\n",
      "96/223, train_loss: 0.1456, step time: 0.1212\n",
      "97/223, train_loss: 0.1307, step time: 0.1101\n",
      "98/223, train_loss: 0.1369, step time: 0.1114\n",
      "99/223, train_loss: 0.1488, step time: 0.1130\n",
      "100/223, train_loss: 0.1479, step time: 0.1096\n",
      "101/223, train_loss: 0.1592, step time: 0.1123\n",
      "102/223, train_loss: 0.1640, step time: 0.1131\n",
      "103/223, train_loss: 0.1559, step time: 0.1069\n",
      "104/223, train_loss: 0.1371, step time: 0.1049\n",
      "105/223, train_loss: 0.1455, step time: 0.1225\n",
      "106/223, train_loss: 0.1468, step time: 0.1060\n",
      "107/223, train_loss: 0.1577, step time: 0.1090\n",
      "108/223, train_loss: 0.1469, step time: 0.1080\n",
      "109/223, train_loss: 0.1397, step time: 0.1236\n",
      "110/223, train_loss: 0.1521, step time: 0.1163\n",
      "111/223, train_loss: 0.1341, step time: 0.1123\n",
      "112/223, train_loss: 0.1435, step time: 0.1077\n",
      "113/223, train_loss: 0.1318, step time: 0.1099\n",
      "114/223, train_loss: 0.1486, step time: 0.1100\n",
      "115/223, train_loss: 0.1490, step time: 0.1131\n",
      "116/223, train_loss: 0.1458, step time: 0.1221\n",
      "117/223, train_loss: 0.1417, step time: 0.1049\n",
      "118/223, train_loss: 0.1402, step time: 0.0996\n",
      "119/223, train_loss: 0.1448, step time: 0.1300\n",
      "120/223, train_loss: 0.1456, step time: 0.1155\n",
      "121/223, train_loss: 0.1570, step time: 0.1113\n",
      "122/223, train_loss: 0.1320, step time: 0.1163\n",
      "123/223, train_loss: 0.1319, step time: 0.1184\n",
      "124/223, train_loss: 0.1344, step time: 0.1061\n",
      "125/223, train_loss: 0.1496, step time: 0.1135\n",
      "126/223, train_loss: 0.1258, step time: 0.1042\n",
      "127/223, train_loss: 0.1398, step time: 0.1087\n",
      "128/223, train_loss: 0.1389, step time: 0.1011\n",
      "129/223, train_loss: 0.1370, step time: 0.1220\n",
      "130/223, train_loss: 0.1381, step time: 0.1131\n",
      "131/223, train_loss: 0.1417, step time: 0.1047\n",
      "132/223, train_loss: 0.1569, step time: 0.1072\n",
      "133/223, train_loss: 0.1410, step time: 0.1168\n",
      "134/223, train_loss: 0.1426, step time: 0.1092\n",
      "135/223, train_loss: 0.1539, step time: 0.1000\n",
      "136/223, train_loss: 0.1467, step time: 0.1016\n",
      "137/223, train_loss: 0.1317, step time: 0.1211\n",
      "138/223, train_loss: 0.1271, step time: 0.1111\n",
      "139/223, train_loss: 0.1509, step time: 0.1256\n",
      "140/223, train_loss: 0.1434, step time: 0.0999\n",
      "141/223, train_loss: 0.1498, step time: 0.1103\n",
      "142/223, train_loss: 0.1354, step time: 0.1005\n",
      "143/223, train_loss: 0.1421, step time: 0.0996\n",
      "144/223, train_loss: 0.1447, step time: 0.1083\n",
      "145/223, train_loss: 0.1652, step time: 0.1007\n",
      "146/223, train_loss: 0.1492, step time: 0.1012\n",
      "147/223, train_loss: 0.1382, step time: 0.0999\n",
      "148/223, train_loss: 0.1393, step time: 0.1021\n",
      "149/223, train_loss: 0.1443, step time: 0.1064\n",
      "150/223, train_loss: 0.1300, step time: 0.1004\n",
      "151/223, train_loss: 0.1524, step time: 0.1007\n",
      "152/223, train_loss: 0.1459, step time: 0.1315\n",
      "153/223, train_loss: 0.1402, step time: 0.1122\n",
      "154/223, train_loss: 0.1397, step time: 0.1119\n",
      "155/223, train_loss: 0.1499, step time: 0.1123\n",
      "156/223, train_loss: 0.1354, step time: 0.1000\n",
      "157/223, train_loss: 0.1252, step time: 0.1022\n",
      "158/223, train_loss: 0.1436, step time: 0.1003\n",
      "159/223, train_loss: 0.1452, step time: 0.0999\n",
      "160/223, train_loss: 0.1541, step time: 0.1038\n",
      "161/223, train_loss: 0.1476, step time: 0.1251\n",
      "162/223, train_loss: 0.1320, step time: 0.1037\n",
      "163/223, train_loss: 0.1548, step time: 0.1005\n",
      "164/223, train_loss: 0.1321, step time: 0.1240\n",
      "165/223, train_loss: 0.1379, step time: 0.1145\n",
      "166/223, train_loss: 0.1453, step time: 0.1157\n",
      "167/223, train_loss: 0.1472, step time: 0.1263\n",
      "168/223, train_loss: 0.1311, step time: 0.1180\n",
      "169/223, train_loss: 0.1446, step time: 0.1158\n",
      "170/223, train_loss: 0.1451, step time: 0.1112\n",
      "171/223, train_loss: 0.1460, step time: 0.1079\n",
      "172/223, train_loss: 0.1573, step time: 0.1098\n",
      "173/223, train_loss: 0.1559, step time: 0.1068\n",
      "174/223, train_loss: 0.1709, step time: 0.1033\n",
      "175/223, train_loss: 0.1580, step time: 0.1186\n",
      "176/223, train_loss: 0.1374, step time: 0.1002\n",
      "177/223, train_loss: 0.1498, step time: 0.1007\n",
      "178/223, train_loss: 0.1450, step time: 0.1151\n",
      "179/223, train_loss: 0.1352, step time: 0.1011\n",
      "180/223, train_loss: 0.1340, step time: 0.1055\n",
      "181/223, train_loss: 0.1394, step time: 0.1077\n",
      "182/223, train_loss: 0.1354, step time: 0.1112\n",
      "183/223, train_loss: 0.1403, step time: 0.1006\n",
      "184/223, train_loss: 0.1302, step time: 0.1071\n",
      "185/223, train_loss: 0.1383, step time: 0.1126\n",
      "186/223, train_loss: 0.1306, step time: 0.1044\n",
      "187/223, train_loss: 0.1375, step time: 0.1189\n",
      "188/223, train_loss: 0.1304, step time: 0.1134\n",
      "189/223, train_loss: 0.1404, step time: 0.1249\n",
      "190/223, train_loss: 0.1400, step time: 0.1146\n",
      "191/223, train_loss: 0.1563, step time: 0.1278\n",
      "192/223, train_loss: 0.1522, step time: 0.1097\n",
      "193/223, train_loss: 0.1353, step time: 0.1075\n",
      "194/223, train_loss: 0.1468, step time: 0.1016\n",
      "195/223, train_loss: 0.1575, step time: 0.0997\n",
      "196/223, train_loss: 0.1401, step time: 0.1031\n",
      "197/223, train_loss: 0.1485, step time: 0.1059\n",
      "198/223, train_loss: 0.1475, step time: 0.1267\n",
      "199/223, train_loss: 0.1680, step time: 0.1278\n",
      "200/223, train_loss: 0.1283, step time: 0.0995\n",
      "201/223, train_loss: 0.1344, step time: 0.1002\n",
      "202/223, train_loss: 0.1485, step time: 0.0988\n",
      "203/223, train_loss: 0.1591, step time: 0.1041\n",
      "204/223, train_loss: 0.1400, step time: 0.1209\n",
      "205/223, train_loss: 0.3443, step time: 0.1013\n",
      "206/223, train_loss: 0.1656, step time: 0.1002\n",
      "207/223, train_loss: 0.1530, step time: 0.0994\n",
      "208/223, train_loss: 0.1342, step time: 0.1027\n",
      "209/223, train_loss: 0.1497, step time: 0.1010\n",
      "210/223, train_loss: 0.1388, step time: 0.0997\n",
      "211/223, train_loss: 0.1450, step time: 0.1001\n",
      "212/223, train_loss: 0.1593, step time: 0.1073\n",
      "213/223, train_loss: 0.1534, step time: 0.1060\n",
      "214/223, train_loss: 0.1566, step time: 0.1298\n",
      "215/223, train_loss: 0.1493, step time: 0.1169\n",
      "216/223, train_loss: 0.1437, step time: 0.1267\n",
      "217/223, train_loss: 0.1479, step time: 0.1002\n",
      "218/223, train_loss: 0.1397, step time: 0.1000\n",
      "219/223, train_loss: 0.1434, step time: 0.0995\n",
      "220/223, train_loss: 0.1423, step time: 0.1006\n",
      "221/223, train_loss: 0.1531, step time: 0.0999\n",
      "222/223, train_loss: 0.1596, step time: 0.0995\n",
      "223/223, train_loss: 0.1567, step time: 0.1002\n",
      "epoch 31 average loss: 0.1460\n",
      "time consuming of epoch 31 is: 86.2494\n",
      "----------\n",
      "epoch 32/300\n",
      "1/223, train_loss: 0.1590, step time: 0.1070\n",
      "2/223, train_loss: 0.1674, step time: 0.1136\n",
      "3/223, train_loss: 0.1477, step time: 0.1274\n",
      "4/223, train_loss: 0.1335, step time: 0.1239\n",
      "5/223, train_loss: 0.1532, step time: 0.1191\n",
      "6/223, train_loss: 0.1448, step time: 0.1266\n",
      "7/223, train_loss: 0.1515, step time: 0.1243\n",
      "8/223, train_loss: 0.1441, step time: 0.1256\n",
      "9/223, train_loss: 0.1386, step time: 0.1053\n",
      "10/223, train_loss: 0.1525, step time: 0.1175\n",
      "11/223, train_loss: 0.1406, step time: 0.1187\n",
      "12/223, train_loss: 0.1592, step time: 0.1104\n",
      "13/223, train_loss: 0.1566, step time: 0.1057\n",
      "14/223, train_loss: 0.1447, step time: 0.1116\n",
      "15/223, train_loss: 0.1588, step time: 0.1008\n",
      "16/223, train_loss: 0.1505, step time: 0.1013\n",
      "17/223, train_loss: 0.1640, step time: 0.1124\n",
      "18/223, train_loss: 0.1376, step time: 0.1045\n",
      "19/223, train_loss: 0.1678, step time: 0.1313\n",
      "20/223, train_loss: 0.1587, step time: 0.1102\n",
      "21/223, train_loss: 0.1407, step time: 0.1187\n",
      "22/223, train_loss: 0.1417, step time: 0.1020\n",
      "23/223, train_loss: 0.1501, step time: 0.1133\n",
      "24/223, train_loss: 0.1514, step time: 0.1002\n",
      "25/223, train_loss: 0.1487, step time: 0.1033\n",
      "26/223, train_loss: 0.1513, step time: 0.1004\n",
      "27/223, train_loss: 0.1331, step time: 0.1002\n",
      "28/223, train_loss: 0.1304, step time: 0.1004\n",
      "29/223, train_loss: 0.1539, step time: 0.0992\n",
      "30/223, train_loss: 0.1430, step time: 0.0997\n",
      "31/223, train_loss: 0.1444, step time: 0.1140\n",
      "32/223, train_loss: 0.1618, step time: 0.1031\n",
      "33/223, train_loss: 0.1437, step time: 0.1163\n",
      "34/223, train_loss: 0.1537, step time: 0.1060\n",
      "35/223, train_loss: 0.1386, step time: 0.1068\n",
      "36/223, train_loss: 0.1471, step time: 0.1064\n",
      "37/223, train_loss: 0.1497, step time: 0.1617\n",
      "38/223, train_loss: 0.1468, step time: 0.1001\n",
      "39/223, train_loss: 0.1735, step time: 0.1057\n",
      "40/223, train_loss: 0.1329, step time: 0.1313\n",
      "41/223, train_loss: 0.1529, step time: 0.1102\n",
      "42/223, train_loss: 0.1522, step time: 0.1100\n",
      "43/223, train_loss: 0.1399, step time: 0.1064\n",
      "44/223, train_loss: 0.1465, step time: 0.1102\n",
      "45/223, train_loss: 0.1386, step time: 0.1087\n",
      "46/223, train_loss: 0.1768, step time: 0.1062\n",
      "47/223, train_loss: 0.1480, step time: 0.1146\n",
      "48/223, train_loss: 0.1471, step time: 0.1101\n",
      "49/223, train_loss: 0.1453, step time: 0.1005\n",
      "50/223, train_loss: 0.1344, step time: 0.1067\n",
      "51/223, train_loss: 0.1537, step time: 0.1008\n",
      "52/223, train_loss: 0.1458, step time: 0.1008\n",
      "53/223, train_loss: 0.1396, step time: 0.1006\n",
      "54/223, train_loss: 0.1386, step time: 0.1312\n",
      "55/223, train_loss: 0.1441, step time: 0.1212\n",
      "56/223, train_loss: 0.1493, step time: 0.1063\n",
      "57/223, train_loss: 0.1527, step time: 0.1198\n",
      "58/223, train_loss: 0.1410, step time: 0.1119\n",
      "59/223, train_loss: 0.1410, step time: 0.1159\n",
      "60/223, train_loss: 0.1516, step time: 0.1003\n",
      "61/223, train_loss: 0.1559, step time: 0.1215\n",
      "62/223, train_loss: 0.1438, step time: 0.1004\n",
      "63/223, train_loss: 0.1479, step time: 0.1006\n",
      "64/223, train_loss: 0.1500, step time: 0.1188\n",
      "65/223, train_loss: 0.1454, step time: 0.1137\n",
      "66/223, train_loss: 0.1424, step time: 0.1129\n",
      "67/223, train_loss: 0.1366, step time: 0.1006\n",
      "68/223, train_loss: 0.1477, step time: 0.1004\n",
      "69/223, train_loss: 0.1500, step time: 0.1004\n",
      "70/223, train_loss: 0.1458, step time: 0.1099\n",
      "71/223, train_loss: 0.1360, step time: 0.1118\n",
      "72/223, train_loss: 0.1475, step time: 0.1222\n",
      "73/223, train_loss: 0.1333, step time: 0.0999\n",
      "74/223, train_loss: 0.1679, step time: 0.1209\n",
      "75/223, train_loss: 0.1498, step time: 0.1703\n",
      "76/223, train_loss: 0.1427, step time: 0.1057\n",
      "77/223, train_loss: 0.1326, step time: 0.1057\n",
      "78/223, train_loss: 0.1291, step time: 0.1097\n",
      "79/223, train_loss: 0.1453, step time: 0.1029\n",
      "80/223, train_loss: 0.1491, step time: 0.1075\n",
      "81/223, train_loss: 0.1329, step time: 0.1163\n",
      "82/223, train_loss: 0.1420, step time: 0.1139\n",
      "83/223, train_loss: 0.1554, step time: 0.1113\n",
      "84/223, train_loss: 0.1442, step time: 0.1092\n",
      "85/223, train_loss: 0.1574, step time: 0.1145\n",
      "86/223, train_loss: 0.1608, step time: 0.1172\n",
      "87/223, train_loss: 0.1436, step time: 0.1139\n",
      "88/223, train_loss: 0.1399, step time: 0.1081\n",
      "89/223, train_loss: 0.1648, step time: 0.1149\n",
      "90/223, train_loss: 0.1373, step time: 0.1133\n",
      "91/223, train_loss: 0.1487, step time: 0.1142\n",
      "92/223, train_loss: 0.1333, step time: 0.1004\n",
      "93/223, train_loss: 0.1424, step time: 0.1013\n",
      "94/223, train_loss: 0.1320, step time: 0.0998\n",
      "95/223, train_loss: 0.1453, step time: 0.1034\n",
      "96/223, train_loss: 0.1354, step time: 0.0996\n",
      "97/223, train_loss: 0.1613, step time: 0.1009\n",
      "98/223, train_loss: 0.1619, step time: 0.1014\n",
      "99/223, train_loss: 0.1465, step time: 0.1233\n",
      "100/223, train_loss: 0.1460, step time: 0.1004\n",
      "101/223, train_loss: 0.1282, step time: 0.1007\n",
      "102/223, train_loss: 0.1620, step time: 0.1086\n",
      "103/223, train_loss: 0.1250, step time: 0.1003\n",
      "104/223, train_loss: 0.1423, step time: 0.1003\n",
      "105/223, train_loss: 0.1698, step time: 0.1012\n",
      "106/223, train_loss: 0.1455, step time: 0.1077\n",
      "107/223, train_loss: 0.1342, step time: 0.1283\n",
      "108/223, train_loss: 0.1387, step time: 0.1212\n",
      "109/223, train_loss: 0.1495, step time: 0.1001\n",
      "110/223, train_loss: 0.1572, step time: 0.1386\n",
      "111/223, train_loss: 0.1412, step time: 0.1011\n",
      "112/223, train_loss: 0.1426, step time: 0.1004\n",
      "113/223, train_loss: 0.1458, step time: 0.1003\n",
      "114/223, train_loss: 0.1560, step time: 0.1053\n",
      "115/223, train_loss: 0.1561, step time: 0.1056\n",
      "116/223, train_loss: 0.1493, step time: 0.1028\n",
      "117/223, train_loss: 0.1443, step time: 0.1010\n",
      "118/223, train_loss: 0.1424, step time: 0.1173\n",
      "119/223, train_loss: 0.1405, step time: 0.1119\n",
      "120/223, train_loss: 0.1561, step time: 0.1056\n",
      "121/223, train_loss: 0.1446, step time: 0.0999\n",
      "122/223, train_loss: 0.3272, step time: 0.1141\n",
      "123/223, train_loss: 0.1396, step time: 0.1052\n",
      "124/223, train_loss: 0.1352, step time: 0.1228\n",
      "125/223, train_loss: 0.1307, step time: 0.1000\n",
      "126/223, train_loss: 0.1262, step time: 0.1119\n",
      "127/223, train_loss: 0.1610, step time: 0.1045\n",
      "128/223, train_loss: 0.1575, step time: 0.1004\n",
      "129/223, train_loss: 0.1394, step time: 0.1023\n",
      "130/223, train_loss: 0.1563, step time: 0.1112\n",
      "131/223, train_loss: 0.1433, step time: 0.1065\n",
      "132/223, train_loss: 0.1497, step time: 0.1205\n",
      "133/223, train_loss: 0.1468, step time: 0.1171\n",
      "134/223, train_loss: 0.1354, step time: 0.1146\n",
      "135/223, train_loss: 0.1479, step time: 0.1096\n",
      "136/223, train_loss: 0.1584, step time: 0.1180\n",
      "137/223, train_loss: 0.1490, step time: 0.0994\n",
      "138/223, train_loss: 0.1483, step time: 0.1081\n",
      "139/223, train_loss: 0.1421, step time: 0.1162\n",
      "140/223, train_loss: 0.1364, step time: 0.1236\n",
      "141/223, train_loss: 0.1423, step time: 0.1196\n",
      "142/223, train_loss: 0.1420, step time: 0.1225\n",
      "143/223, train_loss: 0.1332, step time: 0.1017\n",
      "144/223, train_loss: 0.1457, step time: 0.1323\n",
      "145/223, train_loss: 0.1495, step time: 0.1098\n",
      "146/223, train_loss: 0.1319, step time: 0.1326\n",
      "147/223, train_loss: 0.1450, step time: 0.1108\n",
      "148/223, train_loss: 0.1581, step time: 0.1157\n",
      "149/223, train_loss: 0.1505, step time: 0.1043\n",
      "150/223, train_loss: 0.1291, step time: 0.1016\n",
      "151/223, train_loss: 0.1523, step time: 0.1078\n",
      "152/223, train_loss: 0.1388, step time: 0.1259\n",
      "153/223, train_loss: 0.1406, step time: 0.1048\n",
      "154/223, train_loss: 0.1410, step time: 0.1143\n",
      "155/223, train_loss: 0.1578, step time: 0.0992\n",
      "156/223, train_loss: 0.1310, step time: 0.1087\n",
      "157/223, train_loss: 0.1379, step time: 0.1076\n",
      "158/223, train_loss: 0.1317, step time: 0.1162\n",
      "159/223, train_loss: 0.1481, step time: 0.1092\n",
      "160/223, train_loss: 0.1540, step time: 0.1310\n",
      "161/223, train_loss: 0.1437, step time: 0.1094\n",
      "162/223, train_loss: 0.1406, step time: 0.1464\n",
      "163/223, train_loss: 0.1697, step time: 0.1002\n",
      "164/223, train_loss: 0.1575, step time: 0.1039\n",
      "165/223, train_loss: 0.1496, step time: 0.1013\n",
      "166/223, train_loss: 0.1527, step time: 0.1101\n",
      "167/223, train_loss: 0.1576, step time: 0.1106\n",
      "168/223, train_loss: 0.1479, step time: 0.1100\n",
      "169/223, train_loss: 0.1492, step time: 0.1133\n",
      "170/223, train_loss: 0.1597, step time: 0.1055\n",
      "171/223, train_loss: 0.1387, step time: 0.1208\n",
      "172/223, train_loss: 0.1507, step time: 0.1147\n",
      "173/223, train_loss: 0.1419, step time: 0.0996\n",
      "174/223, train_loss: 0.1460, step time: 0.1133\n",
      "175/223, train_loss: 0.1377, step time: 0.0987\n",
      "176/223, train_loss: 0.1488, step time: 0.1041\n",
      "177/223, train_loss: 0.1415, step time: 0.1005\n",
      "178/223, train_loss: 0.1346, step time: 0.1068\n",
      "179/223, train_loss: 0.1492, step time: 0.1008\n",
      "180/223, train_loss: 0.1346, step time: 0.1046\n",
      "181/223, train_loss: 0.1608, step time: 0.0987\n",
      "182/223, train_loss: 0.1327, step time: 0.1012\n",
      "183/223, train_loss: 0.1313, step time: 0.1004\n",
      "184/223, train_loss: 0.1723, step time: 0.1094\n",
      "185/223, train_loss: 0.1492, step time: 0.1174\n",
      "186/223, train_loss: 0.1575, step time: 0.1137\n",
      "187/223, train_loss: 0.1345, step time: 0.1110\n",
      "188/223, train_loss: 0.1399, step time: 0.0997\n",
      "189/223, train_loss: 0.1447, step time: 0.1032\n",
      "190/223, train_loss: 0.1386, step time: 0.1000\n",
      "191/223, train_loss: 0.1470, step time: 0.1002\n",
      "192/223, train_loss: 0.1572, step time: 0.1005\n",
      "193/223, train_loss: 0.1501, step time: 0.1299\n",
      "194/223, train_loss: 0.1365, step time: 0.1048\n",
      "195/223, train_loss: 0.1406, step time: 0.1001\n",
      "196/223, train_loss: 0.1447, step time: 0.1001\n",
      "197/223, train_loss: 0.1486, step time: 0.1005\n",
      "198/223, train_loss: 0.1252, step time: 0.1002\n",
      "199/223, train_loss: 0.1294, step time: 0.0996\n",
      "200/223, train_loss: 0.1543, step time: 0.1021\n",
      "201/223, train_loss: 0.1266, step time: 0.1031\n",
      "202/223, train_loss: 0.1559, step time: 0.1305\n",
      "203/223, train_loss: 0.1361, step time: 0.1028\n",
      "204/223, train_loss: 0.1269, step time: 0.1008\n",
      "205/223, train_loss: 0.1503, step time: 0.1010\n",
      "206/223, train_loss: 0.1480, step time: 0.1022\n",
      "207/223, train_loss: 0.1538, step time: 0.0999\n",
      "208/223, train_loss: 0.1337, step time: 0.0998\n",
      "209/223, train_loss: 0.1432, step time: 0.1401\n",
      "210/223, train_loss: 0.1331, step time: 0.1176\n",
      "211/223, train_loss: 0.1534, step time: 0.1036\n",
      "212/223, train_loss: 0.1313, step time: 0.1170\n",
      "213/223, train_loss: 0.1541, step time: 0.1014\n",
      "214/223, train_loss: 0.1567, step time: 0.1014\n",
      "215/223, train_loss: 0.1546, step time: 0.1007\n",
      "216/223, train_loss: 0.1427, step time: 0.0999\n",
      "217/223, train_loss: 0.1521, step time: 0.1060\n",
      "218/223, train_loss: 0.1350, step time: 0.1002\n",
      "219/223, train_loss: 0.1509, step time: 0.0993\n",
      "220/223, train_loss: 0.1376, step time: 0.0991\n",
      "221/223, train_loss: 0.1432, step time: 0.1005\n",
      "222/223, train_loss: 0.1524, step time: 0.0992\n",
      "223/223, train_loss: 0.1504, step time: 0.0994\n",
      "epoch 32 average loss: 0.1469\n",
      "time consuming of epoch 32 is: 87.3099\n",
      "----------\n",
      "epoch 33/300\n",
      "1/223, train_loss: 0.1542, step time: 0.1128\n",
      "2/223, train_loss: 0.1443, step time: 0.1135\n",
      "3/223, train_loss: 0.1473, step time: 0.1060\n",
      "4/223, train_loss: 0.1666, step time: 0.1010\n",
      "5/223, train_loss: 0.1599, step time: 0.1132\n",
      "6/223, train_loss: 0.1556, step time: 0.1059\n",
      "7/223, train_loss: 0.1493, step time: 0.1777\n",
      "8/223, train_loss: 0.1363, step time: 0.1000\n",
      "9/223, train_loss: 0.1445, step time: 0.1238\n",
      "10/223, train_loss: 0.1293, step time: 0.1004\n",
      "11/223, train_loss: 0.1434, step time: 0.1007\n",
      "12/223, train_loss: 0.1459, step time: 0.1052\n",
      "13/223, train_loss: 0.1450, step time: 0.1188\n",
      "14/223, train_loss: 0.1514, step time: 0.1080\n",
      "15/223, train_loss: 0.1419, step time: 0.1007\n",
      "16/223, train_loss: 0.1323, step time: 0.1123\n",
      "17/223, train_loss: 0.1585, step time: 0.1167\n",
      "18/223, train_loss: 0.1588, step time: 0.1120\n",
      "19/223, train_loss: 0.1455, step time: 0.1000\n",
      "20/223, train_loss: 0.1435, step time: 0.1092\n",
      "21/223, train_loss: 0.1485, step time: 0.1036\n",
      "22/223, train_loss: 0.1571, step time: 0.1000\n",
      "23/223, train_loss: 0.1306, step time: 0.1085\n",
      "24/223, train_loss: 0.1485, step time: 0.1132\n",
      "25/223, train_loss: 0.1445, step time: 0.1138\n",
      "26/223, train_loss: 0.1278, step time: 0.1067\n",
      "27/223, train_loss: 0.1423, step time: 0.1233\n",
      "28/223, train_loss: 0.1504, step time: 0.1125\n",
      "29/223, train_loss: 0.1576, step time: 0.1105\n",
      "30/223, train_loss: 0.1368, step time: 0.1160\n",
      "31/223, train_loss: 0.1576, step time: 0.1109\n",
      "32/223, train_loss: 0.1374, step time: 0.1110\n",
      "33/223, train_loss: 0.1297, step time: 0.1100\n",
      "34/223, train_loss: 0.1467, step time: 0.1168\n",
      "35/223, train_loss: 0.1301, step time: 0.1102\n",
      "36/223, train_loss: 0.1408, step time: 0.1148\n",
      "37/223, train_loss: 0.1438, step time: 0.1072\n",
      "38/223, train_loss: 0.1578, step time: 0.1094\n",
      "39/223, train_loss: 0.1274, step time: 0.1013\n",
      "40/223, train_loss: 0.1456, step time: 0.1005\n",
      "41/223, train_loss: 0.1511, step time: 0.1054\n",
      "42/223, train_loss: 0.1515, step time: 0.1148\n",
      "43/223, train_loss: 0.3381, step time: 0.1033\n",
      "44/223, train_loss: 0.1437, step time: 0.1061\n",
      "45/223, train_loss: 0.1439, step time: 0.1125\n",
      "46/223, train_loss: 0.1401, step time: 0.0994\n",
      "47/223, train_loss: 0.1294, step time: 0.1003\n",
      "48/223, train_loss: 0.1375, step time: 0.1228\n",
      "49/223, train_loss: 0.1307, step time: 0.1092\n",
      "50/223, train_loss: 0.1567, step time: 0.1032\n",
      "51/223, train_loss: 0.1317, step time: 0.0996\n",
      "52/223, train_loss: 0.1454, step time: 0.1005\n",
      "53/223, train_loss: 0.1548, step time: 0.1000\n",
      "54/223, train_loss: 0.1523, step time: 0.0998\n",
      "55/223, train_loss: 0.1312, step time: 0.0998\n",
      "56/223, train_loss: 0.1574, step time: 0.1034\n",
      "57/223, train_loss: 0.1668, step time: 0.0995\n",
      "58/223, train_loss: 0.1505, step time: 0.1077\n",
      "59/223, train_loss: 0.1448, step time: 0.0997\n",
      "60/223, train_loss: 0.1414, step time: 0.1125\n",
      "61/223, train_loss: 0.1255, step time: 0.0994\n",
      "62/223, train_loss: 0.1505, step time: 0.1175\n",
      "63/223, train_loss: 0.1375, step time: 0.1254\n",
      "64/223, train_loss: 0.1326, step time: 0.1002\n",
      "65/223, train_loss: 0.1276, step time: 0.1303\n",
      "66/223, train_loss: 0.1344, step time: 0.1011\n",
      "67/223, train_loss: 0.1433, step time: 0.1003\n",
      "68/223, train_loss: 0.1459, step time: 0.0999\n",
      "69/223, train_loss: 0.1399, step time: 0.1096\n",
      "70/223, train_loss: 0.1577, step time: 0.1037\n",
      "71/223, train_loss: 0.1475, step time: 0.1012\n",
      "72/223, train_loss: 0.1465, step time: 0.1078\n",
      "73/223, train_loss: 0.1426, step time: 0.1103\n",
      "74/223, train_loss: 0.1493, step time: 0.1144\n",
      "75/223, train_loss: 0.1481, step time: 0.1016\n",
      "76/223, train_loss: 0.1407, step time: 0.1040\n",
      "77/223, train_loss: 0.1628, step time: 0.0997\n",
      "78/223, train_loss: 0.1401, step time: 0.1176\n",
      "79/223, train_loss: 0.1588, step time: 0.1292\n",
      "80/223, train_loss: 0.1524, step time: 0.1001\n",
      "81/223, train_loss: 0.1422, step time: 0.1003\n",
      "82/223, train_loss: 0.1597, step time: 0.1150\n",
      "83/223, train_loss: 0.1485, step time: 0.1539\n",
      "84/223, train_loss: 0.1372, step time: 0.1278\n",
      "85/223, train_loss: 0.1347, step time: 0.1068\n",
      "86/223, train_loss: 0.1408, step time: 0.1204\n",
      "87/223, train_loss: 0.1397, step time: 0.1000\n",
      "88/223, train_loss: 0.1508, step time: 0.1016\n",
      "89/223, train_loss: 0.1560, step time: 0.1171\n",
      "90/223, train_loss: 0.1469, step time: 0.1019\n",
      "91/223, train_loss: 0.1532, step time: 0.1170\n",
      "92/223, train_loss: 0.1521, step time: 0.1053\n",
      "93/223, train_loss: 0.1457, step time: 0.1129\n",
      "94/223, train_loss: 0.1258, step time: 0.1062\n",
      "95/223, train_loss: 0.1322, step time: 0.1139\n",
      "96/223, train_loss: 0.1437, step time: 0.1342\n",
      "97/223, train_loss: 0.1561, step time: 0.1004\n",
      "98/223, train_loss: 0.1479, step time: 0.1142\n",
      "99/223, train_loss: 0.1333, step time: 0.0997\n",
      "100/223, train_loss: 0.1249, step time: 0.0996\n",
      "101/223, train_loss: 0.1460, step time: 0.1003\n",
      "102/223, train_loss: 0.1478, step time: 0.1123\n",
      "103/223, train_loss: 0.1432, step time: 0.1107\n",
      "104/223, train_loss: 0.1320, step time: 0.1195\n",
      "105/223, train_loss: 0.1429, step time: 0.1078\n",
      "106/223, train_loss: 0.1426, step time: 0.1147\n",
      "107/223, train_loss: 0.1504, step time: 0.1010\n",
      "108/223, train_loss: 0.1487, step time: 0.1000\n",
      "109/223, train_loss: 0.1374, step time: 0.1008\n",
      "110/223, train_loss: 0.1378, step time: 0.1310\n",
      "111/223, train_loss: 0.1371, step time: 0.1523\n",
      "112/223, train_loss: 0.1473, step time: 0.1008\n",
      "113/223, train_loss: 0.1455, step time: 0.1004\n",
      "114/223, train_loss: 0.1405, step time: 0.1123\n",
      "115/223, train_loss: 0.1423, step time: 0.0994\n",
      "116/223, train_loss: 0.1606, step time: 0.1036\n",
      "117/223, train_loss: 0.1405, step time: 0.1180\n",
      "118/223, train_loss: 0.1288, step time: 0.1070\n",
      "119/223, train_loss: 0.1492, step time: 0.1334\n",
      "120/223, train_loss: 0.1483, step time: 0.1156\n",
      "121/223, train_loss: 0.1409, step time: 0.1295\n",
      "122/223, train_loss: 0.1544, step time: 0.0995\n",
      "123/223, train_loss: 0.1370, step time: 0.1073\n",
      "124/223, train_loss: 0.1287, step time: 0.1051\n",
      "125/223, train_loss: 0.1473, step time: 0.1335\n",
      "126/223, train_loss: 0.1552, step time: 0.1089\n",
      "127/223, train_loss: 0.1357, step time: 0.1312\n",
      "128/223, train_loss: 0.1332, step time: 0.1312\n",
      "129/223, train_loss: 0.1505, step time: 0.1204\n",
      "130/223, train_loss: 0.1429, step time: 0.1052\n",
      "131/223, train_loss: 0.1270, step time: 0.1002\n",
      "132/223, train_loss: 0.1461, step time: 0.1003\n",
      "133/223, train_loss: 0.1444, step time: 0.1007\n",
      "134/223, train_loss: 0.1381, step time: 0.1017\n",
      "135/223, train_loss: 0.1377, step time: 0.0996\n",
      "136/223, train_loss: 0.1448, step time: 0.1009\n",
      "137/223, train_loss: 0.1358, step time: 0.1011\n",
      "138/223, train_loss: 0.1348, step time: 0.1095\n",
      "139/223, train_loss: 0.1394, step time: 0.0995\n",
      "140/223, train_loss: 0.1612, step time: 0.1203\n",
      "141/223, train_loss: 0.1248, step time: 0.1192\n",
      "142/223, train_loss: 0.1539, step time: 0.1196\n",
      "143/223, train_loss: 0.1475, step time: 0.1095\n",
      "144/223, train_loss: 0.1330, step time: 0.1151\n",
      "145/223, train_loss: 0.1514, step time: 0.1140\n",
      "146/223, train_loss: 0.1335, step time: 0.1030\n",
      "147/223, train_loss: 0.1273, step time: 0.1003\n",
      "148/223, train_loss: 0.1439, step time: 0.1007\n",
      "149/223, train_loss: 0.1389, step time: 0.1011\n",
      "150/223, train_loss: 0.1527, step time: 0.1080\n",
      "151/223, train_loss: 0.1372, step time: 0.1004\n",
      "152/223, train_loss: 0.1667, step time: 0.1147\n",
      "153/223, train_loss: 0.1471, step time: 0.1099\n",
      "154/223, train_loss: 0.1322, step time: 0.1291\n",
      "155/223, train_loss: 0.1493, step time: 0.0997\n",
      "156/223, train_loss: 0.1297, step time: 0.1002\n",
      "157/223, train_loss: 0.1402, step time: 0.1094\n",
      "158/223, train_loss: 0.1397, step time: 0.1002\n",
      "159/223, train_loss: 0.1470, step time: 0.1205\n",
      "160/223, train_loss: 0.1328, step time: 0.1303\n",
      "161/223, train_loss: 0.1399, step time: 0.1209\n",
      "162/223, train_loss: 0.1371, step time: 0.1221\n",
      "163/223, train_loss: 0.1621, step time: 0.1009\n",
      "164/223, train_loss: 0.1348, step time: 0.1004\n",
      "165/223, train_loss: 0.1438, step time: 0.1152\n",
      "166/223, train_loss: 0.1403, step time: 0.1148\n",
      "167/223, train_loss: 0.1293, step time: 0.1089\n",
      "168/223, train_loss: 0.1465, step time: 0.1003\n",
      "169/223, train_loss: 0.1364, step time: 0.1000\n",
      "170/223, train_loss: 0.1428, step time: 0.1167\n",
      "171/223, train_loss: 0.1451, step time: 0.1056\n",
      "172/223, train_loss: 0.1347, step time: 0.1002\n",
      "173/223, train_loss: 0.1374, step time: 0.1005\n",
      "174/223, train_loss: 0.1330, step time: 0.1027\n",
      "175/223, train_loss: 0.1481, step time: 0.1056\n",
      "176/223, train_loss: 0.1376, step time: 0.1087\n",
      "177/223, train_loss: 0.1365, step time: 0.1082\n",
      "178/223, train_loss: 0.1378, step time: 0.1068\n",
      "179/223, train_loss: 0.1355, step time: 0.1036\n",
      "180/223, train_loss: 0.1398, step time: 0.1010\n",
      "181/223, train_loss: 0.1333, step time: 0.1068\n",
      "182/223, train_loss: 0.1392, step time: 0.1137\n",
      "183/223, train_loss: 0.1464, step time: 0.0996\n",
      "184/223, train_loss: 0.1557, step time: 0.1005\n",
      "185/223, train_loss: 0.1301, step time: 0.1117\n",
      "186/223, train_loss: 0.1388, step time: 0.1003\n",
      "187/223, train_loss: 0.1522, step time: 0.1000\n",
      "188/223, train_loss: 0.1504, step time: 0.1000\n",
      "189/223, train_loss: 0.1498, step time: 0.1347\n",
      "190/223, train_loss: 0.1378, step time: 0.1022\n",
      "191/223, train_loss: 0.1380, step time: 0.1146\n",
      "192/223, train_loss: 0.1452, step time: 0.1003\n",
      "193/223, train_loss: 0.1353, step time: 0.1005\n",
      "194/223, train_loss: 0.1446, step time: 0.0995\n",
      "195/223, train_loss: 0.1387, step time: 0.1019\n",
      "196/223, train_loss: 0.1484, step time: 0.1146\n",
      "197/223, train_loss: 0.1400, step time: 0.0998\n",
      "198/223, train_loss: 0.1419, step time: 0.1033\n",
      "199/223, train_loss: 0.1596, step time: 0.1091\n",
      "200/223, train_loss: 0.1359, step time: 0.1003\n",
      "201/223, train_loss: 0.1523, step time: 0.1003\n",
      "202/223, train_loss: 0.1605, step time: 0.1155\n",
      "203/223, train_loss: 0.1205, step time: 0.1234\n",
      "204/223, train_loss: 0.1404, step time: 0.1017\n",
      "205/223, train_loss: 0.1489, step time: 0.1007\n",
      "206/223, train_loss: 0.1271, step time: 0.1133\n",
      "207/223, train_loss: 0.1517, step time: 0.1112\n",
      "208/223, train_loss: 0.1387, step time: 0.1017\n",
      "209/223, train_loss: 0.1346, step time: 0.1002\n",
      "210/223, train_loss: 0.1512, step time: 0.1244\n",
      "211/223, train_loss: 0.1336, step time: 0.1112\n",
      "212/223, train_loss: 0.1567, step time: 0.1122\n",
      "213/223, train_loss: 0.1426, step time: 0.1086\n",
      "214/223, train_loss: 0.1497, step time: 0.1002\n",
      "215/223, train_loss: 0.1381, step time: 0.1252\n",
      "216/223, train_loss: 0.1305, step time: 0.0998\n",
      "217/223, train_loss: 0.1427, step time: 0.1002\n",
      "218/223, train_loss: 0.1312, step time: 0.1005\n",
      "219/223, train_loss: 0.1277, step time: 0.1000\n",
      "220/223, train_loss: 0.1393, step time: 0.1005\n",
      "221/223, train_loss: 0.1271, step time: 0.1006\n",
      "222/223, train_loss: 0.1410, step time: 0.0987\n",
      "223/223, train_loss: 0.1385, step time: 0.0992\n",
      "epoch 33 average loss: 0.1439\n",
      "time consuming of epoch 33 is: 85.7060\n",
      "----------\n",
      "epoch 34/300\n",
      "1/223, train_loss: 0.1308, step time: 0.1298\n",
      "2/223, train_loss: 0.1268, step time: 0.1190\n",
      "3/223, train_loss: 0.1556, step time: 0.1140\n",
      "4/223, train_loss: 0.1532, step time: 0.1079\n",
      "5/223, train_loss: 0.1420, step time: 0.1082\n",
      "6/223, train_loss: 0.1453, step time: 0.1146\n",
      "7/223, train_loss: 0.1495, step time: 0.0999\n",
      "8/223, train_loss: 0.1388, step time: 0.1103\n",
      "9/223, train_loss: 0.1662, step time: 0.1065\n",
      "10/223, train_loss: 0.1345, step time: 0.0992\n",
      "11/223, train_loss: 0.1335, step time: 0.1114\n",
      "12/223, train_loss: 0.1344, step time: 0.1154\n",
      "13/223, train_loss: 0.1521, step time: 0.1090\n",
      "14/223, train_loss: 0.1524, step time: 0.1010\n",
      "15/223, train_loss: 0.1480, step time: 0.0998\n",
      "16/223, train_loss: 0.1362, step time: 0.1004\n",
      "17/223, train_loss: 0.1444, step time: 0.1138\n",
      "18/223, train_loss: 0.1459, step time: 0.1099\n",
      "19/223, train_loss: 0.1576, step time: 0.1045\n",
      "20/223, train_loss: 0.1532, step time: 0.1226\n",
      "21/223, train_loss: 0.1453, step time: 0.1162\n",
      "22/223, train_loss: 0.1519, step time: 0.1096\n",
      "23/223, train_loss: 0.1527, step time: 0.1266\n",
      "24/223, train_loss: 0.1428, step time: 0.1158\n",
      "25/223, train_loss: 0.1228, step time: 0.1277\n",
      "26/223, train_loss: 0.1348, step time: 0.0998\n",
      "27/223, train_loss: 0.1582, step time: 0.1041\n",
      "28/223, train_loss: 0.1472, step time: 0.1210\n",
      "29/223, train_loss: 0.1331, step time: 0.1015\n",
      "30/223, train_loss: 0.1341, step time: 0.1009\n",
      "31/223, train_loss: 0.1334, step time: 0.1057\n",
      "32/223, train_loss: 0.1405, step time: 0.1001\n",
      "33/223, train_loss: 0.1447, step time: 0.0999\n",
      "34/223, train_loss: 0.1265, step time: 0.1034\n",
      "35/223, train_loss: 0.1521, step time: 0.1188\n",
      "36/223, train_loss: 0.1561, step time: 0.1009\n",
      "37/223, train_loss: 0.1357, step time: 0.1081\n",
      "38/223, train_loss: 0.1324, step time: 0.1006\n",
      "39/223, train_loss: 0.1531, step time: 0.1036\n",
      "40/223, train_loss: 0.1332, step time: 0.0999\n",
      "41/223, train_loss: 0.1561, step time: 0.1137\n",
      "42/223, train_loss: 0.1568, step time: 0.1021\n",
      "43/223, train_loss: 0.1495, step time: 0.1002\n",
      "44/223, train_loss: 0.1460, step time: 0.1087\n",
      "45/223, train_loss: 0.1441, step time: 0.1067\n",
      "46/223, train_loss: 0.1493, step time: 0.1066\n",
      "47/223, train_loss: 0.1274, step time: 0.1012\n",
      "48/223, train_loss: 0.1368, step time: 0.1012\n",
      "49/223, train_loss: 0.1616, step time: 0.1110\n",
      "50/223, train_loss: 0.1519, step time: 0.0994\n",
      "51/223, train_loss: 0.1471, step time: 0.1040\n",
      "52/223, train_loss: 0.1270, step time: 0.1056\n",
      "53/223, train_loss: 0.1439, step time: 0.1140\n",
      "54/223, train_loss: 0.1485, step time: 0.1188\n",
      "55/223, train_loss: 0.1561, step time: 0.1196\n",
      "56/223, train_loss: 0.1449, step time: 0.1174\n",
      "57/223, train_loss: 0.1517, step time: 0.1032\n",
      "58/223, train_loss: 0.1299, step time: 0.1185\n",
      "59/223, train_loss: 0.1650, step time: 0.1123\n",
      "60/223, train_loss: 0.1278, step time: 0.1062\n",
      "61/223, train_loss: 0.1403, step time: 0.1126\n",
      "62/223, train_loss: 0.1285, step time: 0.1235\n",
      "63/223, train_loss: 0.1420, step time: 0.1051\n",
      "64/223, train_loss: 0.1451, step time: 0.1108\n",
      "65/223, train_loss: 0.1490, step time: 0.1111\n",
      "66/223, train_loss: 0.1667, step time: 0.1049\n",
      "67/223, train_loss: 0.1359, step time: 0.1203\n",
      "68/223, train_loss: 0.1661, step time: 0.1226\n",
      "69/223, train_loss: 0.1467, step time: 0.1143\n",
      "70/223, train_loss: 0.1583, step time: 0.1159\n",
      "71/223, train_loss: 0.1378, step time: 0.1078\n",
      "72/223, train_loss: 0.1384, step time: 0.1110\n",
      "73/223, train_loss: 0.1459, step time: 0.1060\n",
      "74/223, train_loss: 0.1253, step time: 0.1186\n",
      "75/223, train_loss: 0.1406, step time: 0.1163\n",
      "76/223, train_loss: 0.1310, step time: 0.1105\n",
      "77/223, train_loss: 0.1433, step time: 0.1219\n",
      "78/223, train_loss: 0.1409, step time: 0.1135\n",
      "79/223, train_loss: 0.1414, step time: 0.1001\n",
      "80/223, train_loss: 0.1447, step time: 0.1011\n",
      "81/223, train_loss: 0.1528, step time: 0.1007\n",
      "82/223, train_loss: 0.1303, step time: 0.0988\n",
      "83/223, train_loss: 0.1578, step time: 0.1003\n",
      "84/223, train_loss: 0.1614, step time: 0.0996\n",
      "85/223, train_loss: 0.1339, step time: 0.0995\n",
      "86/223, train_loss: 0.1251, step time: 0.0996\n",
      "87/223, train_loss: 0.1499, step time: 0.0993\n",
      "88/223, train_loss: 0.1349, step time: 0.1043\n",
      "89/223, train_loss: 0.1293, step time: 0.1201\n",
      "90/223, train_loss: 0.1393, step time: 0.1123\n",
      "91/223, train_loss: 0.1549, step time: 0.1094\n",
      "92/223, train_loss: 0.1318, step time: 0.1125\n",
      "93/223, train_loss: 0.1475, step time: 0.1102\n",
      "94/223, train_loss: 0.1287, step time: 0.1104\n",
      "95/223, train_loss: 0.1258, step time: 0.1007\n",
      "96/223, train_loss: 0.1403, step time: 0.1004\n",
      "97/223, train_loss: 0.1429, step time: 0.1143\n",
      "98/223, train_loss: 0.1351, step time: 0.1205\n",
      "99/223, train_loss: 0.1357, step time: 0.1045\n",
      "100/223, train_loss: 0.1482, step time: 0.1074\n",
      "101/223, train_loss: 0.1567, step time: 0.1070\n",
      "102/223, train_loss: 0.1383, step time: 0.0999\n",
      "103/223, train_loss: 0.1445, step time: 0.1273\n",
      "104/223, train_loss: 0.1509, step time: 0.1017\n",
      "105/223, train_loss: 0.1476, step time: 0.1293\n",
      "106/223, train_loss: 0.1273, step time: 0.1109\n",
      "107/223, train_loss: 0.1261, step time: 0.1058\n",
      "108/223, train_loss: 0.1451, step time: 0.1049\n",
      "109/223, train_loss: 0.1359, step time: 0.1062\n",
      "110/223, train_loss: 0.1514, step time: 0.0998\n",
      "111/223, train_loss: 0.1530, step time: 0.1003\n",
      "112/223, train_loss: 0.1542, step time: 0.1063\n",
      "113/223, train_loss: 0.1367, step time: 0.1058\n",
      "114/223, train_loss: 0.1398, step time: 0.1003\n",
      "115/223, train_loss: 0.1390, step time: 0.0999\n",
      "116/223, train_loss: 0.1512, step time: 0.1052\n",
      "117/223, train_loss: 0.1558, step time: 0.1199\n",
      "118/223, train_loss: 0.1517, step time: 0.1062\n",
      "119/223, train_loss: 0.1331, step time: 0.1004\n",
      "120/223, train_loss: 0.1429, step time: 0.1004\n",
      "121/223, train_loss: 0.1468, step time: 0.1051\n",
      "122/223, train_loss: 0.1373, step time: 0.1100\n",
      "123/223, train_loss: 0.1353, step time: 0.1007\n",
      "124/223, train_loss: 0.1261, step time: 0.1028\n",
      "125/223, train_loss: 0.1350, step time: 0.0999\n",
      "126/223, train_loss: 0.1518, step time: 0.1072\n",
      "127/223, train_loss: 0.1574, step time: 0.1048\n",
      "128/223, train_loss: 0.1406, step time: 0.1219\n",
      "129/223, train_loss: 0.1500, step time: 0.1183\n",
      "130/223, train_loss: 0.1446, step time: 0.1068\n",
      "131/223, train_loss: 0.1263, step time: 0.1012\n",
      "132/223, train_loss: 0.1440, step time: 0.1062\n",
      "133/223, train_loss: 0.1483, step time: 0.1117\n",
      "134/223, train_loss: 0.1429, step time: 0.1034\n",
      "135/223, train_loss: 0.1565, step time: 0.1098\n",
      "136/223, train_loss: 0.1469, step time: 0.1001\n",
      "137/223, train_loss: 0.1552, step time: 0.0997\n",
      "138/223, train_loss: 0.1349, step time: 0.1294\n",
      "139/223, train_loss: 0.1445, step time: 0.1126\n",
      "140/223, train_loss: 0.1364, step time: 0.1190\n",
      "141/223, train_loss: 0.1584, step time: 0.1028\n",
      "142/223, train_loss: 0.1466, step time: 0.1210\n",
      "143/223, train_loss: 0.1354, step time: 0.1054\n",
      "144/223, train_loss: 0.1506, step time: 0.1271\n",
      "145/223, train_loss: 0.1392, step time: 0.1007\n",
      "146/223, train_loss: 0.1471, step time: 0.1203\n",
      "147/223, train_loss: 0.1345, step time: 0.1032\n",
      "148/223, train_loss: 0.1505, step time: 0.1113\n",
      "149/223, train_loss: 0.1254, step time: 0.1007\n",
      "150/223, train_loss: 0.1362, step time: 0.0998\n",
      "151/223, train_loss: 0.1354, step time: 0.1008\n",
      "152/223, train_loss: 0.1342, step time: 0.1008\n",
      "153/223, train_loss: 0.1350, step time: 0.1144\n",
      "154/223, train_loss: 0.1307, step time: 0.1018\n",
      "155/223, train_loss: 0.1332, step time: 0.1199\n",
      "156/223, train_loss: 0.1300, step time: 0.1059\n",
      "157/223, train_loss: 0.1493, step time: 0.1058\n",
      "158/223, train_loss: 0.1506, step time: 0.1082\n",
      "159/223, train_loss: 0.1301, step time: 0.1016\n",
      "160/223, train_loss: 0.1255, step time: 0.1048\n",
      "161/223, train_loss: 0.1616, step time: 0.1009\n",
      "162/223, train_loss: 0.1447, step time: 0.0997\n",
      "163/223, train_loss: 0.1349, step time: 0.1000\n",
      "164/223, train_loss: 0.1469, step time: 0.1058\n",
      "165/223, train_loss: 0.1487, step time: 0.1144\n",
      "166/223, train_loss: 0.1567, step time: 0.1146\n",
      "167/223, train_loss: 0.1515, step time: 0.1001\n",
      "168/223, train_loss: 0.1517, step time: 0.1003\n",
      "169/223, train_loss: 0.1415, step time: 0.1070\n",
      "170/223, train_loss: 0.1332, step time: 0.1144\n",
      "171/223, train_loss: 0.1485, step time: 0.1087\n",
      "172/223, train_loss: 0.1338, step time: 0.0994\n",
      "173/223, train_loss: 0.1336, step time: 0.1004\n",
      "174/223, train_loss: 0.1536, step time: 0.1016\n",
      "175/223, train_loss: 0.1454, step time: 0.1005\n",
      "176/223, train_loss: 0.1513, step time: 0.1250\n",
      "177/223, train_loss: 0.1516, step time: 0.1100\n",
      "178/223, train_loss: 0.1648, step time: 0.0996\n",
      "179/223, train_loss: 0.1557, step time: 0.1003\n",
      "180/223, train_loss: 0.1353, step time: 0.1051\n",
      "181/223, train_loss: 0.1397, step time: 0.1001\n",
      "182/223, train_loss: 0.1567, step time: 0.1000\n",
      "183/223, train_loss: 0.1340, step time: 0.1041\n",
      "184/223, train_loss: 0.1417, step time: 0.1065\n",
      "185/223, train_loss: 0.1506, step time: 0.1041\n",
      "186/223, train_loss: 0.1510, step time: 0.0993\n",
      "187/223, train_loss: 0.1497, step time: 0.0999\n",
      "188/223, train_loss: 0.1436, step time: 0.1129\n",
      "189/223, train_loss: 0.1411, step time: 0.0998\n",
      "190/223, train_loss: 0.1354, step time: 0.1128\n",
      "191/223, train_loss: 0.1453, step time: 0.1011\n",
      "192/223, train_loss: 0.1454, step time: 0.1009\n",
      "193/223, train_loss: 0.1260, step time: 0.1127\n",
      "194/223, train_loss: 0.1463, step time: 0.1112\n",
      "195/223, train_loss: 0.1353, step time: 0.1055\n",
      "196/223, train_loss: 0.1427, step time: 0.1044\n",
      "197/223, train_loss: 0.1505, step time: 0.1158\n",
      "198/223, train_loss: 0.1319, step time: 0.1091\n",
      "199/223, train_loss: 0.1395, step time: 0.1157\n",
      "200/223, train_loss: 0.1452, step time: 0.1064\n",
      "201/223, train_loss: 0.1341, step time: 0.1145\n",
      "202/223, train_loss: 0.1278, step time: 0.1063\n",
      "203/223, train_loss: 0.1411, step time: 0.1149\n",
      "204/223, train_loss: 0.1348, step time: 0.1039\n",
      "205/223, train_loss: 0.1269, step time: 0.1143\n",
      "206/223, train_loss: 0.1414, step time: 0.1095\n",
      "207/223, train_loss: 0.1319, step time: 0.1077\n",
      "208/223, train_loss: 0.1580, step time: 0.1292\n",
      "209/223, train_loss: 0.1354, step time: 0.1017\n",
      "210/223, train_loss: 0.1401, step time: 0.1082\n",
      "211/223, train_loss: 0.1529, step time: 0.1258\n",
      "212/223, train_loss: 0.1583, step time: 0.1109\n",
      "213/223, train_loss: 0.1461, step time: 0.0999\n",
      "214/223, train_loss: 0.1446, step time: 0.1138\n",
      "215/223, train_loss: 0.1493, step time: 0.1279\n",
      "216/223, train_loss: 0.1548, step time: 0.1112\n",
      "217/223, train_loss: 0.3384, step time: 0.1084\n",
      "218/223, train_loss: 0.1235, step time: 0.1055\n",
      "219/223, train_loss: 0.1300, step time: 0.1189\n",
      "220/223, train_loss: 0.1549, step time: 0.1319\n",
      "221/223, train_loss: 0.1360, step time: 0.0997\n",
      "222/223, train_loss: 0.1384, step time: 0.1002\n",
      "223/223, train_loss: 0.1375, step time: 0.0995\n",
      "epoch 34 average loss: 0.1438\n",
      "time consuming of epoch 34 is: 88.1413\n",
      "----------\n",
      "epoch 35/300\n",
      "1/223, train_loss: 0.1249, step time: 0.1215\n",
      "2/223, train_loss: 0.1433, step time: 0.1004\n",
      "3/223, train_loss: 0.1347, step time: 0.1585\n",
      "4/223, train_loss: 0.1471, step time: 0.1027\n",
      "5/223, train_loss: 0.1386, step time: 0.1211\n",
      "6/223, train_loss: 0.1213, step time: 0.1000\n",
      "7/223, train_loss: 0.1495, step time: 0.1165\n",
      "8/223, train_loss: 0.1168, step time: 0.1172\n",
      "9/223, train_loss: 0.1443, step time: 0.1106\n",
      "10/223, train_loss: 0.1411, step time: 0.0997\n",
      "11/223, train_loss: 0.1433, step time: 0.1008\n",
      "12/223, train_loss: 0.1220, step time: 0.1047\n",
      "13/223, train_loss: 0.1508, step time: 0.1074\n",
      "14/223, train_loss: 0.1288, step time: 0.1060\n",
      "15/223, train_loss: 0.1374, step time: 0.1005\n",
      "16/223, train_loss: 0.1531, step time: 0.1007\n",
      "17/223, train_loss: 0.1387, step time: 0.1085\n",
      "18/223, train_loss: 0.1545, step time: 0.0999\n",
      "19/223, train_loss: 0.1374, step time: 0.1010\n",
      "20/223, train_loss: 0.1496, step time: 0.1010\n",
      "21/223, train_loss: 0.1628, step time: 0.1131\n",
      "22/223, train_loss: 0.1641, step time: 0.1045\n",
      "23/223, train_loss: 0.1350, step time: 0.1084\n",
      "24/223, train_loss: 0.1405, step time: 0.1008\n",
      "25/223, train_loss: 0.1637, step time: 0.1059\n",
      "26/223, train_loss: 0.1461, step time: 0.1005\n",
      "27/223, train_loss: 0.1244, step time: 0.1049\n",
      "28/223, train_loss: 0.1351, step time: 0.1104\n",
      "29/223, train_loss: 0.1387, step time: 0.1223\n",
      "30/223, train_loss: 0.1402, step time: 0.1028\n",
      "31/223, train_loss: 0.1399, step time: 0.1006\n",
      "32/223, train_loss: 0.1320, step time: 0.1010\n",
      "33/223, train_loss: 0.1427, step time: 0.1320\n",
      "34/223, train_loss: 0.1364, step time: 0.1098\n",
      "35/223, train_loss: 0.1372, step time: 0.1000\n",
      "36/223, train_loss: 0.1414, step time: 0.1079\n",
      "37/223, train_loss: 0.1491, step time: 0.1167\n",
      "38/223, train_loss: 0.1410, step time: 0.1021\n",
      "39/223, train_loss: 0.1449, step time: 0.1076\n",
      "40/223, train_loss: 0.1348, step time: 0.1096\n",
      "41/223, train_loss: 0.1367, step time: 0.1011\n",
      "42/223, train_loss: 0.1488, step time: 0.1017\n",
      "43/223, train_loss: 0.1346, step time: 0.1128\n",
      "44/223, train_loss: 0.1283, step time: 0.1000\n",
      "45/223, train_loss: 0.1318, step time: 0.0992\n",
      "46/223, train_loss: 0.1343, step time: 0.1141\n",
      "47/223, train_loss: 0.1387, step time: 0.1098\n",
      "48/223, train_loss: 0.1331, step time: 0.1120\n",
      "49/223, train_loss: 0.1436, step time: 0.1044\n",
      "50/223, train_loss: 0.1419, step time: 0.1004\n",
      "51/223, train_loss: 0.1325, step time: 0.1075\n",
      "52/223, train_loss: 0.1391, step time: 0.1012\n",
      "53/223, train_loss: 0.1536, step time: 0.1129\n",
      "54/223, train_loss: 0.1437, step time: 0.1267\n",
      "55/223, train_loss: 0.1347, step time: 0.1002\n",
      "56/223, train_loss: 0.1296, step time: 0.1023\n",
      "57/223, train_loss: 0.1380, step time: 0.1108\n",
      "58/223, train_loss: 0.1243, step time: 0.1157\n",
      "59/223, train_loss: 0.1364, step time: 0.1001\n",
      "60/223, train_loss: 0.1503, step time: 0.1003\n",
      "61/223, train_loss: 0.1402, step time: 0.1082\n",
      "62/223, train_loss: 0.1392, step time: 0.1250\n",
      "63/223, train_loss: 0.1542, step time: 0.1203\n",
      "64/223, train_loss: 0.1319, step time: 0.1174\n",
      "65/223, train_loss: 0.1426, step time: 0.1188\n",
      "66/223, train_loss: 0.1292, step time: 0.1096\n",
      "67/223, train_loss: 0.1259, step time: 0.1029\n",
      "68/223, train_loss: 0.1273, step time: 0.1078\n",
      "69/223, train_loss: 0.1422, step time: 0.1221\n",
      "70/223, train_loss: 0.1431, step time: 0.0999\n",
      "71/223, train_loss: 0.1392, step time: 0.1189\n",
      "72/223, train_loss: 0.1535, step time: 0.1009\n",
      "73/223, train_loss: 0.1430, step time: 0.1138\n",
      "74/223, train_loss: 0.1366, step time: 0.1000\n",
      "75/223, train_loss: 0.1448, step time: 0.0993\n",
      "76/223, train_loss: 0.1504, step time: 0.1002\n",
      "77/223, train_loss: 0.1582, step time: 0.1006\n",
      "78/223, train_loss: 0.1336, step time: 0.1003\n",
      "79/223, train_loss: 0.1290, step time: 0.1091\n",
      "80/223, train_loss: 0.1358, step time: 0.1079\n",
      "81/223, train_loss: 0.1487, step time: 0.1162\n",
      "82/223, train_loss: 0.1490, step time: 0.1009\n",
      "83/223, train_loss: 0.1313, step time: 0.1064\n",
      "84/223, train_loss: 0.1455, step time: 0.1178\n",
      "85/223, train_loss: 0.1554, step time: 0.1133\n",
      "86/223, train_loss: 0.1469, step time: 0.1097\n",
      "87/223, train_loss: 0.1328, step time: 0.1333\n",
      "88/223, train_loss: 0.1423, step time: 0.1096\n",
      "89/223, train_loss: 0.1380, step time: 0.1088\n",
      "90/223, train_loss: 0.1404, step time: 0.1276\n",
      "91/223, train_loss: 0.1291, step time: 0.1103\n",
      "92/223, train_loss: 0.1332, step time: 0.0992\n",
      "93/223, train_loss: 0.1656, step time: 0.1183\n",
      "94/223, train_loss: 0.1319, step time: 0.1018\n",
      "95/223, train_loss: 0.1171, step time: 0.1201\n",
      "96/223, train_loss: 0.1460, step time: 0.1005\n",
      "97/223, train_loss: 0.1478, step time: 0.1004\n",
      "98/223, train_loss: 0.1613, step time: 0.1076\n",
      "99/223, train_loss: 0.1278, step time: 0.1011\n",
      "100/223, train_loss: 0.1411, step time: 0.0990\n",
      "101/223, train_loss: 0.1529, step time: 0.1009\n",
      "102/223, train_loss: 0.1304, step time: 0.1012\n",
      "103/223, train_loss: 0.1471, step time: 0.1170\n",
      "104/223, train_loss: 0.1546, step time: 0.1096\n",
      "105/223, train_loss: 0.1380, step time: 0.1098\n",
      "106/223, train_loss: 0.1417, step time: 0.1079\n",
      "107/223, train_loss: 0.1333, step time: 0.1086\n",
      "108/223, train_loss: 0.1540, step time: 0.1136\n",
      "109/223, train_loss: 0.1244, step time: 0.1005\n",
      "110/223, train_loss: 0.1445, step time: 0.1047\n",
      "111/223, train_loss: 0.1401, step time: 0.1229\n",
      "112/223, train_loss: 0.1353, step time: 0.1120\n",
      "113/223, train_loss: 0.1300, step time: 0.1064\n",
      "114/223, train_loss: 0.1306, step time: 0.0999\n",
      "115/223, train_loss: 0.1534, step time: 0.1184\n",
      "116/223, train_loss: 0.1415, step time: 0.1056\n",
      "117/223, train_loss: 0.1373, step time: 0.1174\n",
      "118/223, train_loss: 0.1372, step time: 0.1015\n",
      "119/223, train_loss: 0.1391, step time: 0.1249\n",
      "120/223, train_loss: 0.1305, step time: 0.1012\n",
      "121/223, train_loss: 0.1333, step time: 0.1011\n",
      "122/223, train_loss: 0.1329, step time: 0.1064\n",
      "123/223, train_loss: 0.1505, step time: 0.1042\n",
      "124/223, train_loss: 0.1532, step time: 0.1254\n",
      "125/223, train_loss: 0.1323, step time: 0.1173\n",
      "126/223, train_loss: 0.1452, step time: 0.1096\n",
      "127/223, train_loss: 0.1222, step time: 0.1066\n",
      "128/223, train_loss: 0.1369, step time: 0.1061\n",
      "129/223, train_loss: 0.1358, step time: 0.1285\n",
      "130/223, train_loss: 0.1487, step time: 0.1113\n",
      "131/223, train_loss: 0.1597, step time: 0.1167\n",
      "132/223, train_loss: 0.1364, step time: 0.1058\n",
      "133/223, train_loss: 0.1539, step time: 0.1242\n",
      "134/223, train_loss: 0.1351, step time: 0.1077\n",
      "135/223, train_loss: 0.1345, step time: 0.1064\n",
      "136/223, train_loss: 0.1373, step time: 0.1061\n",
      "137/223, train_loss: 0.1358, step time: 0.1073\n",
      "138/223, train_loss: 0.1534, step time: 0.1374\n",
      "139/223, train_loss: 0.1409, step time: 0.1182\n",
      "140/223, train_loss: 0.1265, step time: 0.1163\n",
      "141/223, train_loss: 0.1273, step time: 0.1374\n",
      "142/223, train_loss: 0.1477, step time: 0.1104\n",
      "143/223, train_loss: 0.1420, step time: 0.1152\n",
      "144/223, train_loss: 0.1438, step time: 0.1007\n",
      "145/223, train_loss: 0.1404, step time: 0.1001\n",
      "146/223, train_loss: 0.1489, step time: 0.1055\n",
      "147/223, train_loss: 0.1343, step time: 0.1123\n",
      "148/223, train_loss: 0.1613, step time: 0.1115\n",
      "149/223, train_loss: 0.1448, step time: 0.1025\n",
      "150/223, train_loss: 0.1364, step time: 0.1003\n",
      "151/223, train_loss: 0.1376, step time: 0.1185\n",
      "152/223, train_loss: 0.1464, step time: 0.1095\n",
      "153/223, train_loss: 0.1508, step time: 0.1212\n",
      "154/223, train_loss: 0.1293, step time: 0.1230\n",
      "155/223, train_loss: 0.1533, step time: 0.1073\n",
      "156/223, train_loss: 0.1352, step time: 0.1007\n",
      "157/223, train_loss: 0.1384, step time: 0.1128\n",
      "158/223, train_loss: 0.1433, step time: 0.1016\n",
      "159/223, train_loss: 0.1556, step time: 0.1057\n",
      "160/223, train_loss: 0.1415, step time: 0.1236\n",
      "161/223, train_loss: 0.1348, step time: 0.1127\n",
      "162/223, train_loss: 0.1370, step time: 0.1008\n",
      "163/223, train_loss: 0.1389, step time: 0.1007\n",
      "164/223, train_loss: 0.1424, step time: 0.1006\n",
      "165/223, train_loss: 0.1294, step time: 0.1000\n",
      "166/223, train_loss: 0.1412, step time: 0.1048\n",
      "167/223, train_loss: 0.1302, step time: 0.0999\n",
      "168/223, train_loss: 0.1231, step time: 0.1069\n",
      "169/223, train_loss: 0.1485, step time: 0.1053\n",
      "170/223, train_loss: 0.1366, step time: 0.0998\n",
      "171/223, train_loss: 0.1334, step time: 0.1198\n",
      "172/223, train_loss: 0.1489, step time: 0.1161\n",
      "173/223, train_loss: 0.1361, step time: 0.1006\n",
      "174/223, train_loss: 0.1614, step time: 0.0998\n",
      "175/223, train_loss: 0.1304, step time: 0.1097\n",
      "176/223, train_loss: 0.1364, step time: 0.0994\n",
      "177/223, train_loss: 0.1437, step time: 0.1054\n",
      "178/223, train_loss: 0.1351, step time: 0.1191\n",
      "179/223, train_loss: 0.1486, step time: 0.1145\n",
      "180/223, train_loss: 0.1322, step time: 0.1000\n",
      "181/223, train_loss: 0.1326, step time: 0.0999\n",
      "182/223, train_loss: 0.1461, step time: 0.1020\n",
      "183/223, train_loss: 0.1270, step time: 0.1140\n",
      "184/223, train_loss: 0.3320, step time: 0.0996\n",
      "185/223, train_loss: 0.1503, step time: 0.0994\n",
      "186/223, train_loss: 0.1484, step time: 0.1129\n",
      "187/223, train_loss: 0.1672, step time: 0.0998\n",
      "188/223, train_loss: 0.1513, step time: 0.1182\n",
      "189/223, train_loss: 0.1576, step time: 0.1001\n",
      "190/223, train_loss: 0.1276, step time: 0.1001\n",
      "191/223, train_loss: 0.1235, step time: 0.1376\n",
      "192/223, train_loss: 0.1313, step time: 0.0994\n",
      "193/223, train_loss: 0.1483, step time: 0.0995\n",
      "194/223, train_loss: 0.1252, step time: 0.0994\n",
      "195/223, train_loss: 0.1356, step time: 0.1270\n",
      "196/223, train_loss: 0.1251, step time: 0.1197\n",
      "197/223, train_loss: 0.1491, step time: 0.1011\n",
      "198/223, train_loss: 0.1303, step time: 0.1004\n",
      "199/223, train_loss: 0.1211, step time: 0.1079\n",
      "200/223, train_loss: 0.1334, step time: 0.1166\n",
      "201/223, train_loss: 0.1280, step time: 0.1188\n",
      "202/223, train_loss: 0.1295, step time: 0.0998\n",
      "203/223, train_loss: 0.1476, step time: 0.1059\n",
      "204/223, train_loss: 0.1409, step time: 0.1172\n",
      "205/223, train_loss: 0.1417, step time: 0.1003\n",
      "206/223, train_loss: 0.1494, step time: 0.1151\n",
      "207/223, train_loss: 0.1388, step time: 0.1045\n",
      "208/223, train_loss: 0.1560, step time: 0.1432\n",
      "209/223, train_loss: 0.1622, step time: 0.1251\n",
      "210/223, train_loss: 0.1367, step time: 0.0994\n",
      "211/223, train_loss: 0.1437, step time: 0.0994\n",
      "212/223, train_loss: 0.1352, step time: 0.1259\n",
      "213/223, train_loss: 0.1329, step time: 0.1306\n",
      "214/223, train_loss: 0.1584, step time: 0.1000\n",
      "215/223, train_loss: 0.1575, step time: 0.1001\n",
      "216/223, train_loss: 0.1445, step time: 0.0997\n",
      "217/223, train_loss: 0.1378, step time: 0.1007\n",
      "218/223, train_loss: 0.1419, step time: 0.1032\n",
      "219/223, train_loss: 0.1475, step time: 0.1004\n",
      "220/223, train_loss: 0.1440, step time: 0.0992\n",
      "221/223, train_loss: 0.1374, step time: 0.1004\n",
      "222/223, train_loss: 0.1447, step time: 0.1055\n",
      "223/223, train_loss: 0.1285, step time: 0.0992\n",
      "epoch 35 average loss: 0.1412\n",
      "saved new best metric model\n",
      "current epoch: 35 current mean dice: 0.8224 tc: 0.9045 wt: 0.8382 et: 0.7247\n",
      "best mean dice: 0.8224 at epoch: 35\n",
      "time consuming of epoch 35 is: 92.0134\n",
      "----------\n",
      "epoch 36/300\n",
      "1/223, train_loss: 0.1465, step time: 0.1059\n",
      "2/223, train_loss: 0.1416, step time: 0.0995\n",
      "3/223, train_loss: 0.1432, step time: 0.1014\n",
      "4/223, train_loss: 0.1561, step time: 0.1005\n",
      "5/223, train_loss: 0.1571, step time: 0.1021\n",
      "6/223, train_loss: 0.1626, step time: 0.1055\n",
      "7/223, train_loss: 0.1359, step time: 0.1004\n",
      "8/223, train_loss: 0.1290, step time: 0.1009\n",
      "9/223, train_loss: 0.1483, step time: 0.1058\n",
      "10/223, train_loss: 0.1416, step time: 0.1211\n",
      "11/223, train_loss: 0.1484, step time: 0.1247\n",
      "12/223, train_loss: 0.1478, step time: 0.1198\n",
      "13/223, train_loss: 0.1307, step time: 0.1000\n",
      "14/223, train_loss: 0.1295, step time: 0.1070\n",
      "15/223, train_loss: 0.1295, step time: 0.1014\n",
      "16/223, train_loss: 0.1362, step time: 0.1050\n",
      "17/223, train_loss: 0.1404, step time: 0.1101\n",
      "18/223, train_loss: 0.1410, step time: 0.1098\n",
      "19/223, train_loss: 0.1167, step time: 0.1254\n",
      "20/223, train_loss: 0.1375, step time: 0.1156\n",
      "21/223, train_loss: 0.1307, step time: 0.1097\n",
      "22/223, train_loss: 0.1310, step time: 0.0995\n",
      "23/223, train_loss: 0.1327, step time: 0.1008\n",
      "24/223, train_loss: 0.1363, step time: 0.1082\n",
      "25/223, train_loss: 0.1517, step time: 0.1152\n",
      "26/223, train_loss: 0.1444, step time: 0.1114\n",
      "27/223, train_loss: 0.1321, step time: 0.1263\n",
      "28/223, train_loss: 0.1584, step time: 0.1130\n",
      "29/223, train_loss: 0.1428, step time: 0.1052\n",
      "30/223, train_loss: 0.1367, step time: 0.1003\n",
      "31/223, train_loss: 0.1473, step time: 0.1000\n",
      "32/223, train_loss: 0.1430, step time: 0.1036\n",
      "33/223, train_loss: 0.1356, step time: 0.1203\n",
      "34/223, train_loss: 0.1522, step time: 0.1343\n",
      "35/223, train_loss: 0.1473, step time: 0.1008\n",
      "36/223, train_loss: 0.1478, step time: 0.1002\n",
      "37/223, train_loss: 0.1365, step time: 0.1052\n",
      "38/223, train_loss: 0.1347, step time: 0.0997\n",
      "39/223, train_loss: 0.1339, step time: 0.0997\n",
      "40/223, train_loss: 0.1350, step time: 0.1017\n",
      "41/223, train_loss: 0.1362, step time: 0.1219\n",
      "42/223, train_loss: 0.1379, step time: 0.1032\n",
      "43/223, train_loss: 0.1373, step time: 0.1075\n",
      "44/223, train_loss: 0.1376, step time: 0.1003\n",
      "45/223, train_loss: 0.1496, step time: 0.1225\n",
      "46/223, train_loss: 0.1221, step time: 0.1017\n",
      "47/223, train_loss: 0.1399, step time: 0.1015\n",
      "48/223, train_loss: 0.1563, step time: 0.1164\n",
      "49/223, train_loss: 0.1311, step time: 0.1147\n",
      "50/223, train_loss: 0.1440, step time: 0.1012\n",
      "51/223, train_loss: 0.1413, step time: 0.1002\n",
      "52/223, train_loss: 0.1393, step time: 0.1005\n",
      "53/223, train_loss: 0.1277, step time: 0.1131\n",
      "54/223, train_loss: 0.1264, step time: 0.1055\n",
      "55/223, train_loss: 0.1290, step time: 0.1004\n",
      "56/223, train_loss: 0.1402, step time: 0.1087\n",
      "57/223, train_loss: 0.1356, step time: 0.1013\n",
      "58/223, train_loss: 0.3306, step time: 0.1014\n",
      "59/223, train_loss: 0.1273, step time: 0.1007\n",
      "60/223, train_loss: 0.1360, step time: 0.1023\n",
      "61/223, train_loss: 0.1426, step time: 0.1108\n",
      "62/223, train_loss: 0.1484, step time: 0.1122\n",
      "63/223, train_loss: 0.1573, step time: 0.1007\n",
      "64/223, train_loss: 0.1452, step time: 0.1003\n",
      "65/223, train_loss: 0.1399, step time: 0.1150\n",
      "66/223, train_loss: 0.1279, step time: 0.1002\n",
      "67/223, train_loss: 0.1406, step time: 0.1006\n",
      "68/223, train_loss: 0.1317, step time: 0.1012\n",
      "69/223, train_loss: 0.1443, step time: 0.1180\n",
      "70/223, train_loss: 0.1493, step time: 0.1162\n",
      "71/223, train_loss: 0.1243, step time: 0.1086\n",
      "72/223, train_loss: 0.1430, step time: 0.1034\n",
      "73/223, train_loss: 0.1567, step time: 0.1214\n",
      "74/223, train_loss: 0.1618, step time: 0.1179\n",
      "75/223, train_loss: 0.1286, step time: 0.1005\n",
      "76/223, train_loss: 0.1418, step time: 0.1242\n",
      "77/223, train_loss: 0.1449, step time: 0.1179\n",
      "78/223, train_loss: 0.1438, step time: 0.1041\n",
      "79/223, train_loss: 0.1342, step time: 0.1035\n",
      "80/223, train_loss: 0.1248, step time: 0.0997\n",
      "81/223, train_loss: 0.1546, step time: 0.0998\n",
      "82/223, train_loss: 0.1539, step time: 0.1007\n",
      "83/223, train_loss: 0.1543, step time: 0.1008\n",
      "84/223, train_loss: 0.1406, step time: 0.1015\n",
      "85/223, train_loss: 0.1537, step time: 0.1005\n",
      "86/223, train_loss: 0.1286, step time: 0.1005\n",
      "87/223, train_loss: 0.1399, step time: 0.1071\n",
      "88/223, train_loss: 0.1485, step time: 0.1150\n",
      "89/223, train_loss: 0.1606, step time: 0.1084\n",
      "90/223, train_loss: 0.1307, step time: 0.1054\n",
      "91/223, train_loss: 0.1317, step time: 0.1181\n",
      "92/223, train_loss: 0.1466, step time: 0.1174\n",
      "93/223, train_loss: 0.1315, step time: 0.1159\n",
      "94/223, train_loss: 0.1545, step time: 0.1006\n",
      "95/223, train_loss: 0.1299, step time: 0.1003\n",
      "96/223, train_loss: 0.1433, step time: 0.1035\n",
      "97/223, train_loss: 0.1394, step time: 0.1134\n",
      "98/223, train_loss: 0.1329, step time: 0.1163\n",
      "99/223, train_loss: 0.1321, step time: 0.1172\n",
      "100/223, train_loss: 0.1288, step time: 0.1105\n",
      "101/223, train_loss: 0.1510, step time: 0.1050\n",
      "102/223, train_loss: 0.1464, step time: 0.1148\n",
      "103/223, train_loss: 0.1236, step time: 0.1113\n",
      "104/223, train_loss: 0.1227, step time: 0.1207\n",
      "105/223, train_loss: 0.1503, step time: 0.1105\n",
      "106/223, train_loss: 0.1298, step time: 0.1042\n",
      "107/223, train_loss: 0.1439, step time: 0.1361\n",
      "108/223, train_loss: 0.1574, step time: 0.1197\n",
      "109/223, train_loss: 0.1360, step time: 0.1101\n",
      "110/223, train_loss: 0.1539, step time: 0.1320\n",
      "111/223, train_loss: 0.1406, step time: 0.1048\n",
      "112/223, train_loss: 0.1215, step time: 0.0996\n",
      "113/223, train_loss: 0.1454, step time: 0.1034\n",
      "114/223, train_loss: 0.1464, step time: 0.1257\n",
      "115/223, train_loss: 0.1447, step time: 0.1011\n",
      "116/223, train_loss: 0.1305, step time: 0.1003\n",
      "117/223, train_loss: 0.1337, step time: 0.0996\n",
      "118/223, train_loss: 0.1316, step time: 0.0994\n",
      "119/223, train_loss: 0.1398, step time: 0.0991\n",
      "120/223, train_loss: 0.1314, step time: 0.1010\n",
      "121/223, train_loss: 0.1417, step time: 0.0996\n",
      "122/223, train_loss: 0.1361, step time: 0.0996\n",
      "123/223, train_loss: 0.1284, step time: 0.0995\n",
      "124/223, train_loss: 0.1363, step time: 0.0997\n",
      "125/223, train_loss: 0.1271, step time: 0.0994\n",
      "126/223, train_loss: 0.1347, step time: 0.1000\n",
      "127/223, train_loss: 0.1371, step time: 0.0998\n",
      "128/223, train_loss: 0.1293, step time: 0.1131\n",
      "129/223, train_loss: 0.1353, step time: 0.1216\n",
      "130/223, train_loss: 0.1263, step time: 0.1118\n",
      "131/223, train_loss: 0.1415, step time: 0.1106\n",
      "132/223, train_loss: 0.1474, step time: 0.1337\n",
      "133/223, train_loss: 0.1460, step time: 0.1089\n",
      "134/223, train_loss: 0.1221, step time: 0.1190\n",
      "135/223, train_loss: 0.1269, step time: 0.1119\n",
      "136/223, train_loss: 0.1391, step time: 0.1356\n",
      "137/223, train_loss: 0.1514, step time: 0.1175\n",
      "138/223, train_loss: 0.1200, step time: 0.1048\n",
      "139/223, train_loss: 0.1437, step time: 0.1086\n",
      "140/223, train_loss: 0.1541, step time: 0.1183\n",
      "141/223, train_loss: 0.1370, step time: 0.1109\n",
      "142/223, train_loss: 0.1384, step time: 0.1195\n",
      "143/223, train_loss: 0.1328, step time: 0.1121\n",
      "144/223, train_loss: 0.1461, step time: 0.1112\n",
      "145/223, train_loss: 0.1694, step time: 0.1219\n",
      "146/223, train_loss: 0.1465, step time: 0.1128\n",
      "147/223, train_loss: 0.1462, step time: 0.1055\n",
      "148/223, train_loss: 0.1394, step time: 0.1065\n",
      "149/223, train_loss: 0.1300, step time: 0.1138\n",
      "150/223, train_loss: 0.1750, step time: 0.1215\n",
      "151/223, train_loss: 0.1281, step time: 0.1168\n",
      "152/223, train_loss: 0.1546, step time: 0.1082\n",
      "153/223, train_loss: 0.1426, step time: 0.1000\n",
      "154/223, train_loss: 0.1606, step time: 0.1025\n",
      "155/223, train_loss: 0.1268, step time: 0.1047\n",
      "156/223, train_loss: 0.1346, step time: 0.1129\n",
      "157/223, train_loss: 0.1372, step time: 0.1215\n",
      "158/223, train_loss: 0.1394, step time: 0.1065\n",
      "159/223, train_loss: 0.1427, step time: 0.1236\n",
      "160/223, train_loss: 0.1622, step time: 0.1000\n",
      "161/223, train_loss: 0.1290, step time: 0.1167\n",
      "162/223, train_loss: 0.1526, step time: 0.1057\n",
      "163/223, train_loss: 0.1517, step time: 0.1275\n",
      "164/223, train_loss: 0.1293, step time: 0.1188\n",
      "165/223, train_loss: 0.1557, step time: 0.1200\n",
      "166/223, train_loss: 0.1578, step time: 0.1113\n",
      "167/223, train_loss: 0.1478, step time: 0.1076\n",
      "168/223, train_loss: 0.1454, step time: 0.1248\n",
      "169/223, train_loss: 0.1428, step time: 0.1152\n",
      "170/223, train_loss: 0.1492, step time: 0.1088\n",
      "171/223, train_loss: 0.1299, step time: 0.0998\n",
      "172/223, train_loss: 0.1257, step time: 0.1100\n",
      "173/223, train_loss: 0.1287, step time: 0.1004\n",
      "174/223, train_loss: 0.1582, step time: 0.1007\n",
      "175/223, train_loss: 0.1381, step time: 0.0998\n",
      "176/223, train_loss: 0.1314, step time: 0.1066\n",
      "177/223, train_loss: 0.1551, step time: 0.1095\n",
      "178/223, train_loss: 0.1479, step time: 0.1222\n",
      "179/223, train_loss: 0.1392, step time: 0.1227\n",
      "180/223, train_loss: 0.1303, step time: 0.1033\n",
      "181/223, train_loss: 0.1372, step time: 0.1084\n",
      "182/223, train_loss: 0.1452, step time: 0.1259\n",
      "183/223, train_loss: 0.1374, step time: 0.1137\n",
      "184/223, train_loss: 0.1477, step time: 0.1118\n",
      "185/223, train_loss: 0.1346, step time: 0.1031\n",
      "186/223, train_loss: 0.1291, step time: 0.0989\n",
      "187/223, train_loss: 0.1342, step time: 0.0995\n",
      "188/223, train_loss: 0.1295, step time: 0.1067\n",
      "189/223, train_loss: 0.1356, step time: 0.1076\n",
      "190/223, train_loss: 0.1331, step time: 0.1032\n",
      "191/223, train_loss: 0.1397, step time: 0.1007\n",
      "192/223, train_loss: 0.1320, step time: 0.1007\n",
      "193/223, train_loss: 0.1406, step time: 0.0995\n",
      "194/223, train_loss: 0.1498, step time: 0.0993\n",
      "195/223, train_loss: 0.1438, step time: 0.1004\n",
      "196/223, train_loss: 0.1377, step time: 0.1034\n",
      "197/223, train_loss: 0.1544, step time: 0.0992\n",
      "198/223, train_loss: 0.1292, step time: 0.0992\n",
      "199/223, train_loss: 0.1473, step time: 0.1003\n",
      "200/223, train_loss: 0.1380, step time: 0.0992\n",
      "201/223, train_loss: 0.1343, step time: 0.0990\n",
      "202/223, train_loss: 0.1337, step time: 0.0996\n",
      "203/223, train_loss: 0.1470, step time: 0.0999\n",
      "204/223, train_loss: 0.1402, step time: 0.1030\n",
      "205/223, train_loss: 0.1400, step time: 0.1009\n",
      "206/223, train_loss: 0.1309, step time: 0.1008\n",
      "207/223, train_loss: 0.1376, step time: 0.1166\n",
      "208/223, train_loss: 0.1486, step time: 0.1054\n",
      "209/223, train_loss: 0.1647, step time: 0.1067\n",
      "210/223, train_loss: 0.1268, step time: 0.1075\n",
      "211/223, train_loss: 0.1462, step time: 0.1139\n",
      "212/223, train_loss: 0.1355, step time: 0.1235\n",
      "213/223, train_loss: 0.1450, step time: 0.1093\n",
      "214/223, train_loss: 0.1491, step time: 0.1140\n",
      "215/223, train_loss: 0.1442, step time: 0.1167\n",
      "216/223, train_loss: 0.1355, step time: 0.1142\n",
      "217/223, train_loss: 0.1433, step time: 0.1077\n",
      "218/223, train_loss: 0.1502, step time: 0.1036\n",
      "219/223, train_loss: 0.1251, step time: 0.0998\n",
      "220/223, train_loss: 0.1342, step time: 0.1449\n",
      "221/223, train_loss: 0.1177, step time: 0.0995\n",
      "222/223, train_loss: 0.1525, step time: 0.0983\n",
      "223/223, train_loss: 0.1399, step time: 0.0989\n",
      "epoch 36 average loss: 0.1411\n",
      "time consuming of epoch 36 is: 91.8542\n",
      "----------\n",
      "epoch 37/300\n",
      "1/223, train_loss: 0.1454, step time: 0.1129\n",
      "2/223, train_loss: 0.1302, step time: 0.1043\n",
      "3/223, train_loss: 0.1458, step time: 0.1058\n",
      "4/223, train_loss: 0.1429, step time: 0.1179\n",
      "5/223, train_loss: 0.1352, step time: 0.0998\n",
      "6/223, train_loss: 0.1427, step time: 0.1006\n",
      "7/223, train_loss: 0.1226, step time: 0.1006\n",
      "8/223, train_loss: 0.1469, step time: 0.1018\n",
      "9/223, train_loss: 0.1488, step time: 0.1000\n",
      "10/223, train_loss: 0.1503, step time: 0.1011\n",
      "11/223, train_loss: 0.1413, step time: 0.1007\n",
      "12/223, train_loss: 0.1393, step time: 0.1004\n",
      "13/223, train_loss: 0.1625, step time: 0.1012\n",
      "14/223, train_loss: 0.1419, step time: 0.1017\n",
      "15/223, train_loss: 0.1269, step time: 0.1001\n",
      "16/223, train_loss: 0.1540, step time: 0.1175\n",
      "17/223, train_loss: 0.1269, step time: 0.1081\n",
      "18/223, train_loss: 0.1626, step time: 0.1002\n",
      "19/223, train_loss: 0.1496, step time: 0.1078\n",
      "20/223, train_loss: 0.1372, step time: 0.1102\n",
      "21/223, train_loss: 0.1405, step time: 0.1053\n",
      "22/223, train_loss: 0.1567, step time: 0.1138\n",
      "23/223, train_loss: 0.1199, step time: 0.1104\n",
      "24/223, train_loss: 0.1319, step time: 0.1009\n",
      "25/223, train_loss: 0.1512, step time: 0.1112\n",
      "26/223, train_loss: 0.1459, step time: 0.1086\n",
      "27/223, train_loss: 0.1414, step time: 0.1202\n",
      "28/223, train_loss: 0.1481, step time: 0.1149\n",
      "29/223, train_loss: 0.1338, step time: 0.1033\n",
      "30/223, train_loss: 0.1546, step time: 0.1032\n",
      "31/223, train_loss: 0.1317, step time: 0.1200\n",
      "32/223, train_loss: 0.1303, step time: 0.1001\n",
      "33/223, train_loss: 0.1424, step time: 0.1067\n",
      "34/223, train_loss: 0.1422, step time: 0.1126\n",
      "35/223, train_loss: 0.1444, step time: 0.1109\n",
      "36/223, train_loss: 0.1498, step time: 0.1156\n",
      "37/223, train_loss: 0.1447, step time: 0.1198\n",
      "38/223, train_loss: 0.1327, step time: 0.1132\n",
      "39/223, train_loss: 0.1464, step time: 0.1086\n",
      "40/223, train_loss: 0.1321, step time: 0.1003\n",
      "41/223, train_loss: 0.3284, step time: 0.1126\n",
      "42/223, train_loss: 0.1267, step time: 0.1242\n",
      "43/223, train_loss: 0.1262, step time: 0.1136\n",
      "44/223, train_loss: 0.1352, step time: 0.1007\n",
      "45/223, train_loss: 0.1407, step time: 0.1173\n",
      "46/223, train_loss: 0.1389, step time: 0.1022\n",
      "47/223, train_loss: 0.1291, step time: 0.1019\n",
      "48/223, train_loss: 0.1245, step time: 0.1022\n",
      "49/223, train_loss: 0.1462, step time: 0.1057\n",
      "50/223, train_loss: 0.1249, step time: 0.0999\n",
      "51/223, train_loss: 0.1512, step time: 0.1004\n",
      "52/223, train_loss: 0.1541, step time: 0.1054\n",
      "53/223, train_loss: 0.1371, step time: 0.1017\n",
      "54/223, train_loss: 0.1343, step time: 0.0992\n",
      "55/223, train_loss: 0.1379, step time: 0.1068\n",
      "56/223, train_loss: 0.1300, step time: 0.1161\n",
      "57/223, train_loss: 0.1327, step time: 0.1195\n",
      "58/223, train_loss: 0.1416, step time: 0.1404\n",
      "59/223, train_loss: 0.1465, step time: 0.1143\n",
      "60/223, train_loss: 0.1286, step time: 0.1002\n",
      "61/223, train_loss: 0.1165, step time: 0.1168\n",
      "62/223, train_loss: 0.1199, step time: 0.1102\n",
      "63/223, train_loss: 0.1316, step time: 0.1255\n",
      "64/223, train_loss: 0.1262, step time: 0.0998\n",
      "65/223, train_loss: 0.1461, step time: 0.1122\n",
      "66/223, train_loss: 0.1334, step time: 0.1007\n",
      "67/223, train_loss: 0.1397, step time: 0.0999\n",
      "68/223, train_loss: 0.1241, step time: 0.1012\n",
      "69/223, train_loss: 0.1387, step time: 0.0999\n",
      "70/223, train_loss: 0.1276, step time: 0.1000\n",
      "71/223, train_loss: 0.1420, step time: 0.1006\n",
      "72/223, train_loss: 0.1494, step time: 0.1012\n",
      "73/223, train_loss: 0.1337, step time: 0.1148\n",
      "74/223, train_loss: 0.1285, step time: 0.1007\n",
      "75/223, train_loss: 0.1540, step time: 0.1009\n",
      "76/223, train_loss: 0.1407, step time: 0.1088\n",
      "77/223, train_loss: 0.1332, step time: 0.1054\n",
      "78/223, train_loss: 0.1440, step time: 0.0998\n",
      "79/223, train_loss: 0.1369, step time: 0.1002\n",
      "80/223, train_loss: 0.1394, step time: 0.1056\n",
      "81/223, train_loss: 0.1212, step time: 0.1115\n",
      "82/223, train_loss: 0.1204, step time: 0.1034\n",
      "83/223, train_loss: 0.1482, step time: 0.1023\n",
      "84/223, train_loss: 0.1347, step time: 0.1116\n",
      "85/223, train_loss: 0.1378, step time: 0.1016\n",
      "86/223, train_loss: 0.1172, step time: 0.1008\n",
      "87/223, train_loss: 0.1334, step time: 0.1012\n",
      "88/223, train_loss: 0.1257, step time: 0.1022\n",
      "89/223, train_loss: 0.1506, step time: 0.1011\n",
      "90/223, train_loss: 0.1349, step time: 0.1018\n",
      "91/223, train_loss: 0.1359, step time: 0.1001\n",
      "92/223, train_loss: 0.1370, step time: 0.1084\n",
      "93/223, train_loss: 0.1419, step time: 0.1128\n",
      "94/223, train_loss: 0.1409, step time: 0.1251\n",
      "95/223, train_loss: 0.1272, step time: 0.1174\n",
      "96/223, train_loss: 0.1293, step time: 0.0997\n",
      "97/223, train_loss: 0.1445, step time: 0.1142\n",
      "98/223, train_loss: 0.1519, step time: 0.1009\n",
      "99/223, train_loss: 0.1286, step time: 0.1013\n",
      "100/223, train_loss: 0.1218, step time: 0.1011\n",
      "101/223, train_loss: 0.1349, step time: 0.1197\n",
      "102/223, train_loss: 0.1475, step time: 0.1143\n",
      "103/223, train_loss: 0.1288, step time: 0.1243\n",
      "104/223, train_loss: 0.1301, step time: 0.1001\n",
      "105/223, train_loss: 0.1364, step time: 0.1092\n",
      "106/223, train_loss: 0.1540, step time: 0.1084\n",
      "107/223, train_loss: 0.1550, step time: 0.1035\n",
      "108/223, train_loss: 0.1379, step time: 0.1308\n",
      "109/223, train_loss: 0.1190, step time: 0.1041\n",
      "110/223, train_loss: 0.1408, step time: 0.1205\n",
      "111/223, train_loss: 0.1422, step time: 0.1104\n",
      "112/223, train_loss: 0.1436, step time: 0.1000\n",
      "113/223, train_loss: 0.1375, step time: 0.1044\n",
      "114/223, train_loss: 0.1425, step time: 0.1152\n",
      "115/223, train_loss: 0.1355, step time: 0.1009\n",
      "116/223, train_loss: 0.1409, step time: 0.1211\n",
      "117/223, train_loss: 0.1485, step time: 0.1028\n",
      "118/223, train_loss: 0.1334, step time: 0.1323\n",
      "119/223, train_loss: 0.1522, step time: 0.1004\n",
      "120/223, train_loss: 0.1341, step time: 0.1132\n",
      "121/223, train_loss: 0.1434, step time: 0.1196\n",
      "122/223, train_loss: 0.1385, step time: 0.1285\n",
      "123/223, train_loss: 0.1537, step time: 0.1259\n",
      "124/223, train_loss: 0.1527, step time: 0.1274\n",
      "125/223, train_loss: 0.1326, step time: 0.1177\n",
      "126/223, train_loss: 0.1452, step time: 0.1276\n",
      "127/223, train_loss: 0.1342, step time: 0.1214\n",
      "128/223, train_loss: 0.1371, step time: 0.1071\n",
      "129/223, train_loss: 0.1265, step time: 0.1060\n",
      "130/223, train_loss: 0.1314, step time: 0.1264\n",
      "131/223, train_loss: 0.1303, step time: 0.1051\n",
      "132/223, train_loss: 0.1387, step time: 0.1001\n",
      "133/223, train_loss: 0.1383, step time: 0.1049\n",
      "134/223, train_loss: 0.1444, step time: 0.1100\n",
      "135/223, train_loss: 0.1351, step time: 0.1094\n",
      "136/223, train_loss: 0.1454, step time: 0.1159\n",
      "137/223, train_loss: 0.1355, step time: 0.1075\n",
      "138/223, train_loss: 0.1239, step time: 0.1239\n",
      "139/223, train_loss: 0.1341, step time: 0.1062\n",
      "140/223, train_loss: 0.1339, step time: 0.1220\n",
      "141/223, train_loss: 0.1378, step time: 0.1145\n",
      "142/223, train_loss: 0.1359, step time: 0.1137\n",
      "143/223, train_loss: 0.1460, step time: 0.1172\n",
      "144/223, train_loss: 0.1481, step time: 0.1378\n",
      "145/223, train_loss: 0.1290, step time: 0.0997\n",
      "146/223, train_loss: 0.1357, step time: 0.0986\n",
      "147/223, train_loss: 0.1484, step time: 0.0986\n",
      "148/223, train_loss: 0.1336, step time: 0.0985\n",
      "149/223, train_loss: 0.1528, step time: 0.1132\n",
      "150/223, train_loss: 0.1525, step time: 0.0999\n",
      "151/223, train_loss: 0.1434, step time: 0.1017\n",
      "152/223, train_loss: 0.1325, step time: 0.1034\n",
      "153/223, train_loss: 0.1302, step time: 0.1120\n",
      "154/223, train_loss: 0.1366, step time: 0.1006\n",
      "155/223, train_loss: 0.1463, step time: 0.1007\n",
      "156/223, train_loss: 0.1357, step time: 0.1061\n",
      "157/223, train_loss: 0.1269, step time: 0.1116\n",
      "158/223, train_loss: 0.1450, step time: 0.1181\n",
      "159/223, train_loss: 0.1392, step time: 0.0995\n",
      "160/223, train_loss: 0.1442, step time: 0.1005\n",
      "161/223, train_loss: 0.1536, step time: 0.1099\n",
      "162/223, train_loss: 0.1661, step time: 0.1006\n",
      "163/223, train_loss: 0.1275, step time: 0.0999\n",
      "164/223, train_loss: 0.1597, step time: 0.1063\n",
      "165/223, train_loss: 0.1507, step time: 0.1137\n",
      "166/223, train_loss: 0.1506, step time: 0.1031\n",
      "167/223, train_loss: 0.1320, step time: 0.1007\n",
      "168/223, train_loss: 0.1498, step time: 0.1044\n",
      "169/223, train_loss: 0.1214, step time: 0.1287\n",
      "170/223, train_loss: 0.1340, step time: 0.1156\n",
      "171/223, train_loss: 0.1447, step time: 0.1040\n",
      "172/223, train_loss: 0.1178, step time: 0.1006\n",
      "173/223, train_loss: 0.1363, step time: 0.1251\n",
      "174/223, train_loss: 0.1285, step time: 0.1395\n",
      "175/223, train_loss: 0.1343, step time: 0.1298\n",
      "176/223, train_loss: 0.1390, step time: 0.1120\n",
      "177/223, train_loss: 0.1377, step time: 0.1095\n",
      "178/223, train_loss: 0.1459, step time: 0.1098\n",
      "179/223, train_loss: 0.1311, step time: 0.1136\n",
      "180/223, train_loss: 0.1464, step time: 0.1086\n",
      "181/223, train_loss: 0.1542, step time: 0.1046\n",
      "182/223, train_loss: 0.1320, step time: 0.1167\n",
      "183/223, train_loss: 0.1367, step time: 0.1011\n",
      "184/223, train_loss: 0.1317, step time: 0.1289\n",
      "185/223, train_loss: 0.1475, step time: 0.1073\n",
      "186/223, train_loss: 0.1391, step time: 0.1326\n",
      "187/223, train_loss: 0.1328, step time: 0.1191\n",
      "188/223, train_loss: 0.1387, step time: 0.1003\n",
      "189/223, train_loss: 0.1474, step time: 0.1195\n",
      "190/223, train_loss: 0.1471, step time: 0.1117\n",
      "191/223, train_loss: 0.1299, step time: 0.1002\n",
      "192/223, train_loss: 0.1581, step time: 0.1008\n",
      "193/223, train_loss: 0.1221, step time: 0.1162\n",
      "194/223, train_loss: 0.1407, step time: 0.1122\n",
      "195/223, train_loss: 0.1396, step time: 0.1074\n",
      "196/223, train_loss: 0.1376, step time: 0.1002\n",
      "197/223, train_loss: 0.1467, step time: 0.1202\n",
      "198/223, train_loss: 0.1584, step time: 0.1045\n",
      "199/223, train_loss: 0.1465, step time: 0.1250\n",
      "200/223, train_loss: 0.1375, step time: 0.1145\n",
      "201/223, train_loss: 0.1316, step time: 0.1027\n",
      "202/223, train_loss: 0.1373, step time: 0.1032\n",
      "203/223, train_loss: 0.1349, step time: 0.1125\n",
      "204/223, train_loss: 0.1463, step time: 0.1147\n",
      "205/223, train_loss: 0.1648, step time: 0.1002\n",
      "206/223, train_loss: 0.1398, step time: 0.1182\n",
      "207/223, train_loss: 0.1370, step time: 0.1005\n",
      "208/223, train_loss: 0.1346, step time: 0.1059\n",
      "209/223, train_loss: 0.1349, step time: 0.1101\n",
      "210/223, train_loss: 0.1373, step time: 0.1002\n",
      "211/223, train_loss: 0.1455, step time: 0.1008\n",
      "212/223, train_loss: 0.1532, step time: 0.1162\n",
      "213/223, train_loss: 0.1442, step time: 0.1260\n",
      "214/223, train_loss: 0.1456, step time: 0.1219\n",
      "215/223, train_loss: 0.1339, step time: 0.1158\n",
      "216/223, train_loss: 0.1385, step time: 0.1041\n",
      "217/223, train_loss: 0.1569, step time: 0.1017\n",
      "218/223, train_loss: 0.1315, step time: 0.1016\n",
      "219/223, train_loss: 0.1353, step time: 0.1002\n",
      "220/223, train_loss: 0.1504, step time: 0.1055\n",
      "221/223, train_loss: 0.1431, step time: 0.0985\n",
      "222/223, train_loss: 0.1420, step time: 0.0996\n",
      "223/223, train_loss: 0.1462, step time: 0.0998\n",
      "epoch 37 average loss: 0.1400\n",
      "time consuming of epoch 37 is: 86.2773\n",
      "----------\n",
      "epoch 38/300\n",
      "1/223, train_loss: 0.1378, step time: 0.1096\n",
      "2/223, train_loss: 0.1309, step time: 0.1017\n",
      "3/223, train_loss: 0.1319, step time: 0.1047\n",
      "4/223, train_loss: 0.1469, step time: 0.1188\n",
      "5/223, train_loss: 0.1394, step time: 0.1143\n",
      "6/223, train_loss: 0.1349, step time: 0.1100\n",
      "7/223, train_loss: 0.1227, step time: 0.1237\n",
      "8/223, train_loss: 0.1303, step time: 0.1061\n",
      "9/223, train_loss: 0.1330, step time: 0.1005\n",
      "10/223, train_loss: 0.1444, step time: 0.1070\n",
      "11/223, train_loss: 0.1428, step time: 0.1097\n",
      "12/223, train_loss: 0.1274, step time: 0.1027\n",
      "13/223, train_loss: 0.1257, step time: 0.1014\n",
      "14/223, train_loss: 0.1492, step time: 0.1112\n",
      "15/223, train_loss: 0.1486, step time: 0.1099\n",
      "16/223, train_loss: 0.1384, step time: 0.1100\n",
      "17/223, train_loss: 0.1550, step time: 0.1126\n",
      "18/223, train_loss: 0.1505, step time: 0.1248\n",
      "19/223, train_loss: 0.1495, step time: 0.1086\n",
      "20/223, train_loss: 0.1375, step time: 0.1047\n",
      "21/223, train_loss: 0.1295, step time: 0.1147\n",
      "22/223, train_loss: 0.1315, step time: 0.1001\n",
      "23/223, train_loss: 0.1344, step time: 0.1175\n",
      "24/223, train_loss: 0.1462, step time: 0.1078\n",
      "25/223, train_loss: 0.1310, step time: 0.1048\n",
      "26/223, train_loss: 0.1328, step time: 0.1090\n",
      "27/223, train_loss: 0.1401, step time: 0.1135\n",
      "28/223, train_loss: 0.1375, step time: 0.0998\n",
      "29/223, train_loss: 0.1178, step time: 0.1323\n",
      "30/223, train_loss: 0.1241, step time: 0.1161\n",
      "31/223, train_loss: 0.1233, step time: 0.0997\n",
      "32/223, train_loss: 0.1270, step time: 0.1009\n",
      "33/223, train_loss: 0.1353, step time: 0.1163\n",
      "34/223, train_loss: 0.1529, step time: 0.0998\n",
      "35/223, train_loss: 0.1387, step time: 0.1136\n",
      "36/223, train_loss: 0.1304, step time: 0.1053\n",
      "37/223, train_loss: 0.1319, step time: 0.1244\n",
      "38/223, train_loss: 0.1436, step time: 0.1109\n",
      "39/223, train_loss: 0.1271, step time: 0.1122\n",
      "40/223, train_loss: 0.1324, step time: 0.0999\n",
      "41/223, train_loss: 0.1275, step time: 0.1004\n",
      "42/223, train_loss: 0.1298, step time: 0.1000\n",
      "43/223, train_loss: 0.1504, step time: 0.1016\n",
      "44/223, train_loss: 0.1309, step time: 0.1002\n",
      "45/223, train_loss: 0.1353, step time: 0.1190\n",
      "46/223, train_loss: 0.1444, step time: 0.1000\n",
      "47/223, train_loss: 0.1362, step time: 0.1001\n",
      "48/223, train_loss: 0.1366, step time: 0.1147\n",
      "49/223, train_loss: 0.1359, step time: 0.0997\n",
      "50/223, train_loss: 0.1427, step time: 0.0994\n",
      "51/223, train_loss: 0.1514, step time: 0.1002\n",
      "52/223, train_loss: 0.1289, step time: 0.1087\n",
      "53/223, train_loss: 0.1342, step time: 0.1028\n",
      "54/223, train_loss: 0.1247, step time: 0.0999\n",
      "55/223, train_loss: 0.1435, step time: 0.1016\n",
      "56/223, train_loss: 0.1321, step time: 0.1265\n",
      "57/223, train_loss: 0.1408, step time: 0.1108\n",
      "58/223, train_loss: 0.1446, step time: 0.1005\n",
      "59/223, train_loss: 0.1476, step time: 0.0996\n",
      "60/223, train_loss: 0.1331, step time: 0.1170\n",
      "61/223, train_loss: 0.1447, step time: 0.1185\n",
      "62/223, train_loss: 0.1342, step time: 0.0994\n",
      "63/223, train_loss: 0.1335, step time: 0.0999\n",
      "64/223, train_loss: 0.1313, step time: 0.1086\n",
      "65/223, train_loss: 0.1675, step time: 0.1028\n",
      "66/223, train_loss: 0.1479, step time: 0.1216\n",
      "67/223, train_loss: 0.1554, step time: 0.1174\n",
      "68/223, train_loss: 0.1386, step time: 0.0994\n",
      "69/223, train_loss: 0.1441, step time: 0.1090\n",
      "70/223, train_loss: 0.1333, step time: 0.1001\n",
      "71/223, train_loss: 0.1331, step time: 0.0992\n",
      "72/223, train_loss: 0.1340, step time: 0.1011\n",
      "73/223, train_loss: 0.1349, step time: 0.1474\n",
      "74/223, train_loss: 0.1444, step time: 0.1002\n",
      "75/223, train_loss: 0.1338, step time: 0.1208\n",
      "76/223, train_loss: 0.1284, step time: 0.1009\n",
      "77/223, train_loss: 0.1178, step time: 0.1159\n",
      "78/223, train_loss: 0.1221, step time: 0.1217\n",
      "79/223, train_loss: 0.1403, step time: 0.1130\n",
      "80/223, train_loss: 0.1364, step time: 0.0993\n",
      "81/223, train_loss: 0.1499, step time: 0.1098\n",
      "82/223, train_loss: 0.1417, step time: 0.1064\n",
      "83/223, train_loss: 0.1369, step time: 0.1013\n",
      "84/223, train_loss: 0.1313, step time: 0.0999\n",
      "85/223, train_loss: 0.1187, step time: 0.1269\n",
      "86/223, train_loss: 0.1306, step time: 0.1049\n",
      "87/223, train_loss: 0.1274, step time: 0.1099\n",
      "88/223, train_loss: 0.1493, step time: 0.0997\n",
      "89/223, train_loss: 0.1350, step time: 0.1041\n",
      "90/223, train_loss: 0.1387, step time: 0.1003\n",
      "91/223, train_loss: 0.1434, step time: 0.1003\n",
      "92/223, train_loss: 0.1376, step time: 0.1073\n",
      "93/223, train_loss: 0.1460, step time: 0.1006\n",
      "94/223, train_loss: 0.1324, step time: 0.1003\n",
      "95/223, train_loss: 0.1260, step time: 0.1038\n",
      "96/223, train_loss: 0.1213, step time: 0.0988\n",
      "97/223, train_loss: 0.1386, step time: 0.1100\n",
      "98/223, train_loss: 0.1467, step time: 0.0998\n",
      "99/223, train_loss: 0.1418, step time: 0.0998\n",
      "100/223, train_loss: 0.1456, step time: 0.1059\n",
      "101/223, train_loss: 0.1262, step time: 0.1218\n",
      "102/223, train_loss: 0.1295, step time: 0.1107\n",
      "103/223, train_loss: 0.1282, step time: 0.1020\n",
      "104/223, train_loss: 0.1266, step time: 0.1057\n",
      "105/223, train_loss: 0.1353, step time: 0.0993\n",
      "106/223, train_loss: 0.1458, step time: 0.0986\n",
      "107/223, train_loss: 0.1239, step time: 0.0988\n",
      "108/223, train_loss: 0.1329, step time: 0.1262\n",
      "109/223, train_loss: 0.1231, step time: 0.1112\n",
      "110/223, train_loss: 0.1334, step time: 0.0997\n",
      "111/223, train_loss: 0.1355, step time: 0.1006\n",
      "112/223, train_loss: 0.1349, step time: 0.1067\n",
      "113/223, train_loss: 0.1457, step time: 0.1113\n",
      "114/223, train_loss: 0.1321, step time: 0.1003\n",
      "115/223, train_loss: 0.1341, step time: 0.1081\n",
      "116/223, train_loss: 0.1498, step time: 0.1099\n",
      "117/223, train_loss: 0.1469, step time: 0.1161\n",
      "118/223, train_loss: 0.1284, step time: 0.1005\n",
      "119/223, train_loss: 0.1353, step time: 0.1139\n",
      "120/223, train_loss: 0.1536, step time: 0.1007\n",
      "121/223, train_loss: 0.1401, step time: 0.1000\n",
      "122/223, train_loss: 0.1362, step time: 0.0998\n",
      "123/223, train_loss: 0.1219, step time: 0.1000\n",
      "124/223, train_loss: 0.1477, step time: 0.1149\n",
      "125/223, train_loss: 0.1501, step time: 0.1191\n",
      "126/223, train_loss: 0.1320, step time: 0.1236\n",
      "127/223, train_loss: 0.1336, step time: 0.1005\n",
      "128/223, train_loss: 0.1466, step time: 0.1028\n",
      "129/223, train_loss: 0.1375, step time: 0.1275\n",
      "130/223, train_loss: 0.1301, step time: 0.1099\n",
      "131/223, train_loss: 0.1402, step time: 0.1086\n",
      "132/223, train_loss: 0.1383, step time: 0.1045\n",
      "133/223, train_loss: 0.1332, step time: 0.1053\n",
      "134/223, train_loss: 0.1192, step time: 0.1191\n",
      "135/223, train_loss: 0.1317, step time: 0.1234\n",
      "136/223, train_loss: 0.1354, step time: 0.1167\n",
      "137/223, train_loss: 0.1292, step time: 0.1069\n",
      "138/223, train_loss: 0.1306, step time: 0.1168\n",
      "139/223, train_loss: 0.1358, step time: 0.1172\n",
      "140/223, train_loss: 0.1365, step time: 0.1078\n",
      "141/223, train_loss: 0.1311, step time: 0.1030\n",
      "142/223, train_loss: 0.1419, step time: 0.1006\n",
      "143/223, train_loss: 0.1465, step time: 0.0999\n",
      "144/223, train_loss: 0.1532, step time: 0.1079\n",
      "145/223, train_loss: 0.1238, step time: 0.1026\n",
      "146/223, train_loss: 0.1426, step time: 0.1005\n",
      "147/223, train_loss: 0.1348, step time: 0.1000\n",
      "148/223, train_loss: 0.1324, step time: 0.1032\n",
      "149/223, train_loss: 0.1689, step time: 0.1169\n",
      "150/223, train_loss: 0.1498, step time: 0.1108\n",
      "151/223, train_loss: 0.1420, step time: 0.1007\n",
      "152/223, train_loss: 0.1345, step time: 0.1007\n",
      "153/223, train_loss: 0.1405, step time: 0.1149\n",
      "154/223, train_loss: 0.1282, step time: 0.1069\n",
      "155/223, train_loss: 0.1366, step time: 0.1004\n",
      "156/223, train_loss: 0.1409, step time: 0.1004\n",
      "157/223, train_loss: 0.1446, step time: 0.1144\n",
      "158/223, train_loss: 0.1269, step time: 0.1014\n",
      "159/223, train_loss: 0.1510, step time: 0.1105\n",
      "160/223, train_loss: 0.1418, step time: 0.1003\n",
      "161/223, train_loss: 0.1360, step time: 0.1071\n",
      "162/223, train_loss: 0.1365, step time: 0.1072\n",
      "163/223, train_loss: 0.1369, step time: 0.1071\n",
      "164/223, train_loss: 0.1408, step time: 0.1051\n",
      "165/223, train_loss: 0.1326, step time: 0.1003\n",
      "166/223, train_loss: 0.1447, step time: 0.1055\n",
      "167/223, train_loss: 0.1418, step time: 0.1006\n",
      "168/223, train_loss: 0.1356, step time: 0.1064\n",
      "169/223, train_loss: 0.1435, step time: 0.1161\n",
      "170/223, train_loss: 0.1351, step time: 0.1014\n",
      "171/223, train_loss: 0.1343, step time: 0.1279\n",
      "172/223, train_loss: 0.1330, step time: 0.1265\n",
      "173/223, train_loss: 0.1334, step time: 0.1170\n",
      "174/223, train_loss: 0.1298, step time: 0.1007\n",
      "175/223, train_loss: 0.1360, step time: 0.1014\n",
      "176/223, train_loss: 0.1462, step time: 0.1019\n",
      "177/223, train_loss: 0.1389, step time: 0.1002\n",
      "178/223, train_loss: 0.1288, step time: 0.1127\n",
      "179/223, train_loss: 0.1440, step time: 0.1099\n",
      "180/223, train_loss: 0.1235, step time: 0.1005\n",
      "181/223, train_loss: 0.1434, step time: 0.1140\n",
      "182/223, train_loss: 0.1380, step time: 0.1145\n",
      "183/223, train_loss: 0.1300, step time: 0.1304\n",
      "184/223, train_loss: 0.1290, step time: 0.1022\n",
      "185/223, train_loss: 0.1606, step time: 0.1095\n",
      "186/223, train_loss: 0.1581, step time: 0.1099\n",
      "187/223, train_loss: 0.1699, step time: 0.1375\n",
      "188/223, train_loss: 0.1370, step time: 0.1069\n",
      "189/223, train_loss: 0.1369, step time: 0.1063\n",
      "190/223, train_loss: 0.1445, step time: 0.1069\n",
      "191/223, train_loss: 0.1506, step time: 0.1000\n",
      "192/223, train_loss: 0.1256, step time: 0.1091\n",
      "193/223, train_loss: 0.1277, step time: 0.1178\n",
      "194/223, train_loss: 0.1471, step time: 0.1293\n",
      "195/223, train_loss: 0.1396, step time: 0.1152\n",
      "196/223, train_loss: 0.1327, step time: 0.0999\n",
      "197/223, train_loss: 0.1361, step time: 0.1135\n",
      "198/223, train_loss: 0.1213, step time: 0.1178\n",
      "199/223, train_loss: 0.1304, step time: 0.1005\n",
      "200/223, train_loss: 0.1408, step time: 0.1034\n",
      "201/223, train_loss: 0.1505, step time: 0.1216\n",
      "202/223, train_loss: 0.1397, step time: 0.1455\n",
      "203/223, train_loss: 0.3251, step time: 0.1013\n",
      "204/223, train_loss: 0.1636, step time: 0.1004\n",
      "205/223, train_loss: 0.1399, step time: 0.1021\n",
      "206/223, train_loss: 0.1280, step time: 0.1110\n",
      "207/223, train_loss: 0.1425, step time: 0.1157\n",
      "208/223, train_loss: 0.1530, step time: 0.1034\n",
      "209/223, train_loss: 0.1430, step time: 0.1169\n",
      "210/223, train_loss: 0.1353, step time: 0.1144\n",
      "211/223, train_loss: 0.1283, step time: 0.1108\n",
      "212/223, train_loss: 0.1237, step time: 0.1106\n",
      "213/223, train_loss: 0.1401, step time: 0.1051\n",
      "214/223, train_loss: 0.1447, step time: 0.1003\n",
      "215/223, train_loss: 0.1656, step time: 0.1154\n",
      "216/223, train_loss: 0.1497, step time: 0.1176\n",
      "217/223, train_loss: 0.1662, step time: 0.1032\n",
      "218/223, train_loss: 0.1463, step time: 0.1010\n",
      "219/223, train_loss: 0.1446, step time: 0.1001\n",
      "220/223, train_loss: 0.1504, step time: 0.1000\n",
      "221/223, train_loss: 0.1317, step time: 0.0986\n",
      "222/223, train_loss: 0.1423, step time: 0.0999\n",
      "223/223, train_loss: 0.1307, step time: 0.0992\n",
      "epoch 38 average loss: 0.1387\n",
      "time consuming of epoch 38 is: 88.6122\n",
      "----------\n",
      "epoch 39/300\n",
      "1/223, train_loss: 0.1410, step time: 0.1099\n",
      "2/223, train_loss: 0.1349, step time: 0.1017\n",
      "3/223, train_loss: 0.1350, step time: 0.1007\n",
      "4/223, train_loss: 0.1314, step time: 0.1160\n",
      "5/223, train_loss: 0.1459, step time: 0.1037\n",
      "6/223, train_loss: 0.1387, step time: 0.1127\n",
      "7/223, train_loss: 0.1438, step time: 0.1131\n",
      "8/223, train_loss: 0.1315, step time: 0.1161\n",
      "9/223, train_loss: 0.1536, step time: 0.1010\n",
      "10/223, train_loss: 0.1433, step time: 0.1058\n",
      "11/223, train_loss: 0.1300, step time: 0.1168\n",
      "12/223, train_loss: 0.1497, step time: 0.1067\n",
      "13/223, train_loss: 0.1379, step time: 0.1144\n",
      "14/223, train_loss: 0.1362, step time: 0.0999\n",
      "15/223, train_loss: 0.1375, step time: 0.1162\n",
      "16/223, train_loss: 0.1417, step time: 0.1007\n",
      "17/223, train_loss: 0.1355, step time: 0.1276\n",
      "18/223, train_loss: 0.1342, step time: 0.1104\n",
      "19/223, train_loss: 0.1547, step time: 0.1139\n",
      "20/223, train_loss: 0.1495, step time: 0.1107\n",
      "21/223, train_loss: 0.1278, step time: 0.1054\n",
      "22/223, train_loss: 0.1387, step time: 0.1005\n",
      "23/223, train_loss: 0.1335, step time: 0.1005\n",
      "24/223, train_loss: 0.1353, step time: 0.1011\n",
      "25/223, train_loss: 0.1500, step time: 0.1045\n",
      "26/223, train_loss: 0.1249, step time: 0.1302\n",
      "27/223, train_loss: 0.1222, step time: 0.1136\n",
      "28/223, train_loss: 0.1360, step time: 0.1361\n",
      "29/223, train_loss: 0.1225, step time: 0.1185\n",
      "30/223, train_loss: 0.1268, step time: 0.1090\n",
      "31/223, train_loss: 0.1481, step time: 0.1072\n",
      "32/223, train_loss: 0.1219, step time: 0.0996\n",
      "33/223, train_loss: 0.1313, step time: 0.1092\n",
      "34/223, train_loss: 0.1331, step time: 0.1032\n",
      "35/223, train_loss: 0.1303, step time: 0.1401\n",
      "36/223, train_loss: 0.1277, step time: 0.1106\n",
      "37/223, train_loss: 0.1251, step time: 0.1013\n",
      "38/223, train_loss: 0.1493, step time: 0.0994\n",
      "39/223, train_loss: 0.1190, step time: 0.1019\n",
      "40/223, train_loss: 0.1384, step time: 0.1049\n",
      "41/223, train_loss: 0.1470, step time: 0.0997\n",
      "42/223, train_loss: 0.1243, step time: 0.1289\n",
      "43/223, train_loss: 0.1381, step time: 0.1061\n",
      "44/223, train_loss: 0.1374, step time: 0.1156\n",
      "45/223, train_loss: 0.1404, step time: 0.1174\n",
      "46/223, train_loss: 0.1443, step time: 0.1354\n",
      "47/223, train_loss: 0.1373, step time: 0.1111\n",
      "48/223, train_loss: 0.1260, step time: 0.1070\n",
      "49/223, train_loss: 0.1367, step time: 0.1121\n",
      "50/223, train_loss: 0.1519, step time: 0.1358\n",
      "51/223, train_loss: 0.1311, step time: 0.1116\n",
      "52/223, train_loss: 0.1322, step time: 0.1165\n",
      "53/223, train_loss: 0.1351, step time: 0.1020\n",
      "54/223, train_loss: 0.1245, step time: 0.1125\n",
      "55/223, train_loss: 0.1362, step time: 0.1006\n",
      "56/223, train_loss: 0.1180, step time: 0.1159\n",
      "57/223, train_loss: 0.1270, step time: 0.1083\n",
      "58/223, train_loss: 0.1317, step time: 0.1133\n",
      "59/223, train_loss: 0.1311, step time: 0.1060\n",
      "60/223, train_loss: 0.1235, step time: 0.1135\n",
      "61/223, train_loss: 0.1252, step time: 0.1128\n",
      "62/223, train_loss: 0.1212, step time: 0.1177\n",
      "63/223, train_loss: 0.1388, step time: 0.1145\n",
      "64/223, train_loss: 0.1357, step time: 0.1077\n",
      "65/223, train_loss: 0.1407, step time: 0.1124\n",
      "66/223, train_loss: 0.1332, step time: 0.1132\n",
      "67/223, train_loss: 0.1513, step time: 0.1005\n",
      "68/223, train_loss: 0.1386, step time: 0.1005\n",
      "69/223, train_loss: 0.1263, step time: 0.1107\n",
      "70/223, train_loss: 0.1368, step time: 0.1228\n",
      "71/223, train_loss: 0.1515, step time: 0.1017\n",
      "72/223, train_loss: 0.1315, step time: 0.1008\n",
      "73/223, train_loss: 0.1340, step time: 0.1081\n",
      "74/223, train_loss: 0.1237, step time: 0.1170\n",
      "75/223, train_loss: 0.1351, step time: 0.1163\n",
      "76/223, train_loss: 0.1378, step time: 0.1166\n",
      "77/223, train_loss: 0.1429, step time: 0.1069\n",
      "78/223, train_loss: 0.1277, step time: 0.1007\n",
      "79/223, train_loss: 0.1189, step time: 0.1212\n",
      "80/223, train_loss: 0.1458, step time: 0.1099\n",
      "81/223, train_loss: 0.1280, step time: 0.1147\n",
      "82/223, train_loss: 0.1437, step time: 0.0993\n",
      "83/223, train_loss: 0.1608, step time: 0.1080\n",
      "84/223, train_loss: 0.1418, step time: 0.0995\n",
      "85/223, train_loss: 0.1368, step time: 0.0985\n",
      "86/223, train_loss: 0.1488, step time: 0.1088\n",
      "87/223, train_loss: 0.1374, step time: 0.1169\n",
      "88/223, train_loss: 0.1355, step time: 0.1051\n",
      "89/223, train_loss: 0.1332, step time: 0.1110\n",
      "90/223, train_loss: 0.1459, step time: 0.1034\n",
      "91/223, train_loss: 0.1331, step time: 0.1145\n",
      "92/223, train_loss: 0.1513, step time: 0.1141\n",
      "93/223, train_loss: 0.1400, step time: 0.1041\n",
      "94/223, train_loss: 0.1400, step time: 0.1304\n",
      "95/223, train_loss: 0.1397, step time: 0.1011\n",
      "96/223, train_loss: 0.1450, step time: 0.1028\n",
      "97/223, train_loss: 0.1480, step time: 0.1098\n",
      "98/223, train_loss: 0.1479, step time: 0.1057\n",
      "99/223, train_loss: 0.1545, step time: 0.1005\n",
      "100/223, train_loss: 0.1294, step time: 0.1006\n",
      "101/223, train_loss: 0.1231, step time: 0.1069\n",
      "102/223, train_loss: 0.1213, step time: 0.1162\n",
      "103/223, train_loss: 0.1586, step time: 0.1083\n",
      "104/223, train_loss: 0.1303, step time: 0.1008\n",
      "105/223, train_loss: 0.1656, step time: 0.1002\n",
      "106/223, train_loss: 0.1627, step time: 0.1014\n",
      "107/223, train_loss: 0.1311, step time: 0.1002\n",
      "108/223, train_loss: 0.1203, step time: 0.1032\n",
      "109/223, train_loss: 0.1360, step time: 0.1010\n",
      "110/223, train_loss: 0.1378, step time: 0.0996\n",
      "111/223, train_loss: 0.1301, step time: 0.1073\n",
      "112/223, train_loss: 0.1443, step time: 0.1009\n",
      "113/223, train_loss: 0.1417, step time: 0.1020\n",
      "114/223, train_loss: 0.1322, step time: 0.1083\n",
      "115/223, train_loss: 0.1462, step time: 0.1065\n",
      "116/223, train_loss: 0.1392, step time: 0.1027\n",
      "117/223, train_loss: 0.1368, step time: 0.1010\n",
      "118/223, train_loss: 0.1324, step time: 0.1003\n",
      "119/223, train_loss: 0.1538, step time: 0.1067\n",
      "120/223, train_loss: 0.1564, step time: 0.1036\n",
      "121/223, train_loss: 0.1456, step time: 0.1079\n",
      "122/223, train_loss: 0.1400, step time: 0.1321\n",
      "123/223, train_loss: 0.1412, step time: 0.1166\n",
      "124/223, train_loss: 0.1487, step time: 0.1093\n",
      "125/223, train_loss: 0.1332, step time: 0.1143\n",
      "126/223, train_loss: 0.1576, step time: 0.1001\n",
      "127/223, train_loss: 0.1427, step time: 0.1076\n",
      "128/223, train_loss: 0.1316, step time: 0.1134\n",
      "129/223, train_loss: 0.1439, step time: 0.1061\n",
      "130/223, train_loss: 0.1332, step time: 0.1090\n",
      "131/223, train_loss: 0.3275, step time: 0.0998\n",
      "132/223, train_loss: 0.1647, step time: 0.1003\n",
      "133/223, train_loss: 0.1327, step time: 0.1022\n",
      "134/223, train_loss: 0.1283, step time: 0.1011\n",
      "135/223, train_loss: 0.1401, step time: 0.1060\n",
      "136/223, train_loss: 0.1420, step time: 0.1132\n",
      "137/223, train_loss: 0.1547, step time: 0.1220\n",
      "138/223, train_loss: 0.1390, step time: 0.0997\n",
      "139/223, train_loss: 0.1603, step time: 0.1006\n",
      "140/223, train_loss: 0.1237, step time: 0.1172\n",
      "141/223, train_loss: 0.1399, step time: 0.0994\n",
      "142/223, train_loss: 0.1375, step time: 0.1180\n",
      "143/223, train_loss: 0.1240, step time: 0.1001\n",
      "144/223, train_loss: 0.1411, step time: 0.1009\n",
      "145/223, train_loss: 0.1242, step time: 0.1202\n",
      "146/223, train_loss: 0.1372, step time: 0.1136\n",
      "147/223, train_loss: 0.1458, step time: 0.1055\n",
      "148/223, train_loss: 0.1584, step time: 0.1180\n",
      "149/223, train_loss: 0.1302, step time: 0.1219\n",
      "150/223, train_loss: 0.1457, step time: 0.1004\n",
      "151/223, train_loss: 0.1348, step time: 0.1000\n",
      "152/223, train_loss: 0.1229, step time: 0.1010\n",
      "153/223, train_loss: 0.1546, step time: 0.1078\n",
      "154/223, train_loss: 0.1416, step time: 0.1002\n",
      "155/223, train_loss: 0.1421, step time: 0.1081\n",
      "156/223, train_loss: 0.1318, step time: 0.0998\n",
      "157/223, train_loss: 0.1409, step time: 0.1241\n",
      "158/223, train_loss: 0.1547, step time: 0.0998\n",
      "159/223, train_loss: 0.1583, step time: 0.1024\n",
      "160/223, train_loss: 0.1308, step time: 0.1003\n",
      "161/223, train_loss: 0.1546, step time: 0.0985\n",
      "162/223, train_loss: 0.1342, step time: 0.1151\n",
      "163/223, train_loss: 0.1437, step time: 0.1076\n",
      "164/223, train_loss: 0.1283, step time: 0.1101\n",
      "165/223, train_loss: 0.1313, step time: 0.1186\n",
      "166/223, train_loss: 0.1441, step time: 0.0992\n",
      "167/223, train_loss: 0.1343, step time: 0.1010\n",
      "168/223, train_loss: 0.1421, step time: 0.1049\n",
      "169/223, train_loss: 0.1259, step time: 0.1493\n",
      "170/223, train_loss: 0.1451, step time: 0.1190\n",
      "171/223, train_loss: 0.1440, step time: 0.1110\n",
      "172/223, train_loss: 0.1348, step time: 0.1093\n",
      "173/223, train_loss: 0.1499, step time: 0.1003\n",
      "174/223, train_loss: 0.1349, step time: 0.1122\n",
      "175/223, train_loss: 0.1373, step time: 0.0996\n",
      "176/223, train_loss: 0.1410, step time: 0.0993\n",
      "177/223, train_loss: 0.1393, step time: 0.1013\n",
      "178/223, train_loss: 0.1356, step time: 0.1135\n",
      "179/223, train_loss: 0.1224, step time: 0.1023\n",
      "180/223, train_loss: 0.1237, step time: 0.0999\n",
      "181/223, train_loss: 0.1274, step time: 0.1012\n",
      "182/223, train_loss: 0.1502, step time: 0.1164\n",
      "183/223, train_loss: 0.1427, step time: 0.1028\n",
      "184/223, train_loss: 0.1288, step time: 0.0996\n",
      "185/223, train_loss: 0.1500, step time: 0.1240\n",
      "186/223, train_loss: 0.1277, step time: 0.1245\n",
      "187/223, train_loss: 0.1451, step time: 0.1033\n",
      "188/223, train_loss: 0.1167, step time: 0.1004\n",
      "189/223, train_loss: 0.1261, step time: 0.1119\n",
      "190/223, train_loss: 0.1243, step time: 0.1275\n",
      "191/223, train_loss: 0.1276, step time: 0.1021\n",
      "192/223, train_loss: 0.1371, step time: 0.1011\n",
      "193/223, train_loss: 0.1451, step time: 0.1002\n",
      "194/223, train_loss: 0.1473, step time: 0.1037\n",
      "195/223, train_loss: 0.1481, step time: 0.1102\n",
      "196/223, train_loss: 0.1496, step time: 0.1006\n",
      "197/223, train_loss: 0.1432, step time: 0.0998\n",
      "198/223, train_loss: 0.1477, step time: 0.0995\n",
      "199/223, train_loss: 0.1283, step time: 0.1002\n",
      "200/223, train_loss: 0.1289, step time: 0.1013\n",
      "201/223, train_loss: 0.1390, step time: 0.0992\n",
      "202/223, train_loss: 0.1429, step time: 0.1003\n",
      "203/223, train_loss: 0.1256, step time: 0.1003\n",
      "204/223, train_loss: 0.1726, step time: 0.1001\n",
      "205/223, train_loss: 0.1554, step time: 0.0993\n",
      "206/223, train_loss: 0.1367, step time: 0.1182\n",
      "207/223, train_loss: 0.1343, step time: 0.1140\n",
      "208/223, train_loss: 0.1391, step time: 0.1115\n",
      "209/223, train_loss: 0.1413, step time: 0.1047\n",
      "210/223, train_loss: 0.1507, step time: 0.1223\n",
      "211/223, train_loss: 0.1235, step time: 0.1005\n",
      "212/223, train_loss: 0.1472, step time: 0.1057\n",
      "213/223, train_loss: 0.1319, step time: 0.1085\n",
      "214/223, train_loss: 0.1251, step time: 0.1151\n",
      "215/223, train_loss: 0.1427, step time: 0.1185\n",
      "216/223, train_loss: 0.1292, step time: 0.1003\n",
      "217/223, train_loss: 0.1340, step time: 0.1013\n",
      "218/223, train_loss: 0.1446, step time: 0.0999\n",
      "219/223, train_loss: 0.1396, step time: 0.1003\n",
      "220/223, train_loss: 0.1258, step time: 0.1012\n",
      "221/223, train_loss: 0.1402, step time: 0.0991\n",
      "222/223, train_loss: 0.1342, step time: 0.0995\n",
      "223/223, train_loss: 0.1232, step time: 0.0997\n",
      "epoch 39 average loss: 0.1388\n",
      "time consuming of epoch 39 is: 89.6867\n",
      "----------\n",
      "epoch 40/300\n",
      "1/223, train_loss: 0.1299, step time: 0.1063\n",
      "2/223, train_loss: 0.1388, step time: 0.1019\n",
      "3/223, train_loss: 0.1260, step time: 0.1100\n",
      "4/223, train_loss: 0.1382, step time: 0.1011\n",
      "5/223, train_loss: 0.1503, step time: 0.1095\n",
      "6/223, train_loss: 0.1194, step time: 0.1262\n",
      "7/223, train_loss: 0.1393, step time: 0.1050\n",
      "8/223, train_loss: 0.1264, step time: 0.1137\n",
      "9/223, train_loss: 0.1250, step time: 0.1011\n",
      "10/223, train_loss: 0.1433, step time: 0.1000\n",
      "11/223, train_loss: 0.1286, step time: 0.1000\n",
      "12/223, train_loss: 0.1443, step time: 0.1005\n",
      "13/223, train_loss: 0.1301, step time: 0.1041\n",
      "14/223, train_loss: 0.1281, step time: 0.1092\n",
      "15/223, train_loss: 0.1238, step time: 0.1085\n",
      "16/223, train_loss: 0.1352, step time: 0.1106\n",
      "17/223, train_loss: 0.1418, step time: 0.1029\n",
      "18/223, train_loss: 0.1299, step time: 0.1183\n",
      "19/223, train_loss: 0.1362, step time: 0.1032\n",
      "20/223, train_loss: 0.1602, step time: 0.1031\n",
      "21/223, train_loss: 0.1446, step time: 0.1068\n",
      "22/223, train_loss: 0.1379, step time: 0.1215\n",
      "23/223, train_loss: 0.1456, step time: 0.1232\n",
      "24/223, train_loss: 0.1426, step time: 0.1146\n",
      "25/223, train_loss: 0.1380, step time: 0.1092\n",
      "26/223, train_loss: 0.1549, step time: 0.1078\n",
      "27/223, train_loss: 0.1387, step time: 0.0999\n",
      "28/223, train_loss: 0.1498, step time: 0.1090\n",
      "29/223, train_loss: 0.1398, step time: 0.1106\n",
      "30/223, train_loss: 0.1480, step time: 0.1075\n",
      "31/223, train_loss: 0.1478, step time: 0.1013\n",
      "32/223, train_loss: 0.1311, step time: 0.1243\n",
      "33/223, train_loss: 0.1378, step time: 0.0999\n",
      "34/223, train_loss: 0.1560, step time: 0.1028\n",
      "35/223, train_loss: 0.1371, step time: 0.0996\n",
      "36/223, train_loss: 0.1396, step time: 0.1008\n",
      "37/223, train_loss: 0.1467, step time: 0.1154\n",
      "38/223, train_loss: 0.1461, step time: 0.0996\n",
      "39/223, train_loss: 0.1345, step time: 0.1001\n",
      "40/223, train_loss: 0.1345, step time: 0.1130\n",
      "41/223, train_loss: 0.1372, step time: 0.1229\n",
      "42/223, train_loss: 0.1296, step time: 0.1009\n",
      "43/223, train_loss: 0.1210, step time: 0.1053\n",
      "44/223, train_loss: 0.1446, step time: 0.1243\n",
      "45/223, train_loss: 0.1264, step time: 0.1001\n",
      "46/223, train_loss: 0.1199, step time: 0.1080\n",
      "47/223, train_loss: 0.1291, step time: 0.1120\n",
      "48/223, train_loss: 0.1371, step time: 0.1137\n",
      "49/223, train_loss: 0.1405, step time: 0.0995\n",
      "50/223, train_loss: 0.1249, step time: 0.1003\n",
      "51/223, train_loss: 0.1356, step time: 0.1134\n",
      "52/223, train_loss: 0.1452, step time: 0.1128\n",
      "53/223, train_loss: 0.1311, step time: 0.1070\n",
      "54/223, train_loss: 0.1326, step time: 0.1026\n",
      "55/223, train_loss: 0.1459, step time: 0.1073\n",
      "56/223, train_loss: 0.1428, step time: 0.1005\n",
      "57/223, train_loss: 0.1368, step time: 0.1045\n",
      "58/223, train_loss: 0.1158, step time: 0.0997\n",
      "59/223, train_loss: 0.1376, step time: 0.1002\n",
      "60/223, train_loss: 0.1302, step time: 0.1070\n",
      "61/223, train_loss: 0.1489, step time: 0.0999\n",
      "62/223, train_loss: 0.1271, step time: 0.1006\n",
      "63/223, train_loss: 0.1309, step time: 0.1000\n",
      "64/223, train_loss: 0.1344, step time: 0.1000\n",
      "65/223, train_loss: 0.1225, step time: 0.1022\n",
      "66/223, train_loss: 0.1380, step time: 0.1003\n",
      "67/223, train_loss: 0.1431, step time: 0.1198\n",
      "68/223, train_loss: 0.1275, step time: 0.1134\n",
      "69/223, train_loss: 0.1431, step time: 0.1290\n",
      "70/223, train_loss: 0.1448, step time: 0.1179\n",
      "71/223, train_loss: 0.1403, step time: 0.1091\n",
      "72/223, train_loss: 0.1534, step time: 0.1162\n",
      "73/223, train_loss: 0.1415, step time: 0.1191\n",
      "74/223, train_loss: 0.1447, step time: 0.1183\n",
      "75/223, train_loss: 0.1319, step time: 0.1062\n",
      "76/223, train_loss: 0.1312, step time: 0.0992\n",
      "77/223, train_loss: 0.1265, step time: 0.0988\n",
      "78/223, train_loss: 0.1484, step time: 0.0986\n",
      "79/223, train_loss: 0.1305, step time: 0.1110\n",
      "80/223, train_loss: 0.1467, step time: 0.1129\n",
      "81/223, train_loss: 0.1338, step time: 0.1008\n",
      "82/223, train_loss: 0.1388, step time: 0.0998\n",
      "83/223, train_loss: 0.1298, step time: 0.1130\n",
      "84/223, train_loss: 0.1374, step time: 0.1009\n",
      "85/223, train_loss: 0.1312, step time: 0.1088\n",
      "86/223, train_loss: 0.1400, step time: 0.1018\n",
      "87/223, train_loss: 0.1294, step time: 0.1351\n",
      "88/223, train_loss: 0.1302, step time: 0.1221\n",
      "89/223, train_loss: 0.1420, step time: 0.1001\n",
      "90/223, train_loss: 0.1277, step time: 0.1000\n",
      "91/223, train_loss: 0.1282, step time: 0.1122\n",
      "92/223, train_loss: 0.1326, step time: 0.1419\n",
      "93/223, train_loss: 0.1259, step time: 0.1232\n",
      "94/223, train_loss: 0.1522, step time: 0.1004\n",
      "95/223, train_loss: 0.1359, step time: 0.1153\n",
      "96/223, train_loss: 0.1345, step time: 0.1002\n",
      "97/223, train_loss: 0.1297, step time: 0.1088\n",
      "98/223, train_loss: 0.1392, step time: 0.1107\n",
      "99/223, train_loss: 0.1540, step time: 0.1001\n",
      "100/223, train_loss: 0.1647, step time: 0.1099\n",
      "101/223, train_loss: 0.1319, step time: 0.1139\n",
      "102/223, train_loss: 0.1255, step time: 0.1015\n",
      "103/223, train_loss: 0.1379, step time: 0.1082\n",
      "104/223, train_loss: 0.1347, step time: 0.0998\n",
      "105/223, train_loss: 0.1412, step time: 0.0999\n",
      "106/223, train_loss: 0.3339, step time: 0.1085\n",
      "107/223, train_loss: 0.1444, step time: 0.1034\n",
      "108/223, train_loss: 0.1295, step time: 0.1114\n",
      "109/223, train_loss: 0.1249, step time: 0.1006\n",
      "110/223, train_loss: 0.1281, step time: 0.1012\n",
      "111/223, train_loss: 0.1413, step time: 0.1174\n",
      "112/223, train_loss: 0.1233, step time: 0.1202\n",
      "113/223, train_loss: 0.1354, step time: 0.1108\n",
      "114/223, train_loss: 0.1356, step time: 0.1011\n",
      "115/223, train_loss: 0.1408, step time: 0.1195\n",
      "116/223, train_loss: 0.1248, step time: 0.1006\n",
      "117/223, train_loss: 0.1434, step time: 0.1082\n",
      "118/223, train_loss: 0.1243, step time: 0.1000\n",
      "119/223, train_loss: 0.1436, step time: 0.1019\n",
      "120/223, train_loss: 0.1361, step time: 0.1114\n",
      "121/223, train_loss: 0.1537, step time: 0.1065\n",
      "122/223, train_loss: 0.1289, step time: 0.1011\n",
      "123/223, train_loss: 0.1369, step time: 0.1035\n",
      "124/223, train_loss: 0.1387, step time: 0.1162\n",
      "125/223, train_loss: 0.1324, step time: 0.1016\n",
      "126/223, train_loss: 0.1474, step time: 0.1010\n",
      "127/223, train_loss: 0.1360, step time: 0.1064\n",
      "128/223, train_loss: 0.1527, step time: 0.1006\n",
      "129/223, train_loss: 0.1351, step time: 0.1010\n",
      "130/223, train_loss: 0.1263, step time: 0.1374\n",
      "131/223, train_loss: 0.1257, step time: 0.1118\n",
      "132/223, train_loss: 0.1397, step time: 0.1000\n",
      "133/223, train_loss: 0.1303, step time: 0.1007\n",
      "134/223, train_loss: 0.1351, step time: 0.1236\n",
      "135/223, train_loss: 0.1341, step time: 0.1653\n",
      "136/223, train_loss: 0.1300, step time: 0.1104\n",
      "137/223, train_loss: 0.1306, step time: 0.1006\n",
      "138/223, train_loss: 0.1232, step time: 0.1005\n",
      "139/223, train_loss: 0.1391, step time: 0.1274\n",
      "140/223, train_loss: 0.1394, step time: 0.1093\n",
      "141/223, train_loss: 0.1265, step time: 0.1209\n",
      "142/223, train_loss: 0.1423, step time: 0.1068\n",
      "143/223, train_loss: 0.1391, step time: 0.0997\n",
      "144/223, train_loss: 0.1381, step time: 0.0999\n",
      "145/223, train_loss: 0.1496, step time: 0.1000\n",
      "146/223, train_loss: 0.1235, step time: 0.1054\n",
      "147/223, train_loss: 0.1333, step time: 0.0994\n",
      "148/223, train_loss: 0.1365, step time: 0.1001\n",
      "149/223, train_loss: 0.1496, step time: 0.1593\n",
      "150/223, train_loss: 0.1366, step time: 0.1139\n",
      "151/223, train_loss: 0.1315, step time: 0.1009\n",
      "152/223, train_loss: 0.1305, step time: 0.1010\n",
      "153/223, train_loss: 0.1292, step time: 0.0999\n",
      "154/223, train_loss: 0.1527, step time: 0.1028\n",
      "155/223, train_loss: 0.1255, step time: 0.0993\n",
      "156/223, train_loss: 0.1333, step time: 0.0985\n",
      "157/223, train_loss: 0.1270, step time: 0.1004\n",
      "158/223, train_loss: 0.1405, step time: 0.1059\n",
      "159/223, train_loss: 0.1334, step time: 0.1013\n",
      "160/223, train_loss: 0.1585, step time: 0.1186\n",
      "161/223, train_loss: 0.1428, step time: 0.1100\n",
      "162/223, train_loss: 0.1247, step time: 0.1008\n",
      "163/223, train_loss: 0.1237, step time: 0.0998\n",
      "164/223, train_loss: 0.1264, step time: 0.1149\n",
      "165/223, train_loss: 0.1391, step time: 0.1198\n",
      "166/223, train_loss: 0.1364, step time: 0.0995\n",
      "167/223, train_loss: 0.1495, step time: 0.0993\n",
      "168/223, train_loss: 0.1328, step time: 0.0998\n",
      "169/223, train_loss: 0.1459, step time: 0.1006\n",
      "170/223, train_loss: 0.1505, step time: 0.1011\n",
      "171/223, train_loss: 0.1353, step time: 0.0995\n",
      "172/223, train_loss: 0.1306, step time: 0.1156\n",
      "173/223, train_loss: 0.1174, step time: 0.1132\n",
      "174/223, train_loss: 0.1425, step time: 0.1063\n",
      "175/223, train_loss: 0.1380, step time: 0.1003\n",
      "176/223, train_loss: 0.1251, step time: 0.1014\n",
      "177/223, train_loss: 0.1397, step time: 0.1036\n",
      "178/223, train_loss: 0.1174, step time: 0.1160\n",
      "179/223, train_loss: 0.1434, step time: 0.1182\n",
      "180/223, train_loss: 0.1512, step time: 0.1519\n",
      "181/223, train_loss: 0.1525, step time: 0.1026\n",
      "182/223, train_loss: 0.1438, step time: 0.0999\n",
      "183/223, train_loss: 0.1447, step time: 0.1336\n",
      "184/223, train_loss: 0.1213, step time: 0.1058\n",
      "185/223, train_loss: 0.1416, step time: 0.1131\n",
      "186/223, train_loss: 0.1449, step time: 0.1094\n",
      "187/223, train_loss: 0.1393, step time: 0.1336\n",
      "188/223, train_loss: 0.1269, step time: 0.1147\n",
      "189/223, train_loss: 0.1461, step time: 0.1111\n",
      "190/223, train_loss: 0.1206, step time: 0.1188\n",
      "191/223, train_loss: 0.1201, step time: 0.1245\n",
      "192/223, train_loss: 0.1291, step time: 0.1288\n",
      "193/223, train_loss: 0.1300, step time: 0.1290\n",
      "194/223, train_loss: 0.1285, step time: 0.1170\n",
      "195/223, train_loss: 0.1431, step time: 0.1010\n",
      "196/223, train_loss: 0.1477, step time: 0.1003\n",
      "197/223, train_loss: 0.1329, step time: 0.1094\n",
      "198/223, train_loss: 0.1298, step time: 0.1193\n",
      "199/223, train_loss: 0.1412, step time: 0.1358\n",
      "200/223, train_loss: 0.1190, step time: 0.1156\n",
      "201/223, train_loss: 0.1305, step time: 0.1154\n",
      "202/223, train_loss: 0.1394, step time: 0.1128\n",
      "203/223, train_loss: 0.1329, step time: 0.1613\n",
      "204/223, train_loss: 0.1438, step time: 0.1114\n",
      "205/223, train_loss: 0.1469, step time: 0.1005\n",
      "206/223, train_loss: 0.1261, step time: 0.1273\n",
      "207/223, train_loss: 0.1354, step time: 0.0996\n",
      "208/223, train_loss: 0.1290, step time: 0.1003\n",
      "209/223, train_loss: 0.1267, step time: 0.1025\n",
      "210/223, train_loss: 0.1323, step time: 0.1090\n",
      "211/223, train_loss: 0.1391, step time: 0.0998\n",
      "212/223, train_loss: 0.1327, step time: 0.1003\n",
      "213/223, train_loss: 0.1327, step time: 0.1281\n",
      "214/223, train_loss: 0.1593, step time: 0.1098\n",
      "215/223, train_loss: 0.1517, step time: 0.0995\n",
      "216/223, train_loss: 0.1411, step time: 0.0999\n",
      "217/223, train_loss: 0.1254, step time: 0.0999\n",
      "218/223, train_loss: 0.1303, step time: 0.0993\n",
      "219/223, train_loss: 0.1404, step time: 0.1015\n",
      "220/223, train_loss: 0.1342, step time: 0.1049\n",
      "221/223, train_loss: 0.1254, step time: 0.1112\n",
      "222/223, train_loss: 0.1194, step time: 0.0983\n",
      "223/223, train_loss: 0.1368, step time: 0.0990\n",
      "epoch 40 average loss: 0.1370\n",
      "saved new best metric model\n",
      "current epoch: 40 current mean dice: 0.8270 tc: 0.9073 wt: 0.8436 et: 0.7301\n",
      "best mean dice: 0.8270 at epoch: 40\n",
      "time consuming of epoch 40 is: 91.5870\n",
      "----------\n",
      "epoch 41/300\n",
      "1/223, train_loss: 0.1608, step time: 0.1138\n",
      "2/223, train_loss: 0.1469, step time: 0.1006\n",
      "3/223, train_loss: 0.1519, step time: 0.1003\n",
      "4/223, train_loss: 0.1514, step time: 0.1033\n",
      "5/223, train_loss: 0.1296, step time: 0.1189\n",
      "6/223, train_loss: 0.1326, step time: 0.1135\n",
      "7/223, train_loss: 0.1305, step time: 0.1164\n",
      "8/223, train_loss: 0.1305, step time: 0.1004\n",
      "9/223, train_loss: 0.1432, step time: 0.1103\n",
      "10/223, train_loss: 0.1341, step time: 0.1279\n",
      "11/223, train_loss: 0.1528, step time: 0.1145\n",
      "12/223, train_loss: 0.1276, step time: 0.1038\n",
      "13/223, train_loss: 0.1305, step time: 0.1033\n",
      "14/223, train_loss: 0.1386, step time: 0.1000\n",
      "15/223, train_loss: 0.1336, step time: 0.1113\n",
      "16/223, train_loss: 0.1299, step time: 0.1010\n",
      "17/223, train_loss: 0.1268, step time: 0.1141\n",
      "18/223, train_loss: 0.1255, step time: 0.1107\n",
      "19/223, train_loss: 0.1452, step time: 0.1080\n",
      "20/223, train_loss: 0.1426, step time: 0.1019\n",
      "21/223, train_loss: 0.1322, step time: 0.1092\n",
      "22/223, train_loss: 0.1316, step time: 0.1198\n",
      "23/223, train_loss: 0.1226, step time: 0.1047\n",
      "24/223, train_loss: 0.1411, step time: 0.1080\n",
      "25/223, train_loss: 0.1486, step time: 0.1061\n",
      "26/223, train_loss: 0.1350, step time: 0.1076\n",
      "27/223, train_loss: 0.1334, step time: 0.1144\n",
      "28/223, train_loss: 0.1431, step time: 0.1169\n",
      "29/223, train_loss: 0.1399, step time: 0.1105\n",
      "30/223, train_loss: 0.1312, step time: 0.1261\n",
      "31/223, train_loss: 0.1329, step time: 0.1045\n",
      "32/223, train_loss: 0.1390, step time: 0.1165\n",
      "33/223, train_loss: 0.1326, step time: 0.1078\n",
      "34/223, train_loss: 0.1435, step time: 0.1199\n",
      "35/223, train_loss: 0.1475, step time: 0.1140\n",
      "36/223, train_loss: 0.1477, step time: 0.1064\n",
      "37/223, train_loss: 0.1316, step time: 0.1181\n",
      "38/223, train_loss: 0.1299, step time: 0.1340\n",
      "39/223, train_loss: 0.1425, step time: 0.1103\n",
      "40/223, train_loss: 0.1368, step time: 0.1158\n",
      "41/223, train_loss: 0.1269, step time: 0.1055\n",
      "42/223, train_loss: 0.1255, step time: 0.1000\n",
      "43/223, train_loss: 0.1368, step time: 0.1170\n",
      "44/223, train_loss: 0.1402, step time: 0.1163\n",
      "45/223, train_loss: 0.1169, step time: 0.1011\n",
      "46/223, train_loss: 0.1603, step time: 0.1037\n",
      "47/223, train_loss: 0.1298, step time: 0.1034\n",
      "48/223, train_loss: 0.1344, step time: 0.0998\n",
      "49/223, train_loss: 0.1407, step time: 0.1001\n",
      "50/223, train_loss: 0.1418, step time: 0.0998\n",
      "51/223, train_loss: 0.1266, step time: 0.0998\n",
      "52/223, train_loss: 0.1419, step time: 0.0998\n",
      "53/223, train_loss: 0.1380, step time: 0.0989\n",
      "54/223, train_loss: 0.1312, step time: 0.0989\n",
      "55/223, train_loss: 0.1391, step time: 0.0999\n",
      "56/223, train_loss: 0.1229, step time: 0.1066\n",
      "57/223, train_loss: 0.1299, step time: 0.1075\n",
      "58/223, train_loss: 0.1205, step time: 0.1119\n",
      "59/223, train_loss: 0.1441, step time: 0.1472\n",
      "60/223, train_loss: 0.1308, step time: 0.1044\n",
      "61/223, train_loss: 0.1545, step time: 0.1211\n",
      "62/223, train_loss: 0.1352, step time: 0.1098\n",
      "63/223, train_loss: 0.1420, step time: 0.1202\n",
      "64/223, train_loss: 0.3246, step time: 0.1117\n",
      "65/223, train_loss: 0.1423, step time: 0.1304\n",
      "66/223, train_loss: 0.1406, step time: 0.1049\n",
      "67/223, train_loss: 0.1554, step time: 0.1466\n",
      "68/223, train_loss: 0.1199, step time: 0.1039\n",
      "69/223, train_loss: 0.1471, step time: 0.1005\n",
      "70/223, train_loss: 0.1495, step time: 0.1005\n",
      "71/223, train_loss: 0.1265, step time: 0.0999\n",
      "72/223, train_loss: 0.1289, step time: 0.1012\n",
      "73/223, train_loss: 0.1442, step time: 0.1070\n",
      "74/223, train_loss: 0.1289, step time: 0.1130\n",
      "75/223, train_loss: 0.1323, step time: 0.1005\n",
      "76/223, train_loss: 0.1405, step time: 0.1183\n",
      "77/223, train_loss: 0.1364, step time: 0.1177\n",
      "78/223, train_loss: 0.1251, step time: 0.0997\n",
      "79/223, train_loss: 0.1341, step time: 0.1120\n",
      "80/223, train_loss: 0.1293, step time: 0.1013\n",
      "81/223, train_loss: 0.1247, step time: 0.1001\n",
      "82/223, train_loss: 0.1380, step time: 0.1109\n",
      "83/223, train_loss: 0.1291, step time: 0.1217\n",
      "84/223, train_loss: 0.1200, step time: 0.1094\n",
      "85/223, train_loss: 0.1400, step time: 0.1138\n",
      "86/223, train_loss: 0.1258, step time: 0.1006\n",
      "87/223, train_loss: 0.1257, step time: 0.1010\n",
      "88/223, train_loss: 0.1353, step time: 0.1027\n",
      "89/223, train_loss: 0.1240, step time: 0.1090\n",
      "90/223, train_loss: 0.1477, step time: 0.1001\n",
      "91/223, train_loss: 0.1330, step time: 0.1419\n",
      "92/223, train_loss: 0.1244, step time: 0.1228\n",
      "93/223, train_loss: 0.1257, step time: 0.1081\n",
      "94/223, train_loss: 0.1422, step time: 0.1016\n",
      "95/223, train_loss: 0.1474, step time: 0.1038\n",
      "96/223, train_loss: 0.1288, step time: 0.1004\n",
      "97/223, train_loss: 0.1232, step time: 0.1122\n",
      "98/223, train_loss: 0.1581, step time: 0.1179\n",
      "99/223, train_loss: 0.1550, step time: 0.0999\n",
      "100/223, train_loss: 0.1395, step time: 0.1055\n",
      "101/223, train_loss: 0.1439, step time: 0.1012\n",
      "102/223, train_loss: 0.1401, step time: 0.0998\n",
      "103/223, train_loss: 0.1337, step time: 0.1006\n",
      "104/223, train_loss: 0.1254, step time: 0.1010\n",
      "105/223, train_loss: 0.1381, step time: 0.1000\n",
      "106/223, train_loss: 0.1206, step time: 0.1004\n",
      "107/223, train_loss: 0.1351, step time: 0.1000\n",
      "108/223, train_loss: 0.1287, step time: 0.1056\n",
      "109/223, train_loss: 0.1295, step time: 0.1004\n",
      "110/223, train_loss: 0.1474, step time: 0.1000\n",
      "111/223, train_loss: 0.1574, step time: 0.1081\n",
      "112/223, train_loss: 0.1429, step time: 0.1005\n",
      "113/223, train_loss: 0.1389, step time: 0.1005\n",
      "114/223, train_loss: 0.1319, step time: 0.1002\n",
      "115/223, train_loss: 0.1358, step time: 0.1006\n",
      "116/223, train_loss: 0.1320, step time: 0.1002\n",
      "117/223, train_loss: 0.1393, step time: 0.1043\n",
      "118/223, train_loss: 0.1426, step time: 0.1106\n",
      "119/223, train_loss: 0.1399, step time: 0.1041\n",
      "120/223, train_loss: 0.1299, step time: 0.0994\n",
      "121/223, train_loss: 0.1193, step time: 0.0996\n",
      "122/223, train_loss: 0.1416, step time: 0.1069\n",
      "123/223, train_loss: 0.1271, step time: 0.1053\n",
      "124/223, train_loss: 0.1211, step time: 0.1333\n",
      "125/223, train_loss: 0.1238, step time: 0.1182\n",
      "126/223, train_loss: 0.1354, step time: 0.1138\n",
      "127/223, train_loss: 0.1383, step time: 0.1228\n",
      "128/223, train_loss: 0.1420, step time: 0.1187\n",
      "129/223, train_loss: 0.1269, step time: 0.1093\n",
      "130/223, train_loss: 0.1313, step time: 0.1198\n",
      "131/223, train_loss: 0.1456, step time: 0.1171\n",
      "132/223, train_loss: 0.1181, step time: 0.1072\n",
      "133/223, train_loss: 0.1403, step time: 0.1103\n",
      "134/223, train_loss: 0.1395, step time: 0.1078\n",
      "135/223, train_loss: 0.1322, step time: 0.1225\n",
      "136/223, train_loss: 0.1250, step time: 0.1031\n",
      "137/223, train_loss: 0.1419, step time: 0.1096\n",
      "138/223, train_loss: 0.1318, step time: 0.0996\n",
      "139/223, train_loss: 0.1454, step time: 0.1071\n",
      "140/223, train_loss: 0.1500, step time: 0.1094\n",
      "141/223, train_loss: 0.1219, step time: 0.1127\n",
      "142/223, train_loss: 0.1238, step time: 0.1073\n",
      "143/223, train_loss: 0.1488, step time: 0.1106\n",
      "144/223, train_loss: 0.1416, step time: 0.1005\n",
      "145/223, train_loss: 0.1319, step time: 0.1005\n",
      "146/223, train_loss: 0.1377, step time: 0.0992\n",
      "147/223, train_loss: 0.1228, step time: 0.1008\n",
      "148/223, train_loss: 0.1408, step time: 0.0998\n",
      "149/223, train_loss: 0.1326, step time: 0.1006\n",
      "150/223, train_loss: 0.1363, step time: 0.1012\n",
      "151/223, train_loss: 0.1263, step time: 0.1000\n",
      "152/223, train_loss: 0.1356, step time: 0.0999\n",
      "153/223, train_loss: 0.1309, step time: 0.1005\n",
      "154/223, train_loss: 0.1272, step time: 0.1016\n",
      "155/223, train_loss: 0.1480, step time: 0.1175\n",
      "156/223, train_loss: 0.1370, step time: 0.1221\n",
      "157/223, train_loss: 0.1452, step time: 0.1054\n",
      "158/223, train_loss: 0.1362, step time: 0.1117\n",
      "159/223, train_loss: 0.1293, step time: 0.1001\n",
      "160/223, train_loss: 0.1339, step time: 0.1005\n",
      "161/223, train_loss: 0.1515, step time: 0.1003\n",
      "162/223, train_loss: 0.1385, step time: 0.1112\n",
      "163/223, train_loss: 0.1357, step time: 0.1139\n",
      "164/223, train_loss: 0.1267, step time: 0.0995\n",
      "165/223, train_loss: 0.1383, step time: 0.1002\n",
      "166/223, train_loss: 0.1295, step time: 0.1080\n",
      "167/223, train_loss: 0.1410, step time: 0.0993\n",
      "168/223, train_loss: 0.1254, step time: 0.1030\n",
      "169/223, train_loss: 0.1248, step time: 0.1011\n",
      "170/223, train_loss: 0.1222, step time: 0.1028\n",
      "171/223, train_loss: 0.1317, step time: 0.1142\n",
      "172/223, train_loss: 0.1250, step time: 0.1003\n",
      "173/223, train_loss: 0.1292, step time: 0.1007\n",
      "174/223, train_loss: 0.1406, step time: 0.1243\n",
      "175/223, train_loss: 0.1358, step time: 0.1004\n",
      "176/223, train_loss: 0.1253, step time: 0.1001\n",
      "177/223, train_loss: 0.1376, step time: 0.1002\n",
      "178/223, train_loss: 0.1281, step time: 0.1360\n",
      "179/223, train_loss: 0.1696, step time: 0.1210\n",
      "180/223, train_loss: 0.1170, step time: 0.0999\n",
      "181/223, train_loss: 0.1483, step time: 0.1075\n",
      "182/223, train_loss: 0.1551, step time: 0.0985\n",
      "183/223, train_loss: 0.1384, step time: 0.1161\n",
      "184/223, train_loss: 0.1336, step time: 0.1156\n",
      "185/223, train_loss: 0.1196, step time: 0.1104\n",
      "186/223, train_loss: 0.1294, step time: 0.1030\n",
      "187/223, train_loss: 0.1352, step time: 0.0995\n",
      "188/223, train_loss: 0.1526, step time: 0.1421\n",
      "189/223, train_loss: 0.1349, step time: 0.1001\n",
      "190/223, train_loss: 0.1453, step time: 0.1069\n",
      "191/223, train_loss: 0.1444, step time: 0.1078\n",
      "192/223, train_loss: 0.1310, step time: 0.1130\n",
      "193/223, train_loss: 0.1476, step time: 0.0998\n",
      "194/223, train_loss: 0.1475, step time: 0.1021\n",
      "195/223, train_loss: 0.1262, step time: 0.1103\n",
      "196/223, train_loss: 0.1359, step time: 0.1133\n",
      "197/223, train_loss: 0.1495, step time: 0.1203\n",
      "198/223, train_loss: 0.1245, step time: 0.1098\n",
      "199/223, train_loss: 0.1469, step time: 0.1164\n",
      "200/223, train_loss: 0.1583, step time: 0.1070\n",
      "201/223, train_loss: 0.1320, step time: 0.1258\n",
      "202/223, train_loss: 0.1357, step time: 0.1110\n",
      "203/223, train_loss: 0.1481, step time: 0.1003\n",
      "204/223, train_loss: 0.1262, step time: 0.1019\n",
      "205/223, train_loss: 0.1253, step time: 0.1043\n",
      "206/223, train_loss: 0.1347, step time: 0.1129\n",
      "207/223, train_loss: 0.1383, step time: 0.1140\n",
      "208/223, train_loss: 0.1371, step time: 0.1110\n",
      "209/223, train_loss: 0.1220, step time: 0.1005\n",
      "210/223, train_loss: 0.1205, step time: 0.1214\n",
      "211/223, train_loss: 0.1321, step time: 0.1031\n",
      "212/223, train_loss: 0.1288, step time: 0.1004\n",
      "213/223, train_loss: 0.1334, step time: 0.1047\n",
      "214/223, train_loss: 0.1282, step time: 0.1004\n",
      "215/223, train_loss: 0.1264, step time: 0.1111\n",
      "216/223, train_loss: 0.1293, step time: 0.1089\n",
      "217/223, train_loss: 0.1301, step time: 0.1097\n",
      "218/223, train_loss: 0.1457, step time: 0.1016\n",
      "219/223, train_loss: 0.1428, step time: 0.1001\n",
      "220/223, train_loss: 0.1244, step time: 0.0994\n",
      "221/223, train_loss: 0.1492, step time: 0.1000\n",
      "222/223, train_loss: 0.1519, step time: 0.0997\n",
      "223/223, train_loss: 0.1404, step time: 0.0989\n",
      "epoch 41 average loss: 0.1366\n",
      "time consuming of epoch 41 is: 87.2560\n",
      "----------\n",
      "epoch 42/300\n",
      "1/223, train_loss: 0.1250, step time: 0.1029\n",
      "2/223, train_loss: 0.1278, step time: 0.1054\n",
      "3/223, train_loss: 0.1311, step time: 0.1006\n",
      "4/223, train_loss: 0.1502, step time: 0.0999\n",
      "5/223, train_loss: 0.1415, step time: 0.1433\n",
      "6/223, train_loss: 0.1276, step time: 0.1239\n",
      "7/223, train_loss: 0.1376, step time: 0.1017\n",
      "8/223, train_loss: 0.1398, step time: 0.1008\n",
      "9/223, train_loss: 0.1348, step time: 0.1260\n",
      "10/223, train_loss: 0.1315, step time: 0.0997\n",
      "11/223, train_loss: 0.1334, step time: 0.1097\n",
      "12/223, train_loss: 0.1215, step time: 0.1136\n",
      "13/223, train_loss: 0.1463, step time: 0.1042\n",
      "14/223, train_loss: 0.1327, step time: 0.1095\n",
      "15/223, train_loss: 0.1181, step time: 0.1139\n",
      "16/223, train_loss: 0.1259, step time: 0.1003\n",
      "17/223, train_loss: 0.1383, step time: 0.1153\n",
      "18/223, train_loss: 0.1254, step time: 0.1093\n",
      "19/223, train_loss: 0.1253, step time: 0.1052\n",
      "20/223, train_loss: 0.1411, step time: 0.1101\n",
      "21/223, train_loss: 0.1270, step time: 0.1248\n",
      "22/223, train_loss: 0.1369, step time: 0.1142\n",
      "23/223, train_loss: 0.1369, step time: 0.1009\n",
      "24/223, train_loss: 0.1424, step time: 0.1009\n",
      "25/223, train_loss: 0.1491, step time: 0.1008\n",
      "26/223, train_loss: 0.1304, step time: 0.1068\n",
      "27/223, train_loss: 0.1452, step time: 0.1070\n",
      "28/223, train_loss: 0.1263, step time: 0.1011\n",
      "29/223, train_loss: 0.1272, step time: 0.1005\n",
      "30/223, train_loss: 0.1184, step time: 0.0998\n",
      "31/223, train_loss: 0.1244, step time: 0.1002\n",
      "32/223, train_loss: 0.1269, step time: 0.1276\n",
      "33/223, train_loss: 0.1237, step time: 0.1005\n",
      "34/223, train_loss: 0.1433, step time: 0.1078\n",
      "35/223, train_loss: 0.1271, step time: 0.1039\n",
      "36/223, train_loss: 0.1399, step time: 0.1003\n",
      "37/223, train_loss: 0.1278, step time: 0.1007\n",
      "38/223, train_loss: 0.1230, step time: 0.1007\n",
      "39/223, train_loss: 0.1359, step time: 0.1111\n",
      "40/223, train_loss: 0.1369, step time: 0.1014\n",
      "41/223, train_loss: 0.1372, step time: 0.1060\n",
      "42/223, train_loss: 0.1298, step time: 0.1152\n",
      "43/223, train_loss: 0.1359, step time: 0.1127\n",
      "44/223, train_loss: 0.1326, step time: 0.1276\n",
      "45/223, train_loss: 0.1164, step time: 0.1245\n",
      "46/223, train_loss: 0.1379, step time: 0.1040\n",
      "47/223, train_loss: 0.1336, step time: 0.1152\n",
      "48/223, train_loss: 0.1321, step time: 0.1019\n",
      "49/223, train_loss: 0.1330, step time: 0.1108\n",
      "50/223, train_loss: 0.1204, step time: 0.1439\n",
      "51/223, train_loss: 0.1465, step time: 0.1194\n",
      "52/223, train_loss: 0.1478, step time: 0.1223\n",
      "53/223, train_loss: 0.1449, step time: 0.1265\n",
      "54/223, train_loss: 0.1464, step time: 0.1056\n",
      "55/223, train_loss: 0.1421, step time: 0.1015\n",
      "56/223, train_loss: 0.1267, step time: 0.1094\n",
      "57/223, train_loss: 0.1306, step time: 0.1268\n",
      "58/223, train_loss: 0.1387, step time: 0.1135\n",
      "59/223, train_loss: 0.1250, step time: 0.0996\n",
      "60/223, train_loss: 0.1242, step time: 0.1158\n",
      "61/223, train_loss: 0.1261, step time: 0.1295\n",
      "62/223, train_loss: 0.1213, step time: 0.1261\n",
      "63/223, train_loss: 0.1340, step time: 0.1097\n",
      "64/223, train_loss: 0.1280, step time: 0.1049\n",
      "65/223, train_loss: 0.1370, step time: 0.1056\n",
      "66/223, train_loss: 0.1397, step time: 0.1336\n",
      "67/223, train_loss: 0.1203, step time: 0.1164\n",
      "68/223, train_loss: 0.1369, step time: 0.1152\n",
      "69/223, train_loss: 0.1362, step time: 0.1029\n",
      "70/223, train_loss: 0.1291, step time: 0.1249\n",
      "71/223, train_loss: 0.1379, step time: 0.1101\n",
      "72/223, train_loss: 0.1516, step time: 0.1137\n",
      "73/223, train_loss: 0.1498, step time: 0.1081\n",
      "74/223, train_loss: 0.1574, step time: 0.1049\n",
      "75/223, train_loss: 0.1374, step time: 0.1011\n",
      "76/223, train_loss: 0.1444, step time: 0.1051\n",
      "77/223, train_loss: 0.1401, step time: 0.1257\n",
      "78/223, train_loss: 0.1474, step time: 0.1046\n",
      "79/223, train_loss: 0.1330, step time: 0.1045\n",
      "80/223, train_loss: 0.1579, step time: 0.1131\n",
      "81/223, train_loss: 0.1281, step time: 0.1111\n",
      "82/223, train_loss: 0.1366, step time: 0.1110\n",
      "83/223, train_loss: 0.1374, step time: 0.1098\n",
      "84/223, train_loss: 0.1351, step time: 0.1074\n",
      "85/223, train_loss: 0.1253, step time: 0.1228\n",
      "86/223, train_loss: 0.1523, step time: 0.1015\n",
      "87/223, train_loss: 0.1269, step time: 0.1016\n",
      "88/223, train_loss: 0.1346, step time: 0.1115\n",
      "89/223, train_loss: 0.1293, step time: 0.1030\n",
      "90/223, train_loss: 0.1312, step time: 0.1003\n",
      "91/223, train_loss: 0.1304, step time: 0.1357\n",
      "92/223, train_loss: 0.1319, step time: 0.1000\n",
      "93/223, train_loss: 0.1484, step time: 0.0998\n",
      "94/223, train_loss: 0.1327, step time: 0.1026\n",
      "95/223, train_loss: 0.1284, step time: 0.1300\n",
      "96/223, train_loss: 0.1355, step time: 0.1010\n",
      "97/223, train_loss: 0.1400, step time: 0.1012\n",
      "98/223, train_loss: 0.1392, step time: 0.1000\n",
      "99/223, train_loss: 0.1290, step time: 0.1074\n",
      "100/223, train_loss: 0.1410, step time: 0.1156\n",
      "101/223, train_loss: 0.1252, step time: 0.1010\n",
      "102/223, train_loss: 0.1385, step time: 0.1027\n",
      "103/223, train_loss: 0.1475, step time: 0.1112\n",
      "104/223, train_loss: 0.1286, step time: 0.1144\n",
      "105/223, train_loss: 0.1356, step time: 0.1048\n",
      "106/223, train_loss: 0.1336, step time: 0.1134\n",
      "107/223, train_loss: 0.1340, step time: 0.1003\n",
      "108/223, train_loss: 0.1408, step time: 0.1005\n",
      "109/223, train_loss: 0.1509, step time: 0.1000\n",
      "110/223, train_loss: 0.1313, step time: 0.1036\n",
      "111/223, train_loss: 0.1339, step time: 0.1003\n",
      "112/223, train_loss: 0.1588, step time: 0.1001\n",
      "113/223, train_loss: 0.1489, step time: 0.1069\n",
      "114/223, train_loss: 0.1360, step time: 0.1043\n",
      "115/223, train_loss: 0.1496, step time: 0.1131\n",
      "116/223, train_loss: 0.1342, step time: 0.1001\n",
      "117/223, train_loss: 0.1384, step time: 0.1102\n",
      "118/223, train_loss: 0.1212, step time: 0.1113\n",
      "119/223, train_loss: 0.1444, step time: 0.1057\n",
      "120/223, train_loss: 0.1371, step time: 0.1027\n",
      "121/223, train_loss: 0.1380, step time: 0.1047\n",
      "122/223, train_loss: 0.1276, step time: 0.1005\n",
      "123/223, train_loss: 0.1344, step time: 0.1098\n",
      "124/223, train_loss: 0.1282, step time: 0.1210\n",
      "125/223, train_loss: 0.1420, step time: 0.1075\n",
      "126/223, train_loss: 0.1378, step time: 0.1085\n",
      "127/223, train_loss: 0.1312, step time: 0.1001\n",
      "128/223, train_loss: 0.1266, step time: 0.1005\n",
      "129/223, train_loss: 0.1377, step time: 0.1001\n",
      "130/223, train_loss: 0.1276, step time: 0.1020\n",
      "131/223, train_loss: 0.1209, step time: 0.0994\n",
      "132/223, train_loss: 0.1223, step time: 0.1017\n",
      "133/223, train_loss: 0.1388, step time: 0.1010\n",
      "134/223, train_loss: 0.1336, step time: 0.1006\n",
      "135/223, train_loss: 0.1241, step time: 0.1009\n",
      "136/223, train_loss: 0.1282, step time: 0.1008\n",
      "137/223, train_loss: 0.1294, step time: 0.1138\n",
      "138/223, train_loss: 0.1261, step time: 0.1015\n",
      "139/223, train_loss: 0.1383, step time: 0.1029\n",
      "140/223, train_loss: 0.1248, step time: 0.1048\n",
      "141/223, train_loss: 0.1299, step time: 0.1079\n",
      "142/223, train_loss: 0.1250, step time: 0.1057\n",
      "143/223, train_loss: 0.1223, step time: 0.1000\n",
      "144/223, train_loss: 0.1371, step time: 0.1004\n",
      "145/223, train_loss: 0.1334, step time: 0.1155\n",
      "146/223, train_loss: 0.1172, step time: 0.1003\n",
      "147/223, train_loss: 0.1194, step time: 0.1008\n",
      "148/223, train_loss: 0.1261, step time: 0.1181\n",
      "149/223, train_loss: 0.1461, step time: 0.1143\n",
      "150/223, train_loss: 0.1393, step time: 0.1008\n",
      "151/223, train_loss: 0.1230, step time: 0.1144\n",
      "152/223, train_loss: 0.1263, step time: 0.1029\n",
      "153/223, train_loss: 0.1322, step time: 0.1149\n",
      "154/223, train_loss: 0.1515, step time: 0.1004\n",
      "155/223, train_loss: 0.1269, step time: 0.1010\n",
      "156/223, train_loss: 0.1245, step time: 0.1008\n",
      "157/223, train_loss: 0.1365, step time: 0.1006\n",
      "158/223, train_loss: 0.1258, step time: 0.0995\n",
      "159/223, train_loss: 0.1263, step time: 0.1066\n",
      "160/223, train_loss: 0.1342, step time: 0.1005\n",
      "161/223, train_loss: 0.1177, step time: 0.1004\n",
      "162/223, train_loss: 0.1316, step time: 0.1019\n",
      "163/223, train_loss: 0.1402, step time: 0.1006\n",
      "164/223, train_loss: 0.1397, step time: 0.1013\n",
      "165/223, train_loss: 0.1315, step time: 0.1004\n",
      "166/223, train_loss: 0.1308, step time: 0.1011\n",
      "167/223, train_loss: 0.1236, step time: 0.1114\n",
      "168/223, train_loss: 0.1340, step time: 0.1008\n",
      "169/223, train_loss: 0.1312, step time: 0.1283\n",
      "170/223, train_loss: 0.1297, step time: 0.1051\n",
      "171/223, train_loss: 0.1241, step time: 0.1080\n",
      "172/223, train_loss: 0.1406, step time: 0.1013\n",
      "173/223, train_loss: 0.1507, step time: 0.1087\n",
      "174/223, train_loss: 0.1395, step time: 0.1229\n",
      "175/223, train_loss: 0.1513, step time: 0.1135\n",
      "176/223, train_loss: 0.1285, step time: 0.1152\n",
      "177/223, train_loss: 0.1436, step time: 0.1081\n",
      "178/223, train_loss: 0.1369, step time: 0.1007\n",
      "179/223, train_loss: 0.1474, step time: 0.1002\n",
      "180/223, train_loss: 0.1189, step time: 0.1005\n",
      "181/223, train_loss: 0.1426, step time: 0.0998\n",
      "182/223, train_loss: 0.1332, step time: 0.1002\n",
      "183/223, train_loss: 0.1406, step time: 0.1008\n",
      "184/223, train_loss: 0.1290, step time: 0.1065\n",
      "185/223, train_loss: 0.1447, step time: 0.0999\n",
      "186/223, train_loss: 0.1295, step time: 0.1112\n",
      "187/223, train_loss: 0.1304, step time: 0.1013\n",
      "188/223, train_loss: 0.1274, step time: 0.1014\n",
      "189/223, train_loss: 0.1482, step time: 0.1119\n",
      "190/223, train_loss: 0.1274, step time: 0.1231\n",
      "191/223, train_loss: 0.1285, step time: 0.1017\n",
      "192/223, train_loss: 0.1347, step time: 0.1118\n",
      "193/223, train_loss: 0.1352, step time: 0.1132\n",
      "194/223, train_loss: 0.1291, step time: 0.1146\n",
      "195/223, train_loss: 0.1427, step time: 0.1015\n",
      "196/223, train_loss: 0.1275, step time: 0.1204\n",
      "197/223, train_loss: 0.1246, step time: 0.1192\n",
      "198/223, train_loss: 0.1137, step time: 0.1196\n",
      "199/223, train_loss: 0.1429, step time: 0.1169\n",
      "200/223, train_loss: 0.1217, step time: 0.1010\n",
      "201/223, train_loss: 0.1325, step time: 0.1154\n",
      "202/223, train_loss: 0.1389, step time: 0.1001\n",
      "203/223, train_loss: 0.1416, step time: 0.1029\n",
      "204/223, train_loss: 0.1330, step time: 0.1070\n",
      "205/223, train_loss: 0.1296, step time: 0.1010\n",
      "206/223, train_loss: 0.1226, step time: 0.0995\n",
      "207/223, train_loss: 0.1381, step time: 0.0996\n",
      "208/223, train_loss: 0.1270, step time: 0.0995\n",
      "209/223, train_loss: 0.3368, step time: 0.1225\n",
      "210/223, train_loss: 0.1449, step time: 0.1001\n",
      "211/223, train_loss: 0.1161, step time: 0.1028\n",
      "212/223, train_loss: 0.1373, step time: 0.1136\n",
      "213/223, train_loss: 0.1345, step time: 0.1282\n",
      "214/223, train_loss: 0.1295, step time: 0.1024\n",
      "215/223, train_loss: 0.1392, step time: 0.1007\n",
      "216/223, train_loss: 0.1275, step time: 0.1001\n",
      "217/223, train_loss: 0.1259, step time: 0.1215\n",
      "218/223, train_loss: 0.1547, step time: 0.1150\n",
      "219/223, train_loss: 0.1365, step time: 0.0998\n",
      "220/223, train_loss: 0.1390, step time: 0.1001\n",
      "221/223, train_loss: 0.1181, step time: 0.0990\n",
      "222/223, train_loss: 0.1453, step time: 0.0994\n",
      "223/223, train_loss: 0.1416, step time: 0.0997\n",
      "epoch 42 average loss: 0.1348\n",
      "time consuming of epoch 42 is: 95.4441\n",
      "----------\n",
      "epoch 43/300\n",
      "1/223, train_loss: 0.1390, step time: 0.1063\n",
      "2/223, train_loss: 0.1385, step time: 0.1000\n",
      "3/223, train_loss: 0.1203, step time: 0.1018\n",
      "4/223, train_loss: 0.1507, step time: 0.1012\n",
      "5/223, train_loss: 0.1399, step time: 0.1137\n",
      "6/223, train_loss: 0.1440, step time: 0.1064\n",
      "7/223, train_loss: 0.1459, step time: 0.1277\n",
      "8/223, train_loss: 0.1234, step time: 0.1152\n",
      "9/223, train_loss: 0.1412, step time: 0.1173\n",
      "10/223, train_loss: 0.1306, step time: 0.1299\n",
      "11/223, train_loss: 0.1385, step time: 0.1333\n",
      "12/223, train_loss: 0.1317, step time: 0.1016\n",
      "13/223, train_loss: 0.1256, step time: 0.1214\n",
      "14/223, train_loss: 0.1265, step time: 0.1089\n",
      "15/223, train_loss: 0.1223, step time: 0.1001\n",
      "16/223, train_loss: 0.1383, step time: 0.1119\n",
      "17/223, train_loss: 0.1335, step time: 0.1015\n",
      "18/223, train_loss: 0.1259, step time: 0.0995\n",
      "19/223, train_loss: 0.1329, step time: 0.1012\n",
      "20/223, train_loss: 0.1342, step time: 0.1031\n",
      "21/223, train_loss: 0.1367, step time: 0.1097\n",
      "22/223, train_loss: 0.1384, step time: 0.1012\n",
      "23/223, train_loss: 0.1244, step time: 0.1054\n",
      "24/223, train_loss: 0.1224, step time: 0.1294\n",
      "25/223, train_loss: 0.1397, step time: 0.1041\n",
      "26/223, train_loss: 0.1420, step time: 0.1040\n",
      "27/223, train_loss: 0.1439, step time: 0.1165\n",
      "28/223, train_loss: 0.1338, step time: 0.1421\n",
      "29/223, train_loss: 0.1235, step time: 0.1179\n",
      "30/223, train_loss: 0.1308, step time: 0.1046\n",
      "31/223, train_loss: 0.1456, step time: 0.1241\n",
      "32/223, train_loss: 0.1329, step time: 0.0998\n",
      "33/223, train_loss: 0.1417, step time: 0.1169\n",
      "34/223, train_loss: 0.1198, step time: 0.1127\n",
      "35/223, train_loss: 0.1388, step time: 0.1309\n",
      "36/223, train_loss: 0.1508, step time: 0.1056\n",
      "37/223, train_loss: 0.1486, step time: 0.1153\n",
      "38/223, train_loss: 0.1387, step time: 0.1277\n",
      "39/223, train_loss: 0.1272, step time: 0.1114\n",
      "40/223, train_loss: 0.1299, step time: 0.1338\n",
      "41/223, train_loss: 0.1360, step time: 0.1108\n",
      "42/223, train_loss: 0.1255, step time: 0.1000\n",
      "43/223, train_loss: 0.1375, step time: 0.1101\n",
      "44/223, train_loss: 0.1275, step time: 0.0999\n",
      "45/223, train_loss: 0.1526, step time: 0.1078\n",
      "46/223, train_loss: 0.1305, step time: 0.1057\n",
      "47/223, train_loss: 0.1209, step time: 0.1009\n",
      "48/223, train_loss: 0.1323, step time: 0.1011\n",
      "49/223, train_loss: 0.1313, step time: 0.1076\n",
      "50/223, train_loss: 0.1193, step time: 0.0992\n",
      "51/223, train_loss: 0.1407, step time: 0.1005\n",
      "52/223, train_loss: 0.1212, step time: 0.1076\n",
      "53/223, train_loss: 0.1240, step time: 0.0997\n",
      "54/223, train_loss: 0.1360, step time: 0.1000\n",
      "55/223, train_loss: 0.1463, step time: 0.1005\n",
      "56/223, train_loss: 0.1470, step time: 0.1117\n",
      "57/223, train_loss: 0.1269, step time: 0.1197\n",
      "58/223, train_loss: 0.1280, step time: 0.1005\n",
      "59/223, train_loss: 0.1289, step time: 0.1164\n",
      "60/223, train_loss: 0.1404, step time: 0.1238\n",
      "61/223, train_loss: 0.1319, step time: 0.1100\n",
      "62/223, train_loss: 0.1263, step time: 0.1174\n",
      "63/223, train_loss: 0.1338, step time: 0.1091\n",
      "64/223, train_loss: 0.1293, step time: 0.1141\n",
      "65/223, train_loss: 0.1196, step time: 0.1093\n",
      "66/223, train_loss: 0.1302, step time: 0.1008\n",
      "67/223, train_loss: 0.1246, step time: 0.1046\n",
      "68/223, train_loss: 0.1344, step time: 0.1006\n",
      "69/223, train_loss: 0.1328, step time: 0.0997\n",
      "70/223, train_loss: 0.1399, step time: 0.0997\n",
      "71/223, train_loss: 0.1341, step time: 0.1018\n",
      "72/223, train_loss: 0.1350, step time: 0.1019\n",
      "73/223, train_loss: 0.1501, step time: 0.1005\n",
      "74/223, train_loss: 0.1314, step time: 0.0997\n",
      "75/223, train_loss: 0.1366, step time: 0.1012\n",
      "76/223, train_loss: 0.3325, step time: 0.1173\n",
      "77/223, train_loss: 0.1206, step time: 0.0996\n",
      "78/223, train_loss: 0.1325, step time: 0.1126\n",
      "79/223, train_loss: 0.1252, step time: 0.1039\n",
      "80/223, train_loss: 0.1379, step time: 0.0994\n",
      "81/223, train_loss: 0.1493, step time: 0.1257\n",
      "82/223, train_loss: 0.1297, step time: 0.1130\n",
      "83/223, train_loss: 0.1570, step time: 0.1062\n",
      "84/223, train_loss: 0.1283, step time: 0.1000\n",
      "85/223, train_loss: 0.1417, step time: 0.1113\n",
      "86/223, train_loss: 0.1185, step time: 0.1163\n",
      "87/223, train_loss: 0.1316, step time: 0.1007\n",
      "88/223, train_loss: 0.1165, step time: 0.1001\n",
      "89/223, train_loss: 0.1637, step time: 0.1001\n",
      "90/223, train_loss: 0.1359, step time: 0.0990\n",
      "91/223, train_loss: 0.1299, step time: 0.1001\n",
      "92/223, train_loss: 0.1374, step time: 0.1025\n",
      "93/223, train_loss: 0.1365, step time: 0.1118\n",
      "94/223, train_loss: 0.1278, step time: 0.1086\n",
      "95/223, train_loss: 0.1248, step time: 0.1101\n",
      "96/223, train_loss: 0.1270, step time: 0.1049\n",
      "97/223, train_loss: 0.1345, step time: 0.1198\n",
      "98/223, train_loss: 0.1513, step time: 0.1004\n",
      "99/223, train_loss: 0.1268, step time: 0.1026\n",
      "100/223, train_loss: 0.1285, step time: 0.1003\n",
      "101/223, train_loss: 0.1347, step time: 0.1176\n",
      "102/223, train_loss: 0.1380, step time: 0.0994\n",
      "103/223, train_loss: 0.1376, step time: 0.1228\n",
      "104/223, train_loss: 0.1383, step time: 0.1330\n",
      "105/223, train_loss: 0.1443, step time: 0.1111\n",
      "106/223, train_loss: 0.1577, step time: 0.1617\n",
      "107/223, train_loss: 0.1255, step time: 0.1167\n",
      "108/223, train_loss: 0.1387, step time: 0.1043\n",
      "109/223, train_loss: 0.1212, step time: 0.1094\n",
      "110/223, train_loss: 0.1224, step time: 0.1259\n",
      "111/223, train_loss: 0.1411, step time: 0.1095\n",
      "112/223, train_loss: 0.1287, step time: 0.1067\n",
      "113/223, train_loss: 0.1239, step time: 0.1217\n",
      "114/223, train_loss: 0.1472, step time: 0.1431\n",
      "115/223, train_loss: 0.1610, step time: 0.1223\n",
      "116/223, train_loss: 0.1329, step time: 0.1051\n",
      "117/223, train_loss: 0.1419, step time: 0.1146\n",
      "118/223, train_loss: 0.1531, step time: 0.1109\n",
      "119/223, train_loss: 0.1260, step time: 0.1053\n",
      "120/223, train_loss: 0.1401, step time: 0.1128\n",
      "121/223, train_loss: 0.1320, step time: 0.1065\n",
      "122/223, train_loss: 0.1296, step time: 0.1131\n",
      "123/223, train_loss: 0.1449, step time: 0.1155\n",
      "124/223, train_loss: 0.1405, step time: 0.1179\n",
      "125/223, train_loss: 0.1338, step time: 0.1163\n",
      "126/223, train_loss: 0.1312, step time: 0.1093\n",
      "127/223, train_loss: 0.1290, step time: 0.1344\n",
      "128/223, train_loss: 0.1274, step time: 0.1106\n",
      "129/223, train_loss: 0.1331, step time: 0.1087\n",
      "130/223, train_loss: 0.1384, step time: 0.1000\n",
      "131/223, train_loss: 0.1261, step time: 0.1130\n",
      "132/223, train_loss: 0.1288, step time: 0.1213\n",
      "133/223, train_loss: 0.1326, step time: 0.1053\n",
      "134/223, train_loss: 0.1323, step time: 0.1041\n",
      "135/223, train_loss: 0.1339, step time: 0.1201\n",
      "136/223, train_loss: 0.1407, step time: 0.0995\n",
      "137/223, train_loss: 0.1352, step time: 0.1087\n",
      "138/223, train_loss: 0.1410, step time: 0.1215\n",
      "139/223, train_loss: 0.1491, step time: 0.1193\n",
      "140/223, train_loss: 0.1316, step time: 0.1003\n",
      "141/223, train_loss: 0.1496, step time: 0.1034\n",
      "142/223, train_loss: 0.1402, step time: 0.1010\n",
      "143/223, train_loss: 0.1404, step time: 0.0990\n",
      "144/223, train_loss: 0.1295, step time: 0.1031\n",
      "145/223, train_loss: 0.1247, step time: 0.1001\n",
      "146/223, train_loss: 0.1359, step time: 0.0993\n",
      "147/223, train_loss: 0.1294, step time: 0.0995\n",
      "148/223, train_loss: 0.1365, step time: 0.1032\n",
      "149/223, train_loss: 0.1374, step time: 0.1055\n",
      "150/223, train_loss: 0.1173, step time: 0.1014\n",
      "151/223, train_loss: 0.1456, step time: 0.1004\n",
      "152/223, train_loss: 0.1412, step time: 0.1002\n",
      "153/223, train_loss: 0.1403, step time: 0.0999\n",
      "154/223, train_loss: 0.1257, step time: 0.1063\n",
      "155/223, train_loss: 0.1345, step time: 0.0996\n",
      "156/223, train_loss: 0.1409, step time: 0.1033\n",
      "157/223, train_loss: 0.1326, step time: 0.1002\n",
      "158/223, train_loss: 0.1397, step time: 0.1038\n",
      "159/223, train_loss: 0.1473, step time: 0.0998\n",
      "160/223, train_loss: 0.1340, step time: 0.1025\n",
      "161/223, train_loss: 0.1409, step time: 0.1001\n",
      "162/223, train_loss: 0.1532, step time: 0.1001\n",
      "163/223, train_loss: 0.1623, step time: 0.1001\n",
      "164/223, train_loss: 0.1299, step time: 0.1278\n",
      "165/223, train_loss: 0.1591, step time: 0.1257\n",
      "166/223, train_loss: 0.1366, step time: 0.1021\n",
      "167/223, train_loss: 0.1348, step time: 0.1197\n",
      "168/223, train_loss: 0.1240, step time: 0.1105\n",
      "169/223, train_loss: 0.1353, step time: 0.1549\n",
      "170/223, train_loss: 0.1265, step time: 0.1316\n",
      "171/223, train_loss: 0.1170, step time: 0.1043\n",
      "172/223, train_loss: 0.1224, step time: 0.1238\n",
      "173/223, train_loss: 0.1244, step time: 0.1001\n",
      "174/223, train_loss: 0.1513, step time: 0.1003\n",
      "175/223, train_loss: 0.1239, step time: 0.1005\n",
      "176/223, train_loss: 0.1278, step time: 0.1182\n",
      "177/223, train_loss: 0.1261, step time: 0.1034\n",
      "178/223, train_loss: 0.1403, step time: 0.0999\n",
      "179/223, train_loss: 0.1277, step time: 0.1005\n",
      "180/223, train_loss: 0.1392, step time: 0.1001\n",
      "181/223, train_loss: 0.1401, step time: 0.0996\n",
      "182/223, train_loss: 0.1380, step time: 0.1052\n",
      "183/223, train_loss: 0.1533, step time: 0.1079\n",
      "184/223, train_loss: 0.1274, step time: 0.1034\n",
      "185/223, train_loss: 0.1208, step time: 0.1003\n",
      "186/223, train_loss: 0.1406, step time: 0.0996\n",
      "187/223, train_loss: 0.1587, step time: 0.0996\n",
      "188/223, train_loss: 0.1399, step time: 0.1002\n",
      "189/223, train_loss: 0.1470, step time: 0.1004\n",
      "190/223, train_loss: 0.1407, step time: 0.1000\n",
      "191/223, train_loss: 0.1404, step time: 0.0993\n",
      "192/223, train_loss: 0.1417, step time: 0.1000\n",
      "193/223, train_loss: 0.1484, step time: 0.1015\n",
      "194/223, train_loss: 0.1257, step time: 0.0992\n",
      "195/223, train_loss: 0.1213, step time: 0.0988\n",
      "196/223, train_loss: 0.1167, step time: 0.1002\n",
      "197/223, train_loss: 0.1531, step time: 0.1153\n",
      "198/223, train_loss: 0.1424, step time: 0.1003\n",
      "199/223, train_loss: 0.1396, step time: 0.0997\n",
      "200/223, train_loss: 0.1248, step time: 0.1016\n",
      "201/223, train_loss: 0.1466, step time: 0.1065\n",
      "202/223, train_loss: 0.1292, step time: 0.1082\n",
      "203/223, train_loss: 0.1232, step time: 0.1139\n",
      "204/223, train_loss: 0.1283, step time: 0.1028\n",
      "205/223, train_loss: 0.1237, step time: 0.1266\n",
      "206/223, train_loss: 0.1339, step time: 0.1202\n",
      "207/223, train_loss: 0.1250, step time: 0.1119\n",
      "208/223, train_loss: 0.1243, step time: 0.1049\n",
      "209/223, train_loss: 0.1410, step time: 0.1006\n",
      "210/223, train_loss: 0.1338, step time: 0.1077\n",
      "211/223, train_loss: 0.1270, step time: 0.1104\n",
      "212/223, train_loss: 0.1401, step time: 0.1132\n",
      "213/223, train_loss: 0.1335, step time: 0.1009\n",
      "214/223, train_loss: 0.1386, step time: 0.1161\n",
      "215/223, train_loss: 0.1221, step time: 0.1196\n",
      "216/223, train_loss: 0.1315, step time: 0.1207\n",
      "217/223, train_loss: 0.1346, step time: 0.1001\n",
      "218/223, train_loss: 0.1246, step time: 0.0998\n",
      "219/223, train_loss: 0.1242, step time: 0.1006\n",
      "220/223, train_loss: 0.1293, step time: 0.0998\n",
      "221/223, train_loss: 0.1284, step time: 0.1044\n",
      "222/223, train_loss: 0.1334, step time: 0.1234\n",
      "223/223, train_loss: 0.1374, step time: 0.0997\n",
      "epoch 43 average loss: 0.1355\n",
      "time consuming of epoch 43 is: 91.4314\n",
      "----------\n",
      "epoch 44/300\n",
      "1/223, train_loss: 0.1263, step time: 0.1044\n",
      "2/223, train_loss: 0.1442, step time: 0.1174\n",
      "3/223, train_loss: 0.1437, step time: 0.1050\n",
      "4/223, train_loss: 0.1340, step time: 0.1048\n",
      "5/223, train_loss: 0.1373, step time: 0.1119\n",
      "6/223, train_loss: 0.1217, step time: 0.1060\n",
      "7/223, train_loss: 0.1335, step time: 0.1175\n",
      "8/223, train_loss: 0.1286, step time: 0.1256\n",
      "9/223, train_loss: 0.1332, step time: 0.1124\n",
      "10/223, train_loss: 0.1247, step time: 0.1187\n",
      "11/223, train_loss: 0.1604, step time: 0.1214\n",
      "12/223, train_loss: 0.1420, step time: 0.1095\n",
      "13/223, train_loss: 0.1561, step time: 0.0994\n",
      "14/223, train_loss: 0.1227, step time: 0.1068\n",
      "15/223, train_loss: 0.1322, step time: 0.0998\n",
      "16/223, train_loss: 0.1356, step time: 0.1002\n",
      "17/223, train_loss: 0.1279, step time: 0.1158\n",
      "18/223, train_loss: 0.1649, step time: 0.1107\n",
      "19/223, train_loss: 0.1474, step time: 0.0998\n",
      "20/223, train_loss: 0.1321, step time: 0.1100\n",
      "21/223, train_loss: 0.1325, step time: 0.1108\n",
      "22/223, train_loss: 0.1411, step time: 0.1097\n",
      "23/223, train_loss: 0.1259, step time: 0.0999\n",
      "24/223, train_loss: 0.1187, step time: 0.1020\n",
      "25/223, train_loss: 0.1178, step time: 0.1060\n",
      "26/223, train_loss: 0.1332, step time: 0.1162\n",
      "27/223, train_loss: 0.1327, step time: 0.1052\n",
      "28/223, train_loss: 0.1339, step time: 0.1007\n",
      "29/223, train_loss: 0.1187, step time: 0.1176\n",
      "30/223, train_loss: 0.1301, step time: 0.1006\n",
      "31/223, train_loss: 0.1448, step time: 0.1001\n",
      "32/223, train_loss: 0.1402, step time: 0.1063\n",
      "33/223, train_loss: 0.1336, step time: 0.1005\n",
      "34/223, train_loss: 0.1483, step time: 0.1136\n",
      "35/223, train_loss: 0.1183, step time: 0.1009\n",
      "36/223, train_loss: 0.1287, step time: 0.1057\n",
      "37/223, train_loss: 0.1433, step time: 0.1077\n",
      "38/223, train_loss: 0.1367, step time: 0.1155\n",
      "39/223, train_loss: 0.1423, step time: 0.0998\n",
      "40/223, train_loss: 0.1378, step time: 0.1003\n",
      "41/223, train_loss: 0.1370, step time: 0.1084\n",
      "42/223, train_loss: 0.1172, step time: 0.1125\n",
      "43/223, train_loss: 0.1241, step time: 0.1001\n",
      "44/223, train_loss: 0.1224, step time: 0.1005\n",
      "45/223, train_loss: 0.1340, step time: 0.1114\n",
      "46/223, train_loss: 0.1212, step time: 0.1069\n",
      "47/223, train_loss: 0.1358, step time: 0.0998\n",
      "48/223, train_loss: 0.1257, step time: 0.1012\n",
      "49/223, train_loss: 0.1346, step time: 0.0995\n",
      "50/223, train_loss: 0.1320, step time: 0.0993\n",
      "51/223, train_loss: 0.1389, step time: 0.1076\n",
      "52/223, train_loss: 0.1263, step time: 0.1144\n",
      "53/223, train_loss: 0.1376, step time: 0.1137\n",
      "54/223, train_loss: 0.1280, step time: 0.1105\n",
      "55/223, train_loss: 0.1241, step time: 0.1008\n",
      "56/223, train_loss: 0.1539, step time: 0.1003\n",
      "57/223, train_loss: 0.1308, step time: 0.1043\n",
      "58/223, train_loss: 0.1331, step time: 0.0997\n",
      "59/223, train_loss: 0.1413, step time: 0.1019\n",
      "60/223, train_loss: 0.1201, step time: 0.1075\n",
      "61/223, train_loss: 0.1280, step time: 0.1142\n",
      "62/223, train_loss: 0.1357, step time: 0.1163\n",
      "63/223, train_loss: 0.1298, step time: 0.1004\n",
      "64/223, train_loss: 0.1388, step time: 0.1003\n",
      "65/223, train_loss: 0.1303, step time: 0.1197\n",
      "66/223, train_loss: 0.1248, step time: 0.1005\n",
      "67/223, train_loss: 0.1277, step time: 0.1009\n",
      "68/223, train_loss: 0.1329, step time: 0.0996\n",
      "69/223, train_loss: 0.1455, step time: 0.1202\n",
      "70/223, train_loss: 0.1207, step time: 0.1122\n",
      "71/223, train_loss: 0.1258, step time: 0.1073\n",
      "72/223, train_loss: 0.1379, step time: 0.1106\n",
      "73/223, train_loss: 0.1268, step time: 0.1076\n",
      "74/223, train_loss: 0.1291, step time: 0.1196\n",
      "75/223, train_loss: 0.1357, step time: 0.1097\n",
      "76/223, train_loss: 0.1258, step time: 0.1163\n",
      "77/223, train_loss: 0.1426, step time: 0.1058\n",
      "78/223, train_loss: 0.1353, step time: 0.1120\n",
      "79/223, train_loss: 0.1389, step time: 0.1285\n",
      "80/223, train_loss: 0.1163, step time: 0.0995\n",
      "81/223, train_loss: 0.1196, step time: 0.1075\n",
      "82/223, train_loss: 0.1315, step time: 0.1058\n",
      "83/223, train_loss: 0.1365, step time: 0.1210\n",
      "84/223, train_loss: 0.1422, step time: 0.1056\n",
      "85/223, train_loss: 0.1178, step time: 0.1011\n",
      "86/223, train_loss: 0.1392, step time: 0.1131\n",
      "87/223, train_loss: 0.1481, step time: 0.1112\n",
      "88/223, train_loss: 0.1317, step time: 0.1111\n",
      "89/223, train_loss: 0.1273, step time: 0.0996\n",
      "90/223, train_loss: 0.1365, step time: 0.1009\n",
      "91/223, train_loss: 0.1294, step time: 0.1100\n",
      "92/223, train_loss: 0.1282, step time: 0.1184\n",
      "93/223, train_loss: 0.1230, step time: 0.1058\n",
      "94/223, train_loss: 0.1327, step time: 0.1013\n",
      "95/223, train_loss: 0.1501, step time: 0.1001\n",
      "96/223, train_loss: 0.1424, step time: 0.1053\n",
      "97/223, train_loss: 0.1526, step time: 0.1258\n",
      "98/223, train_loss: 0.1334, step time: 0.1030\n",
      "99/223, train_loss: 0.1226, step time: 0.1189\n",
      "100/223, train_loss: 0.1382, step time: 0.1228\n",
      "101/223, train_loss: 0.1457, step time: 0.1051\n",
      "102/223, train_loss: 0.1238, step time: 0.1070\n",
      "103/223, train_loss: 0.1187, step time: 0.1077\n",
      "104/223, train_loss: 0.1402, step time: 0.1259\n",
      "105/223, train_loss: 0.1214, step time: 0.1002\n",
      "106/223, train_loss: 0.1392, step time: 0.0997\n",
      "107/223, train_loss: 0.1231, step time: 0.1085\n",
      "108/223, train_loss: 0.1290, step time: 0.1107\n",
      "109/223, train_loss: 0.1389, step time: 0.1003\n",
      "110/223, train_loss: 0.1298, step time: 0.1071\n",
      "111/223, train_loss: 0.1289, step time: 0.1013\n",
      "112/223, train_loss: 0.1482, step time: 0.1064\n",
      "113/223, train_loss: 0.1357, step time: 0.1064\n",
      "114/223, train_loss: 0.1346, step time: 0.1046\n",
      "115/223, train_loss: 0.1227, step time: 0.1019\n",
      "116/223, train_loss: 0.1254, step time: 0.1203\n",
      "117/223, train_loss: 0.1256, step time: 0.1118\n",
      "118/223, train_loss: 0.1467, step time: 0.1155\n",
      "119/223, train_loss: 0.1270, step time: 0.1145\n",
      "120/223, train_loss: 0.1370, step time: 0.1169\n",
      "121/223, train_loss: 0.1208, step time: 0.1090\n",
      "122/223, train_loss: 0.1318, step time: 0.1089\n",
      "123/223, train_loss: 0.1265, step time: 0.1055\n",
      "124/223, train_loss: 0.1450, step time: 0.1261\n",
      "125/223, train_loss: 0.1379, step time: 0.1089\n",
      "126/223, train_loss: 0.1194, step time: 0.0988\n",
      "127/223, train_loss: 0.1400, step time: 0.0992\n",
      "128/223, train_loss: 0.1375, step time: 0.1039\n",
      "129/223, train_loss: 0.1259, step time: 0.1010\n",
      "130/223, train_loss: 0.1214, step time: 0.1011\n",
      "131/223, train_loss: 0.1413, step time: 0.0996\n",
      "132/223, train_loss: 0.1339, step time: 0.1154\n",
      "133/223, train_loss: 0.1155, step time: 0.1051\n",
      "134/223, train_loss: 0.1366, step time: 0.1020\n",
      "135/223, train_loss: 0.1178, step time: 0.0996\n",
      "136/223, train_loss: 0.1548, step time: 0.0995\n",
      "137/223, train_loss: 0.1295, step time: 0.1004\n",
      "138/223, train_loss: 0.1300, step time: 0.1100\n",
      "139/223, train_loss: 0.1308, step time: 0.1003\n",
      "140/223, train_loss: 0.1263, step time: 0.1002\n",
      "141/223, train_loss: 0.1267, step time: 0.1011\n",
      "142/223, train_loss: 0.1310, step time: 0.1001\n",
      "143/223, train_loss: 0.1262, step time: 0.0993\n",
      "144/223, train_loss: 0.3282, step time: 0.1007\n",
      "145/223, train_loss: 0.1453, step time: 0.1007\n",
      "146/223, train_loss: 0.1427, step time: 0.0999\n",
      "147/223, train_loss: 0.1223, step time: 0.1005\n",
      "148/223, train_loss: 0.1359, step time: 0.1001\n",
      "149/223, train_loss: 0.1386, step time: 0.1093\n",
      "150/223, train_loss: 0.1202, step time: 0.1090\n",
      "151/223, train_loss: 0.1338, step time: 0.1075\n",
      "152/223, train_loss: 0.1308, step time: 0.1178\n",
      "153/223, train_loss: 0.1368, step time: 0.1028\n",
      "154/223, train_loss: 0.1379, step time: 0.0999\n",
      "155/223, train_loss: 0.1391, step time: 0.1006\n",
      "156/223, train_loss: 0.1288, step time: 0.1007\n",
      "157/223, train_loss: 0.1337, step time: 0.1102\n",
      "158/223, train_loss: 0.1376, step time: 0.0998\n",
      "159/223, train_loss: 0.1418, step time: 0.1001\n",
      "160/223, train_loss: 0.1450, step time: 0.1003\n",
      "161/223, train_loss: 0.1332, step time: 0.0998\n",
      "162/223, train_loss: 0.1393, step time: 0.1140\n",
      "163/223, train_loss: 0.1266, step time: 0.0995\n",
      "164/223, train_loss: 0.1335, step time: 0.1042\n",
      "165/223, train_loss: 0.1246, step time: 0.1012\n",
      "166/223, train_loss: 0.1224, step time: 0.1146\n",
      "167/223, train_loss: 0.1388, step time: 0.1007\n",
      "168/223, train_loss: 0.1441, step time: 0.0998\n",
      "169/223, train_loss: 0.1320, step time: 0.0996\n",
      "170/223, train_loss: 0.1285, step time: 0.1097\n",
      "171/223, train_loss: 0.1439, step time: 0.0998\n",
      "172/223, train_loss: 0.1438, step time: 0.1005\n",
      "173/223, train_loss: 0.1427, step time: 0.1003\n",
      "174/223, train_loss: 0.1462, step time: 0.1108\n",
      "175/223, train_loss: 0.1119, step time: 0.0999\n",
      "176/223, train_loss: 0.1245, step time: 0.1002\n",
      "177/223, train_loss: 0.1172, step time: 0.1008\n",
      "178/223, train_loss: 0.1285, step time: 0.1086\n",
      "179/223, train_loss: 0.1262, step time: 0.1006\n",
      "180/223, train_loss: 0.1376, step time: 0.1004\n",
      "181/223, train_loss: 0.1380, step time: 0.1120\n",
      "182/223, train_loss: 0.1264, step time: 0.1005\n",
      "183/223, train_loss: 0.1367, step time: 0.1175\n",
      "184/223, train_loss: 0.1252, step time: 0.1087\n",
      "185/223, train_loss: 0.1286, step time: 0.1145\n",
      "186/223, train_loss: 0.1259, step time: 0.1231\n",
      "187/223, train_loss: 0.1326, step time: 0.1022\n",
      "188/223, train_loss: 0.1388, step time: 0.1104\n",
      "189/223, train_loss: 0.1313, step time: 0.1002\n",
      "190/223, train_loss: 0.1339, step time: 0.1013\n",
      "191/223, train_loss: 0.1238, step time: 0.1013\n",
      "192/223, train_loss: 0.1280, step time: 0.1002\n",
      "193/223, train_loss: 0.1463, step time: 0.0999\n",
      "194/223, train_loss: 0.1210, step time: 0.1019\n",
      "195/223, train_loss: 0.1292, step time: 0.1108\n",
      "196/223, train_loss: 0.1227, step time: 0.0999\n",
      "197/223, train_loss: 0.1212, step time: 0.1014\n",
      "198/223, train_loss: 0.1336, step time: 0.1037\n",
      "199/223, train_loss: 0.1295, step time: 0.1050\n",
      "200/223, train_loss: 0.1343, step time: 0.1283\n",
      "201/223, train_loss: 0.1255, step time: 0.1006\n",
      "202/223, train_loss: 0.1355, step time: 0.0995\n",
      "203/223, train_loss: 0.1400, step time: 0.0999\n",
      "204/223, train_loss: 0.1310, step time: 0.1005\n",
      "205/223, train_loss: 0.1195, step time: 0.1264\n",
      "206/223, train_loss: 0.1274, step time: 0.1064\n",
      "207/223, train_loss: 0.1317, step time: 0.1007\n",
      "208/223, train_loss: 0.1318, step time: 0.1130\n",
      "209/223, train_loss: 0.1315, step time: 0.0996\n",
      "210/223, train_loss: 0.1372, step time: 0.1092\n",
      "211/223, train_loss: 0.1226, step time: 0.1063\n",
      "212/223, train_loss: 0.1456, step time: 0.1330\n",
      "213/223, train_loss: 0.1400, step time: 0.1089\n",
      "214/223, train_loss: 0.1196, step time: 0.1203\n",
      "215/223, train_loss: 0.1237, step time: 0.1001\n",
      "216/223, train_loss: 0.1483, step time: 0.1000\n",
      "217/223, train_loss: 0.1339, step time: 0.1007\n",
      "218/223, train_loss: 0.1470, step time: 0.1009\n",
      "219/223, train_loss: 0.1331, step time: 0.1008\n",
      "220/223, train_loss: 0.1501, step time: 0.1012\n",
      "221/223, train_loss: 0.1211, step time: 0.0999\n",
      "222/223, train_loss: 0.1332, step time: 0.0997\n",
      "223/223, train_loss: 0.1369, step time: 0.0992\n",
      "epoch 44 average loss: 0.1336\n",
      "time consuming of epoch 44 is: 85.7692\n",
      "----------\n",
      "epoch 45/300\n",
      "1/223, train_loss: 0.1608, step time: 0.1076\n",
      "2/223, train_loss: 0.1344, step time: 0.1038\n",
      "3/223, train_loss: 0.1429, step time: 0.1006\n",
      "4/223, train_loss: 0.1408, step time: 0.0998\n",
      "5/223, train_loss: 0.1192, step time: 0.1153\n",
      "6/223, train_loss: 0.1209, step time: 0.1071\n",
      "7/223, train_loss: 0.1510, step time: 0.1520\n",
      "8/223, train_loss: 0.1407, step time: 0.1150\n",
      "9/223, train_loss: 0.1229, step time: 0.1005\n",
      "10/223, train_loss: 0.1262, step time: 0.1107\n",
      "11/223, train_loss: 0.1357, step time: 0.1223\n",
      "12/223, train_loss: 0.1285, step time: 0.1186\n",
      "13/223, train_loss: 0.1317, step time: 0.1049\n",
      "14/223, train_loss: 0.1315, step time: 0.1163\n",
      "15/223, train_loss: 0.1338, step time: 0.1006\n",
      "16/223, train_loss: 0.1300, step time: 0.1005\n",
      "17/223, train_loss: 0.1299, step time: 0.1029\n",
      "18/223, train_loss: 0.1400, step time: 0.1001\n",
      "19/223, train_loss: 0.1257, step time: 0.1252\n",
      "20/223, train_loss: 0.1415, step time: 0.1149\n",
      "21/223, train_loss: 0.1426, step time: 0.1246\n",
      "22/223, train_loss: 0.1457, step time: 0.1055\n",
      "23/223, train_loss: 0.1227, step time: 0.1097\n",
      "24/223, train_loss: 0.1373, step time: 0.1111\n",
      "25/223, train_loss: 0.1256, step time: 0.1001\n",
      "26/223, train_loss: 0.1094, step time: 0.1128\n",
      "27/223, train_loss: 0.1439, step time: 0.1050\n",
      "28/223, train_loss: 0.1355, step time: 0.1149\n",
      "29/223, train_loss: 0.1255, step time: 0.1197\n",
      "30/223, train_loss: 0.1196, step time: 0.1183\n",
      "31/223, train_loss: 0.1269, step time: 0.1006\n",
      "32/223, train_loss: 0.1370, step time: 0.1004\n",
      "33/223, train_loss: 0.1166, step time: 0.1451\n",
      "34/223, train_loss: 0.1452, step time: 0.0999\n",
      "35/223, train_loss: 0.1290, step time: 0.1003\n",
      "36/223, train_loss: 0.1396, step time: 0.1012\n",
      "37/223, train_loss: 0.1284, step time: 0.1374\n",
      "38/223, train_loss: 0.1257, step time: 0.1120\n",
      "39/223, train_loss: 0.1411, step time: 0.1003\n",
      "40/223, train_loss: 0.1255, step time: 0.1168\n",
      "41/223, train_loss: 0.1306, step time: 0.1115\n",
      "42/223, train_loss: 0.1312, step time: 0.1108\n",
      "43/223, train_loss: 0.1356, step time: 0.1320\n",
      "44/223, train_loss: 0.1307, step time: 0.1201\n",
      "45/223, train_loss: 0.1152, step time: 0.1112\n",
      "46/223, train_loss: 0.1279, step time: 0.1033\n",
      "47/223, train_loss: 0.1316, step time: 0.1115\n",
      "48/223, train_loss: 0.1385, step time: 0.0997\n",
      "49/223, train_loss: 0.1314, step time: 0.1006\n",
      "50/223, train_loss: 0.1422, step time: 0.0997\n",
      "51/223, train_loss: 0.1380, step time: 0.1053\n",
      "52/223, train_loss: 0.1224, step time: 0.1044\n",
      "53/223, train_loss: 0.1470, step time: 0.1023\n",
      "54/223, train_loss: 0.1311, step time: 0.1165\n",
      "55/223, train_loss: 0.1181, step time: 0.1175\n",
      "56/223, train_loss: 0.1310, step time: 0.1050\n",
      "57/223, train_loss: 0.1253, step time: 0.1089\n",
      "58/223, train_loss: 0.1262, step time: 0.1034\n",
      "59/223, train_loss: 0.1159, step time: 0.1005\n",
      "60/223, train_loss: 0.1466, step time: 0.1005\n",
      "61/223, train_loss: 0.1338, step time: 0.1038\n",
      "62/223, train_loss: 0.1233, step time: 0.1096\n",
      "63/223, train_loss: 0.1260, step time: 0.1016\n",
      "64/223, train_loss: 0.1218, step time: 0.1004\n",
      "65/223, train_loss: 0.1607, step time: 0.1122\n",
      "66/223, train_loss: 0.1230, step time: 0.1002\n",
      "67/223, train_loss: 0.1363, step time: 0.1119\n",
      "68/223, train_loss: 0.1261, step time: 0.0988\n",
      "69/223, train_loss: 0.1325, step time: 0.1288\n",
      "70/223, train_loss: 0.1354, step time: 0.1005\n",
      "71/223, train_loss: 0.1243, step time: 0.1008\n",
      "72/223, train_loss: 0.1150, step time: 0.1026\n",
      "73/223, train_loss: 0.1542, step time: 0.1093\n",
      "74/223, train_loss: 0.1203, step time: 0.1128\n",
      "75/223, train_loss: 0.1295, step time: 0.0996\n",
      "76/223, train_loss: 0.1373, step time: 0.1006\n",
      "77/223, train_loss: 0.1301, step time: 0.1109\n",
      "78/223, train_loss: 0.1339, step time: 0.1057\n",
      "79/223, train_loss: 0.1346, step time: 0.1282\n",
      "80/223, train_loss: 0.1513, step time: 0.1022\n",
      "81/223, train_loss: 0.1329, step time: 0.1009\n",
      "82/223, train_loss: 0.1254, step time: 0.1086\n",
      "83/223, train_loss: 0.1254, step time: 0.1053\n",
      "84/223, train_loss: 0.1282, step time: 0.1144\n",
      "85/223, train_loss: 0.1303, step time: 0.1253\n",
      "86/223, train_loss: 0.1341, step time: 0.1140\n",
      "87/223, train_loss: 0.1509, step time: 0.1007\n",
      "88/223, train_loss: 0.1267, step time: 0.1153\n",
      "89/223, train_loss: 0.1237, step time: 0.1279\n",
      "90/223, train_loss: 0.1419, step time: 0.1210\n",
      "91/223, train_loss: 0.1226, step time: 0.1032\n",
      "92/223, train_loss: 0.1156, step time: 0.1180\n",
      "93/223, train_loss: 0.1498, step time: 0.1180\n",
      "94/223, train_loss: 0.1312, step time: 0.1213\n",
      "95/223, train_loss: 0.1389, step time: 0.1137\n",
      "96/223, train_loss: 0.1225, step time: 0.1041\n",
      "97/223, train_loss: 0.1350, step time: 0.0992\n",
      "98/223, train_loss: 0.1480, step time: 0.1207\n",
      "99/223, train_loss: 0.1331, step time: 0.1153\n",
      "100/223, train_loss: 0.1306, step time: 0.1299\n",
      "101/223, train_loss: 0.1467, step time: 0.1035\n",
      "102/223, train_loss: 0.1351, step time: 0.1325\n",
      "103/223, train_loss: 0.1222, step time: 0.1044\n",
      "104/223, train_loss: 0.1442, step time: 0.1146\n",
      "105/223, train_loss: 0.1336, step time: 0.1116\n",
      "106/223, train_loss: 0.1333, step time: 0.1144\n",
      "107/223, train_loss: 0.1415, step time: 0.1094\n",
      "108/223, train_loss: 0.1452, step time: 0.1123\n",
      "109/223, train_loss: 0.1221, step time: 0.1049\n",
      "110/223, train_loss: 0.1588, step time: 0.1231\n",
      "111/223, train_loss: 0.1300, step time: 0.1175\n",
      "112/223, train_loss: 0.1234, step time: 0.1253\n",
      "113/223, train_loss: 0.1480, step time: 0.1038\n",
      "114/223, train_loss: 0.1340, step time: 0.1033\n",
      "115/223, train_loss: 0.1303, step time: 0.1017\n",
      "116/223, train_loss: 0.1188, step time: 0.1253\n",
      "117/223, train_loss: 0.1306, step time: 0.1057\n",
      "118/223, train_loss: 0.1275, step time: 0.1230\n",
      "119/223, train_loss: 0.1299, step time: 0.1002\n",
      "120/223, train_loss: 0.1391, step time: 0.1004\n",
      "121/223, train_loss: 0.1373, step time: 0.1157\n",
      "122/223, train_loss: 0.1292, step time: 0.1126\n",
      "123/223, train_loss: 0.1254, step time: 0.1072\n",
      "124/223, train_loss: 0.1406, step time: 0.1163\n",
      "125/223, train_loss: 0.1231, step time: 0.1186\n",
      "126/223, train_loss: 0.1282, step time: 0.1001\n",
      "127/223, train_loss: 0.1414, step time: 0.1008\n",
      "128/223, train_loss: 0.1234, step time: 0.1144\n",
      "129/223, train_loss: 0.1282, step time: 0.1104\n",
      "130/223, train_loss: 0.1295, step time: 0.1061\n",
      "131/223, train_loss: 0.1230, step time: 0.1029\n",
      "132/223, train_loss: 0.1270, step time: 0.1031\n",
      "133/223, train_loss: 0.1312, step time: 0.1146\n",
      "134/223, train_loss: 0.1208, step time: 0.1113\n",
      "135/223, train_loss: 0.1261, step time: 0.1000\n",
      "136/223, train_loss: 0.1440, step time: 0.1025\n",
      "137/223, train_loss: 0.1395, step time: 0.1133\n",
      "138/223, train_loss: 0.1342, step time: 0.1192\n",
      "139/223, train_loss: 0.1243, step time: 0.0995\n",
      "140/223, train_loss: 0.1471, step time: 0.1010\n",
      "141/223, train_loss: 0.1571, step time: 0.0998\n",
      "142/223, train_loss: 0.1288, step time: 0.1002\n",
      "143/223, train_loss: 0.1373, step time: 0.1084\n",
      "144/223, train_loss: 0.1552, step time: 0.1336\n",
      "145/223, train_loss: 0.1417, step time: 0.1379\n",
      "146/223, train_loss: 0.1330, step time: 0.1015\n",
      "147/223, train_loss: 0.1336, step time: 0.1000\n",
      "148/223, train_loss: 0.1359, step time: 0.1003\n",
      "149/223, train_loss: 0.1289, step time: 0.1128\n",
      "150/223, train_loss: 0.1340, step time: 0.1136\n",
      "151/223, train_loss: 0.1273, step time: 0.0991\n",
      "152/223, train_loss: 0.1175, step time: 0.1001\n",
      "153/223, train_loss: 0.1393, step time: 0.0997\n",
      "154/223, train_loss: 0.1371, step time: 0.1075\n",
      "155/223, train_loss: 0.1225, step time: 0.1139\n",
      "156/223, train_loss: 0.1127, step time: 0.1080\n",
      "157/223, train_loss: 0.1192, step time: 0.1004\n",
      "158/223, train_loss: 0.1314, step time: 0.1009\n",
      "159/223, train_loss: 0.3199, step time: 0.1328\n",
      "160/223, train_loss: 0.1414, step time: 0.1195\n",
      "161/223, train_loss: 0.1317, step time: 0.1080\n",
      "162/223, train_loss: 0.1270, step time: 0.1088\n",
      "163/223, train_loss: 0.1411, step time: 0.1097\n",
      "164/223, train_loss: 0.1366, step time: 0.1040\n",
      "165/223, train_loss: 0.1393, step time: 0.1009\n",
      "166/223, train_loss: 0.1523, step time: 0.1006\n",
      "167/223, train_loss: 0.1367, step time: 0.1008\n",
      "168/223, train_loss: 0.1373, step time: 0.1541\n",
      "169/223, train_loss: 0.1522, step time: 0.0994\n",
      "170/223, train_loss: 0.1369, step time: 0.1159\n",
      "171/223, train_loss: 0.1287, step time: 0.1118\n",
      "172/223, train_loss: 0.1280, step time: 0.1296\n",
      "173/223, train_loss: 0.1227, step time: 0.1032\n",
      "174/223, train_loss: 0.1370, step time: 0.1093\n",
      "175/223, train_loss: 0.1252, step time: 0.1327\n",
      "176/223, train_loss: 0.1205, step time: 0.1004\n",
      "177/223, train_loss: 0.1232, step time: 0.1050\n",
      "178/223, train_loss: 0.1172, step time: 0.1065\n",
      "179/223, train_loss: 0.1287, step time: 0.1246\n",
      "180/223, train_loss: 0.1345, step time: 0.0999\n",
      "181/223, train_loss: 0.1251, step time: 0.1152\n",
      "182/223, train_loss: 0.1339, step time: 0.1006\n",
      "183/223, train_loss: 0.1245, step time: 0.1002\n",
      "184/223, train_loss: 0.1299, step time: 0.1077\n",
      "185/223, train_loss: 0.1359, step time: 0.1075\n",
      "186/223, train_loss: 0.1295, step time: 0.1135\n",
      "187/223, train_loss: 0.1205, step time: 0.1216\n",
      "188/223, train_loss: 0.1217, step time: 0.1283\n",
      "189/223, train_loss: 0.1409, step time: 0.1145\n",
      "190/223, train_loss: 0.1357, step time: 0.1177\n",
      "191/223, train_loss: 0.1321, step time: 0.1200\n",
      "192/223, train_loss: 0.1124, step time: 0.1110\n",
      "193/223, train_loss: 0.1342, step time: 0.1043\n",
      "194/223, train_loss: 0.1212, step time: 0.1080\n",
      "195/223, train_loss: 0.1340, step time: 0.1012\n",
      "196/223, train_loss: 0.1337, step time: 0.0999\n",
      "197/223, train_loss: 0.1227, step time: 0.1068\n",
      "198/223, train_loss: 0.1303, step time: 0.1010\n",
      "199/223, train_loss: 0.1218, step time: 0.1155\n",
      "200/223, train_loss: 0.1313, step time: 0.1176\n",
      "201/223, train_loss: 0.1547, step time: 0.1107\n",
      "202/223, train_loss: 0.1283, step time: 0.1209\n",
      "203/223, train_loss: 0.1418, step time: 0.0995\n",
      "204/223, train_loss: 0.1317, step time: 0.1011\n",
      "205/223, train_loss: 0.1360, step time: 0.1279\n",
      "206/223, train_loss: 0.1369, step time: 0.1226\n",
      "207/223, train_loss: 0.1328, step time: 0.1208\n",
      "208/223, train_loss: 0.1460, step time: 0.1223\n",
      "209/223, train_loss: 0.1330, step time: 0.1008\n",
      "210/223, train_loss: 0.1257, step time: 0.1007\n",
      "211/223, train_loss: 0.1279, step time: 0.1172\n",
      "212/223, train_loss: 0.1342, step time: 0.1040\n",
      "213/223, train_loss: 0.1318, step time: 0.1121\n",
      "214/223, train_loss: 0.1269, step time: 0.0989\n",
      "215/223, train_loss: 0.1216, step time: 0.1263\n",
      "216/223, train_loss: 0.1295, step time: 0.1177\n",
      "217/223, train_loss: 0.1174, step time: 0.1052\n",
      "218/223, train_loss: 0.1373, step time: 0.0992\n",
      "219/223, train_loss: 0.1359, step time: 0.0996\n",
      "220/223, train_loss: 0.1365, step time: 0.1044\n",
      "221/223, train_loss: 0.1325, step time: 0.0999\n",
      "222/223, train_loss: 0.1341, step time: 0.1003\n",
      "223/223, train_loss: 0.1313, step time: 0.0998\n",
      "epoch 45 average loss: 0.1331\n",
      "saved new best metric model\n",
      "current epoch: 45 current mean dice: 0.8323 tc: 0.9092 wt: 0.8463 et: 0.7415\n",
      "best mean dice: 0.8323 at epoch: 45\n",
      "time consuming of epoch 45 is: 89.3293\n",
      "----------\n",
      "epoch 46/300\n",
      "1/223, train_loss: 0.1373, step time: 0.1066\n",
      "2/223, train_loss: 0.1212, step time: 0.1002\n",
      "3/223, train_loss: 0.1403, step time: 0.1102\n",
      "4/223, train_loss: 0.1316, step time: 0.1130\n",
      "5/223, train_loss: 0.1572, step time: 0.1001\n",
      "6/223, train_loss: 0.1315, step time: 0.1070\n",
      "7/223, train_loss: 0.1258, step time: 0.1254\n",
      "8/223, train_loss: 0.1304, step time: 0.0999\n",
      "9/223, train_loss: 0.1387, step time: 0.1130\n",
      "10/223, train_loss: 0.1299, step time: 0.1177\n",
      "11/223, train_loss: 0.1234, step time: 0.1001\n",
      "12/223, train_loss: 0.1307, step time: 0.1002\n",
      "13/223, train_loss: 0.1259, step time: 0.1193\n",
      "14/223, train_loss: 0.1225, step time: 0.1087\n",
      "15/223, train_loss: 0.1540, step time: 0.1185\n",
      "16/223, train_loss: 0.1312, step time: 0.1191\n",
      "17/223, train_loss: 0.1331, step time: 0.1160\n",
      "18/223, train_loss: 0.1316, step time: 0.1125\n",
      "19/223, train_loss: 0.1298, step time: 0.1063\n",
      "20/223, train_loss: 0.1335, step time: 0.0998\n",
      "21/223, train_loss: 0.1273, step time: 0.1026\n",
      "22/223, train_loss: 0.1438, step time: 0.1283\n",
      "23/223, train_loss: 0.1189, step time: 0.1233\n",
      "24/223, train_loss: 0.1211, step time: 0.1001\n",
      "25/223, train_loss: 0.1300, step time: 0.1165\n",
      "26/223, train_loss: 0.1287, step time: 0.1036\n",
      "27/223, train_loss: 0.1273, step time: 0.1126\n",
      "28/223, train_loss: 0.1236, step time: 0.1152\n",
      "29/223, train_loss: 0.1258, step time: 0.1050\n",
      "30/223, train_loss: 0.1332, step time: 0.1010\n",
      "31/223, train_loss: 0.1305, step time: 0.1049\n",
      "32/223, train_loss: 0.1305, step time: 0.1203\n",
      "33/223, train_loss: 0.1332, step time: 0.1081\n",
      "34/223, train_loss: 0.1281, step time: 0.1224\n",
      "35/223, train_loss: 0.1295, step time: 0.1071\n",
      "36/223, train_loss: 0.1260, step time: 0.1092\n",
      "37/223, train_loss: 0.1295, step time: 0.1000\n",
      "38/223, train_loss: 0.1302, step time: 0.1083\n",
      "39/223, train_loss: 0.1308, step time: 0.1002\n",
      "40/223, train_loss: 0.1292, step time: 0.1011\n",
      "41/223, train_loss: 0.1367, step time: 0.1240\n",
      "42/223, train_loss: 0.1331, step time: 0.1020\n",
      "43/223, train_loss: 0.1384, step time: 0.1011\n",
      "44/223, train_loss: 0.1484, step time: 0.1007\n",
      "45/223, train_loss: 0.1301, step time: 0.1018\n",
      "46/223, train_loss: 0.1295, step time: 0.0996\n",
      "47/223, train_loss: 0.1400, step time: 0.1004\n",
      "48/223, train_loss: 0.1221, step time: 0.1054\n",
      "49/223, train_loss: 0.1425, step time: 0.1332\n",
      "50/223, train_loss: 0.1341, step time: 0.1003\n",
      "51/223, train_loss: 0.1219, step time: 0.0992\n",
      "52/223, train_loss: 0.1132, step time: 0.1144\n",
      "53/223, train_loss: 0.1377, step time: 0.1056\n",
      "54/223, train_loss: 0.1242, step time: 0.0999\n",
      "55/223, train_loss: 0.1201, step time: 0.1004\n",
      "56/223, train_loss: 0.1322, step time: 0.1007\n",
      "57/223, train_loss: 0.1321, step time: 0.1123\n",
      "58/223, train_loss: 0.1263, step time: 0.1058\n",
      "59/223, train_loss: 0.1194, step time: 0.0998\n",
      "60/223, train_loss: 0.1543, step time: 0.1033\n",
      "61/223, train_loss: 0.1413, step time: 0.1133\n",
      "62/223, train_loss: 0.1310, step time: 0.1103\n",
      "63/223, train_loss: 0.1250, step time: 0.1131\n",
      "64/223, train_loss: 0.1317, step time: 0.1000\n",
      "65/223, train_loss: 0.1294, step time: 0.1069\n",
      "66/223, train_loss: 0.1389, step time: 0.1053\n",
      "67/223, train_loss: 0.1262, step time: 0.1055\n",
      "68/223, train_loss: 0.1359, step time: 0.1059\n",
      "69/223, train_loss: 0.1501, step time: 0.1010\n",
      "70/223, train_loss: 0.1223, step time: 0.1145\n",
      "71/223, train_loss: 0.1307, step time: 0.1006\n",
      "72/223, train_loss: 0.1180, step time: 0.1005\n",
      "73/223, train_loss: 0.1322, step time: 0.1129\n",
      "74/223, train_loss: 0.1205, step time: 0.1002\n",
      "75/223, train_loss: 0.1347, step time: 0.1000\n",
      "76/223, train_loss: 0.1435, step time: 0.1141\n",
      "77/223, train_loss: 0.1372, step time: 0.1132\n",
      "78/223, train_loss: 0.1210, step time: 0.1019\n",
      "79/223, train_loss: 0.1190, step time: 0.1004\n",
      "80/223, train_loss: 0.1252, step time: 0.1017\n",
      "81/223, train_loss: 0.1209, step time: 0.1146\n",
      "82/223, train_loss: 0.1377, step time: 0.1005\n",
      "83/223, train_loss: 0.1318, step time: 0.0997\n",
      "84/223, train_loss: 0.1177, step time: 0.1294\n",
      "85/223, train_loss: 0.1282, step time: 0.1065\n",
      "86/223, train_loss: 0.1218, step time: 0.1131\n",
      "87/223, train_loss: 0.1407, step time: 0.1166\n",
      "88/223, train_loss: 0.1377, step time: 0.1112\n",
      "89/223, train_loss: 0.1322, step time: 0.1138\n",
      "90/223, train_loss: 0.1227, step time: 0.1097\n",
      "91/223, train_loss: 0.1265, step time: 0.1003\n",
      "92/223, train_loss: 0.1315, step time: 0.1074\n",
      "93/223, train_loss: 0.1212, step time: 0.1167\n",
      "94/223, train_loss: 0.1262, step time: 0.1088\n",
      "95/223, train_loss: 0.1473, step time: 0.1001\n",
      "96/223, train_loss: 0.1480, step time: 0.0992\n",
      "97/223, train_loss: 0.1463, step time: 0.1159\n",
      "98/223, train_loss: 0.1204, step time: 0.1627\n",
      "99/223, train_loss: 0.1303, step time: 0.1237\n",
      "100/223, train_loss: 0.1470, step time: 0.1005\n",
      "101/223, train_loss: 0.1372, step time: 0.0994\n",
      "102/223, train_loss: 0.1310, step time: 0.0986\n",
      "103/223, train_loss: 0.1164, step time: 0.0994\n",
      "104/223, train_loss: 0.1248, step time: 0.1034\n",
      "105/223, train_loss: 0.1317, step time: 0.0996\n",
      "106/223, train_loss: 0.1231, step time: 0.0995\n",
      "107/223, train_loss: 0.1238, step time: 0.0997\n",
      "108/223, train_loss: 0.1585, step time: 0.1004\n",
      "109/223, train_loss: 0.1330, step time: 0.0995\n",
      "110/223, train_loss: 0.1181, step time: 0.1001\n",
      "111/223, train_loss: 0.1148, step time: 0.0999\n",
      "112/223, train_loss: 0.1235, step time: 0.1003\n",
      "113/223, train_loss: 0.1355, step time: 0.0996\n",
      "114/223, train_loss: 0.1344, step time: 0.0994\n",
      "115/223, train_loss: 0.1433, step time: 0.1001\n",
      "116/223, train_loss: 0.1289, step time: 0.1213\n",
      "117/223, train_loss: 0.1222, step time: 0.0985\n",
      "118/223, train_loss: 0.1318, step time: 0.0999\n",
      "119/223, train_loss: 0.1263, step time: 0.1001\n",
      "120/223, train_loss: 0.1357, step time: 0.1107\n",
      "121/223, train_loss: 0.1199, step time: 0.0987\n",
      "122/223, train_loss: 0.1184, step time: 0.0999\n",
      "123/223, train_loss: 0.1334, step time: 0.0991\n",
      "124/223, train_loss: 0.1278, step time: 0.1005\n",
      "125/223, train_loss: 0.1437, step time: 0.1004\n",
      "126/223, train_loss: 0.1413, step time: 0.1001\n",
      "127/223, train_loss: 0.1265, step time: 0.0994\n",
      "128/223, train_loss: 0.1357, step time: 0.1081\n",
      "129/223, train_loss: 0.1294, step time: 0.1060\n",
      "130/223, train_loss: 0.1462, step time: 0.1037\n",
      "131/223, train_loss: 0.1357, step time: 0.1530\n",
      "132/223, train_loss: 0.1367, step time: 0.1008\n",
      "133/223, train_loss: 0.1358, step time: 0.1088\n",
      "134/223, train_loss: 0.1440, step time: 0.1007\n",
      "135/223, train_loss: 0.1307, step time: 0.1009\n",
      "136/223, train_loss: 0.1322, step time: 0.1011\n",
      "137/223, train_loss: 0.1312, step time: 0.1102\n",
      "138/223, train_loss: 0.1286, step time: 0.1009\n",
      "139/223, train_loss: 0.1232, step time: 0.1147\n",
      "140/223, train_loss: 0.1239, step time: 0.1002\n",
      "141/223, train_loss: 0.1334, step time: 0.1191\n",
      "142/223, train_loss: 0.1270, step time: 0.1030\n",
      "143/223, train_loss: 0.1495, step time: 0.1023\n",
      "144/223, train_loss: 0.1127, step time: 0.1013\n",
      "145/223, train_loss: 0.1206, step time: 0.1062\n",
      "146/223, train_loss: 0.1226, step time: 0.1086\n",
      "147/223, train_loss: 0.1194, step time: 0.1289\n",
      "148/223, train_loss: 0.1356, step time: 0.1160\n",
      "149/223, train_loss: 0.1214, step time: 0.1031\n",
      "150/223, train_loss: 0.1473, step time: 0.1002\n",
      "151/223, train_loss: 0.1207, step time: 0.1012\n",
      "152/223, train_loss: 0.1305, step time: 0.1033\n",
      "153/223, train_loss: 0.1494, step time: 0.1098\n",
      "154/223, train_loss: 0.1292, step time: 0.1016\n",
      "155/223, train_loss: 0.1418, step time: 0.1005\n",
      "156/223, train_loss: 0.1453, step time: 0.1044\n",
      "157/223, train_loss: 0.1410, step time: 0.1129\n",
      "158/223, train_loss: 0.1333, step time: 0.1076\n",
      "159/223, train_loss: 0.1218, step time: 0.1040\n",
      "160/223, train_loss: 0.1288, step time: 0.1009\n",
      "161/223, train_loss: 0.1279, step time: 0.1000\n",
      "162/223, train_loss: 0.1432, step time: 0.1003\n",
      "163/223, train_loss: 0.1320, step time: 0.1014\n",
      "164/223, train_loss: 0.1373, step time: 0.1135\n",
      "165/223, train_loss: 0.1218, step time: 0.1136\n",
      "166/223, train_loss: 0.1300, step time: 0.1038\n",
      "167/223, train_loss: 0.1365, step time: 0.0997\n",
      "168/223, train_loss: 0.1463, step time: 0.1006\n",
      "169/223, train_loss: 0.1429, step time: 0.1092\n",
      "170/223, train_loss: 0.1381, step time: 0.1099\n",
      "171/223, train_loss: 0.1401, step time: 0.1325\n",
      "172/223, train_loss: 0.1290, step time: 0.1165\n",
      "173/223, train_loss: 0.1252, step time: 0.1106\n",
      "174/223, train_loss: 0.1203, step time: 0.1121\n",
      "175/223, train_loss: 0.1415, step time: 0.1003\n",
      "176/223, train_loss: 0.1422, step time: 0.1075\n",
      "177/223, train_loss: 0.1364, step time: 0.0985\n",
      "178/223, train_loss: 0.1381, step time: 0.0990\n",
      "179/223, train_loss: 0.1194, step time: 0.0986\n",
      "180/223, train_loss: 0.1176, step time: 0.1134\n",
      "181/223, train_loss: 0.1404, step time: 0.0999\n",
      "182/223, train_loss: 0.1238, step time: 0.0996\n",
      "183/223, train_loss: 0.1294, step time: 0.0993\n",
      "184/223, train_loss: 0.1331, step time: 0.1097\n",
      "185/223, train_loss: 0.1385, step time: 0.1153\n",
      "186/223, train_loss: 0.1270, step time: 0.1147\n",
      "187/223, train_loss: 0.1227, step time: 0.1312\n",
      "188/223, train_loss: 0.1456, step time: 0.1004\n",
      "189/223, train_loss: 0.1512, step time: 0.1052\n",
      "190/223, train_loss: 0.1442, step time: 0.1008\n",
      "191/223, train_loss: 0.1418, step time: 0.1003\n",
      "192/223, train_loss: 0.1297, step time: 0.1014\n",
      "193/223, train_loss: 0.1276, step time: 0.1495\n",
      "194/223, train_loss: 0.1324, step time: 0.1097\n",
      "195/223, train_loss: 0.1379, step time: 0.0998\n",
      "196/223, train_loss: 0.1301, step time: 0.1009\n",
      "197/223, train_loss: 0.1429, step time: 0.1346\n",
      "198/223, train_loss: 0.1286, step time: 0.1033\n",
      "199/223, train_loss: 0.1371, step time: 0.0999\n",
      "200/223, train_loss: 0.1496, step time: 0.1006\n",
      "201/223, train_loss: 0.3229, step time: 0.1161\n",
      "202/223, train_loss: 0.1505, step time: 0.1112\n",
      "203/223, train_loss: 0.1299, step time: 0.0992\n",
      "204/223, train_loss: 0.1329, step time: 0.1069\n",
      "205/223, train_loss: 0.1176, step time: 0.1002\n",
      "206/223, train_loss: 0.1333, step time: 0.1229\n",
      "207/223, train_loss: 0.1203, step time: 0.1004\n",
      "208/223, train_loss: 0.1314, step time: 0.1005\n",
      "209/223, train_loss: 0.1351, step time: 0.1071\n",
      "210/223, train_loss: 0.1424, step time: 0.1047\n",
      "211/223, train_loss: 0.1270, step time: 0.0996\n",
      "212/223, train_loss: 0.1195, step time: 0.1007\n",
      "213/223, train_loss: 0.1323, step time: 0.1170\n",
      "214/223, train_loss: 0.1307, step time: 0.1000\n",
      "215/223, train_loss: 0.1406, step time: 0.1008\n",
      "216/223, train_loss: 0.1253, step time: 0.1011\n",
      "217/223, train_loss: 0.1264, step time: 0.1068\n",
      "218/223, train_loss: 0.1371, step time: 0.0997\n",
      "219/223, train_loss: 0.1179, step time: 0.1005\n",
      "220/223, train_loss: 0.1355, step time: 0.1004\n",
      "221/223, train_loss: 0.1329, step time: 0.0992\n",
      "222/223, train_loss: 0.1267, step time: 0.0997\n",
      "223/223, train_loss: 0.1464, step time: 0.0999\n",
      "epoch 46 average loss: 0.1327\n",
      "time consuming of epoch 46 is: 94.3238\n",
      "----------\n",
      "epoch 47/300\n",
      "1/223, train_loss: 0.1211, step time: 0.1015\n",
      "2/223, train_loss: 0.1272, step time: 0.1000\n",
      "3/223, train_loss: 0.1339, step time: 0.1139\n",
      "4/223, train_loss: 0.1453, step time: 0.1216\n",
      "5/223, train_loss: 0.1106, step time: 0.1113\n",
      "6/223, train_loss: 0.1319, step time: 0.1006\n",
      "7/223, train_loss: 0.1252, step time: 0.1053\n",
      "8/223, train_loss: 0.1479, step time: 0.1214\n",
      "9/223, train_loss: 0.1308, step time: 0.1147\n",
      "10/223, train_loss: 0.1235, step time: 0.0999\n",
      "11/223, train_loss: 0.1267, step time: 0.1128\n",
      "12/223, train_loss: 0.1333, step time: 0.0994\n",
      "13/223, train_loss: 0.1189, step time: 0.0998\n",
      "14/223, train_loss: 0.1360, step time: 0.1008\n",
      "15/223, train_loss: 0.1360, step time: 0.1113\n",
      "16/223, train_loss: 0.1389, step time: 0.1017\n",
      "17/223, train_loss: 0.1229, step time: 0.1005\n",
      "18/223, train_loss: 0.1323, step time: 0.1121\n",
      "19/223, train_loss: 0.3166, step time: 0.1059\n",
      "20/223, train_loss: 0.1236, step time: 0.1004\n",
      "21/223, train_loss: 0.1413, step time: 0.1006\n",
      "22/223, train_loss: 0.1247, step time: 0.0987\n",
      "23/223, train_loss: 0.1280, step time: 0.0984\n",
      "24/223, train_loss: 0.1330, step time: 0.0988\n",
      "25/223, train_loss: 0.1358, step time: 0.1237\n",
      "26/223, train_loss: 0.1344, step time: 0.1001\n",
      "27/223, train_loss: 0.1356, step time: 0.1106\n",
      "28/223, train_loss: 0.1400, step time: 0.1080\n",
      "29/223, train_loss: 0.1246, step time: 0.1080\n",
      "30/223, train_loss: 0.1324, step time: 0.1070\n",
      "31/223, train_loss: 0.1213, step time: 0.1228\n",
      "32/223, train_loss: 0.1430, step time: 0.1005\n",
      "33/223, train_loss: 0.1422, step time: 0.1056\n",
      "34/223, train_loss: 0.1183, step time: 0.1008\n",
      "35/223, train_loss: 0.1433, step time: 0.0983\n",
      "36/223, train_loss: 0.1328, step time: 0.0999\n",
      "37/223, train_loss: 0.1436, step time: 0.1000\n",
      "38/223, train_loss: 0.1152, step time: 0.1043\n",
      "39/223, train_loss: 0.1566, step time: 0.1122\n",
      "40/223, train_loss: 0.1392, step time: 0.0986\n",
      "41/223, train_loss: 0.1252, step time: 0.1174\n",
      "42/223, train_loss: 0.1295, step time: 0.0999\n",
      "43/223, train_loss: 0.1211, step time: 0.1026\n",
      "44/223, train_loss: 0.1436, step time: 0.1093\n",
      "45/223, train_loss: 0.1205, step time: 0.1278\n",
      "46/223, train_loss: 0.1390, step time: 0.1082\n",
      "47/223, train_loss: 0.1173, step time: 0.1222\n",
      "48/223, train_loss: 0.1347, step time: 0.1020\n",
      "49/223, train_loss: 0.1307, step time: 0.1094\n",
      "50/223, train_loss: 0.1372, step time: 0.1061\n",
      "51/223, train_loss: 0.1243, step time: 0.1174\n",
      "52/223, train_loss: 0.1451, step time: 0.1082\n",
      "53/223, train_loss: 0.1279, step time: 0.1120\n",
      "54/223, train_loss: 0.1360, step time: 0.1002\n",
      "55/223, train_loss: 0.1212, step time: 0.1258\n",
      "56/223, train_loss: 0.1529, step time: 0.1145\n",
      "57/223, train_loss: 0.1425, step time: 0.1130\n",
      "58/223, train_loss: 0.1297, step time: 0.1170\n",
      "59/223, train_loss: 0.1353, step time: 0.1164\n",
      "60/223, train_loss: 0.1213, step time: 0.1199\n",
      "61/223, train_loss: 0.1311, step time: 0.1125\n",
      "62/223, train_loss: 0.1156, step time: 0.1089\n",
      "63/223, train_loss: 0.1247, step time: 0.1053\n",
      "64/223, train_loss: 0.1479, step time: 0.1102\n",
      "65/223, train_loss: 0.1490, step time: 0.0993\n",
      "66/223, train_loss: 0.1302, step time: 0.1173\n",
      "67/223, train_loss: 0.1222, step time: 0.1004\n",
      "68/223, train_loss: 0.1354, step time: 0.1104\n",
      "69/223, train_loss: 0.1211, step time: 0.1147\n",
      "70/223, train_loss: 0.1230, step time: 0.1002\n",
      "71/223, train_loss: 0.1409, step time: 0.0997\n",
      "72/223, train_loss: 0.1222, step time: 0.1004\n",
      "73/223, train_loss: 0.1314, step time: 0.1168\n",
      "74/223, train_loss: 0.1416, step time: 0.1321\n",
      "75/223, train_loss: 0.1313, step time: 0.1021\n",
      "76/223, train_loss: 0.1353, step time: 0.1102\n",
      "77/223, train_loss: 0.1358, step time: 0.1097\n",
      "78/223, train_loss: 0.1322, step time: 0.1002\n",
      "79/223, train_loss: 0.1284, step time: 0.1139\n",
      "80/223, train_loss: 0.1373, step time: 0.1005\n",
      "81/223, train_loss: 0.1336, step time: 0.1002\n",
      "82/223, train_loss: 0.1362, step time: 0.1129\n",
      "83/223, train_loss: 0.1376, step time: 0.1106\n",
      "84/223, train_loss: 0.1296, step time: 0.1053\n",
      "85/223, train_loss: 0.1256, step time: 0.0995\n",
      "86/223, train_loss: 0.1213, step time: 0.1214\n",
      "87/223, train_loss: 0.1301, step time: 0.1385\n",
      "88/223, train_loss: 0.1307, step time: 0.1001\n",
      "89/223, train_loss: 0.1317, step time: 0.1143\n",
      "90/223, train_loss: 0.1273, step time: 0.1041\n",
      "91/223, train_loss: 0.1281, step time: 0.0996\n",
      "92/223, train_loss: 0.1413, step time: 0.1003\n",
      "93/223, train_loss: 0.1222, step time: 0.1254\n",
      "94/223, train_loss: 0.1293, step time: 0.1000\n",
      "95/223, train_loss: 0.1315, step time: 0.1001\n",
      "96/223, train_loss: 0.1394, step time: 0.1088\n",
      "97/223, train_loss: 0.1257, step time: 0.1077\n",
      "98/223, train_loss: 0.1218, step time: 0.1004\n",
      "99/223, train_loss: 0.1388, step time: 0.0992\n",
      "100/223, train_loss: 0.1515, step time: 0.0998\n",
      "101/223, train_loss: 0.1295, step time: 0.1002\n",
      "102/223, train_loss: 0.1163, step time: 0.0996\n",
      "103/223, train_loss: 0.1239, step time: 0.0997\n",
      "104/223, train_loss: 0.1360, step time: 0.1005\n",
      "105/223, train_loss: 0.1286, step time: 0.1061\n",
      "106/223, train_loss: 0.1435, step time: 0.1005\n",
      "107/223, train_loss: 0.1354, step time: 0.1002\n",
      "108/223, train_loss: 0.1413, step time: 0.1005\n",
      "109/223, train_loss: 0.1462, step time: 0.1127\n",
      "110/223, train_loss: 0.1402, step time: 0.1082\n",
      "111/223, train_loss: 0.1276, step time: 0.1176\n",
      "112/223, train_loss: 0.1295, step time: 0.1005\n",
      "113/223, train_loss: 0.1439, step time: 0.1040\n",
      "114/223, train_loss: 0.1342, step time: 0.1001\n",
      "115/223, train_loss: 0.1185, step time: 0.1004\n",
      "116/223, train_loss: 0.1220, step time: 0.1143\n",
      "117/223, train_loss: 0.1456, step time: 0.1099\n",
      "118/223, train_loss: 0.1433, step time: 0.1047\n",
      "119/223, train_loss: 0.1236, step time: 0.1007\n",
      "120/223, train_loss: 0.1277, step time: 0.0990\n",
      "121/223, train_loss: 0.1183, step time: 0.0993\n",
      "122/223, train_loss: 0.1305, step time: 0.1004\n",
      "123/223, train_loss: 0.1291, step time: 0.0999\n",
      "124/223, train_loss: 0.1229, step time: 0.1014\n",
      "125/223, train_loss: 0.1345, step time: 0.1010\n",
      "126/223, train_loss: 0.1392, step time: 0.1002\n",
      "127/223, train_loss: 0.1231, step time: 0.1049\n",
      "128/223, train_loss: 0.1320, step time: 0.0996\n",
      "129/223, train_loss: 0.1473, step time: 0.1004\n",
      "130/223, train_loss: 0.1401, step time: 0.1000\n",
      "131/223, train_loss: 0.1163, step time: 0.0997\n",
      "132/223, train_loss: 0.1357, step time: 0.1006\n",
      "133/223, train_loss: 0.1328, step time: 0.1200\n",
      "134/223, train_loss: 0.1290, step time: 0.1316\n",
      "135/223, train_loss: 0.1344, step time: 0.1121\n",
      "136/223, train_loss: 0.1469, step time: 0.1032\n",
      "137/223, train_loss: 0.1292, step time: 0.1205\n",
      "138/223, train_loss: 0.1207, step time: 0.1185\n",
      "139/223, train_loss: 0.1350, step time: 0.1053\n",
      "140/223, train_loss: 0.1301, step time: 0.1057\n",
      "141/223, train_loss: 0.1366, step time: 0.1036\n",
      "142/223, train_loss: 0.1513, step time: 0.1103\n",
      "143/223, train_loss: 0.1384, step time: 0.0994\n",
      "144/223, train_loss: 0.1453, step time: 0.1110\n",
      "145/223, train_loss: 0.1384, step time: 0.1112\n",
      "146/223, train_loss: 0.1273, step time: 0.0996\n",
      "147/223, train_loss: 0.1232, step time: 0.1256\n",
      "148/223, train_loss: 0.1414, step time: 0.0997\n",
      "149/223, train_loss: 0.1293, step time: 0.0997\n",
      "150/223, train_loss: 0.1319, step time: 0.1002\n",
      "151/223, train_loss: 0.1319, step time: 0.1005\n",
      "152/223, train_loss: 0.1262, step time: 0.1008\n",
      "153/223, train_loss: 0.1234, step time: 0.1061\n",
      "154/223, train_loss: 0.1227, step time: 0.1030\n",
      "155/223, train_loss: 0.1224, step time: 0.1110\n",
      "156/223, train_loss: 0.1450, step time: 0.1012\n",
      "157/223, train_loss: 0.1323, step time: 0.1008\n",
      "158/223, train_loss: 0.1334, step time: 0.1010\n",
      "159/223, train_loss: 0.1333, step time: 0.1006\n",
      "160/223, train_loss: 0.1167, step time: 0.1002\n",
      "161/223, train_loss: 0.1250, step time: 0.1049\n",
      "162/223, train_loss: 0.1341, step time: 0.0996\n",
      "163/223, train_loss: 0.1307, step time: 0.1064\n",
      "164/223, train_loss: 0.1218, step time: 0.1189\n",
      "165/223, train_loss: 0.1296, step time: 0.1145\n",
      "166/223, train_loss: 0.1248, step time: 0.1167\n",
      "167/223, train_loss: 0.1242, step time: 0.1106\n",
      "168/223, train_loss: 0.1402, step time: 0.0997\n",
      "169/223, train_loss: 0.1386, step time: 0.1003\n",
      "170/223, train_loss: 0.1426, step time: 0.1130\n",
      "171/223, train_loss: 0.1210, step time: 0.1012\n",
      "172/223, train_loss: 0.1307, step time: 0.1104\n",
      "173/223, train_loss: 0.1310, step time: 0.1063\n",
      "174/223, train_loss: 0.1482, step time: 0.1072\n",
      "175/223, train_loss: 0.1370, step time: 0.1024\n",
      "176/223, train_loss: 0.1257, step time: 0.1072\n",
      "177/223, train_loss: 0.1365, step time: 0.0998\n",
      "178/223, train_loss: 0.1253, step time: 0.1003\n",
      "179/223, train_loss: 0.1332, step time: 0.1004\n",
      "180/223, train_loss: 0.1306, step time: 0.0998\n",
      "181/223, train_loss: 0.1163, step time: 0.1088\n",
      "182/223, train_loss: 0.1340, step time: 0.1126\n",
      "183/223, train_loss: 0.1471, step time: 0.0995\n",
      "184/223, train_loss: 0.1413, step time: 0.1080\n",
      "185/223, train_loss: 0.1412, step time: 0.1005\n",
      "186/223, train_loss: 0.1246, step time: 0.1048\n",
      "187/223, train_loss: 0.1369, step time: 0.1086\n",
      "188/223, train_loss: 0.1476, step time: 0.1000\n",
      "189/223, train_loss: 0.1305, step time: 0.1011\n",
      "190/223, train_loss: 0.1201, step time: 0.0997\n",
      "191/223, train_loss: 0.1417, step time: 0.1112\n",
      "192/223, train_loss: 0.1261, step time: 0.1056\n",
      "193/223, train_loss: 0.1451, step time: 0.1062\n",
      "194/223, train_loss: 0.1192, step time: 0.1003\n",
      "195/223, train_loss: 0.1350, step time: 0.1012\n",
      "196/223, train_loss: 0.1330, step time: 0.1091\n",
      "197/223, train_loss: 0.1272, step time: 0.1061\n",
      "198/223, train_loss: 0.1340, step time: 0.0996\n",
      "199/223, train_loss: 0.1143, step time: 0.1038\n",
      "200/223, train_loss: 0.1462, step time: 0.1024\n",
      "201/223, train_loss: 0.1342, step time: 0.0994\n",
      "202/223, train_loss: 0.1221, step time: 0.1004\n",
      "203/223, train_loss: 0.1403, step time: 0.1005\n",
      "204/223, train_loss: 0.1412, step time: 0.1003\n",
      "205/223, train_loss: 0.1185, step time: 0.0995\n",
      "206/223, train_loss: 0.1206, step time: 0.1152\n",
      "207/223, train_loss: 0.1370, step time: 0.1021\n",
      "208/223, train_loss: 0.1369, step time: 0.0998\n",
      "209/223, train_loss: 0.1165, step time: 0.1004\n",
      "210/223, train_loss: 0.1222, step time: 0.1117\n",
      "211/223, train_loss: 0.1256, step time: 0.1099\n",
      "212/223, train_loss: 0.1262, step time: 0.1129\n",
      "213/223, train_loss: 0.1289, step time: 0.0999\n",
      "214/223, train_loss: 0.1343, step time: 0.1071\n",
      "215/223, train_loss: 0.1344, step time: 0.1013\n",
      "216/223, train_loss: 0.1379, step time: 0.1006\n",
      "217/223, train_loss: 0.1218, step time: 0.0995\n",
      "218/223, train_loss: 0.1200, step time: 0.0994\n",
      "219/223, train_loss: 0.1256, step time: 0.1002\n",
      "220/223, train_loss: 0.1215, step time: 0.1001\n",
      "221/223, train_loss: 0.1217, step time: 0.1004\n",
      "222/223, train_loss: 0.1255, step time: 0.1005\n",
      "223/223, train_loss: 0.1430, step time: 0.0996\n",
      "epoch 47 average loss: 0.1325\n",
      "time consuming of epoch 47 is: 86.2744\n",
      "----------\n",
      "epoch 48/300\n",
      "1/223, train_loss: 0.1201, step time: 0.1019\n",
      "2/223, train_loss: 0.1244, step time: 0.0997\n",
      "3/223, train_loss: 0.1183, step time: 0.1012\n",
      "4/223, train_loss: 0.1293, step time: 0.1173\n",
      "5/223, train_loss: 0.1215, step time: 0.1009\n",
      "6/223, train_loss: 0.1182, step time: 0.0995\n",
      "7/223, train_loss: 0.1302, step time: 0.0997\n",
      "8/223, train_loss: 0.1304, step time: 0.1206\n",
      "9/223, train_loss: 0.1389, step time: 0.1003\n",
      "10/223, train_loss: 0.1280, step time: 0.1010\n",
      "11/223, train_loss: 0.1487, step time: 0.1042\n",
      "12/223, train_loss: 0.1275, step time: 0.1002\n",
      "13/223, train_loss: 0.1246, step time: 0.1018\n",
      "14/223, train_loss: 0.1399, step time: 0.1251\n",
      "15/223, train_loss: 0.1318, step time: 0.1150\n",
      "16/223, train_loss: 0.1421, step time: 0.1080\n",
      "17/223, train_loss: 0.1315, step time: 0.1058\n",
      "18/223, train_loss: 0.1245, step time: 0.1202\n",
      "19/223, train_loss: 0.1365, step time: 0.1110\n",
      "20/223, train_loss: 0.1154, step time: 0.1222\n",
      "21/223, train_loss: 0.1167, step time: 0.1082\n",
      "22/223, train_loss: 0.1324, step time: 0.1164\n",
      "23/223, train_loss: 0.1112, step time: 0.1295\n",
      "24/223, train_loss: 0.1412, step time: 0.1091\n",
      "25/223, train_loss: 0.1322, step time: 0.1181\n",
      "26/223, train_loss: 0.1244, step time: 0.1089\n",
      "27/223, train_loss: 0.1326, step time: 0.1263\n",
      "28/223, train_loss: 0.1225, step time: 0.1063\n",
      "29/223, train_loss: 0.1128, step time: 0.1177\n",
      "30/223, train_loss: 0.1323, step time: 0.1102\n",
      "31/223, train_loss: 0.1298, step time: 0.1017\n",
      "32/223, train_loss: 0.1224, step time: 0.1036\n",
      "33/223, train_loss: 0.1359, step time: 0.1135\n",
      "34/223, train_loss: 0.1239, step time: 0.1144\n",
      "35/223, train_loss: 0.1358, step time: 0.1002\n",
      "36/223, train_loss: 0.1297, step time: 0.1049\n",
      "37/223, train_loss: 0.1352, step time: 0.1044\n",
      "38/223, train_loss: 0.1373, step time: 0.1022\n",
      "39/223, train_loss: 0.1435, step time: 0.1020\n",
      "40/223, train_loss: 0.1321, step time: 0.1008\n",
      "41/223, train_loss: 0.1403, step time: 0.1066\n",
      "42/223, train_loss: 0.1345, step time: 0.1008\n",
      "43/223, train_loss: 0.1295, step time: 0.1003\n",
      "44/223, train_loss: 0.1249, step time: 0.0993\n",
      "45/223, train_loss: 0.1356, step time: 0.1140\n",
      "46/223, train_loss: 0.1197, step time: 0.0996\n",
      "47/223, train_loss: 0.1169, step time: 0.1025\n",
      "48/223, train_loss: 0.1297, step time: 0.1007\n",
      "49/223, train_loss: 0.1374, step time: 0.1003\n",
      "50/223, train_loss: 0.1226, step time: 0.0998\n",
      "51/223, train_loss: 0.1146, step time: 0.1013\n",
      "52/223, train_loss: 0.1482, step time: 0.1109\n",
      "53/223, train_loss: 0.1251, step time: 0.1003\n",
      "54/223, train_loss: 0.1330, step time: 0.1001\n",
      "55/223, train_loss: 0.1330, step time: 0.1005\n",
      "56/223, train_loss: 0.1309, step time: 0.1017\n",
      "57/223, train_loss: 0.1308, step time: 0.0997\n",
      "58/223, train_loss: 0.1360, step time: 0.1012\n",
      "59/223, train_loss: 0.1158, step time: 0.1004\n",
      "60/223, train_loss: 0.1393, step time: 0.1004\n",
      "61/223, train_loss: 0.1124, step time: 0.1147\n",
      "62/223, train_loss: 0.1357, step time: 0.1022\n",
      "63/223, train_loss: 0.1149, step time: 0.1001\n",
      "64/223, train_loss: 0.1210, step time: 0.1143\n",
      "65/223, train_loss: 0.1273, step time: 0.1059\n",
      "66/223, train_loss: 0.1304, step time: 0.1190\n",
      "67/223, train_loss: 0.1258, step time: 0.0997\n",
      "68/223, train_loss: 0.1328, step time: 0.1003\n",
      "69/223, train_loss: 0.1373, step time: 0.1057\n",
      "70/223, train_loss: 0.1267, step time: 0.1071\n",
      "71/223, train_loss: 0.1438, step time: 0.1003\n",
      "72/223, train_loss: 0.1329, step time: 0.1000\n",
      "73/223, train_loss: 0.1221, step time: 0.1240\n",
      "74/223, train_loss: 0.1526, step time: 0.0998\n",
      "75/223, train_loss: 0.1375, step time: 0.0998\n",
      "76/223, train_loss: 0.1345, step time: 0.1002\n",
      "77/223, train_loss: 0.1534, step time: 0.0996\n",
      "78/223, train_loss: 0.1244, step time: 0.1012\n",
      "79/223, train_loss: 0.1281, step time: 0.1007\n",
      "80/223, train_loss: 0.1423, step time: 0.1055\n",
      "81/223, train_loss: 0.1446, step time: 0.1036\n",
      "82/223, train_loss: 0.1153, step time: 0.0997\n",
      "83/223, train_loss: 0.1290, step time: 0.1009\n",
      "84/223, train_loss: 0.1240, step time: 0.1101\n",
      "85/223, train_loss: 0.1294, step time: 0.1080\n",
      "86/223, train_loss: 0.1372, step time: 0.1050\n",
      "87/223, train_loss: 0.1146, step time: 0.1042\n",
      "88/223, train_loss: 0.1129, step time: 0.1000\n",
      "89/223, train_loss: 0.1247, step time: 0.1017\n",
      "90/223, train_loss: 0.1230, step time: 0.1034\n",
      "91/223, train_loss: 0.1408, step time: 0.1176\n",
      "92/223, train_loss: 0.1167, step time: 0.1019\n",
      "93/223, train_loss: 0.1374, step time: 0.1275\n",
      "94/223, train_loss: 0.1298, step time: 0.1211\n",
      "95/223, train_loss: 0.1292, step time: 0.1047\n",
      "96/223, train_loss: 0.1119, step time: 0.1007\n",
      "97/223, train_loss: 0.1525, step time: 0.0999\n",
      "98/223, train_loss: 0.1397, step time: 0.1002\n",
      "99/223, train_loss: 0.1257, step time: 0.1005\n",
      "100/223, train_loss: 0.1177, step time: 0.1005\n",
      "101/223, train_loss: 0.1280, step time: 0.1016\n",
      "102/223, train_loss: 0.1209, step time: 0.1003\n",
      "103/223, train_loss: 0.1337, step time: 0.1004\n",
      "104/223, train_loss: 0.1334, step time: 0.1055\n",
      "105/223, train_loss: 0.1192, step time: 0.1127\n",
      "106/223, train_loss: 0.1267, step time: 0.1198\n",
      "107/223, train_loss: 0.1229, step time: 0.1229\n",
      "108/223, train_loss: 0.1384, step time: 0.1133\n",
      "109/223, train_loss: 0.1271, step time: 0.1045\n",
      "110/223, train_loss: 0.1590, step time: 0.1013\n",
      "111/223, train_loss: 0.1302, step time: 0.0999\n",
      "112/223, train_loss: 0.1467, step time: 0.1002\n",
      "113/223, train_loss: 0.1326, step time: 0.1348\n",
      "114/223, train_loss: 0.1430, step time: 0.1077\n",
      "115/223, train_loss: 0.1290, step time: 0.1003\n",
      "116/223, train_loss: 0.1336, step time: 0.1188\n",
      "117/223, train_loss: 0.1317, step time: 0.1169\n",
      "118/223, train_loss: 0.1257, step time: 0.1005\n",
      "119/223, train_loss: 0.1290, step time: 0.1004\n",
      "120/223, train_loss: 0.1319, step time: 0.0999\n",
      "121/223, train_loss: 0.1569, step time: 0.1063\n",
      "122/223, train_loss: 0.1390, step time: 0.0997\n",
      "123/223, train_loss: 0.1412, step time: 0.1006\n",
      "124/223, train_loss: 0.1385, step time: 0.1138\n",
      "125/223, train_loss: 0.1207, step time: 0.1224\n",
      "126/223, train_loss: 0.1207, step time: 0.1229\n",
      "127/223, train_loss: 0.1532, step time: 0.1135\n",
      "128/223, train_loss: 0.1300, step time: 0.1165\n",
      "129/223, train_loss: 0.1218, step time: 0.1129\n",
      "130/223, train_loss: 0.1365, step time: 0.1055\n",
      "131/223, train_loss: 0.1359, step time: 0.1318\n",
      "132/223, train_loss: 0.1260, step time: 0.1030\n",
      "133/223, train_loss: 0.1193, step time: 0.1167\n",
      "134/223, train_loss: 0.1202, step time: 0.1074\n",
      "135/223, train_loss: 0.1276, step time: 0.1058\n",
      "136/223, train_loss: 0.3185, step time: 0.1113\n",
      "137/223, train_loss: 0.1355, step time: 0.0997\n",
      "138/223, train_loss: 0.1265, step time: 0.1083\n",
      "139/223, train_loss: 0.1292, step time: 0.1182\n",
      "140/223, train_loss: 0.1316, step time: 0.1005\n",
      "141/223, train_loss: 0.1380, step time: 0.1005\n",
      "142/223, train_loss: 0.1335, step time: 0.0996\n",
      "143/223, train_loss: 0.1262, step time: 0.1129\n",
      "144/223, train_loss: 0.1486, step time: 0.1000\n",
      "145/223, train_loss: 0.1342, step time: 0.1115\n",
      "146/223, train_loss: 0.1455, step time: 0.1001\n",
      "147/223, train_loss: 0.1237, step time: 0.1004\n",
      "148/223, train_loss: 0.1364, step time: 0.1017\n",
      "149/223, train_loss: 0.1131, step time: 0.1003\n",
      "150/223, train_loss: 0.1282, step time: 0.1150\n",
      "151/223, train_loss: 0.1260, step time: 0.1089\n",
      "152/223, train_loss: 0.1338, step time: 0.1071\n",
      "153/223, train_loss: 0.1400, step time: 0.0999\n",
      "154/223, train_loss: 0.1415, step time: 0.1010\n",
      "155/223, train_loss: 0.1310, step time: 0.1005\n",
      "156/223, train_loss: 0.1346, step time: 0.1148\n",
      "157/223, train_loss: 0.1278, step time: 0.1136\n",
      "158/223, train_loss: 0.1411, step time: 0.1008\n",
      "159/223, train_loss: 0.1522, step time: 0.1000\n",
      "160/223, train_loss: 0.1519, step time: 0.1001\n",
      "161/223, train_loss: 0.1316, step time: 0.1299\n",
      "162/223, train_loss: 0.1325, step time: 0.1125\n",
      "163/223, train_loss: 0.1394, step time: 0.1192\n",
      "164/223, train_loss: 0.1272, step time: 0.1004\n",
      "165/223, train_loss: 0.1372, step time: 0.1018\n",
      "166/223, train_loss: 0.1299, step time: 0.1116\n",
      "167/223, train_loss: 0.1468, step time: 0.1366\n",
      "168/223, train_loss: 0.1257, step time: 0.1015\n",
      "169/223, train_loss: 0.1358, step time: 0.1089\n",
      "170/223, train_loss: 0.1274, step time: 0.1007\n",
      "171/223, train_loss: 0.1399, step time: 0.0996\n",
      "172/223, train_loss: 0.1329, step time: 0.1004\n",
      "173/223, train_loss: 0.1171, step time: 0.1140\n",
      "174/223, train_loss: 0.1475, step time: 0.1072\n",
      "175/223, train_loss: 0.1320, step time: 0.1145\n",
      "176/223, train_loss: 0.1329, step time: 0.1259\n",
      "177/223, train_loss: 0.1254, step time: 0.1003\n",
      "178/223, train_loss: 0.1264, step time: 0.1047\n",
      "179/223, train_loss: 0.1347, step time: 0.1141\n",
      "180/223, train_loss: 0.1410, step time: 0.1240\n",
      "181/223, train_loss: 0.1412, step time: 0.1330\n",
      "182/223, train_loss: 0.1274, step time: 0.1002\n",
      "183/223, train_loss: 0.1211, step time: 0.1005\n",
      "184/223, train_loss: 0.1277, step time: 0.1103\n",
      "185/223, train_loss: 0.1313, step time: 0.1161\n",
      "186/223, train_loss: 0.1389, step time: 0.1000\n",
      "187/223, train_loss: 0.1323, step time: 0.1016\n",
      "188/223, train_loss: 0.1289, step time: 0.1041\n",
      "189/223, train_loss: 0.1403, step time: 0.1138\n",
      "190/223, train_loss: 0.1247, step time: 0.1006\n",
      "191/223, train_loss: 0.1395, step time: 0.0997\n",
      "192/223, train_loss: 0.1102, step time: 0.1056\n",
      "193/223, train_loss: 0.1321, step time: 0.1117\n",
      "194/223, train_loss: 0.1248, step time: 0.1064\n",
      "195/223, train_loss: 0.1358, step time: 0.1180\n",
      "196/223, train_loss: 0.1182, step time: 0.1006\n",
      "197/223, train_loss: 0.1241, step time: 0.0996\n",
      "198/223, train_loss: 0.1242, step time: 0.1097\n",
      "199/223, train_loss: 0.1142, step time: 0.1012\n",
      "200/223, train_loss: 0.1406, step time: 0.1006\n",
      "201/223, train_loss: 0.1288, step time: 0.1448\n",
      "202/223, train_loss: 0.1380, step time: 0.1184\n",
      "203/223, train_loss: 0.1326, step time: 0.1091\n",
      "204/223, train_loss: 0.1246, step time: 0.1006\n",
      "205/223, train_loss: 0.1331, step time: 0.1175\n",
      "206/223, train_loss: 0.1227, step time: 0.0996\n",
      "207/223, train_loss: 0.1411, step time: 0.1131\n",
      "208/223, train_loss: 0.1261, step time: 0.1007\n",
      "209/223, train_loss: 0.1296, step time: 0.1234\n",
      "210/223, train_loss: 0.1365, step time: 0.1264\n",
      "211/223, train_loss: 0.1357, step time: 0.1049\n",
      "212/223, train_loss: 0.1356, step time: 0.1009\n",
      "213/223, train_loss: 0.1458, step time: 0.1153\n",
      "214/223, train_loss: 0.1290, step time: 0.1053\n",
      "215/223, train_loss: 0.1362, step time: 0.1009\n",
      "216/223, train_loss: 0.1537, step time: 0.1015\n",
      "217/223, train_loss: 0.1259, step time: 0.1002\n",
      "218/223, train_loss: 0.1416, step time: 0.1123\n",
      "219/223, train_loss: 0.1338, step time: 0.1444\n",
      "220/223, train_loss: 0.1105, step time: 0.0997\n",
      "221/223, train_loss: 0.1189, step time: 0.0989\n",
      "222/223, train_loss: 0.1337, step time: 0.0987\n",
      "223/223, train_loss: 0.1237, step time: 0.0991\n",
      "epoch 48 average loss: 0.1319\n",
      "time consuming of epoch 48 is: 87.7343\n",
      "----------\n",
      "epoch 49/300\n",
      "1/223, train_loss: 0.1213, step time: 0.1081\n",
      "2/223, train_loss: 0.1285, step time: 0.1178\n",
      "3/223, train_loss: 0.1228, step time: 0.1103\n",
      "4/223, train_loss: 0.1432, step time: 0.1059\n",
      "5/223, train_loss: 0.1364, step time: 0.1032\n",
      "6/223, train_loss: 0.1252, step time: 0.1167\n",
      "7/223, train_loss: 0.1347, step time: 0.1024\n",
      "8/223, train_loss: 0.1293, step time: 0.1170\n",
      "9/223, train_loss: 0.1319, step time: 0.1076\n",
      "10/223, train_loss: 0.1250, step time: 0.1178\n",
      "11/223, train_loss: 0.1353, step time: 0.1058\n",
      "12/223, train_loss: 0.1365, step time: 0.1180\n",
      "13/223, train_loss: 0.1349, step time: 0.1218\n",
      "14/223, train_loss: 0.1372, step time: 0.1240\n",
      "15/223, train_loss: 0.1268, step time: 0.0999\n",
      "16/223, train_loss: 0.1306, step time: 0.1057\n",
      "17/223, train_loss: 0.1356, step time: 0.0999\n",
      "18/223, train_loss: 0.1369, step time: 0.1061\n",
      "19/223, train_loss: 0.1422, step time: 0.1206\n",
      "20/223, train_loss: 0.1442, step time: 0.1117\n",
      "21/223, train_loss: 0.1349, step time: 0.0997\n",
      "22/223, train_loss: 0.1310, step time: 0.1155\n",
      "23/223, train_loss: 0.1334, step time: 0.1019\n",
      "24/223, train_loss: 0.1359, step time: 0.1016\n",
      "25/223, train_loss: 0.1262, step time: 0.1035\n",
      "26/223, train_loss: 0.1284, step time: 0.1035\n",
      "27/223, train_loss: 0.1202, step time: 0.1180\n",
      "28/223, train_loss: 0.1206, step time: 0.1079\n",
      "29/223, train_loss: 0.1312, step time: 0.1015\n",
      "30/223, train_loss: 0.1298, step time: 0.1073\n",
      "31/223, train_loss: 0.1196, step time: 0.1276\n",
      "32/223, train_loss: 0.1248, step time: 0.1056\n",
      "33/223, train_loss: 0.1143, step time: 0.1003\n",
      "34/223, train_loss: 0.1264, step time: 0.1035\n",
      "35/223, train_loss: 0.1214, step time: 0.1298\n",
      "36/223, train_loss: 0.1433, step time: 0.1010\n",
      "37/223, train_loss: 0.1267, step time: 0.1014\n",
      "38/223, train_loss: 0.1222, step time: 0.1100\n",
      "39/223, train_loss: 0.1245, step time: 0.1088\n",
      "40/223, train_loss: 0.1288, step time: 0.1047\n",
      "41/223, train_loss: 0.1285, step time: 0.1106\n",
      "42/223, train_loss: 0.1204, step time: 0.0989\n",
      "43/223, train_loss: 0.1487, step time: 0.1081\n",
      "44/223, train_loss: 0.1259, step time: 0.1018\n",
      "45/223, train_loss: 0.1273, step time: 0.0999\n",
      "46/223, train_loss: 0.1359, step time: 0.1005\n",
      "47/223, train_loss: 0.1200, step time: 0.1003\n",
      "48/223, train_loss: 0.1398, step time: 0.1012\n",
      "49/223, train_loss: 0.1137, step time: 0.1008\n",
      "50/223, train_loss: 0.1191, step time: 0.0997\n",
      "51/223, train_loss: 0.1348, step time: 0.1001\n",
      "52/223, train_loss: 0.1182, step time: 0.1002\n",
      "53/223, train_loss: 0.1350, step time: 0.1066\n",
      "54/223, train_loss: 0.1293, step time: 0.1159\n",
      "55/223, train_loss: 0.1473, step time: 0.1004\n",
      "56/223, train_loss: 0.1400, step time: 0.1004\n",
      "57/223, train_loss: 0.1228, step time: 0.1017\n",
      "58/223, train_loss: 0.1381, step time: 0.1195\n",
      "59/223, train_loss: 0.1270, step time: 0.1133\n",
      "60/223, train_loss: 0.1285, step time: 0.1012\n",
      "61/223, train_loss: 0.1488, step time: 0.1016\n",
      "62/223, train_loss: 0.1220, step time: 0.1303\n",
      "63/223, train_loss: 0.1372, step time: 0.1045\n",
      "64/223, train_loss: 0.1452, step time: 0.1053\n",
      "65/223, train_loss: 0.1429, step time: 0.1134\n",
      "66/223, train_loss: 0.1414, step time: 0.1064\n",
      "67/223, train_loss: 0.1543, step time: 0.1001\n",
      "68/223, train_loss: 0.1236, step time: 0.1221\n",
      "69/223, train_loss: 0.1311, step time: 0.0999\n",
      "70/223, train_loss: 0.1159, step time: 0.1030\n",
      "71/223, train_loss: 0.1402, step time: 0.0999\n",
      "72/223, train_loss: 0.1219, step time: 0.1017\n",
      "73/223, train_loss: 0.1336, step time: 0.1005\n",
      "74/223, train_loss: 0.1365, step time: 0.1198\n",
      "75/223, train_loss: 0.1233, step time: 0.1000\n",
      "76/223, train_loss: 0.1424, step time: 0.1005\n",
      "77/223, train_loss: 0.1203, step time: 0.1002\n",
      "78/223, train_loss: 0.1209, step time: 0.1105\n",
      "79/223, train_loss: 0.1060, step time: 0.0998\n",
      "80/223, train_loss: 0.1445, step time: 0.1064\n",
      "81/223, train_loss: 0.1380, step time: 0.1001\n",
      "82/223, train_loss: 0.1327, step time: 0.1164\n",
      "83/223, train_loss: 0.1298, step time: 0.1174\n",
      "84/223, train_loss: 0.1291, step time: 0.1132\n",
      "85/223, train_loss: 0.1227, step time: 0.1089\n",
      "86/223, train_loss: 0.1238, step time: 0.1158\n",
      "87/223, train_loss: 0.1277, step time: 0.1019\n",
      "88/223, train_loss: 0.1336, step time: 0.1004\n",
      "89/223, train_loss: 0.1295, step time: 0.1016\n",
      "90/223, train_loss: 0.1311, step time: 0.1178\n",
      "91/223, train_loss: 0.1252, step time: 0.1028\n",
      "92/223, train_loss: 0.1358, step time: 0.1157\n",
      "93/223, train_loss: 0.1274, step time: 0.1077\n",
      "94/223, train_loss: 0.1229, step time: 0.1096\n",
      "95/223, train_loss: 0.1208, step time: 0.1056\n",
      "96/223, train_loss: 0.1253, step time: 0.1005\n",
      "97/223, train_loss: 0.1217, step time: 0.1015\n",
      "98/223, train_loss: 0.1365, step time: 0.1072\n",
      "99/223, train_loss: 0.1222, step time: 0.1049\n",
      "100/223, train_loss: 0.1334, step time: 0.0996\n",
      "101/223, train_loss: 0.1394, step time: 0.1084\n",
      "102/223, train_loss: 0.1326, step time: 0.1123\n",
      "103/223, train_loss: 0.1428, step time: 0.1084\n",
      "104/223, train_loss: 0.1246, step time: 0.1001\n",
      "105/223, train_loss: 0.1269, step time: 0.1009\n",
      "106/223, train_loss: 0.1219, step time: 0.1005\n",
      "107/223, train_loss: 0.1281, step time: 0.1004\n",
      "108/223, train_loss: 0.1246, step time: 0.1005\n",
      "109/223, train_loss: 0.1359, step time: 0.1182\n",
      "110/223, train_loss: 0.1321, step time: 0.1002\n",
      "111/223, train_loss: 0.1235, step time: 0.1017\n",
      "112/223, train_loss: 0.1393, step time: 0.0999\n",
      "113/223, train_loss: 0.1255, step time: 0.1008\n",
      "114/223, train_loss: 0.1265, step time: 0.1000\n",
      "115/223, train_loss: 0.1188, step time: 0.1003\n",
      "116/223, train_loss: 0.1298, step time: 0.1007\n",
      "117/223, train_loss: 0.1229, step time: 0.1031\n",
      "118/223, train_loss: 0.1326, step time: 0.1165\n",
      "119/223, train_loss: 0.1386, step time: 0.1138\n",
      "120/223, train_loss: 0.1243, step time: 0.1007\n",
      "121/223, train_loss: 0.1333, step time: 0.0995\n",
      "122/223, train_loss: 0.1329, step time: 0.1574\n",
      "123/223, train_loss: 0.1505, step time: 0.1003\n",
      "124/223, train_loss: 0.1436, step time: 0.0999\n",
      "125/223, train_loss: 0.1226, step time: 0.1208\n",
      "126/223, train_loss: 0.1353, step time: 0.1064\n",
      "127/223, train_loss: 0.1164, step time: 0.1278\n",
      "128/223, train_loss: 0.1345, step time: 0.1206\n",
      "129/223, train_loss: 0.1261, step time: 0.1345\n",
      "130/223, train_loss: 0.1201, step time: 0.1207\n",
      "131/223, train_loss: 0.1160, step time: 0.1016\n",
      "132/223, train_loss: 0.1280, step time: 0.1183\n",
      "133/223, train_loss: 0.1576, step time: 0.1165\n",
      "134/223, train_loss: 0.1398, step time: 0.1135\n",
      "135/223, train_loss: 0.1207, step time: 0.1459\n",
      "136/223, train_loss: 0.1225, step time: 0.1320\n",
      "137/223, train_loss: 0.1243, step time: 0.1068\n",
      "138/223, train_loss: 0.1391, step time: 0.1029\n",
      "139/223, train_loss: 0.1387, step time: 0.1009\n",
      "140/223, train_loss: 0.1327, step time: 0.1109\n",
      "141/223, train_loss: 0.1450, step time: 0.1085\n",
      "142/223, train_loss: 0.1327, step time: 0.1078\n",
      "143/223, train_loss: 0.1325, step time: 0.1000\n",
      "144/223, train_loss: 0.1282, step time: 0.1129\n",
      "145/223, train_loss: 0.1235, step time: 0.1002\n",
      "146/223, train_loss: 0.1270, step time: 0.1156\n",
      "147/223, train_loss: 0.1337, step time: 0.1439\n",
      "148/223, train_loss: 0.1344, step time: 0.1426\n",
      "149/223, train_loss: 0.1451, step time: 0.1131\n",
      "150/223, train_loss: 0.1430, step time: 0.1066\n",
      "151/223, train_loss: 0.1445, step time: 0.1211\n",
      "152/223, train_loss: 0.1235, step time: 0.1146\n",
      "153/223, train_loss: 0.1222, step time: 0.1035\n",
      "154/223, train_loss: 0.1355, step time: 0.1129\n",
      "155/223, train_loss: 0.1189, step time: 0.1102\n",
      "156/223, train_loss: 0.1368, step time: 0.1011\n",
      "157/223, train_loss: 0.1404, step time: 0.1004\n",
      "158/223, train_loss: 0.1221, step time: 0.1034\n",
      "159/223, train_loss: 0.1238, step time: 0.0996\n",
      "160/223, train_loss: 0.1319, step time: 0.1071\n",
      "161/223, train_loss: 0.1257, step time: 0.1220\n",
      "162/223, train_loss: 0.1279, step time: 0.1076\n",
      "163/223, train_loss: 0.1328, step time: 0.1005\n",
      "164/223, train_loss: 0.1273, step time: 0.1405\n",
      "165/223, train_loss: 0.1299, step time: 0.1110\n",
      "166/223, train_loss: 0.1314, step time: 0.1150\n",
      "167/223, train_loss: 0.1228, step time: 0.1211\n",
      "168/223, train_loss: 0.1264, step time: 0.1177\n",
      "169/223, train_loss: 0.1563, step time: 0.1085\n",
      "170/223, train_loss: 0.3326, step time: 0.1314\n",
      "171/223, train_loss: 0.1364, step time: 0.1011\n",
      "172/223, train_loss: 0.1283, step time: 0.1103\n",
      "173/223, train_loss: 0.1534, step time: 0.1076\n",
      "174/223, train_loss: 0.1282, step time: 0.1148\n",
      "175/223, train_loss: 0.1210, step time: 0.1050\n",
      "176/223, train_loss: 0.1328, step time: 0.1132\n",
      "177/223, train_loss: 0.1270, step time: 0.1061\n",
      "178/223, train_loss: 0.1299, step time: 0.1189\n",
      "179/223, train_loss: 0.1171, step time: 0.1151\n",
      "180/223, train_loss: 0.1400, step time: 0.1083\n",
      "181/223, train_loss: 0.1377, step time: 0.1107\n",
      "182/223, train_loss: 0.1234, step time: 0.1027\n",
      "183/223, train_loss: 0.1355, step time: 0.1128\n",
      "184/223, train_loss: 0.1279, step time: 0.1053\n",
      "185/223, train_loss: 0.1300, step time: 0.1067\n",
      "186/223, train_loss: 0.1313, step time: 0.1058\n",
      "187/223, train_loss: 0.1421, step time: 0.1102\n",
      "188/223, train_loss: 0.1303, step time: 0.1077\n",
      "189/223, train_loss: 0.1269, step time: 0.1005\n",
      "190/223, train_loss: 0.1379, step time: 0.1191\n",
      "191/223, train_loss: 0.1357, step time: 0.1335\n",
      "192/223, train_loss: 0.1517, step time: 0.1150\n",
      "193/223, train_loss: 0.1428, step time: 0.1467\n",
      "194/223, train_loss: 0.1376, step time: 0.0997\n",
      "195/223, train_loss: 0.1184, step time: 0.1009\n",
      "196/223, train_loss: 0.1433, step time: 0.1350\n",
      "197/223, train_loss: 0.1219, step time: 0.1070\n",
      "198/223, train_loss: 0.1246, step time: 0.1146\n",
      "199/223, train_loss: 0.1252, step time: 0.1076\n",
      "200/223, train_loss: 0.1370, step time: 0.1140\n",
      "201/223, train_loss: 0.1421, step time: 0.1004\n",
      "202/223, train_loss: 0.1382, step time: 0.1163\n",
      "203/223, train_loss: 0.1327, step time: 0.1002\n",
      "204/223, train_loss: 0.1317, step time: 0.1165\n",
      "205/223, train_loss: 0.1355, step time: 0.0996\n",
      "206/223, train_loss: 0.1337, step time: 0.1055\n",
      "207/223, train_loss: 0.1259, step time: 0.1242\n",
      "208/223, train_loss: 0.1293, step time: 0.1018\n",
      "209/223, train_loss: 0.1296, step time: 0.1000\n",
      "210/223, train_loss: 0.1338, step time: 0.1000\n",
      "211/223, train_loss: 0.1274, step time: 0.1188\n",
      "212/223, train_loss: 0.1286, step time: 0.1315\n",
      "213/223, train_loss: 0.1328, step time: 0.1015\n",
      "214/223, train_loss: 0.1314, step time: 0.0995\n",
      "215/223, train_loss: 0.1104, step time: 0.1001\n",
      "216/223, train_loss: 0.1218, step time: 0.1010\n",
      "217/223, train_loss: 0.1242, step time: 0.1342\n",
      "218/223, train_loss: 0.1432, step time: 0.1002\n",
      "219/223, train_loss: 0.1408, step time: 0.1005\n",
      "220/223, train_loss: 0.1293, step time: 0.1007\n",
      "221/223, train_loss: 0.1156, step time: 0.0998\n",
      "222/223, train_loss: 0.1480, step time: 0.0996\n",
      "223/223, train_loss: 0.1347, step time: 0.0997\n",
      "epoch 49 average loss: 0.1319\n",
      "time consuming of epoch 49 is: 89.0339\n",
      "----------\n",
      "epoch 50/300\n",
      "1/223, train_loss: 0.1358, step time: 0.1060\n",
      "2/223, train_loss: 0.1257, step time: 0.1001\n",
      "3/223, train_loss: 0.1500, step time: 0.1000\n",
      "4/223, train_loss: 0.3193, step time: 0.1190\n",
      "5/223, train_loss: 0.1270, step time: 0.1213\n",
      "6/223, train_loss: 0.1220, step time: 0.1013\n",
      "7/223, train_loss: 0.1259, step time: 0.1487\n",
      "8/223, train_loss: 0.1307, step time: 0.1167\n",
      "9/223, train_loss: 0.1344, step time: 0.1008\n",
      "10/223, train_loss: 0.1211, step time: 0.1001\n",
      "11/223, train_loss: 0.1308, step time: 0.1006\n",
      "12/223, train_loss: 0.1239, step time: 0.1059\n",
      "13/223, train_loss: 0.1352, step time: 0.1164\n",
      "14/223, train_loss: 0.1218, step time: 0.1177\n",
      "15/223, train_loss: 0.1191, step time: 0.0998\n",
      "16/223, train_loss: 0.1371, step time: 0.1084\n",
      "17/223, train_loss: 0.1346, step time: 0.1015\n",
      "18/223, train_loss: 0.1433, step time: 0.1002\n",
      "19/223, train_loss: 0.1330, step time: 0.1176\n",
      "20/223, train_loss: 0.1200, step time: 0.1058\n",
      "21/223, train_loss: 0.1292, step time: 0.1107\n",
      "22/223, train_loss: 0.1243, step time: 0.1096\n",
      "23/223, train_loss: 0.1374, step time: 0.1296\n",
      "24/223, train_loss: 0.1127, step time: 0.1154\n",
      "25/223, train_loss: 0.1459, step time: 0.1176\n",
      "26/223, train_loss: 0.1375, step time: 0.1091\n",
      "27/223, train_loss: 0.1248, step time: 0.1106\n",
      "28/223, train_loss: 0.1384, step time: 0.1039\n",
      "29/223, train_loss: 0.1225, step time: 0.1171\n",
      "30/223, train_loss: 0.1251, step time: 0.1055\n",
      "31/223, train_loss: 0.1078, step time: 0.1113\n",
      "32/223, train_loss: 0.1173, step time: 0.1073\n",
      "33/223, train_loss: 0.1169, step time: 0.1013\n",
      "34/223, train_loss: 0.1225, step time: 0.1141\n",
      "35/223, train_loss: 0.1186, step time: 0.1129\n",
      "36/223, train_loss: 0.1293, step time: 0.1057\n",
      "37/223, train_loss: 0.1348, step time: 0.1198\n",
      "38/223, train_loss: 0.1256, step time: 0.1211\n",
      "39/223, train_loss: 0.1336, step time: 0.1004\n",
      "40/223, train_loss: 0.1397, step time: 0.1016\n",
      "41/223, train_loss: 0.1366, step time: 0.1001\n",
      "42/223, train_loss: 0.1335, step time: 0.1015\n",
      "43/223, train_loss: 0.1212, step time: 0.1008\n",
      "44/223, train_loss: 0.1307, step time: 0.1031\n",
      "45/223, train_loss: 0.1304, step time: 0.1036\n",
      "46/223, train_loss: 0.1121, step time: 0.1282\n",
      "47/223, train_loss: 0.1337, step time: 0.1007\n",
      "48/223, train_loss: 0.1238, step time: 0.1002\n",
      "49/223, train_loss: 0.1225, step time: 0.1243\n",
      "50/223, train_loss: 0.1189, step time: 0.1103\n",
      "51/223, train_loss: 0.1148, step time: 0.1088\n",
      "52/223, train_loss: 0.1199, step time: 0.0996\n",
      "53/223, train_loss: 0.1148, step time: 0.1151\n",
      "54/223, train_loss: 0.1428, step time: 0.1083\n",
      "55/223, train_loss: 0.1348, step time: 0.1003\n",
      "56/223, train_loss: 0.1392, step time: 0.1035\n",
      "57/223, train_loss: 0.1216, step time: 0.1183\n",
      "58/223, train_loss: 0.1325, step time: 0.0998\n",
      "59/223, train_loss: 0.1183, step time: 0.1008\n",
      "60/223, train_loss: 0.1262, step time: 0.0999\n",
      "61/223, train_loss: 0.1292, step time: 0.1203\n",
      "62/223, train_loss: 0.1286, step time: 0.1216\n",
      "63/223, train_loss: 0.1398, step time: 0.1158\n",
      "64/223, train_loss: 0.1424, step time: 0.1255\n",
      "65/223, train_loss: 0.1259, step time: 0.1151\n",
      "66/223, train_loss: 0.1340, step time: 0.1033\n",
      "67/223, train_loss: 0.1301, step time: 0.1363\n",
      "68/223, train_loss: 0.1368, step time: 0.1237\n",
      "69/223, train_loss: 0.1302, step time: 0.1045\n",
      "70/223, train_loss: 0.1281, step time: 0.1147\n",
      "71/223, train_loss: 0.1219, step time: 0.1157\n",
      "72/223, train_loss: 0.1256, step time: 0.1143\n",
      "73/223, train_loss: 0.1328, step time: 0.1153\n",
      "74/223, train_loss: 0.1351, step time: 0.1068\n",
      "75/223, train_loss: 0.1173, step time: 0.1415\n",
      "76/223, train_loss: 0.1378, step time: 0.1162\n",
      "77/223, train_loss: 0.1529, step time: 0.1134\n",
      "78/223, train_loss: 0.1495, step time: 0.1095\n",
      "79/223, train_loss: 0.1406, step time: 0.1050\n",
      "80/223, train_loss: 0.1410, step time: 0.1139\n",
      "81/223, train_loss: 0.1300, step time: 0.1111\n",
      "82/223, train_loss: 0.1324, step time: 0.1013\n",
      "83/223, train_loss: 0.1382, step time: 0.1147\n",
      "84/223, train_loss: 0.1215, step time: 0.1127\n",
      "85/223, train_loss: 0.1410, step time: 0.1046\n",
      "86/223, train_loss: 0.1197, step time: 0.1209\n",
      "87/223, train_loss: 0.1201, step time: 0.1134\n",
      "88/223, train_loss: 0.1143, step time: 0.1036\n",
      "89/223, train_loss: 0.1364, step time: 0.1137\n",
      "90/223, train_loss: 0.1256, step time: 0.1063\n",
      "91/223, train_loss: 0.1317, step time: 0.1047\n",
      "92/223, train_loss: 0.1231, step time: 0.1428\n",
      "93/223, train_loss: 0.1477, step time: 0.1156\n",
      "94/223, train_loss: 0.1355, step time: 0.1100\n",
      "95/223, train_loss: 0.1453, step time: 0.1159\n",
      "96/223, train_loss: 0.1230, step time: 0.1142\n",
      "97/223, train_loss: 0.1346, step time: 0.1003\n",
      "98/223, train_loss: 0.1226, step time: 0.1084\n",
      "99/223, train_loss: 0.1263, step time: 0.1101\n",
      "100/223, train_loss: 0.1220, step time: 0.1047\n",
      "101/223, train_loss: 0.1249, step time: 0.1172\n",
      "102/223, train_loss: 0.1254, step time: 0.1136\n",
      "103/223, train_loss: 0.1418, step time: 0.1020\n",
      "104/223, train_loss: 0.1166, step time: 0.1338\n",
      "105/223, train_loss: 0.1174, step time: 0.1069\n",
      "106/223, train_loss: 0.1221, step time: 0.1085\n",
      "107/223, train_loss: 0.1471, step time: 0.1123\n",
      "108/223, train_loss: 0.1276, step time: 0.1178\n",
      "109/223, train_loss: 0.1342, step time: 0.1174\n",
      "110/223, train_loss: 0.1231, step time: 0.1022\n",
      "111/223, train_loss: 0.1448, step time: 0.0996\n",
      "112/223, train_loss: 0.1158, step time: 0.0994\n",
      "113/223, train_loss: 0.1274, step time: 0.1138\n",
      "114/223, train_loss: 0.1276, step time: 0.1093\n",
      "115/223, train_loss: 0.1182, step time: 0.1180\n",
      "116/223, train_loss: 0.1222, step time: 0.1004\n",
      "117/223, train_loss: 0.1157, step time: 0.1072\n",
      "118/223, train_loss: 0.1337, step time: 0.1058\n",
      "119/223, train_loss: 0.1294, step time: 0.1121\n",
      "120/223, train_loss: 0.1250, step time: 0.1098\n",
      "121/223, train_loss: 0.1401, step time: 0.1006\n",
      "122/223, train_loss: 0.1215, step time: 0.1182\n",
      "123/223, train_loss: 0.1228, step time: 0.1053\n",
      "124/223, train_loss: 0.1439, step time: 0.1130\n",
      "125/223, train_loss: 0.1210, step time: 0.1008\n",
      "126/223, train_loss: 0.1392, step time: 0.1022\n",
      "127/223, train_loss: 0.1207, step time: 0.0989\n",
      "128/223, train_loss: 0.1361, step time: 0.1015\n",
      "129/223, train_loss: 0.1279, step time: 0.1007\n",
      "130/223, train_loss: 0.1305, step time: 0.1163\n",
      "131/223, train_loss: 0.1361, step time: 0.1349\n",
      "132/223, train_loss: 0.1319, step time: 0.1079\n",
      "133/223, train_loss: 0.1228, step time: 0.1085\n",
      "134/223, train_loss: 0.1451, step time: 0.1184\n",
      "135/223, train_loss: 0.1299, step time: 0.1115\n",
      "136/223, train_loss: 0.1374, step time: 0.1136\n",
      "137/223, train_loss: 0.1525, step time: 0.1187\n",
      "138/223, train_loss: 0.1413, step time: 0.1300\n",
      "139/223, train_loss: 0.1352, step time: 0.1197\n",
      "140/223, train_loss: 0.1434, step time: 0.1188\n",
      "141/223, train_loss: 0.1261, step time: 0.1134\n",
      "142/223, train_loss: 0.1583, step time: 0.1028\n",
      "143/223, train_loss: 0.1323, step time: 0.1246\n",
      "144/223, train_loss: 0.1214, step time: 0.1134\n",
      "145/223, train_loss: 0.1364, step time: 0.1050\n",
      "146/223, train_loss: 0.1182, step time: 0.1004\n",
      "147/223, train_loss: 0.1306, step time: 0.0997\n",
      "148/223, train_loss: 0.1407, step time: 0.1433\n",
      "149/223, train_loss: 0.1141, step time: 0.1021\n",
      "150/223, train_loss: 0.1515, step time: 0.1152\n",
      "151/223, train_loss: 0.1209, step time: 0.1259\n",
      "152/223, train_loss: 0.1380, step time: 0.1048\n",
      "153/223, train_loss: 0.1200, step time: 0.1054\n",
      "154/223, train_loss: 0.1402, step time: 0.1077\n",
      "155/223, train_loss: 0.1386, step time: 0.1145\n",
      "156/223, train_loss: 0.1229, step time: 0.1231\n",
      "157/223, train_loss: 0.1308, step time: 0.1191\n",
      "158/223, train_loss: 0.1303, step time: 0.1084\n",
      "159/223, train_loss: 0.1219, step time: 0.1039\n",
      "160/223, train_loss: 0.1250, step time: 0.1008\n",
      "161/223, train_loss: 0.1250, step time: 0.1192\n",
      "162/223, train_loss: 0.1329, step time: 0.1158\n",
      "163/223, train_loss: 0.1177, step time: 0.1119\n",
      "164/223, train_loss: 0.1365, step time: 0.1262\n",
      "165/223, train_loss: 0.1189, step time: 0.1005\n",
      "166/223, train_loss: 0.1417, step time: 0.1021\n",
      "167/223, train_loss: 0.1280, step time: 0.1034\n",
      "168/223, train_loss: 0.1273, step time: 0.1301\n",
      "169/223, train_loss: 0.1387, step time: 0.1100\n",
      "170/223, train_loss: 0.1397, step time: 0.1004\n",
      "171/223, train_loss: 0.1243, step time: 0.1017\n",
      "172/223, train_loss: 0.1290, step time: 0.0999\n",
      "173/223, train_loss: 0.1196, step time: 0.1470\n",
      "174/223, train_loss: 0.1224, step time: 0.1213\n",
      "175/223, train_loss: 0.1165, step time: 0.0999\n",
      "176/223, train_loss: 0.1226, step time: 0.1176\n",
      "177/223, train_loss: 0.1274, step time: 0.1000\n",
      "178/223, train_loss: 0.1253, step time: 0.0993\n",
      "179/223, train_loss: 0.1209, step time: 0.0991\n",
      "180/223, train_loss: 0.1538, step time: 0.1175\n",
      "181/223, train_loss: 0.1354, step time: 0.0995\n",
      "182/223, train_loss: 0.1343, step time: 0.0986\n",
      "183/223, train_loss: 0.1319, step time: 0.0988\n",
      "184/223, train_loss: 0.1467, step time: 0.0996\n",
      "185/223, train_loss: 0.1236, step time: 0.1142\n",
      "186/223, train_loss: 0.1175, step time: 0.1374\n",
      "187/223, train_loss: 0.1355, step time: 0.1005\n",
      "188/223, train_loss: 0.1372, step time: 0.1035\n",
      "189/223, train_loss: 0.1160, step time: 0.0995\n",
      "190/223, train_loss: 0.1218, step time: 0.1000\n",
      "191/223, train_loss: 0.1198, step time: 0.1002\n",
      "192/223, train_loss: 0.1164, step time: 0.1232\n",
      "193/223, train_loss: 0.1343, step time: 0.1008\n",
      "194/223, train_loss: 0.1236, step time: 0.1014\n",
      "195/223, train_loss: 0.1341, step time: 0.1094\n",
      "196/223, train_loss: 0.1216, step time: 0.1005\n",
      "197/223, train_loss: 0.1339, step time: 0.1490\n",
      "198/223, train_loss: 0.1262, step time: 0.1315\n",
      "199/223, train_loss: 0.1289, step time: 0.1006\n",
      "200/223, train_loss: 0.1265, step time: 0.1020\n",
      "201/223, train_loss: 0.1470, step time: 0.1002\n",
      "202/223, train_loss: 0.1350, step time: 0.1005\n",
      "203/223, train_loss: 0.1251, step time: 0.1001\n",
      "204/223, train_loss: 0.1321, step time: 0.1186\n",
      "205/223, train_loss: 0.1243, step time: 0.1078\n",
      "206/223, train_loss: 0.1387, step time: 0.1077\n",
      "207/223, train_loss: 0.1381, step time: 0.1256\n",
      "208/223, train_loss: 0.1252, step time: 0.1004\n",
      "209/223, train_loss: 0.1285, step time: 0.1028\n",
      "210/223, train_loss: 0.1361, step time: 0.1010\n",
      "211/223, train_loss: 0.1288, step time: 0.1017\n",
      "212/223, train_loss: 0.1356, step time: 0.1005\n",
      "213/223, train_loss: 0.1322, step time: 0.1027\n",
      "214/223, train_loss: 0.1368, step time: 0.1002\n",
      "215/223, train_loss: 0.1409, step time: 0.1127\n",
      "216/223, train_loss: 0.1305, step time: 0.1021\n",
      "217/223, train_loss: 0.1238, step time: 0.1021\n",
      "218/223, train_loss: 0.1242, step time: 0.0999\n",
      "219/223, train_loss: 0.1350, step time: 0.1002\n",
      "220/223, train_loss: 0.1314, step time: 0.1034\n",
      "221/223, train_loss: 0.1304, step time: 0.0987\n",
      "222/223, train_loss: 0.1232, step time: 0.1142\n",
      "223/223, train_loss: 0.1430, step time: 0.1068\n",
      "epoch 50 average loss: 0.1307\n",
      "saved new best metric model\n",
      "current epoch: 50 current mean dice: 0.8330 tc: 0.9084 wt: 0.8463 et: 0.7443\n",
      "best mean dice: 0.8330 at epoch: 50\n",
      "time consuming of epoch 50 is: 91.6088\n",
      "----------\n",
      "epoch 51/300\n",
      "1/223, train_loss: 0.1218, step time: 0.1145\n",
      "2/223, train_loss: 0.1325, step time: 0.1083\n",
      "3/223, train_loss: 0.1319, step time: 0.1083\n",
      "4/223, train_loss: 0.1179, step time: 0.1094\n",
      "5/223, train_loss: 0.1446, step time: 0.1121\n",
      "6/223, train_loss: 0.1209, step time: 0.1004\n",
      "7/223, train_loss: 0.1192, step time: 0.1005\n",
      "8/223, train_loss: 0.1235, step time: 0.1002\n",
      "9/223, train_loss: 0.1323, step time: 0.1141\n",
      "10/223, train_loss: 0.1125, step time: 0.1116\n",
      "11/223, train_loss: 0.1192, step time: 0.1020\n",
      "12/223, train_loss: 0.1424, step time: 0.1161\n",
      "13/223, train_loss: 0.1222, step time: 0.1002\n",
      "14/223, train_loss: 0.1168, step time: 0.1016\n",
      "15/223, train_loss: 0.1232, step time: 0.1039\n",
      "16/223, train_loss: 0.1273, step time: 0.1004\n",
      "17/223, train_loss: 0.1209, step time: 0.0992\n",
      "18/223, train_loss: 0.1200, step time: 0.0996\n",
      "19/223, train_loss: 0.1250, step time: 0.1244\n",
      "20/223, train_loss: 0.1435, step time: 0.1001\n",
      "21/223, train_loss: 0.1479, step time: 0.1037\n",
      "22/223, train_loss: 0.1276, step time: 0.1004\n",
      "23/223, train_loss: 0.1317, step time: 0.1178\n",
      "24/223, train_loss: 0.1298, step time: 0.1078\n",
      "25/223, train_loss: 0.1350, step time: 0.1067\n",
      "26/223, train_loss: 0.1305, step time: 0.1129\n",
      "27/223, train_loss: 0.1204, step time: 0.1174\n",
      "28/223, train_loss: 0.1247, step time: 0.1011\n",
      "29/223, train_loss: 0.1266, step time: 0.1131\n",
      "30/223, train_loss: 0.1366, step time: 0.1183\n",
      "31/223, train_loss: 0.1338, step time: 0.1051\n",
      "32/223, train_loss: 0.1308, step time: 0.1105\n",
      "33/223, train_loss: 0.1411, step time: 0.1156\n",
      "34/223, train_loss: 0.1236, step time: 0.0995\n",
      "35/223, train_loss: 0.1332, step time: 0.1054\n",
      "36/223, train_loss: 0.1276, step time: 0.1004\n",
      "37/223, train_loss: 0.1316, step time: 0.1005\n",
      "38/223, train_loss: 0.1266, step time: 0.1015\n",
      "39/223, train_loss: 0.1269, step time: 0.1005\n",
      "40/223, train_loss: 0.1321, step time: 0.1006\n",
      "41/223, train_loss: 0.1320, step time: 0.1040\n",
      "42/223, train_loss: 0.1274, step time: 0.1043\n",
      "43/223, train_loss: 0.1215, step time: 0.1036\n",
      "44/223, train_loss: 0.1388, step time: 0.1002\n",
      "45/223, train_loss: 0.1280, step time: 0.1001\n",
      "46/223, train_loss: 0.1513, step time: 0.1090\n",
      "47/223, train_loss: 0.1432, step time: 0.1004\n",
      "48/223, train_loss: 0.1202, step time: 0.0992\n",
      "49/223, train_loss: 0.1208, step time: 0.1105\n",
      "50/223, train_loss: 0.1192, step time: 0.1043\n",
      "51/223, train_loss: 0.1229, step time: 0.1025\n",
      "52/223, train_loss: 0.1299, step time: 0.1008\n",
      "53/223, train_loss: 0.1330, step time: 0.0996\n",
      "54/223, train_loss: 0.1628, step time: 0.1165\n",
      "55/223, train_loss: 0.1185, step time: 0.1136\n",
      "56/223, train_loss: 0.1267, step time: 0.1014\n",
      "57/223, train_loss: 0.1303, step time: 0.1052\n",
      "58/223, train_loss: 0.1375, step time: 0.1038\n",
      "59/223, train_loss: 0.1574, step time: 0.1290\n",
      "60/223, train_loss: 0.1142, step time: 0.1000\n",
      "61/223, train_loss: 0.1352, step time: 0.1268\n",
      "62/223, train_loss: 0.1364, step time: 0.1088\n",
      "63/223, train_loss: 0.1290, step time: 0.1055\n",
      "64/223, train_loss: 0.1304, step time: 0.1003\n",
      "65/223, train_loss: 0.1356, step time: 0.1148\n",
      "66/223, train_loss: 0.1392, step time: 0.1006\n",
      "67/223, train_loss: 0.1278, step time: 0.1084\n",
      "68/223, train_loss: 0.1407, step time: 0.1062\n",
      "69/223, train_loss: 0.1356, step time: 0.1065\n",
      "70/223, train_loss: 0.1291, step time: 0.1185\n",
      "71/223, train_loss: 0.1226, step time: 0.0994\n",
      "72/223, train_loss: 0.1233, step time: 0.0986\n",
      "73/223, train_loss: 0.1239, step time: 0.1220\n",
      "74/223, train_loss: 0.1271, step time: 0.1077\n",
      "75/223, train_loss: 0.1393, step time: 0.1038\n",
      "76/223, train_loss: 0.1335, step time: 0.1085\n",
      "77/223, train_loss: 0.1366, step time: 0.1154\n",
      "78/223, train_loss: 0.1251, step time: 0.1100\n",
      "79/223, train_loss: 0.1233, step time: 0.1007\n",
      "80/223, train_loss: 0.1405, step time: 0.1005\n",
      "81/223, train_loss: 0.1293, step time: 0.1147\n",
      "82/223, train_loss: 0.1417, step time: 0.1000\n",
      "83/223, train_loss: 0.1390, step time: 0.1005\n",
      "84/223, train_loss: 0.1237, step time: 0.1032\n",
      "85/223, train_loss: 0.1213, step time: 0.1181\n",
      "86/223, train_loss: 0.1216, step time: 0.1173\n",
      "87/223, train_loss: 0.1365, step time: 0.1213\n",
      "88/223, train_loss: 0.1188, step time: 0.1140\n",
      "89/223, train_loss: 0.1309, step time: 0.1023\n",
      "90/223, train_loss: 0.1120, step time: 0.1296\n",
      "91/223, train_loss: 0.1361, step time: 0.1364\n",
      "92/223, train_loss: 0.1338, step time: 0.1086\n",
      "93/223, train_loss: 0.1268, step time: 0.1062\n",
      "94/223, train_loss: 0.1323, step time: 0.1089\n",
      "95/223, train_loss: 0.1290, step time: 0.1277\n",
      "96/223, train_loss: 0.1302, step time: 0.1189\n",
      "97/223, train_loss: 0.1292, step time: 0.1231\n",
      "98/223, train_loss: 0.1322, step time: 0.1286\n",
      "99/223, train_loss: 0.1250, step time: 0.1005\n",
      "100/223, train_loss: 0.1469, step time: 0.1214\n",
      "101/223, train_loss: 0.1255, step time: 0.1199\n",
      "102/223, train_loss: 0.1265, step time: 0.1002\n",
      "103/223, train_loss: 0.1246, step time: 0.1004\n",
      "104/223, train_loss: 0.1317, step time: 0.1064\n",
      "105/223, train_loss: 0.1392, step time: 0.1095\n",
      "106/223, train_loss: 0.1366, step time: 0.1039\n",
      "107/223, train_loss: 0.1192, step time: 0.1059\n",
      "108/223, train_loss: 0.1289, step time: 0.1009\n",
      "109/223, train_loss: 0.1391, step time: 0.1217\n",
      "110/223, train_loss: 0.1275, step time: 0.1059\n",
      "111/223, train_loss: 0.1236, step time: 0.1138\n",
      "112/223, train_loss: 0.1322, step time: 0.1182\n",
      "113/223, train_loss: 0.1369, step time: 0.1064\n",
      "114/223, train_loss: 0.1303, step time: 0.1344\n",
      "115/223, train_loss: 0.1331, step time: 0.1017\n",
      "116/223, train_loss: 0.1332, step time: 0.1303\n",
      "117/223, train_loss: 0.1444, step time: 0.0998\n",
      "118/223, train_loss: 0.1143, step time: 0.0997\n",
      "119/223, train_loss: 0.1241, step time: 0.0989\n",
      "120/223, train_loss: 0.1238, step time: 0.1002\n",
      "121/223, train_loss: 0.1295, step time: 0.1002\n",
      "122/223, train_loss: 0.1239, step time: 0.1006\n",
      "123/223, train_loss: 0.1245, step time: 0.1004\n",
      "124/223, train_loss: 0.1222, step time: 0.0999\n",
      "125/223, train_loss: 0.1446, step time: 0.0996\n",
      "126/223, train_loss: 0.1343, step time: 0.1002\n",
      "127/223, train_loss: 0.1143, step time: 0.0998\n",
      "128/223, train_loss: 0.1465, step time: 0.1022\n",
      "129/223, train_loss: 0.1266, step time: 0.1179\n",
      "130/223, train_loss: 0.1245, step time: 0.1216\n",
      "131/223, train_loss: 0.1302, step time: 0.1206\n",
      "132/223, train_loss: 0.1329, step time: 0.1009\n",
      "133/223, train_loss: 0.1309, step time: 0.1030\n",
      "134/223, train_loss: 0.1319, step time: 0.1003\n",
      "135/223, train_loss: 0.1200, step time: 0.1070\n",
      "136/223, train_loss: 0.1219, step time: 0.1165\n",
      "137/223, train_loss: 0.1487, step time: 0.1069\n",
      "138/223, train_loss: 0.1223, step time: 0.1486\n",
      "139/223, train_loss: 0.1227, step time: 0.1150\n",
      "140/223, train_loss: 0.1422, step time: 0.1024\n",
      "141/223, train_loss: 0.1226, step time: 0.1050\n",
      "142/223, train_loss: 0.1413, step time: 0.1025\n",
      "143/223, train_loss: 0.1237, step time: 0.1047\n",
      "144/223, train_loss: 0.1204, step time: 0.1100\n",
      "145/223, train_loss: 0.1245, step time: 0.1115\n",
      "146/223, train_loss: 0.1262, step time: 0.1007\n",
      "147/223, train_loss: 0.1182, step time: 0.1017\n",
      "148/223, train_loss: 0.1209, step time: 0.1013\n",
      "149/223, train_loss: 0.1392, step time: 0.1100\n",
      "150/223, train_loss: 0.1265, step time: 0.1010\n",
      "151/223, train_loss: 0.1226, step time: 0.1070\n",
      "152/223, train_loss: 0.1228, step time: 0.1025\n",
      "153/223, train_loss: 0.1289, step time: 0.1126\n",
      "154/223, train_loss: 0.1285, step time: 0.1007\n",
      "155/223, train_loss: 0.1189, step time: 0.0993\n",
      "156/223, train_loss: 0.1242, step time: 0.1024\n",
      "157/223, train_loss: 0.1146, step time: 0.1106\n",
      "158/223, train_loss: 0.1179, step time: 0.1016\n",
      "159/223, train_loss: 0.1317, step time: 0.1002\n",
      "160/223, train_loss: 0.1493, step time: 0.1089\n",
      "161/223, train_loss: 0.1269, step time: 0.1040\n",
      "162/223, train_loss: 0.1324, step time: 0.1167\n",
      "163/223, train_loss: 0.3301, step time: 0.1054\n",
      "164/223, train_loss: 0.1248, step time: 0.1031\n",
      "165/223, train_loss: 0.1205, step time: 0.0999\n",
      "166/223, train_loss: 0.1236, step time: 0.1064\n",
      "167/223, train_loss: 0.1219, step time: 0.1016\n",
      "168/223, train_loss: 0.1234, step time: 0.1025\n",
      "169/223, train_loss: 0.1232, step time: 0.1006\n",
      "170/223, train_loss: 0.1401, step time: 0.1035\n",
      "171/223, train_loss: 0.1223, step time: 0.1279\n",
      "172/223, train_loss: 0.1198, step time: 0.1076\n",
      "173/223, train_loss: 0.1338, step time: 0.1166\n",
      "174/223, train_loss: 0.1156, step time: 0.1169\n",
      "175/223, train_loss: 0.1498, step time: 0.1042\n",
      "176/223, train_loss: 0.1238, step time: 0.0998\n",
      "177/223, train_loss: 0.1370, step time: 0.0995\n",
      "178/223, train_loss: 0.1490, step time: 0.0993\n",
      "179/223, train_loss: 0.1240, step time: 0.1002\n",
      "180/223, train_loss: 0.1265, step time: 0.1009\n",
      "181/223, train_loss: 0.1272, step time: 0.1048\n",
      "182/223, train_loss: 0.1433, step time: 0.1006\n",
      "183/223, train_loss: 0.1303, step time: 0.1198\n",
      "184/223, train_loss: 0.1319, step time: 0.1302\n",
      "185/223, train_loss: 0.1248, step time: 0.1000\n",
      "186/223, train_loss: 0.1228, step time: 0.1119\n",
      "187/223, train_loss: 0.1321, step time: 0.1007\n",
      "188/223, train_loss: 0.1267, step time: 0.1213\n",
      "189/223, train_loss: 0.1295, step time: 0.1022\n",
      "190/223, train_loss: 0.1304, step time: 0.1011\n",
      "191/223, train_loss: 0.1307, step time: 0.0997\n",
      "192/223, train_loss: 0.1288, step time: 0.1113\n",
      "193/223, train_loss: 0.1262, step time: 0.1063\n",
      "194/223, train_loss: 0.1355, step time: 0.1008\n",
      "195/223, train_loss: 0.1209, step time: 0.1012\n",
      "196/223, train_loss: 0.1412, step time: 0.1002\n",
      "197/223, train_loss: 0.1425, step time: 0.1006\n",
      "198/223, train_loss: 0.1566, step time: 0.1175\n",
      "199/223, train_loss: 0.1428, step time: 0.1145\n",
      "200/223, train_loss: 0.1340, step time: 0.1113\n",
      "201/223, train_loss: 0.1288, step time: 0.1061\n",
      "202/223, train_loss: 0.1358, step time: 0.1234\n",
      "203/223, train_loss: 0.1433, step time: 0.1248\n",
      "204/223, train_loss: 0.1370, step time: 0.1001\n",
      "205/223, train_loss: 0.1314, step time: 0.1123\n",
      "206/223, train_loss: 0.1394, step time: 0.1033\n",
      "207/223, train_loss: 0.1257, step time: 0.1001\n",
      "208/223, train_loss: 0.1289, step time: 0.1087\n",
      "209/223, train_loss: 0.1313, step time: 0.1115\n",
      "210/223, train_loss: 0.1318, step time: 0.1002\n",
      "211/223, train_loss: 0.1224, step time: 0.1171\n",
      "212/223, train_loss: 0.1210, step time: 0.1181\n",
      "213/223, train_loss: 0.1355, step time: 0.1012\n",
      "214/223, train_loss: 0.1342, step time: 0.1094\n",
      "215/223, train_loss: 0.1116, step time: 0.1140\n",
      "216/223, train_loss: 0.1364, step time: 0.1077\n",
      "217/223, train_loss: 0.1238, step time: 0.1020\n",
      "218/223, train_loss: 0.1290, step time: 0.1005\n",
      "219/223, train_loss: 0.1309, step time: 0.1001\n",
      "220/223, train_loss: 0.1256, step time: 0.1001\n",
      "221/223, train_loss: 0.1204, step time: 0.0999\n",
      "222/223, train_loss: 0.1311, step time: 0.0996\n",
      "223/223, train_loss: 0.1266, step time: 0.0996\n",
      "epoch 51 average loss: 0.1306\n",
      "time consuming of epoch 51 is: 88.7833\n",
      "----------\n",
      "epoch 52/300\n",
      "1/223, train_loss: 0.1438, step time: 0.1053\n",
      "2/223, train_loss: 0.1123, step time: 0.1000\n",
      "3/223, train_loss: 0.1427, step time: 0.1011\n",
      "4/223, train_loss: 0.1513, step time: 0.1057\n",
      "5/223, train_loss: 0.1221, step time: 0.1088\n",
      "6/223, train_loss: 0.1303, step time: 0.1260\n",
      "7/223, train_loss: 0.1333, step time: 0.1003\n",
      "8/223, train_loss: 0.1364, step time: 0.0998\n",
      "9/223, train_loss: 0.1412, step time: 0.1099\n",
      "10/223, train_loss: 0.1277, step time: 0.1668\n",
      "11/223, train_loss: 0.1366, step time: 0.1164\n",
      "12/223, train_loss: 0.1305, step time: 0.1097\n",
      "13/223, train_loss: 0.1378, step time: 0.0995\n",
      "14/223, train_loss: 0.1303, step time: 0.1098\n",
      "15/223, train_loss: 0.1359, step time: 0.1002\n",
      "16/223, train_loss: 0.1416, step time: 0.1012\n",
      "17/223, train_loss: 0.1250, step time: 0.1221\n",
      "18/223, train_loss: 0.1278, step time: 0.1303\n",
      "19/223, train_loss: 0.1251, step time: 0.1003\n",
      "20/223, train_loss: 0.1445, step time: 0.1164\n",
      "21/223, train_loss: 0.1327, step time: 0.1009\n",
      "22/223, train_loss: 0.1198, step time: 0.1180\n",
      "23/223, train_loss: 0.1556, step time: 0.1062\n",
      "24/223, train_loss: 0.1317, step time: 0.1100\n",
      "25/223, train_loss: 0.1227, step time: 0.1043\n",
      "26/223, train_loss: 0.1223, step time: 0.1047\n",
      "27/223, train_loss: 0.1515, step time: 0.1184\n",
      "28/223, train_loss: 0.1272, step time: 0.0993\n",
      "29/223, train_loss: 0.1424, step time: 0.1013\n",
      "30/223, train_loss: 0.1264, step time: 0.1052\n",
      "31/223, train_loss: 0.1382, step time: 0.1001\n",
      "32/223, train_loss: 0.1425, step time: 0.0999\n",
      "33/223, train_loss: 0.1334, step time: 0.1112\n",
      "34/223, train_loss: 0.1216, step time: 0.1136\n",
      "35/223, train_loss: 0.1405, step time: 0.1006\n",
      "36/223, train_loss: 0.1243, step time: 0.1002\n",
      "37/223, train_loss: 0.1179, step time: 0.1092\n",
      "38/223, train_loss: 0.1212, step time: 0.1125\n",
      "39/223, train_loss: 0.1266, step time: 0.1133\n",
      "40/223, train_loss: 0.1202, step time: 0.1055\n",
      "41/223, train_loss: 0.1426, step time: 0.1093\n",
      "42/223, train_loss: 0.1143, step time: 0.1061\n",
      "43/223, train_loss: 0.1182, step time: 0.1137\n",
      "44/223, train_loss: 0.1225, step time: 0.1352\n",
      "45/223, train_loss: 0.1372, step time: 0.0995\n",
      "46/223, train_loss: 0.1363, step time: 0.1058\n",
      "47/223, train_loss: 0.1278, step time: 0.1176\n",
      "48/223, train_loss: 0.1186, step time: 0.1058\n",
      "49/223, train_loss: 0.1315, step time: 0.1003\n",
      "50/223, train_loss: 0.1200, step time: 0.1008\n",
      "51/223, train_loss: 0.1406, step time: 0.1159\n",
      "52/223, train_loss: 0.1202, step time: 0.1017\n",
      "53/223, train_loss: 0.1422, step time: 0.1054\n",
      "54/223, train_loss: 0.1231, step time: 0.1105\n",
      "55/223, train_loss: 0.1209, step time: 0.1173\n",
      "56/223, train_loss: 0.1353, step time: 0.1060\n",
      "57/223, train_loss: 0.1183, step time: 0.1081\n",
      "58/223, train_loss: 0.1336, step time: 0.1033\n",
      "59/223, train_loss: 0.1187, step time: 0.1217\n",
      "60/223, train_loss: 0.1345, step time: 0.1131\n",
      "61/223, train_loss: 0.1162, step time: 0.1000\n",
      "62/223, train_loss: 0.1350, step time: 0.1004\n",
      "63/223, train_loss: 0.1448, step time: 0.1145\n",
      "64/223, train_loss: 0.1346, step time: 0.1119\n",
      "65/223, train_loss: 0.1194, step time: 0.1110\n",
      "66/223, train_loss: 0.1291, step time: 0.1160\n",
      "67/223, train_loss: 0.1216, step time: 0.1035\n",
      "68/223, train_loss: 0.1235, step time: 0.1009\n",
      "69/223, train_loss: 0.1093, step time: 0.1116\n",
      "70/223, train_loss: 0.1266, step time: 0.1004\n",
      "71/223, train_loss: 0.1519, step time: 0.1006\n",
      "72/223, train_loss: 0.1195, step time: 0.1002\n",
      "73/223, train_loss: 0.1406, step time: 0.1241\n",
      "74/223, train_loss: 0.1377, step time: 0.1068\n",
      "75/223, train_loss: 0.1273, step time: 0.1054\n",
      "76/223, train_loss: 0.1241, step time: 0.1006\n",
      "77/223, train_loss: 0.1354, step time: 0.1052\n",
      "78/223, train_loss: 0.1359, step time: 0.1064\n",
      "79/223, train_loss: 0.1447, step time: 0.1120\n",
      "80/223, train_loss: 0.1322, step time: 0.1003\n",
      "81/223, train_loss: 0.1287, step time: 0.1021\n",
      "82/223, train_loss: 0.1206, step time: 0.1000\n",
      "83/223, train_loss: 0.1407, step time: 0.1086\n",
      "84/223, train_loss: 0.1310, step time: 0.1004\n",
      "85/223, train_loss: 0.1296, step time: 0.1012\n",
      "86/223, train_loss: 0.1242, step time: 0.1810\n",
      "87/223, train_loss: 0.1244, step time: 0.1145\n",
      "88/223, train_loss: 0.1249, step time: 0.1158\n",
      "89/223, train_loss: 0.1336, step time: 0.1006\n",
      "90/223, train_loss: 0.1237, step time: 0.1002\n",
      "91/223, train_loss: 0.1269, step time: 0.1008\n",
      "92/223, train_loss: 0.1231, step time: 0.1081\n",
      "93/223, train_loss: 0.1383, step time: 0.1073\n",
      "94/223, train_loss: 0.1196, step time: 0.1166\n",
      "95/223, train_loss: 0.1331, step time: 0.1022\n",
      "96/223, train_loss: 0.1334, step time: 0.1064\n",
      "97/223, train_loss: 0.1265, step time: 0.1090\n",
      "98/223, train_loss: 0.1196, step time: 0.1012\n",
      "99/223, train_loss: 0.1296, step time: 0.1004\n",
      "100/223, train_loss: 0.1319, step time: 0.1000\n",
      "101/223, train_loss: 0.1074, step time: 0.1163\n",
      "102/223, train_loss: 0.1285, step time: 0.1380\n",
      "103/223, train_loss: 0.1261, step time: 0.1035\n",
      "104/223, train_loss: 0.1300, step time: 0.1011\n",
      "105/223, train_loss: 0.1226, step time: 0.0996\n",
      "106/223, train_loss: 0.1259, step time: 0.0997\n",
      "107/223, train_loss: 0.1377, step time: 0.0999\n",
      "108/223, train_loss: 0.1349, step time: 0.0996\n",
      "109/223, train_loss: 0.1398, step time: 0.1298\n",
      "110/223, train_loss: 0.1219, step time: 0.1203\n",
      "111/223, train_loss: 0.1355, step time: 0.1007\n",
      "112/223, train_loss: 0.1315, step time: 0.0999\n",
      "113/223, train_loss: 0.1380, step time: 0.1056\n",
      "114/223, train_loss: 0.1264, step time: 0.1172\n",
      "115/223, train_loss: 0.1375, step time: 0.1232\n",
      "116/223, train_loss: 0.1474, step time: 0.1008\n",
      "117/223, train_loss: 0.1369, step time: 0.1129\n",
      "118/223, train_loss: 0.1220, step time: 0.1158\n",
      "119/223, train_loss: 0.1241, step time: 0.1011\n",
      "120/223, train_loss: 0.1191, step time: 0.0995\n",
      "121/223, train_loss: 0.1190, step time: 0.1128\n",
      "122/223, train_loss: 0.1222, step time: 0.1292\n",
      "123/223, train_loss: 0.1349, step time: 0.1142\n",
      "124/223, train_loss: 0.1252, step time: 0.1176\n",
      "125/223, train_loss: 0.1211, step time: 0.0998\n",
      "126/223, train_loss: 0.1252, step time: 0.1234\n",
      "127/223, train_loss: 0.1169, step time: 0.1275\n",
      "128/223, train_loss: 0.1183, step time: 0.0996\n",
      "129/223, train_loss: 0.1406, step time: 0.1058\n",
      "130/223, train_loss: 0.1136, step time: 0.1124\n",
      "131/223, train_loss: 0.1253, step time: 0.1107\n",
      "132/223, train_loss: 0.1107, step time: 0.0999\n",
      "133/223, train_loss: 0.1273, step time: 0.1070\n",
      "134/223, train_loss: 0.1232, step time: 0.1010\n",
      "135/223, train_loss: 0.1393, step time: 0.1145\n",
      "136/223, train_loss: 0.1134, step time: 0.0998\n",
      "137/223, train_loss: 0.1351, step time: 0.1000\n",
      "138/223, train_loss: 0.1328, step time: 0.1077\n",
      "139/223, train_loss: 0.1175, step time: 0.1057\n",
      "140/223, train_loss: 0.1329, step time: 0.1012\n",
      "141/223, train_loss: 0.1286, step time: 0.1024\n",
      "142/223, train_loss: 0.1356, step time: 0.1008\n",
      "143/223, train_loss: 0.1290, step time: 0.1028\n",
      "144/223, train_loss: 0.1167, step time: 0.1071\n",
      "145/223, train_loss: 0.1257, step time: 0.1100\n",
      "146/223, train_loss: 0.1232, step time: 0.1130\n",
      "147/223, train_loss: 0.1212, step time: 0.1040\n",
      "148/223, train_loss: 0.1248, step time: 0.1113\n",
      "149/223, train_loss: 0.1322, step time: 0.0998\n",
      "150/223, train_loss: 0.1173, step time: 0.1027\n",
      "151/223, train_loss: 0.1269, step time: 0.1111\n",
      "152/223, train_loss: 0.1326, step time: 0.1003\n",
      "153/223, train_loss: 0.1475, step time: 0.1092\n",
      "154/223, train_loss: 0.1332, step time: 0.1028\n",
      "155/223, train_loss: 0.1249, step time: 0.1124\n",
      "156/223, train_loss: 0.1258, step time: 0.1147\n",
      "157/223, train_loss: 0.1665, step time: 0.1026\n",
      "158/223, train_loss: 0.1322, step time: 0.1038\n",
      "159/223, train_loss: 0.1540, step time: 0.1021\n",
      "160/223, train_loss: 0.1136, step time: 0.1004\n",
      "161/223, train_loss: 0.1335, step time: 0.1158\n",
      "162/223, train_loss: 0.1238, step time: 0.0998\n",
      "163/223, train_loss: 0.3220, step time: 0.1014\n",
      "164/223, train_loss: 0.1269, step time: 0.1001\n",
      "165/223, train_loss: 0.1382, step time: 0.0998\n",
      "166/223, train_loss: 0.1384, step time: 0.0996\n",
      "167/223, train_loss: 0.1331, step time: 0.1000\n",
      "168/223, train_loss: 0.1246, step time: 0.1095\n",
      "169/223, train_loss: 0.1273, step time: 0.1099\n",
      "170/223, train_loss: 0.1386, step time: 0.1072\n",
      "171/223, train_loss: 0.1217, step time: 0.1029\n",
      "172/223, train_loss: 0.1171, step time: 0.1008\n",
      "173/223, train_loss: 0.1299, step time: 0.1000\n",
      "174/223, train_loss: 0.1353, step time: 0.1008\n",
      "175/223, train_loss: 0.1394, step time: 0.0998\n",
      "176/223, train_loss: 0.1172, step time: 0.1066\n",
      "177/223, train_loss: 0.1237, step time: 0.1075\n",
      "178/223, train_loss: 0.1304, step time: 0.0998\n",
      "179/223, train_loss: 0.1402, step time: 0.1172\n",
      "180/223, train_loss: 0.1262, step time: 0.1172\n",
      "181/223, train_loss: 0.1209, step time: 0.0997\n",
      "182/223, train_loss: 0.1215, step time: 0.0996\n",
      "183/223, train_loss: 0.1304, step time: 0.1003\n",
      "184/223, train_loss: 0.1196, step time: 0.1003\n",
      "185/223, train_loss: 0.1213, step time: 0.1266\n",
      "186/223, train_loss: 0.1209, step time: 0.1276\n",
      "187/223, train_loss: 0.1352, step time: 0.0992\n",
      "188/223, train_loss: 0.1259, step time: 0.1045\n",
      "189/223, train_loss: 0.1186, step time: 0.1273\n",
      "190/223, train_loss: 0.1132, step time: 0.1219\n",
      "191/223, train_loss: 0.1262, step time: 0.1031\n",
      "192/223, train_loss: 0.1231, step time: 0.1100\n",
      "193/223, train_loss: 0.1216, step time: 0.1212\n",
      "194/223, train_loss: 0.1223, step time: 0.1048\n",
      "195/223, train_loss: 0.1202, step time: 0.0995\n",
      "196/223, train_loss: 0.1294, step time: 0.1042\n",
      "197/223, train_loss: 0.1253, step time: 0.1035\n",
      "198/223, train_loss: 0.1278, step time: 0.1074\n",
      "199/223, train_loss: 0.1413, step time: 0.1221\n",
      "200/223, train_loss: 0.1334, step time: 0.1130\n",
      "201/223, train_loss: 0.1196, step time: 0.1131\n",
      "202/223, train_loss: 0.1441, step time: 0.0998\n",
      "203/223, train_loss: 0.1234, step time: 0.1106\n",
      "204/223, train_loss: 0.1285, step time: 0.1070\n",
      "205/223, train_loss: 0.1252, step time: 0.0997\n",
      "206/223, train_loss: 0.1366, step time: 0.1106\n",
      "207/223, train_loss: 0.1130, step time: 0.1258\n",
      "208/223, train_loss: 0.1302, step time: 0.1276\n",
      "209/223, train_loss: 0.1275, step time: 0.1129\n",
      "210/223, train_loss: 0.1266, step time: 0.1218\n",
      "211/223, train_loss: 0.1367, step time: 0.0999\n",
      "212/223, train_loss: 0.1319, step time: 0.1104\n",
      "213/223, train_loss: 0.1284, step time: 0.1007\n",
      "214/223, train_loss: 0.1376, step time: 0.1006\n",
      "215/223, train_loss: 0.1340, step time: 0.1155\n",
      "216/223, train_loss: 0.1256, step time: 0.1007\n",
      "217/223, train_loss: 0.1206, step time: 0.1232\n",
      "218/223, train_loss: 0.1310, step time: 0.1085\n",
      "219/223, train_loss: 0.1267, step time: 0.0999\n",
      "220/223, train_loss: 0.1286, step time: 0.1002\n",
      "221/223, train_loss: 0.1438, step time: 0.1003\n",
      "222/223, train_loss: 0.1318, step time: 0.1000\n",
      "223/223, train_loss: 0.1316, step time: 0.0999\n",
      "epoch 52 average loss: 0.1300\n",
      "time consuming of epoch 52 is: 87.4345\n",
      "----------\n",
      "epoch 53/300\n",
      "1/223, train_loss: 0.1264, step time: 0.1005\n",
      "2/223, train_loss: 0.1406, step time: 0.1119\n",
      "3/223, train_loss: 0.1387, step time: 0.0998\n",
      "4/223, train_loss: 0.1171, step time: 0.0997\n",
      "5/223, train_loss: 0.1280, step time: 0.1066\n",
      "6/223, train_loss: 0.1158, step time: 0.1081\n",
      "7/223, train_loss: 0.1167, step time: 0.1151\n",
      "8/223, train_loss: 0.1299, step time: 0.1114\n",
      "9/223, train_loss: 0.1294, step time: 0.1076\n",
      "10/223, train_loss: 0.1259, step time: 0.1033\n",
      "11/223, train_loss: 0.1388, step time: 0.1004\n",
      "12/223, train_loss: 0.1329, step time: 0.1102\n",
      "13/223, train_loss: 0.1306, step time: 0.1196\n",
      "14/223, train_loss: 0.1192, step time: 0.1007\n",
      "15/223, train_loss: 0.1349, step time: 0.0992\n",
      "16/223, train_loss: 0.1385, step time: 0.1125\n",
      "17/223, train_loss: 0.1238, step time: 0.1129\n",
      "18/223, train_loss: 0.1275, step time: 0.1098\n",
      "19/223, train_loss: 0.1382, step time: 0.1000\n",
      "20/223, train_loss: 0.1437, step time: 0.1017\n",
      "21/223, train_loss: 0.1325, step time: 0.1173\n",
      "22/223, train_loss: 0.1254, step time: 0.1008\n",
      "23/223, train_loss: 0.1330, step time: 0.1080\n",
      "24/223, train_loss: 0.1128, step time: 0.1028\n",
      "25/223, train_loss: 0.1316, step time: 0.1026\n",
      "26/223, train_loss: 0.1379, step time: 0.1076\n",
      "27/223, train_loss: 0.1232, step time: 0.1012\n",
      "28/223, train_loss: 0.1442, step time: 0.1120\n",
      "29/223, train_loss: 0.1164, step time: 0.1006\n",
      "30/223, train_loss: 0.1289, step time: 0.1146\n",
      "31/223, train_loss: 0.1205, step time: 0.1138\n",
      "32/223, train_loss: 0.1403, step time: 0.1089\n",
      "33/223, train_loss: 0.1264, step time: 0.1182\n",
      "34/223, train_loss: 0.1364, step time: 0.1002\n",
      "35/223, train_loss: 0.1378, step time: 0.1021\n",
      "36/223, train_loss: 0.1339, step time: 0.1056\n",
      "37/223, train_loss: 0.1360, step time: 0.1057\n",
      "38/223, train_loss: 0.1176, step time: 0.1421\n",
      "39/223, train_loss: 0.1233, step time: 0.1004\n",
      "40/223, train_loss: 0.1174, step time: 0.1022\n",
      "41/223, train_loss: 0.1357, step time: 0.1103\n",
      "42/223, train_loss: 0.1243, step time: 0.1310\n",
      "43/223, train_loss: 0.1427, step time: 0.1047\n",
      "44/223, train_loss: 0.1234, step time: 0.1097\n",
      "45/223, train_loss: 0.1312, step time: 0.1058\n",
      "46/223, train_loss: 0.1278, step time: 0.1078\n",
      "47/223, train_loss: 0.1499, step time: 0.1173\n",
      "48/223, train_loss: 0.1219, step time: 0.1004\n",
      "49/223, train_loss: 0.1134, step time: 0.1089\n",
      "50/223, train_loss: 0.1260, step time: 0.1160\n",
      "51/223, train_loss: 0.1278, step time: 0.1159\n",
      "52/223, train_loss: 0.1370, step time: 0.1111\n",
      "53/223, train_loss: 0.1339, step time: 0.1154\n",
      "54/223, train_loss: 0.1292, step time: 0.1048\n",
      "55/223, train_loss: 0.1311, step time: 0.1017\n",
      "56/223, train_loss: 0.1221, step time: 0.1044\n",
      "57/223, train_loss: 0.1353, step time: 0.1103\n",
      "58/223, train_loss: 0.1197, step time: 0.1105\n",
      "59/223, train_loss: 0.1306, step time: 0.1058\n",
      "60/223, train_loss: 0.1272, step time: 0.1014\n",
      "61/223, train_loss: 0.1483, step time: 0.1077\n",
      "62/223, train_loss: 0.1257, step time: 0.1011\n",
      "63/223, train_loss: 0.1265, step time: 0.1014\n",
      "64/223, train_loss: 0.1287, step time: 0.1006\n",
      "65/223, train_loss: 0.1361, step time: 0.1119\n",
      "66/223, train_loss: 0.1200, step time: 0.1104\n",
      "67/223, train_loss: 0.1300, step time: 0.1119\n",
      "68/223, train_loss: 0.1209, step time: 0.1048\n",
      "69/223, train_loss: 0.1385, step time: 0.1045\n",
      "70/223, train_loss: 0.1151, step time: 0.1001\n",
      "71/223, train_loss: 0.1388, step time: 0.1002\n",
      "72/223, train_loss: 0.1389, step time: 0.1010\n",
      "73/223, train_loss: 0.1174, step time: 0.1067\n",
      "74/223, train_loss: 0.1408, step time: 0.1595\n",
      "75/223, train_loss: 0.1108, step time: 0.1206\n",
      "76/223, train_loss: 0.1510, step time: 0.1226\n",
      "77/223, train_loss: 0.1320, step time: 0.1126\n",
      "78/223, train_loss: 0.1170, step time: 0.1062\n",
      "79/223, train_loss: 0.1465, step time: 0.1175\n",
      "80/223, train_loss: 0.1196, step time: 0.1000\n",
      "81/223, train_loss: 0.1393, step time: 0.1035\n",
      "82/223, train_loss: 0.1385, step time: 0.1003\n",
      "83/223, train_loss: 0.1254, step time: 0.1007\n",
      "84/223, train_loss: 0.1343, step time: 0.1003\n",
      "85/223, train_loss: 0.1179, step time: 0.1047\n",
      "86/223, train_loss: 0.1449, step time: 0.1050\n",
      "87/223, train_loss: 0.1270, step time: 0.1080\n",
      "88/223, train_loss: 0.1220, step time: 0.1047\n",
      "89/223, train_loss: 0.1235, step time: 0.1018\n",
      "90/223, train_loss: 0.1424, step time: 0.1120\n",
      "91/223, train_loss: 0.1250, step time: 0.1239\n",
      "92/223, train_loss: 0.1170, step time: 0.1233\n",
      "93/223, train_loss: 0.1187, step time: 0.1053\n",
      "94/223, train_loss: 0.1278, step time: 0.1003\n",
      "95/223, train_loss: 0.1250, step time: 0.1003\n",
      "96/223, train_loss: 0.1256, step time: 0.0999\n",
      "97/223, train_loss: 0.1320, step time: 0.1057\n",
      "98/223, train_loss: 0.1305, step time: 0.1115\n",
      "99/223, train_loss: 0.1430, step time: 0.1002\n",
      "100/223, train_loss: 0.1243, step time: 0.1001\n",
      "101/223, train_loss: 0.1195, step time: 0.1111\n",
      "102/223, train_loss: 0.1285, step time: 0.1106\n",
      "103/223, train_loss: 0.1346, step time: 0.1000\n",
      "104/223, train_loss: 0.1298, step time: 0.1013\n",
      "105/223, train_loss: 0.1371, step time: 0.1034\n",
      "106/223, train_loss: 0.1224, step time: 0.1020\n",
      "107/223, train_loss: 0.1400, step time: 0.0999\n",
      "108/223, train_loss: 0.1362, step time: 0.1005\n",
      "109/223, train_loss: 0.1229, step time: 0.1112\n",
      "110/223, train_loss: 0.1318, step time: 0.1004\n",
      "111/223, train_loss: 0.1260, step time: 0.1636\n",
      "112/223, train_loss: 0.1301, step time: 0.1050\n",
      "113/223, train_loss: 0.1159, step time: 0.0998\n",
      "114/223, train_loss: 0.1327, step time: 0.1052\n",
      "115/223, train_loss: 0.1217, step time: 0.1002\n",
      "116/223, train_loss: 0.1241, step time: 0.1018\n",
      "117/223, train_loss: 0.1383, step time: 0.1033\n",
      "118/223, train_loss: 0.1242, step time: 0.1032\n",
      "119/223, train_loss: 0.1239, step time: 0.1068\n",
      "120/223, train_loss: 0.1588, step time: 0.0998\n",
      "121/223, train_loss: 0.1384, step time: 0.1006\n",
      "122/223, train_loss: 0.1260, step time: 0.0998\n",
      "123/223, train_loss: 0.1316, step time: 0.1105\n",
      "124/223, train_loss: 0.1327, step time: 0.1230\n",
      "125/223, train_loss: 0.1202, step time: 0.1078\n",
      "126/223, train_loss: 0.1247, step time: 0.1171\n",
      "127/223, train_loss: 0.1186, step time: 0.1086\n",
      "128/223, train_loss: 0.1184, step time: 0.0996\n",
      "129/223, train_loss: 0.3274, step time: 0.1056\n",
      "130/223, train_loss: 0.1324, step time: 0.1097\n",
      "131/223, train_loss: 0.1163, step time: 0.1289\n",
      "132/223, train_loss: 0.1270, step time: 0.1231\n",
      "133/223, train_loss: 0.1196, step time: 0.1184\n",
      "134/223, train_loss: 0.1246, step time: 0.1091\n",
      "135/223, train_loss: 0.1284, step time: 0.1097\n",
      "136/223, train_loss: 0.1423, step time: 0.1388\n",
      "137/223, train_loss: 0.1418, step time: 0.1095\n",
      "138/223, train_loss: 0.1194, step time: 0.0999\n",
      "139/223, train_loss: 0.1381, step time: 0.0999\n",
      "140/223, train_loss: 0.1446, step time: 0.0999\n",
      "141/223, train_loss: 0.1198, step time: 0.1045\n",
      "142/223, train_loss: 0.1294, step time: 0.0992\n",
      "143/223, train_loss: 0.1515, step time: 0.0991\n",
      "144/223, train_loss: 0.1275, step time: 0.1008\n",
      "145/223, train_loss: 0.1142, step time: 0.1024\n",
      "146/223, train_loss: 0.1314, step time: 0.0995\n",
      "147/223, train_loss: 0.1325, step time: 0.0993\n",
      "148/223, train_loss: 0.1262, step time: 0.0997\n",
      "149/223, train_loss: 0.1259, step time: 0.1038\n",
      "150/223, train_loss: 0.1271, step time: 0.0996\n",
      "151/223, train_loss: 0.1335, step time: 0.0992\n",
      "152/223, train_loss: 0.1206, step time: 0.1006\n",
      "153/223, train_loss: 0.1180, step time: 0.1009\n",
      "154/223, train_loss: 0.1224, step time: 0.1111\n",
      "155/223, train_loss: 0.1130, step time: 0.1413\n",
      "156/223, train_loss: 0.1182, step time: 0.1002\n",
      "157/223, train_loss: 0.1216, step time: 0.1005\n",
      "158/223, train_loss: 0.1301, step time: 0.1074\n",
      "159/223, train_loss: 0.1333, step time: 0.1000\n",
      "160/223, train_loss: 0.1279, step time: 0.1006\n",
      "161/223, train_loss: 0.1279, step time: 0.1581\n",
      "162/223, train_loss: 0.1190, step time: 0.1001\n",
      "163/223, train_loss: 0.1195, step time: 0.1027\n",
      "164/223, train_loss: 0.1240, step time: 0.1014\n",
      "165/223, train_loss: 0.1349, step time: 0.0997\n",
      "166/223, train_loss: 0.1315, step time: 0.1020\n",
      "167/223, train_loss: 0.1239, step time: 0.1056\n",
      "168/223, train_loss: 0.1335, step time: 0.1017\n",
      "169/223, train_loss: 0.1323, step time: 0.1023\n",
      "170/223, train_loss: 0.1234, step time: 0.1152\n",
      "171/223, train_loss: 0.1179, step time: 0.1060\n",
      "172/223, train_loss: 0.1313, step time: 0.1002\n",
      "173/223, train_loss: 0.1239, step time: 0.1175\n",
      "174/223, train_loss: 0.1194, step time: 0.1003\n",
      "175/223, train_loss: 0.1240, step time: 0.0998\n",
      "176/223, train_loss: 0.1328, step time: 0.0992\n",
      "177/223, train_loss: 0.1243, step time: 0.1088\n",
      "178/223, train_loss: 0.1400, step time: 0.1059\n",
      "179/223, train_loss: 0.1386, step time: 0.1003\n",
      "180/223, train_loss: 0.1101, step time: 0.1047\n",
      "181/223, train_loss: 0.1448, step time: 0.1000\n",
      "182/223, train_loss: 0.1340, step time: 0.1070\n",
      "183/223, train_loss: 0.1214, step time: 0.1084\n",
      "184/223, train_loss: 0.1344, step time: 0.1054\n",
      "185/223, train_loss: 0.1191, step time: 0.1050\n",
      "186/223, train_loss: 0.1205, step time: 0.1021\n",
      "187/223, train_loss: 0.1261, step time: 0.1022\n",
      "188/223, train_loss: 0.1262, step time: 0.1048\n",
      "189/223, train_loss: 0.1330, step time: 0.1003\n",
      "190/223, train_loss: 0.1158, step time: 0.0986\n",
      "191/223, train_loss: 0.1189, step time: 0.1057\n",
      "192/223, train_loss: 0.1114, step time: 0.1103\n",
      "193/223, train_loss: 0.1367, step time: 0.1098\n",
      "194/223, train_loss: 0.1199, step time: 0.1119\n",
      "195/223, train_loss: 0.1371, step time: 0.1242\n",
      "196/223, train_loss: 0.1248, step time: 0.1215\n",
      "197/223, train_loss: 0.1263, step time: 0.1048\n",
      "198/223, train_loss: 0.1227, step time: 0.1246\n",
      "199/223, train_loss: 0.1345, step time: 0.1109\n",
      "200/223, train_loss: 0.1280, step time: 0.1150\n",
      "201/223, train_loss: 0.1297, step time: 0.1197\n",
      "202/223, train_loss: 0.1217, step time: 0.1113\n",
      "203/223, train_loss: 0.1242, step time: 0.1124\n",
      "204/223, train_loss: 0.1281, step time: 0.1008\n",
      "205/223, train_loss: 0.1246, step time: 0.1053\n",
      "206/223, train_loss: 0.1292, step time: 0.1015\n",
      "207/223, train_loss: 0.1184, step time: 0.1082\n",
      "208/223, train_loss: 0.1220, step time: 0.1307\n",
      "209/223, train_loss: 0.1461, step time: 0.1139\n",
      "210/223, train_loss: 0.1200, step time: 0.1256\n",
      "211/223, train_loss: 0.1120, step time: 0.1010\n",
      "212/223, train_loss: 0.1342, step time: 0.1048\n",
      "213/223, train_loss: 0.1401, step time: 0.1180\n",
      "214/223, train_loss: 0.1460, step time: 0.1004\n",
      "215/223, train_loss: 0.1299, step time: 0.1199\n",
      "216/223, train_loss: 0.1278, step time: 0.0997\n",
      "217/223, train_loss: 0.1307, step time: 0.1357\n",
      "218/223, train_loss: 0.1325, step time: 0.1001\n",
      "219/223, train_loss: 0.1223, step time: 0.1015\n",
      "220/223, train_loss: 0.1135, step time: 0.1004\n",
      "221/223, train_loss: 0.1385, step time: 0.1035\n",
      "222/223, train_loss: 0.1328, step time: 0.0998\n",
      "223/223, train_loss: 0.1240, step time: 0.0997\n",
      "epoch 53 average loss: 0.1294\n",
      "time consuming of epoch 53 is: 91.7101\n",
      "----------\n",
      "epoch 54/300\n",
      "1/223, train_loss: 0.1259, step time: 0.1033\n",
      "2/223, train_loss: 0.1256, step time: 0.1099\n",
      "3/223, train_loss: 0.1497, step time: 0.1294\n",
      "4/223, train_loss: 0.1331, step time: 0.1146\n",
      "5/223, train_loss: 0.1267, step time: 0.1095\n",
      "6/223, train_loss: 0.1349, step time: 0.1121\n",
      "7/223, train_loss: 0.1132, step time: 0.1064\n",
      "8/223, train_loss: 0.1384, step time: 0.1005\n",
      "9/223, train_loss: 0.1190, step time: 0.1107\n",
      "10/223, train_loss: 0.1217, step time: 0.1021\n",
      "11/223, train_loss: 0.1174, step time: 0.1149\n",
      "12/223, train_loss: 0.1098, step time: 0.1027\n",
      "13/223, train_loss: 0.1201, step time: 0.1235\n",
      "14/223, train_loss: 0.1201, step time: 0.1536\n",
      "15/223, train_loss: 0.1416, step time: 0.1155\n",
      "16/223, train_loss: 0.1385, step time: 0.1038\n",
      "17/223, train_loss: 0.1196, step time: 0.1091\n",
      "18/223, train_loss: 0.1415, step time: 0.1090\n",
      "19/223, train_loss: 0.1268, step time: 0.1249\n",
      "20/223, train_loss: 0.3139, step time: 0.1060\n",
      "21/223, train_loss: 0.1330, step time: 0.1089\n",
      "22/223, train_loss: 0.1252, step time: 0.1112\n",
      "23/223, train_loss: 0.1209, step time: 0.1162\n",
      "24/223, train_loss: 0.1251, step time: 0.1149\n",
      "25/223, train_loss: 0.1380, step time: 0.1142\n",
      "26/223, train_loss: 0.1353, step time: 0.1040\n",
      "27/223, train_loss: 0.1315, step time: 0.0996\n",
      "28/223, train_loss: 0.1221, step time: 0.1024\n",
      "29/223, train_loss: 0.1340, step time: 0.1113\n",
      "30/223, train_loss: 0.1283, step time: 0.1045\n",
      "31/223, train_loss: 0.1389, step time: 0.1009\n",
      "32/223, train_loss: 0.1157, step time: 0.1001\n",
      "33/223, train_loss: 0.1323, step time: 0.1007\n",
      "34/223, train_loss: 0.1235, step time: 0.1067\n",
      "35/223, train_loss: 0.1274, step time: 0.1147\n",
      "36/223, train_loss: 0.1292, step time: 0.1098\n",
      "37/223, train_loss: 0.1250, step time: 0.1111\n",
      "38/223, train_loss: 0.1216, step time: 0.1030\n",
      "39/223, train_loss: 0.1395, step time: 0.1021\n",
      "40/223, train_loss: 0.1244, step time: 0.1013\n",
      "41/223, train_loss: 0.1472, step time: 0.1109\n",
      "42/223, train_loss: 0.1195, step time: 0.1013\n",
      "43/223, train_loss: 0.1247, step time: 0.1202\n",
      "44/223, train_loss: 0.1301, step time: 0.1003\n",
      "45/223, train_loss: 0.1395, step time: 0.1080\n",
      "46/223, train_loss: 0.1251, step time: 0.1005\n",
      "47/223, train_loss: 0.1199, step time: 0.1003\n",
      "48/223, train_loss: 0.1482, step time: 0.1055\n",
      "49/223, train_loss: 0.1241, step time: 0.1103\n",
      "50/223, train_loss: 0.1324, step time: 0.0999\n",
      "51/223, train_loss: 0.1285, step time: 0.1138\n",
      "52/223, train_loss: 0.1234, step time: 0.1153\n",
      "53/223, train_loss: 0.1324, step time: 0.1226\n",
      "54/223, train_loss: 0.1426, step time: 0.1106\n",
      "55/223, train_loss: 0.1292, step time: 0.1053\n",
      "56/223, train_loss: 0.1177, step time: 0.0995\n",
      "57/223, train_loss: 0.1078, step time: 0.1003\n",
      "58/223, train_loss: 0.1279, step time: 0.1203\n",
      "59/223, train_loss: 0.1258, step time: 0.1009\n",
      "60/223, train_loss: 0.1147, step time: 0.1003\n",
      "61/223, train_loss: 0.1465, step time: 0.1358\n",
      "62/223, train_loss: 0.1261, step time: 0.1122\n",
      "63/223, train_loss: 0.1283, step time: 0.1124\n",
      "64/223, train_loss: 0.1291, step time: 0.1102\n",
      "65/223, train_loss: 0.1114, step time: 0.1006\n",
      "66/223, train_loss: 0.1304, step time: 0.1115\n",
      "67/223, train_loss: 0.1516, step time: 0.1113\n",
      "68/223, train_loss: 0.1225, step time: 0.1275\n",
      "69/223, train_loss: 0.1202, step time: 0.1005\n",
      "70/223, train_loss: 0.1217, step time: 0.1066\n",
      "71/223, train_loss: 0.1334, step time: 0.1161\n",
      "72/223, train_loss: 0.1119, step time: 0.1078\n",
      "73/223, train_loss: 0.1190, step time: 0.1067\n",
      "74/223, train_loss: 0.1229, step time: 0.0993\n",
      "75/223, train_loss: 0.1342, step time: 0.1028\n",
      "76/223, train_loss: 0.1235, step time: 0.1124\n",
      "77/223, train_loss: 0.1502, step time: 0.1073\n",
      "78/223, train_loss: 0.1273, step time: 0.1007\n",
      "79/223, train_loss: 0.1225, step time: 0.1082\n",
      "80/223, train_loss: 0.1583, step time: 0.1005\n",
      "81/223, train_loss: 0.1361, step time: 0.1210\n",
      "82/223, train_loss: 0.1264, step time: 0.1038\n",
      "83/223, train_loss: 0.1463, step time: 0.1004\n",
      "84/223, train_loss: 0.1199, step time: 0.1071\n",
      "85/223, train_loss: 0.1352, step time: 0.1063\n",
      "86/223, train_loss: 0.1330, step time: 0.1194\n",
      "87/223, train_loss: 0.1382, step time: 0.0999\n",
      "88/223, train_loss: 0.1400, step time: 0.1004\n",
      "89/223, train_loss: 0.1318, step time: 0.1244\n",
      "90/223, train_loss: 0.1280, step time: 0.1194\n",
      "91/223, train_loss: 0.1149, step time: 0.1007\n",
      "92/223, train_loss: 0.1109, step time: 0.1001\n",
      "93/223, train_loss: 0.1238, step time: 0.0997\n",
      "94/223, train_loss: 0.1333, step time: 0.1136\n",
      "95/223, train_loss: 0.1216, step time: 0.1157\n",
      "96/223, train_loss: 0.1197, step time: 0.1117\n",
      "97/223, train_loss: 0.1174, step time: 0.1123\n",
      "98/223, train_loss: 0.1349, step time: 0.1227\n",
      "99/223, train_loss: 0.1286, step time: 0.1268\n",
      "100/223, train_loss: 0.1230, step time: 0.1071\n",
      "101/223, train_loss: 0.1207, step time: 0.1109\n",
      "102/223, train_loss: 0.1381, step time: 0.1070\n",
      "103/223, train_loss: 0.1230, step time: 0.1143\n",
      "104/223, train_loss: 0.1304, step time: 0.1002\n",
      "105/223, train_loss: 0.1131, step time: 0.1039\n",
      "106/223, train_loss: 0.1271, step time: 0.1088\n",
      "107/223, train_loss: 0.1402, step time: 0.1014\n",
      "108/223, train_loss: 0.1271, step time: 0.1007\n",
      "109/223, train_loss: 0.1454, step time: 0.1065\n",
      "110/223, train_loss: 0.1477, step time: 0.1018\n",
      "111/223, train_loss: 0.1382, step time: 0.1005\n",
      "112/223, train_loss: 0.1401, step time: 0.1006\n",
      "113/223, train_loss: 0.1241, step time: 0.1256\n",
      "114/223, train_loss: 0.1215, step time: 0.1025\n",
      "115/223, train_loss: 0.1239, step time: 0.1012\n",
      "116/223, train_loss: 0.1422, step time: 0.1010\n",
      "117/223, train_loss: 0.1185, step time: 0.1000\n",
      "118/223, train_loss: 0.1235, step time: 0.0997\n",
      "119/223, train_loss: 0.1275, step time: 0.1003\n",
      "120/223, train_loss: 0.1311, step time: 0.0997\n",
      "121/223, train_loss: 0.1213, step time: 0.0994\n",
      "122/223, train_loss: 0.1228, step time: 0.1001\n",
      "123/223, train_loss: 0.1246, step time: 0.1203\n",
      "124/223, train_loss: 0.1315, step time: 0.0996\n",
      "125/223, train_loss: 0.1165, step time: 0.1086\n",
      "126/223, train_loss: 0.1247, step time: 0.1159\n",
      "127/223, train_loss: 0.1405, step time: 0.1159\n",
      "128/223, train_loss: 0.1232, step time: 0.1075\n",
      "129/223, train_loss: 0.1321, step time: 0.1129\n",
      "130/223, train_loss: 0.1225, step time: 0.1125\n",
      "131/223, train_loss: 0.1211, step time: 0.1244\n",
      "132/223, train_loss: 0.1405, step time: 0.0997\n",
      "133/223, train_loss: 0.1299, step time: 0.1064\n",
      "134/223, train_loss: 0.1275, step time: 0.1353\n",
      "135/223, train_loss: 0.1314, step time: 0.1344\n",
      "136/223, train_loss: 0.1235, step time: 0.1073\n",
      "137/223, train_loss: 0.1292, step time: 0.1019\n",
      "138/223, train_loss: 0.1137, step time: 0.1227\n",
      "139/223, train_loss: 0.1262, step time: 0.1341\n",
      "140/223, train_loss: 0.1346, step time: 0.1044\n",
      "141/223, train_loss: 0.1319, step time: 0.1145\n",
      "142/223, train_loss: 0.1209, step time: 0.1151\n",
      "143/223, train_loss: 0.1196, step time: 0.1165\n",
      "144/223, train_loss: 0.1318, step time: 0.1145\n",
      "145/223, train_loss: 0.1252, step time: 0.1013\n",
      "146/223, train_loss: 0.1129, step time: 0.1112\n",
      "147/223, train_loss: 0.1442, step time: 0.1180\n",
      "148/223, train_loss: 0.1159, step time: 0.1221\n",
      "149/223, train_loss: 0.1273, step time: 0.1005\n",
      "150/223, train_loss: 0.1387, step time: 0.1058\n",
      "151/223, train_loss: 0.1473, step time: 0.1023\n",
      "152/223, train_loss: 0.1307, step time: 0.0998\n",
      "153/223, train_loss: 0.1272, step time: 0.1242\n",
      "154/223, train_loss: 0.1230, step time: 0.1017\n",
      "155/223, train_loss: 0.1293, step time: 0.1003\n",
      "156/223, train_loss: 0.1172, step time: 0.1019\n",
      "157/223, train_loss: 0.1348, step time: 0.1092\n",
      "158/223, train_loss: 0.1114, step time: 0.1071\n",
      "159/223, train_loss: 0.1187, step time: 0.1048\n",
      "160/223, train_loss: 0.1262, step time: 0.1142\n",
      "161/223, train_loss: 0.1239, step time: 0.1099\n",
      "162/223, train_loss: 0.1327, step time: 0.1140\n",
      "163/223, train_loss: 0.1311, step time: 0.1158\n",
      "164/223, train_loss: 0.1253, step time: 0.1195\n",
      "165/223, train_loss: 0.1209, step time: 0.1110\n",
      "166/223, train_loss: 0.1161, step time: 0.1187\n",
      "167/223, train_loss: 0.1331, step time: 0.1175\n",
      "168/223, train_loss: 0.1150, step time: 0.1150\n",
      "169/223, train_loss: 0.1233, step time: 0.1092\n",
      "170/223, train_loss: 0.1245, step time: 0.1249\n",
      "171/223, train_loss: 0.1408, step time: 0.1307\n",
      "172/223, train_loss: 0.1342, step time: 0.1052\n",
      "173/223, train_loss: 0.1396, step time: 0.0998\n",
      "174/223, train_loss: 0.1316, step time: 0.0997\n",
      "175/223, train_loss: 0.1270, step time: 0.1000\n",
      "176/223, train_loss: 0.1265, step time: 0.1422\n",
      "177/223, train_loss: 0.1267, step time: 0.1149\n",
      "178/223, train_loss: 0.1198, step time: 0.1008\n",
      "179/223, train_loss: 0.1258, step time: 0.1003\n",
      "180/223, train_loss: 0.1432, step time: 0.1025\n",
      "181/223, train_loss: 0.1185, step time: 0.1071\n",
      "182/223, train_loss: 0.1254, step time: 0.1012\n",
      "183/223, train_loss: 0.1337, step time: 0.1003\n",
      "184/223, train_loss: 0.1157, step time: 0.1000\n",
      "185/223, train_loss: 0.1261, step time: 0.1217\n",
      "186/223, train_loss: 0.1277, step time: 0.1113\n",
      "187/223, train_loss: 0.1307, step time: 0.1167\n",
      "188/223, train_loss: 0.1204, step time: 0.1236\n",
      "189/223, train_loss: 0.1222, step time: 0.1001\n",
      "190/223, train_loss: 0.1267, step time: 0.1002\n",
      "191/223, train_loss: 0.1200, step time: 0.1000\n",
      "192/223, train_loss: 0.1294, step time: 0.1139\n",
      "193/223, train_loss: 0.1234, step time: 0.1003\n",
      "194/223, train_loss: 0.1136, step time: 0.1115\n",
      "195/223, train_loss: 0.1302, step time: 0.1134\n",
      "196/223, train_loss: 0.1218, step time: 0.1004\n",
      "197/223, train_loss: 0.1325, step time: 0.1006\n",
      "198/223, train_loss: 0.1250, step time: 0.1002\n",
      "199/223, train_loss: 0.1258, step time: 0.0997\n",
      "200/223, train_loss: 0.1264, step time: 0.1236\n",
      "201/223, train_loss: 0.1225, step time: 0.1028\n",
      "202/223, train_loss: 0.1151, step time: 0.0993\n",
      "203/223, train_loss: 0.1267, step time: 0.0990\n",
      "204/223, train_loss: 0.1262, step time: 0.0995\n",
      "205/223, train_loss: 0.1434, step time: 0.1001\n",
      "206/223, train_loss: 0.1184, step time: 0.1000\n",
      "207/223, train_loss: 0.1226, step time: 0.1007\n",
      "208/223, train_loss: 0.1256, step time: 0.1018\n",
      "209/223, train_loss: 0.1413, step time: 0.1485\n",
      "210/223, train_loss: 0.1338, step time: 0.1373\n",
      "211/223, train_loss: 0.1307, step time: 0.0998\n",
      "212/223, train_loss: 0.1318, step time: 0.1005\n",
      "213/223, train_loss: 0.1291, step time: 0.1042\n",
      "214/223, train_loss: 0.1328, step time: 0.0997\n",
      "215/223, train_loss: 0.1414, step time: 0.1001\n",
      "216/223, train_loss: 0.1267, step time: 0.1025\n",
      "217/223, train_loss: 0.1164, step time: 0.1008\n",
      "218/223, train_loss: 0.1221, step time: 0.0996\n",
      "219/223, train_loss: 0.1496, step time: 0.0998\n",
      "220/223, train_loss: 0.1224, step time: 0.1005\n",
      "221/223, train_loss: 0.1307, step time: 0.0994\n",
      "222/223, train_loss: 0.1142, step time: 0.0993\n",
      "223/223, train_loss: 0.1179, step time: 0.1000\n",
      "epoch 54 average loss: 0.1287\n",
      "time consuming of epoch 54 is: 87.7812\n",
      "----------\n",
      "epoch 55/300\n",
      "1/223, train_loss: 0.1360, step time: 0.1037\n",
      "2/223, train_loss: 0.1470, step time: 0.1119\n",
      "3/223, train_loss: 0.1298, step time: 0.1138\n",
      "4/223, train_loss: 0.1412, step time: 0.1138\n",
      "5/223, train_loss: 0.1504, step time: 0.1092\n",
      "6/223, train_loss: 0.1171, step time: 0.1088\n",
      "7/223, train_loss: 0.1380, step time: 0.1121\n",
      "8/223, train_loss: 0.1281, step time: 0.1099\n",
      "9/223, train_loss: 0.1268, step time: 0.1063\n",
      "10/223, train_loss: 0.1327, step time: 0.1078\n",
      "11/223, train_loss: 0.1161, step time: 0.1062\n",
      "12/223, train_loss: 0.1314, step time: 0.1272\n",
      "13/223, train_loss: 0.1162, step time: 0.1112\n",
      "14/223, train_loss: 0.1447, step time: 0.1458\n",
      "15/223, train_loss: 0.1371, step time: 0.1005\n",
      "16/223, train_loss: 0.1316, step time: 0.1067\n",
      "17/223, train_loss: 0.1184, step time: 0.0998\n",
      "18/223, train_loss: 0.1375, step time: 0.1039\n",
      "19/223, train_loss: 0.1273, step time: 0.0996\n",
      "20/223, train_loss: 0.1361, step time: 0.1095\n",
      "21/223, train_loss: 0.1428, step time: 0.1111\n",
      "22/223, train_loss: 0.1118, step time: 0.1006\n",
      "23/223, train_loss: 0.1190, step time: 0.1005\n",
      "24/223, train_loss: 0.1167, step time: 0.1002\n",
      "25/223, train_loss: 0.1262, step time: 0.1100\n",
      "26/223, train_loss: 0.1272, step time: 0.0998\n",
      "27/223, train_loss: 0.1265, step time: 0.0993\n",
      "28/223, train_loss: 0.1443, step time: 0.1049\n",
      "29/223, train_loss: 0.1234, step time: 0.1057\n",
      "30/223, train_loss: 0.1170, step time: 0.1004\n",
      "31/223, train_loss: 0.1252, step time: 0.1004\n",
      "32/223, train_loss: 0.1272, step time: 0.1014\n",
      "33/223, train_loss: 0.1396, step time: 0.1016\n",
      "34/223, train_loss: 0.1325, step time: 0.1036\n",
      "35/223, train_loss: 0.1267, step time: 0.0997\n",
      "36/223, train_loss: 0.1297, step time: 0.0991\n",
      "37/223, train_loss: 0.1189, step time: 0.1070\n",
      "38/223, train_loss: 0.1202, step time: 0.1055\n",
      "39/223, train_loss: 0.1278, step time: 0.1243\n",
      "40/223, train_loss: 0.1232, step time: 0.1002\n",
      "41/223, train_loss: 0.1242, step time: 0.1386\n",
      "42/223, train_loss: 0.1131, step time: 0.1111\n",
      "43/223, train_loss: 0.1205, step time: 0.1149\n",
      "44/223, train_loss: 0.1125, step time: 0.1052\n",
      "45/223, train_loss: 0.1186, step time: 0.1063\n",
      "46/223, train_loss: 0.1234, step time: 0.1004\n",
      "47/223, train_loss: 0.1334, step time: 0.1045\n",
      "48/223, train_loss: 0.1342, step time: 0.1006\n",
      "49/223, train_loss: 0.1240, step time: 0.1011\n",
      "50/223, train_loss: 0.1367, step time: 0.1049\n",
      "51/223, train_loss: 0.1092, step time: 0.1134\n",
      "52/223, train_loss: 0.1266, step time: 0.0997\n",
      "53/223, train_loss: 0.1244, step time: 0.1085\n",
      "54/223, train_loss: 0.1301, step time: 0.1183\n",
      "55/223, train_loss: 0.1241, step time: 0.1020\n",
      "56/223, train_loss: 0.1155, step time: 0.1118\n",
      "57/223, train_loss: 0.1348, step time: 0.1254\n",
      "58/223, train_loss: 0.1201, step time: 0.1103\n",
      "59/223, train_loss: 0.1388, step time: 0.1146\n",
      "60/223, train_loss: 0.1276, step time: 0.1158\n",
      "61/223, train_loss: 0.1229, step time: 0.1015\n",
      "62/223, train_loss: 0.1177, step time: 0.1092\n",
      "63/223, train_loss: 0.1192, step time: 0.1067\n",
      "64/223, train_loss: 0.1291, step time: 0.1004\n",
      "65/223, train_loss: 0.1469, step time: 0.1034\n",
      "66/223, train_loss: 0.1211, step time: 0.1183\n",
      "67/223, train_loss: 0.1135, step time: 0.1125\n",
      "68/223, train_loss: 0.1161, step time: 0.1018\n",
      "69/223, train_loss: 0.1280, step time: 0.1105\n",
      "70/223, train_loss: 0.1212, step time: 0.1232\n",
      "71/223, train_loss: 0.1367, step time: 0.1001\n",
      "72/223, train_loss: 0.1190, step time: 0.1009\n",
      "73/223, train_loss: 0.1228, step time: 0.1098\n",
      "74/223, train_loss: 0.1166, step time: 0.1160\n",
      "75/223, train_loss: 0.1199, step time: 0.1011\n",
      "76/223, train_loss: 0.1302, step time: 0.1145\n",
      "77/223, train_loss: 0.1227, step time: 0.1042\n",
      "78/223, train_loss: 0.1208, step time: 0.1364\n",
      "79/223, train_loss: 0.1397, step time: 0.1275\n",
      "80/223, train_loss: 0.1341, step time: 0.1084\n",
      "81/223, train_loss: 0.1264, step time: 0.1118\n",
      "82/223, train_loss: 0.1246, step time: 0.1001\n",
      "83/223, train_loss: 0.1359, step time: 0.1006\n",
      "84/223, train_loss: 0.1261, step time: 0.1036\n",
      "85/223, train_loss: 0.1113, step time: 0.1050\n",
      "86/223, train_loss: 0.1304, step time: 0.1287\n",
      "87/223, train_loss: 0.1413, step time: 0.1149\n",
      "88/223, train_loss: 0.1365, step time: 0.1136\n",
      "89/223, train_loss: 0.1227, step time: 0.1062\n",
      "90/223, train_loss: 0.1288, step time: 0.1191\n",
      "91/223, train_loss: 0.1181, step time: 0.1002\n",
      "92/223, train_loss: 0.1235, step time: 0.1006\n",
      "93/223, train_loss: 0.1348, step time: 0.1215\n",
      "94/223, train_loss: 0.1373, step time: 0.1034\n",
      "95/223, train_loss: 0.1408, step time: 0.1282\n",
      "96/223, train_loss: 0.1317, step time: 0.1080\n",
      "97/223, train_loss: 0.1262, step time: 0.1180\n",
      "98/223, train_loss: 0.1184, step time: 0.1100\n",
      "99/223, train_loss: 0.1184, step time: 0.1003\n",
      "100/223, train_loss: 0.1182, step time: 0.1023\n",
      "101/223, train_loss: 0.1221, step time: 0.1042\n",
      "102/223, train_loss: 0.1213, step time: 0.0997\n",
      "103/223, train_loss: 0.1209, step time: 0.1230\n",
      "104/223, train_loss: 0.1331, step time: 0.1009\n",
      "105/223, train_loss: 0.1198, step time: 0.1269\n",
      "106/223, train_loss: 0.1293, step time: 0.1008\n",
      "107/223, train_loss: 0.1339, step time: 0.1010\n",
      "108/223, train_loss: 0.1342, step time: 0.1014\n",
      "109/223, train_loss: 0.1213, step time: 0.1062\n",
      "110/223, train_loss: 0.1242, step time: 0.1074\n",
      "111/223, train_loss: 0.1270, step time: 0.1067\n",
      "112/223, train_loss: 0.1237, step time: 0.1003\n",
      "113/223, train_loss: 0.1425, step time: 0.1007\n",
      "114/223, train_loss: 0.1324, step time: 0.1017\n",
      "115/223, train_loss: 0.1346, step time: 0.1011\n",
      "116/223, train_loss: 0.1327, step time: 0.1008\n",
      "117/223, train_loss: 0.1344, step time: 0.1077\n",
      "118/223, train_loss: 0.1321, step time: 0.1063\n",
      "119/223, train_loss: 0.1363, step time: 0.1145\n",
      "120/223, train_loss: 0.1258, step time: 0.1007\n",
      "121/223, train_loss: 0.1290, step time: 0.1002\n",
      "122/223, train_loss: 0.1388, step time: 0.1035\n",
      "123/223, train_loss: 0.1461, step time: 0.1000\n",
      "124/223, train_loss: 0.1118, step time: 0.1078\n",
      "125/223, train_loss: 0.1340, step time: 0.1268\n",
      "126/223, train_loss: 0.1268, step time: 0.1002\n",
      "127/223, train_loss: 0.1172, step time: 0.1001\n",
      "128/223, train_loss: 0.1345, step time: 0.1054\n",
      "129/223, train_loss: 0.1284, step time: 0.1192\n",
      "130/223, train_loss: 0.1177, step time: 0.1018\n",
      "131/223, train_loss: 0.1245, step time: 0.0993\n",
      "132/223, train_loss: 0.1320, step time: 0.1127\n",
      "133/223, train_loss: 0.1497, step time: 0.1016\n",
      "134/223, train_loss: 0.1276, step time: 0.1009\n",
      "135/223, train_loss: 0.1219, step time: 0.1006\n",
      "136/223, train_loss: 0.1410, step time: 0.1106\n",
      "137/223, train_loss: 0.1146, step time: 0.1082\n",
      "138/223, train_loss: 0.1239, step time: 0.1005\n",
      "139/223, train_loss: 0.1192, step time: 0.1006\n",
      "140/223, train_loss: 0.1148, step time: 0.1000\n",
      "141/223, train_loss: 0.1292, step time: 0.1238\n",
      "142/223, train_loss: 0.1428, step time: 0.1001\n",
      "143/223, train_loss: 0.1253, step time: 0.1001\n",
      "144/223, train_loss: 0.1361, step time: 0.0999\n",
      "145/223, train_loss: 0.1308, step time: 0.1005\n",
      "146/223, train_loss: 0.1243, step time: 0.1007\n",
      "147/223, train_loss: 0.1358, step time: 0.1009\n",
      "148/223, train_loss: 0.1283, step time: 0.1011\n",
      "149/223, train_loss: 0.1399, step time: 0.0996\n",
      "150/223, train_loss: 0.1134, step time: 0.1084\n",
      "151/223, train_loss: 0.1482, step time: 0.1026\n",
      "152/223, train_loss: 0.1238, step time: 0.1192\n",
      "153/223, train_loss: 0.1106, step time: 0.1131\n",
      "154/223, train_loss: 0.1225, step time: 0.1109\n",
      "155/223, train_loss: 0.1208, step time: 0.1184\n",
      "156/223, train_loss: 0.1119, step time: 0.1003\n",
      "157/223, train_loss: 0.1113, step time: 0.1087\n",
      "158/223, train_loss: 0.1344, step time: 0.1095\n",
      "159/223, train_loss: 0.1275, step time: 0.1104\n",
      "160/223, train_loss: 0.1352, step time: 0.1152\n",
      "161/223, train_loss: 0.1282, step time: 0.1158\n",
      "162/223, train_loss: 0.1343, step time: 0.1155\n",
      "163/223, train_loss: 0.1317, step time: 0.1043\n",
      "164/223, train_loss: 0.1281, step time: 0.1276\n",
      "165/223, train_loss: 0.1303, step time: 0.1004\n",
      "166/223, train_loss: 0.1476, step time: 0.1008\n",
      "167/223, train_loss: 0.1292, step time: 0.1001\n",
      "168/223, train_loss: 0.1363, step time: 0.1025\n",
      "169/223, train_loss: 0.1317, step time: 0.1051\n",
      "170/223, train_loss: 0.1291, step time: 0.1133\n",
      "171/223, train_loss: 0.1213, step time: 0.1034\n",
      "172/223, train_loss: 0.1281, step time: 0.1238\n",
      "173/223, train_loss: 0.1338, step time: 0.1154\n",
      "174/223, train_loss: 0.1327, step time: 0.1013\n",
      "175/223, train_loss: 0.1197, step time: 0.1378\n",
      "176/223, train_loss: 0.1261, step time: 0.1073\n",
      "177/223, train_loss: 0.1361, step time: 0.1102\n",
      "178/223, train_loss: 0.1178, step time: 0.1003\n",
      "179/223, train_loss: 0.1115, step time: 0.1338\n",
      "180/223, train_loss: 0.1221, step time: 0.1162\n",
      "181/223, train_loss: 0.1220, step time: 0.1046\n",
      "182/223, train_loss: 0.1358, step time: 0.1072\n",
      "183/223, train_loss: 0.1303, step time: 0.1148\n",
      "184/223, train_loss: 0.1202, step time: 0.1081\n",
      "185/223, train_loss: 0.1242, step time: 0.1109\n",
      "186/223, train_loss: 0.1162, step time: 0.1107\n",
      "187/223, train_loss: 0.1431, step time: 0.1218\n",
      "188/223, train_loss: 0.1140, step time: 0.1076\n",
      "189/223, train_loss: 0.1239, step time: 0.1048\n",
      "190/223, train_loss: 0.1283, step time: 0.1108\n",
      "191/223, train_loss: 0.1256, step time: 0.1331\n",
      "192/223, train_loss: 0.1285, step time: 0.1041\n",
      "193/223, train_loss: 0.1390, step time: 0.1011\n",
      "194/223, train_loss: 0.1232, step time: 0.1357\n",
      "195/223, train_loss: 0.1382, step time: 0.1089\n",
      "196/223, train_loss: 0.1156, step time: 0.1105\n",
      "197/223, train_loss: 0.1250, step time: 0.1102\n",
      "198/223, train_loss: 0.1372, step time: 0.1146\n",
      "199/223, train_loss: 0.1216, step time: 0.1274\n",
      "200/223, train_loss: 0.1330, step time: 0.1108\n",
      "201/223, train_loss: 0.1228, step time: 0.1171\n",
      "202/223, train_loss: 0.1284, step time: 0.1129\n",
      "203/223, train_loss: 0.1385, step time: 0.1126\n",
      "204/223, train_loss: 0.1365, step time: 0.0997\n",
      "205/223, train_loss: 0.1274, step time: 0.1065\n",
      "206/223, train_loss: 0.1274, step time: 0.1058\n",
      "207/223, train_loss: 0.1183, step time: 0.1035\n",
      "208/223, train_loss: 0.1253, step time: 0.1045\n",
      "209/223, train_loss: 0.1124, step time: 0.1066\n",
      "210/223, train_loss: 0.1198, step time: 0.1000\n",
      "211/223, train_loss: 0.1199, step time: 0.1119\n",
      "212/223, train_loss: 0.1386, step time: 0.1080\n",
      "213/223, train_loss: 0.1438, step time: 0.1237\n",
      "214/223, train_loss: 0.1282, step time: 0.1228\n",
      "215/223, train_loss: 0.1148, step time: 0.0999\n",
      "216/223, train_loss: 0.1502, step time: 0.0999\n",
      "217/223, train_loss: 0.3189, step time: 0.0999\n",
      "218/223, train_loss: 0.1221, step time: 0.1001\n",
      "219/223, train_loss: 0.1152, step time: 0.0996\n",
      "220/223, train_loss: 0.1258, step time: 0.1002\n",
      "221/223, train_loss: 0.1296, step time: 0.0988\n",
      "222/223, train_loss: 0.1207, step time: 0.0989\n",
      "223/223, train_loss: 0.1287, step time: 0.1002\n",
      "epoch 55 average loss: 0.1284\n",
      "saved new best metric model\n",
      "current epoch: 55 current mean dice: 0.8340 tc: 0.9086 wt: 0.8465 et: 0.7471\n",
      "best mean dice: 0.8340 at epoch: 55\n",
      "time consuming of epoch 55 is: 89.9747\n",
      "----------\n",
      "epoch 56/300\n",
      "1/223, train_loss: 0.1183, step time: 0.1050\n",
      "2/223, train_loss: 0.1376, step time: 0.1056\n",
      "3/223, train_loss: 0.1161, step time: 0.1086\n",
      "4/223, train_loss: 0.1296, step time: 0.1007\n",
      "5/223, train_loss: 0.1315, step time: 0.1074\n",
      "6/223, train_loss: 0.1408, step time: 0.1002\n",
      "7/223, train_loss: 0.1334, step time: 0.1176\n",
      "8/223, train_loss: 0.1223, step time: 0.1013\n",
      "9/223, train_loss: 0.1268, step time: 0.1182\n",
      "10/223, train_loss: 0.1295, step time: 0.1188\n",
      "11/223, train_loss: 0.1187, step time: 0.1031\n",
      "12/223, train_loss: 0.1205, step time: 0.1013\n",
      "13/223, train_loss: 0.1263, step time: 0.1204\n",
      "14/223, train_loss: 0.1410, step time: 0.1063\n",
      "15/223, train_loss: 0.1422, step time: 0.1008\n",
      "16/223, train_loss: 0.1393, step time: 0.0989\n",
      "17/223, train_loss: 0.1268, step time: 0.1102\n",
      "18/223, train_loss: 0.1349, step time: 0.1019\n",
      "19/223, train_loss: 0.1130, step time: 0.1018\n",
      "20/223, train_loss: 0.1258, step time: 0.1004\n",
      "21/223, train_loss: 0.1350, step time: 0.1142\n",
      "22/223, train_loss: 0.1296, step time: 0.0988\n",
      "23/223, train_loss: 0.1455, step time: 0.1109\n",
      "24/223, train_loss: 0.1227, step time: 0.1141\n",
      "25/223, train_loss: 0.1206, step time: 0.1079\n",
      "26/223, train_loss: 0.1218, step time: 0.1247\n",
      "27/223, train_loss: 0.1283, step time: 0.1311\n",
      "28/223, train_loss: 0.1320, step time: 0.1124\n",
      "29/223, train_loss: 0.1291, step time: 0.1126\n",
      "30/223, train_loss: 0.1166, step time: 0.1179\n",
      "31/223, train_loss: 0.1203, step time: 0.1098\n",
      "32/223, train_loss: 0.1274, step time: 0.1331\n",
      "33/223, train_loss: 0.1408, step time: 0.0994\n",
      "34/223, train_loss: 0.1245, step time: 0.1004\n",
      "35/223, train_loss: 0.1356, step time: 0.1004\n",
      "36/223, train_loss: 0.1281, step time: 0.1011\n",
      "37/223, train_loss: 0.1267, step time: 0.1002\n",
      "38/223, train_loss: 0.1263, step time: 0.1004\n",
      "39/223, train_loss: 0.1448, step time: 0.1004\n",
      "40/223, train_loss: 0.1265, step time: 0.1017\n",
      "41/223, train_loss: 0.1279, step time: 0.1008\n",
      "42/223, train_loss: 0.1273, step time: 0.0990\n",
      "43/223, train_loss: 0.1497, step time: 0.0994\n",
      "44/223, train_loss: 0.1221, step time: 0.0996\n",
      "45/223, train_loss: 0.1304, step time: 0.1074\n",
      "46/223, train_loss: 0.1321, step time: 0.0992\n",
      "47/223, train_loss: 0.1394, step time: 0.0994\n",
      "48/223, train_loss: 0.1467, step time: 0.0980\n",
      "49/223, train_loss: 0.1155, step time: 0.1003\n",
      "50/223, train_loss: 0.1335, step time: 0.1002\n",
      "51/223, train_loss: 0.1304, step time: 0.1003\n",
      "52/223, train_loss: 0.1211, step time: 0.1057\n",
      "53/223, train_loss: 0.1249, step time: 0.1005\n",
      "54/223, train_loss: 0.1281, step time: 0.1045\n",
      "55/223, train_loss: 0.1253, step time: 0.1203\n",
      "56/223, train_loss: 0.1380, step time: 0.1145\n",
      "57/223, train_loss: 0.1206, step time: 0.1093\n",
      "58/223, train_loss: 0.1213, step time: 0.1009\n",
      "59/223, train_loss: 0.1223, step time: 0.1180\n",
      "60/223, train_loss: 0.1180, step time: 0.1016\n",
      "61/223, train_loss: 0.1223, step time: 0.0999\n",
      "62/223, train_loss: 0.1210, step time: 0.1098\n",
      "63/223, train_loss: 0.1246, step time: 0.1040\n",
      "64/223, train_loss: 0.1293, step time: 0.1003\n",
      "65/223, train_loss: 0.1248, step time: 0.1003\n",
      "66/223, train_loss: 0.1220, step time: 0.1144\n",
      "67/223, train_loss: 0.1320, step time: 0.1002\n",
      "68/223, train_loss: 0.1258, step time: 0.1056\n",
      "69/223, train_loss: 0.1210, step time: 0.1054\n",
      "70/223, train_loss: 0.1215, step time: 0.1103\n",
      "71/223, train_loss: 0.1286, step time: 0.1130\n",
      "72/223, train_loss: 0.1136, step time: 0.1112\n",
      "73/223, train_loss: 0.1283, step time: 0.1234\n",
      "74/223, train_loss: 0.1352, step time: 0.1028\n",
      "75/223, train_loss: 0.1323, step time: 0.0999\n",
      "76/223, train_loss: 0.1291, step time: 0.0996\n",
      "77/223, train_loss: 0.1410, step time: 0.1052\n",
      "78/223, train_loss: 0.1376, step time: 0.1242\n",
      "79/223, train_loss: 0.1214, step time: 0.1034\n",
      "80/223, train_loss: 0.1366, step time: 0.1030\n",
      "81/223, train_loss: 0.1371, step time: 0.1014\n",
      "82/223, train_loss: 0.1242, step time: 0.1068\n",
      "83/223, train_loss: 0.1378, step time: 0.1199\n",
      "84/223, train_loss: 0.1204, step time: 0.1093\n",
      "85/223, train_loss: 0.1104, step time: 0.1001\n",
      "86/223, train_loss: 0.1147, step time: 0.1134\n",
      "87/223, train_loss: 0.1147, step time: 0.1125\n",
      "88/223, train_loss: 0.1272, step time: 0.1001\n",
      "89/223, train_loss: 0.1264, step time: 0.0998\n",
      "90/223, train_loss: 0.1317, step time: 0.1160\n",
      "91/223, train_loss: 0.1234, step time: 0.1135\n",
      "92/223, train_loss: 0.1198, step time: 0.1015\n",
      "93/223, train_loss: 0.1311, step time: 0.1012\n",
      "94/223, train_loss: 0.1201, step time: 0.1220\n",
      "95/223, train_loss: 0.1268, step time: 0.1160\n",
      "96/223, train_loss: 0.1164, step time: 0.1054\n",
      "97/223, train_loss: 0.1337, step time: 0.0997\n",
      "98/223, train_loss: 0.1405, step time: 0.1031\n",
      "99/223, train_loss: 0.1458, step time: 0.1116\n",
      "100/223, train_loss: 0.1269, step time: 0.1113\n",
      "101/223, train_loss: 0.1314, step time: 0.1105\n",
      "102/223, train_loss: 0.1387, step time: 0.1078\n",
      "103/223, train_loss: 0.1163, step time: 0.1018\n",
      "104/223, train_loss: 0.1422, step time: 0.1015\n",
      "105/223, train_loss: 0.1272, step time: 0.1112\n",
      "106/223, train_loss: 0.1296, step time: 0.1090\n",
      "107/223, train_loss: 0.1343, step time: 0.1022\n",
      "108/223, train_loss: 0.1367, step time: 0.1250\n",
      "109/223, train_loss: 0.1404, step time: 0.0992\n",
      "110/223, train_loss: 0.1341, step time: 0.1088\n",
      "111/223, train_loss: 0.3265, step time: 0.1069\n",
      "112/223, train_loss: 0.1314, step time: 0.1125\n",
      "113/223, train_loss: 0.1362, step time: 0.1090\n",
      "114/223, train_loss: 0.1251, step time: 0.0993\n",
      "115/223, train_loss: 0.1244, step time: 0.1431\n",
      "116/223, train_loss: 0.1244, step time: 0.1262\n",
      "117/223, train_loss: 0.1240, step time: 0.1002\n",
      "118/223, train_loss: 0.1297, step time: 0.1006\n",
      "119/223, train_loss: 0.1418, step time: 0.1004\n",
      "120/223, train_loss: 0.1365, step time: 0.1288\n",
      "121/223, train_loss: 0.1186, step time: 0.1155\n",
      "122/223, train_loss: 0.1368, step time: 0.1036\n",
      "123/223, train_loss: 0.1225, step time: 0.1010\n",
      "124/223, train_loss: 0.1421, step time: 0.1022\n",
      "125/223, train_loss: 0.1111, step time: 0.0995\n",
      "126/223, train_loss: 0.1255, step time: 0.1000\n",
      "127/223, train_loss: 0.1181, step time: 0.1008\n",
      "128/223, train_loss: 0.1412, step time: 0.1045\n",
      "129/223, train_loss: 0.1379, step time: 0.1007\n",
      "130/223, train_loss: 0.1262, step time: 0.1012\n",
      "131/223, train_loss: 0.1426, step time: 0.0994\n",
      "132/223, train_loss: 0.1306, step time: 0.1125\n",
      "133/223, train_loss: 0.1281, step time: 0.1157\n",
      "134/223, train_loss: 0.1255, step time: 0.1123\n",
      "135/223, train_loss: 0.1269, step time: 0.1060\n",
      "136/223, train_loss: 0.1231, step time: 0.1197\n",
      "137/223, train_loss: 0.1471, step time: 0.0999\n",
      "138/223, train_loss: 0.1272, step time: 0.1000\n",
      "139/223, train_loss: 0.1239, step time: 0.1008\n",
      "140/223, train_loss: 0.1244, step time: 0.1037\n",
      "141/223, train_loss: 0.1238, step time: 0.1091\n",
      "142/223, train_loss: 0.1330, step time: 0.1126\n",
      "143/223, train_loss: 0.1355, step time: 0.1109\n",
      "144/223, train_loss: 0.1324, step time: 0.1084\n",
      "145/223, train_loss: 0.1317, step time: 0.1160\n",
      "146/223, train_loss: 0.1147, step time: 0.1088\n",
      "147/223, train_loss: 0.1180, step time: 0.1073\n",
      "148/223, train_loss: 0.1162, step time: 0.1167\n",
      "149/223, train_loss: 0.1348, step time: 0.1124\n",
      "150/223, train_loss: 0.1306, step time: 0.1022\n",
      "151/223, train_loss: 0.1326, step time: 0.1058\n",
      "152/223, train_loss: 0.1189, step time: 0.1133\n",
      "153/223, train_loss: 0.1239, step time: 0.1005\n",
      "154/223, train_loss: 0.1204, step time: 0.1003\n",
      "155/223, train_loss: 0.1244, step time: 0.1148\n",
      "156/223, train_loss: 0.1244, step time: 0.1194\n",
      "157/223, train_loss: 0.1375, step time: 0.1001\n",
      "158/223, train_loss: 0.1282, step time: 0.1006\n",
      "159/223, train_loss: 0.1185, step time: 0.1058\n",
      "160/223, train_loss: 0.1303, step time: 0.1007\n",
      "161/223, train_loss: 0.1260, step time: 0.0997\n",
      "162/223, train_loss: 0.1300, step time: 0.1001\n",
      "163/223, train_loss: 0.1163, step time: 0.1074\n",
      "164/223, train_loss: 0.1215, step time: 0.0996\n",
      "165/223, train_loss: 0.1050, step time: 0.1109\n",
      "166/223, train_loss: 0.1310, step time: 0.1043\n",
      "167/223, train_loss: 0.1338, step time: 0.1063\n",
      "168/223, train_loss: 0.1216, step time: 0.0992\n",
      "169/223, train_loss: 0.1172, step time: 0.1087\n",
      "170/223, train_loss: 0.1278, step time: 0.0992\n",
      "171/223, train_loss: 0.1323, step time: 0.1048\n",
      "172/223, train_loss: 0.1291, step time: 0.0986\n",
      "173/223, train_loss: 0.1246, step time: 0.1084\n",
      "174/223, train_loss: 0.1150, step time: 0.0987\n",
      "175/223, train_loss: 0.1198, step time: 0.1159\n",
      "176/223, train_loss: 0.1225, step time: 0.1018\n",
      "177/223, train_loss: 0.1285, step time: 0.1127\n",
      "178/223, train_loss: 0.1322, step time: 0.1012\n",
      "179/223, train_loss: 0.1455, step time: 0.1046\n",
      "180/223, train_loss: 0.1159, step time: 0.1102\n",
      "181/223, train_loss: 0.1158, step time: 0.1081\n",
      "182/223, train_loss: 0.1269, step time: 0.1008\n",
      "183/223, train_loss: 0.1250, step time: 0.1001\n",
      "184/223, train_loss: 0.1096, step time: 0.1154\n",
      "185/223, train_loss: 0.1190, step time: 0.1086\n",
      "186/223, train_loss: 0.1335, step time: 0.1242\n",
      "187/223, train_loss: 0.1204, step time: 0.1056\n",
      "188/223, train_loss: 0.1135, step time: 0.1172\n",
      "189/223, train_loss: 0.1133, step time: 0.1062\n",
      "190/223, train_loss: 0.1249, step time: 0.1063\n",
      "191/223, train_loss: 0.1236, step time: 0.1098\n",
      "192/223, train_loss: 0.1345, step time: 0.1129\n",
      "193/223, train_loss: 0.1257, step time: 0.1155\n",
      "194/223, train_loss: 0.1399, step time: 0.1147\n",
      "195/223, train_loss: 0.1303, step time: 0.1085\n",
      "196/223, train_loss: 0.1397, step time: 0.1003\n",
      "197/223, train_loss: 0.1414, step time: 0.1104\n",
      "198/223, train_loss: 0.1363, step time: 0.1002\n",
      "199/223, train_loss: 0.1203, step time: 0.1083\n",
      "200/223, train_loss: 0.1249, step time: 0.1068\n",
      "201/223, train_loss: 0.1093, step time: 0.1076\n",
      "202/223, train_loss: 0.1304, step time: 0.1011\n",
      "203/223, train_loss: 0.1169, step time: 0.1124\n",
      "204/223, train_loss: 0.1329, step time: 0.1151\n",
      "205/223, train_loss: 0.1329, step time: 0.1093\n",
      "206/223, train_loss: 0.1095, step time: 0.1228\n",
      "207/223, train_loss: 0.1361, step time: 0.1100\n",
      "208/223, train_loss: 0.1386, step time: 0.0998\n",
      "209/223, train_loss: 0.1216, step time: 0.1192\n",
      "210/223, train_loss: 0.1381, step time: 0.1246\n",
      "211/223, train_loss: 0.1314, step time: 0.1003\n",
      "212/223, train_loss: 0.1277, step time: 0.1098\n",
      "213/223, train_loss: 0.1278, step time: 0.1192\n",
      "214/223, train_loss: 0.1252, step time: 0.1161\n",
      "215/223, train_loss: 0.1252, step time: 0.1099\n",
      "216/223, train_loss: 0.1408, step time: 0.1084\n",
      "217/223, train_loss: 0.1235, step time: 0.1199\n",
      "218/223, train_loss: 0.1189, step time: 0.1124\n",
      "219/223, train_loss: 0.1268, step time: 0.0996\n",
      "220/223, train_loss: 0.1245, step time: 0.0997\n",
      "221/223, train_loss: 0.1310, step time: 0.0986\n",
      "222/223, train_loss: 0.1371, step time: 0.0991\n",
      "223/223, train_loss: 0.1241, step time: 0.0994\n",
      "epoch 56 average loss: 0.1287\n",
      "time consuming of epoch 56 is: 91.4077\n",
      "----------\n",
      "epoch 57/300\n",
      "1/223, train_loss: 0.1333, step time: 0.1088\n",
      "2/223, train_loss: 0.1316, step time: 0.1035\n",
      "3/223, train_loss: 0.1128, step time: 0.1178\n",
      "4/223, train_loss: 0.1214, step time: 0.1280\n",
      "5/223, train_loss: 0.1407, step time: 0.1142\n",
      "6/223, train_loss: 0.1377, step time: 0.1211\n",
      "7/223, train_loss: 0.1222, step time: 0.1252\n",
      "8/223, train_loss: 0.1263, step time: 0.1095\n",
      "9/223, train_loss: 0.1261, step time: 0.1154\n",
      "10/223, train_loss: 0.1389, step time: 0.1073\n",
      "11/223, train_loss: 0.1159, step time: 0.1138\n",
      "12/223, train_loss: 0.1185, step time: 0.1105\n",
      "13/223, train_loss: 0.1212, step time: 0.1187\n",
      "14/223, train_loss: 0.1272, step time: 0.1039\n",
      "15/223, train_loss: 0.1514, step time: 0.1005\n",
      "16/223, train_loss: 0.1220, step time: 0.1001\n",
      "17/223, train_loss: 0.1244, step time: 0.1222\n",
      "18/223, train_loss: 0.1236, step time: 0.1118\n",
      "19/223, train_loss: 0.1143, step time: 0.0993\n",
      "20/223, train_loss: 0.1498, step time: 0.1106\n",
      "21/223, train_loss: 0.1316, step time: 0.1052\n",
      "22/223, train_loss: 0.1228, step time: 0.1069\n",
      "23/223, train_loss: 0.1285, step time: 0.1108\n",
      "24/223, train_loss: 0.1183, step time: 0.1003\n",
      "25/223, train_loss: 0.1271, step time: 0.1111\n",
      "26/223, train_loss: 0.1435, step time: 0.0988\n",
      "27/223, train_loss: 0.1425, step time: 0.1132\n",
      "28/223, train_loss: 0.1305, step time: 0.1160\n",
      "29/223, train_loss: 0.1178, step time: 0.1115\n",
      "30/223, train_loss: 0.1352, step time: 0.1093\n",
      "31/223, train_loss: 0.1262, step time: 0.1099\n",
      "32/223, train_loss: 0.1248, step time: 0.1165\n",
      "33/223, train_loss: 0.1394, step time: 0.1066\n",
      "34/223, train_loss: 0.1208, step time: 0.1153\n",
      "35/223, train_loss: 0.1349, step time: 0.1139\n",
      "36/223, train_loss: 0.1305, step time: 0.1125\n",
      "37/223, train_loss: 0.1239, step time: 0.1162\n",
      "38/223, train_loss: 0.1346, step time: 0.1161\n",
      "39/223, train_loss: 0.1390, step time: 0.1039\n",
      "40/223, train_loss: 0.1360, step time: 0.1166\n",
      "41/223, train_loss: 0.1302, step time: 0.1083\n",
      "42/223, train_loss: 0.1158, step time: 0.1132\n",
      "43/223, train_loss: 0.1304, step time: 0.1051\n",
      "44/223, train_loss: 0.1346, step time: 0.1098\n",
      "45/223, train_loss: 0.1378, step time: 0.1229\n",
      "46/223, train_loss: 0.1221, step time: 0.1318\n",
      "47/223, train_loss: 0.1310, step time: 0.1163\n",
      "48/223, train_loss: 0.1248, step time: 0.1364\n",
      "49/223, train_loss: 0.1201, step time: 0.1005\n",
      "50/223, train_loss: 0.1303, step time: 0.0997\n",
      "51/223, train_loss: 0.1248, step time: 0.0995\n",
      "52/223, train_loss: 0.1271, step time: 0.0997\n",
      "53/223, train_loss: 0.1167, step time: 0.1005\n",
      "54/223, train_loss: 0.1395, step time: 0.0993\n",
      "55/223, train_loss: 0.1236, step time: 0.0997\n",
      "56/223, train_loss: 0.1186, step time: 0.1004\n",
      "57/223, train_loss: 0.1134, step time: 0.1038\n",
      "58/223, train_loss: 0.1265, step time: 0.1001\n",
      "59/223, train_loss: 0.1305, step time: 0.0999\n",
      "60/223, train_loss: 0.1111, step time: 0.0991\n",
      "61/223, train_loss: 0.1282, step time: 0.1010\n",
      "62/223, train_loss: 0.1293, step time: 0.1001\n",
      "63/223, train_loss: 0.1176, step time: 0.0987\n",
      "64/223, train_loss: 0.1232, step time: 0.1082\n",
      "65/223, train_loss: 0.1297, step time: 0.1049\n",
      "66/223, train_loss: 0.1265, step time: 0.1234\n",
      "67/223, train_loss: 0.1285, step time: 0.1019\n",
      "68/223, train_loss: 0.1256, step time: 0.1125\n",
      "69/223, train_loss: 0.1276, step time: 0.1082\n",
      "70/223, train_loss: 0.1196, step time: 0.1106\n",
      "71/223, train_loss: 0.1168, step time: 0.1315\n",
      "72/223, train_loss: 0.1225, step time: 0.1011\n",
      "73/223, train_loss: 0.1318, step time: 0.1063\n",
      "74/223, train_loss: 0.1243, step time: 0.1012\n",
      "75/223, train_loss: 0.1207, step time: 0.0998\n",
      "76/223, train_loss: 0.1124, step time: 0.0995\n",
      "77/223, train_loss: 0.1345, step time: 0.1335\n",
      "78/223, train_loss: 0.1292, step time: 0.1070\n",
      "79/223, train_loss: 0.1354, step time: 0.1258\n",
      "80/223, train_loss: 0.1204, step time: 0.1004\n",
      "81/223, train_loss: 0.1260, step time: 0.1179\n",
      "82/223, train_loss: 0.1234, step time: 0.1169\n",
      "83/223, train_loss: 0.1326, step time: 0.1178\n",
      "84/223, train_loss: 0.1194, step time: 0.1058\n",
      "85/223, train_loss: 0.1240, step time: 0.1109\n",
      "86/223, train_loss: 0.1152, step time: 0.0990\n",
      "87/223, train_loss: 0.1138, step time: 0.0998\n",
      "88/223, train_loss: 0.1162, step time: 0.0995\n",
      "89/223, train_loss: 0.1227, step time: 0.0996\n",
      "90/223, train_loss: 0.1312, step time: 0.1093\n",
      "91/223, train_loss: 0.3310, step time: 0.1152\n",
      "92/223, train_loss: 0.1184, step time: 0.1191\n",
      "93/223, train_loss: 0.1216, step time: 0.1180\n",
      "94/223, train_loss: 0.1320, step time: 0.1139\n",
      "95/223, train_loss: 0.1317, step time: 0.0998\n",
      "96/223, train_loss: 0.1298, step time: 0.1002\n",
      "97/223, train_loss: 0.1162, step time: 0.1110\n",
      "98/223, train_loss: 0.1245, step time: 0.1090\n",
      "99/223, train_loss: 0.1163, step time: 0.1104\n",
      "100/223, train_loss: 0.1344, step time: 0.1256\n",
      "101/223, train_loss: 0.1202, step time: 0.1145\n",
      "102/223, train_loss: 0.1277, step time: 0.1142\n",
      "103/223, train_loss: 0.1284, step time: 0.1005\n",
      "104/223, train_loss: 0.1295, step time: 0.1231\n",
      "105/223, train_loss: 0.1303, step time: 0.1008\n",
      "106/223, train_loss: 0.1221, step time: 0.0997\n",
      "107/223, train_loss: 0.1369, step time: 0.0989\n",
      "108/223, train_loss: 0.1323, step time: 0.1002\n",
      "109/223, train_loss: 0.1129, step time: 0.1003\n",
      "110/223, train_loss: 0.1294, step time: 0.1000\n",
      "111/223, train_loss: 0.1178, step time: 0.1005\n",
      "112/223, train_loss: 0.1157, step time: 0.0997\n",
      "113/223, train_loss: 0.1245, step time: 0.1010\n",
      "114/223, train_loss: 0.1161, step time: 0.1002\n",
      "115/223, train_loss: 0.1347, step time: 0.0999\n",
      "116/223, train_loss: 0.1344, step time: 0.0995\n",
      "117/223, train_loss: 0.1364, step time: 0.1044\n",
      "118/223, train_loss: 0.1253, step time: 0.1000\n",
      "119/223, train_loss: 0.1326, step time: 0.1001\n",
      "120/223, train_loss: 0.1323, step time: 0.0993\n",
      "121/223, train_loss: 0.1282, step time: 0.1002\n",
      "122/223, train_loss: 0.1181, step time: 0.0999\n",
      "123/223, train_loss: 0.1282, step time: 0.0986\n",
      "124/223, train_loss: 0.1269, step time: 0.0985\n",
      "125/223, train_loss: 0.1351, step time: 0.0993\n",
      "126/223, train_loss: 0.1362, step time: 0.1004\n",
      "127/223, train_loss: 0.1242, step time: 0.1006\n",
      "128/223, train_loss: 0.1130, step time: 0.1002\n",
      "129/223, train_loss: 0.1404, step time: 0.1082\n",
      "130/223, train_loss: 0.1278, step time: 0.1327\n",
      "131/223, train_loss: 0.1376, step time: 0.1053\n",
      "132/223, train_loss: 0.1215, step time: 0.1359\n",
      "133/223, train_loss: 0.1227, step time: 0.1151\n",
      "134/223, train_loss: 0.1340, step time: 0.1061\n",
      "135/223, train_loss: 0.1250, step time: 0.1090\n",
      "136/223, train_loss: 0.1400, step time: 0.1228\n",
      "137/223, train_loss: 0.1257, step time: 0.1273\n",
      "138/223, train_loss: 0.1248, step time: 0.1365\n",
      "139/223, train_loss: 0.1247, step time: 0.1054\n",
      "140/223, train_loss: 0.1360, step time: 0.1007\n",
      "141/223, train_loss: 0.1171, step time: 0.1053\n",
      "142/223, train_loss: 0.1170, step time: 0.1321\n",
      "143/223, train_loss: 0.1255, step time: 0.1006\n",
      "144/223, train_loss: 0.1277, step time: 0.1016\n",
      "145/223, train_loss: 0.1354, step time: 0.1036\n",
      "146/223, train_loss: 0.1324, step time: 0.1118\n",
      "147/223, train_loss: 0.1254, step time: 0.1008\n",
      "148/223, train_loss: 0.1224, step time: 0.1131\n",
      "149/223, train_loss: 0.1147, step time: 0.1087\n",
      "150/223, train_loss: 0.1080, step time: 0.1122\n",
      "151/223, train_loss: 0.1242, step time: 0.1004\n",
      "152/223, train_loss: 0.1311, step time: 0.1004\n",
      "153/223, train_loss: 0.1292, step time: 0.1091\n",
      "154/223, train_loss: 0.1375, step time: 0.1116\n",
      "155/223, train_loss: 0.1164, step time: 0.1003\n",
      "156/223, train_loss: 0.1225, step time: 0.1003\n",
      "157/223, train_loss: 0.1245, step time: 0.1100\n",
      "158/223, train_loss: 0.1220, step time: 0.1005\n",
      "159/223, train_loss: 0.1242, step time: 0.1007\n",
      "160/223, train_loss: 0.1234, step time: 0.1016\n",
      "161/223, train_loss: 0.1262, step time: 0.1008\n",
      "162/223, train_loss: 0.1311, step time: 0.1118\n",
      "163/223, train_loss: 0.1251, step time: 0.0998\n",
      "164/223, train_loss: 0.1161, step time: 0.1004\n",
      "165/223, train_loss: 0.1394, step time: 0.1093\n",
      "166/223, train_loss: 0.1225, step time: 0.1003\n",
      "167/223, train_loss: 0.1449, step time: 0.1003\n",
      "168/223, train_loss: 0.1322, step time: 0.1009\n",
      "169/223, train_loss: 0.1387, step time: 0.1005\n",
      "170/223, train_loss: 0.1220, step time: 0.1500\n",
      "171/223, train_loss: 0.1107, step time: 0.1183\n",
      "172/223, train_loss: 0.1175, step time: 0.1007\n",
      "173/223, train_loss: 0.1342, step time: 0.0995\n",
      "174/223, train_loss: 0.1117, step time: 0.1173\n",
      "175/223, train_loss: 0.1355, step time: 0.1115\n",
      "176/223, train_loss: 0.1341, step time: 0.1014\n",
      "177/223, train_loss: 0.1231, step time: 0.1009\n",
      "178/223, train_loss: 0.1235, step time: 0.1059\n",
      "179/223, train_loss: 0.1392, step time: 0.1007\n",
      "180/223, train_loss: 0.1146, step time: 0.1009\n",
      "181/223, train_loss: 0.1418, step time: 0.1010\n",
      "182/223, train_loss: 0.1309, step time: 0.1022\n",
      "183/223, train_loss: 0.1148, step time: 0.1025\n",
      "184/223, train_loss: 0.1239, step time: 0.1010\n",
      "185/223, train_loss: 0.1295, step time: 0.1032\n",
      "186/223, train_loss: 0.1186, step time: 0.1141\n",
      "187/223, train_loss: 0.1167, step time: 0.1085\n",
      "188/223, train_loss: 0.1348, step time: 0.1004\n",
      "189/223, train_loss: 0.1315, step time: 0.1002\n",
      "190/223, train_loss: 0.1258, step time: 0.1031\n",
      "191/223, train_loss: 0.1159, step time: 0.1115\n",
      "192/223, train_loss: 0.1267, step time: 0.1075\n",
      "193/223, train_loss: 0.1446, step time: 0.1087\n",
      "194/223, train_loss: 0.1298, step time: 0.1227\n",
      "195/223, train_loss: 0.1461, step time: 0.1110\n",
      "196/223, train_loss: 0.1149, step time: 0.0996\n",
      "197/223, train_loss: 0.1343, step time: 0.1039\n",
      "198/223, train_loss: 0.1214, step time: 0.1173\n",
      "199/223, train_loss: 0.1264, step time: 0.1108\n",
      "200/223, train_loss: 0.1206, step time: 0.1102\n",
      "201/223, train_loss: 0.1178, step time: 0.1006\n",
      "202/223, train_loss: 0.1138, step time: 0.1037\n",
      "203/223, train_loss: 0.1248, step time: 0.1005\n",
      "204/223, train_loss: 0.1411, step time: 0.1006\n",
      "205/223, train_loss: 0.1282, step time: 0.1010\n",
      "206/223, train_loss: 0.1256, step time: 0.1463\n",
      "207/223, train_loss: 0.1378, step time: 0.1173\n",
      "208/223, train_loss: 0.1247, step time: 0.1069\n",
      "209/223, train_loss: 0.1150, step time: 0.1012\n",
      "210/223, train_loss: 0.1358, step time: 0.0998\n",
      "211/223, train_loss: 0.1296, step time: 0.1132\n",
      "212/223, train_loss: 0.1201, step time: 0.1050\n",
      "213/223, train_loss: 0.1318, step time: 0.0998\n",
      "214/223, train_loss: 0.1320, step time: 0.1167\n",
      "215/223, train_loss: 0.1291, step time: 0.0999\n",
      "216/223, train_loss: 0.1239, step time: 0.0997\n",
      "217/223, train_loss: 0.1143, step time: 0.1040\n",
      "218/223, train_loss: 0.1162, step time: 0.1001\n",
      "219/223, train_loss: 0.1158, step time: 0.1005\n",
      "220/223, train_loss: 0.1146, step time: 0.0998\n",
      "221/223, train_loss: 0.1184, step time: 0.1014\n",
      "222/223, train_loss: 0.1167, step time: 0.0993\n",
      "223/223, train_loss: 0.1330, step time: 0.0990\n",
      "epoch 57 average loss: 0.1274\n",
      "time consuming of epoch 57 is: 96.2856\n",
      "----------\n",
      "epoch 58/300\n",
      "1/223, train_loss: 0.1406, step time: 0.1045\n",
      "2/223, train_loss: 0.1200, step time: 0.1006\n",
      "3/223, train_loss: 0.1327, step time: 0.0999\n",
      "4/223, train_loss: 0.1336, step time: 0.1009\n",
      "5/223, train_loss: 0.1294, step time: 0.1108\n",
      "6/223, train_loss: 0.1353, step time: 0.1092\n",
      "7/223, train_loss: 0.1222, step time: 0.1006\n",
      "8/223, train_loss: 0.1183, step time: 0.1002\n",
      "9/223, train_loss: 0.1267, step time: 0.1037\n",
      "10/223, train_loss: 0.1298, step time: 0.1151\n",
      "11/223, train_loss: 0.1277, step time: 0.1071\n",
      "12/223, train_loss: 0.1170, step time: 0.1097\n",
      "13/223, train_loss: 0.1241, step time: 0.1039\n",
      "14/223, train_loss: 0.1254, step time: 0.1256\n",
      "15/223, train_loss: 0.1214, step time: 0.1213\n",
      "16/223, train_loss: 0.1170, step time: 0.1124\n",
      "17/223, train_loss: 0.1104, step time: 0.1196\n",
      "18/223, train_loss: 0.1306, step time: 0.1141\n",
      "19/223, train_loss: 0.1401, step time: 0.1003\n",
      "20/223, train_loss: 0.1284, step time: 0.1098\n",
      "21/223, train_loss: 0.1168, step time: 0.1003\n",
      "22/223, train_loss: 0.1252, step time: 0.1069\n",
      "23/223, train_loss: 0.1203, step time: 0.1370\n",
      "24/223, train_loss: 0.1092, step time: 0.1084\n",
      "25/223, train_loss: 0.1222, step time: 0.1178\n",
      "26/223, train_loss: 0.1188, step time: 0.1234\n",
      "27/223, train_loss: 0.1248, step time: 0.1256\n",
      "28/223, train_loss: 0.1260, step time: 0.1004\n",
      "29/223, train_loss: 0.1265, step time: 0.1058\n",
      "30/223, train_loss: 0.1281, step time: 0.1120\n",
      "31/223, train_loss: 0.1494, step time: 0.1084\n",
      "32/223, train_loss: 0.1158, step time: 0.1346\n",
      "33/223, train_loss: 0.1454, step time: 0.1108\n",
      "34/223, train_loss: 0.1127, step time: 0.1141\n",
      "35/223, train_loss: 0.1316, step time: 0.1058\n",
      "36/223, train_loss: 0.1205, step time: 0.1188\n",
      "37/223, train_loss: 0.1429, step time: 0.1086\n",
      "38/223, train_loss: 0.3335, step time: 0.1100\n",
      "39/223, train_loss: 0.1217, step time: 0.1104\n",
      "40/223, train_loss: 0.1158, step time: 0.1013\n",
      "41/223, train_loss: 0.1155, step time: 0.0998\n",
      "42/223, train_loss: 0.1361, step time: 0.1201\n",
      "43/223, train_loss: 0.1254, step time: 0.1159\n",
      "44/223, train_loss: 0.1320, step time: 0.1050\n",
      "45/223, train_loss: 0.1312, step time: 0.1002\n",
      "46/223, train_loss: 0.1134, step time: 0.1100\n",
      "47/223, train_loss: 0.1257, step time: 0.1028\n",
      "48/223, train_loss: 0.1184, step time: 0.1017\n",
      "49/223, train_loss: 0.1262, step time: 0.1162\n",
      "50/223, train_loss: 0.1283, step time: 0.1004\n",
      "51/223, train_loss: 0.1340, step time: 0.1095\n",
      "52/223, train_loss: 0.1326, step time: 0.1305\n",
      "53/223, train_loss: 0.1479, step time: 0.1094\n",
      "54/223, train_loss: 0.1336, step time: 0.1052\n",
      "55/223, train_loss: 0.1178, step time: 0.1061\n",
      "56/223, train_loss: 0.1208, step time: 0.1008\n",
      "57/223, train_loss: 0.1245, step time: 0.1006\n",
      "58/223, train_loss: 0.1424, step time: 0.1021\n",
      "59/223, train_loss: 0.1315, step time: 0.1031\n",
      "60/223, train_loss: 0.1324, step time: 0.1059\n",
      "61/223, train_loss: 0.1324, step time: 0.1003\n",
      "62/223, train_loss: 0.1258, step time: 0.1006\n",
      "63/223, train_loss: 0.1176, step time: 0.1012\n",
      "64/223, train_loss: 0.1217, step time: 0.1019\n",
      "65/223, train_loss: 0.1269, step time: 0.1004\n",
      "66/223, train_loss: 0.1488, step time: 0.1014\n",
      "67/223, train_loss: 0.1190, step time: 0.1004\n",
      "68/223, train_loss: 0.1250, step time: 0.1016\n",
      "69/223, train_loss: 0.1301, step time: 0.1170\n",
      "70/223, train_loss: 0.1256, step time: 0.1020\n",
      "71/223, train_loss: 0.1171, step time: 0.1004\n",
      "72/223, train_loss: 0.1195, step time: 0.1046\n",
      "73/223, train_loss: 0.1321, step time: 0.1189\n",
      "74/223, train_loss: 0.1278, step time: 0.1140\n",
      "75/223, train_loss: 0.1195, step time: 0.1007\n",
      "76/223, train_loss: 0.1112, step time: 0.1109\n",
      "77/223, train_loss: 0.1297, step time: 0.1117\n",
      "78/223, train_loss: 0.1425, step time: 0.1003\n",
      "79/223, train_loss: 0.1290, step time: 0.1000\n",
      "80/223, train_loss: 0.1239, step time: 0.1006\n",
      "81/223, train_loss: 0.1341, step time: 0.1186\n",
      "82/223, train_loss: 0.1144, step time: 0.1244\n",
      "83/223, train_loss: 0.1132, step time: 0.1202\n",
      "84/223, train_loss: 0.1227, step time: 0.1017\n",
      "85/223, train_loss: 0.1316, step time: 0.1039\n",
      "86/223, train_loss: 0.1527, step time: 0.1318\n",
      "87/223, train_loss: 0.1154, step time: 0.1243\n",
      "88/223, train_loss: 0.1248, step time: 0.1080\n",
      "89/223, train_loss: 0.1167, step time: 0.1110\n",
      "90/223, train_loss: 0.1243, step time: 0.1005\n",
      "91/223, train_loss: 0.1279, step time: 0.1000\n",
      "92/223, train_loss: 0.1149, step time: 0.1005\n",
      "93/223, train_loss: 0.1269, step time: 0.1035\n",
      "94/223, train_loss: 0.1358, step time: 0.1219\n",
      "95/223, train_loss: 0.1196, step time: 0.1076\n",
      "96/223, train_loss: 0.1179, step time: 0.1228\n",
      "97/223, train_loss: 0.1331, step time: 0.1135\n",
      "98/223, train_loss: 0.1117, step time: 0.1007\n",
      "99/223, train_loss: 0.1269, step time: 0.1007\n",
      "100/223, train_loss: 0.1407, step time: 0.1005\n",
      "101/223, train_loss: 0.1535, step time: 0.1056\n",
      "102/223, train_loss: 0.1103, step time: 0.1418\n",
      "103/223, train_loss: 0.1278, step time: 0.1266\n",
      "104/223, train_loss: 0.1320, step time: 0.1058\n",
      "105/223, train_loss: 0.1364, step time: 0.1120\n",
      "106/223, train_loss: 0.1247, step time: 0.1121\n",
      "107/223, train_loss: 0.1476, step time: 0.1003\n",
      "108/223, train_loss: 0.1302, step time: 0.1025\n",
      "109/223, train_loss: 0.1276, step time: 0.1064\n",
      "110/223, train_loss: 0.1430, step time: 0.1140\n",
      "111/223, train_loss: 0.1242, step time: 0.1138\n",
      "112/223, train_loss: 0.1338, step time: 0.1087\n",
      "113/223, train_loss: 0.1313, step time: 0.1002\n",
      "114/223, train_loss: 0.1181, step time: 0.1020\n",
      "115/223, train_loss: 0.1288, step time: 0.0997\n",
      "116/223, train_loss: 0.1325, step time: 0.1068\n",
      "117/223, train_loss: 0.1221, step time: 0.1000\n",
      "118/223, train_loss: 0.1217, step time: 0.1048\n",
      "119/223, train_loss: 0.1211, step time: 0.1006\n",
      "120/223, train_loss: 0.1299, step time: 0.0997\n",
      "121/223, train_loss: 0.1262, step time: 0.1052\n",
      "122/223, train_loss: 0.1257, step time: 0.1159\n",
      "123/223, train_loss: 0.1249, step time: 0.1102\n",
      "124/223, train_loss: 0.1174, step time: 0.0999\n",
      "125/223, train_loss: 0.1343, step time: 0.1051\n",
      "126/223, train_loss: 0.1190, step time: 0.0996\n",
      "127/223, train_loss: 0.1343, step time: 0.1075\n",
      "128/223, train_loss: 0.1309, step time: 0.1007\n",
      "129/223, train_loss: 0.1235, step time: 0.1167\n",
      "130/223, train_loss: 0.1152, step time: 0.1101\n",
      "131/223, train_loss: 0.1301, step time: 0.1058\n",
      "132/223, train_loss: 0.1144, step time: 0.1107\n",
      "133/223, train_loss: 0.1164, step time: 0.1152\n",
      "134/223, train_loss: 0.1113, step time: 0.1152\n",
      "135/223, train_loss: 0.1334, step time: 0.1006\n",
      "136/223, train_loss: 0.1194, step time: 0.1008\n",
      "137/223, train_loss: 0.1401, step time: 0.1017\n",
      "138/223, train_loss: 0.1341, step time: 0.1042\n",
      "139/223, train_loss: 0.1334, step time: 0.0997\n",
      "140/223, train_loss: 0.1217, step time: 0.1117\n",
      "141/223, train_loss: 0.1186, step time: 0.1008\n",
      "142/223, train_loss: 0.1310, step time: 0.0999\n",
      "143/223, train_loss: 0.1237, step time: 0.1012\n",
      "144/223, train_loss: 0.1201, step time: 0.1212\n",
      "145/223, train_loss: 0.1217, step time: 0.1095\n",
      "146/223, train_loss: 0.1330, step time: 0.1046\n",
      "147/223, train_loss: 0.1215, step time: 0.1005\n",
      "148/223, train_loss: 0.1154, step time: 0.1004\n",
      "149/223, train_loss: 0.1303, step time: 0.1138\n",
      "150/223, train_loss: 0.1155, step time: 0.1143\n",
      "151/223, train_loss: 0.1235, step time: 0.1015\n",
      "152/223, train_loss: 0.1158, step time: 0.1011\n",
      "153/223, train_loss: 0.1187, step time: 0.1004\n",
      "154/223, train_loss: 0.1233, step time: 0.1050\n",
      "155/223, train_loss: 0.1268, step time: 0.1206\n",
      "156/223, train_loss: 0.1262, step time: 0.1154\n",
      "157/223, train_loss: 0.1474, step time: 0.0998\n",
      "158/223, train_loss: 0.1357, step time: 0.1238\n",
      "159/223, train_loss: 0.1236, step time: 0.1237\n",
      "160/223, train_loss: 0.1179, step time: 0.1024\n",
      "161/223, train_loss: 0.1138, step time: 0.1063\n",
      "162/223, train_loss: 0.1278, step time: 0.1005\n",
      "163/223, train_loss: 0.1225, step time: 0.1120\n",
      "164/223, train_loss: 0.1245, step time: 0.1006\n",
      "165/223, train_loss: 0.1196, step time: 0.1092\n",
      "166/223, train_loss: 0.1242, step time: 0.1150\n",
      "167/223, train_loss: 0.1181, step time: 0.1097\n",
      "168/223, train_loss: 0.1177, step time: 0.0999\n",
      "169/223, train_loss: 0.1280, step time: 0.1133\n",
      "170/223, train_loss: 0.1191, step time: 0.1006\n",
      "171/223, train_loss: 0.1220, step time: 0.1007\n",
      "172/223, train_loss: 0.1196, step time: 0.1014\n",
      "173/223, train_loss: 0.1269, step time: 0.1105\n",
      "174/223, train_loss: 0.1224, step time: 0.1239\n",
      "175/223, train_loss: 0.1252, step time: 0.1156\n",
      "176/223, train_loss: 0.1212, step time: 0.1085\n",
      "177/223, train_loss: 0.1261, step time: 0.1237\n",
      "178/223, train_loss: 0.1309, step time: 0.1137\n",
      "179/223, train_loss: 0.1364, step time: 0.1138\n",
      "180/223, train_loss: 0.1174, step time: 0.1094\n",
      "181/223, train_loss: 0.1331, step time: 0.1224\n",
      "182/223, train_loss: 0.1380, step time: 0.1141\n",
      "183/223, train_loss: 0.1392, step time: 0.1077\n",
      "184/223, train_loss: 0.1213, step time: 0.1096\n",
      "185/223, train_loss: 0.1186, step time: 0.1254\n",
      "186/223, train_loss: 0.1211, step time: 0.1126\n",
      "187/223, train_loss: 0.1272, step time: 0.1139\n",
      "188/223, train_loss: 0.1273, step time: 0.1089\n",
      "189/223, train_loss: 0.1353, step time: 0.1078\n",
      "190/223, train_loss: 0.1348, step time: 0.1252\n",
      "191/223, train_loss: 0.1273, step time: 0.1005\n",
      "192/223, train_loss: 0.1186, step time: 0.1001\n",
      "193/223, train_loss: 0.1337, step time: 0.0993\n",
      "194/223, train_loss: 0.1263, step time: 0.0991\n",
      "195/223, train_loss: 0.1327, step time: 0.0990\n",
      "196/223, train_loss: 0.1396, step time: 0.0999\n",
      "197/223, train_loss: 0.1187, step time: 0.0990\n",
      "198/223, train_loss: 0.1235, step time: 0.0988\n",
      "199/223, train_loss: 0.1199, step time: 0.1001\n",
      "200/223, train_loss: 0.1315, step time: 0.1005\n",
      "201/223, train_loss: 0.1225, step time: 0.1052\n",
      "202/223, train_loss: 0.1224, step time: 0.1393\n",
      "203/223, train_loss: 0.1175, step time: 0.1007\n",
      "204/223, train_loss: 0.1411, step time: 0.1055\n",
      "205/223, train_loss: 0.1273, step time: 0.1015\n",
      "206/223, train_loss: 0.1324, step time: 0.1006\n",
      "207/223, train_loss: 0.1257, step time: 0.1142\n",
      "208/223, train_loss: 0.1275, step time: 0.1023\n",
      "209/223, train_loss: 0.1131, step time: 0.1460\n",
      "210/223, train_loss: 0.1326, step time: 0.1253\n",
      "211/223, train_loss: 0.1207, step time: 0.1006\n",
      "212/223, train_loss: 0.1323, step time: 0.1008\n",
      "213/223, train_loss: 0.1183, step time: 0.1071\n",
      "214/223, train_loss: 0.1279, step time: 0.1009\n",
      "215/223, train_loss: 0.1183, step time: 0.1207\n",
      "216/223, train_loss: 0.1057, step time: 0.1233\n",
      "217/223, train_loss: 0.1392, step time: 0.1002\n",
      "218/223, train_loss: 0.1309, step time: 0.1004\n",
      "219/223, train_loss: 0.1340, step time: 0.0997\n",
      "220/223, train_loss: 0.1198, step time: 0.1011\n",
      "221/223, train_loss: 0.1168, step time: 0.1002\n",
      "222/223, train_loss: 0.1214, step time: 0.0984\n",
      "223/223, train_loss: 0.1154, step time: 0.0999\n",
      "epoch 58 average loss: 0.1270\n",
      "time consuming of epoch 58 is: 87.4050\n",
      "----------\n",
      "epoch 59/300\n",
      "1/223, train_loss: 0.1255, step time: 0.1195\n",
      "2/223, train_loss: 0.1160, step time: 0.1013\n",
      "3/223, train_loss: 0.1228, step time: 0.1008\n",
      "4/223, train_loss: 0.1367, step time: 0.1031\n",
      "5/223, train_loss: 0.1358, step time: 0.1009\n",
      "6/223, train_loss: 0.1361, step time: 0.1008\n",
      "7/223, train_loss: 0.1302, step time: 0.0998\n",
      "8/223, train_loss: 0.1241, step time: 0.1006\n",
      "9/223, train_loss: 0.1277, step time: 0.1014\n",
      "10/223, train_loss: 0.1196, step time: 0.1003\n",
      "11/223, train_loss: 0.1328, step time: 0.1032\n",
      "12/223, train_loss: 0.1196, step time: 0.1003\n",
      "13/223, train_loss: 0.1170, step time: 0.1013\n",
      "14/223, train_loss: 0.1395, step time: 0.1197\n",
      "15/223, train_loss: 0.1201, step time: 0.1048\n",
      "16/223, train_loss: 0.1325, step time: 0.1005\n",
      "17/223, train_loss: 0.1286, step time: 0.1003\n",
      "18/223, train_loss: 0.1376, step time: 0.1037\n",
      "19/223, train_loss: 0.1218, step time: 0.0998\n",
      "20/223, train_loss: 0.1172, step time: 0.0996\n",
      "21/223, train_loss: 0.1278, step time: 0.1001\n",
      "22/223, train_loss: 0.1222, step time: 0.1041\n",
      "23/223, train_loss: 0.1428, step time: 0.0995\n",
      "24/223, train_loss: 0.1132, step time: 0.0991\n",
      "25/223, train_loss: 0.1227, step time: 0.0998\n",
      "26/223, train_loss: 0.1178, step time: 0.1028\n",
      "27/223, train_loss: 0.1290, step time: 0.1000\n",
      "28/223, train_loss: 0.1457, step time: 0.0988\n",
      "29/223, train_loss: 0.1330, step time: 0.0998\n",
      "30/223, train_loss: 0.1149, step time: 0.1021\n",
      "31/223, train_loss: 0.1233, step time: 0.1003\n",
      "32/223, train_loss: 0.1245, step time: 0.0994\n",
      "33/223, train_loss: 0.1290, step time: 0.1049\n",
      "34/223, train_loss: 0.1243, step time: 0.1046\n",
      "35/223, train_loss: 0.1160, step time: 0.1087\n",
      "36/223, train_loss: 0.1343, step time: 0.1188\n",
      "37/223, train_loss: 0.1293, step time: 0.1515\n",
      "38/223, train_loss: 0.1512, step time: 0.1100\n",
      "39/223, train_loss: 0.1329, step time: 0.0999\n",
      "40/223, train_loss: 0.1207, step time: 0.1001\n",
      "41/223, train_loss: 0.1158, step time: 0.1004\n",
      "42/223, train_loss: 0.1258, step time: 0.1004\n",
      "43/223, train_loss: 0.1199, step time: 0.1122\n",
      "44/223, train_loss: 0.1337, step time: 0.1154\n",
      "45/223, train_loss: 0.1311, step time: 0.1216\n",
      "46/223, train_loss: 0.1270, step time: 0.1101\n",
      "47/223, train_loss: 0.1109, step time: 0.1124\n",
      "48/223, train_loss: 0.1210, step time: 0.1122\n",
      "49/223, train_loss: 0.1315, step time: 0.1128\n",
      "50/223, train_loss: 0.1317, step time: 0.1217\n",
      "51/223, train_loss: 0.1303, step time: 0.1149\n",
      "52/223, train_loss: 0.1214, step time: 0.1002\n",
      "53/223, train_loss: 0.1325, step time: 0.1300\n",
      "54/223, train_loss: 0.1073, step time: 0.1183\n",
      "55/223, train_loss: 0.1230, step time: 0.1042\n",
      "56/223, train_loss: 0.1206, step time: 0.1221\n",
      "57/223, train_loss: 0.1228, step time: 0.1094\n",
      "58/223, train_loss: 0.1333, step time: 0.1061\n",
      "59/223, train_loss: 0.1289, step time: 0.1107\n",
      "60/223, train_loss: 0.1253, step time: 0.1251\n",
      "61/223, train_loss: 0.1155, step time: 0.1180\n",
      "62/223, train_loss: 0.1374, step time: 0.1102\n",
      "63/223, train_loss: 0.1234, step time: 0.1099\n",
      "64/223, train_loss: 0.1231, step time: 0.1040\n",
      "65/223, train_loss: 0.1327, step time: 0.1136\n",
      "66/223, train_loss: 0.1265, step time: 0.1182\n",
      "67/223, train_loss: 0.1243, step time: 0.1180\n",
      "68/223, train_loss: 0.1163, step time: 0.1057\n",
      "69/223, train_loss: 0.1185, step time: 0.1007\n",
      "70/223, train_loss: 0.1392, step time: 0.1012\n",
      "71/223, train_loss: 0.1288, step time: 0.1014\n",
      "72/223, train_loss: 0.1192, step time: 0.1137\n",
      "73/223, train_loss: 0.1298, step time: 0.1009\n",
      "74/223, train_loss: 0.1383, step time: 0.1204\n",
      "75/223, train_loss: 0.1330, step time: 0.1042\n",
      "76/223, train_loss: 0.1273, step time: 0.1058\n",
      "77/223, train_loss: 0.1380, step time: 0.1003\n",
      "78/223, train_loss: 0.1165, step time: 0.1011\n",
      "79/223, train_loss: 0.1283, step time: 0.1004\n",
      "80/223, train_loss: 0.1170, step time: 0.1003\n",
      "81/223, train_loss: 0.1346, step time: 0.1002\n",
      "82/223, train_loss: 0.1219, step time: 0.1141\n",
      "83/223, train_loss: 0.1051, step time: 0.1046\n",
      "84/223, train_loss: 0.1295, step time: 0.0994\n",
      "85/223, train_loss: 0.1194, step time: 0.1173\n",
      "86/223, train_loss: 0.1305, step time: 0.1122\n",
      "87/223, train_loss: 0.1211, step time: 0.1035\n",
      "88/223, train_loss: 0.1283, step time: 0.1109\n",
      "89/223, train_loss: 0.1282, step time: 0.1049\n",
      "90/223, train_loss: 0.1260, step time: 0.1083\n",
      "91/223, train_loss: 0.1189, step time: 0.1110\n",
      "92/223, train_loss: 0.1265, step time: 0.1089\n",
      "93/223, train_loss: 0.1081, step time: 0.1189\n",
      "94/223, train_loss: 0.1141, step time: 0.1071\n",
      "95/223, train_loss: 0.1342, step time: 0.1011\n",
      "96/223, train_loss: 0.1168, step time: 0.1227\n",
      "97/223, train_loss: 0.1320, step time: 0.1228\n",
      "98/223, train_loss: 0.1287, step time: 0.1108\n",
      "99/223, train_loss: 0.1113, step time: 0.1054\n",
      "100/223, train_loss: 0.1204, step time: 0.1127\n",
      "101/223, train_loss: 0.1229, step time: 0.1060\n",
      "102/223, train_loss: 0.1198, step time: 0.1070\n",
      "103/223, train_loss: 0.1363, step time: 0.1182\n",
      "104/223, train_loss: 0.1236, step time: 0.1163\n",
      "105/223, train_loss: 0.1310, step time: 0.1109\n",
      "106/223, train_loss: 0.1321, step time: 0.1240\n",
      "107/223, train_loss: 0.1284, step time: 0.1063\n",
      "108/223, train_loss: 0.1202, step time: 0.1051\n",
      "109/223, train_loss: 0.1148, step time: 0.1253\n",
      "110/223, train_loss: 0.1114, step time: 0.1125\n",
      "111/223, train_loss: 0.1286, step time: 0.1034\n",
      "112/223, train_loss: 0.1212, step time: 0.1199\n",
      "113/223, train_loss: 0.1266, step time: 0.1214\n",
      "114/223, train_loss: 0.1116, step time: 0.1147\n",
      "115/223, train_loss: 0.3197, step time: 0.1106\n",
      "116/223, train_loss: 0.1386, step time: 0.1113\n",
      "117/223, train_loss: 0.1212, step time: 0.1117\n",
      "118/223, train_loss: 0.1259, step time: 0.1047\n",
      "119/223, train_loss: 0.1263, step time: 0.1154\n",
      "120/223, train_loss: 0.1236, step time: 0.1002\n",
      "121/223, train_loss: 0.1266, step time: 0.1269\n",
      "122/223, train_loss: 0.1178, step time: 0.1126\n",
      "123/223, train_loss: 0.1252, step time: 0.1135\n",
      "124/223, train_loss: 0.1116, step time: 0.1003\n",
      "125/223, train_loss: 0.1246, step time: 0.1007\n",
      "126/223, train_loss: 0.1223, step time: 0.1120\n",
      "127/223, train_loss: 0.1260, step time: 0.1103\n",
      "128/223, train_loss: 0.1191, step time: 0.1149\n",
      "129/223, train_loss: 0.1317, step time: 0.1008\n",
      "130/223, train_loss: 0.1346, step time: 0.1015\n",
      "131/223, train_loss: 0.1408, step time: 0.1048\n",
      "132/223, train_loss: 0.1273, step time: 0.1381\n",
      "133/223, train_loss: 0.1167, step time: 0.0998\n",
      "134/223, train_loss: 0.1188, step time: 0.1156\n",
      "135/223, train_loss: 0.1223, step time: 0.1131\n",
      "136/223, train_loss: 0.1142, step time: 0.1154\n",
      "137/223, train_loss: 0.1344, step time: 0.1143\n",
      "138/223, train_loss: 0.1297, step time: 0.1091\n",
      "139/223, train_loss: 0.1273, step time: 0.1146\n",
      "140/223, train_loss: 0.1242, step time: 0.1009\n",
      "141/223, train_loss: 0.1141, step time: 0.1105\n",
      "142/223, train_loss: 0.1257, step time: 0.1024\n",
      "143/223, train_loss: 0.1159, step time: 0.1056\n",
      "144/223, train_loss: 0.1276, step time: 0.1132\n",
      "145/223, train_loss: 0.1158, step time: 0.1213\n",
      "146/223, train_loss: 0.1357, step time: 0.1106\n",
      "147/223, train_loss: 0.1384, step time: 0.1080\n",
      "148/223, train_loss: 0.1297, step time: 0.1117\n",
      "149/223, train_loss: 0.1192, step time: 0.1093\n",
      "150/223, train_loss: 0.1279, step time: 0.1096\n",
      "151/223, train_loss: 0.1462, step time: 0.1098\n",
      "152/223, train_loss: 0.1245, step time: 0.1030\n",
      "153/223, train_loss: 0.1213, step time: 0.0989\n",
      "154/223, train_loss: 0.1224, step time: 0.0990\n",
      "155/223, train_loss: 0.1206, step time: 0.1189\n",
      "156/223, train_loss: 0.1352, step time: 0.1140\n",
      "157/223, train_loss: 0.1266, step time: 0.1008\n",
      "158/223, train_loss: 0.1462, step time: 0.1146\n",
      "159/223, train_loss: 0.1351, step time: 0.1187\n",
      "160/223, train_loss: 0.1603, step time: 0.1116\n",
      "161/223, train_loss: 0.1189, step time: 0.1308\n",
      "162/223, train_loss: 0.1448, step time: 0.1008\n",
      "163/223, train_loss: 0.1320, step time: 0.1027\n",
      "164/223, train_loss: 0.1181, step time: 0.1006\n",
      "165/223, train_loss: 0.1281, step time: 0.1336\n",
      "166/223, train_loss: 0.1203, step time: 0.1110\n",
      "167/223, train_loss: 0.1185, step time: 0.1064\n",
      "168/223, train_loss: 0.1414, step time: 0.1023\n",
      "169/223, train_loss: 0.1298, step time: 0.1173\n",
      "170/223, train_loss: 0.1265, step time: 0.1139\n",
      "171/223, train_loss: 0.1319, step time: 0.1083\n",
      "172/223, train_loss: 0.1251, step time: 0.1044\n",
      "173/223, train_loss: 0.1107, step time: 0.0994\n",
      "174/223, train_loss: 0.1284, step time: 0.1007\n",
      "175/223, train_loss: 0.1427, step time: 0.1025\n",
      "176/223, train_loss: 0.1317, step time: 0.1081\n",
      "177/223, train_loss: 0.1216, step time: 0.1195\n",
      "178/223, train_loss: 0.1271, step time: 0.1134\n",
      "179/223, train_loss: 0.1415, step time: 0.1030\n",
      "180/223, train_loss: 0.1275, step time: 0.1120\n",
      "181/223, train_loss: 0.1416, step time: 0.1082\n",
      "182/223, train_loss: 0.1351, step time: 0.1228\n",
      "183/223, train_loss: 0.1303, step time: 0.1033\n",
      "184/223, train_loss: 0.1325, step time: 0.1042\n",
      "185/223, train_loss: 0.1187, step time: 0.0999\n",
      "186/223, train_loss: 0.1301, step time: 0.1007\n",
      "187/223, train_loss: 0.1395, step time: 0.0997\n",
      "188/223, train_loss: 0.1228, step time: 0.0996\n",
      "189/223, train_loss: 0.1259, step time: 0.1011\n",
      "190/223, train_loss: 0.1240, step time: 0.1156\n",
      "191/223, train_loss: 0.1288, step time: 0.1004\n",
      "192/223, train_loss: 0.1136, step time: 0.1088\n",
      "193/223, train_loss: 0.1145, step time: 0.1006\n",
      "194/223, train_loss: 0.1269, step time: 0.1017\n",
      "195/223, train_loss: 0.1241, step time: 0.1085\n",
      "196/223, train_loss: 0.1181, step time: 0.1032\n",
      "197/223, train_loss: 0.1220, step time: 0.1023\n",
      "198/223, train_loss: 0.1299, step time: 0.1005\n",
      "199/223, train_loss: 0.1326, step time: 0.1007\n",
      "200/223, train_loss: 0.1269, step time: 0.1023\n",
      "201/223, train_loss: 0.1308, step time: 0.1231\n",
      "202/223, train_loss: 0.1140, step time: 0.1040\n",
      "203/223, train_loss: 0.1309, step time: 0.1188\n",
      "204/223, train_loss: 0.1281, step time: 0.1002\n",
      "205/223, train_loss: 0.1269, step time: 0.1004\n",
      "206/223, train_loss: 0.1295, step time: 0.1011\n",
      "207/223, train_loss: 0.1276, step time: 0.1069\n",
      "208/223, train_loss: 0.1312, step time: 0.1205\n",
      "209/223, train_loss: 0.1412, step time: 0.1246\n",
      "210/223, train_loss: 0.1163, step time: 0.1009\n",
      "211/223, train_loss: 0.1222, step time: 0.1002\n",
      "212/223, train_loss: 0.1249, step time: 0.1227\n",
      "213/223, train_loss: 0.1371, step time: 0.1320\n",
      "214/223, train_loss: 0.1264, step time: 0.1110\n",
      "215/223, train_loss: 0.1223, step time: 0.0986\n",
      "216/223, train_loss: 0.1336, step time: 0.1177\n",
      "217/223, train_loss: 0.1416, step time: 0.1026\n",
      "218/223, train_loss: 0.1268, step time: 0.1006\n",
      "219/223, train_loss: 0.1252, step time: 0.1002\n",
      "220/223, train_loss: 0.1173, step time: 0.0985\n",
      "221/223, train_loss: 0.1195, step time: 0.0998\n",
      "222/223, train_loss: 0.1210, step time: 0.0997\n",
      "223/223, train_loss: 0.1122, step time: 0.1000\n",
      "epoch 59 average loss: 0.1272\n",
      "time consuming of epoch 59 is: 90.1694\n",
      "----------\n",
      "epoch 60/300\n",
      "1/223, train_loss: 0.1301, step time: 0.1084\n",
      "2/223, train_loss: 0.1194, step time: 0.1029\n",
      "3/223, train_loss: 0.1298, step time: 0.1114\n",
      "4/223, train_loss: 0.1217, step time: 0.1115\n",
      "5/223, train_loss: 0.1262, step time: 0.1493\n",
      "6/223, train_loss: 0.1278, step time: 0.1003\n",
      "7/223, train_loss: 0.1480, step time: 0.0997\n",
      "8/223, train_loss: 0.1268, step time: 0.1019\n",
      "9/223, train_loss: 0.1294, step time: 0.1004\n",
      "10/223, train_loss: 0.1315, step time: 0.1160\n",
      "11/223, train_loss: 0.1246, step time: 0.1005\n",
      "12/223, train_loss: 0.1431, step time: 0.1004\n",
      "13/223, train_loss: 0.1143, step time: 0.1126\n",
      "14/223, train_loss: 0.1377, step time: 0.1010\n",
      "15/223, train_loss: 0.1217, step time: 0.1011\n",
      "16/223, train_loss: 0.1143, step time: 0.1005\n",
      "17/223, train_loss: 0.1220, step time: 0.1272\n",
      "18/223, train_loss: 0.1296, step time: 0.1095\n",
      "19/223, train_loss: 0.1248, step time: 0.1100\n",
      "20/223, train_loss: 0.1315, step time: 0.1108\n",
      "21/223, train_loss: 0.1398, step time: 0.1132\n",
      "22/223, train_loss: 0.1389, step time: 0.1005\n",
      "23/223, train_loss: 0.1106, step time: 0.1030\n",
      "24/223, train_loss: 0.1294, step time: 0.1001\n",
      "25/223, train_loss: 0.1143, step time: 0.1162\n",
      "26/223, train_loss: 0.1357, step time: 0.1082\n",
      "27/223, train_loss: 0.1240, step time: 0.0996\n",
      "28/223, train_loss: 0.1233, step time: 0.1002\n",
      "29/223, train_loss: 0.1242, step time: 0.1259\n",
      "30/223, train_loss: 0.1429, step time: 0.1156\n",
      "31/223, train_loss: 0.1419, step time: 0.1004\n",
      "32/223, train_loss: 0.1307, step time: 0.1037\n",
      "33/223, train_loss: 0.1259, step time: 0.1376\n",
      "34/223, train_loss: 0.1165, step time: 0.1118\n",
      "35/223, train_loss: 0.1309, step time: 0.1275\n",
      "36/223, train_loss: 0.1346, step time: 0.1185\n",
      "37/223, train_loss: 0.1234, step time: 0.1118\n",
      "38/223, train_loss: 0.1218, step time: 0.1144\n",
      "39/223, train_loss: 0.1234, step time: 0.1083\n",
      "40/223, train_loss: 0.1157, step time: 0.1031\n",
      "41/223, train_loss: 0.1252, step time: 0.1082\n",
      "42/223, train_loss: 0.1287, step time: 0.1170\n",
      "43/223, train_loss: 0.1320, step time: 0.1063\n",
      "44/223, train_loss: 0.1287, step time: 0.1085\n",
      "45/223, train_loss: 0.1336, step time: 0.1134\n",
      "46/223, train_loss: 0.1272, step time: 0.1138\n",
      "47/223, train_loss: 0.1197, step time: 0.1060\n",
      "48/223, train_loss: 0.3162, step time: 0.1001\n",
      "49/223, train_loss: 0.1500, step time: 0.1039\n",
      "50/223, train_loss: 0.1411, step time: 0.1051\n",
      "51/223, train_loss: 0.1207, step time: 0.1112\n",
      "52/223, train_loss: 0.1211, step time: 0.1024\n",
      "53/223, train_loss: 0.1169, step time: 0.1010\n",
      "54/223, train_loss: 0.1127, step time: 0.1233\n",
      "55/223, train_loss: 0.1148, step time: 0.1088\n",
      "56/223, train_loss: 0.1256, step time: 0.1017\n",
      "57/223, train_loss: 0.1202, step time: 0.0997\n",
      "58/223, train_loss: 0.1141, step time: 0.1007\n",
      "59/223, train_loss: 0.1261, step time: 0.1007\n",
      "60/223, train_loss: 0.1160, step time: 0.0993\n",
      "61/223, train_loss: 0.1257, step time: 0.1167\n",
      "62/223, train_loss: 0.1363, step time: 0.1107\n",
      "63/223, train_loss: 0.1139, step time: 0.1002\n",
      "64/223, train_loss: 0.1151, step time: 0.1140\n",
      "65/223, train_loss: 0.1309, step time: 0.1061\n",
      "66/223, train_loss: 0.1162, step time: 0.1189\n",
      "67/223, train_loss: 0.1249, step time: 0.1149\n",
      "68/223, train_loss: 0.1148, step time: 0.1024\n",
      "69/223, train_loss: 0.1159, step time: 0.1246\n",
      "70/223, train_loss: 0.1175, step time: 0.1097\n",
      "71/223, train_loss: 0.1208, step time: 0.1077\n",
      "72/223, train_loss: 0.1147, step time: 0.1021\n",
      "73/223, train_loss: 0.1321, step time: 0.0999\n",
      "74/223, train_loss: 0.1086, step time: 0.1085\n",
      "75/223, train_loss: 0.1313, step time: 0.1240\n",
      "76/223, train_loss: 0.1313, step time: 0.1004\n",
      "77/223, train_loss: 0.1251, step time: 0.1003\n",
      "78/223, train_loss: 0.1386, step time: 0.1099\n",
      "79/223, train_loss: 0.1329, step time: 0.1139\n",
      "80/223, train_loss: 0.1197, step time: 0.1091\n",
      "81/223, train_loss: 0.1277, step time: 0.1115\n",
      "82/223, train_loss: 0.1186, step time: 0.1112\n",
      "83/223, train_loss: 0.1239, step time: 0.1074\n",
      "84/223, train_loss: 0.1243, step time: 0.0998\n",
      "85/223, train_loss: 0.1350, step time: 0.1003\n",
      "86/223, train_loss: 0.1243, step time: 0.1012\n",
      "87/223, train_loss: 0.1218, step time: 0.1001\n",
      "88/223, train_loss: 0.1370, step time: 0.1004\n",
      "89/223, train_loss: 0.1175, step time: 0.0998\n",
      "90/223, train_loss: 0.1250, step time: 0.1159\n",
      "91/223, train_loss: 0.1351, step time: 0.1194\n",
      "92/223, train_loss: 0.1200, step time: 0.1173\n",
      "93/223, train_loss: 0.1278, step time: 0.1051\n",
      "94/223, train_loss: 0.1380, step time: 0.1068\n",
      "95/223, train_loss: 0.1326, step time: 0.1008\n",
      "96/223, train_loss: 0.1224, step time: 0.1012\n",
      "97/223, train_loss: 0.1254, step time: 0.1064\n",
      "98/223, train_loss: 0.1185, step time: 0.1053\n",
      "99/223, train_loss: 0.1300, step time: 0.0998\n",
      "100/223, train_loss: 0.1332, step time: 0.1006\n",
      "101/223, train_loss: 0.1239, step time: 0.1025\n",
      "102/223, train_loss: 0.1320, step time: 0.1073\n",
      "103/223, train_loss: 0.1193, step time: 0.1035\n",
      "104/223, train_loss: 0.1238, step time: 0.1015\n",
      "105/223, train_loss: 0.1296, step time: 0.1060\n",
      "106/223, train_loss: 0.1313, step time: 0.1094\n",
      "107/223, train_loss: 0.1326, step time: 0.1003\n",
      "108/223, train_loss: 0.1305, step time: 0.1006\n",
      "109/223, train_loss: 0.1258, step time: 0.1054\n",
      "110/223, train_loss: 0.1362, step time: 0.1172\n",
      "111/223, train_loss: 0.1265, step time: 0.1092\n",
      "112/223, train_loss: 0.1309, step time: 0.1007\n",
      "113/223, train_loss: 0.1342, step time: 0.1098\n",
      "114/223, train_loss: 0.1162, step time: 0.1031\n",
      "115/223, train_loss: 0.1224, step time: 0.1000\n",
      "116/223, train_loss: 0.1251, step time: 0.1001\n",
      "117/223, train_loss: 0.1269, step time: 0.1006\n",
      "118/223, train_loss: 0.1226, step time: 0.1130\n",
      "119/223, train_loss: 0.1115, step time: 0.1029\n",
      "120/223, train_loss: 0.1198, step time: 0.1195\n",
      "121/223, train_loss: 0.1258, step time: 0.1001\n",
      "122/223, train_loss: 0.1168, step time: 0.1009\n",
      "123/223, train_loss: 0.1182, step time: 0.1000\n",
      "124/223, train_loss: 0.1132, step time: 0.0998\n",
      "125/223, train_loss: 0.1427, step time: 0.1007\n",
      "126/223, train_loss: 0.1141, step time: 0.1082\n",
      "127/223, train_loss: 0.1306, step time: 0.1005\n",
      "128/223, train_loss: 0.1280, step time: 0.1051\n",
      "129/223, train_loss: 0.1327, step time: 0.1005\n",
      "130/223, train_loss: 0.1069, step time: 0.1118\n",
      "131/223, train_loss: 0.1208, step time: 0.1008\n",
      "132/223, train_loss: 0.1280, step time: 0.1009\n",
      "133/223, train_loss: 0.1244, step time: 0.1177\n",
      "134/223, train_loss: 0.1395, step time: 0.1047\n",
      "135/223, train_loss: 0.1278, step time: 0.1111\n",
      "136/223, train_loss: 0.1291, step time: 0.1117\n",
      "137/223, train_loss: 0.1292, step time: 0.1101\n",
      "138/223, train_loss: 0.1224, step time: 0.0998\n",
      "139/223, train_loss: 0.1160, step time: 0.1023\n",
      "140/223, train_loss: 0.1214, step time: 0.1006\n",
      "141/223, train_loss: 0.1181, step time: 0.1099\n",
      "142/223, train_loss: 0.1329, step time: 0.0997\n",
      "143/223, train_loss: 0.1392, step time: 0.1002\n",
      "144/223, train_loss: 0.1178, step time: 0.1001\n",
      "145/223, train_loss: 0.1332, step time: 0.1060\n",
      "146/223, train_loss: 0.1319, step time: 0.1082\n",
      "147/223, train_loss: 0.1281, step time: 0.0990\n",
      "148/223, train_loss: 0.1265, step time: 0.1004\n",
      "149/223, train_loss: 0.1277, step time: 0.1018\n",
      "150/223, train_loss: 0.1121, step time: 0.1000\n",
      "151/223, train_loss: 0.1350, step time: 0.1008\n",
      "152/223, train_loss: 0.1315, step time: 0.1011\n",
      "153/223, train_loss: 0.1446, step time: 0.1001\n",
      "154/223, train_loss: 0.1322, step time: 0.1009\n",
      "155/223, train_loss: 0.1296, step time: 0.1019\n",
      "156/223, train_loss: 0.1292, step time: 0.1018\n",
      "157/223, train_loss: 0.1193, step time: 0.1006\n",
      "158/223, train_loss: 0.1234, step time: 0.1029\n",
      "159/223, train_loss: 0.1302, step time: 0.1339\n",
      "160/223, train_loss: 0.1261, step time: 0.1184\n",
      "161/223, train_loss: 0.1196, step time: 0.1024\n",
      "162/223, train_loss: 0.1155, step time: 0.1092\n",
      "163/223, train_loss: 0.1216, step time: 0.1006\n",
      "164/223, train_loss: 0.1291, step time: 0.1000\n",
      "165/223, train_loss: 0.1386, step time: 0.1000\n",
      "166/223, train_loss: 0.1186, step time: 0.1011\n",
      "167/223, train_loss: 0.1140, step time: 0.1234\n",
      "168/223, train_loss: 0.1272, step time: 0.1121\n",
      "169/223, train_loss: 0.1485, step time: 0.1032\n",
      "170/223, train_loss: 0.1212, step time: 0.1202\n",
      "171/223, train_loss: 0.1161, step time: 0.1137\n",
      "172/223, train_loss: 0.1366, step time: 0.1119\n",
      "173/223, train_loss: 0.1210, step time: 0.1041\n",
      "174/223, train_loss: 0.1167, step time: 0.1063\n",
      "175/223, train_loss: 0.1208, step time: 0.1017\n",
      "176/223, train_loss: 0.1390, step time: 0.1004\n",
      "177/223, train_loss: 0.1311, step time: 0.1090\n",
      "178/223, train_loss: 0.1190, step time: 0.1125\n",
      "179/223, train_loss: 0.1335, step time: 0.0998\n",
      "180/223, train_loss: 0.1141, step time: 0.1075\n",
      "181/223, train_loss: 0.1221, step time: 0.1004\n",
      "182/223, train_loss: 0.1104, step time: 0.1156\n",
      "183/223, train_loss: 0.1290, step time: 0.1013\n",
      "184/223, train_loss: 0.1250, step time: 0.1116\n",
      "185/223, train_loss: 0.1248, step time: 0.1183\n",
      "186/223, train_loss: 0.1427, step time: 0.1001\n",
      "187/223, train_loss: 0.1314, step time: 0.1196\n",
      "188/223, train_loss: 0.1248, step time: 0.1234\n",
      "189/223, train_loss: 0.1278, step time: 0.1084\n",
      "190/223, train_loss: 0.1283, step time: 0.1141\n",
      "191/223, train_loss: 0.1193, step time: 0.1185\n",
      "192/223, train_loss: 0.1239, step time: 0.1239\n",
      "193/223, train_loss: 0.1140, step time: 0.1133\n",
      "194/223, train_loss: 0.1165, step time: 0.1112\n",
      "195/223, train_loss: 0.1168, step time: 0.1101\n",
      "196/223, train_loss: 0.1180, step time: 0.1046\n",
      "197/223, train_loss: 0.1413, step time: 0.1005\n",
      "198/223, train_loss: 0.1223, step time: 0.1126\n",
      "199/223, train_loss: 0.1271, step time: 0.1115\n",
      "200/223, train_loss: 0.1283, step time: 0.1134\n",
      "201/223, train_loss: 0.1127, step time: 0.1003\n",
      "202/223, train_loss: 0.1176, step time: 0.1168\n",
      "203/223, train_loss: 0.1147, step time: 0.1093\n",
      "204/223, train_loss: 0.1117, step time: 0.1109\n",
      "205/223, train_loss: 0.1307, step time: 0.1034\n",
      "206/223, train_loss: 0.1206, step time: 0.1165\n",
      "207/223, train_loss: 0.1199, step time: 0.1140\n",
      "208/223, train_loss: 0.1259, step time: 0.1164\n",
      "209/223, train_loss: 0.1146, step time: 0.1051\n",
      "210/223, train_loss: 0.1254, step time: 0.1122\n",
      "211/223, train_loss: 0.1273, step time: 0.1000\n",
      "212/223, train_loss: 0.1113, step time: 0.0995\n",
      "213/223, train_loss: 0.1157, step time: 0.1005\n",
      "214/223, train_loss: 0.1220, step time: 0.1132\n",
      "215/223, train_loss: 0.1162, step time: 0.1002\n",
      "216/223, train_loss: 0.1226, step time: 0.1007\n",
      "217/223, train_loss: 0.1236, step time: 0.0998\n",
      "218/223, train_loss: 0.1134, step time: 0.0990\n",
      "219/223, train_loss: 0.1182, step time: 0.0992\n",
      "220/223, train_loss: 0.1189, step time: 0.0989\n",
      "221/223, train_loss: 0.1094, step time: 0.0995\n",
      "222/223, train_loss: 0.1146, step time: 0.1001\n",
      "223/223, train_loss: 0.1153, step time: 0.1000\n",
      "epoch 60 average loss: 0.1260\n",
      "saved new best metric model\n",
      "current epoch: 60 current mean dice: 0.8390 tc: 0.9124 wt: 0.8520 et: 0.7527\n",
      "best mean dice: 0.8390 at epoch: 60\n",
      "time consuming of epoch 60 is: 90.1459\n",
      "----------\n",
      "epoch 61/300\n",
      "1/223, train_loss: 0.1285, step time: 0.1159\n",
      "2/223, train_loss: 0.1274, step time: 0.1060\n",
      "3/223, train_loss: 0.1319, step time: 0.1009\n",
      "4/223, train_loss: 0.1168, step time: 0.1137\n",
      "5/223, train_loss: 0.1259, step time: 0.1081\n",
      "6/223, train_loss: 0.1287, step time: 0.1086\n",
      "7/223, train_loss: 0.1251, step time: 0.1095\n",
      "8/223, train_loss: 0.1130, step time: 0.1125\n",
      "9/223, train_loss: 0.1178, step time: 0.1122\n",
      "10/223, train_loss: 0.1133, step time: 0.1097\n",
      "11/223, train_loss: 0.1188, step time: 0.1206\n",
      "12/223, train_loss: 0.1260, step time: 0.1005\n",
      "13/223, train_loss: 0.1360, step time: 0.1154\n",
      "14/223, train_loss: 0.1450, step time: 0.1021\n",
      "15/223, train_loss: 0.1182, step time: 0.1016\n",
      "16/223, train_loss: 0.1288, step time: 0.1002\n",
      "17/223, train_loss: 0.1212, step time: 0.1221\n",
      "18/223, train_loss: 0.1316, step time: 0.1076\n",
      "19/223, train_loss: 0.1244, step time: 0.1153\n",
      "20/223, train_loss: 0.1319, step time: 0.1060\n",
      "21/223, train_loss: 0.1159, step time: 0.1154\n",
      "22/223, train_loss: 0.1277, step time: 0.1073\n",
      "23/223, train_loss: 0.1326, step time: 0.1120\n",
      "24/223, train_loss: 0.1194, step time: 0.1076\n",
      "25/223, train_loss: 0.1246, step time: 0.1125\n",
      "26/223, train_loss: 0.1204, step time: 0.1063\n",
      "27/223, train_loss: 0.1275, step time: 0.1222\n",
      "28/223, train_loss: 0.1080, step time: 0.1076\n",
      "29/223, train_loss: 0.1245, step time: 0.1157\n",
      "30/223, train_loss: 0.1168, step time: 0.1009\n",
      "31/223, train_loss: 0.1172, step time: 0.1320\n",
      "32/223, train_loss: 0.1190, step time: 0.1235\n",
      "33/223, train_loss: 0.1203, step time: 0.1169\n",
      "34/223, train_loss: 0.1296, step time: 0.1083\n",
      "35/223, train_loss: 0.1179, step time: 0.1007\n",
      "36/223, train_loss: 0.1271, step time: 0.1106\n",
      "37/223, train_loss: 0.1164, step time: 0.1138\n",
      "38/223, train_loss: 0.1219, step time: 0.1074\n",
      "39/223, train_loss: 0.1130, step time: 0.1046\n",
      "40/223, train_loss: 0.1328, step time: 0.1066\n",
      "41/223, train_loss: 0.1345, step time: 0.1056\n",
      "42/223, train_loss: 0.1216, step time: 0.1132\n",
      "43/223, train_loss: 0.1160, step time: 0.1223\n",
      "44/223, train_loss: 0.1273, step time: 0.1135\n",
      "45/223, train_loss: 0.1239, step time: 0.1233\n",
      "46/223, train_loss: 0.1198, step time: 0.1044\n",
      "47/223, train_loss: 0.1202, step time: 0.1019\n",
      "48/223, train_loss: 0.1221, step time: 0.1005\n",
      "49/223, train_loss: 0.1366, step time: 0.1149\n",
      "50/223, train_loss: 0.1301, step time: 0.1121\n",
      "51/223, train_loss: 0.1345, step time: 0.1084\n",
      "52/223, train_loss: 0.1320, step time: 0.1060\n",
      "53/223, train_loss: 0.1329, step time: 0.1005\n",
      "54/223, train_loss: 0.1321, step time: 0.1140\n",
      "55/223, train_loss: 0.1297, step time: 0.1057\n",
      "56/223, train_loss: 0.1359, step time: 0.1080\n",
      "57/223, train_loss: 0.1531, step time: 0.1075\n",
      "58/223, train_loss: 0.1187, step time: 0.1004\n",
      "59/223, train_loss: 0.1122, step time: 0.1317\n",
      "60/223, train_loss: 0.1236, step time: 0.1478\n",
      "61/223, train_loss: 0.1166, step time: 0.1141\n",
      "62/223, train_loss: 0.1437, step time: 0.1004\n",
      "63/223, train_loss: 0.1097, step time: 0.0986\n",
      "64/223, train_loss: 0.1348, step time: 0.0983\n",
      "65/223, train_loss: 0.1215, step time: 0.1059\n",
      "66/223, train_loss: 0.1170, step time: 0.1040\n",
      "67/223, train_loss: 0.1307, step time: 0.0983\n",
      "68/223, train_loss: 0.1201, step time: 0.0986\n",
      "69/223, train_loss: 0.1228, step time: 0.1130\n",
      "70/223, train_loss: 0.1154, step time: 0.1221\n",
      "71/223, train_loss: 0.1298, step time: 0.1018\n",
      "72/223, train_loss: 0.1239, step time: 0.1018\n",
      "73/223, train_loss: 0.1325, step time: 0.1197\n",
      "74/223, train_loss: 0.1199, step time: 0.1057\n",
      "75/223, train_loss: 0.1431, step time: 0.1266\n",
      "76/223, train_loss: 0.1179, step time: 0.1267\n",
      "77/223, train_loss: 0.1176, step time: 0.1125\n",
      "78/223, train_loss: 0.1176, step time: 0.1028\n",
      "79/223, train_loss: 0.1186, step time: 0.1054\n",
      "80/223, train_loss: 0.1244, step time: 0.1008\n",
      "81/223, train_loss: 0.1349, step time: 0.1204\n",
      "82/223, train_loss: 0.1350, step time: 0.1125\n",
      "83/223, train_loss: 0.1193, step time: 0.1127\n",
      "84/223, train_loss: 0.1180, step time: 0.1187\n",
      "85/223, train_loss: 0.1470, step time: 0.0997\n",
      "86/223, train_loss: 0.1200, step time: 0.1008\n",
      "87/223, train_loss: 0.1339, step time: 0.1129\n",
      "88/223, train_loss: 0.1176, step time: 0.1039\n",
      "89/223, train_loss: 0.1221, step time: 0.1064\n",
      "90/223, train_loss: 0.1274, step time: 0.0998\n",
      "91/223, train_loss: 0.1384, step time: 0.1233\n",
      "92/223, train_loss: 0.1326, step time: 0.1244\n",
      "93/223, train_loss: 0.1343, step time: 0.1072\n",
      "94/223, train_loss: 0.1171, step time: 0.1169\n",
      "95/223, train_loss: 0.1188, step time: 0.1191\n",
      "96/223, train_loss: 0.1364, step time: 0.1095\n",
      "97/223, train_loss: 0.1133, step time: 0.1128\n",
      "98/223, train_loss: 0.1293, step time: 0.1182\n",
      "99/223, train_loss: 0.1111, step time: 0.1075\n",
      "100/223, train_loss: 0.1276, step time: 0.1180\n",
      "101/223, train_loss: 0.1240, step time: 0.1214\n",
      "102/223, train_loss: 0.1259, step time: 0.1133\n",
      "103/223, train_loss: 0.1281, step time: 0.1166\n",
      "104/223, train_loss: 0.1329, step time: 0.1333\n",
      "105/223, train_loss: 0.1084, step time: 0.1110\n",
      "106/223, train_loss: 0.1234, step time: 0.1003\n",
      "107/223, train_loss: 0.1255, step time: 0.1022\n",
      "108/223, train_loss: 0.1290, step time: 0.1273\n",
      "109/223, train_loss: 0.1170, step time: 0.1261\n",
      "110/223, train_loss: 0.1204, step time: 0.1081\n",
      "111/223, train_loss: 0.1364, step time: 0.0994\n",
      "112/223, train_loss: 0.1210, step time: 0.1107\n",
      "113/223, train_loss: 0.1173, step time: 0.1007\n",
      "114/223, train_loss: 0.1369, step time: 0.1026\n",
      "115/223, train_loss: 0.1350, step time: 0.1002\n",
      "116/223, train_loss: 0.3300, step time: 0.1111\n",
      "117/223, train_loss: 0.1048, step time: 0.1036\n",
      "118/223, train_loss: 0.1259, step time: 0.0998\n",
      "119/223, train_loss: 0.1252, step time: 0.1003\n",
      "120/223, train_loss: 0.1206, step time: 0.1048\n",
      "121/223, train_loss: 0.1355, step time: 0.1006\n",
      "122/223, train_loss: 0.1292, step time: 0.1031\n",
      "123/223, train_loss: 0.1258, step time: 0.1094\n",
      "124/223, train_loss: 0.1195, step time: 0.1211\n",
      "125/223, train_loss: 0.1317, step time: 0.1163\n",
      "126/223, train_loss: 0.1230, step time: 0.1077\n",
      "127/223, train_loss: 0.1228, step time: 0.1245\n",
      "128/223, train_loss: 0.1291, step time: 0.1273\n",
      "129/223, train_loss: 0.1192, step time: 0.1006\n",
      "130/223, train_loss: 0.1438, step time: 0.1129\n",
      "131/223, train_loss: 0.1333, step time: 0.1177\n",
      "132/223, train_loss: 0.1121, step time: 0.1012\n",
      "133/223, train_loss: 0.1271, step time: 0.1044\n",
      "134/223, train_loss: 0.1180, step time: 0.1098\n",
      "135/223, train_loss: 0.1129, step time: 0.1002\n",
      "136/223, train_loss: 0.1402, step time: 0.1004\n",
      "137/223, train_loss: 0.1396, step time: 0.1026\n",
      "138/223, train_loss: 0.1329, step time: 0.1233\n",
      "139/223, train_loss: 0.1414, step time: 0.1009\n",
      "140/223, train_loss: 0.1276, step time: 0.0998\n",
      "141/223, train_loss: 0.1251, step time: 0.1080\n",
      "142/223, train_loss: 0.1218, step time: 0.1138\n",
      "143/223, train_loss: 0.1245, step time: 0.1062\n",
      "144/223, train_loss: 0.1213, step time: 0.1072\n",
      "145/223, train_loss: 0.1188, step time: 0.1083\n",
      "146/223, train_loss: 0.1327, step time: 0.1207\n",
      "147/223, train_loss: 0.1377, step time: 0.1267\n",
      "148/223, train_loss: 0.1332, step time: 0.1149\n",
      "149/223, train_loss: 0.1210, step time: 0.1165\n",
      "150/223, train_loss: 0.1320, step time: 0.1242\n",
      "151/223, train_loss: 0.1142, step time: 0.1023\n",
      "152/223, train_loss: 0.1351, step time: 0.1007\n",
      "153/223, train_loss: 0.1188, step time: 0.0989\n",
      "154/223, train_loss: 0.1127, step time: 0.1057\n",
      "155/223, train_loss: 0.1152, step time: 0.0998\n",
      "156/223, train_loss: 0.1187, step time: 0.1170\n",
      "157/223, train_loss: 0.1422, step time: 0.1193\n",
      "158/223, train_loss: 0.1271, step time: 0.1220\n",
      "159/223, train_loss: 0.1303, step time: 0.1151\n",
      "160/223, train_loss: 0.1256, step time: 0.1006\n",
      "161/223, train_loss: 0.1161, step time: 0.1058\n",
      "162/223, train_loss: 0.1218, step time: 0.1369\n",
      "163/223, train_loss: 0.1262, step time: 0.1224\n",
      "164/223, train_loss: 0.1387, step time: 0.1099\n",
      "165/223, train_loss: 0.1332, step time: 0.1001\n",
      "166/223, train_loss: 0.1271, step time: 0.0998\n",
      "167/223, train_loss: 0.1323, step time: 0.1067\n",
      "168/223, train_loss: 0.1164, step time: 0.1118\n",
      "169/223, train_loss: 0.1185, step time: 0.1234\n",
      "170/223, train_loss: 0.1266, step time: 0.1162\n",
      "171/223, train_loss: 0.1294, step time: 0.1172\n",
      "172/223, train_loss: 0.1324, step time: 0.1163\n",
      "173/223, train_loss: 0.1226, step time: 0.1066\n",
      "174/223, train_loss: 0.1374, step time: 0.1235\n",
      "175/223, train_loss: 0.1181, step time: 0.1394\n",
      "176/223, train_loss: 0.1336, step time: 0.1111\n",
      "177/223, train_loss: 0.1234, step time: 0.1144\n",
      "178/223, train_loss: 0.1278, step time: 0.1133\n",
      "179/223, train_loss: 0.1195, step time: 0.1119\n",
      "180/223, train_loss: 0.1292, step time: 0.1088\n",
      "181/223, train_loss: 0.1165, step time: 0.1027\n",
      "182/223, train_loss: 0.1312, step time: 0.1135\n",
      "183/223, train_loss: 0.1264, step time: 0.1086\n",
      "184/223, train_loss: 0.1158, step time: 0.0999\n",
      "185/223, train_loss: 0.1100, step time: 0.1080\n",
      "186/223, train_loss: 0.1182, step time: 0.1090\n",
      "187/223, train_loss: 0.1191, step time: 0.1256\n",
      "188/223, train_loss: 0.1292, step time: 0.1080\n",
      "189/223, train_loss: 0.1234, step time: 0.1116\n",
      "190/223, train_loss: 0.1150, step time: 0.1004\n",
      "191/223, train_loss: 0.1308, step time: 0.1005\n",
      "192/223, train_loss: 0.1153, step time: 0.1106\n",
      "193/223, train_loss: 0.1200, step time: 0.1237\n",
      "194/223, train_loss: 0.1208, step time: 0.1342\n",
      "195/223, train_loss: 0.1223, step time: 0.0993\n",
      "196/223, train_loss: 0.1225, step time: 0.1005\n",
      "197/223, train_loss: 0.1230, step time: 0.1008\n",
      "198/223, train_loss: 0.1315, step time: 0.0993\n",
      "199/223, train_loss: 0.1142, step time: 0.1008\n",
      "200/223, train_loss: 0.1312, step time: 0.1094\n",
      "201/223, train_loss: 0.1130, step time: 0.1237\n",
      "202/223, train_loss: 0.1298, step time: 0.1009\n",
      "203/223, train_loss: 0.1144, step time: 0.1064\n",
      "204/223, train_loss: 0.1118, step time: 0.1007\n",
      "205/223, train_loss: 0.1211, step time: 0.0997\n",
      "206/223, train_loss: 0.1324, step time: 0.0998\n",
      "207/223, train_loss: 0.1322, step time: 0.1011\n",
      "208/223, train_loss: 0.1337, step time: 0.1132\n",
      "209/223, train_loss: 0.1398, step time: 0.1021\n",
      "210/223, train_loss: 0.1113, step time: 0.0994\n",
      "211/223, train_loss: 0.1046, step time: 0.1005\n",
      "212/223, train_loss: 0.1037, step time: 0.1064\n",
      "213/223, train_loss: 0.1193, step time: 0.1079\n",
      "214/223, train_loss: 0.1335, step time: 0.1128\n",
      "215/223, train_loss: 0.1188, step time: 0.1168\n",
      "216/223, train_loss: 0.1227, step time: 0.0995\n",
      "217/223, train_loss: 0.1183, step time: 0.0999\n",
      "218/223, train_loss: 0.1242, step time: 0.1057\n",
      "219/223, train_loss: 0.1469, step time: 0.1063\n",
      "220/223, train_loss: 0.1279, step time: 0.0996\n",
      "221/223, train_loss: 0.1261, step time: 0.0987\n",
      "222/223, train_loss: 0.1204, step time: 0.0995\n",
      "223/223, train_loss: 0.1210, step time: 0.0998\n",
      "epoch 61 average loss: 0.1259\n",
      "time consuming of epoch 61 is: 86.5527\n",
      "----------\n",
      "epoch 62/300\n",
      "1/223, train_loss: 0.1291, step time: 0.1050\n",
      "2/223, train_loss: 0.1241, step time: 0.1005\n",
      "3/223, train_loss: 0.1220, step time: 0.1222\n",
      "4/223, train_loss: 0.1196, step time: 0.1001\n",
      "5/223, train_loss: 0.1344, step time: 0.1051\n",
      "6/223, train_loss: 0.1309, step time: 0.1118\n",
      "7/223, train_loss: 0.1468, step time: 0.1200\n",
      "8/223, train_loss: 0.1272, step time: 0.1199\n",
      "9/223, train_loss: 0.1253, step time: 0.1043\n",
      "10/223, train_loss: 0.1166, step time: 0.1003\n",
      "11/223, train_loss: 0.1192, step time: 0.1003\n",
      "12/223, train_loss: 0.1231, step time: 0.0997\n",
      "13/223, train_loss: 0.1275, step time: 0.1141\n",
      "14/223, train_loss: 0.1242, step time: 0.1103\n",
      "15/223, train_loss: 0.1191, step time: 0.1109\n",
      "16/223, train_loss: 0.1222, step time: 0.1007\n",
      "17/223, train_loss: 0.1294, step time: 0.1221\n",
      "18/223, train_loss: 0.1244, step time: 0.1144\n",
      "19/223, train_loss: 0.1246, step time: 0.1057\n",
      "20/223, train_loss: 0.1213, step time: 0.1051\n",
      "21/223, train_loss: 0.1299, step time: 0.1158\n",
      "22/223, train_loss: 0.1284, step time: 0.1106\n",
      "23/223, train_loss: 0.1252, step time: 0.1164\n",
      "24/223, train_loss: 0.1111, step time: 0.0991\n",
      "25/223, train_loss: 0.1142, step time: 0.1111\n",
      "26/223, train_loss: 0.1383, step time: 0.1074\n",
      "27/223, train_loss: 0.1341, step time: 0.1155\n",
      "28/223, train_loss: 0.1263, step time: 0.1097\n",
      "29/223, train_loss: 0.1135, step time: 0.1127\n",
      "30/223, train_loss: 0.1127, step time: 0.1078\n",
      "31/223, train_loss: 0.3155, step time: 0.1127\n",
      "32/223, train_loss: 0.1063, step time: 0.1228\n",
      "33/223, train_loss: 0.1168, step time: 0.0999\n",
      "34/223, train_loss: 0.1372, step time: 0.1140\n",
      "35/223, train_loss: 0.1201, step time: 0.1101\n",
      "36/223, train_loss: 0.1187, step time: 0.1023\n",
      "37/223, train_loss: 0.1206, step time: 0.1036\n",
      "38/223, train_loss: 0.1247, step time: 0.1151\n",
      "39/223, train_loss: 0.1374, step time: 0.1059\n",
      "40/223, train_loss: 0.1139, step time: 0.1096\n",
      "41/223, train_loss: 0.1346, step time: 0.1047\n",
      "42/223, train_loss: 0.1215, step time: 0.1018\n",
      "43/223, train_loss: 0.1240, step time: 0.1156\n",
      "44/223, train_loss: 0.1055, step time: 0.1063\n",
      "45/223, train_loss: 0.1242, step time: 0.1108\n",
      "46/223, train_loss: 0.1231, step time: 0.1085\n",
      "47/223, train_loss: 0.1396, step time: 0.1159\n",
      "48/223, train_loss: 0.1254, step time: 0.1090\n",
      "49/223, train_loss: 0.1139, step time: 0.1090\n",
      "50/223, train_loss: 0.1246, step time: 0.1202\n",
      "51/223, train_loss: 0.1378, step time: 0.1232\n",
      "52/223, train_loss: 0.1323, step time: 0.1053\n",
      "53/223, train_loss: 0.1188, step time: 0.1143\n",
      "54/223, train_loss: 0.1304, step time: 0.1021\n",
      "55/223, train_loss: 0.1329, step time: 0.1174\n",
      "56/223, train_loss: 0.1322, step time: 0.1009\n",
      "57/223, train_loss: 0.1258, step time: 0.1225\n",
      "58/223, train_loss: 0.1283, step time: 0.1109\n",
      "59/223, train_loss: 0.1233, step time: 0.1153\n",
      "60/223, train_loss: 0.1083, step time: 0.1172\n",
      "61/223, train_loss: 0.1352, step time: 0.1010\n",
      "62/223, train_loss: 0.1364, step time: 0.1123\n",
      "63/223, train_loss: 0.1301, step time: 0.1009\n",
      "64/223, train_loss: 0.1198, step time: 0.1038\n",
      "65/223, train_loss: 0.1270, step time: 0.1000\n",
      "66/223, train_loss: 0.1311, step time: 0.1042\n",
      "67/223, train_loss: 0.1350, step time: 0.0988\n",
      "68/223, train_loss: 0.1282, step time: 0.0993\n",
      "69/223, train_loss: 0.1118, step time: 0.1127\n",
      "70/223, train_loss: 0.1256, step time: 0.1010\n",
      "71/223, train_loss: 0.1167, step time: 0.1052\n",
      "72/223, train_loss: 0.1124, step time: 0.1003\n",
      "73/223, train_loss: 0.1284, step time: 0.1089\n",
      "74/223, train_loss: 0.1300, step time: 0.1039\n",
      "75/223, train_loss: 0.1452, step time: 0.1050\n",
      "76/223, train_loss: 0.1168, step time: 0.1189\n",
      "77/223, train_loss: 0.1176, step time: 0.1067\n",
      "78/223, train_loss: 0.1316, step time: 0.1220\n",
      "79/223, train_loss: 0.1349, step time: 0.1150\n",
      "80/223, train_loss: 0.1227, step time: 0.1083\n",
      "81/223, train_loss: 0.1333, step time: 0.1144\n",
      "82/223, train_loss: 0.1248, step time: 0.1313\n",
      "83/223, train_loss: 0.1226, step time: 0.1169\n",
      "84/223, train_loss: 0.1150, step time: 0.1106\n",
      "85/223, train_loss: 0.1207, step time: 0.1061\n",
      "86/223, train_loss: 0.1175, step time: 0.1113\n",
      "87/223, train_loss: 0.1304, step time: 0.1314\n",
      "88/223, train_loss: 0.1154, step time: 0.1119\n",
      "89/223, train_loss: 0.1191, step time: 0.1012\n",
      "90/223, train_loss: 0.1299, step time: 0.1090\n",
      "91/223, train_loss: 0.1194, step time: 0.1154\n",
      "92/223, train_loss: 0.1342, step time: 0.1151\n",
      "93/223, train_loss: 0.1242, step time: 0.1006\n",
      "94/223, train_loss: 0.1233, step time: 0.1084\n",
      "95/223, train_loss: 0.1239, step time: 0.1003\n",
      "96/223, train_loss: 0.1163, step time: 0.1090\n",
      "97/223, train_loss: 0.1248, step time: 0.1106\n",
      "98/223, train_loss: 0.1383, step time: 0.1103\n",
      "99/223, train_loss: 0.1279, step time: 0.0997\n",
      "100/223, train_loss: 0.1233, step time: 0.1011\n",
      "101/223, train_loss: 0.1156, step time: 0.1133\n",
      "102/223, train_loss: 0.1167, step time: 0.1171\n",
      "103/223, train_loss: 0.1189, step time: 0.1084\n",
      "104/223, train_loss: 0.1242, step time: 0.1003\n",
      "105/223, train_loss: 0.1141, step time: 0.1106\n",
      "106/223, train_loss: 0.1420, step time: 0.1002\n",
      "107/223, train_loss: 0.1328, step time: 0.1109\n",
      "108/223, train_loss: 0.1290, step time: 0.1213\n",
      "109/223, train_loss: 0.1192, step time: 0.1137\n",
      "110/223, train_loss: 0.1200, step time: 0.1047\n",
      "111/223, train_loss: 0.1215, step time: 0.1111\n",
      "112/223, train_loss: 0.1233, step time: 0.1194\n",
      "113/223, train_loss: 0.1203, step time: 0.1016\n",
      "114/223, train_loss: 0.1385, step time: 0.1003\n",
      "115/223, train_loss: 0.1238, step time: 0.0996\n",
      "116/223, train_loss: 0.1203, step time: 0.1007\n",
      "117/223, train_loss: 0.1445, step time: 0.1020\n",
      "118/223, train_loss: 0.1266, step time: 0.0990\n",
      "119/223, train_loss: 0.1214, step time: 0.0992\n",
      "120/223, train_loss: 0.1244, step time: 0.0996\n",
      "121/223, train_loss: 0.1210, step time: 0.1008\n",
      "122/223, train_loss: 0.1180, step time: 0.1001\n",
      "123/223, train_loss: 0.1212, step time: 0.0994\n",
      "124/223, train_loss: 0.1125, step time: 0.1001\n",
      "125/223, train_loss: 0.1336, step time: 0.1084\n",
      "126/223, train_loss: 0.1225, step time: 0.0992\n",
      "127/223, train_loss: 0.1198, step time: 0.0996\n",
      "128/223, train_loss: 0.1145, step time: 0.1003\n",
      "129/223, train_loss: 0.1179, step time: 0.1085\n",
      "130/223, train_loss: 0.1267, step time: 0.1211\n",
      "131/223, train_loss: 0.1259, step time: 0.1421\n",
      "132/223, train_loss: 0.1270, step time: 0.1376\n",
      "133/223, train_loss: 0.1236, step time: 0.1017\n",
      "134/223, train_loss: 0.1205, step time: 0.0996\n",
      "135/223, train_loss: 0.1176, step time: 0.0987\n",
      "136/223, train_loss: 0.1211, step time: 0.0986\n",
      "137/223, train_loss: 0.1229, step time: 0.0998\n",
      "138/223, train_loss: 0.1128, step time: 0.0995\n",
      "139/223, train_loss: 0.1284, step time: 0.1004\n",
      "140/223, train_loss: 0.1193, step time: 0.0993\n",
      "141/223, train_loss: 0.1153, step time: 0.1045\n",
      "142/223, train_loss: 0.1227, step time: 0.1012\n",
      "143/223, train_loss: 0.1369, step time: 0.1014\n",
      "144/223, train_loss: 0.1299, step time: 0.1025\n",
      "145/223, train_loss: 0.1392, step time: 0.1332\n",
      "146/223, train_loss: 0.1319, step time: 0.1008\n",
      "147/223, train_loss: 0.1334, step time: 0.1008\n",
      "148/223, train_loss: 0.1389, step time: 0.1027\n",
      "149/223, train_loss: 0.1374, step time: 0.1084\n",
      "150/223, train_loss: 0.1216, step time: 0.1101\n",
      "151/223, train_loss: 0.1140, step time: 0.1118\n",
      "152/223, train_loss: 0.1130, step time: 0.1080\n",
      "153/223, train_loss: 0.1143, step time: 0.1006\n",
      "154/223, train_loss: 0.1345, step time: 0.1058\n",
      "155/223, train_loss: 0.1177, step time: 0.1001\n",
      "156/223, train_loss: 0.1335, step time: 0.1353\n",
      "157/223, train_loss: 0.1125, step time: 0.1046\n",
      "158/223, train_loss: 0.1218, step time: 0.0995\n",
      "159/223, train_loss: 0.1186, step time: 0.0998\n",
      "160/223, train_loss: 0.1163, step time: 0.1006\n",
      "161/223, train_loss: 0.1245, step time: 0.1017\n",
      "162/223, train_loss: 0.1360, step time: 0.1069\n",
      "163/223, train_loss: 0.1220, step time: 0.1005\n",
      "164/223, train_loss: 0.1176, step time: 0.0998\n",
      "165/223, train_loss: 0.1163, step time: 0.1011\n",
      "166/223, train_loss: 0.1163, step time: 0.1126\n",
      "167/223, train_loss: 0.1223, step time: 0.1027\n",
      "168/223, train_loss: 0.1350, step time: 0.1100\n",
      "169/223, train_loss: 0.1283, step time: 0.1098\n",
      "170/223, train_loss: 0.1327, step time: 0.1169\n",
      "171/223, train_loss: 0.1284, step time: 0.1004\n",
      "172/223, train_loss: 0.1214, step time: 0.1010\n",
      "173/223, train_loss: 0.1150, step time: 0.1014\n",
      "174/223, train_loss: 0.1230, step time: 0.1061\n",
      "175/223, train_loss: 0.1345, step time: 0.0999\n",
      "176/223, train_loss: 0.1470, step time: 0.1011\n",
      "177/223, train_loss: 0.1254, step time: 0.1128\n",
      "178/223, train_loss: 0.1247, step time: 0.1055\n",
      "179/223, train_loss: 0.1224, step time: 0.1273\n",
      "180/223, train_loss: 0.1362, step time: 0.1144\n",
      "181/223, train_loss: 0.1204, step time: 0.1003\n",
      "182/223, train_loss: 0.1251, step time: 0.1180\n",
      "183/223, train_loss: 0.1160, step time: 0.1152\n",
      "184/223, train_loss: 0.1398, step time: 0.1089\n",
      "185/223, train_loss: 0.1170, step time: 0.1024\n",
      "186/223, train_loss: 0.1271, step time: 0.1159\n",
      "187/223, train_loss: 0.1443, step time: 0.1182\n",
      "188/223, train_loss: 0.1167, step time: 0.1000\n",
      "189/223, train_loss: 0.1204, step time: 0.1229\n",
      "190/223, train_loss: 0.1170, step time: 0.1039\n",
      "191/223, train_loss: 0.1251, step time: 0.1007\n",
      "192/223, train_loss: 0.1265, step time: 0.1126\n",
      "193/223, train_loss: 0.1084, step time: 0.1206\n",
      "194/223, train_loss: 0.1187, step time: 0.1095\n",
      "195/223, train_loss: 0.1391, step time: 0.1148\n",
      "196/223, train_loss: 0.1383, step time: 0.1295\n",
      "197/223, train_loss: 0.1224, step time: 0.1229\n",
      "198/223, train_loss: 0.1298, step time: 0.1035\n",
      "199/223, train_loss: 0.1157, step time: 0.1163\n",
      "200/223, train_loss: 0.1366, step time: 0.1126\n",
      "201/223, train_loss: 0.1374, step time: 0.1075\n",
      "202/223, train_loss: 0.1180, step time: 0.1137\n",
      "203/223, train_loss: 0.1375, step time: 0.1165\n",
      "204/223, train_loss: 0.1226, step time: 0.1288\n",
      "205/223, train_loss: 0.1198, step time: 0.1039\n",
      "206/223, train_loss: 0.1265, step time: 0.0997\n",
      "207/223, train_loss: 0.1285, step time: 0.1082\n",
      "208/223, train_loss: 0.1251, step time: 0.1006\n",
      "209/223, train_loss: 0.1097, step time: 0.1119\n",
      "210/223, train_loss: 0.1218, step time: 0.1131\n",
      "211/223, train_loss: 0.1285, step time: 0.1205\n",
      "212/223, train_loss: 0.1257, step time: 0.1143\n",
      "213/223, train_loss: 0.1100, step time: 0.1148\n",
      "214/223, train_loss: 0.1288, step time: 0.1077\n",
      "215/223, train_loss: 0.1313, step time: 0.1348\n",
      "216/223, train_loss: 0.1105, step time: 0.0991\n",
      "217/223, train_loss: 0.1253, step time: 0.0990\n",
      "218/223, train_loss: 0.1149, step time: 0.0999\n",
      "219/223, train_loss: 0.1224, step time: 0.1159\n",
      "220/223, train_loss: 0.1241, step time: 0.1005\n",
      "221/223, train_loss: 0.1146, step time: 0.1006\n",
      "222/223, train_loss: 0.1210, step time: 0.0993\n",
      "223/223, train_loss: 0.1135, step time: 0.0996\n",
      "epoch 62 average loss: 0.1254\n",
      "time consuming of epoch 62 is: 91.1480\n",
      "----------\n",
      "epoch 63/300\n",
      "1/223, train_loss: 0.1303, step time: 0.1055\n",
      "2/223, train_loss: 0.1175, step time: 0.1000\n",
      "3/223, train_loss: 0.1285, step time: 0.1014\n",
      "4/223, train_loss: 0.1331, step time: 0.1014\n",
      "5/223, train_loss: 0.1274, step time: 0.0998\n",
      "6/223, train_loss: 0.1322, step time: 0.1121\n",
      "7/223, train_loss: 0.1289, step time: 0.1095\n",
      "8/223, train_loss: 0.1328, step time: 0.1140\n",
      "9/223, train_loss: 0.1327, step time: 0.1081\n",
      "10/223, train_loss: 0.1307, step time: 0.1211\n",
      "11/223, train_loss: 0.1137, step time: 0.1019\n",
      "12/223, train_loss: 0.1160, step time: 0.1001\n",
      "13/223, train_loss: 0.1316, step time: 0.1028\n",
      "14/223, train_loss: 0.1279, step time: 0.1005\n",
      "15/223, train_loss: 0.1180, step time: 0.1170\n",
      "16/223, train_loss: 0.1175, step time: 0.1011\n",
      "17/223, train_loss: 0.1095, step time: 0.1011\n",
      "18/223, train_loss: 0.1186, step time: 0.1308\n",
      "19/223, train_loss: 0.1219, step time: 0.1323\n",
      "20/223, train_loss: 0.1186, step time: 0.1016\n",
      "21/223, train_loss: 0.1332, step time: 0.1169\n",
      "22/223, train_loss: 0.1166, step time: 0.1008\n",
      "23/223, train_loss: 0.1335, step time: 0.1015\n",
      "24/223, train_loss: 0.1189, step time: 0.1003\n",
      "25/223, train_loss: 0.1169, step time: 0.0993\n",
      "26/223, train_loss: 0.1406, step time: 0.1004\n",
      "27/223, train_loss: 0.1194, step time: 0.1008\n",
      "28/223, train_loss: 0.1304, step time: 0.1153\n",
      "29/223, train_loss: 0.1322, step time: 0.1130\n",
      "30/223, train_loss: 0.1394, step time: 0.1006\n",
      "31/223, train_loss: 0.1308, step time: 0.1255\n",
      "32/223, train_loss: 0.1224, step time: 0.1133\n",
      "33/223, train_loss: 0.1453, step time: 0.1062\n",
      "34/223, train_loss: 0.1139, step time: 0.1172\n",
      "35/223, train_loss: 0.1212, step time: 0.1130\n",
      "36/223, train_loss: 0.1348, step time: 0.1004\n",
      "37/223, train_loss: 0.1214, step time: 0.1152\n",
      "38/223, train_loss: 0.1321, step time: 0.1022\n",
      "39/223, train_loss: 0.1381, step time: 0.1199\n",
      "40/223, train_loss: 0.1138, step time: 0.1065\n",
      "41/223, train_loss: 0.1408, step time: 0.1014\n",
      "42/223, train_loss: 0.1317, step time: 0.1223\n",
      "43/223, train_loss: 0.1217, step time: 0.1023\n",
      "44/223, train_loss: 0.1221, step time: 0.1002\n",
      "45/223, train_loss: 0.1356, step time: 0.1164\n",
      "46/223, train_loss: 0.1311, step time: 0.1001\n",
      "47/223, train_loss: 0.1336, step time: 0.1104\n",
      "48/223, train_loss: 0.1162, step time: 0.1027\n",
      "49/223, train_loss: 0.1176, step time: 0.1271\n",
      "50/223, train_loss: 0.1324, step time: 0.1028\n",
      "51/223, train_loss: 0.1183, step time: 0.1006\n",
      "52/223, train_loss: 0.1246, step time: 0.1049\n",
      "53/223, train_loss: 0.1258, step time: 0.1047\n",
      "54/223, train_loss: 0.1240, step time: 0.1081\n",
      "55/223, train_loss: 0.1169, step time: 0.1120\n",
      "56/223, train_loss: 0.1189, step time: 0.1006\n",
      "57/223, train_loss: 0.1141, step time: 0.1105\n",
      "58/223, train_loss: 0.1313, step time: 0.1129\n",
      "59/223, train_loss: 0.1258, step time: 0.1179\n",
      "60/223, train_loss: 0.1148, step time: 0.1303\n",
      "61/223, train_loss: 0.1210, step time: 0.1175\n",
      "62/223, train_loss: 0.1203, step time: 0.1233\n",
      "63/223, train_loss: 0.1125, step time: 0.1113\n",
      "64/223, train_loss: 0.1275, step time: 0.1019\n",
      "65/223, train_loss: 0.1276, step time: 0.1387\n",
      "66/223, train_loss: 0.1202, step time: 0.1126\n",
      "67/223, train_loss: 0.1327, step time: 0.1182\n",
      "68/223, train_loss: 0.1298, step time: 0.1013\n",
      "69/223, train_loss: 0.1042, step time: 0.1214\n",
      "70/223, train_loss: 0.1084, step time: 0.1266\n",
      "71/223, train_loss: 0.1294, step time: 0.1008\n",
      "72/223, train_loss: 0.1320, step time: 0.1008\n",
      "73/223, train_loss: 0.1314, step time: 0.1262\n",
      "74/223, train_loss: 0.1093, step time: 0.0999\n",
      "75/223, train_loss: 0.1227, step time: 0.1097\n",
      "76/223, train_loss: 0.1491, step time: 0.1142\n",
      "77/223, train_loss: 0.1218, step time: 0.1069\n",
      "78/223, train_loss: 0.1185, step time: 0.1196\n",
      "79/223, train_loss: 0.1358, step time: 0.1078\n",
      "80/223, train_loss: 0.1266, step time: 0.1315\n",
      "81/223, train_loss: 0.1319, step time: 0.1014\n",
      "82/223, train_loss: 0.1301, step time: 0.1160\n",
      "83/223, train_loss: 0.1276, step time: 0.1000\n",
      "84/223, train_loss: 0.1194, step time: 0.1205\n",
      "85/223, train_loss: 0.1190, step time: 0.1003\n",
      "86/223, train_loss: 0.1256, step time: 0.1006\n",
      "87/223, train_loss: 0.1210, step time: 0.0990\n",
      "88/223, train_loss: 0.1159, step time: 0.0994\n",
      "89/223, train_loss: 0.1087, step time: 0.1008\n",
      "90/223, train_loss: 0.1232, step time: 0.1077\n",
      "91/223, train_loss: 0.1321, step time: 0.1187\n",
      "92/223, train_loss: 0.1157, step time: 0.1051\n",
      "93/223, train_loss: 0.1210, step time: 0.1158\n",
      "94/223, train_loss: 0.1349, step time: 0.1107\n",
      "95/223, train_loss: 0.1189, step time: 0.1007\n",
      "96/223, train_loss: 0.1192, step time: 0.1014\n",
      "97/223, train_loss: 0.1300, step time: 0.1132\n",
      "98/223, train_loss: 0.1179, step time: 0.1066\n",
      "99/223, train_loss: 0.1267, step time: 0.1137\n",
      "100/223, train_loss: 0.1264, step time: 0.1136\n",
      "101/223, train_loss: 0.1245, step time: 0.1003\n",
      "102/223, train_loss: 0.1106, step time: 0.1077\n",
      "103/223, train_loss: 0.1306, step time: 0.1002\n",
      "104/223, train_loss: 0.1300, step time: 0.1001\n",
      "105/223, train_loss: 0.1237, step time: 0.1036\n",
      "106/223, train_loss: 0.1148, step time: 0.1083\n",
      "107/223, train_loss: 0.1140, step time: 0.1127\n",
      "108/223, train_loss: 0.1231, step time: 0.1071\n",
      "109/223, train_loss: 0.1181, step time: 0.1085\n",
      "110/223, train_loss: 0.1271, step time: 0.1072\n",
      "111/223, train_loss: 0.1247, step time: 0.1203\n",
      "112/223, train_loss: 0.1253, step time: 0.1171\n",
      "113/223, train_loss: 0.1266, step time: 0.1122\n",
      "114/223, train_loss: 0.1250, step time: 0.1006\n",
      "115/223, train_loss: 0.1325, step time: 0.1009\n",
      "116/223, train_loss: 0.1207, step time: 0.1142\n",
      "117/223, train_loss: 0.1257, step time: 0.1067\n",
      "118/223, train_loss: 0.1136, step time: 0.1416\n",
      "119/223, train_loss: 0.1188, step time: 0.1200\n",
      "120/223, train_loss: 0.1230, step time: 0.1007\n",
      "121/223, train_loss: 0.1308, step time: 0.1095\n",
      "122/223, train_loss: 0.1241, step time: 0.1030\n",
      "123/223, train_loss: 0.1280, step time: 0.1042\n",
      "124/223, train_loss: 0.1315, step time: 0.1279\n",
      "125/223, train_loss: 0.1197, step time: 0.1104\n",
      "126/223, train_loss: 0.1452, step time: 0.1155\n",
      "127/223, train_loss: 0.1268, step time: 0.1003\n",
      "128/223, train_loss: 0.1171, step time: 0.1007\n",
      "129/223, train_loss: 0.1168, step time: 0.0992\n",
      "130/223, train_loss: 0.1160, step time: 0.0994\n",
      "131/223, train_loss: 0.1161, step time: 0.0996\n",
      "132/223, train_loss: 0.1329, step time: 0.1008\n",
      "133/223, train_loss: 0.1194, step time: 0.0998\n",
      "134/223, train_loss: 0.3244, step time: 0.0996\n",
      "135/223, train_loss: 0.1247, step time: 0.0997\n",
      "136/223, train_loss: 0.1107, step time: 0.1038\n",
      "137/223, train_loss: 0.1249, step time: 0.1178\n",
      "138/223, train_loss: 0.1196, step time: 0.1167\n",
      "139/223, train_loss: 0.1142, step time: 0.1350\n",
      "140/223, train_loss: 0.1085, step time: 0.1013\n",
      "141/223, train_loss: 0.1174, step time: 0.1000\n",
      "142/223, train_loss: 0.1203, step time: 0.1001\n",
      "143/223, train_loss: 0.1147, step time: 0.1158\n",
      "144/223, train_loss: 0.1366, step time: 0.1143\n",
      "145/223, train_loss: 0.1141, step time: 0.1130\n",
      "146/223, train_loss: 0.1091, step time: 0.1096\n",
      "147/223, train_loss: 0.1181, step time: 0.0993\n",
      "148/223, train_loss: 0.1324, step time: 0.1100\n",
      "149/223, train_loss: 0.1100, step time: 0.1001\n",
      "150/223, train_loss: 0.1216, step time: 0.1039\n",
      "151/223, train_loss: 0.1337, step time: 0.1116\n",
      "152/223, train_loss: 0.1295, step time: 0.0992\n",
      "153/223, train_loss: 0.1129, step time: 0.1178\n",
      "154/223, train_loss: 0.1285, step time: 0.1031\n",
      "155/223, train_loss: 0.1214, step time: 0.1138\n",
      "156/223, train_loss: 0.1204, step time: 0.0989\n",
      "157/223, train_loss: 0.1284, step time: 0.1086\n",
      "158/223, train_loss: 0.1310, step time: 0.1216\n",
      "159/223, train_loss: 0.1256, step time: 0.1018\n",
      "160/223, train_loss: 0.1266, step time: 0.1158\n",
      "161/223, train_loss: 0.1383, step time: 0.1049\n",
      "162/223, train_loss: 0.1171, step time: 0.1364\n",
      "163/223, train_loss: 0.1525, step time: 0.1061\n",
      "164/223, train_loss: 0.1243, step time: 0.1032\n",
      "165/223, train_loss: 0.1200, step time: 0.1015\n",
      "166/223, train_loss: 0.1253, step time: 0.1046\n",
      "167/223, train_loss: 0.1165, step time: 0.1154\n",
      "168/223, train_loss: 0.1391, step time: 0.1018\n",
      "169/223, train_loss: 0.1170, step time: 0.1237\n",
      "170/223, train_loss: 0.1262, step time: 0.1005\n",
      "171/223, train_loss: 0.1229, step time: 0.1170\n",
      "172/223, train_loss: 0.1597, step time: 0.1006\n",
      "173/223, train_loss: 0.1327, step time: 0.1252\n",
      "174/223, train_loss: 0.1355, step time: 0.1378\n",
      "175/223, train_loss: 0.1501, step time: 0.1007\n",
      "176/223, train_loss: 0.1379, step time: 0.1001\n",
      "177/223, train_loss: 0.1198, step time: 0.1001\n",
      "178/223, train_loss: 0.1272, step time: 0.1062\n",
      "179/223, train_loss: 0.1102, step time: 0.1165\n",
      "180/223, train_loss: 0.1155, step time: 0.1306\n",
      "181/223, train_loss: 0.1279, step time: 0.1072\n",
      "182/223, train_loss: 0.1385, step time: 0.1197\n",
      "183/223, train_loss: 0.1262, step time: 0.1017\n",
      "184/223, train_loss: 0.1203, step time: 0.1010\n",
      "185/223, train_loss: 0.1164, step time: 0.0999\n",
      "186/223, train_loss: 0.1295, step time: 0.1140\n",
      "187/223, train_loss: 0.1262, step time: 0.1118\n",
      "188/223, train_loss: 0.1075, step time: 0.1020\n",
      "189/223, train_loss: 0.1328, step time: 0.1079\n",
      "190/223, train_loss: 0.1248, step time: 0.1061\n",
      "191/223, train_loss: 0.1160, step time: 0.1152\n",
      "192/223, train_loss: 0.1356, step time: 0.1093\n",
      "193/223, train_loss: 0.1110, step time: 0.1085\n",
      "194/223, train_loss: 0.1259, step time: 0.1127\n",
      "195/223, train_loss: 0.1254, step time: 0.1139\n",
      "196/223, train_loss: 0.1255, step time: 0.1122\n",
      "197/223, train_loss: 0.1451, step time: 0.1001\n",
      "198/223, train_loss: 0.1152, step time: 0.1017\n",
      "199/223, train_loss: 0.1350, step time: 0.1282\n",
      "200/223, train_loss: 0.1193, step time: 0.1196\n",
      "201/223, train_loss: 0.1267, step time: 0.1061\n",
      "202/223, train_loss: 0.1201, step time: 0.1004\n",
      "203/223, train_loss: 0.1244, step time: 0.1004\n",
      "204/223, train_loss: 0.1237, step time: 0.1138\n",
      "205/223, train_loss: 0.1167, step time: 0.1081\n",
      "206/223, train_loss: 0.1257, step time: 0.0991\n",
      "207/223, train_loss: 0.1231, step time: 0.1001\n",
      "208/223, train_loss: 0.1256, step time: 0.1055\n",
      "209/223, train_loss: 0.1317, step time: 0.1144\n",
      "210/223, train_loss: 0.1343, step time: 0.1007\n",
      "211/223, train_loss: 0.1132, step time: 0.1127\n",
      "212/223, train_loss: 0.1201, step time: 0.1228\n",
      "213/223, train_loss: 0.1143, step time: 0.1015\n",
      "214/223, train_loss: 0.1332, step time: 0.0999\n",
      "215/223, train_loss: 0.1136, step time: 0.1004\n",
      "216/223, train_loss: 0.1276, step time: 0.1140\n",
      "217/223, train_loss: 0.1228, step time: 0.0996\n",
      "218/223, train_loss: 0.1165, step time: 0.1008\n",
      "219/223, train_loss: 0.1204, step time: 0.1007\n",
      "220/223, train_loss: 0.1245, step time: 0.1021\n",
      "221/223, train_loss: 0.1247, step time: 0.0993\n",
      "222/223, train_loss: 0.1296, step time: 0.0994\n",
      "223/223, train_loss: 0.1146, step time: 0.0995\n",
      "epoch 63 average loss: 0.1254\n",
      "time consuming of epoch 63 is: 88.1241\n",
      "----------\n",
      "epoch 64/300\n",
      "1/223, train_loss: 0.1306, step time: 0.1248\n",
      "2/223, train_loss: 0.1317, step time: 0.1005\n",
      "3/223, train_loss: 0.1259, step time: 0.1011\n",
      "4/223, train_loss: 0.1226, step time: 0.1054\n",
      "5/223, train_loss: 0.1110, step time: 0.1012\n",
      "6/223, train_loss: 0.1215, step time: 0.1004\n",
      "7/223, train_loss: 0.1285, step time: 0.1022\n",
      "8/223, train_loss: 0.1190, step time: 0.1014\n",
      "9/223, train_loss: 0.1320, step time: 0.1115\n",
      "10/223, train_loss: 0.1142, step time: 0.1318\n",
      "11/223, train_loss: 0.1231, step time: 0.1060\n",
      "12/223, train_loss: 0.1340, step time: 0.1196\n",
      "13/223, train_loss: 0.1424, step time: 0.1188\n",
      "14/223, train_loss: 0.1129, step time: 0.1680\n",
      "15/223, train_loss: 0.1232, step time: 0.1127\n",
      "16/223, train_loss: 0.1251, step time: 0.1156\n",
      "17/223, train_loss: 0.1269, step time: 0.1002\n",
      "18/223, train_loss: 0.1149, step time: 0.1225\n",
      "19/223, train_loss: 0.1170, step time: 0.1168\n",
      "20/223, train_loss: 0.1213, step time: 0.1183\n",
      "21/223, train_loss: 0.1077, step time: 0.1335\n",
      "22/223, train_loss: 0.1472, step time: 0.1000\n",
      "23/223, train_loss: 0.1307, step time: 0.1004\n",
      "24/223, train_loss: 0.1222, step time: 0.1076\n",
      "25/223, train_loss: 0.1264, step time: 0.1047\n",
      "26/223, train_loss: 0.1240, step time: 0.0998\n",
      "27/223, train_loss: 0.1163, step time: 0.1070\n",
      "28/223, train_loss: 0.1320, step time: 0.1168\n",
      "29/223, train_loss: 0.1228, step time: 0.1219\n",
      "30/223, train_loss: 0.1107, step time: 0.0999\n",
      "31/223, train_loss: 0.1236, step time: 0.0995\n",
      "32/223, train_loss: 0.1208, step time: 0.1130\n",
      "33/223, train_loss: 0.1336, step time: 0.1242\n",
      "34/223, train_loss: 0.1245, step time: 0.1480\n",
      "35/223, train_loss: 0.1144, step time: 0.1132\n",
      "36/223, train_loss: 0.1168, step time: 0.1009\n",
      "37/223, train_loss: 0.1298, step time: 0.1078\n",
      "38/223, train_loss: 0.1242, step time: 0.1011\n",
      "39/223, train_loss: 0.1232, step time: 0.1009\n",
      "40/223, train_loss: 0.1219, step time: 0.1092\n",
      "41/223, train_loss: 0.1315, step time: 0.1112\n",
      "42/223, train_loss: 0.1438, step time: 0.1078\n",
      "43/223, train_loss: 0.1221, step time: 0.1169\n",
      "44/223, train_loss: 0.1285, step time: 0.1013\n",
      "45/223, train_loss: 0.1167, step time: 0.1135\n",
      "46/223, train_loss: 0.1430, step time: 0.1066\n",
      "47/223, train_loss: 0.1143, step time: 0.1169\n",
      "48/223, train_loss: 0.1331, step time: 0.0994\n",
      "49/223, train_loss: 0.1199, step time: 0.1215\n",
      "50/223, train_loss: 0.1216, step time: 0.1305\n",
      "51/223, train_loss: 0.1198, step time: 0.1041\n",
      "52/223, train_loss: 0.3157, step time: 0.1027\n",
      "53/223, train_loss: 0.1070, step time: 0.1236\n",
      "54/223, train_loss: 0.1188, step time: 0.1115\n",
      "55/223, train_loss: 0.1270, step time: 0.1326\n",
      "56/223, train_loss: 0.1305, step time: 0.1108\n",
      "57/223, train_loss: 0.1273, step time: 0.1167\n",
      "58/223, train_loss: 0.1297, step time: 0.0997\n",
      "59/223, train_loss: 0.1185, step time: 0.1095\n",
      "60/223, train_loss: 0.1319, step time: 0.1070\n",
      "61/223, train_loss: 0.1158, step time: 0.1096\n",
      "62/223, train_loss: 0.1188, step time: 0.1006\n",
      "63/223, train_loss: 0.1193, step time: 0.1303\n",
      "64/223, train_loss: 0.1279, step time: 0.1038\n",
      "65/223, train_loss: 0.1391, step time: 0.1136\n",
      "66/223, train_loss: 0.1305, step time: 0.1006\n",
      "67/223, train_loss: 0.1385, step time: 0.0997\n",
      "68/223, train_loss: 0.1530, step time: 0.1004\n",
      "69/223, train_loss: 0.1239, step time: 0.1000\n",
      "70/223, train_loss: 0.1284, step time: 0.0996\n",
      "71/223, train_loss: 0.1141, step time: 0.1003\n",
      "72/223, train_loss: 0.1409, step time: 0.1114\n",
      "73/223, train_loss: 0.1352, step time: 0.1191\n",
      "74/223, train_loss: 0.1399, step time: 0.1126\n",
      "75/223, train_loss: 0.1315, step time: 0.1007\n",
      "76/223, train_loss: 0.1290, step time: 0.1056\n",
      "77/223, train_loss: 0.1167, step time: 0.1211\n",
      "78/223, train_loss: 0.1159, step time: 0.1014\n",
      "79/223, train_loss: 0.1315, step time: 0.1007\n",
      "80/223, train_loss: 0.1296, step time: 0.1006\n",
      "81/223, train_loss: 0.1190, step time: 0.1318\n",
      "82/223, train_loss: 0.1233, step time: 0.0998\n",
      "83/223, train_loss: 0.1200, step time: 0.1111\n",
      "84/223, train_loss: 0.1164, step time: 0.1065\n",
      "85/223, train_loss: 0.1322, step time: 0.1130\n",
      "86/223, train_loss: 0.1221, step time: 0.1110\n",
      "87/223, train_loss: 0.1341, step time: 0.1180\n",
      "88/223, train_loss: 0.1148, step time: 0.1062\n",
      "89/223, train_loss: 0.1232, step time: 0.1169\n",
      "90/223, train_loss: 0.1287, step time: 0.1107\n",
      "91/223, train_loss: 0.1186, step time: 0.1115\n",
      "92/223, train_loss: 0.1175, step time: 0.1094\n",
      "93/223, train_loss: 0.1226, step time: 0.1034\n",
      "94/223, train_loss: 0.1308, step time: 0.1004\n",
      "95/223, train_loss: 0.1136, step time: 0.1035\n",
      "96/223, train_loss: 0.1202, step time: 0.0996\n",
      "97/223, train_loss: 0.1398, step time: 0.1109\n",
      "98/223, train_loss: 0.1134, step time: 0.1001\n",
      "99/223, train_loss: 0.1360, step time: 0.1008\n",
      "100/223, train_loss: 0.1268, step time: 0.1107\n",
      "101/223, train_loss: 0.1510, step time: 0.1213\n",
      "102/223, train_loss: 0.1154, step time: 0.1287\n",
      "103/223, train_loss: 0.1286, step time: 0.1006\n",
      "104/223, train_loss: 0.1171, step time: 0.1002\n",
      "105/223, train_loss: 0.1168, step time: 0.1006\n",
      "106/223, train_loss: 0.1311, step time: 0.1002\n",
      "107/223, train_loss: 0.1295, step time: 0.1014\n",
      "108/223, train_loss: 0.1157, step time: 0.1116\n",
      "109/223, train_loss: 0.1199, step time: 0.1140\n",
      "110/223, train_loss: 0.1223, step time: 0.1066\n",
      "111/223, train_loss: 0.1117, step time: 0.1013\n",
      "112/223, train_loss: 0.1182, step time: 0.1058\n",
      "113/223, train_loss: 0.1257, step time: 0.1214\n",
      "114/223, train_loss: 0.1244, step time: 0.1191\n",
      "115/223, train_loss: 0.1355, step time: 0.1178\n",
      "116/223, train_loss: 0.1364, step time: 0.1005\n",
      "117/223, train_loss: 0.1254, step time: 0.1111\n",
      "118/223, train_loss: 0.1219, step time: 0.1006\n",
      "119/223, train_loss: 0.1152, step time: 0.1005\n",
      "120/223, train_loss: 0.1264, step time: 0.0998\n",
      "121/223, train_loss: 0.1185, step time: 0.1003\n",
      "122/223, train_loss: 0.1207, step time: 0.1002\n",
      "123/223, train_loss: 0.1217, step time: 0.1012\n",
      "124/223, train_loss: 0.1242, step time: 0.1075\n",
      "125/223, train_loss: 0.1097, step time: 0.1052\n",
      "126/223, train_loss: 0.1194, step time: 0.1158\n",
      "127/223, train_loss: 0.1168, step time: 0.1107\n",
      "128/223, train_loss: 0.1197, step time: 0.1010\n",
      "129/223, train_loss: 0.1206, step time: 0.1003\n",
      "130/223, train_loss: 0.1161, step time: 0.1223\n",
      "131/223, train_loss: 0.1296, step time: 0.1031\n",
      "132/223, train_loss: 0.1258, step time: 0.1056\n",
      "133/223, train_loss: 0.1270, step time: 0.1003\n",
      "134/223, train_loss: 0.1170, step time: 0.1026\n",
      "135/223, train_loss: 0.1270, step time: 0.1018\n",
      "136/223, train_loss: 0.1181, step time: 0.1043\n",
      "137/223, train_loss: 0.1318, step time: 0.1079\n",
      "138/223, train_loss: 0.1361, step time: 0.1039\n",
      "139/223, train_loss: 0.1153, step time: 0.1110\n",
      "140/223, train_loss: 0.1256, step time: 0.1121\n",
      "141/223, train_loss: 0.1118, step time: 0.1228\n",
      "142/223, train_loss: 0.1415, step time: 0.1098\n",
      "143/223, train_loss: 0.1352, step time: 0.1246\n",
      "144/223, train_loss: 0.1121, step time: 0.1038\n",
      "145/223, train_loss: 0.1231, step time: 0.1006\n",
      "146/223, train_loss: 0.1237, step time: 0.1002\n",
      "147/223, train_loss: 0.1134, step time: 0.1009\n",
      "148/223, train_loss: 0.1291, step time: 0.1155\n",
      "149/223, train_loss: 0.1128, step time: 0.1007\n",
      "150/223, train_loss: 0.1221, step time: 0.1059\n",
      "151/223, train_loss: 0.1146, step time: 0.1087\n",
      "152/223, train_loss: 0.1150, step time: 0.1009\n",
      "153/223, train_loss: 0.1149, step time: 0.1084\n",
      "154/223, train_loss: 0.1435, step time: 0.1054\n",
      "155/223, train_loss: 0.1210, step time: 0.1140\n",
      "156/223, train_loss: 0.1325, step time: 0.1005\n",
      "157/223, train_loss: 0.1103, step time: 0.1270\n",
      "158/223, train_loss: 0.1114, step time: 0.1121\n",
      "159/223, train_loss: 0.1139, step time: 0.1133\n",
      "160/223, train_loss: 0.1181, step time: 0.1119\n",
      "161/223, train_loss: 0.1175, step time: 0.1055\n",
      "162/223, train_loss: 0.1285, step time: 0.1177\n",
      "163/223, train_loss: 0.1144, step time: 0.1198\n",
      "164/223, train_loss: 0.1137, step time: 0.0995\n",
      "165/223, train_loss: 0.1348, step time: 0.1104\n",
      "166/223, train_loss: 0.1189, step time: 0.1268\n",
      "167/223, train_loss: 0.1214, step time: 0.1052\n",
      "168/223, train_loss: 0.1288, step time: 0.1002\n",
      "169/223, train_loss: 0.1201, step time: 0.1139\n",
      "170/223, train_loss: 0.1201, step time: 0.1046\n",
      "171/223, train_loss: 0.1215, step time: 0.1085\n",
      "172/223, train_loss: 0.1061, step time: 0.1009\n",
      "173/223, train_loss: 0.1192, step time: 0.1002\n",
      "174/223, train_loss: 0.1323, step time: 0.1022\n",
      "175/223, train_loss: 0.1331, step time: 0.1096\n",
      "176/223, train_loss: 0.1254, step time: 0.1194\n",
      "177/223, train_loss: 0.1294, step time: 0.1001\n",
      "178/223, train_loss: 0.1311, step time: 0.1001\n",
      "179/223, train_loss: 0.1225, step time: 0.1002\n",
      "180/223, train_loss: 0.1299, step time: 0.1011\n",
      "181/223, train_loss: 0.1261, step time: 0.1129\n",
      "182/223, train_loss: 0.1207, step time: 0.1108\n",
      "183/223, train_loss: 0.1222, step time: 0.1200\n",
      "184/223, train_loss: 0.1089, step time: 0.1017\n",
      "185/223, train_loss: 0.1326, step time: 0.1076\n",
      "186/223, train_loss: 0.1189, step time: 0.0998\n",
      "187/223, train_loss: 0.1381, step time: 0.0997\n",
      "188/223, train_loss: 0.1476, step time: 0.1046\n",
      "189/223, train_loss: 0.1329, step time: 0.1047\n",
      "190/223, train_loss: 0.1177, step time: 0.1162\n",
      "191/223, train_loss: 0.1221, step time: 0.1032\n",
      "192/223, train_loss: 0.1422, step time: 0.1084\n",
      "193/223, train_loss: 0.1149, step time: 0.1007\n",
      "194/223, train_loss: 0.1125, step time: 0.1060\n",
      "195/223, train_loss: 0.1157, step time: 0.1225\n",
      "196/223, train_loss: 0.1382, step time: 0.1058\n",
      "197/223, train_loss: 0.1386, step time: 0.1123\n",
      "198/223, train_loss: 0.1148, step time: 0.1092\n",
      "199/223, train_loss: 0.1222, step time: 0.1142\n",
      "200/223, train_loss: 0.1251, step time: 0.1056\n",
      "201/223, train_loss: 0.1241, step time: 0.1105\n",
      "202/223, train_loss: 0.1329, step time: 0.1003\n",
      "203/223, train_loss: 0.1202, step time: 0.0997\n",
      "204/223, train_loss: 0.1221, step time: 0.1009\n",
      "205/223, train_loss: 0.1224, step time: 0.1095\n",
      "206/223, train_loss: 0.1208, step time: 0.1134\n",
      "207/223, train_loss: 0.1335, step time: 0.1036\n",
      "208/223, train_loss: 0.1162, step time: 0.1002\n",
      "209/223, train_loss: 0.1100, step time: 0.1128\n",
      "210/223, train_loss: 0.1206, step time: 0.1028\n",
      "211/223, train_loss: 0.1146, step time: 0.1029\n",
      "212/223, train_loss: 0.1145, step time: 0.1007\n",
      "213/223, train_loss: 0.1189, step time: 0.1192\n",
      "214/223, train_loss: 0.1344, step time: 0.1008\n",
      "215/223, train_loss: 0.1100, step time: 0.1011\n",
      "216/223, train_loss: 0.1176, step time: 0.1013\n",
      "217/223, train_loss: 0.1190, step time: 0.1006\n",
      "218/223, train_loss: 0.1166, step time: 0.1225\n",
      "219/223, train_loss: 0.1151, step time: 0.1003\n",
      "220/223, train_loss: 0.1251, step time: 0.0993\n",
      "221/223, train_loss: 0.1223, step time: 0.0994\n",
      "222/223, train_loss: 0.1451, step time: 0.0997\n",
      "223/223, train_loss: 0.1139, step time: 0.0997\n",
      "epoch 64 average loss: 0.1248\n",
      "time consuming of epoch 64 is: 89.0980\n",
      "----------\n",
      "epoch 65/300\n",
      "1/223, train_loss: 0.1214, step time: 0.1058\n",
      "2/223, train_loss: 0.1287, step time: 0.1004\n",
      "3/223, train_loss: 0.1084, step time: 0.1178\n",
      "4/223, train_loss: 0.1228, step time: 0.1002\n",
      "5/223, train_loss: 0.1218, step time: 0.1096\n",
      "6/223, train_loss: 0.1300, step time: 0.0999\n",
      "7/223, train_loss: 0.1183, step time: 0.1049\n",
      "8/223, train_loss: 0.1333, step time: 0.1115\n",
      "9/223, train_loss: 0.1110, step time: 0.1776\n",
      "10/223, train_loss: 0.1236, step time: 0.0999\n",
      "11/223, train_loss: 0.1303, step time: 0.1269\n",
      "12/223, train_loss: 0.1308, step time: 0.1010\n",
      "13/223, train_loss: 0.1155, step time: 0.1338\n",
      "14/223, train_loss: 0.1291, step time: 0.1045\n",
      "15/223, train_loss: 0.1151, step time: 0.1004\n",
      "16/223, train_loss: 0.1103, step time: 0.0994\n",
      "17/223, train_loss: 0.1151, step time: 0.1144\n",
      "18/223, train_loss: 0.1172, step time: 0.1159\n",
      "19/223, train_loss: 0.1255, step time: 0.1001\n",
      "20/223, train_loss: 0.1275, step time: 0.1002\n",
      "21/223, train_loss: 0.1241, step time: 0.1128\n",
      "22/223, train_loss: 0.1133, step time: 0.1067\n",
      "23/223, train_loss: 0.1196, step time: 0.1060\n",
      "24/223, train_loss: 0.1158, step time: 0.1003\n",
      "25/223, train_loss: 0.1196, step time: 0.1072\n",
      "26/223, train_loss: 0.1223, step time: 0.1091\n",
      "27/223, train_loss: 0.1099, step time: 0.1008\n",
      "28/223, train_loss: 0.1163, step time: 0.1012\n",
      "29/223, train_loss: 0.1535, step time: 0.1080\n",
      "30/223, train_loss: 0.1086, step time: 0.1596\n",
      "31/223, train_loss: 0.1271, step time: 0.1006\n",
      "32/223, train_loss: 0.1289, step time: 0.1003\n",
      "33/223, train_loss: 0.1502, step time: 0.1006\n",
      "34/223, train_loss: 0.1212, step time: 0.1004\n",
      "35/223, train_loss: 0.1212, step time: 0.0994\n",
      "36/223, train_loss: 0.1346, step time: 0.1264\n",
      "37/223, train_loss: 0.1296, step time: 0.1337\n",
      "38/223, train_loss: 0.1338, step time: 0.0991\n",
      "39/223, train_loss: 0.1420, step time: 0.1013\n",
      "40/223, train_loss: 0.1297, step time: 0.1088\n",
      "41/223, train_loss: 0.1227, step time: 0.1106\n",
      "42/223, train_loss: 0.1144, step time: 0.1001\n",
      "43/223, train_loss: 0.1201, step time: 0.1001\n",
      "44/223, train_loss: 0.1363, step time: 0.1002\n",
      "45/223, train_loss: 0.1250, step time: 0.1101\n",
      "46/223, train_loss: 0.1303, step time: 0.1095\n",
      "47/223, train_loss: 0.1394, step time: 0.1205\n",
      "48/223, train_loss: 0.1202, step time: 0.1102\n",
      "49/223, train_loss: 0.1202, step time: 0.1072\n",
      "50/223, train_loss: 0.1273, step time: 0.1090\n",
      "51/223, train_loss: 0.1312, step time: 0.1064\n",
      "52/223, train_loss: 0.1262, step time: 0.1138\n",
      "53/223, train_loss: 0.1284, step time: 0.1098\n",
      "54/223, train_loss: 0.1255, step time: 0.1089\n",
      "55/223, train_loss: 0.1277, step time: 0.1188\n",
      "56/223, train_loss: 0.1237, step time: 0.1168\n",
      "57/223, train_loss: 0.1166, step time: 0.1158\n",
      "58/223, train_loss: 0.1185, step time: 0.1120\n",
      "59/223, train_loss: 0.1120, step time: 0.1249\n",
      "60/223, train_loss: 0.1246, step time: 0.1138\n",
      "61/223, train_loss: 0.1261, step time: 0.1097\n",
      "62/223, train_loss: 0.1191, step time: 0.1098\n",
      "63/223, train_loss: 0.1253, step time: 0.1104\n",
      "64/223, train_loss: 0.1252, step time: 0.1011\n",
      "65/223, train_loss: 0.1160, step time: 0.1001\n",
      "66/223, train_loss: 0.1374, step time: 0.1496\n",
      "67/223, train_loss: 0.1330, step time: 0.0999\n",
      "68/223, train_loss: 0.1344, step time: 0.0998\n",
      "69/223, train_loss: 0.1238, step time: 0.1005\n",
      "70/223, train_loss: 0.1163, step time: 0.1062\n",
      "71/223, train_loss: 0.1228, step time: 0.1240\n",
      "72/223, train_loss: 0.1256, step time: 0.1007\n",
      "73/223, train_loss: 0.1171, step time: 0.1018\n",
      "74/223, train_loss: 0.1183, step time: 0.0995\n",
      "75/223, train_loss: 0.1386, step time: 0.1012\n",
      "76/223, train_loss: 0.1379, step time: 0.1065\n",
      "77/223, train_loss: 0.1079, step time: 0.1040\n",
      "78/223, train_loss: 0.1302, step time: 0.0989\n",
      "79/223, train_loss: 0.1207, step time: 0.1009\n",
      "80/223, train_loss: 0.1323, step time: 0.1252\n",
      "81/223, train_loss: 0.1212, step time: 0.1172\n",
      "82/223, train_loss: 0.1182, step time: 0.0990\n",
      "83/223, train_loss: 0.1240, step time: 0.0994\n",
      "84/223, train_loss: 0.1198, step time: 0.1318\n",
      "85/223, train_loss: 0.1278, step time: 0.1151\n",
      "86/223, train_loss: 0.1057, step time: 0.1107\n",
      "87/223, train_loss: 0.1312, step time: 0.1005\n",
      "88/223, train_loss: 0.1109, step time: 0.0999\n",
      "89/223, train_loss: 0.1272, step time: 0.1145\n",
      "90/223, train_loss: 0.1256, step time: 0.1171\n",
      "91/223, train_loss: 0.1317, step time: 0.1009\n",
      "92/223, train_loss: 0.1173, step time: 0.1016\n",
      "93/223, train_loss: 0.1142, step time: 0.1128\n",
      "94/223, train_loss: 0.1145, step time: 0.1026\n",
      "95/223, train_loss: 0.1297, step time: 0.1323\n",
      "96/223, train_loss: 0.1115, step time: 0.1118\n",
      "97/223, train_loss: 0.1187, step time: 0.1064\n",
      "98/223, train_loss: 0.1237, step time: 0.1109\n",
      "99/223, train_loss: 0.1170, step time: 0.1098\n",
      "100/223, train_loss: 0.1315, step time: 0.1000\n",
      "101/223, train_loss: 0.1209, step time: 0.1096\n",
      "102/223, train_loss: 0.1228, step time: 0.1316\n",
      "103/223, train_loss: 0.1405, step time: 0.1083\n",
      "104/223, train_loss: 0.1210, step time: 0.1102\n",
      "105/223, train_loss: 0.1208, step time: 0.1008\n",
      "106/223, train_loss: 0.1303, step time: 0.1003\n",
      "107/223, train_loss: 0.1229, step time: 0.1176\n",
      "108/223, train_loss: 0.1254, step time: 0.1010\n",
      "109/223, train_loss: 0.1321, step time: 0.1002\n",
      "110/223, train_loss: 0.1189, step time: 0.1145\n",
      "111/223, train_loss: 0.1153, step time: 0.1221\n",
      "112/223, train_loss: 0.1320, step time: 0.0998\n",
      "113/223, train_loss: 0.1137, step time: 0.1081\n",
      "114/223, train_loss: 0.1136, step time: 0.1150\n",
      "115/223, train_loss: 0.1201, step time: 0.1003\n",
      "116/223, train_loss: 0.1222, step time: 0.1023\n",
      "117/223, train_loss: 0.1280, step time: 0.1150\n",
      "118/223, train_loss: 0.1391, step time: 0.1083\n",
      "119/223, train_loss: 0.1179, step time: 0.1127\n",
      "120/223, train_loss: 0.1353, step time: 0.1570\n",
      "121/223, train_loss: 0.1199, step time: 0.1086\n",
      "122/223, train_loss: 0.1292, step time: 0.1089\n",
      "123/223, train_loss: 0.1193, step time: 0.1086\n",
      "124/223, train_loss: 0.1152, step time: 0.1124\n",
      "125/223, train_loss: 0.1252, step time: 0.1099\n",
      "126/223, train_loss: 0.1230, step time: 0.1003\n",
      "127/223, train_loss: 0.1228, step time: 0.1000\n",
      "128/223, train_loss: 0.1273, step time: 0.1097\n",
      "129/223, train_loss: 0.1203, step time: 0.1410\n",
      "130/223, train_loss: 0.1274, step time: 0.1322\n",
      "131/223, train_loss: 0.1204, step time: 0.1022\n",
      "132/223, train_loss: 0.1216, step time: 0.1007\n",
      "133/223, train_loss: 0.1307, step time: 0.1153\n",
      "134/223, train_loss: 0.1168, step time: 0.1003\n",
      "135/223, train_loss: 0.1209, step time: 0.1075\n",
      "136/223, train_loss: 0.1170, step time: 0.1108\n",
      "137/223, train_loss: 0.1236, step time: 0.1101\n",
      "138/223, train_loss: 0.1326, step time: 0.1103\n",
      "139/223, train_loss: 0.1179, step time: 0.1005\n",
      "140/223, train_loss: 0.1246, step time: 0.1014\n",
      "141/223, train_loss: 0.1342, step time: 0.1265\n",
      "142/223, train_loss: 0.1174, step time: 0.1005\n",
      "143/223, train_loss: 0.1262, step time: 0.1280\n",
      "144/223, train_loss: 0.1351, step time: 0.1002\n",
      "145/223, train_loss: 0.1396, step time: 0.1036\n",
      "146/223, train_loss: 0.1169, step time: 0.1096\n",
      "147/223, train_loss: 0.1102, step time: 0.1036\n",
      "148/223, train_loss: 0.1250, step time: 0.1138\n",
      "149/223, train_loss: 0.1224, step time: 0.1004\n",
      "150/223, train_loss: 0.1244, step time: 0.0993\n",
      "151/223, train_loss: 0.1263, step time: 0.0999\n",
      "152/223, train_loss: 0.1313, step time: 0.1023\n",
      "153/223, train_loss: 0.1187, step time: 0.1232\n",
      "154/223, train_loss: 0.1263, step time: 0.1125\n",
      "155/223, train_loss: 0.1157, step time: 0.1094\n",
      "156/223, train_loss: 0.1211, step time: 0.1033\n",
      "157/223, train_loss: 0.1146, step time: 0.1044\n",
      "158/223, train_loss: 0.1195, step time: 0.1032\n",
      "159/223, train_loss: 0.1275, step time: 0.1010\n",
      "160/223, train_loss: 0.1211, step time: 0.1077\n",
      "161/223, train_loss: 0.1231, step time: 0.1022\n",
      "162/223, train_loss: 0.1194, step time: 0.1000\n",
      "163/223, train_loss: 0.1271, step time: 0.1006\n",
      "164/223, train_loss: 0.1154, step time: 0.1087\n",
      "165/223, train_loss: 0.1135, step time: 0.1143\n",
      "166/223, train_loss: 0.1146, step time: 0.1130\n",
      "167/223, train_loss: 0.1428, step time: 0.1149\n",
      "168/223, train_loss: 0.1237, step time: 0.1207\n",
      "169/223, train_loss: 0.1165, step time: 0.1084\n",
      "170/223, train_loss: 0.1325, step time: 0.1004\n",
      "171/223, train_loss: 0.1128, step time: 0.1001\n",
      "172/223, train_loss: 0.1381, step time: 0.1018\n",
      "173/223, train_loss: 0.1214, step time: 0.1100\n",
      "174/223, train_loss: 0.1189, step time: 0.1003\n",
      "175/223, train_loss: 0.1324, step time: 0.1101\n",
      "176/223, train_loss: 0.1296, step time: 0.1000\n",
      "177/223, train_loss: 0.1303, step time: 0.1013\n",
      "178/223, train_loss: 0.1257, step time: 0.0997\n",
      "179/223, train_loss: 0.1242, step time: 0.1008\n",
      "180/223, train_loss: 0.1319, step time: 0.0996\n",
      "181/223, train_loss: 0.1059, step time: 0.1003\n",
      "182/223, train_loss: 0.1174, step time: 0.1016\n",
      "183/223, train_loss: 0.1175, step time: 0.1002\n",
      "184/223, train_loss: 0.1227, step time: 0.1032\n",
      "185/223, train_loss: 0.1209, step time: 0.1063\n",
      "186/223, train_loss: 0.1064, step time: 0.1065\n",
      "187/223, train_loss: 0.1253, step time: 0.1058\n",
      "188/223, train_loss: 0.1393, step time: 0.1146\n",
      "189/223, train_loss: 0.1156, step time: 0.1149\n",
      "190/223, train_loss: 0.1289, step time: 0.1108\n",
      "191/223, train_loss: 0.1201, step time: 0.1322\n",
      "192/223, train_loss: 0.1473, step time: 0.1010\n",
      "193/223, train_loss: 0.1291, step time: 0.1104\n",
      "194/223, train_loss: 0.1363, step time: 0.1025\n",
      "195/223, train_loss: 0.1229, step time: 0.1178\n",
      "196/223, train_loss: 0.1273, step time: 0.1007\n",
      "197/223, train_loss: 0.1358, step time: 0.1006\n",
      "198/223, train_loss: 0.1434, step time: 0.1003\n",
      "199/223, train_loss: 0.1415, step time: 0.1000\n",
      "200/223, train_loss: 0.1154, step time: 0.1115\n",
      "201/223, train_loss: 0.1071, step time: 0.1018\n",
      "202/223, train_loss: 0.1305, step time: 0.1010\n",
      "203/223, train_loss: 0.1326, step time: 0.1163\n",
      "204/223, train_loss: 0.1263, step time: 0.1063\n",
      "205/223, train_loss: 0.1222, step time: 0.1132\n",
      "206/223, train_loss: 0.1251, step time: 0.0994\n",
      "207/223, train_loss: 0.1140, step time: 0.1008\n",
      "208/223, train_loss: 0.1389, step time: 0.1035\n",
      "209/223, train_loss: 0.1266, step time: 0.1114\n",
      "210/223, train_loss: 0.1300, step time: 0.1070\n",
      "211/223, train_loss: 0.1214, step time: 0.1194\n",
      "212/223, train_loss: 0.1244, step time: 0.1000\n",
      "213/223, train_loss: 0.1146, step time: 0.1125\n",
      "214/223, train_loss: 0.1222, step time: 0.1048\n",
      "215/223, train_loss: 0.1159, step time: 0.1652\n",
      "216/223, train_loss: 0.1272, step time: 0.1191\n",
      "217/223, train_loss: 0.1112, step time: 0.1008\n",
      "218/223, train_loss: 0.1296, step time: 0.1058\n",
      "219/223, train_loss: 0.1240, step time: 0.1001\n",
      "220/223, train_loss: 0.1162, step time: 0.0995\n",
      "221/223, train_loss: 0.1308, step time: 0.0992\n",
      "222/223, train_loss: 0.1172, step time: 0.0996\n",
      "223/223, train_loss: 0.3138, step time: 0.1000\n",
      "epoch 65 average loss: 0.1248\n",
      "saved new best metric model\n",
      "current epoch: 65 current mean dice: 0.8406 tc: 0.9128 wt: 0.8533 et: 0.7558\n",
      "best mean dice: 0.8406 at epoch: 65\n",
      "time consuming of epoch 65 is: 91.6685\n",
      "----------\n",
      "epoch 66/300\n",
      "1/223, train_loss: 0.1247, step time: 0.1029\n",
      "2/223, train_loss: 0.1229, step time: 0.1099\n",
      "3/223, train_loss: 0.1168, step time: 0.1115\n",
      "4/223, train_loss: 0.1185, step time: 0.1165\n",
      "5/223, train_loss: 0.1208, step time: 0.1022\n",
      "6/223, train_loss: 0.1148, step time: 0.0999\n",
      "7/223, train_loss: 0.1116, step time: 0.1096\n",
      "8/223, train_loss: 0.1403, step time: 0.1227\n",
      "9/223, train_loss: 0.1122, step time: 0.1083\n",
      "10/223, train_loss: 0.1261, step time: 0.1086\n",
      "11/223, train_loss: 0.1242, step time: 0.1256\n",
      "12/223, train_loss: 0.1382, step time: 0.1107\n",
      "13/223, train_loss: 0.1202, step time: 0.1159\n",
      "14/223, train_loss: 0.1148, step time: 0.1110\n",
      "15/223, train_loss: 0.1134, step time: 0.1083\n",
      "16/223, train_loss: 0.1196, step time: 0.1008\n",
      "17/223, train_loss: 0.1113, step time: 0.1321\n",
      "18/223, train_loss: 0.1230, step time: 0.1244\n",
      "19/223, train_loss: 0.1284, step time: 0.1125\n",
      "20/223, train_loss: 0.1164, step time: 0.1123\n",
      "21/223, train_loss: 0.1185, step time: 0.1092\n",
      "22/223, train_loss: 0.1183, step time: 0.0997\n",
      "23/223, train_loss: 0.1156, step time: 0.1013\n",
      "24/223, train_loss: 0.1293, step time: 0.1033\n",
      "25/223, train_loss: 0.1199, step time: 0.1011\n",
      "26/223, train_loss: 0.1128, step time: 0.1045\n",
      "27/223, train_loss: 0.1179, step time: 0.1034\n",
      "28/223, train_loss: 0.1169, step time: 0.1047\n",
      "29/223, train_loss: 0.1297, step time: 0.1076\n",
      "30/223, train_loss: 0.1140, step time: 0.1115\n",
      "31/223, train_loss: 0.1225, step time: 0.1105\n",
      "32/223, train_loss: 0.1303, step time: 0.1127\n",
      "33/223, train_loss: 0.1104, step time: 0.1101\n",
      "34/223, train_loss: 0.1134, step time: 0.1141\n",
      "35/223, train_loss: 0.1190, step time: 0.1159\n",
      "36/223, train_loss: 0.1189, step time: 0.1179\n",
      "37/223, train_loss: 0.1262, step time: 0.1071\n",
      "38/223, train_loss: 0.1093, step time: 0.1173\n",
      "39/223, train_loss: 0.1098, step time: 0.1218\n",
      "40/223, train_loss: 0.1342, step time: 0.1020\n",
      "41/223, train_loss: 0.1067, step time: 0.1009\n",
      "42/223, train_loss: 0.1087, step time: 0.1030\n",
      "43/223, train_loss: 0.1168, step time: 0.1260\n",
      "44/223, train_loss: 0.1349, step time: 0.1176\n",
      "45/223, train_loss: 0.1154, step time: 0.1066\n",
      "46/223, train_loss: 0.1263, step time: 0.1125\n",
      "47/223, train_loss: 0.1306, step time: 0.1048\n",
      "48/223, train_loss: 0.1168, step time: 0.1080\n",
      "49/223, train_loss: 0.1129, step time: 0.1067\n",
      "50/223, train_loss: 0.1384, step time: 0.1049\n",
      "51/223, train_loss: 0.1239, step time: 0.1286\n",
      "52/223, train_loss: 0.1212, step time: 0.1074\n",
      "53/223, train_loss: 0.1167, step time: 0.1037\n",
      "54/223, train_loss: 0.1526, step time: 0.1275\n",
      "55/223, train_loss: 0.1247, step time: 0.1151\n",
      "56/223, train_loss: 0.1308, step time: 0.0992\n",
      "57/223, train_loss: 0.1365, step time: 0.1162\n",
      "58/223, train_loss: 0.1269, step time: 0.1046\n",
      "59/223, train_loss: 0.1132, step time: 0.0995\n",
      "60/223, train_loss: 0.1231, step time: 0.0994\n",
      "61/223, train_loss: 0.1101, step time: 0.1143\n",
      "62/223, train_loss: 0.1277, step time: 0.1048\n",
      "63/223, train_loss: 0.1140, step time: 0.1129\n",
      "64/223, train_loss: 0.1288, step time: 0.1052\n",
      "65/223, train_loss: 0.1218, step time: 0.1018\n",
      "66/223, train_loss: 0.1234, step time: 0.0997\n",
      "67/223, train_loss: 0.1136, step time: 0.0995\n",
      "68/223, train_loss: 0.1252, step time: 0.0987\n",
      "69/223, train_loss: 0.1175, step time: 0.1388\n",
      "70/223, train_loss: 0.1214, step time: 0.0993\n",
      "71/223, train_loss: 0.1182, step time: 0.0996\n",
      "72/223, train_loss: 0.1142, step time: 0.0995\n",
      "73/223, train_loss: 0.1138, step time: 0.1348\n",
      "74/223, train_loss: 0.1238, step time: 0.1156\n",
      "75/223, train_loss: 0.1197, step time: 0.1330\n",
      "76/223, train_loss: 0.1410, step time: 0.1084\n",
      "77/223, train_loss: 0.1107, step time: 0.1039\n",
      "78/223, train_loss: 0.1211, step time: 0.1144\n",
      "79/223, train_loss: 0.1186, step time: 0.1006\n",
      "80/223, train_loss: 0.1139, step time: 0.1145\n",
      "81/223, train_loss: 0.1210, step time: 0.1061\n",
      "82/223, train_loss: 0.1157, step time: 0.1061\n",
      "83/223, train_loss: 0.1257, step time: 0.1142\n",
      "84/223, train_loss: 0.1271, step time: 0.1091\n",
      "85/223, train_loss: 0.1129, step time: 0.1130\n",
      "86/223, train_loss: 0.1229, step time: 0.1214\n",
      "87/223, train_loss: 0.1403, step time: 0.1003\n",
      "88/223, train_loss: 0.1294, step time: 0.0981\n",
      "89/223, train_loss: 0.1234, step time: 0.1269\n",
      "90/223, train_loss: 0.1211, step time: 0.1051\n",
      "91/223, train_loss: 0.1323, step time: 0.1069\n",
      "92/223, train_loss: 0.1230, step time: 0.1059\n",
      "93/223, train_loss: 0.1192, step time: 0.1106\n",
      "94/223, train_loss: 0.1325, step time: 0.1034\n",
      "95/223, train_loss: 0.1334, step time: 0.0995\n",
      "96/223, train_loss: 0.1042, step time: 0.1000\n",
      "97/223, train_loss: 0.1122, step time: 0.1116\n",
      "98/223, train_loss: 0.1241, step time: 0.0998\n",
      "99/223, train_loss: 0.1181, step time: 0.1001\n",
      "100/223, train_loss: 0.1278, step time: 0.1128\n",
      "101/223, train_loss: 0.1210, step time: 0.0999\n",
      "102/223, train_loss: 0.1253, step time: 0.1010\n",
      "103/223, train_loss: 0.1272, step time: 0.1359\n",
      "104/223, train_loss: 0.1225, step time: 0.1141\n",
      "105/223, train_loss: 0.1284, step time: 0.0996\n",
      "106/223, train_loss: 0.1353, step time: 0.1035\n",
      "107/223, train_loss: 0.1208, step time: 0.1610\n",
      "108/223, train_loss: 0.1401, step time: 0.1158\n",
      "109/223, train_loss: 0.1350, step time: 0.1077\n",
      "110/223, train_loss: 0.1227, step time: 0.1030\n",
      "111/223, train_loss: 0.1101, step time: 0.1004\n",
      "112/223, train_loss: 0.1067, step time: 0.1109\n",
      "113/223, train_loss: 0.1161, step time: 0.1010\n",
      "114/223, train_loss: 0.1378, step time: 0.1004\n",
      "115/223, train_loss: 0.1197, step time: 0.0999\n",
      "116/223, train_loss: 0.1313, step time: 0.1133\n",
      "117/223, train_loss: 0.1158, step time: 0.1300\n",
      "118/223, train_loss: 0.1166, step time: 0.1178\n",
      "119/223, train_loss: 0.1398, step time: 0.1006\n",
      "120/223, train_loss: 0.1443, step time: 0.1005\n",
      "121/223, train_loss: 0.1207, step time: 0.1075\n",
      "122/223, train_loss: 0.1526, step time: 0.1121\n",
      "123/223, train_loss: 0.1253, step time: 0.1136\n",
      "124/223, train_loss: 0.1378, step time: 0.1091\n",
      "125/223, train_loss: 0.1282, step time: 0.1092\n",
      "126/223, train_loss: 0.1371, step time: 0.1089\n",
      "127/223, train_loss: 0.1192, step time: 0.1206\n",
      "128/223, train_loss: 0.1264, step time: 0.1004\n",
      "129/223, train_loss: 0.1267, step time: 0.1205\n",
      "130/223, train_loss: 0.1217, step time: 0.1244\n",
      "131/223, train_loss: 0.1196, step time: 0.1021\n",
      "132/223, train_loss: 0.1224, step time: 0.1014\n",
      "133/223, train_loss: 0.1212, step time: 0.1117\n",
      "134/223, train_loss: 0.1398, step time: 0.1120\n",
      "135/223, train_loss: 0.1273, step time: 0.1073\n",
      "136/223, train_loss: 0.1293, step time: 0.1011\n",
      "137/223, train_loss: 0.1207, step time: 0.1166\n",
      "138/223, train_loss: 0.1261, step time: 0.1173\n",
      "139/223, train_loss: 0.1170, step time: 0.1169\n",
      "140/223, train_loss: 0.1200, step time: 0.1199\n",
      "141/223, train_loss: 0.1256, step time: 0.1127\n",
      "142/223, train_loss: 0.1471, step time: 0.1081\n",
      "143/223, train_loss: 0.1256, step time: 0.1153\n",
      "144/223, train_loss: 0.1220, step time: 0.1038\n",
      "145/223, train_loss: 0.1221, step time: 0.1152\n",
      "146/223, train_loss: 0.1305, step time: 0.1200\n",
      "147/223, train_loss: 0.1232, step time: 0.1206\n",
      "148/223, train_loss: 0.1216, step time: 0.1347\n",
      "149/223, train_loss: 0.1312, step time: 0.1297\n",
      "150/223, train_loss: 0.1273, step time: 0.1025\n",
      "151/223, train_loss: 0.1381, step time: 0.1107\n",
      "152/223, train_loss: 0.1390, step time: 0.1020\n",
      "153/223, train_loss: 0.1242, step time: 0.1066\n",
      "154/223, train_loss: 0.1274, step time: 0.0996\n",
      "155/223, train_loss: 0.1127, step time: 0.1015\n",
      "156/223, train_loss: 0.1331, step time: 0.1012\n",
      "157/223, train_loss: 0.1138, step time: 0.0996\n",
      "158/223, train_loss: 0.1257, step time: 0.0993\n",
      "159/223, train_loss: 0.1159, step time: 0.0998\n",
      "160/223, train_loss: 0.1183, step time: 0.1185\n",
      "161/223, train_loss: 0.1253, step time: 0.0996\n",
      "162/223, train_loss: 0.1130, step time: 0.1290\n",
      "163/223, train_loss: 0.1253, step time: 0.1144\n",
      "164/223, train_loss: 0.1199, step time: 0.1002\n",
      "165/223, train_loss: 0.1189, step time: 0.1006\n",
      "166/223, train_loss: 0.1198, step time: 0.1041\n",
      "167/223, train_loss: 0.1213, step time: 0.1005\n",
      "168/223, train_loss: 0.1359, step time: 0.1059\n",
      "169/223, train_loss: 0.1166, step time: 0.1136\n",
      "170/223, train_loss: 0.1160, step time: 0.1006\n",
      "171/223, train_loss: 0.1147, step time: 0.1005\n",
      "172/223, train_loss: 0.1195, step time: 0.1148\n",
      "173/223, train_loss: 0.1338, step time: 0.1178\n",
      "174/223, train_loss: 0.1160, step time: 0.1002\n",
      "175/223, train_loss: 0.1444, step time: 0.1009\n",
      "176/223, train_loss: 0.1292, step time: 0.1143\n",
      "177/223, train_loss: 0.1257, step time: 0.0994\n",
      "178/223, train_loss: 0.1369, step time: 0.1007\n",
      "179/223, train_loss: 0.1112, step time: 0.1008\n",
      "180/223, train_loss: 0.1071, step time: 0.1213\n",
      "181/223, train_loss: 0.1154, step time: 0.1151\n",
      "182/223, train_loss: 0.1173, step time: 0.1114\n",
      "183/223, train_loss: 0.1277, step time: 0.1009\n",
      "184/223, train_loss: 0.1241, step time: 0.1010\n",
      "185/223, train_loss: 0.1327, step time: 0.1145\n",
      "186/223, train_loss: 0.1222, step time: 0.1020\n",
      "187/223, train_loss: 0.1182, step time: 0.1087\n",
      "188/223, train_loss: 0.1592, step time: 0.1186\n",
      "189/223, train_loss: 0.1225, step time: 0.1131\n",
      "190/223, train_loss: 0.1565, step time: 0.1181\n",
      "191/223, train_loss: 0.1201, step time: 0.1008\n",
      "192/223, train_loss: 0.1360, step time: 0.1005\n",
      "193/223, train_loss: 0.1523, step time: 0.1014\n",
      "194/223, train_loss: 0.1420, step time: 0.1002\n",
      "195/223, train_loss: 0.1302, step time: 0.0994\n",
      "196/223, train_loss: 0.1165, step time: 0.1032\n",
      "197/223, train_loss: 0.1261, step time: 0.0996\n",
      "198/223, train_loss: 0.1333, step time: 0.1010\n",
      "199/223, train_loss: 0.1310, step time: 0.1020\n",
      "200/223, train_loss: 0.1371, step time: 0.1460\n",
      "201/223, train_loss: 0.1429, step time: 0.0998\n",
      "202/223, train_loss: 0.1204, step time: 0.1134\n",
      "203/223, train_loss: 0.1281, step time: 0.1168\n",
      "204/223, train_loss: 0.1294, step time: 0.1135\n",
      "205/223, train_loss: 0.1317, step time: 0.1213\n",
      "206/223, train_loss: 0.1331, step time: 0.1206\n",
      "207/223, train_loss: 0.1319, step time: 0.1001\n",
      "208/223, train_loss: 0.1279, step time: 0.1111\n",
      "209/223, train_loss: 0.1341, step time: 0.0995\n",
      "210/223, train_loss: 0.1340, step time: 0.1317\n",
      "211/223, train_loss: 0.1319, step time: 0.1182\n",
      "212/223, train_loss: 0.1147, step time: 0.1142\n",
      "213/223, train_loss: 0.1313, step time: 0.1100\n",
      "214/223, train_loss: 0.1750, step time: 0.1124\n",
      "215/223, train_loss: 0.3279, step time: 0.1079\n",
      "216/223, train_loss: 0.1297, step time: 0.1144\n",
      "217/223, train_loss: 0.1480, step time: 0.1119\n",
      "218/223, train_loss: 0.1228, step time: 0.1112\n",
      "219/223, train_loss: 0.1339, step time: 0.1149\n",
      "220/223, train_loss: 0.1121, step time: 0.1280\n",
      "221/223, train_loss: 0.1301, step time: 0.1003\n",
      "222/223, train_loss: 0.1415, step time: 0.1004\n",
      "223/223, train_loss: 0.1420, step time: 0.0994\n",
      "epoch 66 average loss: 0.1258\n",
      "time consuming of epoch 66 is: 89.1200\n",
      "----------\n",
      "epoch 67/300\n",
      "1/223, train_loss: 0.1296, step time: 0.1061\n",
      "2/223, train_loss: 0.3208, step time: 0.1042\n",
      "3/223, train_loss: 0.1324, step time: 0.1006\n",
      "4/223, train_loss: 0.1537, step time: 0.1014\n",
      "5/223, train_loss: 0.1253, step time: 0.1065\n",
      "6/223, train_loss: 0.1433, step time: 0.1108\n",
      "7/223, train_loss: 0.1880, step time: 0.0988\n",
      "8/223, train_loss: 0.1451, step time: 0.1192\n",
      "9/223, train_loss: 0.1525, step time: 0.1175\n",
      "10/223, train_loss: 0.1242, step time: 0.1201\n",
      "11/223, train_loss: 0.1541, step time: 0.1226\n",
      "12/223, train_loss: 0.1591, step time: 0.1160\n",
      "13/223, train_loss: 0.1570, step time: 0.1016\n",
      "14/223, train_loss: 0.1226, step time: 0.1005\n",
      "15/223, train_loss: 0.1675, step time: 0.1067\n",
      "16/223, train_loss: 0.1500, step time: 0.1009\n",
      "17/223, train_loss: 0.1519, step time: 0.1014\n",
      "18/223, train_loss: 0.1473, step time: 0.1004\n",
      "19/223, train_loss: 0.1341, step time: 0.0997\n",
      "20/223, train_loss: 0.1363, step time: 0.1020\n",
      "21/223, train_loss: 0.1538, step time: 0.1071\n",
      "22/223, train_loss: 0.1318, step time: 0.1090\n",
      "23/223, train_loss: 0.1402, step time: 0.1006\n",
      "24/223, train_loss: 0.1384, step time: 0.1013\n",
      "25/223, train_loss: 0.1342, step time: 0.0999\n",
      "26/223, train_loss: 0.1406, step time: 0.1085\n",
      "27/223, train_loss: 0.1488, step time: 0.1278\n",
      "28/223, train_loss: 0.1259, step time: 0.1043\n",
      "29/223, train_loss: 0.1289, step time: 0.1007\n",
      "30/223, train_loss: 0.1143, step time: 0.1072\n",
      "31/223, train_loss: 0.1396, step time: 0.1007\n",
      "32/223, train_loss: 0.1267, step time: 0.1141\n",
      "33/223, train_loss: 0.1450, step time: 0.0996\n",
      "34/223, train_loss: 0.1209, step time: 0.1089\n",
      "35/223, train_loss: 0.1218, step time: 0.1009\n",
      "36/223, train_loss: 0.1107, step time: 0.1156\n",
      "37/223, train_loss: 0.1241, step time: 0.1002\n",
      "38/223, train_loss: 0.1246, step time: 0.1025\n",
      "39/223, train_loss: 0.1391, step time: 0.1197\n",
      "40/223, train_loss: 0.1258, step time: 0.1199\n",
      "41/223, train_loss: 0.1308, step time: 0.1052\n",
      "42/223, train_loss: 0.1187, step time: 0.1001\n",
      "43/223, train_loss: 0.1234, step time: 0.1014\n",
      "44/223, train_loss: 0.1246, step time: 0.1111\n",
      "45/223, train_loss: 0.1225, step time: 0.1009\n",
      "46/223, train_loss: 0.1152, step time: 0.1166\n",
      "47/223, train_loss: 0.1348, step time: 0.1008\n",
      "48/223, train_loss: 0.1171, step time: 0.1005\n",
      "49/223, train_loss: 0.1170, step time: 0.1001\n",
      "50/223, train_loss: 0.1232, step time: 0.0999\n",
      "51/223, train_loss: 0.1251, step time: 0.1002\n",
      "52/223, train_loss: 0.1235, step time: 0.1008\n",
      "53/223, train_loss: 0.1322, step time: 0.1057\n",
      "54/223, train_loss: 0.1094, step time: 0.1056\n",
      "55/223, train_loss: 0.1159, step time: 0.1075\n",
      "56/223, train_loss: 0.1516, step time: 0.1097\n",
      "57/223, train_loss: 0.1175, step time: 0.1128\n",
      "58/223, train_loss: 0.1288, step time: 0.1203\n",
      "59/223, train_loss: 0.1434, step time: 0.1293\n",
      "60/223, train_loss: 0.1342, step time: 0.1115\n",
      "61/223, train_loss: 0.1360, step time: 0.1197\n",
      "62/223, train_loss: 0.1232, step time: 0.1218\n",
      "63/223, train_loss: 0.1437, step time: 0.1135\n",
      "64/223, train_loss: 0.1451, step time: 0.1003\n",
      "65/223, train_loss: 0.1420, step time: 0.1167\n",
      "66/223, train_loss: 0.1395, step time: 0.1128\n",
      "67/223, train_loss: 0.1165, step time: 0.1065\n",
      "68/223, train_loss: 0.1444, step time: 0.1239\n",
      "69/223, train_loss: 0.1167, step time: 0.1156\n",
      "70/223, train_loss: 0.1417, step time: 0.1134\n",
      "71/223, train_loss: 0.1108, step time: 0.1114\n",
      "72/223, train_loss: 0.1298, step time: 0.1265\n",
      "73/223, train_loss: 0.1343, step time: 0.1211\n",
      "74/223, train_loss: 0.1244, step time: 0.1176\n",
      "75/223, train_loss: 0.1315, step time: 0.1166\n",
      "76/223, train_loss: 0.1279, step time: 0.1014\n",
      "77/223, train_loss: 0.1262, step time: 0.1189\n",
      "78/223, train_loss: 0.1258, step time: 0.1083\n",
      "79/223, train_loss: 0.1233, step time: 0.1220\n",
      "80/223, train_loss: 0.1461, step time: 0.1133\n",
      "81/223, train_loss: 0.1234, step time: 0.1098\n",
      "82/223, train_loss: 0.1181, step time: 0.1119\n",
      "83/223, train_loss: 0.1202, step time: 0.1007\n",
      "84/223, train_loss: 0.1233, step time: 0.1045\n",
      "85/223, train_loss: 0.1234, step time: 0.1177\n",
      "86/223, train_loss: 0.1197, step time: 0.1130\n",
      "87/223, train_loss: 0.1126, step time: 0.1102\n",
      "88/223, train_loss: 0.1290, step time: 0.0998\n",
      "89/223, train_loss: 0.1406, step time: 0.1186\n",
      "90/223, train_loss: 0.1274, step time: 0.1059\n",
      "91/223, train_loss: 0.1213, step time: 0.1051\n",
      "92/223, train_loss: 0.1143, step time: 0.1233\n",
      "93/223, train_loss: 0.1274, step time: 0.0988\n",
      "94/223, train_loss: 0.1269, step time: 0.1052\n",
      "95/223, train_loss: 0.1181, step time: 0.1142\n",
      "96/223, train_loss: 0.1168, step time: 0.1006\n",
      "97/223, train_loss: 0.1190, step time: 0.1139\n",
      "98/223, train_loss: 0.1357, step time: 0.1007\n",
      "99/223, train_loss: 0.1182, step time: 0.1002\n",
      "100/223, train_loss: 0.1232, step time: 0.0996\n",
      "101/223, train_loss: 0.1281, step time: 0.1098\n",
      "102/223, train_loss: 0.1265, step time: 0.1218\n",
      "103/223, train_loss: 0.1178, step time: 0.1183\n",
      "104/223, train_loss: 0.1381, step time: 0.1026\n",
      "105/223, train_loss: 0.1371, step time: 0.1255\n",
      "106/223, train_loss: 0.1379, step time: 0.1152\n",
      "107/223, train_loss: 0.1254, step time: 0.1181\n",
      "108/223, train_loss: 0.1217, step time: 0.1034\n",
      "109/223, train_loss: 0.1400, step time: 0.1007\n",
      "110/223, train_loss: 0.1224, step time: 0.1447\n",
      "111/223, train_loss: 0.1106, step time: 0.1557\n",
      "112/223, train_loss: 0.1105, step time: 0.1003\n",
      "113/223, train_loss: 0.1277, step time: 0.1109\n",
      "114/223, train_loss: 0.1078, step time: 0.1090\n",
      "115/223, train_loss: 0.1257, step time: 0.1000\n",
      "116/223, train_loss: 0.1137, step time: 0.1022\n",
      "117/223, train_loss: 0.1191, step time: 0.1003\n",
      "118/223, train_loss: 0.1212, step time: 0.1051\n",
      "119/223, train_loss: 0.1215, step time: 0.1007\n",
      "120/223, train_loss: 0.1168, step time: 0.1007\n",
      "121/223, train_loss: 0.1095, step time: 0.1004\n",
      "122/223, train_loss: 0.1134, step time: 0.1082\n",
      "123/223, train_loss: 0.1231, step time: 0.1299\n",
      "124/223, train_loss: 0.1429, step time: 0.1202\n",
      "125/223, train_loss: 0.1233, step time: 0.1003\n",
      "126/223, train_loss: 0.1163, step time: 0.1011\n",
      "127/223, train_loss: 0.1168, step time: 0.1008\n",
      "128/223, train_loss: 0.1259, step time: 0.1012\n",
      "129/223, train_loss: 0.1136, step time: 0.1006\n",
      "130/223, train_loss: 0.1253, step time: 0.1104\n",
      "131/223, train_loss: 0.1188, step time: 0.1144\n",
      "132/223, train_loss: 0.1141, step time: 0.1022\n",
      "133/223, train_loss: 0.1147, step time: 0.0986\n",
      "134/223, train_loss: 0.1064, step time: 0.1235\n",
      "135/223, train_loss: 0.1213, step time: 0.1030\n",
      "136/223, train_loss: 0.1169, step time: 0.1101\n",
      "137/223, train_loss: 0.1356, step time: 0.1081\n",
      "138/223, train_loss: 0.1132, step time: 0.1112\n",
      "139/223, train_loss: 0.1243, step time: 0.1079\n",
      "140/223, train_loss: 0.1383, step time: 0.1003\n",
      "141/223, train_loss: 0.1432, step time: 0.1003\n",
      "142/223, train_loss: 0.1238, step time: 0.1085\n",
      "143/223, train_loss: 0.1314, step time: 0.1056\n",
      "144/223, train_loss: 0.1231, step time: 0.1213\n",
      "145/223, train_loss: 0.1206, step time: 0.1085\n",
      "146/223, train_loss: 0.1062, step time: 0.1000\n",
      "147/223, train_loss: 0.1279, step time: 0.1005\n",
      "148/223, train_loss: 0.1148, step time: 0.1188\n",
      "149/223, train_loss: 0.1259, step time: 0.1119\n",
      "150/223, train_loss: 0.1143, step time: 0.1057\n",
      "151/223, train_loss: 0.1310, step time: 0.1175\n",
      "152/223, train_loss: 0.1332, step time: 0.1273\n",
      "153/223, train_loss: 0.1110, step time: 0.1137\n",
      "154/223, train_loss: 0.1298, step time: 0.1002\n",
      "155/223, train_loss: 0.1351, step time: 0.1001\n",
      "156/223, train_loss: 0.1140, step time: 0.1005\n",
      "157/223, train_loss: 0.1116, step time: 0.1170\n",
      "158/223, train_loss: 0.1148, step time: 0.1120\n",
      "159/223, train_loss: 0.1312, step time: 0.1108\n",
      "160/223, train_loss: 0.1433, step time: 0.1126\n",
      "161/223, train_loss: 0.1251, step time: 0.1204\n",
      "162/223, train_loss: 0.1340, step time: 0.0998\n",
      "163/223, train_loss: 0.1295, step time: 0.1073\n",
      "164/223, train_loss: 0.1214, step time: 0.1057\n",
      "165/223, train_loss: 0.1128, step time: 0.1114\n",
      "166/223, train_loss: 0.1235, step time: 0.1127\n",
      "167/223, train_loss: 0.1180, step time: 0.1070\n",
      "168/223, train_loss: 0.1225, step time: 0.1160\n",
      "169/223, train_loss: 0.1288, step time: 0.0998\n",
      "170/223, train_loss: 0.1193, step time: 0.1185\n",
      "171/223, train_loss: 0.1308, step time: 0.1017\n",
      "172/223, train_loss: 0.1069, step time: 0.1016\n",
      "173/223, train_loss: 0.1176, step time: 0.1069\n",
      "174/223, train_loss: 0.1214, step time: 0.1076\n",
      "175/223, train_loss: 0.1348, step time: 0.1072\n",
      "176/223, train_loss: 0.1282, step time: 0.1047\n",
      "177/223, train_loss: 0.1361, step time: 0.1136\n",
      "178/223, train_loss: 0.1308, step time: 0.1046\n",
      "179/223, train_loss: 0.1189, step time: 0.1195\n",
      "180/223, train_loss: 0.1293, step time: 0.1406\n",
      "181/223, train_loss: 0.1245, step time: 0.1005\n",
      "182/223, train_loss: 0.1230, step time: 0.1100\n",
      "183/223, train_loss: 0.1264, step time: 0.1111\n",
      "184/223, train_loss: 0.1196, step time: 0.1013\n",
      "185/223, train_loss: 0.1293, step time: 0.1209\n",
      "186/223, train_loss: 0.1338, step time: 0.1133\n",
      "187/223, train_loss: 0.1251, step time: 0.1061\n",
      "188/223, train_loss: 0.1200, step time: 0.1154\n",
      "189/223, train_loss: 0.1429, step time: 0.1049\n",
      "190/223, train_loss: 0.1167, step time: 0.1007\n",
      "191/223, train_loss: 0.1174, step time: 0.1062\n",
      "192/223, train_loss: 0.1263, step time: 0.1273\n",
      "193/223, train_loss: 0.1350, step time: 0.1193\n",
      "194/223, train_loss: 0.1167, step time: 0.1180\n",
      "195/223, train_loss: 0.1137, step time: 0.1225\n",
      "196/223, train_loss: 0.1277, step time: 0.1161\n",
      "197/223, train_loss: 0.1158, step time: 0.1008\n",
      "198/223, train_loss: 0.1312, step time: 0.1006\n",
      "199/223, train_loss: 0.1303, step time: 0.1371\n",
      "200/223, train_loss: 0.1162, step time: 0.1308\n",
      "201/223, train_loss: 0.1103, step time: 0.1121\n",
      "202/223, train_loss: 0.1255, step time: 0.1078\n",
      "203/223, train_loss: 0.1131, step time: 0.1128\n",
      "204/223, train_loss: 0.1317, step time: 0.1262\n",
      "205/223, train_loss: 0.1113, step time: 0.1018\n",
      "206/223, train_loss: 0.1311, step time: 0.0991\n",
      "207/223, train_loss: 0.1332, step time: 0.1095\n",
      "208/223, train_loss: 0.1222, step time: 0.0986\n",
      "209/223, train_loss: 0.1320, step time: 0.1131\n",
      "210/223, train_loss: 0.1328, step time: 0.1193\n",
      "211/223, train_loss: 0.1284, step time: 0.0997\n",
      "212/223, train_loss: 0.1162, step time: 0.1003\n",
      "213/223, train_loss: 0.1281, step time: 0.1144\n",
      "214/223, train_loss: 0.1300, step time: 0.1231\n",
      "215/223, train_loss: 0.1173, step time: 0.1035\n",
      "216/223, train_loss: 0.1195, step time: 0.1046\n",
      "217/223, train_loss: 0.1254, step time: 0.1221\n",
      "218/223, train_loss: 0.1195, step time: 0.1029\n",
      "219/223, train_loss: 0.1229, step time: 0.1002\n",
      "220/223, train_loss: 0.1294, step time: 0.1006\n",
      "221/223, train_loss: 0.1195, step time: 0.1012\n",
      "222/223, train_loss: 0.1121, step time: 0.0989\n",
      "223/223, train_loss: 0.1171, step time: 0.0998\n",
      "epoch 67 average loss: 0.1278\n",
      "time consuming of epoch 67 is: 86.9102\n",
      "----------\n",
      "epoch 68/300\n",
      "1/223, train_loss: 0.1273, step time: 0.1007\n",
      "2/223, train_loss: 0.1118, step time: 0.0998\n",
      "3/223, train_loss: 0.1159, step time: 0.1005\n",
      "4/223, train_loss: 0.1169, step time: 0.1006\n",
      "5/223, train_loss: 0.1308, step time: 0.1105\n",
      "6/223, train_loss: 0.1413, step time: 0.1099\n",
      "7/223, train_loss: 0.1268, step time: 0.1189\n",
      "8/223, train_loss: 0.1302, step time: 0.1113\n",
      "9/223, train_loss: 0.1208, step time: 0.1158\n",
      "10/223, train_loss: 0.1135, step time: 0.1001\n",
      "11/223, train_loss: 0.1187, step time: 0.1007\n",
      "12/223, train_loss: 0.1248, step time: 0.1058\n",
      "13/223, train_loss: 0.1189, step time: 0.1282\n",
      "14/223, train_loss: 0.1323, step time: 0.1153\n",
      "15/223, train_loss: 0.1275, step time: 0.1184\n",
      "16/223, train_loss: 0.1192, step time: 0.1137\n",
      "17/223, train_loss: 0.1197, step time: 0.1299\n",
      "18/223, train_loss: 0.1132, step time: 0.1198\n",
      "19/223, train_loss: 0.1147, step time: 0.1012\n",
      "20/223, train_loss: 0.1366, step time: 0.1014\n",
      "21/223, train_loss: 0.1301, step time: 0.1052\n",
      "22/223, train_loss: 0.1174, step time: 0.1099\n",
      "23/223, train_loss: 0.1272, step time: 0.1083\n",
      "24/223, train_loss: 0.3160, step time: 0.1008\n",
      "25/223, train_loss: 0.1205, step time: 0.1136\n",
      "26/223, train_loss: 0.1207, step time: 0.1003\n",
      "27/223, train_loss: 0.1222, step time: 0.0993\n",
      "28/223, train_loss: 0.1328, step time: 0.1179\n",
      "29/223, train_loss: 0.1321, step time: 0.0998\n",
      "30/223, train_loss: 0.1054, step time: 0.1119\n",
      "31/223, train_loss: 0.1107, step time: 0.1148\n",
      "32/223, train_loss: 0.1329, step time: 0.1003\n",
      "33/223, train_loss: 0.1157, step time: 0.1000\n",
      "34/223, train_loss: 0.1249, step time: 0.0998\n",
      "35/223, train_loss: 0.1296, step time: 0.1000\n",
      "36/223, train_loss: 0.1235, step time: 0.1197\n",
      "37/223, train_loss: 0.1205, step time: 0.1138\n",
      "38/223, train_loss: 0.1277, step time: 0.1037\n",
      "39/223, train_loss: 0.1006, step time: 0.1498\n",
      "40/223, train_loss: 0.1222, step time: 0.1061\n",
      "41/223, train_loss: 0.1310, step time: 0.1034\n",
      "42/223, train_loss: 0.1194, step time: 0.1037\n",
      "43/223, train_loss: 0.1305, step time: 0.1027\n",
      "44/223, train_loss: 0.1175, step time: 0.1105\n",
      "45/223, train_loss: 0.1187, step time: 0.1211\n",
      "46/223, train_loss: 0.1299, step time: 0.0997\n",
      "47/223, train_loss: 0.1156, step time: 0.0997\n",
      "48/223, train_loss: 0.1294, step time: 0.1056\n",
      "49/223, train_loss: 0.1349, step time: 0.1110\n",
      "50/223, train_loss: 0.1242, step time: 0.1098\n",
      "51/223, train_loss: 0.1327, step time: 0.1065\n",
      "52/223, train_loss: 0.1250, step time: 0.0997\n",
      "53/223, train_loss: 0.1237, step time: 0.1019\n",
      "54/223, train_loss: 0.1273, step time: 0.1198\n",
      "55/223, train_loss: 0.1357, step time: 0.1111\n",
      "56/223, train_loss: 0.1233, step time: 0.1001\n",
      "57/223, train_loss: 0.1296, step time: 0.1132\n",
      "58/223, train_loss: 0.1341, step time: 0.1005\n",
      "59/223, train_loss: 0.1358, step time: 0.1013\n",
      "60/223, train_loss: 0.1227, step time: 0.1288\n",
      "61/223, train_loss: 0.1448, step time: 0.0992\n",
      "62/223, train_loss: 0.1336, step time: 0.1075\n",
      "63/223, train_loss: 0.1166, step time: 0.1127\n",
      "64/223, train_loss: 0.1277, step time: 0.1202\n",
      "65/223, train_loss: 0.1301, step time: 0.1123\n",
      "66/223, train_loss: 0.1243, step time: 0.1218\n",
      "67/223, train_loss: 0.1310, step time: 0.1116\n",
      "68/223, train_loss: 0.1293, step time: 0.1174\n",
      "69/223, train_loss: 0.1304, step time: 0.0994\n",
      "70/223, train_loss: 0.1292, step time: 0.1052\n",
      "71/223, train_loss: 0.1209, step time: 0.1315\n",
      "72/223, train_loss: 0.1276, step time: 0.1260\n",
      "73/223, train_loss: 0.1397, step time: 0.1062\n",
      "74/223, train_loss: 0.1169, step time: 0.1043\n",
      "75/223, train_loss: 0.1454, step time: 0.1077\n",
      "76/223, train_loss: 0.1187, step time: 0.1510\n",
      "77/223, train_loss: 0.1254, step time: 0.0997\n",
      "78/223, train_loss: 0.1172, step time: 0.1130\n",
      "79/223, train_loss: 0.1131, step time: 0.1216\n",
      "80/223, train_loss: 0.1153, step time: 0.1103\n",
      "81/223, train_loss: 0.1358, step time: 0.1063\n",
      "82/223, train_loss: 0.1226, step time: 0.1001\n",
      "83/223, train_loss: 0.1198, step time: 0.1163\n",
      "84/223, train_loss: 0.1271, step time: 0.1047\n",
      "85/223, train_loss: 0.1178, step time: 0.1230\n",
      "86/223, train_loss: 0.1324, step time: 0.1047\n",
      "87/223, train_loss: 0.1302, step time: 0.1264\n",
      "88/223, train_loss: 0.1266, step time: 0.1042\n",
      "89/223, train_loss: 0.1345, step time: 0.1044\n",
      "90/223, train_loss: 0.1204, step time: 0.0999\n",
      "91/223, train_loss: 0.1342, step time: 0.1225\n",
      "92/223, train_loss: 0.1235, step time: 0.1103\n",
      "93/223, train_loss: 0.1192, step time: 0.1012\n",
      "94/223, train_loss: 0.1310, step time: 0.1071\n",
      "95/223, train_loss: 0.1391, step time: 0.1176\n",
      "96/223, train_loss: 0.1173, step time: 0.1175\n",
      "97/223, train_loss: 0.1398, step time: 0.1146\n",
      "98/223, train_loss: 0.1230, step time: 0.1328\n",
      "99/223, train_loss: 0.1375, step time: 0.1208\n",
      "100/223, train_loss: 0.1154, step time: 0.1114\n",
      "101/223, train_loss: 0.1232, step time: 0.0998\n",
      "102/223, train_loss: 0.1242, step time: 0.0999\n",
      "103/223, train_loss: 0.1294, step time: 0.0999\n",
      "104/223, train_loss: 0.1321, step time: 0.1008\n",
      "105/223, train_loss: 0.1389, step time: 0.1042\n",
      "106/223, train_loss: 0.1277, step time: 0.1233\n",
      "107/223, train_loss: 0.1400, step time: 0.1004\n",
      "108/223, train_loss: 0.1285, step time: 0.0999\n",
      "109/223, train_loss: 0.1254, step time: 0.1002\n",
      "110/223, train_loss: 0.1313, step time: 0.1010\n",
      "111/223, train_loss: 0.1305, step time: 0.1135\n",
      "112/223, train_loss: 0.1106, step time: 0.1168\n",
      "113/223, train_loss: 0.1159, step time: 0.1091\n",
      "114/223, train_loss: 0.1260, step time: 0.0990\n",
      "115/223, train_loss: 0.1237, step time: 0.1126\n",
      "116/223, train_loss: 0.1076, step time: 0.0994\n",
      "117/223, train_loss: 0.1422, step time: 0.1109\n",
      "118/223, train_loss: 0.1129, step time: 0.1178\n",
      "119/223, train_loss: 0.1187, step time: 0.1049\n",
      "120/223, train_loss: 0.1163, step time: 0.1001\n",
      "121/223, train_loss: 0.1305, step time: 0.1010\n",
      "122/223, train_loss: 0.1135, step time: 0.1019\n",
      "123/223, train_loss: 0.1331, step time: 0.1054\n",
      "124/223, train_loss: 0.1130, step time: 0.1176\n",
      "125/223, train_loss: 0.1196, step time: 0.1059\n",
      "126/223, train_loss: 0.1271, step time: 0.1016\n",
      "127/223, train_loss: 0.1100, step time: 0.1123\n",
      "128/223, train_loss: 0.1220, step time: 0.1014\n",
      "129/223, train_loss: 0.1294, step time: 0.1142\n",
      "130/223, train_loss: 0.1184, step time: 0.0998\n",
      "131/223, train_loss: 0.1190, step time: 0.1009\n",
      "132/223, train_loss: 0.1180, step time: 0.1001\n",
      "133/223, train_loss: 0.1259, step time: 0.1087\n",
      "134/223, train_loss: 0.1233, step time: 0.1018\n",
      "135/223, train_loss: 0.1189, step time: 0.1205\n",
      "136/223, train_loss: 0.1089, step time: 0.1006\n",
      "137/223, train_loss: 0.1324, step time: 0.1003\n",
      "138/223, train_loss: 0.1213, step time: 0.1068\n",
      "139/223, train_loss: 0.1394, step time: 0.1192\n",
      "140/223, train_loss: 0.1240, step time: 0.1095\n",
      "141/223, train_loss: 0.1183, step time: 0.1108\n",
      "142/223, train_loss: 0.1149, step time: 0.1171\n",
      "143/223, train_loss: 0.1240, step time: 0.1019\n",
      "144/223, train_loss: 0.1285, step time: 0.1022\n",
      "145/223, train_loss: 0.1201, step time: 0.1003\n",
      "146/223, train_loss: 0.1070, step time: 0.1027\n",
      "147/223, train_loss: 0.1250, step time: 0.1009\n",
      "148/223, train_loss: 0.1134, step time: 0.1016\n",
      "149/223, train_loss: 0.1339, step time: 0.1167\n",
      "150/223, train_loss: 0.1129, step time: 0.1228\n",
      "151/223, train_loss: 0.1182, step time: 0.1126\n",
      "152/223, train_loss: 0.1305, step time: 0.1144\n",
      "153/223, train_loss: 0.1268, step time: 0.1002\n",
      "154/223, train_loss: 0.1170, step time: 0.1132\n",
      "155/223, train_loss: 0.1160, step time: 0.1001\n",
      "156/223, train_loss: 0.1323, step time: 0.0999\n",
      "157/223, train_loss: 0.1287, step time: 0.1044\n",
      "158/223, train_loss: 0.1280, step time: 0.1009\n",
      "159/223, train_loss: 0.1116, step time: 0.1073\n",
      "160/223, train_loss: 0.1170, step time: 0.1112\n",
      "161/223, train_loss: 0.1205, step time: 0.1155\n",
      "162/223, train_loss: 0.1273, step time: 0.1006\n",
      "163/223, train_loss: 0.1352, step time: 0.1056\n",
      "164/223, train_loss: 0.1244, step time: 0.1107\n",
      "165/223, train_loss: 0.1234, step time: 0.1142\n",
      "166/223, train_loss: 0.1081, step time: 0.0997\n",
      "167/223, train_loss: 0.1283, step time: 0.1014\n",
      "168/223, train_loss: 0.1377, step time: 0.1007\n",
      "169/223, train_loss: 0.1222, step time: 0.1001\n",
      "170/223, train_loss: 0.1184, step time: 0.1005\n",
      "171/223, train_loss: 0.1305, step time: 0.1000\n",
      "172/223, train_loss: 0.1219, step time: 0.1021\n",
      "173/223, train_loss: 0.1240, step time: 0.1039\n",
      "174/223, train_loss: 0.1212, step time: 0.1119\n",
      "175/223, train_loss: 0.1218, step time: 0.1006\n",
      "176/223, train_loss: 0.1438, step time: 0.1072\n",
      "177/223, train_loss: 0.1293, step time: 0.1025\n",
      "178/223, train_loss: 0.1163, step time: 0.1266\n",
      "179/223, train_loss: 0.1245, step time: 0.1003\n",
      "180/223, train_loss: 0.1222, step time: 0.0993\n",
      "181/223, train_loss: 0.1148, step time: 0.1156\n",
      "182/223, train_loss: 0.1144, step time: 0.1001\n",
      "183/223, train_loss: 0.1156, step time: 0.0998\n",
      "184/223, train_loss: 0.1202, step time: 0.1037\n",
      "185/223, train_loss: 0.1202, step time: 0.1059\n",
      "186/223, train_loss: 0.1115, step time: 0.1008\n",
      "187/223, train_loss: 0.1313, step time: 0.1002\n",
      "188/223, train_loss: 0.1156, step time: 0.1001\n",
      "189/223, train_loss: 0.1144, step time: 0.1083\n",
      "190/223, train_loss: 0.1186, step time: 0.1110\n",
      "191/223, train_loss: 0.1177, step time: 0.1066\n",
      "192/223, train_loss: 0.1221, step time: 0.1214\n",
      "193/223, train_loss: 0.1274, step time: 0.1030\n",
      "194/223, train_loss: 0.1311, step time: 0.1211\n",
      "195/223, train_loss: 0.1295, step time: 0.1010\n",
      "196/223, train_loss: 0.1261, step time: 0.0998\n",
      "197/223, train_loss: 0.1280, step time: 0.1202\n",
      "198/223, train_loss: 0.1241, step time: 0.1027\n",
      "199/223, train_loss: 0.1097, step time: 0.1075\n",
      "200/223, train_loss: 0.1143, step time: 0.1013\n",
      "201/223, train_loss: 0.1331, step time: 0.0995\n",
      "202/223, train_loss: 0.1211, step time: 0.1059\n",
      "203/223, train_loss: 0.1216, step time: 0.1136\n",
      "204/223, train_loss: 0.1169, step time: 0.0999\n",
      "205/223, train_loss: 0.1196, step time: 0.1037\n",
      "206/223, train_loss: 0.1232, step time: 0.1006\n",
      "207/223, train_loss: 0.1317, step time: 0.1116\n",
      "208/223, train_loss: 0.1171, step time: 0.1210\n",
      "209/223, train_loss: 0.1353, step time: 0.0997\n",
      "210/223, train_loss: 0.1200, step time: 0.0992\n",
      "211/223, train_loss: 0.1131, step time: 0.1024\n",
      "212/223, train_loss: 0.1130, step time: 0.1400\n",
      "213/223, train_loss: 0.1133, step time: 0.1009\n",
      "214/223, train_loss: 0.1304, step time: 0.1001\n",
      "215/223, train_loss: 0.1192, step time: 0.1006\n",
      "216/223, train_loss: 0.1303, step time: 0.1000\n",
      "217/223, train_loss: 0.1212, step time: 0.1007\n",
      "218/223, train_loss: 0.1159, step time: 0.1033\n",
      "219/223, train_loss: 0.1204, step time: 0.1318\n",
      "220/223, train_loss: 0.1073, step time: 0.0999\n",
      "221/223, train_loss: 0.1296, step time: 0.0987\n",
      "222/223, train_loss: 0.1095, step time: 0.1001\n",
      "223/223, train_loss: 0.1213, step time: 0.0994\n",
      "epoch 68 average loss: 0.1246\n",
      "time consuming of epoch 68 is: 90.3644\n",
      "----------\n",
      "epoch 69/300\n",
      "1/223, train_loss: 0.1228, step time: 0.1146\n",
      "2/223, train_loss: 0.1182, step time: 0.1105\n",
      "3/223, train_loss: 0.1126, step time: 0.1139\n",
      "4/223, train_loss: 0.1130, step time: 0.1049\n",
      "5/223, train_loss: 0.1122, step time: 0.1145\n",
      "6/223, train_loss: 0.1286, step time: 0.1003\n",
      "7/223, train_loss: 0.1215, step time: 0.1008\n",
      "8/223, train_loss: 0.1374, step time: 0.0999\n",
      "9/223, train_loss: 0.1248, step time: 0.1016\n",
      "10/223, train_loss: 0.1347, step time: 0.1215\n",
      "11/223, train_loss: 0.1200, step time: 0.1058\n",
      "12/223, train_loss: 0.1007, step time: 0.1024\n",
      "13/223, train_loss: 0.1256, step time: 0.1078\n",
      "14/223, train_loss: 0.1186, step time: 0.1129\n",
      "15/223, train_loss: 0.1248, step time: 0.1280\n",
      "16/223, train_loss: 0.1182, step time: 0.1126\n",
      "17/223, train_loss: 0.1122, step time: 0.1149\n",
      "18/223, train_loss: 0.1107, step time: 0.1144\n",
      "19/223, train_loss: 0.1295, step time: 0.1157\n",
      "20/223, train_loss: 0.1257, step time: 0.1001\n",
      "21/223, train_loss: 0.1236, step time: 0.1106\n",
      "22/223, train_loss: 0.1267, step time: 0.1035\n",
      "23/223, train_loss: 0.1252, step time: 0.1003\n",
      "24/223, train_loss: 0.1351, step time: 0.1000\n",
      "25/223, train_loss: 0.3295, step time: 0.1157\n",
      "26/223, train_loss: 0.1135, step time: 0.1004\n",
      "27/223, train_loss: 0.1251, step time: 0.0998\n",
      "28/223, train_loss: 0.1081, step time: 0.1004\n",
      "29/223, train_loss: 0.1131, step time: 0.1005\n",
      "30/223, train_loss: 0.1214, step time: 0.1332\n",
      "31/223, train_loss: 0.1130, step time: 0.1204\n",
      "32/223, train_loss: 0.1072, step time: 0.1010\n",
      "33/223, train_loss: 0.1305, step time: 0.1061\n",
      "34/223, train_loss: 0.1198, step time: 0.1106\n",
      "35/223, train_loss: 0.1168, step time: 0.1250\n",
      "36/223, train_loss: 0.1074, step time: 0.1315\n",
      "37/223, train_loss: 0.1157, step time: 0.1068\n",
      "38/223, train_loss: 0.1271, step time: 0.1144\n",
      "39/223, train_loss: 0.1178, step time: 0.1146\n",
      "40/223, train_loss: 0.1253, step time: 0.1202\n",
      "41/223, train_loss: 0.1232, step time: 0.1000\n",
      "42/223, train_loss: 0.1178, step time: 0.1190\n",
      "43/223, train_loss: 0.1150, step time: 0.1038\n",
      "44/223, train_loss: 0.1166, step time: 0.1057\n",
      "45/223, train_loss: 0.1385, step time: 0.1057\n",
      "46/223, train_loss: 0.1211, step time: 0.1138\n",
      "47/223, train_loss: 0.1230, step time: 0.1037\n",
      "48/223, train_loss: 0.1255, step time: 0.1149\n",
      "49/223, train_loss: 0.1229, step time: 0.1008\n",
      "50/223, train_loss: 0.1203, step time: 0.1164\n",
      "51/223, train_loss: 0.1128, step time: 0.1005\n",
      "52/223, train_loss: 0.1222, step time: 0.1008\n",
      "53/223, train_loss: 0.1154, step time: 0.1219\n",
      "54/223, train_loss: 0.1378, step time: 0.0996\n",
      "55/223, train_loss: 0.1132, step time: 0.1003\n",
      "56/223, train_loss: 0.1241, step time: 0.1337\n",
      "57/223, train_loss: 0.1246, step time: 0.1002\n",
      "58/223, train_loss: 0.1113, step time: 0.1072\n",
      "59/223, train_loss: 0.1294, step time: 0.1000\n",
      "60/223, train_loss: 0.1272, step time: 0.1006\n",
      "61/223, train_loss: 0.1220, step time: 0.1105\n",
      "62/223, train_loss: 0.1355, step time: 0.1152\n",
      "63/223, train_loss: 0.1246, step time: 0.1104\n",
      "64/223, train_loss: 0.1182, step time: 0.0998\n",
      "65/223, train_loss: 0.1295, step time: 0.1003\n",
      "66/223, train_loss: 0.1162, step time: 0.1012\n",
      "67/223, train_loss: 0.1115, step time: 0.1007\n",
      "68/223, train_loss: 0.1285, step time: 0.0998\n",
      "69/223, train_loss: 0.1388, step time: 0.1060\n",
      "70/223, train_loss: 0.1141, step time: 0.1028\n",
      "71/223, train_loss: 0.1203, step time: 0.1059\n",
      "72/223, train_loss: 0.1204, step time: 0.1099\n",
      "73/223, train_loss: 0.1361, step time: 0.1001\n",
      "74/223, train_loss: 0.1222, step time: 0.1001\n",
      "75/223, train_loss: 0.1162, step time: 0.1056\n",
      "76/223, train_loss: 0.1229, step time: 0.1002\n",
      "77/223, train_loss: 0.1276, step time: 0.1143\n",
      "78/223, train_loss: 0.1213, step time: 0.1078\n",
      "79/223, train_loss: 0.1157, step time: 0.1011\n",
      "80/223, train_loss: 0.1291, step time: 0.1005\n",
      "81/223, train_loss: 0.1118, step time: 0.1151\n",
      "82/223, train_loss: 0.1114, step time: 0.1008\n",
      "83/223, train_loss: 0.1351, step time: 0.1008\n",
      "84/223, train_loss: 0.1177, step time: 0.1162\n",
      "85/223, train_loss: 0.1077, step time: 0.1253\n",
      "86/223, train_loss: 0.1117, step time: 0.1163\n",
      "87/223, train_loss: 0.1099, step time: 0.1070\n",
      "88/223, train_loss: 0.1348, step time: 0.1084\n",
      "89/223, train_loss: 0.1114, step time: 0.1101\n",
      "90/223, train_loss: 0.1323, step time: 0.1000\n",
      "91/223, train_loss: 0.1060, step time: 0.0990\n",
      "92/223, train_loss: 0.1220, step time: 0.1003\n",
      "93/223, train_loss: 0.1269, step time: 0.1124\n",
      "94/223, train_loss: 0.1208, step time: 0.1021\n",
      "95/223, train_loss: 0.1248, step time: 0.1547\n",
      "96/223, train_loss: 0.1308, step time: 0.1010\n",
      "97/223, train_loss: 0.1310, step time: 0.0999\n",
      "98/223, train_loss: 0.1143, step time: 0.0999\n",
      "99/223, train_loss: 0.1217, step time: 0.1008\n",
      "100/223, train_loss: 0.1069, step time: 0.1012\n",
      "101/223, train_loss: 0.1100, step time: 0.1109\n",
      "102/223, train_loss: 0.1144, step time: 0.0989\n",
      "103/223, train_loss: 0.1129, step time: 0.1003\n",
      "104/223, train_loss: 0.1281, step time: 0.0996\n",
      "105/223, train_loss: 0.1218, step time: 0.1004\n",
      "106/223, train_loss: 0.1433, step time: 0.1014\n",
      "107/223, train_loss: 0.1190, step time: 0.1002\n",
      "108/223, train_loss: 0.1257, step time: 0.1010\n",
      "109/223, train_loss: 0.1269, step time: 0.1042\n",
      "110/223, train_loss: 0.1322, step time: 0.1004\n",
      "111/223, train_loss: 0.1156, step time: 0.0996\n",
      "112/223, train_loss: 0.1267, step time: 0.1001\n",
      "113/223, train_loss: 0.1208, step time: 0.1049\n",
      "114/223, train_loss: 0.1086, step time: 0.1069\n",
      "115/223, train_loss: 0.1297, step time: 0.1116\n",
      "116/223, train_loss: 0.1140, step time: 0.1104\n",
      "117/223, train_loss: 0.1293, step time: 0.1240\n",
      "118/223, train_loss: 0.1244, step time: 0.1512\n",
      "119/223, train_loss: 0.1210, step time: 0.1043\n",
      "120/223, train_loss: 0.1165, step time: 0.1137\n",
      "121/223, train_loss: 0.1132, step time: 0.1052\n",
      "122/223, train_loss: 0.1428, step time: 0.0998\n",
      "123/223, train_loss: 0.1116, step time: 0.1195\n",
      "124/223, train_loss: 0.1199, step time: 0.1296\n",
      "125/223, train_loss: 0.1174, step time: 0.1194\n",
      "126/223, train_loss: 0.1255, step time: 0.1097\n",
      "127/223, train_loss: 0.1316, step time: 0.1085\n",
      "128/223, train_loss: 0.1417, step time: 0.1005\n",
      "129/223, train_loss: 0.1240, step time: 0.1017\n",
      "130/223, train_loss: 0.1223, step time: 0.1105\n",
      "131/223, train_loss: 0.1169, step time: 0.1024\n",
      "132/223, train_loss: 0.1108, step time: 0.1079\n",
      "133/223, train_loss: 0.1237, step time: 0.1000\n",
      "134/223, train_loss: 0.1195, step time: 0.1149\n",
      "135/223, train_loss: 0.1221, step time: 0.0996\n",
      "136/223, train_loss: 0.1102, step time: 0.0993\n",
      "137/223, train_loss: 0.1212, step time: 0.1046\n",
      "138/223, train_loss: 0.1244, step time: 0.1110\n",
      "139/223, train_loss: 0.1142, step time: 0.1051\n",
      "140/223, train_loss: 0.1136, step time: 0.1141\n",
      "141/223, train_loss: 0.1430, step time: 0.1097\n",
      "142/223, train_loss: 0.1314, step time: 0.1142\n",
      "143/223, train_loss: 0.1112, step time: 0.1252\n",
      "144/223, train_loss: 0.1165, step time: 0.1239\n",
      "145/223, train_loss: 0.1158, step time: 0.1000\n",
      "146/223, train_loss: 0.1105, step time: 0.1175\n",
      "147/223, train_loss: 0.1090, step time: 0.1040\n",
      "148/223, train_loss: 0.1271, step time: 0.1109\n",
      "149/223, train_loss: 0.1402, step time: 0.1205\n",
      "150/223, train_loss: 0.1265, step time: 0.1538\n",
      "151/223, train_loss: 0.1164, step time: 0.1123\n",
      "152/223, train_loss: 0.1075, step time: 0.1004\n",
      "153/223, train_loss: 0.1260, step time: 0.1000\n",
      "154/223, train_loss: 0.1174, step time: 0.1018\n",
      "155/223, train_loss: 0.1363, step time: 0.1094\n",
      "156/223, train_loss: 0.1305, step time: 0.1133\n",
      "157/223, train_loss: 0.1232, step time: 0.1044\n",
      "158/223, train_loss: 0.1264, step time: 0.1057\n",
      "159/223, train_loss: 0.1209, step time: 0.1148\n",
      "160/223, train_loss: 0.1178, step time: 0.1127\n",
      "161/223, train_loss: 0.1350, step time: 0.1180\n",
      "162/223, train_loss: 0.1074, step time: 0.1047\n",
      "163/223, train_loss: 0.1188, step time: 0.1134\n",
      "164/223, train_loss: 0.1164, step time: 0.1097\n",
      "165/223, train_loss: 0.1213, step time: 0.1181\n",
      "166/223, train_loss: 0.1331, step time: 0.1104\n",
      "167/223, train_loss: 0.1259, step time: 0.1077\n",
      "168/223, train_loss: 0.1147, step time: 0.1079\n",
      "169/223, train_loss: 0.1294, step time: 0.0994\n",
      "170/223, train_loss: 0.1335, step time: 0.1163\n",
      "171/223, train_loss: 0.1161, step time: 0.1063\n",
      "172/223, train_loss: 0.1113, step time: 0.1118\n",
      "173/223, train_loss: 0.1309, step time: 0.1177\n",
      "174/223, train_loss: 0.1243, step time: 0.0989\n",
      "175/223, train_loss: 0.1255, step time: 0.0995\n",
      "176/223, train_loss: 0.1147, step time: 0.0995\n",
      "177/223, train_loss: 0.1222, step time: 0.1072\n",
      "178/223, train_loss: 0.1257, step time: 0.0997\n",
      "179/223, train_loss: 0.1330, step time: 0.1002\n",
      "180/223, train_loss: 0.1145, step time: 0.1002\n",
      "181/223, train_loss: 0.1116, step time: 0.1099\n",
      "182/223, train_loss: 0.1188, step time: 0.1122\n",
      "183/223, train_loss: 0.1294, step time: 0.1142\n",
      "184/223, train_loss: 0.1334, step time: 0.0985\n",
      "185/223, train_loss: 0.1208, step time: 0.0992\n",
      "186/223, train_loss: 0.1201, step time: 0.1113\n",
      "187/223, train_loss: 0.1302, step time: 0.1090\n",
      "188/223, train_loss: 0.1147, step time: 0.0995\n",
      "189/223, train_loss: 0.1096, step time: 0.0998\n",
      "190/223, train_loss: 0.1280, step time: 0.1704\n",
      "191/223, train_loss: 0.1234, step time: 0.1208\n",
      "192/223, train_loss: 0.1224, step time: 0.1006\n",
      "193/223, train_loss: 0.1274, step time: 0.0998\n",
      "194/223, train_loss: 0.1179, step time: 0.1226\n",
      "195/223, train_loss: 0.1196, step time: 0.1256\n",
      "196/223, train_loss: 0.1277, step time: 0.1186\n",
      "197/223, train_loss: 0.1282, step time: 0.1062\n",
      "198/223, train_loss: 0.1109, step time: 0.1007\n",
      "199/223, train_loss: 0.1328, step time: 0.1457\n",
      "200/223, train_loss: 0.1195, step time: 0.1046\n",
      "201/223, train_loss: 0.1148, step time: 0.0994\n",
      "202/223, train_loss: 0.1321, step time: 0.1268\n",
      "203/223, train_loss: 0.1220, step time: 0.1236\n",
      "204/223, train_loss: 0.1185, step time: 0.1061\n",
      "205/223, train_loss: 0.1385, step time: 0.0997\n",
      "206/223, train_loss: 0.1272, step time: 0.1013\n",
      "207/223, train_loss: 0.1250, step time: 0.1004\n",
      "208/223, train_loss: 0.1225, step time: 0.1156\n",
      "209/223, train_loss: 0.1256, step time: 0.1066\n",
      "210/223, train_loss: 0.1190, step time: 0.1097\n",
      "211/223, train_loss: 0.1226, step time: 0.1108\n",
      "212/223, train_loss: 0.1058, step time: 0.1086\n",
      "213/223, train_loss: 0.1272, step time: 0.1143\n",
      "214/223, train_loss: 0.1253, step time: 0.1155\n",
      "215/223, train_loss: 0.1123, step time: 0.1223\n",
      "216/223, train_loss: 0.1229, step time: 0.1268\n",
      "217/223, train_loss: 0.1282, step time: 0.1016\n",
      "218/223, train_loss: 0.1158, step time: 0.0989\n",
      "219/223, train_loss: 0.1279, step time: 0.0994\n",
      "220/223, train_loss: 0.1275, step time: 0.0990\n",
      "221/223, train_loss: 0.1215, step time: 0.1005\n",
      "222/223, train_loss: 0.1224, step time: 0.1003\n",
      "223/223, train_loss: 0.1157, step time: 0.1000\n",
      "epoch 69 average loss: 0.1227\n",
      "time consuming of epoch 69 is: 91.6975\n",
      "----------\n",
      "epoch 70/300\n",
      "1/223, train_loss: 0.1353, step time: 0.1071\n",
      "2/223, train_loss: 0.1279, step time: 0.1142\n",
      "3/223, train_loss: 0.1164, step time: 0.0999\n",
      "4/223, train_loss: 0.1212, step time: 0.1185\n",
      "5/223, train_loss: 0.1219, step time: 0.1195\n",
      "6/223, train_loss: 0.1207, step time: 0.0991\n",
      "7/223, train_loss: 0.1197, step time: 0.1038\n",
      "8/223, train_loss: 0.1245, step time: 0.1102\n",
      "9/223, train_loss: 0.1143, step time: 0.1096\n",
      "10/223, train_loss: 0.1265, step time: 0.1151\n",
      "11/223, train_loss: 0.1199, step time: 0.1156\n",
      "12/223, train_loss: 0.1238, step time: 0.1062\n",
      "13/223, train_loss: 0.1236, step time: 0.1012\n",
      "14/223, train_loss: 0.1232, step time: 0.1104\n",
      "15/223, train_loss: 0.1171, step time: 0.1107\n",
      "16/223, train_loss: 0.1327, step time: 0.1336\n",
      "17/223, train_loss: 0.1218, step time: 0.1124\n",
      "18/223, train_loss: 0.1061, step time: 0.1126\n",
      "19/223, train_loss: 0.1198, step time: 0.1057\n",
      "20/223, train_loss: 0.1140, step time: 0.1128\n",
      "21/223, train_loss: 0.1264, step time: 0.0997\n",
      "22/223, train_loss: 0.1153, step time: 0.1068\n",
      "23/223, train_loss: 0.1235, step time: 0.1016\n",
      "24/223, train_loss: 0.1325, step time: 0.1118\n",
      "25/223, train_loss: 0.1313, step time: 0.1024\n",
      "26/223, train_loss: 0.1270, step time: 0.0982\n",
      "27/223, train_loss: 0.1130, step time: 0.0988\n",
      "28/223, train_loss: 0.1062, step time: 0.0988\n",
      "29/223, train_loss: 0.1081, step time: 0.1106\n",
      "30/223, train_loss: 0.1095, step time: 0.1161\n",
      "31/223, train_loss: 0.1226, step time: 0.1044\n",
      "32/223, train_loss: 0.1294, step time: 0.1178\n",
      "33/223, train_loss: 0.1186, step time: 0.1023\n",
      "34/223, train_loss: 0.1150, step time: 0.1031\n",
      "35/223, train_loss: 0.1061, step time: 0.1049\n",
      "36/223, train_loss: 0.1233, step time: 0.1075\n",
      "37/223, train_loss: 0.1168, step time: 0.1001\n",
      "38/223, train_loss: 0.1090, step time: 0.1005\n",
      "39/223, train_loss: 0.1160, step time: 0.1012\n",
      "40/223, train_loss: 0.1142, step time: 0.0998\n",
      "41/223, train_loss: 0.1344, step time: 0.1003\n",
      "42/223, train_loss: 0.1270, step time: 0.1117\n",
      "43/223, train_loss: 0.1235, step time: 0.1144\n",
      "44/223, train_loss: 0.1234, step time: 0.1011\n",
      "45/223, train_loss: 0.1215, step time: 0.1054\n",
      "46/223, train_loss: 0.1244, step time: 0.1083\n",
      "47/223, train_loss: 0.1179, step time: 0.1039\n",
      "48/223, train_loss: 0.1339, step time: 0.1167\n",
      "49/223, train_loss: 0.1109, step time: 0.1096\n",
      "50/223, train_loss: 0.1186, step time: 0.1004\n",
      "51/223, train_loss: 0.1267, step time: 0.0997\n",
      "52/223, train_loss: 0.1298, step time: 0.1003\n",
      "53/223, train_loss: 0.1297, step time: 0.0999\n",
      "54/223, train_loss: 0.1353, step time: 0.1000\n",
      "55/223, train_loss: 0.1166, step time: 0.1001\n",
      "56/223, train_loss: 0.1231, step time: 0.1134\n",
      "57/223, train_loss: 0.1128, step time: 0.1123\n",
      "58/223, train_loss: 0.1239, step time: 0.1008\n",
      "59/223, train_loss: 0.1168, step time: 0.1076\n",
      "60/223, train_loss: 0.1271, step time: 0.1031\n",
      "61/223, train_loss: 0.1224, step time: 0.1082\n",
      "62/223, train_loss: 0.1124, step time: 0.0994\n",
      "63/223, train_loss: 0.1243, step time: 0.1006\n",
      "64/223, train_loss: 0.1287, step time: 0.0997\n",
      "65/223, train_loss: 0.1082, step time: 0.1047\n",
      "66/223, train_loss: 0.1165, step time: 0.1063\n",
      "67/223, train_loss: 0.1273, step time: 0.1009\n",
      "68/223, train_loss: 0.1091, step time: 0.1004\n",
      "69/223, train_loss: 0.1173, step time: 0.0993\n",
      "70/223, train_loss: 0.1308, step time: 0.1008\n",
      "71/223, train_loss: 0.1223, step time: 0.1000\n",
      "72/223, train_loss: 0.1254, step time: 0.1065\n",
      "73/223, train_loss: 0.1235, step time: 0.1002\n",
      "74/223, train_loss: 0.1364, step time: 0.0997\n",
      "75/223, train_loss: 0.1207, step time: 0.1039\n",
      "76/223, train_loss: 0.1341, step time: 0.1003\n",
      "77/223, train_loss: 0.1190, step time: 0.0997\n",
      "78/223, train_loss: 0.1241, step time: 0.1003\n",
      "79/223, train_loss: 0.1305, step time: 0.1000\n",
      "80/223, train_loss: 0.1288, step time: 0.1088\n",
      "81/223, train_loss: 0.1302, step time: 0.1100\n",
      "82/223, train_loss: 0.1292, step time: 0.1233\n",
      "83/223, train_loss: 0.1105, step time: 0.1198\n",
      "84/223, train_loss: 0.1114, step time: 0.1094\n",
      "85/223, train_loss: 0.1204, step time: 0.1196\n",
      "86/223, train_loss: 0.1346, step time: 0.1103\n",
      "87/223, train_loss: 0.1264, step time: 0.1158\n",
      "88/223, train_loss: 0.1208, step time: 0.1081\n",
      "89/223, train_loss: 0.1213, step time: 0.1371\n",
      "90/223, train_loss: 0.1119, step time: 0.1270\n",
      "91/223, train_loss: 0.1218, step time: 0.1101\n",
      "92/223, train_loss: 0.1344, step time: 0.1082\n",
      "93/223, train_loss: 0.1113, step time: 0.1076\n",
      "94/223, train_loss: 0.1132, step time: 0.1027\n",
      "95/223, train_loss: 0.1335, step time: 0.1046\n",
      "96/223, train_loss: 0.1195, step time: 0.1019\n",
      "97/223, train_loss: 0.1173, step time: 0.1174\n",
      "98/223, train_loss: 0.1229, step time: 0.1051\n",
      "99/223, train_loss: 0.1046, step time: 0.1125\n",
      "100/223, train_loss: 0.1318, step time: 0.1007\n",
      "101/223, train_loss: 0.1127, step time: 0.1007\n",
      "102/223, train_loss: 0.1304, step time: 0.1003\n",
      "103/223, train_loss: 0.1230, step time: 0.1005\n",
      "104/223, train_loss: 0.1143, step time: 0.1082\n",
      "105/223, train_loss: 0.1271, step time: 0.1257\n",
      "106/223, train_loss: 0.1134, step time: 0.1005\n",
      "107/223, train_loss: 0.1202, step time: 0.1002\n",
      "108/223, train_loss: 0.1162, step time: 0.1007\n",
      "109/223, train_loss: 0.1269, step time: 0.1137\n",
      "110/223, train_loss: 0.1234, step time: 0.1079\n",
      "111/223, train_loss: 0.1118, step time: 0.1051\n",
      "112/223, train_loss: 0.1189, step time: 0.1101\n",
      "113/223, train_loss: 0.1137, step time: 0.1126\n",
      "114/223, train_loss: 0.1079, step time: 0.1004\n",
      "115/223, train_loss: 0.1119, step time: 0.1001\n",
      "116/223, train_loss: 0.1197, step time: 0.1005\n",
      "117/223, train_loss: 0.1112, step time: 0.1096\n",
      "118/223, train_loss: 0.1151, step time: 0.1261\n",
      "119/223, train_loss: 0.1111, step time: 0.1003\n",
      "120/223, train_loss: 0.1199, step time: 0.1095\n",
      "121/223, train_loss: 0.1216, step time: 0.0997\n",
      "122/223, train_loss: 0.1268, step time: 0.1042\n",
      "123/223, train_loss: 0.1243, step time: 0.1146\n",
      "124/223, train_loss: 0.1226, step time: 0.0998\n",
      "125/223, train_loss: 0.1069, step time: 0.1005\n",
      "126/223, train_loss: 0.1278, step time: 0.1001\n",
      "127/223, train_loss: 0.1228, step time: 0.1005\n",
      "128/223, train_loss: 0.1235, step time: 0.1009\n",
      "129/223, train_loss: 0.1170, step time: 0.1002\n",
      "130/223, train_loss: 0.1147, step time: 0.1027\n",
      "131/223, train_loss: 0.1175, step time: 0.1052\n",
      "132/223, train_loss: 0.1315, step time: 0.1101\n",
      "133/223, train_loss: 0.1099, step time: 0.1059\n",
      "134/223, train_loss: 0.1401, step time: 0.1005\n",
      "135/223, train_loss: 0.1140, step time: 0.1026\n",
      "136/223, train_loss: 0.1264, step time: 0.1009\n",
      "137/223, train_loss: 0.1138, step time: 0.1175\n",
      "138/223, train_loss: 0.1240, step time: 0.1011\n",
      "139/223, train_loss: 0.1197, step time: 0.1008\n",
      "140/223, train_loss: 0.1166, step time: 0.1027\n",
      "141/223, train_loss: 0.1221, step time: 0.1197\n",
      "142/223, train_loss: 0.1113, step time: 0.0995\n",
      "143/223, train_loss: 0.1171, step time: 0.1000\n",
      "144/223, train_loss: 0.1222, step time: 0.1220\n",
      "145/223, train_loss: 0.1313, step time: 0.1001\n",
      "146/223, train_loss: 0.1203, step time: 0.1008\n",
      "147/223, train_loss: 0.1362, step time: 0.1003\n",
      "148/223, train_loss: 0.1281, step time: 0.1023\n",
      "149/223, train_loss: 0.1292, step time: 0.1024\n",
      "150/223, train_loss: 0.1179, step time: 0.1197\n",
      "151/223, train_loss: 0.1164, step time: 0.1047\n",
      "152/223, train_loss: 0.1115, step time: 0.1170\n",
      "153/223, train_loss: 0.1213, step time: 0.1001\n",
      "154/223, train_loss: 0.1270, step time: 0.1054\n",
      "155/223, train_loss: 0.1191, step time: 0.1042\n",
      "156/223, train_loss: 0.1284, step time: 0.1273\n",
      "157/223, train_loss: 0.3149, step time: 0.1316\n",
      "158/223, train_loss: 0.1069, step time: 0.1261\n",
      "159/223, train_loss: 0.1171, step time: 0.1066\n",
      "160/223, train_loss: 0.1194, step time: 0.1153\n",
      "161/223, train_loss: 0.1218, step time: 0.1003\n",
      "162/223, train_loss: 0.1109, step time: 0.1023\n",
      "163/223, train_loss: 0.1232, step time: 0.1000\n",
      "164/223, train_loss: 0.1213, step time: 0.1087\n",
      "165/223, train_loss: 0.1131, step time: 0.1018\n",
      "166/223, train_loss: 0.1201, step time: 0.1002\n",
      "167/223, train_loss: 0.1232, step time: 0.1009\n",
      "168/223, train_loss: 0.1208, step time: 0.1356\n",
      "169/223, train_loss: 0.1190, step time: 0.1028\n",
      "170/223, train_loss: 0.1234, step time: 0.1201\n",
      "171/223, train_loss: 0.1174, step time: 0.1229\n",
      "172/223, train_loss: 0.1160, step time: 0.1118\n",
      "173/223, train_loss: 0.1224, step time: 0.1099\n",
      "174/223, train_loss: 0.1322, step time: 0.1000\n",
      "175/223, train_loss: 0.1201, step time: 0.1037\n",
      "176/223, train_loss: 0.1289, step time: 0.1210\n",
      "177/223, train_loss: 0.1208, step time: 0.1189\n",
      "178/223, train_loss: 0.1154, step time: 0.1075\n",
      "179/223, train_loss: 0.1207, step time: 0.1076\n",
      "180/223, train_loss: 0.1108, step time: 0.1004\n",
      "181/223, train_loss: 0.1159, step time: 0.1013\n",
      "182/223, train_loss: 0.1185, step time: 0.0999\n",
      "183/223, train_loss: 0.1278, step time: 0.0998\n",
      "184/223, train_loss: 0.1257, step time: 0.1001\n",
      "185/223, train_loss: 0.1201, step time: 0.1000\n",
      "186/223, train_loss: 0.1149, step time: 0.1169\n",
      "187/223, train_loss: 0.1150, step time: 0.1049\n",
      "188/223, train_loss: 0.1193, step time: 0.1081\n",
      "189/223, train_loss: 0.1320, step time: 0.1002\n",
      "190/223, train_loss: 0.1210, step time: 0.0998\n",
      "191/223, train_loss: 0.1246, step time: 0.1006\n",
      "192/223, train_loss: 0.1112, step time: 0.1036\n",
      "193/223, train_loss: 0.1229, step time: 0.1061\n",
      "194/223, train_loss: 0.1247, step time: 0.1090\n",
      "195/223, train_loss: 0.1196, step time: 0.1010\n",
      "196/223, train_loss: 0.1225, step time: 0.1007\n",
      "197/223, train_loss: 0.1282, step time: 0.1120\n",
      "198/223, train_loss: 0.1376, step time: 0.1039\n",
      "199/223, train_loss: 0.1168, step time: 0.0999\n",
      "200/223, train_loss: 0.1296, step time: 0.1006\n",
      "201/223, train_loss: 0.1182, step time: 0.1181\n",
      "202/223, train_loss: 0.1277, step time: 0.1070\n",
      "203/223, train_loss: 0.1308, step time: 0.1029\n",
      "204/223, train_loss: 0.1223, step time: 0.0997\n",
      "205/223, train_loss: 0.1263, step time: 0.1127\n",
      "206/223, train_loss: 0.1519, step time: 0.1127\n",
      "207/223, train_loss: 0.1122, step time: 0.1182\n",
      "208/223, train_loss: 0.1245, step time: 0.1341\n",
      "209/223, train_loss: 0.1096, step time: 0.0994\n",
      "210/223, train_loss: 0.1106, step time: 0.0994\n",
      "211/223, train_loss: 0.1202, step time: 0.0995\n",
      "212/223, train_loss: 0.1453, step time: 0.1071\n",
      "213/223, train_loss: 0.1307, step time: 0.0993\n",
      "214/223, train_loss: 0.1321, step time: 0.0991\n",
      "215/223, train_loss: 0.1152, step time: 0.0993\n",
      "216/223, train_loss: 0.1183, step time: 0.1269\n",
      "217/223, train_loss: 0.1179, step time: 0.1016\n",
      "218/223, train_loss: 0.1208, step time: 0.1002\n",
      "219/223, train_loss: 0.1133, step time: 0.0994\n",
      "220/223, train_loss: 0.1210, step time: 0.1071\n",
      "221/223, train_loss: 0.1226, step time: 0.1011\n",
      "222/223, train_loss: 0.1207, step time: 0.0999\n",
      "223/223, train_loss: 0.1359, step time: 0.1009\n",
      "epoch 70 average loss: 0.1222\n",
      "saved new best metric model\n",
      "current epoch: 70 current mean dice: 0.8413 tc: 0.9124 wt: 0.8527 et: 0.7588\n",
      "best mean dice: 0.8413 at epoch: 70\n",
      "time consuming of epoch 70 is: 91.2997\n",
      "----------\n",
      "epoch 71/300\n",
      "1/223, train_loss: 0.1151, step time: 0.1042\n",
      "2/223, train_loss: 0.1206, step time: 0.1001\n",
      "3/223, train_loss: 0.1172, step time: 0.1014\n",
      "4/223, train_loss: 0.1053, step time: 0.1138\n",
      "5/223, train_loss: 0.1181, step time: 0.0992\n",
      "6/223, train_loss: 0.1197, step time: 0.1278\n",
      "7/223, train_loss: 0.1249, step time: 0.1094\n",
      "8/223, train_loss: 0.1153, step time: 0.1113\n",
      "9/223, train_loss: 0.1173, step time: 0.0994\n",
      "10/223, train_loss: 0.1230, step time: 0.1257\n",
      "11/223, train_loss: 0.1174, step time: 0.1042\n",
      "12/223, train_loss: 0.1152, step time: 0.1101\n",
      "13/223, train_loss: 0.1315, step time: 0.1097\n",
      "14/223, train_loss: 0.1242, step time: 0.1255\n",
      "15/223, train_loss: 0.1219, step time: 0.1159\n",
      "16/223, train_loss: 0.1347, step time: 0.1165\n",
      "17/223, train_loss: 0.1156, step time: 0.1115\n",
      "18/223, train_loss: 0.1247, step time: 0.1188\n",
      "19/223, train_loss: 0.1347, step time: 0.1071\n",
      "20/223, train_loss: 0.1258, step time: 0.1002\n",
      "21/223, train_loss: 0.1135, step time: 0.1122\n",
      "22/223, train_loss: 0.1246, step time: 0.1076\n",
      "23/223, train_loss: 0.1123, step time: 0.1033\n",
      "24/223, train_loss: 0.1250, step time: 0.1013\n",
      "25/223, train_loss: 0.1206, step time: 0.0995\n",
      "26/223, train_loss: 0.1202, step time: 0.1236\n",
      "27/223, train_loss: 0.1216, step time: 0.1141\n",
      "28/223, train_loss: 0.1208, step time: 0.1120\n",
      "29/223, train_loss: 0.1241, step time: 0.1037\n",
      "30/223, train_loss: 0.1418, step time: 0.1098\n",
      "31/223, train_loss: 0.1189, step time: 0.1103\n",
      "32/223, train_loss: 0.1260, step time: 0.1031\n",
      "33/223, train_loss: 0.1292, step time: 0.0996\n",
      "34/223, train_loss: 0.1232, step time: 0.1000\n",
      "35/223, train_loss: 0.1271, step time: 0.1205\n",
      "36/223, train_loss: 0.1103, step time: 0.1121\n",
      "37/223, train_loss: 0.1138, step time: 0.1065\n",
      "38/223, train_loss: 0.1145, step time: 0.1012\n",
      "39/223, train_loss: 0.1270, step time: 0.1234\n",
      "40/223, train_loss: 0.1232, step time: 0.1140\n",
      "41/223, train_loss: 0.1225, step time: 0.1042\n",
      "42/223, train_loss: 0.1323, step time: 0.1180\n",
      "43/223, train_loss: 0.1095, step time: 0.1062\n",
      "44/223, train_loss: 0.1077, step time: 0.1090\n",
      "45/223, train_loss: 0.1410, step time: 0.1152\n",
      "46/223, train_loss: 0.1197, step time: 0.1056\n",
      "47/223, train_loss: 0.1167, step time: 0.0997\n",
      "48/223, train_loss: 0.1183, step time: 0.1004\n",
      "49/223, train_loss: 0.1203, step time: 0.1111\n",
      "50/223, train_loss: 0.1183, step time: 0.1112\n",
      "51/223, train_loss: 0.1110, step time: 0.1307\n",
      "52/223, train_loss: 0.1366, step time: 0.0999\n",
      "53/223, train_loss: 0.1090, step time: 0.1127\n",
      "54/223, train_loss: 0.1390, step time: 0.1233\n",
      "55/223, train_loss: 0.1325, step time: 0.1079\n",
      "56/223, train_loss: 0.1280, step time: 0.1059\n",
      "57/223, train_loss: 0.1186, step time: 0.1178\n",
      "58/223, train_loss: 0.1219, step time: 0.1358\n",
      "59/223, train_loss: 0.1416, step time: 0.1063\n",
      "60/223, train_loss: 0.1264, step time: 0.1006\n",
      "61/223, train_loss: 0.1133, step time: 0.1143\n",
      "62/223, train_loss: 0.1135, step time: 0.1112\n",
      "63/223, train_loss: 0.1242, step time: 0.1111\n",
      "64/223, train_loss: 0.1242, step time: 0.1067\n",
      "65/223, train_loss: 0.1152, step time: 0.1174\n",
      "66/223, train_loss: 0.1156, step time: 0.1191\n",
      "67/223, train_loss: 0.1259, step time: 0.1162\n",
      "68/223, train_loss: 0.1328, step time: 0.1169\n",
      "69/223, train_loss: 0.1301, step time: 0.1191\n",
      "70/223, train_loss: 0.1227, step time: 0.1119\n",
      "71/223, train_loss: 0.1086, step time: 0.1033\n",
      "72/223, train_loss: 0.1191, step time: 0.1018\n",
      "73/223, train_loss: 0.1105, step time: 0.0997\n",
      "74/223, train_loss: 0.1172, step time: 0.1005\n",
      "75/223, train_loss: 0.1276, step time: 0.0998\n",
      "76/223, train_loss: 0.1147, step time: 0.1117\n",
      "77/223, train_loss: 0.1373, step time: 0.0995\n",
      "78/223, train_loss: 0.1133, step time: 0.1018\n",
      "79/223, train_loss: 0.1232, step time: 0.1092\n",
      "80/223, train_loss: 0.1137, step time: 0.1319\n",
      "81/223, train_loss: 0.1284, step time: 0.1001\n",
      "82/223, train_loss: 0.1134, step time: 0.1090\n",
      "83/223, train_loss: 0.1167, step time: 0.1015\n",
      "84/223, train_loss: 0.1268, step time: 0.1471\n",
      "85/223, train_loss: 0.1230, step time: 0.1004\n",
      "86/223, train_loss: 0.1138, step time: 0.0995\n",
      "87/223, train_loss: 0.1183, step time: 0.1008\n",
      "88/223, train_loss: 0.1221, step time: 0.1056\n",
      "89/223, train_loss: 0.1155, step time: 0.1114\n",
      "90/223, train_loss: 0.1132, step time: 0.1144\n",
      "91/223, train_loss: 0.1221, step time: 0.1306\n",
      "92/223, train_loss: 0.1236, step time: 0.1240\n",
      "93/223, train_loss: 0.1094, step time: 0.1004\n",
      "94/223, train_loss: 0.1107, step time: 0.1009\n",
      "95/223, train_loss: 0.1138, step time: 0.1173\n",
      "96/223, train_loss: 0.1256, step time: 0.1001\n",
      "97/223, train_loss: 0.1252, step time: 0.1102\n",
      "98/223, train_loss: 0.1180, step time: 0.1197\n",
      "99/223, train_loss: 0.1149, step time: 0.1111\n",
      "100/223, train_loss: 0.1091, step time: 0.1068\n",
      "101/223, train_loss: 0.1259, step time: 0.1016\n",
      "102/223, train_loss: 0.1289, step time: 0.1034\n",
      "103/223, train_loss: 0.1287, step time: 0.1001\n",
      "104/223, train_loss: 0.1128, step time: 0.0999\n",
      "105/223, train_loss: 0.1202, step time: 0.1179\n",
      "106/223, train_loss: 0.1173, step time: 0.1001\n",
      "107/223, train_loss: 0.1275, step time: 0.1362\n",
      "108/223, train_loss: 0.1161, step time: 0.1122\n",
      "109/223, train_loss: 0.1035, step time: 0.1091\n",
      "110/223, train_loss: 0.1090, step time: 0.1140\n",
      "111/223, train_loss: 0.1081, step time: 0.1004\n",
      "112/223, train_loss: 0.1311, step time: 0.1100\n",
      "113/223, train_loss: 0.1278, step time: 0.1176\n",
      "114/223, train_loss: 0.1170, step time: 0.1238\n",
      "115/223, train_loss: 0.1206, step time: 0.1135\n",
      "116/223, train_loss: 0.1221, step time: 0.1029\n",
      "117/223, train_loss: 0.1129, step time: 0.1044\n",
      "118/223, train_loss: 0.1178, step time: 0.1292\n",
      "119/223, train_loss: 0.1211, step time: 0.1005\n",
      "120/223, train_loss: 0.1267, step time: 0.1005\n",
      "121/223, train_loss: 0.1176, step time: 0.1096\n",
      "122/223, train_loss: 0.1364, step time: 0.1010\n",
      "123/223, train_loss: 0.1291, step time: 0.1003\n",
      "124/223, train_loss: 0.1095, step time: 0.1016\n",
      "125/223, train_loss: 0.1242, step time: 0.1005\n",
      "126/223, train_loss: 0.1198, step time: 0.1274\n",
      "127/223, train_loss: 0.1244, step time: 0.1027\n",
      "128/223, train_loss: 0.1262, step time: 0.1199\n",
      "129/223, train_loss: 0.1143, step time: 0.1005\n",
      "130/223, train_loss: 0.1226, step time: 0.1003\n",
      "131/223, train_loss: 0.1110, step time: 0.1009\n",
      "132/223, train_loss: 0.1249, step time: 0.1088\n",
      "133/223, train_loss: 0.1315, step time: 0.1177\n",
      "134/223, train_loss: 0.1265, step time: 0.1004\n",
      "135/223, train_loss: 0.1302, step time: 0.1013\n",
      "136/223, train_loss: 0.1178, step time: 0.1253\n",
      "137/223, train_loss: 0.1182, step time: 0.1078\n",
      "138/223, train_loss: 0.1236, step time: 0.1138\n",
      "139/223, train_loss: 0.1221, step time: 0.1003\n",
      "140/223, train_loss: 0.1242, step time: 0.1066\n",
      "141/223, train_loss: 0.1156, step time: 0.1121\n",
      "142/223, train_loss: 0.1336, step time: 0.1050\n",
      "143/223, train_loss: 0.1217, step time: 0.1050\n",
      "144/223, train_loss: 0.1389, step time: 0.1070\n",
      "145/223, train_loss: 0.1182, step time: 0.1075\n",
      "146/223, train_loss: 0.1342, step time: 0.1060\n",
      "147/223, train_loss: 0.1252, step time: 0.1507\n",
      "148/223, train_loss: 0.1233, step time: 0.1009\n",
      "149/223, train_loss: 0.1307, step time: 0.1168\n",
      "150/223, train_loss: 0.1233, step time: 0.1183\n",
      "151/223, train_loss: 0.1212, step time: 0.1159\n",
      "152/223, train_loss: 0.1266, step time: 0.1008\n",
      "153/223, train_loss: 0.1184, step time: 0.1042\n",
      "154/223, train_loss: 0.1230, step time: 0.1105\n",
      "155/223, train_loss: 0.1125, step time: 0.1035\n",
      "156/223, train_loss: 0.1152, step time: 0.1146\n",
      "157/223, train_loss: 0.1179, step time: 0.1003\n",
      "158/223, train_loss: 0.1106, step time: 0.1001\n",
      "159/223, train_loss: 0.1295, step time: 0.1007\n",
      "160/223, train_loss: 0.1255, step time: 0.1017\n",
      "161/223, train_loss: 0.1193, step time: 0.1018\n",
      "162/223, train_loss: 0.1212, step time: 0.1251\n",
      "163/223, train_loss: 0.1175, step time: 0.1189\n",
      "164/223, train_loss: 0.1250, step time: 0.1182\n",
      "165/223, train_loss: 0.1183, step time: 0.1185\n",
      "166/223, train_loss: 0.1196, step time: 0.1260\n",
      "167/223, train_loss: 0.1227, step time: 0.1199\n",
      "168/223, train_loss: 0.1175, step time: 0.1155\n",
      "169/223, train_loss: 0.1098, step time: 0.0993\n",
      "170/223, train_loss: 0.1128, step time: 0.1036\n",
      "171/223, train_loss: 0.1166, step time: 0.1009\n",
      "172/223, train_loss: 0.1153, step time: 0.0999\n",
      "173/223, train_loss: 0.1197, step time: 0.1007\n",
      "174/223, train_loss: 0.1275, step time: 0.1158\n",
      "175/223, train_loss: 0.1395, step time: 0.1138\n",
      "176/223, train_loss: 0.1290, step time: 0.0995\n",
      "177/223, train_loss: 0.1165, step time: 0.1000\n",
      "178/223, train_loss: 0.1323, step time: 0.1021\n",
      "179/223, train_loss: 0.1177, step time: 0.0994\n",
      "180/223, train_loss: 0.1283, step time: 0.1027\n",
      "181/223, train_loss: 0.1387, step time: 0.1167\n",
      "182/223, train_loss: 0.1171, step time: 0.1024\n",
      "183/223, train_loss: 0.1164, step time: 0.1159\n",
      "184/223, train_loss: 0.1267, step time: 0.1000\n",
      "185/223, train_loss: 0.1190, step time: 0.1086\n",
      "186/223, train_loss: 0.1121, step time: 0.1131\n",
      "187/223, train_loss: 0.1174, step time: 0.1008\n",
      "188/223, train_loss: 0.1080, step time: 0.1009\n",
      "189/223, train_loss: 0.1216, step time: 0.1006\n",
      "190/223, train_loss: 0.1176, step time: 0.1141\n",
      "191/223, train_loss: 0.1324, step time: 0.1026\n",
      "192/223, train_loss: 0.1289, step time: 0.0999\n",
      "193/223, train_loss: 0.1106, step time: 0.1030\n",
      "194/223, train_loss: 0.1073, step time: 0.1002\n",
      "195/223, train_loss: 0.1257, step time: 0.1347\n",
      "196/223, train_loss: 0.1080, step time: 0.1003\n",
      "197/223, train_loss: 0.1204, step time: 0.1055\n",
      "198/223, train_loss: 0.1235, step time: 0.1007\n",
      "199/223, train_loss: 0.1158, step time: 0.1048\n",
      "200/223, train_loss: 0.1289, step time: 0.1005\n",
      "201/223, train_loss: 0.1352, step time: 0.1062\n",
      "202/223, train_loss: 0.3103, step time: 0.0998\n",
      "203/223, train_loss: 0.1287, step time: 0.1005\n",
      "204/223, train_loss: 0.1200, step time: 0.1008\n",
      "205/223, train_loss: 0.1420, step time: 0.1181\n",
      "206/223, train_loss: 0.1246, step time: 0.1104\n",
      "207/223, train_loss: 0.1373, step time: 0.1087\n",
      "208/223, train_loss: 0.1106, step time: 0.1006\n",
      "209/223, train_loss: 0.1175, step time: 0.1104\n",
      "210/223, train_loss: 0.1181, step time: 0.1099\n",
      "211/223, train_loss: 0.1191, step time: 0.1006\n",
      "212/223, train_loss: 0.1197, step time: 0.1002\n",
      "213/223, train_loss: 0.1240, step time: 0.1014\n",
      "214/223, train_loss: 0.1213, step time: 0.1054\n",
      "215/223, train_loss: 0.1417, step time: 0.1038\n",
      "216/223, train_loss: 0.1146, step time: 0.1098\n",
      "217/223, train_loss: 0.1143, step time: 0.1130\n",
      "218/223, train_loss: 0.1208, step time: 0.1196\n",
      "219/223, train_loss: 0.1173, step time: 0.1178\n",
      "220/223, train_loss: 0.1074, step time: 0.1031\n",
      "221/223, train_loss: 0.1229, step time: 0.1092\n",
      "222/223, train_loss: 0.1258, step time: 0.1028\n",
      "223/223, train_loss: 0.1251, step time: 0.1009\n",
      "epoch 71 average loss: 0.1222\n",
      "time consuming of epoch 71 is: 86.9194\n",
      "----------\n",
      "epoch 72/300\n",
      "1/223, train_loss: 0.1308, step time: 0.1016\n",
      "2/223, train_loss: 0.1178, step time: 0.1014\n",
      "3/223, train_loss: 0.1309, step time: 0.1039\n",
      "4/223, train_loss: 0.1142, step time: 0.1001\n",
      "5/223, train_loss: 0.1323, step time: 0.1015\n",
      "6/223, train_loss: 0.1186, step time: 0.1039\n",
      "7/223, train_loss: 0.1313, step time: 0.1089\n",
      "8/223, train_loss: 0.1090, step time: 0.1245\n",
      "9/223, train_loss: 0.1194, step time: 0.1208\n",
      "10/223, train_loss: 0.1189, step time: 0.1037\n",
      "11/223, train_loss: 0.1119, step time: 0.1128\n",
      "12/223, train_loss: 0.1177, step time: 0.1180\n",
      "13/223, train_loss: 0.1152, step time: 0.1211\n",
      "14/223, train_loss: 0.1070, step time: 0.1038\n",
      "15/223, train_loss: 0.1331, step time: 0.1151\n",
      "16/223, train_loss: 0.1324, step time: 0.1186\n",
      "17/223, train_loss: 0.1171, step time: 0.1204\n",
      "18/223, train_loss: 0.1245, step time: 0.1138\n",
      "19/223, train_loss: 0.1145, step time: 0.1007\n",
      "20/223, train_loss: 0.1116, step time: 0.1011\n",
      "21/223, train_loss: 0.1167, step time: 0.1179\n",
      "22/223, train_loss: 0.1200, step time: 0.1010\n",
      "23/223, train_loss: 0.1231, step time: 0.1178\n",
      "24/223, train_loss: 0.1127, step time: 0.1246\n",
      "25/223, train_loss: 0.1209, step time: 0.1006\n",
      "26/223, train_loss: 0.1179, step time: 0.1182\n",
      "27/223, train_loss: 0.1220, step time: 0.1028\n",
      "28/223, train_loss: 0.1202, step time: 0.0999\n",
      "29/223, train_loss: 0.1145, step time: 0.1005\n",
      "30/223, train_loss: 0.1195, step time: 0.1016\n",
      "31/223, train_loss: 0.1149, step time: 0.1025\n",
      "32/223, train_loss: 0.1156, step time: 0.0999\n",
      "33/223, train_loss: 0.1168, step time: 0.1127\n",
      "34/223, train_loss: 0.1288, step time: 0.1051\n",
      "35/223, train_loss: 0.1190, step time: 0.1106\n",
      "36/223, train_loss: 0.1350, step time: 0.1041\n",
      "37/223, train_loss: 0.1099, step time: 0.1178\n",
      "38/223, train_loss: 0.1212, step time: 0.1161\n",
      "39/223, train_loss: 0.1140, step time: 0.1003\n",
      "40/223, train_loss: 0.1215, step time: 0.1024\n",
      "41/223, train_loss: 0.1139, step time: 0.0996\n",
      "42/223, train_loss: 0.1160, step time: 0.1003\n",
      "43/223, train_loss: 0.1267, step time: 0.1148\n",
      "44/223, train_loss: 0.1210, step time: 0.1092\n",
      "45/223, train_loss: 0.1243, step time: 0.1079\n",
      "46/223, train_loss: 0.1058, step time: 0.1005\n",
      "47/223, train_loss: 0.1265, step time: 0.1171\n",
      "48/223, train_loss: 0.1096, step time: 0.1029\n",
      "49/223, train_loss: 0.1246, step time: 0.1037\n",
      "50/223, train_loss: 0.1215, step time: 0.1006\n",
      "51/223, train_loss: 0.1239, step time: 0.1110\n",
      "52/223, train_loss: 0.1110, step time: 0.1106\n",
      "53/223, train_loss: 0.1165, step time: 0.1006\n",
      "54/223, train_loss: 0.1091, step time: 0.0998\n",
      "55/223, train_loss: 0.1192, step time: 0.1099\n",
      "56/223, train_loss: 0.1230, step time: 0.1015\n",
      "57/223, train_loss: 0.1220, step time: 0.1187\n",
      "58/223, train_loss: 0.1189, step time: 0.1173\n",
      "59/223, train_loss: 0.1140, step time: 0.1134\n",
      "60/223, train_loss: 0.1170, step time: 0.1116\n",
      "61/223, train_loss: 0.1253, step time: 0.1312\n",
      "62/223, train_loss: 0.1261, step time: 0.1067\n",
      "63/223, train_loss: 0.1117, step time: 0.1158\n",
      "64/223, train_loss: 0.1297, step time: 0.1004\n",
      "65/223, train_loss: 0.1017, step time: 0.1006\n",
      "66/223, train_loss: 0.1133, step time: 0.1002\n",
      "67/223, train_loss: 0.1198, step time: 0.1039\n",
      "68/223, train_loss: 0.1350, step time: 0.1078\n",
      "69/223, train_loss: 0.1371, step time: 0.1004\n",
      "70/223, train_loss: 0.1380, step time: 0.1105\n",
      "71/223, train_loss: 0.1125, step time: 0.1004\n",
      "72/223, train_loss: 0.1147, step time: 0.1090\n",
      "73/223, train_loss: 0.1355, step time: 0.1031\n",
      "74/223, train_loss: 0.1187, step time: 0.1177\n",
      "75/223, train_loss: 0.1165, step time: 0.1061\n",
      "76/223, train_loss: 0.1095, step time: 0.1141\n",
      "77/223, train_loss: 0.1363, step time: 0.1125\n",
      "78/223, train_loss: 0.1274, step time: 0.1247\n",
      "79/223, train_loss: 0.1219, step time: 0.1127\n",
      "80/223, train_loss: 0.1154, step time: 0.1331\n",
      "81/223, train_loss: 0.1203, step time: 0.1027\n",
      "82/223, train_loss: 0.1188, step time: 0.1216\n",
      "83/223, train_loss: 0.1141, step time: 0.1061\n",
      "84/223, train_loss: 0.1181, step time: 0.1099\n",
      "85/223, train_loss: 0.1232, step time: 0.1175\n",
      "86/223, train_loss: 0.1093, step time: 0.1060\n",
      "87/223, train_loss: 0.1200, step time: 0.1013\n",
      "88/223, train_loss: 0.1221, step time: 0.0997\n",
      "89/223, train_loss: 0.1299, step time: 0.1155\n",
      "90/223, train_loss: 0.1162, step time: 0.1095\n",
      "91/223, train_loss: 0.1127, step time: 0.1053\n",
      "92/223, train_loss: 0.1164, step time: 0.1139\n",
      "93/223, train_loss: 0.1415, step time: 0.0997\n",
      "94/223, train_loss: 0.1112, step time: 0.1014\n",
      "95/223, train_loss: 0.1126, step time: 0.1010\n",
      "96/223, train_loss: 0.1445, step time: 0.1175\n",
      "97/223, train_loss: 0.1236, step time: 0.1105\n",
      "98/223, train_loss: 0.1176, step time: 0.1002\n",
      "99/223, train_loss: 0.1122, step time: 0.1003\n",
      "100/223, train_loss: 0.1286, step time: 0.1012\n",
      "101/223, train_loss: 0.1144, step time: 0.1073\n",
      "102/223, train_loss: 0.1287, step time: 0.1014\n",
      "103/223, train_loss: 0.1310, step time: 0.1256\n",
      "104/223, train_loss: 0.1237, step time: 0.1128\n",
      "105/223, train_loss: 0.1189, step time: 0.1112\n",
      "106/223, train_loss: 0.1388, step time: 0.1183\n",
      "107/223, train_loss: 0.1233, step time: 0.1131\n",
      "108/223, train_loss: 0.1057, step time: 0.1062\n",
      "109/223, train_loss: 0.1172, step time: 0.1137\n",
      "110/223, train_loss: 0.3241, step time: 0.1105\n",
      "111/223, train_loss: 0.1197, step time: 0.1002\n",
      "112/223, train_loss: 0.1287, step time: 0.1002\n",
      "113/223, train_loss: 0.1068, step time: 0.1092\n",
      "114/223, train_loss: 0.1261, step time: 0.1274\n",
      "115/223, train_loss: 0.1084, step time: 0.1001\n",
      "116/223, train_loss: 0.1261, step time: 0.1008\n",
      "117/223, train_loss: 0.1276, step time: 0.1048\n",
      "118/223, train_loss: 0.1100, step time: 0.1216\n",
      "119/223, train_loss: 0.1145, step time: 0.1209\n",
      "120/223, train_loss: 0.1228, step time: 0.0998\n",
      "121/223, train_loss: 0.1401, step time: 0.1001\n",
      "122/223, train_loss: 0.1170, step time: 0.0994\n",
      "123/223, train_loss: 0.1149, step time: 0.1012\n",
      "124/223, train_loss: 0.1067, step time: 0.1019\n",
      "125/223, train_loss: 0.1141, step time: 0.1149\n",
      "126/223, train_loss: 0.1277, step time: 0.1081\n",
      "127/223, train_loss: 0.1150, step time: 0.1175\n",
      "128/223, train_loss: 0.1257, step time: 0.1068\n",
      "129/223, train_loss: 0.1237, step time: 0.1200\n",
      "130/223, train_loss: 0.1294, step time: 0.1099\n",
      "131/223, train_loss: 0.1195, step time: 0.1035\n",
      "132/223, train_loss: 0.1007, step time: 0.1211\n",
      "133/223, train_loss: 0.1230, step time: 0.1032\n",
      "134/223, train_loss: 0.1105, step time: 0.1158\n",
      "135/223, train_loss: 0.1170, step time: 0.1176\n",
      "136/223, train_loss: 0.1148, step time: 0.1153\n",
      "137/223, train_loss: 0.1339, step time: 0.1085\n",
      "138/223, train_loss: 0.1291, step time: 0.1112\n",
      "139/223, train_loss: 0.1354, step time: 0.1111\n",
      "140/223, train_loss: 0.1286, step time: 0.1263\n",
      "141/223, train_loss: 0.1134, step time: 0.1177\n",
      "142/223, train_loss: 0.1181, step time: 0.1325\n",
      "143/223, train_loss: 0.1077, step time: 0.1129\n",
      "144/223, train_loss: 0.1263, step time: 0.1106\n",
      "145/223, train_loss: 0.1161, step time: 0.1349\n",
      "146/223, train_loss: 0.1253, step time: 0.1250\n",
      "147/223, train_loss: 0.1308, step time: 0.1130\n",
      "148/223, train_loss: 0.1103, step time: 0.1133\n",
      "149/223, train_loss: 0.1221, step time: 0.1247\n",
      "150/223, train_loss: 0.1200, step time: 0.1045\n",
      "151/223, train_loss: 0.1333, step time: 0.1014\n",
      "152/223, train_loss: 0.1242, step time: 0.1217\n",
      "153/223, train_loss: 0.1132, step time: 0.1066\n",
      "154/223, train_loss: 0.1148, step time: 0.1352\n",
      "155/223, train_loss: 0.1219, step time: 0.1213\n",
      "156/223, train_loss: 0.1403, step time: 0.1009\n",
      "157/223, train_loss: 0.1181, step time: 0.1059\n",
      "158/223, train_loss: 0.1208, step time: 0.1062\n",
      "159/223, train_loss: 0.1217, step time: 0.1008\n",
      "160/223, train_loss: 0.1216, step time: 0.1083\n",
      "161/223, train_loss: 0.1205, step time: 0.1222\n",
      "162/223, train_loss: 0.1162, step time: 0.1039\n",
      "163/223, train_loss: 0.1232, step time: 0.1034\n",
      "164/223, train_loss: 0.1136, step time: 0.1025\n",
      "165/223, train_loss: 0.1256, step time: 0.1121\n",
      "166/223, train_loss: 0.1278, step time: 0.1385\n",
      "167/223, train_loss: 0.1152, step time: 0.1004\n",
      "168/223, train_loss: 0.1221, step time: 0.1110\n",
      "169/223, train_loss: 0.1092, step time: 0.1004\n",
      "170/223, train_loss: 0.1404, step time: 0.0996\n",
      "171/223, train_loss: 0.1214, step time: 0.1002\n",
      "172/223, train_loss: 0.1402, step time: 0.1064\n",
      "173/223, train_loss: 0.1410, step time: 0.0998\n",
      "174/223, train_loss: 0.1213, step time: 0.1008\n",
      "175/223, train_loss: 0.1216, step time: 0.1007\n",
      "176/223, train_loss: 0.1200, step time: 0.1240\n",
      "177/223, train_loss: 0.1167, step time: 0.1319\n",
      "178/223, train_loss: 0.1035, step time: 0.1531\n",
      "179/223, train_loss: 0.1278, step time: 0.1007\n",
      "180/223, train_loss: 0.1214, step time: 0.1012\n",
      "181/223, train_loss: 0.1218, step time: 0.1123\n",
      "182/223, train_loss: 0.1268, step time: 0.1009\n",
      "183/223, train_loss: 0.1253, step time: 0.1129\n",
      "184/223, train_loss: 0.1188, step time: 0.1006\n",
      "185/223, train_loss: 0.1234, step time: 0.1091\n",
      "186/223, train_loss: 0.1279, step time: 0.1117\n",
      "187/223, train_loss: 0.1105, step time: 0.1174\n",
      "188/223, train_loss: 0.1289, step time: 0.1093\n",
      "189/223, train_loss: 0.1248, step time: 0.1054\n",
      "190/223, train_loss: 0.1196, step time: 0.1074\n",
      "191/223, train_loss: 0.1213, step time: 0.1008\n",
      "192/223, train_loss: 0.1135, step time: 0.1022\n",
      "193/223, train_loss: 0.1393, step time: 0.1155\n",
      "194/223, train_loss: 0.1173, step time: 0.1020\n",
      "195/223, train_loss: 0.1280, step time: 0.1102\n",
      "196/223, train_loss: 0.1259, step time: 0.1003\n",
      "197/223, train_loss: 0.1092, step time: 0.1169\n",
      "198/223, train_loss: 0.1292, step time: 0.1126\n",
      "199/223, train_loss: 0.1165, step time: 0.1127\n",
      "200/223, train_loss: 0.1199, step time: 0.0992\n",
      "201/223, train_loss: 0.1217, step time: 0.1053\n",
      "202/223, train_loss: 0.1182, step time: 0.1336\n",
      "203/223, train_loss: 0.1112, step time: 0.1006\n",
      "204/223, train_loss: 0.1270, step time: 0.1005\n",
      "205/223, train_loss: 0.1177, step time: 0.1034\n",
      "206/223, train_loss: 0.1214, step time: 0.1006\n",
      "207/223, train_loss: 0.1127, step time: 0.1002\n",
      "208/223, train_loss: 0.1250, step time: 0.1400\n",
      "209/223, train_loss: 0.1234, step time: 0.1005\n",
      "210/223, train_loss: 0.1225, step time: 0.1005\n",
      "211/223, train_loss: 0.1075, step time: 0.1011\n",
      "212/223, train_loss: 0.1204, step time: 0.1036\n",
      "213/223, train_loss: 0.1215, step time: 0.1046\n",
      "214/223, train_loss: 0.1197, step time: 0.1004\n",
      "215/223, train_loss: 0.1196, step time: 0.1048\n",
      "216/223, train_loss: 0.1332, step time: 0.1148\n",
      "217/223, train_loss: 0.1134, step time: 0.1001\n",
      "218/223, train_loss: 0.1279, step time: 0.1031\n",
      "219/223, train_loss: 0.1213, step time: 0.1012\n",
      "220/223, train_loss: 0.1310, step time: 0.1009\n",
      "221/223, train_loss: 0.1214, step time: 0.1001\n",
      "222/223, train_loss: 0.1290, step time: 0.1004\n",
      "223/223, train_loss: 0.1301, step time: 0.1010\n",
      "epoch 72 average loss: 0.1218\n",
      "time consuming of epoch 72 is: 89.2193\n",
      "----------\n",
      "epoch 73/300\n",
      "1/223, train_loss: 0.1180, step time: 0.1069\n",
      "2/223, train_loss: 0.1269, step time: 0.1083\n",
      "3/223, train_loss: 0.1176, step time: 0.1099\n",
      "4/223, train_loss: 0.1224, step time: 0.1075\n",
      "5/223, train_loss: 0.1221, step time: 0.1009\n",
      "6/223, train_loss: 0.1144, step time: 0.1008\n",
      "7/223, train_loss: 0.1206, step time: 0.1073\n",
      "8/223, train_loss: 0.1254, step time: 0.1060\n",
      "9/223, train_loss: 0.1185, step time: 0.1207\n",
      "10/223, train_loss: 0.1163, step time: 0.1003\n",
      "11/223, train_loss: 0.1263, step time: 0.1126\n",
      "12/223, train_loss: 0.1123, step time: 0.1194\n",
      "13/223, train_loss: 0.1255, step time: 0.1055\n",
      "14/223, train_loss: 0.1178, step time: 0.1036\n",
      "15/223, train_loss: 0.1170, step time: 0.0997\n",
      "16/223, train_loss: 0.1208, step time: 0.1102\n",
      "17/223, train_loss: 0.1300, step time: 0.1111\n",
      "18/223, train_loss: 0.1148, step time: 0.1029\n",
      "19/223, train_loss: 0.1236, step time: 0.1010\n",
      "20/223, train_loss: 0.1159, step time: 0.1010\n",
      "21/223, train_loss: 0.1151, step time: 0.1064\n",
      "22/223, train_loss: 0.1130, step time: 0.1148\n",
      "23/223, train_loss: 0.1106, step time: 0.1037\n",
      "24/223, train_loss: 0.1120, step time: 0.1111\n",
      "25/223, train_loss: 0.1253, step time: 0.1012\n",
      "26/223, train_loss: 0.1323, step time: 0.1008\n",
      "27/223, train_loss: 0.1333, step time: 0.1072\n",
      "28/223, train_loss: 0.1194, step time: 0.1039\n",
      "29/223, train_loss: 0.1272, step time: 0.1042\n",
      "30/223, train_loss: 0.1217, step time: 0.1095\n",
      "31/223, train_loss: 0.1206, step time: 0.1054\n",
      "32/223, train_loss: 0.1187, step time: 0.1082\n",
      "33/223, train_loss: 0.1259, step time: 0.1077\n",
      "34/223, train_loss: 0.1295, step time: 0.1035\n",
      "35/223, train_loss: 0.1187, step time: 0.1205\n",
      "36/223, train_loss: 0.1216, step time: 0.1149\n",
      "37/223, train_loss: 0.1181, step time: 0.1094\n",
      "38/223, train_loss: 0.1108, step time: 0.1137\n",
      "39/223, train_loss: 0.1244, step time: 0.1034\n",
      "40/223, train_loss: 0.1184, step time: 0.1004\n",
      "41/223, train_loss: 0.1245, step time: 0.1147\n",
      "42/223, train_loss: 0.1322, step time: 0.1062\n",
      "43/223, train_loss: 0.1228, step time: 0.1152\n",
      "44/223, train_loss: 0.1279, step time: 0.1006\n",
      "45/223, train_loss: 0.1171, step time: 0.1193\n",
      "46/223, train_loss: 0.1202, step time: 0.1051\n",
      "47/223, train_loss: 0.1081, step time: 0.1002\n",
      "48/223, train_loss: 0.1275, step time: 0.1008\n",
      "49/223, train_loss: 0.1105, step time: 0.1002\n",
      "50/223, train_loss: 0.1266, step time: 0.1002\n",
      "51/223, train_loss: 0.1293, step time: 0.1123\n",
      "52/223, train_loss: 0.1350, step time: 0.1047\n",
      "53/223, train_loss: 0.1203, step time: 0.1107\n",
      "54/223, train_loss: 0.1239, step time: 0.1115\n",
      "55/223, train_loss: 0.1126, step time: 0.1069\n",
      "56/223, train_loss: 0.1164, step time: 0.1011\n",
      "57/223, train_loss: 0.1175, step time: 0.1133\n",
      "58/223, train_loss: 0.1370, step time: 0.1027\n",
      "59/223, train_loss: 0.1361, step time: 0.0997\n",
      "60/223, train_loss: 0.1136, step time: 0.1059\n",
      "61/223, train_loss: 0.1217, step time: 0.1042\n",
      "62/223, train_loss: 0.1160, step time: 0.1001\n",
      "63/223, train_loss: 0.1244, step time: 0.1000\n",
      "64/223, train_loss: 0.1308, step time: 0.1006\n",
      "65/223, train_loss: 0.1102, step time: 0.1316\n",
      "66/223, train_loss: 0.1390, step time: 0.1233\n",
      "67/223, train_loss: 0.1412, step time: 0.1112\n",
      "68/223, train_loss: 0.1112, step time: 0.1182\n",
      "69/223, train_loss: 0.1286, step time: 0.1315\n",
      "70/223, train_loss: 0.1288, step time: 0.1004\n",
      "71/223, train_loss: 0.1371, step time: 0.1021\n",
      "72/223, train_loss: 0.1413, step time: 0.1008\n",
      "73/223, train_loss: 0.1134, step time: 0.1027\n",
      "74/223, train_loss: 0.1278, step time: 0.1204\n",
      "75/223, train_loss: 0.1092, step time: 0.1294\n",
      "76/223, train_loss: 0.1312, step time: 0.1081\n",
      "77/223, train_loss: 0.1229, step time: 0.1420\n",
      "78/223, train_loss: 0.1333, step time: 0.1556\n",
      "79/223, train_loss: 0.1215, step time: 0.1123\n",
      "80/223, train_loss: 0.1237, step time: 0.1005\n",
      "81/223, train_loss: 0.1280, step time: 0.1056\n",
      "82/223, train_loss: 0.1138, step time: 0.1221\n",
      "83/223, train_loss: 0.1209, step time: 0.1390\n",
      "84/223, train_loss: 0.1201, step time: 0.1181\n",
      "85/223, train_loss: 0.1175, step time: 0.1063\n",
      "86/223, train_loss: 0.1253, step time: 0.1056\n",
      "87/223, train_loss: 0.1087, step time: 0.1332\n",
      "88/223, train_loss: 0.1264, step time: 0.1202\n",
      "89/223, train_loss: 0.1355, step time: 0.1198\n",
      "90/223, train_loss: 0.1170, step time: 0.1123\n",
      "91/223, train_loss: 0.1241, step time: 0.1227\n",
      "92/223, train_loss: 0.1124, step time: 0.1278\n",
      "93/223, train_loss: 0.1266, step time: 0.1154\n",
      "94/223, train_loss: 0.1111, step time: 0.1063\n",
      "95/223, train_loss: 0.1214, step time: 0.1117\n",
      "96/223, train_loss: 0.1192, step time: 0.1060\n",
      "97/223, train_loss: 0.1218, step time: 0.1209\n",
      "98/223, train_loss: 0.1204, step time: 0.1245\n",
      "99/223, train_loss: 0.1434, step time: 0.1026\n",
      "100/223, train_loss: 0.1242, step time: 0.1078\n",
      "101/223, train_loss: 0.1086, step time: 0.1005\n",
      "102/223, train_loss: 0.1241, step time: 0.1009\n",
      "103/223, train_loss: 0.1117, step time: 0.1060\n",
      "104/223, train_loss: 0.1290, step time: 0.1057\n",
      "105/223, train_loss: 0.1053, step time: 0.1264\n",
      "106/223, train_loss: 0.1097, step time: 0.1217\n",
      "107/223, train_loss: 0.1114, step time: 0.1143\n",
      "108/223, train_loss: 0.1168, step time: 0.1010\n",
      "109/223, train_loss: 0.1386, step time: 0.1060\n",
      "110/223, train_loss: 0.1139, step time: 0.1152\n",
      "111/223, train_loss: 0.1186, step time: 0.1406\n",
      "112/223, train_loss: 0.1440, step time: 0.1065\n",
      "113/223, train_loss: 0.1242, step time: 0.1147\n",
      "114/223, train_loss: 0.1248, step time: 0.1033\n",
      "115/223, train_loss: 0.1325, step time: 0.1072\n",
      "116/223, train_loss: 0.1157, step time: 0.1101\n",
      "117/223, train_loss: 0.1226, step time: 0.1004\n",
      "118/223, train_loss: 0.1410, step time: 0.1018\n",
      "119/223, train_loss: 0.1338, step time: 0.1003\n",
      "120/223, train_loss: 0.1113, step time: 0.0997\n",
      "121/223, train_loss: 0.1280, step time: 0.1049\n",
      "122/223, train_loss: 0.1301, step time: 0.1000\n",
      "123/223, train_loss: 0.1137, step time: 0.0999\n",
      "124/223, train_loss: 0.1314, step time: 0.1183\n",
      "125/223, train_loss: 0.1097, step time: 0.1092\n",
      "126/223, train_loss: 0.1124, step time: 0.1067\n",
      "127/223, train_loss: 0.1278, step time: 0.1077\n",
      "128/223, train_loss: 0.1282, step time: 0.1100\n",
      "129/223, train_loss: 0.1177, step time: 0.1077\n",
      "130/223, train_loss: 0.1116, step time: 0.1004\n",
      "131/223, train_loss: 0.1173, step time: 0.1063\n",
      "132/223, train_loss: 0.1185, step time: 0.1011\n",
      "133/223, train_loss: 0.1217, step time: 0.1139\n",
      "134/223, train_loss: 0.1104, step time: 0.1480\n",
      "135/223, train_loss: 0.1114, step time: 0.1005\n",
      "136/223, train_loss: 0.1349, step time: 0.1233\n",
      "137/223, train_loss: 0.1204, step time: 0.1081\n",
      "138/223, train_loss: 0.1119, step time: 0.1090\n",
      "139/223, train_loss: 0.1160, step time: 0.1123\n",
      "140/223, train_loss: 0.1443, step time: 0.1128\n",
      "141/223, train_loss: 0.1333, step time: 0.1051\n",
      "142/223, train_loss: 0.1106, step time: 0.1106\n",
      "143/223, train_loss: 0.1215, step time: 0.1146\n",
      "144/223, train_loss: 0.1086, step time: 0.1059\n",
      "145/223, train_loss: 0.1124, step time: 0.1467\n",
      "146/223, train_loss: 0.1194, step time: 0.1118\n",
      "147/223, train_loss: 0.1221, step time: 0.1148\n",
      "148/223, train_loss: 0.1191, step time: 0.1123\n",
      "149/223, train_loss: 0.1210, step time: 0.1021\n",
      "150/223, train_loss: 0.1111, step time: 0.1050\n",
      "151/223, train_loss: 0.1167, step time: 0.1211\n",
      "152/223, train_loss: 0.1137, step time: 0.1067\n",
      "153/223, train_loss: 0.1154, step time: 0.1010\n",
      "154/223, train_loss: 0.1108, step time: 0.1022\n",
      "155/223, train_loss: 0.1154, step time: 0.1127\n",
      "156/223, train_loss: 0.1114, step time: 0.1001\n",
      "157/223, train_loss: 0.1182, step time: 0.1071\n",
      "158/223, train_loss: 0.1285, step time: 0.1316\n",
      "159/223, train_loss: 0.1130, step time: 0.1000\n",
      "160/223, train_loss: 0.1226, step time: 0.1000\n",
      "161/223, train_loss: 0.1159, step time: 0.1159\n",
      "162/223, train_loss: 0.1238, step time: 0.1195\n",
      "163/223, train_loss: 0.1201, step time: 0.1360\n",
      "164/223, train_loss: 0.1130, step time: 0.1018\n",
      "165/223, train_loss: 0.1210, step time: 0.1080\n",
      "166/223, train_loss: 0.1249, step time: 0.1196\n",
      "167/223, train_loss: 0.1278, step time: 0.1311\n",
      "168/223, train_loss: 0.1201, step time: 0.1250\n",
      "169/223, train_loss: 0.1362, step time: 0.1209\n",
      "170/223, train_loss: 0.1158, step time: 0.1113\n",
      "171/223, train_loss: 0.1243, step time: 0.1298\n",
      "172/223, train_loss: 0.1257, step time: 0.0999\n",
      "173/223, train_loss: 0.1330, step time: 0.1107\n",
      "174/223, train_loss: 0.1244, step time: 0.1017\n",
      "175/223, train_loss: 0.1221, step time: 0.1193\n",
      "176/223, train_loss: 0.1202, step time: 0.1008\n",
      "177/223, train_loss: 0.1142, step time: 0.1004\n",
      "178/223, train_loss: 0.1208, step time: 0.1010\n",
      "179/223, train_loss: 0.1179, step time: 0.1015\n",
      "180/223, train_loss: 0.1037, step time: 0.1328\n",
      "181/223, train_loss: 0.1179, step time: 0.1004\n",
      "182/223, train_loss: 0.1188, step time: 0.1000\n",
      "183/223, train_loss: 0.1170, step time: 0.1018\n",
      "184/223, train_loss: 0.1081, step time: 0.1007\n",
      "185/223, train_loss: 0.1175, step time: 0.1200\n",
      "186/223, train_loss: 0.1245, step time: 0.1085\n",
      "187/223, train_loss: 0.1111, step time: 0.1233\n",
      "188/223, train_loss: 0.1162, step time: 0.1091\n",
      "189/223, train_loss: 0.1062, step time: 0.1208\n",
      "190/223, train_loss: 0.1248, step time: 0.1121\n",
      "191/223, train_loss: 0.1068, step time: 0.1006\n",
      "192/223, train_loss: 0.1174, step time: 0.0995\n",
      "193/223, train_loss: 0.1271, step time: 0.1223\n",
      "194/223, train_loss: 0.1361, step time: 0.1136\n",
      "195/223, train_loss: 0.1190, step time: 0.0993\n",
      "196/223, train_loss: 0.1243, step time: 0.1122\n",
      "197/223, train_loss: 0.1223, step time: 0.1071\n",
      "198/223, train_loss: 0.1294, step time: 0.1005\n",
      "199/223, train_loss: 0.1052, step time: 0.1002\n",
      "200/223, train_loss: 0.3198, step time: 0.0999\n",
      "201/223, train_loss: 0.1413, step time: 0.1067\n",
      "202/223, train_loss: 0.1117, step time: 0.1189\n",
      "203/223, train_loss: 0.1349, step time: 0.1144\n",
      "204/223, train_loss: 0.1382, step time: 0.0998\n",
      "205/223, train_loss: 0.1145, step time: 0.1054\n",
      "206/223, train_loss: 0.1288, step time: 0.1150\n",
      "207/223, train_loss: 0.1153, step time: 0.1266\n",
      "208/223, train_loss: 0.1116, step time: 0.1234\n",
      "209/223, train_loss: 0.1255, step time: 0.1017\n",
      "210/223, train_loss: 0.1203, step time: 0.1003\n",
      "211/223, train_loss: 0.1118, step time: 0.1008\n",
      "212/223, train_loss: 0.1244, step time: 0.1057\n",
      "213/223, train_loss: 0.1292, step time: 0.1004\n",
      "214/223, train_loss: 0.1284, step time: 0.0995\n",
      "215/223, train_loss: 0.1195, step time: 0.1169\n",
      "216/223, train_loss: 0.1250, step time: 0.1030\n",
      "217/223, train_loss: 0.1260, step time: 0.1093\n",
      "218/223, train_loss: 0.1261, step time: 0.0996\n",
      "219/223, train_loss: 0.1160, step time: 0.1006\n",
      "220/223, train_loss: 0.1374, step time: 0.1004\n",
      "221/223, train_loss: 0.1182, step time: 0.1001\n",
      "222/223, train_loss: 0.1309, step time: 0.1006\n",
      "223/223, train_loss: 0.1137, step time: 0.1000\n",
      "epoch 73 average loss: 0.1223\n",
      "time consuming of epoch 73 is: 89.2098\n",
      "----------\n",
      "epoch 74/300\n",
      "1/223, train_loss: 0.1141, step time: 0.1016\n",
      "2/223, train_loss: 0.1066, step time: 0.1007\n",
      "3/223, train_loss: 0.1236, step time: 0.1009\n",
      "4/223, train_loss: 0.1245, step time: 0.1230\n",
      "5/223, train_loss: 0.1186, step time: 0.1235\n",
      "6/223, train_loss: 0.1258, step time: 0.1004\n",
      "7/223, train_loss: 0.1209, step time: 0.1092\n",
      "8/223, train_loss: 0.1117, step time: 0.1002\n",
      "9/223, train_loss: 0.1170, step time: 0.1173\n",
      "10/223, train_loss: 0.1171, step time: 0.1134\n",
      "11/223, train_loss: 0.1130, step time: 0.1218\n",
      "12/223, train_loss: 0.1264, step time: 0.1141\n",
      "13/223, train_loss: 0.1275, step time: 0.1096\n",
      "14/223, train_loss: 0.1282, step time: 0.1006\n",
      "15/223, train_loss: 0.1333, step time: 0.0998\n",
      "16/223, train_loss: 0.1277, step time: 0.1075\n",
      "17/223, train_loss: 0.1212, step time: 0.1178\n",
      "18/223, train_loss: 0.1088, step time: 0.0998\n",
      "19/223, train_loss: 0.1198, step time: 0.1016\n",
      "20/223, train_loss: 0.1248, step time: 0.1080\n",
      "21/223, train_loss: 0.1204, step time: 0.1065\n",
      "22/223, train_loss: 0.1076, step time: 0.1026\n",
      "23/223, train_loss: 0.1302, step time: 0.1254\n",
      "24/223, train_loss: 0.1062, step time: 0.1151\n",
      "25/223, train_loss: 0.1224, step time: 0.1288\n",
      "26/223, train_loss: 0.1227, step time: 0.1096\n",
      "27/223, train_loss: 0.1258, step time: 0.1061\n",
      "28/223, train_loss: 0.1041, step time: 0.0999\n",
      "29/223, train_loss: 0.1178, step time: 0.1062\n",
      "30/223, train_loss: 0.1178, step time: 0.1273\n",
      "31/223, train_loss: 0.1162, step time: 0.1163\n",
      "32/223, train_loss: 0.1291, step time: 0.1007\n",
      "33/223, train_loss: 0.1100, step time: 0.1210\n",
      "34/223, train_loss: 0.1306, step time: 0.1008\n",
      "35/223, train_loss: 0.1328, step time: 0.1016\n",
      "36/223, train_loss: 0.1148, step time: 0.1150\n",
      "37/223, train_loss: 0.1169, step time: 0.1164\n",
      "38/223, train_loss: 0.1154, step time: 0.1054\n",
      "39/223, train_loss: 0.1366, step time: 0.1030\n",
      "40/223, train_loss: 0.1210, step time: 0.1005\n",
      "41/223, train_loss: 0.1250, step time: 0.0998\n",
      "42/223, train_loss: 0.1154, step time: 0.1077\n",
      "43/223, train_loss: 0.1174, step time: 0.1000\n",
      "44/223, train_loss: 0.1243, step time: 0.1137\n",
      "45/223, train_loss: 0.1099, step time: 0.1029\n",
      "46/223, train_loss: 0.1046, step time: 0.1115\n",
      "47/223, train_loss: 0.1021, step time: 0.1062\n",
      "48/223, train_loss: 0.1140, step time: 0.1003\n",
      "49/223, train_loss: 0.1292, step time: 0.1064\n",
      "50/223, train_loss: 0.1157, step time: 0.1008\n",
      "51/223, train_loss: 0.1049, step time: 0.1085\n",
      "52/223, train_loss: 0.1196, step time: 0.1064\n",
      "53/223, train_loss: 0.1079, step time: 0.1183\n",
      "54/223, train_loss: 0.1311, step time: 0.1088\n",
      "55/223, train_loss: 0.1171, step time: 0.1005\n",
      "56/223, train_loss: 0.1217, step time: 0.1066\n",
      "57/223, train_loss: 0.1193, step time: 0.1228\n",
      "58/223, train_loss: 0.1173, step time: 0.1261\n",
      "59/223, train_loss: 0.1094, step time: 0.1464\n",
      "60/223, train_loss: 0.1112, step time: 0.1033\n",
      "61/223, train_loss: 0.1291, step time: 0.1091\n",
      "62/223, train_loss: 0.1292, step time: 0.1084\n",
      "63/223, train_loss: 0.1139, step time: 0.1068\n",
      "64/223, train_loss: 0.1272, step time: 0.1079\n",
      "65/223, train_loss: 0.1303, step time: 0.1056\n",
      "66/223, train_loss: 0.1326, step time: 0.0998\n",
      "67/223, train_loss: 0.1501, step time: 0.1104\n",
      "68/223, train_loss: 0.1316, step time: 0.1029\n",
      "69/223, train_loss: 0.1326, step time: 0.1064\n",
      "70/223, train_loss: 0.1266, step time: 0.1000\n",
      "71/223, train_loss: 0.1175, step time: 0.1020\n",
      "72/223, train_loss: 0.1269, step time: 0.1111\n",
      "73/223, train_loss: 0.1279, step time: 0.1074\n",
      "74/223, train_loss: 0.1263, step time: 0.1001\n",
      "75/223, train_loss: 0.1191, step time: 0.1163\n",
      "76/223, train_loss: 0.1181, step time: 0.0999\n",
      "77/223, train_loss: 0.1329, step time: 0.1063\n",
      "78/223, train_loss: 0.1247, step time: 0.1000\n",
      "79/223, train_loss: 0.1298, step time: 0.1007\n",
      "80/223, train_loss: 0.1282, step time: 0.1066\n",
      "81/223, train_loss: 0.1088, step time: 0.1072\n",
      "82/223, train_loss: 0.1344, step time: 0.1279\n",
      "83/223, train_loss: 0.1201, step time: 0.1062\n",
      "84/223, train_loss: 0.1295, step time: 0.1005\n",
      "85/223, train_loss: 0.1326, step time: 0.1279\n",
      "86/223, train_loss: 0.1321, step time: 0.1072\n",
      "87/223, train_loss: 0.1165, step time: 0.1154\n",
      "88/223, train_loss: 0.1173, step time: 0.1009\n",
      "89/223, train_loss: 0.1198, step time: 0.1687\n",
      "90/223, train_loss: 0.1222, step time: 0.1352\n",
      "91/223, train_loss: 0.1274, step time: 0.1000\n",
      "92/223, train_loss: 0.1202, step time: 0.0998\n",
      "93/223, train_loss: 0.1310, step time: 0.1119\n",
      "94/223, train_loss: 0.1088, step time: 0.1106\n",
      "95/223, train_loss: 0.1184, step time: 0.1008\n",
      "96/223, train_loss: 0.1161, step time: 0.0993\n",
      "97/223, train_loss: 0.1187, step time: 0.1371\n",
      "98/223, train_loss: 0.1200, step time: 0.1204\n",
      "99/223, train_loss: 0.1211, step time: 0.1030\n",
      "100/223, train_loss: 0.1347, step time: 0.1112\n",
      "101/223, train_loss: 0.1169, step time: 0.1040\n",
      "102/223, train_loss: 0.1184, step time: 0.1092\n",
      "103/223, train_loss: 0.1165, step time: 0.0987\n",
      "104/223, train_loss: 0.1303, step time: 0.0986\n",
      "105/223, train_loss: 0.1314, step time: 0.1376\n",
      "106/223, train_loss: 0.1145, step time: 0.1127\n",
      "107/223, train_loss: 0.1303, step time: 0.0991\n",
      "108/223, train_loss: 0.1183, step time: 0.1008\n",
      "109/223, train_loss: 0.1129, step time: 0.0992\n",
      "110/223, train_loss: 0.1070, step time: 0.0994\n",
      "111/223, train_loss: 0.1175, step time: 0.0992\n",
      "112/223, train_loss: 0.1130, step time: 0.0995\n",
      "113/223, train_loss: 0.1292, step time: 0.1345\n",
      "114/223, train_loss: 0.1184, step time: 0.0998\n",
      "115/223, train_loss: 0.1271, step time: 0.0992\n",
      "116/223, train_loss: 0.1110, step time: 0.0994\n",
      "117/223, train_loss: 0.1088, step time: 0.1153\n",
      "118/223, train_loss: 0.1134, step time: 0.1004\n",
      "119/223, train_loss: 0.1171, step time: 0.1009\n",
      "120/223, train_loss: 0.1218, step time: 0.1147\n",
      "121/223, train_loss: 0.1204, step time: 0.1246\n",
      "122/223, train_loss: 0.1288, step time: 0.1068\n",
      "123/223, train_loss: 0.1184, step time: 0.1006\n",
      "124/223, train_loss: 0.1150, step time: 0.1007\n",
      "125/223, train_loss: 0.1256, step time: 0.1281\n",
      "126/223, train_loss: 0.1168, step time: 0.1221\n",
      "127/223, train_loss: 0.1204, step time: 0.1099\n",
      "128/223, train_loss: 0.1208, step time: 0.1114\n",
      "129/223, train_loss: 0.1268, step time: 0.1121\n",
      "130/223, train_loss: 0.1108, step time: 0.1077\n",
      "131/223, train_loss: 0.1163, step time: 0.1433\n",
      "132/223, train_loss: 0.1307, step time: 0.0992\n",
      "133/223, train_loss: 0.1220, step time: 0.1063\n",
      "134/223, train_loss: 0.1285, step time: 0.1017\n",
      "135/223, train_loss: 0.1222, step time: 0.1035\n",
      "136/223, train_loss: 0.1214, step time: 0.1247\n",
      "137/223, train_loss: 0.1069, step time: 0.0998\n",
      "138/223, train_loss: 0.1313, step time: 0.1203\n",
      "139/223, train_loss: 0.1176, step time: 0.1096\n",
      "140/223, train_loss: 0.1150, step time: 0.1097\n",
      "141/223, train_loss: 0.1115, step time: 0.1137\n",
      "142/223, train_loss: 0.1214, step time: 0.1044\n",
      "143/223, train_loss: 0.1242, step time: 0.1023\n",
      "144/223, train_loss: 0.1041, step time: 0.1198\n",
      "145/223, train_loss: 0.1252, step time: 0.1180\n",
      "146/223, train_loss: 0.1224, step time: 0.1158\n",
      "147/223, train_loss: 0.1133, step time: 0.1307\n",
      "148/223, train_loss: 0.1245, step time: 0.1120\n",
      "149/223, train_loss: 0.1153, step time: 0.1101\n",
      "150/223, train_loss: 0.1129, step time: 0.1173\n",
      "151/223, train_loss: 0.1076, step time: 0.1135\n",
      "152/223, train_loss: 0.1158, step time: 0.1236\n",
      "153/223, train_loss: 0.1243, step time: 0.1214\n",
      "154/223, train_loss: 0.1087, step time: 0.1114\n",
      "155/223, train_loss: 0.1103, step time: 0.1011\n",
      "156/223, train_loss: 0.1197, step time: 0.1112\n",
      "157/223, train_loss: 0.1138, step time: 0.1109\n",
      "158/223, train_loss: 0.1086, step time: 0.1179\n",
      "159/223, train_loss: 0.1218, step time: 0.1083\n",
      "160/223, train_loss: 0.1210, step time: 0.1000\n",
      "161/223, train_loss: 0.1314, step time: 0.1307\n",
      "162/223, train_loss: 0.1155, step time: 0.1174\n",
      "163/223, train_loss: 0.1143, step time: 0.1032\n",
      "164/223, train_loss: 0.1320, step time: 0.0999\n",
      "165/223, train_loss: 0.1228, step time: 0.1177\n",
      "166/223, train_loss: 0.1163, step time: 0.1008\n",
      "167/223, train_loss: 0.1286, step time: 0.1258\n",
      "168/223, train_loss: 0.1024, step time: 0.1012\n",
      "169/223, train_loss: 0.1112, step time: 0.1133\n",
      "170/223, train_loss: 0.1315, step time: 0.1011\n",
      "171/223, train_loss: 0.1263, step time: 0.1015\n",
      "172/223, train_loss: 0.1142, step time: 0.1025\n",
      "173/223, train_loss: 0.1245, step time: 0.1287\n",
      "174/223, train_loss: 0.1135, step time: 0.1143\n",
      "175/223, train_loss: 0.1145, step time: 0.1059\n",
      "176/223, train_loss: 0.1264, step time: 0.1004\n",
      "177/223, train_loss: 0.1252, step time: 0.1014\n",
      "178/223, train_loss: 0.1328, step time: 0.1002\n",
      "179/223, train_loss: 0.1312, step time: 0.1033\n",
      "180/223, train_loss: 0.1340, step time: 0.1076\n",
      "181/223, train_loss: 0.1180, step time: 0.1024\n",
      "182/223, train_loss: 0.1225, step time: 0.1003\n",
      "183/223, train_loss: 0.1201, step time: 0.1007\n",
      "184/223, train_loss: 0.1222, step time: 0.1074\n",
      "185/223, train_loss: 0.1451, step time: 0.1060\n",
      "186/223, train_loss: 0.1198, step time: 0.1054\n",
      "187/223, train_loss: 0.1360, step time: 0.1062\n",
      "188/223, train_loss: 0.1234, step time: 0.1097\n",
      "189/223, train_loss: 0.1249, step time: 0.1134\n",
      "190/223, train_loss: 0.1166, step time: 0.1109\n",
      "191/223, train_loss: 0.1181, step time: 0.1131\n",
      "192/223, train_loss: 0.1350, step time: 0.1011\n",
      "193/223, train_loss: 0.1335, step time: 0.1011\n",
      "194/223, train_loss: 0.1253, step time: 0.1002\n",
      "195/223, train_loss: 0.1178, step time: 0.0992\n",
      "196/223, train_loss: 0.1381, step time: 0.1113\n",
      "197/223, train_loss: 0.1204, step time: 0.1075\n",
      "198/223, train_loss: 0.1063, step time: 0.1080\n",
      "199/223, train_loss: 0.1226, step time: 0.1042\n",
      "200/223, train_loss: 0.1053, step time: 0.1014\n",
      "201/223, train_loss: 0.1105, step time: 0.1322\n",
      "202/223, train_loss: 0.3133, step time: 0.1238\n",
      "203/223, train_loss: 0.1156, step time: 0.1013\n",
      "204/223, train_loss: 0.1108, step time: 0.1163\n",
      "205/223, train_loss: 0.1152, step time: 0.1085\n",
      "206/223, train_loss: 0.1249, step time: 0.1105\n",
      "207/223, train_loss: 0.1359, step time: 0.0998\n",
      "208/223, train_loss: 0.1233, step time: 0.0986\n",
      "209/223, train_loss: 0.1076, step time: 0.1213\n",
      "210/223, train_loss: 0.1174, step time: 0.1115\n",
      "211/223, train_loss: 0.1210, step time: 0.1143\n",
      "212/223, train_loss: 0.1165, step time: 0.1205\n",
      "213/223, train_loss: 0.1234, step time: 0.1030\n",
      "214/223, train_loss: 0.1246, step time: 0.1002\n",
      "215/223, train_loss: 0.1151, step time: 0.1003\n",
      "216/223, train_loss: 0.1195, step time: 0.1399\n",
      "217/223, train_loss: 0.1275, step time: 0.0998\n",
      "218/223, train_loss: 0.1162, step time: 0.0996\n",
      "219/223, train_loss: 0.1227, step time: 0.1007\n",
      "220/223, train_loss: 0.1122, step time: 0.1051\n",
      "221/223, train_loss: 0.1163, step time: 0.0992\n",
      "222/223, train_loss: 0.1123, step time: 0.0993\n",
      "223/223, train_loss: 0.1300, step time: 0.0993\n",
      "epoch 74 average loss: 0.1215\n",
      "time consuming of epoch 74 is: 87.1160\n",
      "----------\n",
      "epoch 75/300\n",
      "1/223, train_loss: 0.1061, step time: 0.1113\n",
      "2/223, train_loss: 0.1240, step time: 0.1104\n",
      "3/223, train_loss: 0.1199, step time: 0.1021\n",
      "4/223, train_loss: 0.3129, step time: 0.1015\n",
      "5/223, train_loss: 0.1202, step time: 0.1489\n",
      "6/223, train_loss: 0.1119, step time: 0.1143\n",
      "7/223, train_loss: 0.1184, step time: 0.1001\n",
      "8/223, train_loss: 0.1417, step time: 0.0999\n",
      "9/223, train_loss: 0.1174, step time: 0.1151\n",
      "10/223, train_loss: 0.1202, step time: 0.1128\n",
      "11/223, train_loss: 0.1035, step time: 0.1009\n",
      "12/223, train_loss: 0.1121, step time: 0.1002\n",
      "13/223, train_loss: 0.1329, step time: 0.1003\n",
      "14/223, train_loss: 0.1220, step time: 0.1352\n",
      "15/223, train_loss: 0.1079, step time: 0.1187\n",
      "16/223, train_loss: 0.1206, step time: 0.1279\n",
      "17/223, train_loss: 0.1258, step time: 0.1092\n",
      "18/223, train_loss: 0.1231, step time: 0.1175\n",
      "19/223, train_loss: 0.1155, step time: 0.1182\n",
      "20/223, train_loss: 0.1382, step time: 0.1134\n",
      "21/223, train_loss: 0.1164, step time: 0.1055\n",
      "22/223, train_loss: 0.1216, step time: 0.1079\n",
      "23/223, train_loss: 0.1215, step time: 0.1014\n",
      "24/223, train_loss: 0.1218, step time: 0.1092\n",
      "25/223, train_loss: 0.1052, step time: 0.1094\n",
      "26/223, train_loss: 0.1211, step time: 0.1150\n",
      "27/223, train_loss: 0.1087, step time: 0.1209\n",
      "28/223, train_loss: 0.1170, step time: 0.1078\n",
      "29/223, train_loss: 0.1173, step time: 0.1100\n",
      "30/223, train_loss: 0.1337, step time: 0.1000\n",
      "31/223, train_loss: 0.1112, step time: 0.0997\n",
      "32/223, train_loss: 0.1083, step time: 0.1013\n",
      "33/223, train_loss: 0.1314, step time: 0.1083\n",
      "34/223, train_loss: 0.1133, step time: 0.1064\n",
      "35/223, train_loss: 0.1228, step time: 0.1000\n",
      "36/223, train_loss: 0.1182, step time: 0.1015\n",
      "37/223, train_loss: 0.1124, step time: 0.1118\n",
      "38/223, train_loss: 0.1127, step time: 0.1004\n",
      "39/223, train_loss: 0.1064, step time: 0.1002\n",
      "40/223, train_loss: 0.1295, step time: 0.1002\n",
      "41/223, train_loss: 0.1318, step time: 0.1236\n",
      "42/223, train_loss: 0.1226, step time: 0.1014\n",
      "43/223, train_loss: 0.1126, step time: 0.0993\n",
      "44/223, train_loss: 0.1178, step time: 0.1001\n",
      "45/223, train_loss: 0.1174, step time: 0.0999\n",
      "46/223, train_loss: 0.1135, step time: 0.1079\n",
      "47/223, train_loss: 0.1194, step time: 0.0999\n",
      "48/223, train_loss: 0.1303, step time: 0.1010\n",
      "49/223, train_loss: 0.1168, step time: 0.1026\n",
      "50/223, train_loss: 0.1250, step time: 0.1011\n",
      "51/223, train_loss: 0.1210, step time: 0.1009\n",
      "52/223, train_loss: 0.1155, step time: 0.1067\n",
      "53/223, train_loss: 0.1177, step time: 0.1001\n",
      "54/223, train_loss: 0.1179, step time: 0.1010\n",
      "55/223, train_loss: 0.1278, step time: 0.1014\n",
      "56/223, train_loss: 0.1259, step time: 0.1192\n",
      "57/223, train_loss: 0.1381, step time: 0.1103\n",
      "58/223, train_loss: 0.1223, step time: 0.1002\n",
      "59/223, train_loss: 0.1182, step time: 0.1154\n",
      "60/223, train_loss: 0.1199, step time: 0.1190\n",
      "61/223, train_loss: 0.1094, step time: 0.1005\n",
      "62/223, train_loss: 0.1255, step time: 0.1007\n",
      "63/223, train_loss: 0.1117, step time: 0.0998\n",
      "64/223, train_loss: 0.1225, step time: 0.1010\n",
      "65/223, train_loss: 0.1399, step time: 0.1149\n",
      "66/223, train_loss: 0.1146, step time: 0.1105\n",
      "67/223, train_loss: 0.1255, step time: 0.1233\n",
      "68/223, train_loss: 0.1098, step time: 0.1162\n",
      "69/223, train_loss: 0.1163, step time: 0.1026\n",
      "70/223, train_loss: 0.1217, step time: 0.1185\n",
      "71/223, train_loss: 0.1247, step time: 0.1054\n",
      "72/223, train_loss: 0.1184, step time: 0.1131\n",
      "73/223, train_loss: 0.1371, step time: 0.1055\n",
      "74/223, train_loss: 0.1203, step time: 0.1088\n",
      "75/223, train_loss: 0.1318, step time: 0.0998\n",
      "76/223, train_loss: 0.1352, step time: 0.1188\n",
      "77/223, train_loss: 0.1125, step time: 0.1286\n",
      "78/223, train_loss: 0.1120, step time: 0.1140\n",
      "79/223, train_loss: 0.1090, step time: 0.1050\n",
      "80/223, train_loss: 0.1133, step time: 0.1095\n",
      "81/223, train_loss: 0.1175, step time: 0.1087\n",
      "82/223, train_loss: 0.1258, step time: 0.1122\n",
      "83/223, train_loss: 0.1233, step time: 0.1116\n",
      "84/223, train_loss: 0.1089, step time: 0.1000\n",
      "85/223, train_loss: 0.1187, step time: 0.1160\n",
      "86/223, train_loss: 0.1242, step time: 0.1011\n",
      "87/223, train_loss: 0.1082, step time: 0.1005\n",
      "88/223, train_loss: 0.1121, step time: 0.1149\n",
      "89/223, train_loss: 0.1045, step time: 0.1154\n",
      "90/223, train_loss: 0.1203, step time: 0.1024\n",
      "91/223, train_loss: 0.1298, step time: 0.1008\n",
      "92/223, train_loss: 0.1207, step time: 0.1506\n",
      "93/223, train_loss: 0.1169, step time: 0.1022\n",
      "94/223, train_loss: 0.1241, step time: 0.1359\n",
      "95/223, train_loss: 0.1385, step time: 0.1007\n",
      "96/223, train_loss: 0.1099, step time: 0.1005\n",
      "97/223, train_loss: 0.1195, step time: 0.1188\n",
      "98/223, train_loss: 0.1418, step time: 0.0999\n",
      "99/223, train_loss: 0.1129, step time: 0.1031\n",
      "100/223, train_loss: 0.1263, step time: 0.1099\n",
      "101/223, train_loss: 0.1128, step time: 0.1056\n",
      "102/223, train_loss: 0.1249, step time: 0.1000\n",
      "103/223, train_loss: 0.1093, step time: 0.1008\n",
      "104/223, train_loss: 0.1191, step time: 0.1120\n",
      "105/223, train_loss: 0.1089, step time: 0.1049\n",
      "106/223, train_loss: 0.1054, step time: 0.1000\n",
      "107/223, train_loss: 0.1120, step time: 0.1000\n",
      "108/223, train_loss: 0.1115, step time: 0.1222\n",
      "109/223, train_loss: 0.1216, step time: 0.1178\n",
      "110/223, train_loss: 0.1331, step time: 0.1063\n",
      "111/223, train_loss: 0.1301, step time: 0.1002\n",
      "112/223, train_loss: 0.1333, step time: 0.1004\n",
      "113/223, train_loss: 0.1365, step time: 0.1323\n",
      "114/223, train_loss: 0.1220, step time: 0.1223\n",
      "115/223, train_loss: 0.1212, step time: 0.1001\n",
      "116/223, train_loss: 0.1078, step time: 0.1006\n",
      "117/223, train_loss: 0.1252, step time: 0.1118\n",
      "118/223, train_loss: 0.1121, step time: 0.1056\n",
      "119/223, train_loss: 0.1122, step time: 0.1100\n",
      "120/223, train_loss: 0.1210, step time: 0.1014\n",
      "121/223, train_loss: 0.1117, step time: 0.1154\n",
      "122/223, train_loss: 0.1182, step time: 0.1044\n",
      "123/223, train_loss: 0.1064, step time: 0.1000\n",
      "124/223, train_loss: 0.1061, step time: 0.1072\n",
      "125/223, train_loss: 0.1225, step time: 0.1148\n",
      "126/223, train_loss: 0.1059, step time: 0.1264\n",
      "127/223, train_loss: 0.1040, step time: 0.1012\n",
      "128/223, train_loss: 0.1222, step time: 0.1007\n",
      "129/223, train_loss: 0.1272, step time: 0.1154\n",
      "130/223, train_loss: 0.1205, step time: 0.1009\n",
      "131/223, train_loss: 0.1183, step time: 0.1306\n",
      "132/223, train_loss: 0.1187, step time: 0.1013\n",
      "133/223, train_loss: 0.1265, step time: 0.1004\n",
      "134/223, train_loss: 0.1232, step time: 0.1030\n",
      "135/223, train_loss: 0.1282, step time: 0.1126\n",
      "136/223, train_loss: 0.1197, step time: 0.1016\n",
      "137/223, train_loss: 0.1158, step time: 0.1149\n",
      "138/223, train_loss: 0.1131, step time: 0.1283\n",
      "139/223, train_loss: 0.1319, step time: 0.1048\n",
      "140/223, train_loss: 0.1127, step time: 0.1097\n",
      "141/223, train_loss: 0.1289, step time: 0.1161\n",
      "142/223, train_loss: 0.1226, step time: 0.1024\n",
      "143/223, train_loss: 0.1274, step time: 0.1282\n",
      "144/223, train_loss: 0.1226, step time: 0.1147\n",
      "145/223, train_loss: 0.1209, step time: 0.1062\n",
      "146/223, train_loss: 0.1104, step time: 0.1248\n",
      "147/223, train_loss: 0.1143, step time: 0.1205\n",
      "148/223, train_loss: 0.1200, step time: 0.1142\n",
      "149/223, train_loss: 0.1191, step time: 0.1054\n",
      "150/223, train_loss: 0.1217, step time: 0.1006\n",
      "151/223, train_loss: 0.1211, step time: 0.1006\n",
      "152/223, train_loss: 0.1156, step time: 0.1067\n",
      "153/223, train_loss: 0.1075, step time: 0.1230\n",
      "154/223, train_loss: 0.1304, step time: 0.1003\n",
      "155/223, train_loss: 0.1282, step time: 0.1088\n",
      "156/223, train_loss: 0.1193, step time: 0.1145\n",
      "157/223, train_loss: 0.1243, step time: 0.1055\n",
      "158/223, train_loss: 0.1208, step time: 0.1092\n",
      "159/223, train_loss: 0.1246, step time: 0.1264\n",
      "160/223, train_loss: 0.1210, step time: 0.1002\n",
      "161/223, train_loss: 0.1135, step time: 0.1050\n",
      "162/223, train_loss: 0.1191, step time: 0.1294\n",
      "163/223, train_loss: 0.1135, step time: 0.1148\n",
      "164/223, train_loss: 0.1242, step time: 0.1004\n",
      "165/223, train_loss: 0.1207, step time: 0.1068\n",
      "166/223, train_loss: 0.1178, step time: 0.1269\n",
      "167/223, train_loss: 0.1125, step time: 0.1002\n",
      "168/223, train_loss: 0.1100, step time: 0.1130\n",
      "169/223, train_loss: 0.1231, step time: 0.1113\n",
      "170/223, train_loss: 0.1199, step time: 0.0994\n",
      "171/223, train_loss: 0.1283, step time: 0.0998\n",
      "172/223, train_loss: 0.1162, step time: 0.0995\n",
      "173/223, train_loss: 0.1080, step time: 0.1007\n",
      "174/223, train_loss: 0.1322, step time: 0.1002\n",
      "175/223, train_loss: 0.1226, step time: 0.1050\n",
      "176/223, train_loss: 0.1341, step time: 0.1121\n",
      "177/223, train_loss: 0.1256, step time: 0.1065\n",
      "178/223, train_loss: 0.1197, step time: 0.1209\n",
      "179/223, train_loss: 0.1275, step time: 0.0999\n",
      "180/223, train_loss: 0.1151, step time: 0.1004\n",
      "181/223, train_loss: 0.1102, step time: 0.1007\n",
      "182/223, train_loss: 0.1153, step time: 0.1008\n",
      "183/223, train_loss: 0.1097, step time: 0.1005\n",
      "184/223, train_loss: 0.1019, step time: 0.1004\n",
      "185/223, train_loss: 0.1288, step time: 0.1179\n",
      "186/223, train_loss: 0.1437, step time: 0.1104\n",
      "187/223, train_loss: 0.1245, step time: 0.1157\n",
      "188/223, train_loss: 0.1302, step time: 0.1005\n",
      "189/223, train_loss: 0.1143, step time: 0.1105\n",
      "190/223, train_loss: 0.1219, step time: 0.1123\n",
      "191/223, train_loss: 0.1248, step time: 0.0997\n",
      "192/223, train_loss: 0.1166, step time: 0.1426\n",
      "193/223, train_loss: 0.1223, step time: 0.1226\n",
      "194/223, train_loss: 0.1293, step time: 0.1135\n",
      "195/223, train_loss: 0.1279, step time: 0.1351\n",
      "196/223, train_loss: 0.1315, step time: 0.1059\n",
      "197/223, train_loss: 0.1076, step time: 0.1412\n",
      "198/223, train_loss: 0.1205, step time: 0.0999\n",
      "199/223, train_loss: 0.1401, step time: 0.0994\n",
      "200/223, train_loss: 0.1197, step time: 0.1069\n",
      "201/223, train_loss: 0.1167, step time: 0.1292\n",
      "202/223, train_loss: 0.1167, step time: 0.1123\n",
      "203/223, train_loss: 0.1212, step time: 0.1229\n",
      "204/223, train_loss: 0.1297, step time: 0.0995\n",
      "205/223, train_loss: 0.1167, step time: 0.1234\n",
      "206/223, train_loss: 0.1313, step time: 0.1079\n",
      "207/223, train_loss: 0.1228, step time: 0.1080\n",
      "208/223, train_loss: 0.1152, step time: 0.1006\n",
      "209/223, train_loss: 0.1083, step time: 0.1010\n",
      "210/223, train_loss: 0.1241, step time: 0.1009\n",
      "211/223, train_loss: 0.1074, step time: 0.0999\n",
      "212/223, train_loss: 0.1395, step time: 0.1120\n",
      "213/223, train_loss: 0.1296, step time: 0.1145\n",
      "214/223, train_loss: 0.1141, step time: 0.1001\n",
      "215/223, train_loss: 0.1314, step time: 0.1011\n",
      "216/223, train_loss: 0.1221, step time: 0.1158\n",
      "217/223, train_loss: 0.1190, step time: 0.1021\n",
      "218/223, train_loss: 0.1221, step time: 0.1095\n",
      "219/223, train_loss: 0.1166, step time: 0.0985\n",
      "220/223, train_loss: 0.1104, step time: 0.0987\n",
      "221/223, train_loss: 0.1294, step time: 0.0987\n",
      "222/223, train_loss: 0.1188, step time: 0.1000\n",
      "223/223, train_loss: 0.1175, step time: 0.0999\n",
      "epoch 75 average loss: 0.1209\n",
      "saved new best metric model\n",
      "current epoch: 75 current mean dice: 0.8445 tc: 0.9145 wt: 0.8562 et: 0.7629\n",
      "best mean dice: 0.8445 at epoch: 75\n",
      "time consuming of epoch 75 is: 92.0486\n",
      "----------\n",
      "epoch 76/300\n",
      "1/223, train_loss: 0.1088, step time: 0.1036\n",
      "2/223, train_loss: 0.1109, step time: 0.1006\n",
      "3/223, train_loss: 0.1057, step time: 0.1010\n",
      "4/223, train_loss: 0.1208, step time: 0.1008\n",
      "5/223, train_loss: 0.1120, step time: 0.1004\n",
      "6/223, train_loss: 0.1121, step time: 0.1213\n",
      "7/223, train_loss: 0.1334, step time: 0.1004\n",
      "8/223, train_loss: 0.1226, step time: 0.1003\n",
      "9/223, train_loss: 0.1391, step time: 0.1133\n",
      "10/223, train_loss: 0.1239, step time: 0.1013\n",
      "11/223, train_loss: 0.1143, step time: 0.1120\n",
      "12/223, train_loss: 0.1078, step time: 0.0995\n",
      "13/223, train_loss: 0.1000, step time: 0.1069\n",
      "14/223, train_loss: 0.1251, step time: 0.1026\n",
      "15/223, train_loss: 0.1262, step time: 0.1001\n",
      "16/223, train_loss: 0.1157, step time: 0.1004\n",
      "17/223, train_loss: 0.1084, step time: 0.0998\n",
      "18/223, train_loss: 0.1117, step time: 0.1012\n",
      "19/223, train_loss: 0.1218, step time: 0.1054\n",
      "20/223, train_loss: 0.1342, step time: 0.0999\n",
      "21/223, train_loss: 0.1219, step time: 0.1066\n",
      "22/223, train_loss: 0.1233, step time: 0.1033\n",
      "23/223, train_loss: 0.1344, step time: 0.1200\n",
      "24/223, train_loss: 0.1245, step time: 0.1002\n",
      "25/223, train_loss: 0.1451, step time: 0.1179\n",
      "26/223, train_loss: 0.1320, step time: 0.1180\n",
      "27/223, train_loss: 0.1244, step time: 0.1118\n",
      "28/223, train_loss: 0.1092, step time: 0.1018\n",
      "29/223, train_loss: 0.1089, step time: 0.1173\n",
      "30/223, train_loss: 0.1094, step time: 0.1023\n",
      "31/223, train_loss: 0.1095, step time: 0.1002\n",
      "32/223, train_loss: 0.1304, step time: 0.1000\n",
      "33/223, train_loss: 0.1254, step time: 0.1111\n",
      "34/223, train_loss: 0.1294, step time: 0.1215\n",
      "35/223, train_loss: 0.1271, step time: 0.1181\n",
      "36/223, train_loss: 0.1078, step time: 0.0999\n",
      "37/223, train_loss: 0.1224, step time: 0.1086\n",
      "38/223, train_loss: 0.1153, step time: 0.1216\n",
      "39/223, train_loss: 0.1239, step time: 0.1162\n",
      "40/223, train_loss: 0.1251, step time: 0.1123\n",
      "41/223, train_loss: 0.1183, step time: 0.1020\n",
      "42/223, train_loss: 0.1256, step time: 0.1036\n",
      "43/223, train_loss: 0.1331, step time: 0.1190\n",
      "44/223, train_loss: 0.1262, step time: 0.1024\n",
      "45/223, train_loss: 0.1159, step time: 0.1154\n",
      "46/223, train_loss: 0.1112, step time: 0.1028\n",
      "47/223, train_loss: 0.1130, step time: 0.1194\n",
      "48/223, train_loss: 0.1145, step time: 0.1012\n",
      "49/223, train_loss: 0.1190, step time: 0.1165\n",
      "50/223, train_loss: 0.1245, step time: 0.1009\n",
      "51/223, train_loss: 0.1131, step time: 0.1123\n",
      "52/223, train_loss: 0.1202, step time: 0.1133\n",
      "53/223, train_loss: 0.1138, step time: 0.1153\n",
      "54/223, train_loss: 0.1234, step time: 0.1211\n",
      "55/223, train_loss: 0.1189, step time: 0.1000\n",
      "56/223, train_loss: 0.1089, step time: 0.1003\n",
      "57/223, train_loss: 0.1135, step time: 0.1218\n",
      "58/223, train_loss: 0.1340, step time: 0.1143\n",
      "59/223, train_loss: 0.1394, step time: 0.1010\n",
      "60/223, train_loss: 0.1219, step time: 0.0992\n",
      "61/223, train_loss: 0.1294, step time: 0.1042\n",
      "62/223, train_loss: 0.1263, step time: 0.0991\n",
      "63/223, train_loss: 0.1187, step time: 0.0986\n",
      "64/223, train_loss: 0.1177, step time: 0.0985\n",
      "65/223, train_loss: 0.1165, step time: 0.1115\n",
      "66/223, train_loss: 0.1297, step time: 0.1417\n",
      "67/223, train_loss: 0.1141, step time: 0.1214\n",
      "68/223, train_loss: 0.1256, step time: 0.1134\n",
      "69/223, train_loss: 0.1281, step time: 0.1001\n",
      "70/223, train_loss: 0.1219, step time: 0.1203\n",
      "71/223, train_loss: 0.1319, step time: 0.1133\n",
      "72/223, train_loss: 0.1171, step time: 0.0998\n",
      "73/223, train_loss: 0.1157, step time: 0.1126\n",
      "74/223, train_loss: 0.1158, step time: 0.1048\n",
      "75/223, train_loss: 0.1208, step time: 0.1058\n",
      "76/223, train_loss: 0.1203, step time: 0.1004\n",
      "77/223, train_loss: 0.1139, step time: 0.1088\n",
      "78/223, train_loss: 0.1330, step time: 0.1025\n",
      "79/223, train_loss: 0.1139, step time: 0.1646\n",
      "80/223, train_loss: 0.1102, step time: 0.1001\n",
      "81/223, train_loss: 0.1178, step time: 0.1088\n",
      "82/223, train_loss: 0.1226, step time: 0.1004\n",
      "83/223, train_loss: 0.1128, step time: 0.1003\n",
      "84/223, train_loss: 0.1252, step time: 0.1041\n",
      "85/223, train_loss: 0.3188, step time: 0.1045\n",
      "86/223, train_loss: 0.1198, step time: 0.1174\n",
      "87/223, train_loss: 0.1238, step time: 0.1052\n",
      "88/223, train_loss: 0.1208, step time: 0.0999\n",
      "89/223, train_loss: 0.1156, step time: 0.1141\n",
      "90/223, train_loss: 0.1123, step time: 0.1180\n",
      "91/223, train_loss: 0.1248, step time: 0.1121\n",
      "92/223, train_loss: 0.1260, step time: 0.1127\n",
      "93/223, train_loss: 0.1084, step time: 0.1051\n",
      "94/223, train_loss: 0.1187, step time: 0.0992\n",
      "95/223, train_loss: 0.1223, step time: 0.1161\n",
      "96/223, train_loss: 0.1229, step time: 0.1362\n",
      "97/223, train_loss: 0.1302, step time: 0.1078\n",
      "98/223, train_loss: 0.1184, step time: 0.1173\n",
      "99/223, train_loss: 0.1169, step time: 0.1144\n",
      "100/223, train_loss: 0.1193, step time: 0.1128\n",
      "101/223, train_loss: 0.1183, step time: 0.1142\n",
      "102/223, train_loss: 0.1193, step time: 0.1081\n",
      "103/223, train_loss: 0.1222, step time: 0.1161\n",
      "104/223, train_loss: 0.1214, step time: 0.1103\n",
      "105/223, train_loss: 0.1150, step time: 0.1085\n",
      "106/223, train_loss: 0.1178, step time: 0.1081\n",
      "107/223, train_loss: 0.1161, step time: 0.1250\n",
      "108/223, train_loss: 0.1149, step time: 0.1052\n",
      "109/223, train_loss: 0.1417, step time: 0.1343\n",
      "110/223, train_loss: 0.1353, step time: 0.1208\n",
      "111/223, train_loss: 0.1187, step time: 0.1067\n",
      "112/223, train_loss: 0.1272, step time: 0.1001\n",
      "113/223, train_loss: 0.1187, step time: 0.1055\n",
      "114/223, train_loss: 0.1173, step time: 0.1112\n",
      "115/223, train_loss: 0.1283, step time: 0.1262\n",
      "116/223, train_loss: 0.1217, step time: 0.1125\n",
      "117/223, train_loss: 0.1169, step time: 0.1036\n",
      "118/223, train_loss: 0.1164, step time: 0.1029\n",
      "119/223, train_loss: 0.1069, step time: 0.0990\n",
      "120/223, train_loss: 0.1109, step time: 0.0999\n",
      "121/223, train_loss: 0.1028, step time: 0.0996\n",
      "122/223, train_loss: 0.1085, step time: 0.0999\n",
      "123/223, train_loss: 0.1090, step time: 0.0999\n",
      "124/223, train_loss: 0.1277, step time: 0.1027\n",
      "125/223, train_loss: 0.1168, step time: 0.1148\n",
      "126/223, train_loss: 0.1126, step time: 0.1323\n",
      "127/223, train_loss: 0.1189, step time: 0.1287\n",
      "128/223, train_loss: 0.1019, step time: 0.1004\n",
      "129/223, train_loss: 0.1321, step time: 0.1021\n",
      "130/223, train_loss: 0.1347, step time: 0.0995\n",
      "131/223, train_loss: 0.1287, step time: 0.0992\n",
      "132/223, train_loss: 0.1094, step time: 0.0996\n",
      "133/223, train_loss: 0.1291, step time: 0.1063\n",
      "134/223, train_loss: 0.1311, step time: 0.1001\n",
      "135/223, train_loss: 0.1241, step time: 0.1097\n",
      "136/223, train_loss: 0.1099, step time: 0.1109\n",
      "137/223, train_loss: 0.1164, step time: 0.1137\n",
      "138/223, train_loss: 0.1226, step time: 0.1179\n",
      "139/223, train_loss: 0.1154, step time: 0.1211\n",
      "140/223, train_loss: 0.1157, step time: 0.1032\n",
      "141/223, train_loss: 0.1115, step time: 0.1126\n",
      "142/223, train_loss: 0.1074, step time: 0.1009\n",
      "143/223, train_loss: 0.1169, step time: 0.0999\n",
      "144/223, train_loss: 0.1063, step time: 0.1024\n",
      "145/223, train_loss: 0.1174, step time: 0.1122\n",
      "146/223, train_loss: 0.1259, step time: 0.1047\n",
      "147/223, train_loss: 0.1113, step time: 0.1113\n",
      "148/223, train_loss: 0.1242, step time: 0.1010\n",
      "149/223, train_loss: 0.1255, step time: 0.1123\n",
      "150/223, train_loss: 0.1148, step time: 0.1028\n",
      "151/223, train_loss: 0.1014, step time: 0.1123\n",
      "152/223, train_loss: 0.1051, step time: 0.1011\n",
      "153/223, train_loss: 0.1253, step time: 0.1141\n",
      "154/223, train_loss: 0.1294, step time: 0.1007\n",
      "155/223, train_loss: 0.1316, step time: 0.1007\n",
      "156/223, train_loss: 0.1137, step time: 0.1060\n",
      "157/223, train_loss: 0.1198, step time: 0.1009\n",
      "158/223, train_loss: 0.1115, step time: 0.1003\n",
      "159/223, train_loss: 0.1199, step time: 0.1005\n",
      "160/223, train_loss: 0.1239, step time: 0.1169\n",
      "161/223, train_loss: 0.1043, step time: 0.1006\n",
      "162/223, train_loss: 0.1270, step time: 0.1027\n",
      "163/223, train_loss: 0.1066, step time: 0.1011\n",
      "164/223, train_loss: 0.1216, step time: 0.1164\n",
      "165/223, train_loss: 0.1296, step time: 0.1124\n",
      "166/223, train_loss: 0.1216, step time: 0.1012\n",
      "167/223, train_loss: 0.1276, step time: 0.1135\n",
      "168/223, train_loss: 0.1304, step time: 0.1005\n",
      "169/223, train_loss: 0.1120, step time: 0.1014\n",
      "170/223, train_loss: 0.1081, step time: 0.1008\n",
      "171/223, train_loss: 0.1201, step time: 0.1014\n",
      "172/223, train_loss: 0.1250, step time: 0.1109\n",
      "173/223, train_loss: 0.1239, step time: 0.1185\n",
      "174/223, train_loss: 0.1082, step time: 0.1007\n",
      "175/223, train_loss: 0.1187, step time: 0.1007\n",
      "176/223, train_loss: 0.1185, step time: 0.1058\n",
      "177/223, train_loss: 0.1256, step time: 0.1100\n",
      "178/223, train_loss: 0.1173, step time: 0.1013\n",
      "179/223, train_loss: 0.1281, step time: 0.1104\n",
      "180/223, train_loss: 0.1237, step time: 0.1011\n",
      "181/223, train_loss: 0.1107, step time: 0.1106\n",
      "182/223, train_loss: 0.1140, step time: 0.1256\n",
      "183/223, train_loss: 0.1223, step time: 0.1287\n",
      "184/223, train_loss: 0.1148, step time: 0.1251\n",
      "185/223, train_loss: 0.1192, step time: 0.1256\n",
      "186/223, train_loss: 0.1199, step time: 0.1035\n",
      "187/223, train_loss: 0.1101, step time: 0.1061\n",
      "188/223, train_loss: 0.1461, step time: 0.1012\n",
      "189/223, train_loss: 0.1186, step time: 0.1209\n",
      "190/223, train_loss: 0.1098, step time: 0.1187\n",
      "191/223, train_loss: 0.1362, step time: 0.1038\n",
      "192/223, train_loss: 0.1170, step time: 0.1001\n",
      "193/223, train_loss: 0.1548, step time: 0.1162\n",
      "194/223, train_loss: 0.1208, step time: 0.1011\n",
      "195/223, train_loss: 0.1162, step time: 0.1011\n",
      "196/223, train_loss: 0.1263, step time: 0.1003\n",
      "197/223, train_loss: 0.1095, step time: 0.1246\n",
      "198/223, train_loss: 0.1373, step time: 0.1050\n",
      "199/223, train_loss: 0.1349, step time: 0.1109\n",
      "200/223, train_loss: 0.1182, step time: 0.0997\n",
      "201/223, train_loss: 0.1094, step time: 0.1171\n",
      "202/223, train_loss: 0.1209, step time: 0.1004\n",
      "203/223, train_loss: 0.1150, step time: 0.1075\n",
      "204/223, train_loss: 0.1224, step time: 0.1062\n",
      "205/223, train_loss: 0.1146, step time: 0.1002\n",
      "206/223, train_loss: 0.1130, step time: 0.1006\n",
      "207/223, train_loss: 0.1131, step time: 0.1115\n",
      "208/223, train_loss: 0.1189, step time: 0.1052\n",
      "209/223, train_loss: 0.1195, step time: 0.1157\n",
      "210/223, train_loss: 0.1153, step time: 0.1003\n",
      "211/223, train_loss: 0.1155, step time: 0.1011\n",
      "212/223, train_loss: 0.1181, step time: 0.1012\n",
      "213/223, train_loss: 0.1158, step time: 0.1004\n",
      "214/223, train_loss: 0.1146, step time: 0.1185\n",
      "215/223, train_loss: 0.1161, step time: 0.1009\n",
      "216/223, train_loss: 0.1154, step time: 0.1000\n",
      "217/223, train_loss: 0.1181, step time: 0.0998\n",
      "218/223, train_loss: 0.1230, step time: 0.1007\n",
      "219/223, train_loss: 0.1140, step time: 0.1011\n",
      "220/223, train_loss: 0.1312, step time: 0.1013\n",
      "221/223, train_loss: 0.1245, step time: 0.0984\n",
      "222/223, train_loss: 0.1075, step time: 0.1003\n",
      "223/223, train_loss: 0.1306, step time: 0.0993\n",
      "epoch 76 average loss: 0.1207\n",
      "time consuming of epoch 76 is: 88.9759\n",
      "----------\n",
      "epoch 77/300\n",
      "1/223, train_loss: 0.1111, step time: 0.1164\n",
      "2/223, train_loss: 0.1125, step time: 0.1123\n",
      "3/223, train_loss: 0.1220, step time: 0.1209\n",
      "4/223, train_loss: 0.1187, step time: 0.1096\n",
      "5/223, train_loss: 0.1191, step time: 0.1054\n",
      "6/223, train_loss: 0.1073, step time: 0.1147\n",
      "7/223, train_loss: 0.1250, step time: 0.1015\n",
      "8/223, train_loss: 0.1154, step time: 0.1532\n",
      "9/223, train_loss: 0.1248, step time: 0.1004\n",
      "10/223, train_loss: 0.1187, step time: 0.1013\n",
      "11/223, train_loss: 0.1134, step time: 0.1115\n",
      "12/223, train_loss: 0.1247, step time: 0.1008\n",
      "13/223, train_loss: 0.1241, step time: 0.0998\n",
      "14/223, train_loss: 0.1275, step time: 0.0999\n",
      "15/223, train_loss: 0.1261, step time: 0.1007\n",
      "16/223, train_loss: 0.1066, step time: 0.1014\n",
      "17/223, train_loss: 0.3074, step time: 0.0998\n",
      "18/223, train_loss: 0.1225, step time: 0.1007\n",
      "19/223, train_loss: 0.1135, step time: 0.1012\n",
      "20/223, train_loss: 0.1151, step time: 0.1009\n",
      "21/223, train_loss: 0.1116, step time: 0.1024\n",
      "22/223, train_loss: 0.1260, step time: 0.0988\n",
      "23/223, train_loss: 0.1102, step time: 0.0994\n",
      "24/223, train_loss: 0.1139, step time: 0.1006\n",
      "25/223, train_loss: 0.1178, step time: 0.1005\n",
      "26/223, train_loss: 0.1189, step time: 0.0994\n",
      "27/223, train_loss: 0.1269, step time: 0.1002\n",
      "28/223, train_loss: 0.1194, step time: 0.1018\n",
      "29/223, train_loss: 0.1134, step time: 0.1006\n",
      "30/223, train_loss: 0.1501, step time: 0.1002\n",
      "31/223, train_loss: 0.1252, step time: 0.1005\n",
      "32/223, train_loss: 0.1114, step time: 0.1007\n",
      "33/223, train_loss: 0.1145, step time: 0.1015\n",
      "34/223, train_loss: 0.1235, step time: 0.1013\n",
      "35/223, train_loss: 0.1103, step time: 0.1218\n",
      "36/223, train_loss: 0.1258, step time: 0.0999\n",
      "37/223, train_loss: 0.1175, step time: 0.1396\n",
      "38/223, train_loss: 0.1287, step time: 0.1093\n",
      "39/223, train_loss: 0.1064, step time: 0.1188\n",
      "40/223, train_loss: 0.1134, step time: 0.1128\n",
      "41/223, train_loss: 0.1245, step time: 0.1277\n",
      "42/223, train_loss: 0.1233, step time: 0.1103\n",
      "43/223, train_loss: 0.1212, step time: 0.1108\n",
      "44/223, train_loss: 0.1251, step time: 0.1020\n",
      "45/223, train_loss: 0.1093, step time: 0.1007\n",
      "46/223, train_loss: 0.1314, step time: 0.1015\n",
      "47/223, train_loss: 0.1282, step time: 0.1013\n",
      "48/223, train_loss: 0.1245, step time: 0.1005\n",
      "49/223, train_loss: 0.1293, step time: 0.1032\n",
      "50/223, train_loss: 0.1226, step time: 0.1276\n",
      "51/223, train_loss: 0.1228, step time: 0.1139\n",
      "52/223, train_loss: 0.1298, step time: 0.1248\n",
      "53/223, train_loss: 0.1234, step time: 0.1199\n",
      "54/223, train_loss: 0.1168, step time: 0.1102\n",
      "55/223, train_loss: 0.1057, step time: 0.1004\n",
      "56/223, train_loss: 0.1231, step time: 0.1022\n",
      "57/223, train_loss: 0.1162, step time: 0.1130\n",
      "58/223, train_loss: 0.1089, step time: 0.1006\n",
      "59/223, train_loss: 0.1273, step time: 0.1069\n",
      "60/223, train_loss: 0.1232, step time: 0.1205\n",
      "61/223, train_loss: 0.1050, step time: 0.1191\n",
      "62/223, train_loss: 0.1236, step time: 0.1184\n",
      "63/223, train_loss: 0.1050, step time: 0.0999\n",
      "64/223, train_loss: 0.1253, step time: 0.1172\n",
      "65/223, train_loss: 0.1213, step time: 0.1047\n",
      "66/223, train_loss: 0.1069, step time: 0.1367\n",
      "67/223, train_loss: 0.1102, step time: 0.1292\n",
      "68/223, train_loss: 0.1152, step time: 0.1010\n",
      "69/223, train_loss: 0.1054, step time: 0.1150\n",
      "70/223, train_loss: 0.1290, step time: 0.1105\n",
      "71/223, train_loss: 0.1095, step time: 0.1124\n",
      "72/223, train_loss: 0.1106, step time: 0.1102\n",
      "73/223, train_loss: 0.1206, step time: 0.1148\n",
      "74/223, train_loss: 0.1193, step time: 0.0993\n",
      "75/223, train_loss: 0.1073, step time: 0.1074\n",
      "76/223, train_loss: 0.1175, step time: 0.1075\n",
      "77/223, train_loss: 0.1089, step time: 0.1059\n",
      "78/223, train_loss: 0.1254, step time: 0.1077\n",
      "79/223, train_loss: 0.1181, step time: 0.1135\n",
      "80/223, train_loss: 0.1095, step time: 0.1521\n",
      "81/223, train_loss: 0.1348, step time: 0.1544\n",
      "82/223, train_loss: 0.1095, step time: 0.1006\n",
      "83/223, train_loss: 0.1149, step time: 0.0998\n",
      "84/223, train_loss: 0.1325, step time: 0.1005\n",
      "85/223, train_loss: 0.1193, step time: 0.1113\n",
      "86/223, train_loss: 0.1216, step time: 0.1132\n",
      "87/223, train_loss: 0.1133, step time: 0.1268\n",
      "88/223, train_loss: 0.1095, step time: 0.1110\n",
      "89/223, train_loss: 0.1244, step time: 0.1098\n",
      "90/223, train_loss: 0.1157, step time: 0.1057\n",
      "91/223, train_loss: 0.1251, step time: 0.1104\n",
      "92/223, train_loss: 0.1200, step time: 0.1108\n",
      "93/223, train_loss: 0.1208, step time: 0.1004\n",
      "94/223, train_loss: 0.1086, step time: 0.1237\n",
      "95/223, train_loss: 0.1183, step time: 0.1365\n",
      "96/223, train_loss: 0.1147, step time: 0.1091\n",
      "97/223, train_loss: 0.1034, step time: 0.1000\n",
      "98/223, train_loss: 0.1301, step time: 0.1073\n",
      "99/223, train_loss: 0.1181, step time: 0.1003\n",
      "100/223, train_loss: 0.1162, step time: 0.1000\n",
      "101/223, train_loss: 0.1309, step time: 0.1011\n",
      "102/223, train_loss: 0.1127, step time: 0.1076\n",
      "103/223, train_loss: 0.1174, step time: 0.1157\n",
      "104/223, train_loss: 0.1203, step time: 0.1027\n",
      "105/223, train_loss: 0.1043, step time: 0.1010\n",
      "106/223, train_loss: 0.1196, step time: 0.1350\n",
      "107/223, train_loss: 0.1204, step time: 0.1109\n",
      "108/223, train_loss: 0.1309, step time: 0.1079\n",
      "109/223, train_loss: 0.1129, step time: 0.1031\n",
      "110/223, train_loss: 0.1024, step time: 0.1002\n",
      "111/223, train_loss: 0.1349, step time: 0.1002\n",
      "112/223, train_loss: 0.1363, step time: 0.1000\n",
      "113/223, train_loss: 0.1152, step time: 0.1003\n",
      "114/223, train_loss: 0.1165, step time: 0.1126\n",
      "115/223, train_loss: 0.1327, step time: 0.1251\n",
      "116/223, train_loss: 0.1449, step time: 0.1005\n",
      "117/223, train_loss: 0.1246, step time: 0.1004\n",
      "118/223, train_loss: 0.1257, step time: 0.1081\n",
      "119/223, train_loss: 0.1169, step time: 0.1040\n",
      "120/223, train_loss: 0.1245, step time: 0.1013\n",
      "121/223, train_loss: 0.1248, step time: 0.1012\n",
      "122/223, train_loss: 0.1067, step time: 0.1186\n",
      "123/223, train_loss: 0.1265, step time: 0.1008\n",
      "124/223, train_loss: 0.1168, step time: 0.1074\n",
      "125/223, train_loss: 0.1156, step time: 0.1176\n",
      "126/223, train_loss: 0.1245, step time: 0.1312\n",
      "127/223, train_loss: 0.1206, step time: 0.1120\n",
      "128/223, train_loss: 0.1162, step time: 0.1037\n",
      "129/223, train_loss: 0.1160, step time: 0.0990\n",
      "130/223, train_loss: 0.1296, step time: 0.0993\n",
      "131/223, train_loss: 0.1326, step time: 0.1209\n",
      "132/223, train_loss: 0.1081, step time: 0.1048\n",
      "133/223, train_loss: 0.1155, step time: 0.1021\n",
      "134/223, train_loss: 0.1204, step time: 0.1007\n",
      "135/223, train_loss: 0.1252, step time: 0.1006\n",
      "136/223, train_loss: 0.1192, step time: 0.1218\n",
      "137/223, train_loss: 0.1274, step time: 0.1030\n",
      "138/223, train_loss: 0.1164, step time: 0.1178\n",
      "139/223, train_loss: 0.1189, step time: 0.1057\n",
      "140/223, train_loss: 0.1208, step time: 0.1238\n",
      "141/223, train_loss: 0.1180, step time: 0.1166\n",
      "142/223, train_loss: 0.1206, step time: 0.0993\n",
      "143/223, train_loss: 0.1136, step time: 0.1057\n",
      "144/223, train_loss: 0.1170, step time: 0.1158\n",
      "145/223, train_loss: 0.1184, step time: 0.1045\n",
      "146/223, train_loss: 0.1245, step time: 0.1031\n",
      "147/223, train_loss: 0.1286, step time: 0.1175\n",
      "148/223, train_loss: 0.1276, step time: 0.1097\n",
      "149/223, train_loss: 0.1215, step time: 0.1091\n",
      "150/223, train_loss: 0.1204, step time: 0.1158\n",
      "151/223, train_loss: 0.1378, step time: 0.1156\n",
      "152/223, train_loss: 0.1321, step time: 0.1142\n",
      "153/223, train_loss: 0.1149, step time: 0.1002\n",
      "154/223, train_loss: 0.1194, step time: 0.1090\n",
      "155/223, train_loss: 0.1102, step time: 0.1074\n",
      "156/223, train_loss: 0.1266, step time: 0.1121\n",
      "157/223, train_loss: 0.1275, step time: 0.1174\n",
      "158/223, train_loss: 0.1153, step time: 0.1026\n",
      "159/223, train_loss: 0.1200, step time: 0.1091\n",
      "160/223, train_loss: 0.1200, step time: 0.1224\n",
      "161/223, train_loss: 0.1262, step time: 0.1046\n",
      "162/223, train_loss: 0.1246, step time: 0.1207\n",
      "163/223, train_loss: 0.1212, step time: 0.1176\n",
      "164/223, train_loss: 0.1123, step time: 0.1250\n",
      "165/223, train_loss: 0.1243, step time: 0.1300\n",
      "166/223, train_loss: 0.1131, step time: 0.1001\n",
      "167/223, train_loss: 0.1118, step time: 0.0996\n",
      "168/223, train_loss: 0.1259, step time: 0.1001\n",
      "169/223, train_loss: 0.1074, step time: 0.1134\n",
      "170/223, train_loss: 0.1140, step time: 0.0996\n",
      "171/223, train_loss: 0.1304, step time: 0.1000\n",
      "172/223, train_loss: 0.1306, step time: 0.1003\n",
      "173/223, train_loss: 0.1260, step time: 0.1104\n",
      "174/223, train_loss: 0.1300, step time: 0.1006\n",
      "175/223, train_loss: 0.1081, step time: 0.0996\n",
      "176/223, train_loss: 0.1175, step time: 0.1246\n",
      "177/223, train_loss: 0.1200, step time: 0.1071\n",
      "178/223, train_loss: 0.1251, step time: 0.1197\n",
      "179/223, train_loss: 0.1153, step time: 0.1129\n",
      "180/223, train_loss: 0.1142, step time: 0.1243\n",
      "181/223, train_loss: 0.1077, step time: 0.1197\n",
      "182/223, train_loss: 0.1347, step time: 0.1120\n",
      "183/223, train_loss: 0.1211, step time: 0.1014\n",
      "184/223, train_loss: 0.1146, step time: 0.1323\n",
      "185/223, train_loss: 0.1164, step time: 0.1152\n",
      "186/223, train_loss: 0.1212, step time: 0.1119\n",
      "187/223, train_loss: 0.1170, step time: 0.1098\n",
      "188/223, train_loss: 0.1276, step time: 0.1123\n",
      "189/223, train_loss: 0.1353, step time: 0.1043\n",
      "190/223, train_loss: 0.1198, step time: 0.1056\n",
      "191/223, train_loss: 0.1173, step time: 0.0987\n",
      "192/223, train_loss: 0.1245, step time: 0.0996\n",
      "193/223, train_loss: 0.1199, step time: 0.0989\n",
      "194/223, train_loss: 0.1284, step time: 0.1074\n",
      "195/223, train_loss: 0.1181, step time: 0.1116\n",
      "196/223, train_loss: 0.1237, step time: 0.0994\n",
      "197/223, train_loss: 0.1077, step time: 0.0988\n",
      "198/223, train_loss: 0.1151, step time: 0.1044\n",
      "199/223, train_loss: 0.1151, step time: 0.0989\n",
      "200/223, train_loss: 0.1147, step time: 0.1000\n",
      "201/223, train_loss: 0.1174, step time: 0.1039\n",
      "202/223, train_loss: 0.1180, step time: 0.1003\n",
      "203/223, train_loss: 0.1105, step time: 0.1007\n",
      "204/223, train_loss: 0.1174, step time: 0.1072\n",
      "205/223, train_loss: 0.1265, step time: 0.1002\n",
      "206/223, train_loss: 0.1105, step time: 0.1144\n",
      "207/223, train_loss: 0.1223, step time: 0.1017\n",
      "208/223, train_loss: 0.1201, step time: 0.1043\n",
      "209/223, train_loss: 0.1104, step time: 0.1112\n",
      "210/223, train_loss: 0.1137, step time: 0.1225\n",
      "211/223, train_loss: 0.1336, step time: 0.1307\n",
      "212/223, train_loss: 0.1143, step time: 0.1007\n",
      "213/223, train_loss: 0.1181, step time: 0.1065\n",
      "214/223, train_loss: 0.1162, step time: 0.1147\n",
      "215/223, train_loss: 0.1166, step time: 0.1048\n",
      "216/223, train_loss: 0.1120, step time: 0.1067\n",
      "217/223, train_loss: 0.1158, step time: 0.1002\n",
      "218/223, train_loss: 0.1359, step time: 0.1033\n",
      "219/223, train_loss: 0.1210, step time: 0.0995\n",
      "220/223, train_loss: 0.1169, step time: 0.1003\n",
      "221/223, train_loss: 0.1120, step time: 0.0996\n",
      "222/223, train_loss: 0.1106, step time: 0.0989\n",
      "223/223, train_loss: 0.1350, step time: 0.1008\n",
      "epoch 77 average loss: 0.1204\n",
      "time consuming of epoch 77 is: 93.8014\n",
      "----------\n",
      "epoch 78/300\n",
      "1/223, train_loss: 0.1075, step time: 0.1099\n",
      "2/223, train_loss: 0.1258, step time: 0.1278\n",
      "3/223, train_loss: 0.1140, step time: 0.1153\n",
      "4/223, train_loss: 0.1244, step time: 0.1005\n",
      "5/223, train_loss: 0.1156, step time: 0.1133\n",
      "6/223, train_loss: 0.1307, step time: 0.1134\n",
      "7/223, train_loss: 0.1261, step time: 0.1001\n",
      "8/223, train_loss: 0.1231, step time: 0.1146\n",
      "9/223, train_loss: 0.1189, step time: 0.1053\n",
      "10/223, train_loss: 0.1165, step time: 0.1009\n",
      "11/223, train_loss: 0.1213, step time: 0.1008\n",
      "12/223, train_loss: 0.1249, step time: 0.1007\n",
      "13/223, train_loss: 0.1252, step time: 0.1079\n",
      "14/223, train_loss: 0.1322, step time: 0.1004\n",
      "15/223, train_loss: 0.1142, step time: 0.1122\n",
      "16/223, train_loss: 0.1242, step time: 0.1160\n",
      "17/223, train_loss: 0.1226, step time: 0.1183\n",
      "18/223, train_loss: 0.1148, step time: 0.1015\n",
      "19/223, train_loss: 0.1140, step time: 0.1112\n",
      "20/223, train_loss: 0.1160, step time: 0.1117\n",
      "21/223, train_loss: 0.1109, step time: 0.1215\n",
      "22/223, train_loss: 0.1240, step time: 0.1113\n",
      "23/223, train_loss: 0.1145, step time: 0.1114\n",
      "24/223, train_loss: 0.1182, step time: 0.1014\n",
      "25/223, train_loss: 0.1091, step time: 0.1042\n",
      "26/223, train_loss: 0.1264, step time: 0.1007\n",
      "27/223, train_loss: 0.1185, step time: 0.1060\n",
      "28/223, train_loss: 0.1113, step time: 0.1084\n",
      "29/223, train_loss: 0.1042, step time: 0.1056\n",
      "30/223, train_loss: 0.1198, step time: 0.1043\n",
      "31/223, train_loss: 0.1245, step time: 0.1089\n",
      "32/223, train_loss: 0.1138, step time: 0.1014\n",
      "33/223, train_loss: 0.1202, step time: 0.1072\n",
      "34/223, train_loss: 0.1127, step time: 0.1162\n",
      "35/223, train_loss: 0.1020, step time: 0.1064\n",
      "36/223, train_loss: 0.1128, step time: 0.1072\n",
      "37/223, train_loss: 0.1225, step time: 0.1156\n",
      "38/223, train_loss: 0.1156, step time: 0.1118\n",
      "39/223, train_loss: 0.1119, step time: 0.1054\n",
      "40/223, train_loss: 0.1203, step time: 0.1069\n",
      "41/223, train_loss: 0.1329, step time: 0.1107\n",
      "42/223, train_loss: 0.1293, step time: 0.1110\n",
      "43/223, train_loss: 0.1262, step time: 0.1014\n",
      "44/223, train_loss: 0.1371, step time: 0.1020\n",
      "45/223, train_loss: 0.1162, step time: 0.1128\n",
      "46/223, train_loss: 0.1105, step time: 0.0999\n",
      "47/223, train_loss: 0.1124, step time: 0.1134\n",
      "48/223, train_loss: 0.1223, step time: 0.0996\n",
      "49/223, train_loss: 0.1188, step time: 0.1084\n",
      "50/223, train_loss: 0.1054, step time: 0.1001\n",
      "51/223, train_loss: 0.1264, step time: 0.1145\n",
      "52/223, train_loss: 0.1328, step time: 0.1186\n",
      "53/223, train_loss: 0.1139, step time: 0.1010\n",
      "54/223, train_loss: 0.1142, step time: 0.1000\n",
      "55/223, train_loss: 0.1193, step time: 0.0996\n",
      "56/223, train_loss: 0.1335, step time: 0.1008\n",
      "57/223, train_loss: 0.1195, step time: 0.1149\n",
      "58/223, train_loss: 0.1285, step time: 0.1003\n",
      "59/223, train_loss: 0.1280, step time: 0.0999\n",
      "60/223, train_loss: 0.1104, step time: 0.0996\n",
      "61/223, train_loss: 0.1198, step time: 0.1004\n",
      "62/223, train_loss: 0.1236, step time: 0.1025\n",
      "63/223, train_loss: 0.1196, step time: 0.0999\n",
      "64/223, train_loss: 0.1328, step time: 0.0991\n",
      "65/223, train_loss: 0.1123, step time: 0.1001\n",
      "66/223, train_loss: 0.1174, step time: 0.1065\n",
      "67/223, train_loss: 0.1200, step time: 0.1127\n",
      "68/223, train_loss: 0.1205, step time: 0.0995\n",
      "69/223, train_loss: 0.1055, step time: 0.0987\n",
      "70/223, train_loss: 0.1296, step time: 0.1079\n",
      "71/223, train_loss: 0.1037, step time: 0.1142\n",
      "72/223, train_loss: 0.1287, step time: 0.1012\n",
      "73/223, train_loss: 0.1097, step time: 0.1002\n",
      "74/223, train_loss: 0.1237, step time: 0.1292\n",
      "75/223, train_loss: 0.1159, step time: 0.1018\n",
      "76/223, train_loss: 0.1104, step time: 0.0999\n",
      "77/223, train_loss: 0.1283, step time: 0.1005\n",
      "78/223, train_loss: 0.1294, step time: 0.1045\n",
      "79/223, train_loss: 0.1255, step time: 0.0998\n",
      "80/223, train_loss: 0.1118, step time: 0.1218\n",
      "81/223, train_loss: 0.1212, step time: 0.1037\n",
      "82/223, train_loss: 0.1128, step time: 0.1011\n",
      "83/223, train_loss: 0.1136, step time: 0.1039\n",
      "84/223, train_loss: 0.1171, step time: 0.1088\n",
      "85/223, train_loss: 0.1175, step time: 0.1045\n",
      "86/223, train_loss: 0.1155, step time: 0.1046\n",
      "87/223, train_loss: 0.1130, step time: 0.1070\n",
      "88/223, train_loss: 0.1194, step time: 0.1007\n",
      "89/223, train_loss: 0.1220, step time: 0.1202\n",
      "90/223, train_loss: 0.1197, step time: 0.1021\n",
      "91/223, train_loss: 0.1247, step time: 0.1080\n",
      "92/223, train_loss: 0.1149, step time: 0.1176\n",
      "93/223, train_loss: 0.1158, step time: 0.1491\n",
      "94/223, train_loss: 0.1169, step time: 0.1008\n",
      "95/223, train_loss: 0.1306, step time: 0.1204\n",
      "96/223, train_loss: 0.1348, step time: 0.1377\n",
      "97/223, train_loss: 0.1068, step time: 0.1232\n",
      "98/223, train_loss: 0.1266, step time: 0.1002\n",
      "99/223, train_loss: 0.1150, step time: 0.1219\n",
      "100/223, train_loss: 0.1166, step time: 0.1052\n",
      "101/223, train_loss: 0.1116, step time: 0.1070\n",
      "102/223, train_loss: 0.1226, step time: 0.1204\n",
      "103/223, train_loss: 0.1129, step time: 0.1132\n",
      "104/223, train_loss: 0.1197, step time: 0.1128\n",
      "105/223, train_loss: 0.1197, step time: 0.1208\n",
      "106/223, train_loss: 0.1142, step time: 0.1192\n",
      "107/223, train_loss: 0.1258, step time: 0.1097\n",
      "108/223, train_loss: 0.1171, step time: 0.1150\n",
      "109/223, train_loss: 0.1229, step time: 0.1181\n",
      "110/223, train_loss: 0.1202, step time: 0.1259\n",
      "111/223, train_loss: 0.1209, step time: 0.1173\n",
      "112/223, train_loss: 0.1240, step time: 0.1108\n",
      "113/223, train_loss: 0.1252, step time: 0.1181\n",
      "114/223, train_loss: 0.1090, step time: 0.1326\n",
      "115/223, train_loss: 0.1183, step time: 0.1112\n",
      "116/223, train_loss: 0.1224, step time: 0.1039\n",
      "117/223, train_loss: 0.1090, step time: 0.1117\n",
      "118/223, train_loss: 0.1223, step time: 0.1249\n",
      "119/223, train_loss: 0.3146, step time: 0.1133\n",
      "120/223, train_loss: 0.1253, step time: 0.1124\n",
      "121/223, train_loss: 0.1252, step time: 0.1146\n",
      "122/223, train_loss: 0.1202, step time: 0.0997\n",
      "123/223, train_loss: 0.1118, step time: 0.0997\n",
      "124/223, train_loss: 0.1202, step time: 0.1110\n",
      "125/223, train_loss: 0.1166, step time: 0.1103\n",
      "126/223, train_loss: 0.1236, step time: 0.0998\n",
      "127/223, train_loss: 0.1175, step time: 0.1200\n",
      "128/223, train_loss: 0.1143, step time: 0.1145\n",
      "129/223, train_loss: 0.1148, step time: 0.1193\n",
      "130/223, train_loss: 0.1179, step time: 0.1151\n",
      "131/223, train_loss: 0.1130, step time: 0.1117\n",
      "132/223, train_loss: 0.1191, step time: 0.1006\n",
      "133/223, train_loss: 0.1208, step time: 0.1000\n",
      "134/223, train_loss: 0.1273, step time: 0.1001\n",
      "135/223, train_loss: 0.1162, step time: 0.1008\n",
      "136/223, train_loss: 0.1230, step time: 0.1020\n",
      "137/223, train_loss: 0.1236, step time: 0.1106\n",
      "138/223, train_loss: 0.1227, step time: 0.1086\n",
      "139/223, train_loss: 0.1115, step time: 0.1003\n",
      "140/223, train_loss: 0.1125, step time: 0.1313\n",
      "141/223, train_loss: 0.1108, step time: 0.1022\n",
      "142/223, train_loss: 0.1179, step time: 0.1021\n",
      "143/223, train_loss: 0.1218, step time: 0.1170\n",
      "144/223, train_loss: 0.1195, step time: 0.1003\n",
      "145/223, train_loss: 0.1232, step time: 0.1061\n",
      "146/223, train_loss: 0.1174, step time: 0.1076\n",
      "147/223, train_loss: 0.1194, step time: 0.1132\n",
      "148/223, train_loss: 0.1306, step time: 0.1262\n",
      "149/223, train_loss: 0.1286, step time: 0.1014\n",
      "150/223, train_loss: 0.1100, step time: 0.1008\n",
      "151/223, train_loss: 0.1305, step time: 0.1004\n",
      "152/223, train_loss: 0.1161, step time: 0.0985\n",
      "153/223, train_loss: 0.1243, step time: 0.1002\n",
      "154/223, train_loss: 0.1224, step time: 0.1004\n",
      "155/223, train_loss: 0.1218, step time: 0.0996\n",
      "156/223, train_loss: 0.1180, step time: 0.0992\n",
      "157/223, train_loss: 0.1063, step time: 0.1006\n",
      "158/223, train_loss: 0.1312, step time: 0.1227\n",
      "159/223, train_loss: 0.1035, step time: 0.1000\n",
      "160/223, train_loss: 0.1255, step time: 0.0998\n",
      "161/223, train_loss: 0.1071, step time: 0.1006\n",
      "162/223, train_loss: 0.1114, step time: 0.1099\n",
      "163/223, train_loss: 0.1286, step time: 0.0993\n",
      "164/223, train_loss: 0.1159, step time: 0.0995\n",
      "165/223, train_loss: 0.1229, step time: 0.1001\n",
      "166/223, train_loss: 0.1156, step time: 0.1003\n",
      "167/223, train_loss: 0.1192, step time: 0.1000\n",
      "168/223, train_loss: 0.1186, step time: 0.0997\n",
      "169/223, train_loss: 0.1245, step time: 0.1005\n",
      "170/223, train_loss: 0.1112, step time: 0.1224\n",
      "171/223, train_loss: 0.1309, step time: 0.1198\n",
      "172/223, train_loss: 0.1209, step time: 0.1372\n",
      "173/223, train_loss: 0.1210, step time: 0.1392\n",
      "174/223, train_loss: 0.1129, step time: 0.1102\n",
      "175/223, train_loss: 0.1129, step time: 0.1145\n",
      "176/223, train_loss: 0.1232, step time: 0.1256\n",
      "177/223, train_loss: 0.1172, step time: 0.1234\n",
      "178/223, train_loss: 0.1111, step time: 0.1006\n",
      "179/223, train_loss: 0.1119, step time: 0.1058\n",
      "180/223, train_loss: 0.1256, step time: 0.1199\n",
      "181/223, train_loss: 0.1321, step time: 0.1067\n",
      "182/223, train_loss: 0.1194, step time: 0.1033\n",
      "183/223, train_loss: 0.1130, step time: 0.1163\n",
      "184/223, train_loss: 0.1131, step time: 0.1338\n",
      "185/223, train_loss: 0.1256, step time: 0.1182\n",
      "186/223, train_loss: 0.1254, step time: 0.1119\n",
      "187/223, train_loss: 0.1106, step time: 0.1004\n",
      "188/223, train_loss: 0.1137, step time: 0.1005\n",
      "189/223, train_loss: 0.1172, step time: 0.1202\n",
      "190/223, train_loss: 0.1042, step time: 0.1117\n",
      "191/223, train_loss: 0.1123, step time: 0.1000\n",
      "192/223, train_loss: 0.1202, step time: 0.1002\n",
      "193/223, train_loss: 0.1247, step time: 0.1004\n",
      "194/223, train_loss: 0.1123, step time: 0.1261\n",
      "195/223, train_loss: 0.1162, step time: 0.1148\n",
      "196/223, train_loss: 0.1162, step time: 0.1080\n",
      "197/223, train_loss: 0.1324, step time: 0.1060\n",
      "198/223, train_loss: 0.1290, step time: 0.1045\n",
      "199/223, train_loss: 0.1219, step time: 0.1111\n",
      "200/223, train_loss: 0.1244, step time: 0.1000\n",
      "201/223, train_loss: 0.1348, step time: 0.1118\n",
      "202/223, train_loss: 0.1132, step time: 0.1010\n",
      "203/223, train_loss: 0.1203, step time: 0.1053\n",
      "204/223, train_loss: 0.1103, step time: 0.0999\n",
      "205/223, train_loss: 0.1156, step time: 0.1004\n",
      "206/223, train_loss: 0.1252, step time: 0.1112\n",
      "207/223, train_loss: 0.1205, step time: 0.0998\n",
      "208/223, train_loss: 0.1292, step time: 0.1036\n",
      "209/223, train_loss: 0.1167, step time: 0.1163\n",
      "210/223, train_loss: 0.1163, step time: 0.1008\n",
      "211/223, train_loss: 0.1092, step time: 0.1148\n",
      "212/223, train_loss: 0.1218, step time: 0.1138\n",
      "213/223, train_loss: 0.1230, step time: 0.1041\n",
      "214/223, train_loss: 0.1288, step time: 0.1022\n",
      "215/223, train_loss: 0.1095, step time: 0.1009\n",
      "216/223, train_loss: 0.1224, step time: 0.1001\n",
      "217/223, train_loss: 0.1101, step time: 0.1002\n",
      "218/223, train_loss: 0.1167, step time: 0.1019\n",
      "219/223, train_loss: 0.1251, step time: 0.0992\n",
      "220/223, train_loss: 0.1377, step time: 0.0998\n",
      "221/223, train_loss: 0.1238, step time: 0.1002\n",
      "222/223, train_loss: 0.1278, step time: 0.1064\n",
      "223/223, train_loss: 0.1274, step time: 0.0992\n",
      "epoch 78 average loss: 0.1202\n",
      "time consuming of epoch 78 is: 91.0824\n",
      "----------\n",
      "epoch 79/300\n",
      "1/223, train_loss: 0.1235, step time: 0.1008\n",
      "2/223, train_loss: 0.1170, step time: 0.1005\n",
      "3/223, train_loss: 0.1330, step time: 0.1007\n",
      "4/223, train_loss: 0.1170, step time: 0.1008\n",
      "5/223, train_loss: 0.1097, step time: 0.1007\n",
      "6/223, train_loss: 0.1164, step time: 0.0997\n",
      "7/223, train_loss: 0.1342, step time: 0.1009\n",
      "8/223, train_loss: 0.1019, step time: 0.1006\n",
      "9/223, train_loss: 0.1073, step time: 0.1014\n",
      "10/223, train_loss: 0.1219, step time: 0.1002\n",
      "11/223, train_loss: 0.1172, step time: 0.1012\n",
      "12/223, train_loss: 0.1228, step time: 0.1004\n",
      "13/223, train_loss: 0.1243, step time: 0.1336\n",
      "14/223, train_loss: 0.1260, step time: 0.1141\n",
      "15/223, train_loss: 0.1224, step time: 0.1161\n",
      "16/223, train_loss: 0.1119, step time: 0.1197\n",
      "17/223, train_loss: 0.1227, step time: 0.1123\n",
      "18/223, train_loss: 0.1227, step time: 0.1002\n",
      "19/223, train_loss: 0.1211, step time: 0.1094\n",
      "20/223, train_loss: 0.1302, step time: 0.1048\n",
      "21/223, train_loss: 0.1207, step time: 0.1001\n",
      "22/223, train_loss: 0.1216, step time: 0.1435\n",
      "23/223, train_loss: 0.1054, step time: 0.1284\n",
      "24/223, train_loss: 0.1390, step time: 0.1014\n",
      "25/223, train_loss: 0.1170, step time: 0.1056\n",
      "26/223, train_loss: 0.1038, step time: 0.1007\n",
      "27/223, train_loss: 0.1173, step time: 0.1147\n",
      "28/223, train_loss: 0.1215, step time: 0.1037\n",
      "29/223, train_loss: 0.1270, step time: 0.1234\n",
      "30/223, train_loss: 0.1122, step time: 0.0996\n",
      "31/223, train_loss: 0.1297, step time: 0.1081\n",
      "32/223, train_loss: 0.1381, step time: 0.1058\n",
      "33/223, train_loss: 0.1225, step time: 0.1122\n",
      "34/223, train_loss: 0.1155, step time: 0.1236\n",
      "35/223, train_loss: 0.1188, step time: 0.1176\n",
      "36/223, train_loss: 0.1140, step time: 0.1175\n",
      "37/223, train_loss: 0.1052, step time: 0.1113\n",
      "38/223, train_loss: 0.1228, step time: 0.1032\n",
      "39/223, train_loss: 0.1158, step time: 0.1017\n",
      "40/223, train_loss: 0.1180, step time: 0.1003\n",
      "41/223, train_loss: 0.1319, step time: 0.1215\n",
      "42/223, train_loss: 0.1237, step time: 0.1072\n",
      "43/223, train_loss: 0.1140, step time: 0.1080\n",
      "44/223, train_loss: 0.1229, step time: 0.1030\n",
      "45/223, train_loss: 0.1121, step time: 0.1285\n",
      "46/223, train_loss: 0.1200, step time: 0.1133\n",
      "47/223, train_loss: 0.1089, step time: 0.0997\n",
      "48/223, train_loss: 0.1076, step time: 0.1183\n",
      "49/223, train_loss: 0.1196, step time: 0.1028\n",
      "50/223, train_loss: 0.1132, step time: 0.1002\n",
      "51/223, train_loss: 0.1249, step time: 0.1022\n",
      "52/223, train_loss: 0.1119, step time: 0.1114\n",
      "53/223, train_loss: 0.1092, step time: 0.1064\n",
      "54/223, train_loss: 0.1316, step time: 0.1073\n",
      "55/223, train_loss: 0.1124, step time: 0.1187\n",
      "56/223, train_loss: 0.1118, step time: 0.1299\n",
      "57/223, train_loss: 0.1244, step time: 0.1103\n",
      "58/223, train_loss: 0.1361, step time: 0.1011\n",
      "59/223, train_loss: 0.1348, step time: 0.1010\n",
      "60/223, train_loss: 0.1263, step time: 0.1004\n",
      "61/223, train_loss: 0.1180, step time: 0.0997\n",
      "62/223, train_loss: 0.1129, step time: 0.1109\n",
      "63/223, train_loss: 0.1112, step time: 0.1004\n",
      "64/223, train_loss: 0.1314, step time: 0.1370\n",
      "65/223, train_loss: 0.1399, step time: 0.1004\n",
      "66/223, train_loss: 0.1155, step time: 0.1239\n",
      "67/223, train_loss: 0.1164, step time: 0.0995\n",
      "68/223, train_loss: 0.1241, step time: 0.1079\n",
      "69/223, train_loss: 0.1273, step time: 0.1109\n",
      "70/223, train_loss: 0.1109, step time: 0.1061\n",
      "71/223, train_loss: 0.1266, step time: 0.1069\n",
      "72/223, train_loss: 0.1181, step time: 0.1069\n",
      "73/223, train_loss: 0.1114, step time: 0.1117\n",
      "74/223, train_loss: 0.1168, step time: 0.1096\n",
      "75/223, train_loss: 0.1242, step time: 0.1002\n",
      "76/223, train_loss: 0.1129, step time: 0.1050\n",
      "77/223, train_loss: 0.1252, step time: 0.1079\n",
      "78/223, train_loss: 0.1148, step time: 0.1023\n",
      "79/223, train_loss: 0.1101, step time: 0.1085\n",
      "80/223, train_loss: 0.3123, step time: 0.1183\n",
      "81/223, train_loss: 0.1031, step time: 0.1084\n",
      "82/223, train_loss: 0.1209, step time: 0.1120\n",
      "83/223, train_loss: 0.1169, step time: 0.1134\n",
      "84/223, train_loss: 0.1162, step time: 0.1003\n",
      "85/223, train_loss: 0.1221, step time: 0.1008\n",
      "86/223, train_loss: 0.1199, step time: 0.1023\n",
      "87/223, train_loss: 0.1176, step time: 0.1165\n",
      "88/223, train_loss: 0.1154, step time: 0.1028\n",
      "89/223, train_loss: 0.1170, step time: 0.1091\n",
      "90/223, train_loss: 0.1143, step time: 0.0994\n",
      "91/223, train_loss: 0.1224, step time: 0.1008\n",
      "92/223, train_loss: 0.1115, step time: 0.1285\n",
      "93/223, train_loss: 0.1252, step time: 0.1076\n",
      "94/223, train_loss: 0.1131, step time: 0.1015\n",
      "95/223, train_loss: 0.1311, step time: 0.1008\n",
      "96/223, train_loss: 0.1277, step time: 0.1007\n",
      "97/223, train_loss: 0.1039, step time: 0.1437\n",
      "98/223, train_loss: 0.1091, step time: 0.1577\n",
      "99/223, train_loss: 0.1145, step time: 0.1195\n",
      "100/223, train_loss: 0.1213, step time: 0.0999\n",
      "101/223, train_loss: 0.1304, step time: 0.1079\n",
      "102/223, train_loss: 0.1239, step time: 0.1114\n",
      "103/223, train_loss: 0.1254, step time: 0.1041\n",
      "104/223, train_loss: 0.1126, step time: 0.1070\n",
      "105/223, train_loss: 0.1298, step time: 0.1145\n",
      "106/223, train_loss: 0.1197, step time: 0.0998\n",
      "107/223, train_loss: 0.1215, step time: 0.1390\n",
      "108/223, train_loss: 0.1095, step time: 0.1007\n",
      "109/223, train_loss: 0.1123, step time: 0.1068\n",
      "110/223, train_loss: 0.1208, step time: 0.1013\n",
      "111/223, train_loss: 0.1315, step time: 0.1016\n",
      "112/223, train_loss: 0.1119, step time: 0.1192\n",
      "113/223, train_loss: 0.1208, step time: 0.1124\n",
      "114/223, train_loss: 0.1137, step time: 0.1318\n",
      "115/223, train_loss: 0.1230, step time: 0.1005\n",
      "116/223, train_loss: 0.1145, step time: 0.1005\n",
      "117/223, train_loss: 0.1146, step time: 0.1040\n",
      "118/223, train_loss: 0.1206, step time: 0.1120\n",
      "119/223, train_loss: 0.1113, step time: 0.1034\n",
      "120/223, train_loss: 0.1112, step time: 0.1065\n",
      "121/223, train_loss: 0.1195, step time: 0.1123\n",
      "122/223, train_loss: 0.1167, step time: 0.1081\n",
      "123/223, train_loss: 0.1019, step time: 0.1339\n",
      "124/223, train_loss: 0.1273, step time: 0.1185\n",
      "125/223, train_loss: 0.1259, step time: 0.1002\n",
      "126/223, train_loss: 0.1294, step time: 0.1000\n",
      "127/223, train_loss: 0.1253, step time: 0.1003\n",
      "128/223, train_loss: 0.1271, step time: 0.1119\n",
      "129/223, train_loss: 0.1263, step time: 0.1101\n",
      "130/223, train_loss: 0.1150, step time: 0.1004\n",
      "131/223, train_loss: 0.1027, step time: 0.1005\n",
      "132/223, train_loss: 0.1198, step time: 0.1142\n",
      "133/223, train_loss: 0.1219, step time: 0.1182\n",
      "134/223, train_loss: 0.1200, step time: 0.1153\n",
      "135/223, train_loss: 0.1148, step time: 0.1144\n",
      "136/223, train_loss: 0.1118, step time: 0.1008\n",
      "137/223, train_loss: 0.1094, step time: 0.1005\n",
      "138/223, train_loss: 0.1304, step time: 0.1271\n",
      "139/223, train_loss: 0.1224, step time: 0.1231\n",
      "140/223, train_loss: 0.1281, step time: 0.1006\n",
      "141/223, train_loss: 0.1228, step time: 0.1011\n",
      "142/223, train_loss: 0.1066, step time: 0.0993\n",
      "143/223, train_loss: 0.1272, step time: 0.0999\n",
      "144/223, train_loss: 0.1256, step time: 0.1003\n",
      "145/223, train_loss: 0.1219, step time: 0.0997\n",
      "146/223, train_loss: 0.1289, step time: 0.1014\n",
      "147/223, train_loss: 0.1164, step time: 0.0995\n",
      "148/223, train_loss: 0.1250, step time: 0.1028\n",
      "149/223, train_loss: 0.1122, step time: 0.1184\n",
      "150/223, train_loss: 0.1108, step time: 0.1096\n",
      "151/223, train_loss: 0.1164, step time: 0.1014\n",
      "152/223, train_loss: 0.1197, step time: 0.1140\n",
      "153/223, train_loss: 0.1157, step time: 0.1014\n",
      "154/223, train_loss: 0.0998, step time: 0.1100\n",
      "155/223, train_loss: 0.1247, step time: 0.1172\n",
      "156/223, train_loss: 0.1235, step time: 0.1008\n",
      "157/223, train_loss: 0.1239, step time: 0.1082\n",
      "158/223, train_loss: 0.1135, step time: 0.1259\n",
      "159/223, train_loss: 0.1175, step time: 0.1047\n",
      "160/223, train_loss: 0.1267, step time: 0.1191\n",
      "161/223, train_loss: 0.1179, step time: 0.1170\n",
      "162/223, train_loss: 0.1206, step time: 0.1220\n",
      "163/223, train_loss: 0.1277, step time: 0.1060\n",
      "164/223, train_loss: 0.1172, step time: 0.1089\n",
      "165/223, train_loss: 0.1146, step time: 0.1146\n",
      "166/223, train_loss: 0.1235, step time: 0.1211\n",
      "167/223, train_loss: 0.1153, step time: 0.1045\n",
      "168/223, train_loss: 0.1269, step time: 0.1266\n",
      "169/223, train_loss: 0.1130, step time: 0.1109\n",
      "170/223, train_loss: 0.1198, step time: 0.1108\n",
      "171/223, train_loss: 0.1216, step time: 0.1037\n",
      "172/223, train_loss: 0.1184, step time: 0.1084\n",
      "173/223, train_loss: 0.1134, step time: 0.1008\n",
      "174/223, train_loss: 0.1191, step time: 0.1006\n",
      "175/223, train_loss: 0.1144, step time: 0.1334\n",
      "176/223, train_loss: 0.1144, step time: 0.1007\n",
      "177/223, train_loss: 0.1181, step time: 0.1186\n",
      "178/223, train_loss: 0.1229, step time: 0.1097\n",
      "179/223, train_loss: 0.1069, step time: 0.1059\n",
      "180/223, train_loss: 0.1166, step time: 0.1009\n",
      "181/223, train_loss: 0.1116, step time: 0.1020\n",
      "182/223, train_loss: 0.1112, step time: 0.1161\n",
      "183/223, train_loss: 0.1199, step time: 0.1245\n",
      "184/223, train_loss: 0.1167, step time: 0.0998\n",
      "185/223, train_loss: 0.1249, step time: 0.1029\n",
      "186/223, train_loss: 0.1167, step time: 0.1720\n",
      "187/223, train_loss: 0.1078, step time: 0.1128\n",
      "188/223, train_loss: 0.1204, step time: 0.0999\n",
      "189/223, train_loss: 0.1146, step time: 0.0990\n",
      "190/223, train_loss: 0.1261, step time: 0.1076\n",
      "191/223, train_loss: 0.1299, step time: 0.1008\n",
      "192/223, train_loss: 0.1120, step time: 0.1007\n",
      "193/223, train_loss: 0.1222, step time: 0.0998\n",
      "194/223, train_loss: 0.1133, step time: 0.1003\n",
      "195/223, train_loss: 0.1258, step time: 0.0999\n",
      "196/223, train_loss: 0.1196, step time: 0.1014\n",
      "197/223, train_loss: 0.1151, step time: 0.1001\n",
      "198/223, train_loss: 0.1119, step time: 0.1183\n",
      "199/223, train_loss: 0.1287, step time: 0.1344\n",
      "200/223, train_loss: 0.1150, step time: 0.1004\n",
      "201/223, train_loss: 0.1079, step time: 0.1039\n",
      "202/223, train_loss: 0.1462, step time: 0.0997\n",
      "203/223, train_loss: 0.1112, step time: 0.1004\n",
      "204/223, train_loss: 0.1274, step time: 0.1541\n",
      "205/223, train_loss: 0.1244, step time: 0.1155\n",
      "206/223, train_loss: 0.1188, step time: 0.1025\n",
      "207/223, train_loss: 0.1053, step time: 0.1096\n",
      "208/223, train_loss: 0.1191, step time: 0.1101\n",
      "209/223, train_loss: 0.1096, step time: 0.1031\n",
      "210/223, train_loss: 0.1200, step time: 0.0997\n",
      "211/223, train_loss: 0.1195, step time: 0.1002\n",
      "212/223, train_loss: 0.1198, step time: 0.1087\n",
      "213/223, train_loss: 0.1125, step time: 0.1101\n",
      "214/223, train_loss: 0.1151, step time: 0.1148\n",
      "215/223, train_loss: 0.1227, step time: 0.1004\n",
      "216/223, train_loss: 0.1214, step time: 0.1176\n",
      "217/223, train_loss: 0.1193, step time: 0.1141\n",
      "218/223, train_loss: 0.1119, step time: 0.1017\n",
      "219/223, train_loss: 0.1107, step time: 0.1195\n",
      "220/223, train_loss: 0.1353, step time: 0.1258\n",
      "221/223, train_loss: 0.1364, step time: 0.1006\n",
      "222/223, train_loss: 0.1220, step time: 0.0994\n",
      "223/223, train_loss: 0.1279, step time: 0.1001\n",
      "epoch 79 average loss: 0.1200\n",
      "time consuming of epoch 79 is: 89.3998\n",
      "----------\n",
      "epoch 80/300\n",
      "1/223, train_loss: 0.1204, step time: 0.1017\n",
      "2/223, train_loss: 0.1270, step time: 0.1003\n",
      "3/223, train_loss: 0.1251, step time: 0.1006\n",
      "4/223, train_loss: 0.1111, step time: 0.1292\n",
      "5/223, train_loss: 0.1170, step time: 0.1018\n",
      "6/223, train_loss: 0.1287, step time: 0.1250\n",
      "7/223, train_loss: 0.1180, step time: 0.1007\n",
      "8/223, train_loss: 0.1157, step time: 0.1222\n",
      "9/223, train_loss: 0.1277, step time: 0.1111\n",
      "10/223, train_loss: 0.1306, step time: 0.1057\n",
      "11/223, train_loss: 0.1096, step time: 0.1363\n",
      "12/223, train_loss: 0.1278, step time: 0.1185\n",
      "13/223, train_loss: 0.1346, step time: 0.1257\n",
      "14/223, train_loss: 0.1214, step time: 0.1069\n",
      "15/223, train_loss: 0.1137, step time: 0.0992\n",
      "16/223, train_loss: 0.1165, step time: 0.1140\n",
      "17/223, train_loss: 0.1166, step time: 0.1110\n",
      "18/223, train_loss: 0.1065, step time: 0.1203\n",
      "19/223, train_loss: 0.1233, step time: 0.1034\n",
      "20/223, train_loss: 0.1059, step time: 0.1015\n",
      "21/223, train_loss: 0.1227, step time: 0.1083\n",
      "22/223, train_loss: 0.1200, step time: 0.1050\n",
      "23/223, train_loss: 0.1116, step time: 0.1007\n",
      "24/223, train_loss: 0.1181, step time: 0.0994\n",
      "25/223, train_loss: 0.1298, step time: 0.0997\n",
      "26/223, train_loss: 0.1160, step time: 0.0984\n",
      "27/223, train_loss: 0.1177, step time: 0.0992\n",
      "28/223, train_loss: 0.1182, step time: 0.1098\n",
      "29/223, train_loss: 0.1190, step time: 0.1068\n",
      "30/223, train_loss: 0.1195, step time: 0.1093\n",
      "31/223, train_loss: 0.1168, step time: 0.1101\n",
      "32/223, train_loss: 0.1175, step time: 0.1123\n",
      "33/223, train_loss: 0.1028, step time: 0.1035\n",
      "34/223, train_loss: 0.1200, step time: 0.1569\n",
      "35/223, train_loss: 0.1304, step time: 0.1504\n",
      "36/223, train_loss: 0.1243, step time: 0.1008\n",
      "37/223, train_loss: 0.1234, step time: 0.1079\n",
      "38/223, train_loss: 0.1256, step time: 0.1008\n",
      "39/223, train_loss: 0.1190, step time: 0.1066\n",
      "40/223, train_loss: 0.1217, step time: 0.1086\n",
      "41/223, train_loss: 0.1038, step time: 0.1276\n",
      "42/223, train_loss: 0.1202, step time: 0.1016\n",
      "43/223, train_loss: 0.1156, step time: 0.1007\n",
      "44/223, train_loss: 0.1145, step time: 0.1000\n",
      "45/223, train_loss: 0.1372, step time: 0.1107\n",
      "46/223, train_loss: 0.1248, step time: 0.1040\n",
      "47/223, train_loss: 0.1102, step time: 0.1136\n",
      "48/223, train_loss: 0.1096, step time: 0.1000\n",
      "49/223, train_loss: 0.1118, step time: 0.1140\n",
      "50/223, train_loss: 0.1129, step time: 0.1046\n",
      "51/223, train_loss: 0.1187, step time: 0.1006\n",
      "52/223, train_loss: 0.1182, step time: 0.0995\n",
      "53/223, train_loss: 0.1251, step time: 0.1099\n",
      "54/223, train_loss: 0.1188, step time: 0.1168\n",
      "55/223, train_loss: 0.1194, step time: 0.1006\n",
      "56/223, train_loss: 0.1161, step time: 0.1017\n",
      "57/223, train_loss: 0.1152, step time: 0.1077\n",
      "58/223, train_loss: 0.1119, step time: 0.1039\n",
      "59/223, train_loss: 0.1253, step time: 0.1006\n",
      "60/223, train_loss: 0.1241, step time: 0.1428\n",
      "61/223, train_loss: 0.1098, step time: 0.1135\n",
      "62/223, train_loss: 0.1117, step time: 0.1004\n",
      "63/223, train_loss: 0.1128, step time: 0.0995\n",
      "64/223, train_loss: 0.1100, step time: 0.1011\n",
      "65/223, train_loss: 0.1084, step time: 0.1137\n",
      "66/223, train_loss: 0.1180, step time: 0.1541\n",
      "67/223, train_loss: 0.1178, step time: 0.1000\n",
      "68/223, train_loss: 0.1247, step time: 0.1006\n",
      "69/223, train_loss: 0.1116, step time: 0.1089\n",
      "70/223, train_loss: 0.1210, step time: 0.1135\n",
      "71/223, train_loss: 0.1253, step time: 0.1154\n",
      "72/223, train_loss: 0.1167, step time: 0.1009\n",
      "73/223, train_loss: 0.1095, step time: 0.1221\n",
      "74/223, train_loss: 0.1180, step time: 0.0997\n",
      "75/223, train_loss: 0.1173, step time: 0.1057\n",
      "76/223, train_loss: 0.1301, step time: 0.1087\n",
      "77/223, train_loss: 0.1190, step time: 0.1132\n",
      "78/223, train_loss: 0.1242, step time: 0.1088\n",
      "79/223, train_loss: 0.1232, step time: 0.1280\n",
      "80/223, train_loss: 0.1256, step time: 0.1061\n",
      "81/223, train_loss: 0.1103, step time: 0.1028\n",
      "82/223, train_loss: 0.1273, step time: 0.1219\n",
      "83/223, train_loss: 0.1142, step time: 0.1089\n",
      "84/223, train_loss: 0.1199, step time: 0.1087\n",
      "85/223, train_loss: 0.1212, step time: 0.1129\n",
      "86/223, train_loss: 0.1141, step time: 0.1193\n",
      "87/223, train_loss: 0.1286, step time: 0.1000\n",
      "88/223, train_loss: 0.1254, step time: 0.1019\n",
      "89/223, train_loss: 0.1120, step time: 0.1164\n",
      "90/223, train_loss: 0.1163, step time: 0.1165\n",
      "91/223, train_loss: 0.1031, step time: 0.1156\n",
      "92/223, train_loss: 0.1223, step time: 0.1003\n",
      "93/223, train_loss: 0.1250, step time: 0.1007\n",
      "94/223, train_loss: 0.1237, step time: 0.0996\n",
      "95/223, train_loss: 0.1125, step time: 0.1243\n",
      "96/223, train_loss: 0.1081, step time: 0.1025\n",
      "97/223, train_loss: 0.1162, step time: 0.1002\n",
      "98/223, train_loss: 0.1265, step time: 0.0995\n",
      "99/223, train_loss: 0.1192, step time: 0.0994\n",
      "100/223, train_loss: 0.1336, step time: 0.1015\n",
      "101/223, train_loss: 0.1190, step time: 0.1249\n",
      "102/223, train_loss: 0.1257, step time: 0.1075\n",
      "103/223, train_loss: 0.1181, step time: 0.1081\n",
      "104/223, train_loss: 0.1176, step time: 0.1395\n",
      "105/223, train_loss: 0.1099, step time: 0.1228\n",
      "106/223, train_loss: 0.1128, step time: 0.1026\n",
      "107/223, train_loss: 0.1086, step time: 0.1185\n",
      "108/223, train_loss: 0.1115, step time: 0.1013\n",
      "109/223, train_loss: 0.1051, step time: 0.1067\n",
      "110/223, train_loss: 0.1200, step time: 0.1016\n",
      "111/223, train_loss: 0.1208, step time: 0.1005\n",
      "112/223, train_loss: 0.1192, step time: 0.1095\n",
      "113/223, train_loss: 0.1220, step time: 0.1041\n",
      "114/223, train_loss: 0.1050, step time: 0.1015\n",
      "115/223, train_loss: 0.1387, step time: 0.1221\n",
      "116/223, train_loss: 0.1241, step time: 0.1045\n",
      "117/223, train_loss: 0.1244, step time: 0.1022\n",
      "118/223, train_loss: 0.1077, step time: 0.1051\n",
      "119/223, train_loss: 0.1092, step time: 0.1117\n",
      "120/223, train_loss: 0.1159, step time: 0.1003\n",
      "121/223, train_loss: 0.1183, step time: 0.1123\n",
      "122/223, train_loss: 0.1321, step time: 0.1001\n",
      "123/223, train_loss: 0.1169, step time: 0.1078\n",
      "124/223, train_loss: 0.1095, step time: 0.1017\n",
      "125/223, train_loss: 0.1173, step time: 0.1113\n",
      "126/223, train_loss: 0.1123, step time: 0.1115\n",
      "127/223, train_loss: 0.1322, step time: 0.1050\n",
      "128/223, train_loss: 0.1168, step time: 0.1002\n",
      "129/223, train_loss: 0.1110, step time: 0.0998\n",
      "130/223, train_loss: 0.1115, step time: 0.0988\n",
      "131/223, train_loss: 0.1255, step time: 0.1058\n",
      "132/223, train_loss: 0.1164, step time: 0.1063\n",
      "133/223, train_loss: 0.1059, step time: 0.1102\n",
      "134/223, train_loss: 0.1286, step time: 0.1038\n",
      "135/223, train_loss: 0.1152, step time: 0.1259\n",
      "136/223, train_loss: 0.1266, step time: 0.0993\n",
      "137/223, train_loss: 0.1305, step time: 0.1138\n",
      "138/223, train_loss: 0.1167, step time: 0.1195\n",
      "139/223, train_loss: 0.1142, step time: 0.1117\n",
      "140/223, train_loss: 0.1220, step time: 0.1070\n",
      "141/223, train_loss: 0.1030, step time: 0.1092\n",
      "142/223, train_loss: 0.1194, step time: 0.1255\n",
      "143/223, train_loss: 0.1077, step time: 0.1142\n",
      "144/223, train_loss: 0.1173, step time: 0.1092\n",
      "145/223, train_loss: 0.1067, step time: 0.1432\n",
      "146/223, train_loss: 0.1154, step time: 0.1189\n",
      "147/223, train_loss: 0.1179, step time: 0.1225\n",
      "148/223, train_loss: 0.1180, step time: 0.1104\n",
      "149/223, train_loss: 0.1121, step time: 0.1119\n",
      "150/223, train_loss: 0.1237, step time: 0.1068\n",
      "151/223, train_loss: 0.1164, step time: 0.1265\n",
      "152/223, train_loss: 0.3102, step time: 0.1002\n",
      "153/223, train_loss: 0.1264, step time: 0.1214\n",
      "154/223, train_loss: 0.1265, step time: 0.1251\n",
      "155/223, train_loss: 0.1075, step time: 0.1001\n",
      "156/223, train_loss: 0.1484, step time: 0.1004\n",
      "157/223, train_loss: 0.1186, step time: 0.0994\n",
      "158/223, train_loss: 0.1129, step time: 0.0995\n",
      "159/223, train_loss: 0.1236, step time: 0.0999\n",
      "160/223, train_loss: 0.1220, step time: 0.1031\n",
      "161/223, train_loss: 0.1256, step time: 0.1204\n",
      "162/223, train_loss: 0.1243, step time: 0.1166\n",
      "163/223, train_loss: 0.1388, step time: 0.1047\n",
      "164/223, train_loss: 0.1232, step time: 0.1133\n",
      "165/223, train_loss: 0.1169, step time: 0.1140\n",
      "166/223, train_loss: 0.1425, step time: 0.1250\n",
      "167/223, train_loss: 0.1206, step time: 0.1179\n",
      "168/223, train_loss: 0.1168, step time: 0.1128\n",
      "169/223, train_loss: 0.1238, step time: 0.1085\n",
      "170/223, train_loss: 0.1066, step time: 0.0998\n",
      "171/223, train_loss: 0.1192, step time: 0.1006\n",
      "172/223, train_loss: 0.1461, step time: 0.1187\n",
      "173/223, train_loss: 0.1163, step time: 0.1050\n",
      "174/223, train_loss: 0.1150, step time: 0.1201\n",
      "175/223, train_loss: 0.1299, step time: 0.1523\n",
      "176/223, train_loss: 0.1102, step time: 0.1099\n",
      "177/223, train_loss: 0.1388, step time: 0.1004\n",
      "178/223, train_loss: 0.1140, step time: 0.1001\n",
      "179/223, train_loss: 0.1075, step time: 0.1000\n",
      "180/223, train_loss: 0.1179, step time: 0.1111\n",
      "181/223, train_loss: 0.1187, step time: 0.1031\n",
      "182/223, train_loss: 0.1284, step time: 0.1560\n",
      "183/223, train_loss: 0.1184, step time: 0.1204\n",
      "184/223, train_loss: 0.1244, step time: 0.1099\n",
      "185/223, train_loss: 0.1233, step time: 0.1174\n",
      "186/223, train_loss: 0.1159, step time: 0.1109\n",
      "187/223, train_loss: 0.1142, step time: 0.1048\n",
      "188/223, train_loss: 0.1172, step time: 0.1022\n",
      "189/223, train_loss: 0.1207, step time: 0.1157\n",
      "190/223, train_loss: 0.1231, step time: 0.1114\n",
      "191/223, train_loss: 0.1250, step time: 0.1175\n",
      "192/223, train_loss: 0.1218, step time: 0.0989\n",
      "193/223, train_loss: 0.1223, step time: 0.1386\n",
      "194/223, train_loss: 0.1187, step time: 0.0999\n",
      "195/223, train_loss: 0.1298, step time: 0.0991\n",
      "196/223, train_loss: 0.1035, step time: 0.1075\n",
      "197/223, train_loss: 0.1214, step time: 0.1156\n",
      "198/223, train_loss: 0.1269, step time: 0.1012\n",
      "199/223, train_loss: 0.1240, step time: 0.1051\n",
      "200/223, train_loss: 0.1215, step time: 0.1002\n",
      "201/223, train_loss: 0.1130, step time: 0.1056\n",
      "202/223, train_loss: 0.1160, step time: 0.1019\n",
      "203/223, train_loss: 0.1145, step time: 0.0994\n",
      "204/223, train_loss: 0.1259, step time: 0.1012\n",
      "205/223, train_loss: 0.1262, step time: 0.1009\n",
      "206/223, train_loss: 0.1176, step time: 0.1002\n",
      "207/223, train_loss: 0.1234, step time: 0.0996\n",
      "208/223, train_loss: 0.1149, step time: 0.1213\n",
      "209/223, train_loss: 0.1332, step time: 0.1287\n",
      "210/223, train_loss: 0.1330, step time: 0.1230\n",
      "211/223, train_loss: 0.1237, step time: 0.1005\n",
      "212/223, train_loss: 0.1148, step time: 0.1110\n",
      "213/223, train_loss: 0.1176, step time: 0.1012\n",
      "214/223, train_loss: 0.1106, step time: 0.1171\n",
      "215/223, train_loss: 0.1149, step time: 0.1012\n",
      "216/223, train_loss: 0.1244, step time: 0.1005\n",
      "217/223, train_loss: 0.1417, step time: 0.1004\n",
      "218/223, train_loss: 0.1206, step time: 0.1001\n",
      "219/223, train_loss: 0.1163, step time: 0.1000\n",
      "220/223, train_loss: 0.1215, step time: 0.1028\n",
      "221/223, train_loss: 0.1214, step time: 0.0998\n",
      "222/223, train_loss: 0.1203, step time: 0.0991\n",
      "223/223, train_loss: 0.1204, step time: 0.0999\n",
      "epoch 80 average loss: 0.1202\n",
      "current epoch: 80 current mean dice: 0.8417 tc: 0.9137 wt: 0.8537 et: 0.7577\n",
      "best mean dice: 0.8445 at epoch: 75\n",
      "time consuming of epoch 80 is: 92.9578\n",
      "----------\n",
      "epoch 81/300\n",
      "1/223, train_loss: 0.1212, step time: 0.1036\n",
      "2/223, train_loss: 0.1196, step time: 0.0995\n",
      "3/223, train_loss: 0.1167, step time: 0.1004\n",
      "4/223, train_loss: 0.1079, step time: 0.1005\n",
      "5/223, train_loss: 0.1277, step time: 0.1148\n",
      "6/223, train_loss: 0.1204, step time: 0.1361\n",
      "7/223, train_loss: 0.1140, step time: 0.1240\n",
      "8/223, train_loss: 0.1018, step time: 0.1002\n",
      "9/223, train_loss: 0.1110, step time: 0.1086\n",
      "10/223, train_loss: 0.1129, step time: 0.1475\n",
      "11/223, train_loss: 0.1249, step time: 0.1145\n",
      "12/223, train_loss: 0.1232, step time: 0.1016\n",
      "13/223, train_loss: 0.1126, step time: 0.1065\n",
      "14/223, train_loss: 0.1293, step time: 0.1451\n",
      "15/223, train_loss: 0.1128, step time: 0.1026\n",
      "16/223, train_loss: 0.1144, step time: 0.0996\n",
      "17/223, train_loss: 0.1077, step time: 0.1074\n",
      "18/223, train_loss: 0.1323, step time: 0.1001\n",
      "19/223, train_loss: 0.1193, step time: 0.1011\n",
      "20/223, train_loss: 0.1096, step time: 0.1002\n",
      "21/223, train_loss: 0.1328, step time: 0.1056\n",
      "22/223, train_loss: 0.1165, step time: 0.1113\n",
      "23/223, train_loss: 0.1309, step time: 0.1051\n",
      "24/223, train_loss: 0.1145, step time: 0.1009\n",
      "25/223, train_loss: 0.1130, step time: 0.1172\n",
      "26/223, train_loss: 0.1161, step time: 0.1112\n",
      "27/223, train_loss: 0.1141, step time: 0.1083\n",
      "28/223, train_loss: 0.1277, step time: 0.0996\n",
      "29/223, train_loss: 0.1069, step time: 0.1060\n",
      "30/223, train_loss: 0.1142, step time: 0.1140\n",
      "31/223, train_loss: 0.1226, step time: 0.1094\n",
      "32/223, train_loss: 0.1275, step time: 0.1045\n",
      "33/223, train_loss: 0.1093, step time: 0.1058\n",
      "34/223, train_loss: 0.1074, step time: 0.1003\n",
      "35/223, train_loss: 0.1144, step time: 0.1151\n",
      "36/223, train_loss: 0.1083, step time: 0.1159\n",
      "37/223, train_loss: 0.1132, step time: 0.1203\n",
      "38/223, train_loss: 0.1459, step time: 0.1051\n",
      "39/223, train_loss: 0.1212, step time: 0.1048\n",
      "40/223, train_loss: 0.1208, step time: 0.1122\n",
      "41/223, train_loss: 0.1157, step time: 0.1120\n",
      "42/223, train_loss: 0.1048, step time: 0.1546\n",
      "43/223, train_loss: 0.1016, step time: 0.1079\n",
      "44/223, train_loss: 0.1277, step time: 0.1001\n",
      "45/223, train_loss: 0.1151, step time: 0.1116\n",
      "46/223, train_loss: 0.1123, step time: 0.1023\n",
      "47/223, train_loss: 0.1179, step time: 0.1008\n",
      "48/223, train_loss: 0.1083, step time: 0.1005\n",
      "49/223, train_loss: 0.1291, step time: 0.1187\n",
      "50/223, train_loss: 0.1141, step time: 0.1041\n",
      "51/223, train_loss: 0.1068, step time: 0.1132\n",
      "52/223, train_loss: 0.1032, step time: 0.1074\n",
      "53/223, train_loss: 0.1154, step time: 0.1068\n",
      "54/223, train_loss: 0.1150, step time: 0.1133\n",
      "55/223, train_loss: 0.1151, step time: 0.1120\n",
      "56/223, train_loss: 0.1136, step time: 0.1115\n",
      "57/223, train_loss: 0.3071, step time: 0.1026\n",
      "58/223, train_loss: 0.1280, step time: 0.1039\n",
      "59/223, train_loss: 0.1285, step time: 0.1040\n",
      "60/223, train_loss: 0.1301, step time: 0.1537\n",
      "61/223, train_loss: 0.1230, step time: 0.1080\n",
      "62/223, train_loss: 0.1155, step time: 0.1140\n",
      "63/223, train_loss: 0.1316, step time: 0.1140\n",
      "64/223, train_loss: 0.1082, step time: 0.1159\n",
      "65/223, train_loss: 0.1184, step time: 0.1056\n",
      "66/223, train_loss: 0.1307, step time: 0.1211\n",
      "67/223, train_loss: 0.1164, step time: 0.1056\n",
      "68/223, train_loss: 0.1303, step time: 0.1085\n",
      "69/223, train_loss: 0.1226, step time: 0.1054\n",
      "70/223, train_loss: 0.1219, step time: 0.1117\n",
      "71/223, train_loss: 0.1097, step time: 0.1087\n",
      "72/223, train_loss: 0.1274, step time: 0.1008\n",
      "73/223, train_loss: 0.1328, step time: 0.1113\n",
      "74/223, train_loss: 0.1240, step time: 0.0993\n",
      "75/223, train_loss: 0.1416, step time: 0.1203\n",
      "76/223, train_loss: 0.1252, step time: 0.1067\n",
      "77/223, train_loss: 0.1224, step time: 0.1322\n",
      "78/223, train_loss: 0.1146, step time: 0.1109\n",
      "79/223, train_loss: 0.1234, step time: 0.1080\n",
      "80/223, train_loss: 0.1244, step time: 0.1117\n",
      "81/223, train_loss: 0.1251, step time: 0.1000\n",
      "82/223, train_loss: 0.1448, step time: 0.1047\n",
      "83/223, train_loss: 0.1145, step time: 0.1140\n",
      "84/223, train_loss: 0.1155, step time: 0.1072\n",
      "85/223, train_loss: 0.1151, step time: 0.1069\n",
      "86/223, train_loss: 0.1118, step time: 0.1198\n",
      "87/223, train_loss: 0.1128, step time: 0.1208\n",
      "88/223, train_loss: 0.1254, step time: 0.1300\n",
      "89/223, train_loss: 0.1208, step time: 0.1096\n",
      "90/223, train_loss: 0.1159, step time: 0.1030\n",
      "91/223, train_loss: 0.1265, step time: 0.1003\n",
      "92/223, train_loss: 0.1075, step time: 0.1010\n",
      "93/223, train_loss: 0.1159, step time: 0.1140\n",
      "94/223, train_loss: 0.1123, step time: 0.1485\n",
      "95/223, train_loss: 0.1156, step time: 0.1281\n",
      "96/223, train_loss: 0.1097, step time: 0.1903\n",
      "97/223, train_loss: 0.1191, step time: 0.1097\n",
      "98/223, train_loss: 0.1210, step time: 0.1127\n",
      "99/223, train_loss: 0.1092, step time: 0.1145\n",
      "100/223, train_loss: 0.1132, step time: 0.1216\n",
      "101/223, train_loss: 0.1144, step time: 0.1033\n",
      "102/223, train_loss: 0.1215, step time: 0.1024\n",
      "103/223, train_loss: 0.1155, step time: 0.1098\n",
      "104/223, train_loss: 0.1139, step time: 0.1096\n",
      "105/223, train_loss: 0.1085, step time: 0.1150\n",
      "106/223, train_loss: 0.1112, step time: 0.1012\n",
      "107/223, train_loss: 0.1272, step time: 0.1262\n",
      "108/223, train_loss: 0.1126, step time: 0.1066\n",
      "109/223, train_loss: 0.1210, step time: 0.1072\n",
      "110/223, train_loss: 0.1117, step time: 0.1122\n",
      "111/223, train_loss: 0.1083, step time: 0.1118\n",
      "112/223, train_loss: 0.1150, step time: 0.1032\n",
      "113/223, train_loss: 0.1080, step time: 0.1182\n",
      "114/223, train_loss: 0.1034, step time: 0.1001\n",
      "115/223, train_loss: 0.1270, step time: 0.1189\n",
      "116/223, train_loss: 0.1241, step time: 0.1051\n",
      "117/223, train_loss: 0.1096, step time: 0.1014\n",
      "118/223, train_loss: 0.1141, step time: 0.1270\n",
      "119/223, train_loss: 0.1305, step time: 0.1125\n",
      "120/223, train_loss: 0.1162, step time: 0.1078\n",
      "121/223, train_loss: 0.1188, step time: 0.1105\n",
      "122/223, train_loss: 0.1196, step time: 0.1358\n",
      "123/223, train_loss: 0.1117, step time: 0.1182\n",
      "124/223, train_loss: 0.1115, step time: 0.0997\n",
      "125/223, train_loss: 0.1212, step time: 0.0996\n",
      "126/223, train_loss: 0.1213, step time: 0.1122\n",
      "127/223, train_loss: 0.1140, step time: 0.1037\n",
      "128/223, train_loss: 0.1094, step time: 0.1016\n",
      "129/223, train_loss: 0.1184, step time: 0.1049\n",
      "130/223, train_loss: 0.1188, step time: 0.1161\n",
      "131/223, train_loss: 0.1161, step time: 0.1024\n",
      "132/223, train_loss: 0.1246, step time: 0.1198\n",
      "133/223, train_loss: 0.1235, step time: 0.1011\n",
      "134/223, train_loss: 0.1057, step time: 0.1002\n",
      "135/223, train_loss: 0.1198, step time: 0.1123\n",
      "136/223, train_loss: 0.1119, step time: 0.1189\n",
      "137/223, train_loss: 0.1215, step time: 0.1255\n",
      "138/223, train_loss: 0.1243, step time: 0.1120\n",
      "139/223, train_loss: 0.1159, step time: 0.1173\n",
      "140/223, train_loss: 0.1133, step time: 0.1033\n",
      "141/223, train_loss: 0.1125, step time: 0.1248\n",
      "142/223, train_loss: 0.1319, step time: 0.0998\n",
      "143/223, train_loss: 0.1243, step time: 0.1066\n",
      "144/223, train_loss: 0.1265, step time: 0.1007\n",
      "145/223, train_loss: 0.1292, step time: 0.1027\n",
      "146/223, train_loss: 0.1166, step time: 0.1007\n",
      "147/223, train_loss: 0.1193, step time: 0.1200\n",
      "148/223, train_loss: 0.1124, step time: 0.1191\n",
      "149/223, train_loss: 0.1115, step time: 0.1248\n",
      "150/223, train_loss: 0.1121, step time: 0.1006\n",
      "151/223, train_loss: 0.1138, step time: 0.1073\n",
      "152/223, train_loss: 0.1113, step time: 0.1018\n",
      "153/223, train_loss: 0.1078, step time: 0.1051\n",
      "154/223, train_loss: 0.1131, step time: 0.1008\n",
      "155/223, train_loss: 0.1211, step time: 0.1091\n",
      "156/223, train_loss: 0.1237, step time: 0.1004\n",
      "157/223, train_loss: 0.1237, step time: 0.1011\n",
      "158/223, train_loss: 0.1415, step time: 0.1004\n",
      "159/223, train_loss: 0.1255, step time: 0.1116\n",
      "160/223, train_loss: 0.1153, step time: 0.1111\n",
      "161/223, train_loss: 0.1177, step time: 0.1073\n",
      "162/223, train_loss: 0.1348, step time: 0.1006\n",
      "163/223, train_loss: 0.1337, step time: 0.1294\n",
      "164/223, train_loss: 0.1076, step time: 0.1122\n",
      "165/223, train_loss: 0.1189, step time: 0.1071\n",
      "166/223, train_loss: 0.1147, step time: 0.1024\n",
      "167/223, train_loss: 0.1138, step time: 0.1227\n",
      "168/223, train_loss: 0.1246, step time: 0.1014\n",
      "169/223, train_loss: 0.1130, step time: 0.1169\n",
      "170/223, train_loss: 0.1361, step time: 0.1369\n",
      "171/223, train_loss: 0.1221, step time: 0.1211\n",
      "172/223, train_loss: 0.1266, step time: 0.1065\n",
      "173/223, train_loss: 0.1118, step time: 0.1228\n",
      "174/223, train_loss: 0.1131, step time: 0.0999\n",
      "175/223, train_loss: 0.1251, step time: 0.1106\n",
      "176/223, train_loss: 0.1117, step time: 0.1157\n",
      "177/223, train_loss: 0.1076, step time: 0.1112\n",
      "178/223, train_loss: 0.1067, step time: 0.1107\n",
      "179/223, train_loss: 0.1064, step time: 0.1076\n",
      "180/223, train_loss: 0.1108, step time: 0.1014\n",
      "181/223, train_loss: 0.1108, step time: 0.1003\n",
      "182/223, train_loss: 0.1232, step time: 0.1084\n",
      "183/223, train_loss: 0.1112, step time: 0.1393\n",
      "184/223, train_loss: 0.1088, step time: 0.1011\n",
      "185/223, train_loss: 0.1142, step time: 0.1001\n",
      "186/223, train_loss: 0.1156, step time: 0.1191\n",
      "187/223, train_loss: 0.1151, step time: 0.1149\n",
      "188/223, train_loss: 0.1048, step time: 0.1181\n",
      "189/223, train_loss: 0.1135, step time: 0.1011\n",
      "190/223, train_loss: 0.1144, step time: 0.1005\n",
      "191/223, train_loss: 0.1189, step time: 0.1023\n",
      "192/223, train_loss: 0.1211, step time: 0.1048\n",
      "193/223, train_loss: 0.1195, step time: 0.1010\n",
      "194/223, train_loss: 0.1358, step time: 0.1013\n",
      "195/223, train_loss: 0.1210, step time: 0.1024\n",
      "196/223, train_loss: 0.1283, step time: 0.1021\n",
      "197/223, train_loss: 0.1207, step time: 0.1008\n",
      "198/223, train_loss: 0.1316, step time: 0.1222\n",
      "199/223, train_loss: 0.1183, step time: 0.1100\n",
      "200/223, train_loss: 0.1082, step time: 0.1047\n",
      "201/223, train_loss: 0.1216, step time: 0.1009\n",
      "202/223, train_loss: 0.1499, step time: 0.1072\n",
      "203/223, train_loss: 0.1088, step time: 0.1096\n",
      "204/223, train_loss: 0.1227, step time: 0.1213\n",
      "205/223, train_loss: 0.1192, step time: 0.1020\n",
      "206/223, train_loss: 0.1256, step time: 0.1005\n",
      "207/223, train_loss: 0.1143, step time: 0.1666\n",
      "208/223, train_loss: 0.1183, step time: 0.1203\n",
      "209/223, train_loss: 0.1061, step time: 0.1186\n",
      "210/223, train_loss: 0.1264, step time: 0.1054\n",
      "211/223, train_loss: 0.1533, step time: 0.1036\n",
      "212/223, train_loss: 0.1258, step time: 0.1256\n",
      "213/223, train_loss: 0.1140, step time: 0.1184\n",
      "214/223, train_loss: 0.1155, step time: 0.1005\n",
      "215/223, train_loss: 0.1182, step time: 0.1136\n",
      "216/223, train_loss: 0.1127, step time: 0.1148\n",
      "217/223, train_loss: 0.1165, step time: 0.1316\n",
      "218/223, train_loss: 0.1147, step time: 0.1368\n",
      "219/223, train_loss: 0.1146, step time: 0.0995\n",
      "220/223, train_loss: 0.1110, step time: 0.1018\n",
      "221/223, train_loss: 0.1189, step time: 0.1011\n",
      "222/223, train_loss: 0.1260, step time: 0.1094\n",
      "223/223, train_loss: 0.1112, step time: 0.1004\n",
      "epoch 81 average loss: 0.1191\n",
      "time consuming of epoch 81 is: 88.8599\n",
      "----------\n",
      "epoch 82/300\n",
      "1/223, train_loss: 0.1223, step time: 0.1052\n",
      "2/223, train_loss: 0.1338, step time: 0.1055\n",
      "3/223, train_loss: 0.1179, step time: 0.1347\n",
      "4/223, train_loss: 0.1020, step time: 0.1012\n",
      "5/223, train_loss: 0.1175, step time: 0.1204\n",
      "6/223, train_loss: 0.1177, step time: 0.1343\n",
      "7/223, train_loss: 0.1166, step time: 0.1007\n",
      "8/223, train_loss: 0.1087, step time: 0.1009\n",
      "9/223, train_loss: 0.1300, step time: 0.1174\n",
      "10/223, train_loss: 0.1145, step time: 0.1150\n",
      "11/223, train_loss: 0.1163, step time: 0.1003\n",
      "12/223, train_loss: 0.1265, step time: 0.1006\n",
      "13/223, train_loss: 0.1180, step time: 0.1000\n",
      "14/223, train_loss: 0.1295, step time: 0.2265\n",
      "15/223, train_loss: 0.1218, step time: 0.1007\n",
      "16/223, train_loss: 0.1088, step time: 0.1078\n",
      "17/223, train_loss: 0.1271, step time: 0.1135\n",
      "18/223, train_loss: 0.1316, step time: 0.1090\n",
      "19/223, train_loss: 0.1153, step time: 0.1129\n",
      "20/223, train_loss: 0.1249, step time: 0.1471\n",
      "21/223, train_loss: 0.1259, step time: 0.1140\n",
      "22/223, train_loss: 0.1270, step time: 0.1180\n",
      "23/223, train_loss: 0.1009, step time: 0.1025\n",
      "24/223, train_loss: 0.1192, step time: 0.1069\n",
      "25/223, train_loss: 0.1215, step time: 0.0999\n",
      "26/223, train_loss: 0.1068, step time: 0.1253\n",
      "27/223, train_loss: 0.1143, step time: 0.1035\n",
      "28/223, train_loss: 0.1241, step time: 0.1040\n",
      "29/223, train_loss: 0.1107, step time: 0.1148\n",
      "30/223, train_loss: 0.1147, step time: 0.1152\n",
      "31/223, train_loss: 0.1210, step time: 0.1205\n",
      "32/223, train_loss: 0.1071, step time: 0.1002\n",
      "33/223, train_loss: 0.1406, step time: 0.1154\n",
      "34/223, train_loss: 0.1115, step time: 0.1272\n",
      "35/223, train_loss: 0.1070, step time: 0.1262\n",
      "36/223, train_loss: 0.1008, step time: 0.1098\n",
      "37/223, train_loss: 0.1122, step time: 0.1144\n",
      "38/223, train_loss: 0.1100, step time: 0.1116\n",
      "39/223, train_loss: 0.1226, step time: 0.1152\n",
      "40/223, train_loss: 0.1244, step time: 0.1298\n",
      "41/223, train_loss: 0.1137, step time: 0.1008\n",
      "42/223, train_loss: 0.1297, step time: 0.1055\n",
      "43/223, train_loss: 0.1237, step time: 0.1180\n",
      "44/223, train_loss: 0.1138, step time: 0.1126\n",
      "45/223, train_loss: 0.1186, step time: 0.1090\n",
      "46/223, train_loss: 0.1278, step time: 0.1013\n",
      "47/223, train_loss: 0.1339, step time: 0.1012\n",
      "48/223, train_loss: 0.1051, step time: 0.1007\n",
      "49/223, train_loss: 0.1297, step time: 0.1046\n",
      "50/223, train_loss: 0.1121, step time: 0.1045\n",
      "51/223, train_loss: 0.1103, step time: 0.1086\n",
      "52/223, train_loss: 0.1166, step time: 0.1124\n",
      "53/223, train_loss: 0.1201, step time: 0.1009\n",
      "54/223, train_loss: 0.1111, step time: 0.1004\n",
      "55/223, train_loss: 0.1212, step time: 0.1045\n",
      "56/223, train_loss: 0.1196, step time: 0.1005\n",
      "57/223, train_loss: 0.1247, step time: 0.1022\n",
      "58/223, train_loss: 0.1199, step time: 0.1268\n",
      "59/223, train_loss: 0.1202, step time: 0.1002\n",
      "60/223, train_loss: 0.1197, step time: 0.1115\n",
      "61/223, train_loss: 0.1222, step time: 0.1145\n",
      "62/223, train_loss: 0.1212, step time: 0.1075\n",
      "63/223, train_loss: 0.1308, step time: 0.1081\n",
      "64/223, train_loss: 0.1226, step time: 0.1009\n",
      "65/223, train_loss: 0.1084, step time: 0.1058\n",
      "66/223, train_loss: 0.1175, step time: 0.1382\n",
      "67/223, train_loss: 0.1101, step time: 0.1004\n",
      "68/223, train_loss: 0.1277, step time: 0.1181\n",
      "69/223, train_loss: 0.1228, step time: 0.1085\n",
      "70/223, train_loss: 0.1304, step time: 0.0999\n",
      "71/223, train_loss: 0.1189, step time: 0.1083\n",
      "72/223, train_loss: 0.1170, step time: 0.0996\n",
      "73/223, train_loss: 0.1182, step time: 0.0995\n",
      "74/223, train_loss: 0.1192, step time: 0.1165\n",
      "75/223, train_loss: 0.1154, step time: 0.1061\n",
      "76/223, train_loss: 0.1237, step time: 0.1112\n",
      "77/223, train_loss: 0.1080, step time: 0.1179\n",
      "78/223, train_loss: 0.3176, step time: 0.1040\n",
      "79/223, train_loss: 0.1218, step time: 0.1123\n",
      "80/223, train_loss: 0.1259, step time: 0.1196\n",
      "81/223, train_loss: 0.1156, step time: 0.1235\n",
      "82/223, train_loss: 0.1113, step time: 0.1064\n",
      "83/223, train_loss: 0.1203, step time: 0.1049\n",
      "84/223, train_loss: 0.1272, step time: 0.0996\n",
      "85/223, train_loss: 0.1204, step time: 0.1067\n",
      "86/223, train_loss: 0.1142, step time: 0.1051\n",
      "87/223, train_loss: 0.1068, step time: 0.1244\n",
      "88/223, train_loss: 0.1207, step time: 0.1284\n",
      "89/223, train_loss: 0.1127, step time: 0.1139\n",
      "90/223, train_loss: 0.1316, step time: 0.1004\n",
      "91/223, train_loss: 0.1207, step time: 0.1013\n",
      "92/223, train_loss: 0.1242, step time: 0.1056\n",
      "93/223, train_loss: 0.1105, step time: 0.1176\n",
      "94/223, train_loss: 0.1339, step time: 0.1122\n",
      "95/223, train_loss: 0.1177, step time: 0.1128\n",
      "96/223, train_loss: 0.1244, step time: 0.1091\n",
      "97/223, train_loss: 0.1261, step time: 0.1159\n",
      "98/223, train_loss: 0.1271, step time: 0.1003\n",
      "99/223, train_loss: 0.1385, step time: 0.1009\n",
      "100/223, train_loss: 0.1190, step time: 0.1004\n",
      "101/223, train_loss: 0.1255, step time: 0.1000\n",
      "102/223, train_loss: 0.1091, step time: 0.1003\n",
      "103/223, train_loss: 0.1191, step time: 0.1006\n",
      "104/223, train_loss: 0.1309, step time: 0.1184\n",
      "105/223, train_loss: 0.1228, step time: 0.1351\n",
      "106/223, train_loss: 0.1190, step time: 0.1006\n",
      "107/223, train_loss: 0.1344, step time: 0.1010\n",
      "108/223, train_loss: 0.1063, step time: 0.1086\n",
      "109/223, train_loss: 0.1434, step time: 0.1022\n",
      "110/223, train_loss: 0.1142, step time: 0.1095\n",
      "111/223, train_loss: 0.1423, step time: 0.1140\n",
      "112/223, train_loss: 0.1118, step time: 0.1120\n",
      "113/223, train_loss: 0.1119, step time: 0.1009\n",
      "114/223, train_loss: 0.1206, step time: 0.1080\n",
      "115/223, train_loss: 0.1125, step time: 0.1237\n",
      "116/223, train_loss: 0.1219, step time: 0.1335\n",
      "117/223, train_loss: 0.1217, step time: 0.1055\n",
      "118/223, train_loss: 0.1174, step time: 0.1003\n",
      "119/223, train_loss: 0.1222, step time: 0.1006\n",
      "120/223, train_loss: 0.1237, step time: 0.1004\n",
      "121/223, train_loss: 0.1101, step time: 0.1004\n",
      "122/223, train_loss: 0.1114, step time: 0.1003\n",
      "123/223, train_loss: 0.1104, step time: 0.1098\n",
      "124/223, train_loss: 0.1125, step time: 0.1120\n",
      "125/223, train_loss: 0.1332, step time: 0.1248\n",
      "126/223, train_loss: 0.1121, step time: 0.1410\n",
      "127/223, train_loss: 0.1152, step time: 0.0991\n",
      "128/223, train_loss: 0.1186, step time: 0.0993\n",
      "129/223, train_loss: 0.1173, step time: 0.1004\n",
      "130/223, train_loss: 0.1151, step time: 0.1435\n",
      "131/223, train_loss: 0.1370, step time: 0.1202\n",
      "132/223, train_loss: 0.1193, step time: 0.0995\n",
      "133/223, train_loss: 0.1274, step time: 0.1033\n",
      "134/223, train_loss: 0.1093, step time: 0.0991\n",
      "135/223, train_loss: 0.1126, step time: 0.0993\n",
      "136/223, train_loss: 0.1131, step time: 0.0989\n",
      "137/223, train_loss: 0.1191, step time: 0.1093\n",
      "138/223, train_loss: 0.1309, step time: 0.1182\n",
      "139/223, train_loss: 0.1190, step time: 0.1000\n",
      "140/223, train_loss: 0.1154, step time: 0.1047\n",
      "141/223, train_loss: 0.1127, step time: 0.1079\n",
      "142/223, train_loss: 0.1257, step time: 0.0994\n",
      "143/223, train_loss: 0.1247, step time: 0.0992\n",
      "144/223, train_loss: 0.1141, step time: 0.1036\n",
      "145/223, train_loss: 0.1315, step time: 0.1152\n",
      "146/223, train_loss: 0.1159, step time: 0.1121\n",
      "147/223, train_loss: 0.1151, step time: 0.1174\n",
      "148/223, train_loss: 0.1298, step time: 0.1005\n",
      "149/223, train_loss: 0.1207, step time: 0.1179\n",
      "150/223, train_loss: 0.1206, step time: 0.1204\n",
      "151/223, train_loss: 0.1181, step time: 0.1104\n",
      "152/223, train_loss: 0.1264, step time: 0.1082\n",
      "153/223, train_loss: 0.1100, step time: 0.1063\n",
      "154/223, train_loss: 0.1255, step time: 0.1102\n",
      "155/223, train_loss: 0.1206, step time: 0.1057\n",
      "156/223, train_loss: 0.1191, step time: 0.1116\n",
      "157/223, train_loss: 0.1193, step time: 0.1007\n",
      "158/223, train_loss: 0.1211, step time: 0.1006\n",
      "159/223, train_loss: 0.1368, step time: 0.1008\n",
      "160/223, train_loss: 0.1204, step time: 0.1038\n",
      "161/223, train_loss: 0.1331, step time: 0.1002\n",
      "162/223, train_loss: 0.1132, step time: 0.0994\n",
      "163/223, train_loss: 0.1367, step time: 0.1007\n",
      "164/223, train_loss: 0.1066, step time: 0.1029\n",
      "165/223, train_loss: 0.1090, step time: 0.0998\n",
      "166/223, train_loss: 0.1188, step time: 0.1000\n",
      "167/223, train_loss: 0.1230, step time: 0.1007\n",
      "168/223, train_loss: 0.1313, step time: 0.1043\n",
      "169/223, train_loss: 0.1241, step time: 0.0999\n",
      "170/223, train_loss: 0.1099, step time: 0.0993\n",
      "171/223, train_loss: 0.1073, step time: 0.1007\n",
      "172/223, train_loss: 0.1096, step time: 0.1081\n",
      "173/223, train_loss: 0.1106, step time: 0.0990\n",
      "174/223, train_loss: 0.1203, step time: 0.0992\n",
      "175/223, train_loss: 0.1159, step time: 0.1010\n",
      "176/223, train_loss: 0.1149, step time: 0.1117\n",
      "177/223, train_loss: 0.1121, step time: 0.0998\n",
      "178/223, train_loss: 0.1196, step time: 0.0995\n",
      "179/223, train_loss: 0.1268, step time: 0.0995\n",
      "180/223, train_loss: 0.1332, step time: 0.1133\n",
      "181/223, train_loss: 0.1273, step time: 0.1002\n",
      "182/223, train_loss: 0.1063, step time: 0.0999\n",
      "183/223, train_loss: 0.1141, step time: 0.1002\n",
      "184/223, train_loss: 0.1263, step time: 0.1026\n",
      "185/223, train_loss: 0.1140, step time: 0.1055\n",
      "186/223, train_loss: 0.1214, step time: 0.1019\n",
      "187/223, train_loss: 0.1091, step time: 0.1526\n",
      "188/223, train_loss: 0.1186, step time: 0.1068\n",
      "189/223, train_loss: 0.1077, step time: 0.1108\n",
      "190/223, train_loss: 0.1212, step time: 0.1000\n",
      "191/223, train_loss: 0.1259, step time: 0.1004\n",
      "192/223, train_loss: 0.1148, step time: 0.1053\n",
      "193/223, train_loss: 0.1239, step time: 0.1132\n",
      "194/223, train_loss: 0.1149, step time: 0.0996\n",
      "195/223, train_loss: 0.1250, step time: 0.1001\n",
      "196/223, train_loss: 0.1273, step time: 0.1009\n",
      "197/223, train_loss: 0.1234, step time: 0.0998\n",
      "198/223, train_loss: 0.1171, step time: 0.1001\n",
      "199/223, train_loss: 0.1190, step time: 0.1007\n",
      "200/223, train_loss: 0.1098, step time: 0.1057\n",
      "201/223, train_loss: 0.1283, step time: 0.1058\n",
      "202/223, train_loss: 0.1123, step time: 0.1016\n",
      "203/223, train_loss: 0.1190, step time: 0.1149\n",
      "204/223, train_loss: 0.1349, step time: 0.1031\n",
      "205/223, train_loss: 0.1152, step time: 0.1235\n",
      "206/223, train_loss: 0.1163, step time: 0.1360\n",
      "207/223, train_loss: 0.1115, step time: 0.1046\n",
      "208/223, train_loss: 0.1175, step time: 0.1010\n",
      "209/223, train_loss: 0.1057, step time: 0.0994\n",
      "210/223, train_loss: 0.1109, step time: 0.1105\n",
      "211/223, train_loss: 0.1121, step time: 0.1132\n",
      "212/223, train_loss: 0.1121, step time: 0.1249\n",
      "213/223, train_loss: 0.1234, step time: 0.0986\n",
      "214/223, train_loss: 0.1279, step time: 0.1211\n",
      "215/223, train_loss: 0.1014, step time: 0.1122\n",
      "216/223, train_loss: 0.1246, step time: 0.1321\n",
      "217/223, train_loss: 0.1192, step time: 0.1014\n",
      "218/223, train_loss: 0.1118, step time: 0.1041\n",
      "219/223, train_loss: 0.1114, step time: 0.1090\n",
      "220/223, train_loss: 0.1182, step time: 0.1000\n",
      "221/223, train_loss: 0.1205, step time: 0.0999\n",
      "222/223, train_loss: 0.1215, step time: 0.0993\n",
      "223/223, train_loss: 0.1310, step time: 0.0996\n",
      "epoch 82 average loss: 0.1202\n",
      "time consuming of epoch 82 is: 92.2432\n",
      "----------\n",
      "epoch 83/300\n",
      "1/223, train_loss: 0.1307, step time: 0.1016\n",
      "2/223, train_loss: 0.1213, step time: 0.1170\n",
      "3/223, train_loss: 0.1188, step time: 0.1304\n",
      "4/223, train_loss: 0.1067, step time: 0.0999\n",
      "5/223, train_loss: 0.1195, step time: 0.1015\n",
      "6/223, train_loss: 0.1101, step time: 0.1315\n",
      "7/223, train_loss: 0.1146, step time: 0.1009\n",
      "8/223, train_loss: 0.1366, step time: 0.1003\n",
      "9/223, train_loss: 0.1307, step time: 0.1088\n",
      "10/223, train_loss: 0.1026, step time: 0.1000\n",
      "11/223, train_loss: 0.1255, step time: 0.0997\n",
      "12/223, train_loss: 0.1322, step time: 0.1094\n",
      "13/223, train_loss: 0.1146, step time: 0.1006\n",
      "14/223, train_loss: 0.1166, step time: 0.0996\n",
      "15/223, train_loss: 0.1145, step time: 0.1004\n",
      "16/223, train_loss: 0.1284, step time: 0.1146\n",
      "17/223, train_loss: 0.1206, step time: 0.1381\n",
      "18/223, train_loss: 0.1248, step time: 0.1012\n",
      "19/223, train_loss: 0.1165, step time: 0.1110\n",
      "20/223, train_loss: 0.1368, step time: 0.1020\n",
      "21/223, train_loss: 0.1113, step time: 0.1102\n",
      "22/223, train_loss: 0.1168, step time: 0.1170\n",
      "23/223, train_loss: 0.1164, step time: 0.1132\n",
      "24/223, train_loss: 0.1100, step time: 0.1147\n",
      "25/223, train_loss: 0.1085, step time: 0.1158\n",
      "26/223, train_loss: 0.1133, step time: 0.1109\n",
      "27/223, train_loss: 0.1130, step time: 0.1231\n",
      "28/223, train_loss: 0.1125, step time: 0.1005\n",
      "29/223, train_loss: 0.1319, step time: 0.1101\n",
      "30/223, train_loss: 0.1305, step time: 0.1098\n",
      "31/223, train_loss: 0.1162, step time: 0.1001\n",
      "32/223, train_loss: 0.1066, step time: 0.0994\n",
      "33/223, train_loss: 0.1140, step time: 0.1206\n",
      "34/223, train_loss: 0.1133, step time: 0.1088\n",
      "35/223, train_loss: 0.1118, step time: 0.1105\n",
      "36/223, train_loss: 0.1158, step time: 0.1006\n",
      "37/223, train_loss: 0.1138, step time: 0.1008\n",
      "38/223, train_loss: 0.1076, step time: 0.1068\n",
      "39/223, train_loss: 0.1241, step time: 0.1114\n",
      "40/223, train_loss: 0.1154, step time: 0.1033\n",
      "41/223, train_loss: 0.1208, step time: 0.1051\n",
      "42/223, train_loss: 0.1200, step time: 0.1325\n",
      "43/223, train_loss: 0.1170, step time: 0.1010\n",
      "44/223, train_loss: 0.1240, step time: 0.0999\n",
      "45/223, train_loss: 0.1260, step time: 0.1157\n",
      "46/223, train_loss: 0.1340, step time: 0.1336\n",
      "47/223, train_loss: 0.1267, step time: 0.1071\n",
      "48/223, train_loss: 0.1105, step time: 0.1006\n",
      "49/223, train_loss: 0.1120, step time: 0.1166\n",
      "50/223, train_loss: 0.1257, step time: 0.1079\n",
      "51/223, train_loss: 0.1071, step time: 0.1117\n",
      "52/223, train_loss: 0.1157, step time: 0.1076\n",
      "53/223, train_loss: 0.1139, step time: 0.1155\n",
      "54/223, train_loss: 0.1156, step time: 0.1073\n",
      "55/223, train_loss: 0.1082, step time: 0.1091\n",
      "56/223, train_loss: 0.1330, step time: 0.1000\n",
      "57/223, train_loss: 0.1080, step time: 0.1088\n",
      "58/223, train_loss: 0.1263, step time: 0.1086\n",
      "59/223, train_loss: 0.1248, step time: 0.1161\n",
      "60/223, train_loss: 0.1071, step time: 0.1004\n",
      "61/223, train_loss: 0.1137, step time: 0.1003\n",
      "62/223, train_loss: 0.1229, step time: 0.1089\n",
      "63/223, train_loss: 0.1373, step time: 0.1146\n",
      "64/223, train_loss: 0.1292, step time: 0.1012\n",
      "65/223, train_loss: 0.1189, step time: 0.1000\n",
      "66/223, train_loss: 0.1238, step time: 0.1038\n",
      "67/223, train_loss: 0.1336, step time: 0.1021\n",
      "68/223, train_loss: 0.1144, step time: 0.1064\n",
      "69/223, train_loss: 0.1445, step time: 0.1103\n",
      "70/223, train_loss: 0.1216, step time: 0.1045\n",
      "71/223, train_loss: 0.3131, step time: 0.1128\n",
      "72/223, train_loss: 0.1074, step time: 0.1005\n",
      "73/223, train_loss: 0.1206, step time: 0.1544\n",
      "74/223, train_loss: 0.1223, step time: 0.1039\n",
      "75/223, train_loss: 0.1173, step time: 0.1065\n",
      "76/223, train_loss: 0.1102, step time: 0.0999\n",
      "77/223, train_loss: 0.1078, step time: 0.1053\n",
      "78/223, train_loss: 0.1116, step time: 0.1048\n",
      "79/223, train_loss: 0.1337, step time: 0.1205\n",
      "80/223, train_loss: 0.1161, step time: 0.1255\n",
      "81/223, train_loss: 0.1188, step time: 0.1055\n",
      "82/223, train_loss: 0.1147, step time: 0.1418\n",
      "83/223, train_loss: 0.1201, step time: 0.1368\n",
      "84/223, train_loss: 0.1336, step time: 0.1198\n",
      "85/223, train_loss: 0.1235, step time: 0.1050\n",
      "86/223, train_loss: 0.1139, step time: 0.1061\n",
      "87/223, train_loss: 0.1121, step time: 0.1038\n",
      "88/223, train_loss: 0.1168, step time: 0.1292\n",
      "89/223, train_loss: 0.1189, step time: 0.1032\n",
      "90/223, train_loss: 0.1186, step time: 0.1037\n",
      "91/223, train_loss: 0.1189, step time: 0.1062\n",
      "92/223, train_loss: 0.1104, step time: 0.1152\n",
      "93/223, train_loss: 0.1169, step time: 0.1002\n",
      "94/223, train_loss: 0.1200, step time: 0.1325\n",
      "95/223, train_loss: 0.1138, step time: 0.1409\n",
      "96/223, train_loss: 0.1257, step time: 0.1116\n",
      "97/223, train_loss: 0.1083, step time: 0.1119\n",
      "98/223, train_loss: 0.1131, step time: 0.1044\n",
      "99/223, train_loss: 0.1115, step time: 0.1046\n",
      "100/223, train_loss: 0.1153, step time: 0.1073\n",
      "101/223, train_loss: 0.1090, step time: 0.1227\n",
      "102/223, train_loss: 0.1154, step time: 0.1200\n",
      "103/223, train_loss: 0.1136, step time: 0.1161\n",
      "104/223, train_loss: 0.1186, step time: 0.1045\n",
      "105/223, train_loss: 0.1209, step time: 0.1058\n",
      "106/223, train_loss: 0.1255, step time: 0.1118\n",
      "107/223, train_loss: 0.1023, step time: 0.1130\n",
      "108/223, train_loss: 0.1271, step time: 0.1155\n",
      "109/223, train_loss: 0.1124, step time: 0.1104\n",
      "110/223, train_loss: 0.1095, step time: 0.1043\n",
      "111/223, train_loss: 0.1213, step time: 0.1046\n",
      "112/223, train_loss: 0.1080, step time: 0.1004\n",
      "113/223, train_loss: 0.1073, step time: 0.1430\n",
      "114/223, train_loss: 0.1159, step time: 0.1230\n",
      "115/223, train_loss: 0.1206, step time: 0.1105\n",
      "116/223, train_loss: 0.1238, step time: 0.1001\n",
      "117/223, train_loss: 0.1263, step time: 0.1126\n",
      "118/223, train_loss: 0.1200, step time: 0.1003\n",
      "119/223, train_loss: 0.1149, step time: 0.1300\n",
      "120/223, train_loss: 0.1245, step time: 0.1007\n",
      "121/223, train_loss: 0.1109, step time: 0.1454\n",
      "122/223, train_loss: 0.1052, step time: 0.1261\n",
      "123/223, train_loss: 0.1114, step time: 0.1000\n",
      "124/223, train_loss: 0.1109, step time: 0.1005\n",
      "125/223, train_loss: 0.1181, step time: 0.1029\n",
      "126/223, train_loss: 0.1131, step time: 0.1059\n",
      "127/223, train_loss: 0.1248, step time: 0.1016\n",
      "128/223, train_loss: 0.1157, step time: 0.1017\n",
      "129/223, train_loss: 0.1285, step time: 0.0997\n",
      "130/223, train_loss: 0.1098, step time: 0.1243\n",
      "131/223, train_loss: 0.1030, step time: 0.0993\n",
      "132/223, train_loss: 0.1160, step time: 0.1064\n",
      "133/223, train_loss: 0.1100, step time: 0.1027\n",
      "134/223, train_loss: 0.1234, step time: 0.1037\n",
      "135/223, train_loss: 0.1409, step time: 0.0991\n",
      "136/223, train_loss: 0.1168, step time: 0.0991\n",
      "137/223, train_loss: 0.1303, step time: 0.1071\n",
      "138/223, train_loss: 0.1158, step time: 0.1007\n",
      "139/223, train_loss: 0.1123, step time: 0.1174\n",
      "140/223, train_loss: 0.1197, step time: 0.1172\n",
      "141/223, train_loss: 0.1060, step time: 0.1021\n",
      "142/223, train_loss: 0.1208, step time: 0.1009\n",
      "143/223, train_loss: 0.1100, step time: 0.1003\n",
      "144/223, train_loss: 0.1183, step time: 0.1009\n",
      "145/223, train_loss: 0.1088, step time: 0.1040\n",
      "146/223, train_loss: 0.1216, step time: 0.1303\n",
      "147/223, train_loss: 0.1089, step time: 0.1006\n",
      "148/223, train_loss: 0.1217, step time: 0.0993\n",
      "149/223, train_loss: 0.1178, step time: 0.1128\n",
      "150/223, train_loss: 0.1231, step time: 0.1003\n",
      "151/223, train_loss: 0.1175, step time: 0.1051\n",
      "152/223, train_loss: 0.1104, step time: 0.1091\n",
      "153/223, train_loss: 0.1200, step time: 0.1187\n",
      "154/223, train_loss: 0.1266, step time: 0.1171\n",
      "155/223, train_loss: 0.1295, step time: 0.1008\n",
      "156/223, train_loss: 0.1145, step time: 0.1007\n",
      "157/223, train_loss: 0.1158, step time: 0.1000\n",
      "158/223, train_loss: 0.1006, step time: 0.1046\n",
      "159/223, train_loss: 0.1181, step time: 0.1008\n",
      "160/223, train_loss: 0.1223, step time: 0.1075\n",
      "161/223, train_loss: 0.1025, step time: 0.1076\n",
      "162/223, train_loss: 0.1261, step time: 0.1242\n",
      "163/223, train_loss: 0.1130, step time: 0.1134\n",
      "164/223, train_loss: 0.1080, step time: 0.1099\n",
      "165/223, train_loss: 0.1197, step time: 0.1349\n",
      "166/223, train_loss: 0.1122, step time: 0.1011\n",
      "167/223, train_loss: 0.1316, step time: 0.1018\n",
      "168/223, train_loss: 0.1087, step time: 0.1033\n",
      "169/223, train_loss: 0.1153, step time: 0.1094\n",
      "170/223, train_loss: 0.1145, step time: 0.1006\n",
      "171/223, train_loss: 0.1151, step time: 0.1037\n",
      "172/223, train_loss: 0.1117, step time: 0.1039\n",
      "173/223, train_loss: 0.1175, step time: 0.1000\n",
      "174/223, train_loss: 0.1170, step time: 0.1106\n",
      "175/223, train_loss: 0.1151, step time: 0.1022\n",
      "176/223, train_loss: 0.1237, step time: 0.1000\n",
      "177/223, train_loss: 0.1246, step time: 0.1264\n",
      "178/223, train_loss: 0.1059, step time: 0.1150\n",
      "179/223, train_loss: 0.1174, step time: 0.1171\n",
      "180/223, train_loss: 0.1103, step time: 0.1070\n",
      "181/223, train_loss: 0.1276, step time: 0.1170\n",
      "182/223, train_loss: 0.1053, step time: 0.1159\n",
      "183/223, train_loss: 0.1167, step time: 0.1007\n",
      "184/223, train_loss: 0.1312, step time: 0.1091\n",
      "185/223, train_loss: 0.1016, step time: 0.1052\n",
      "186/223, train_loss: 0.1195, step time: 0.1212\n",
      "187/223, train_loss: 0.1109, step time: 0.1062\n",
      "188/223, train_loss: 0.1384, step time: 0.1439\n",
      "189/223, train_loss: 0.1089, step time: 0.1004\n",
      "190/223, train_loss: 0.1192, step time: 0.1477\n",
      "191/223, train_loss: 0.1327, step time: 0.1464\n",
      "192/223, train_loss: 0.1443, step time: 0.1038\n",
      "193/223, train_loss: 0.1207, step time: 0.1026\n",
      "194/223, train_loss: 0.1108, step time: 0.1000\n",
      "195/223, train_loss: 0.1153, step time: 0.1184\n",
      "196/223, train_loss: 0.1258, step time: 0.1157\n",
      "197/223, train_loss: 0.1228, step time: 0.1046\n",
      "198/223, train_loss: 0.1246, step time: 0.1353\n",
      "199/223, train_loss: 0.1151, step time: 0.1119\n",
      "200/223, train_loss: 0.1192, step time: 0.1248\n",
      "201/223, train_loss: 0.1275, step time: 0.1051\n",
      "202/223, train_loss: 0.1067, step time: 0.1005\n",
      "203/223, train_loss: 0.1125, step time: 0.1079\n",
      "204/223, train_loss: 0.1196, step time: 0.1123\n",
      "205/223, train_loss: 0.1212, step time: 0.1116\n",
      "206/223, train_loss: 0.1161, step time: 0.1122\n",
      "207/223, train_loss: 0.1062, step time: 0.1087\n",
      "208/223, train_loss: 0.1103, step time: 0.1016\n",
      "209/223, train_loss: 0.1194, step time: 0.1252\n",
      "210/223, train_loss: 0.1080, step time: 0.1190\n",
      "211/223, train_loss: 0.1108, step time: 0.1129\n",
      "212/223, train_loss: 0.1325, step time: 0.1098\n",
      "213/223, train_loss: 0.1182, step time: 0.1125\n",
      "214/223, train_loss: 0.1356, step time: 0.1002\n",
      "215/223, train_loss: 0.1093, step time: 0.1246\n",
      "216/223, train_loss: 0.1163, step time: 0.1164\n",
      "217/223, train_loss: 0.1255, step time: 0.1046\n",
      "218/223, train_loss: 0.1306, step time: 0.1008\n",
      "219/223, train_loss: 0.1138, step time: 0.1002\n",
      "220/223, train_loss: 0.1220, step time: 0.1009\n",
      "221/223, train_loss: 0.1135, step time: 0.0988\n",
      "222/223, train_loss: 0.1234, step time: 0.0997\n",
      "223/223, train_loss: 0.1177, step time: 0.0998\n",
      "epoch 83 average loss: 0.1189\n",
      "time consuming of epoch 83 is: 86.9242\n",
      "----------\n",
      "epoch 84/300\n",
      "1/223, train_loss: 0.1073, step time: 0.1044\n",
      "2/223, train_loss: 0.1286, step time: 0.1003\n",
      "3/223, train_loss: 0.1075, step time: 0.1036\n",
      "4/223, train_loss: 0.1102, step time: 0.1214\n",
      "5/223, train_loss: 0.1121, step time: 0.1018\n",
      "6/223, train_loss: 0.1223, step time: 0.1021\n",
      "7/223, train_loss: 0.1138, step time: 0.1021\n",
      "8/223, train_loss: 0.1175, step time: 0.1007\n",
      "9/223, train_loss: 0.1156, step time: 0.1380\n",
      "10/223, train_loss: 0.1188, step time: 0.1005\n",
      "11/223, train_loss: 0.1226, step time: 0.1011\n",
      "12/223, train_loss: 0.1372, step time: 0.1071\n",
      "13/223, train_loss: 0.1216, step time: 0.1078\n",
      "14/223, train_loss: 0.1266, step time: 0.1008\n",
      "15/223, train_loss: 0.1105, step time: 0.1004\n",
      "16/223, train_loss: 0.1108, step time: 0.1169\n",
      "17/223, train_loss: 0.1136, step time: 0.1155\n",
      "18/223, train_loss: 0.1202, step time: 0.0998\n",
      "19/223, train_loss: 0.1140, step time: 0.0996\n",
      "20/223, train_loss: 0.1167, step time: 0.1006\n",
      "21/223, train_loss: 0.1039, step time: 0.1134\n",
      "22/223, train_loss: 0.1227, step time: 0.1276\n",
      "23/223, train_loss: 0.1092, step time: 0.1192\n",
      "24/223, train_loss: 0.1190, step time: 0.1070\n",
      "25/223, train_loss: 0.1124, step time: 0.1121\n",
      "26/223, train_loss: 0.1293, step time: 0.1105\n",
      "27/223, train_loss: 0.1091, step time: 0.1107\n",
      "28/223, train_loss: 0.1305, step time: 0.1158\n",
      "29/223, train_loss: 0.1324, step time: 0.1039\n",
      "30/223, train_loss: 0.1327, step time: 0.1258\n",
      "31/223, train_loss: 0.1140, step time: 0.1238\n",
      "32/223, train_loss: 0.1175, step time: 0.1113\n",
      "33/223, train_loss: 0.1300, step time: 0.1134\n",
      "34/223, train_loss: 0.1060, step time: 0.1354\n",
      "35/223, train_loss: 0.1063, step time: 0.1160\n",
      "36/223, train_loss: 0.1135, step time: 0.1006\n",
      "37/223, train_loss: 0.1221, step time: 0.1050\n",
      "38/223, train_loss: 0.1085, step time: 0.1003\n",
      "39/223, train_loss: 0.1144, step time: 0.1011\n",
      "40/223, train_loss: 0.1111, step time: 0.1096\n",
      "41/223, train_loss: 0.1126, step time: 0.1037\n",
      "42/223, train_loss: 0.1013, step time: 0.1017\n",
      "43/223, train_loss: 0.1154, step time: 0.1083\n",
      "44/223, train_loss: 0.1056, step time: 0.1004\n",
      "45/223, train_loss: 0.1121, step time: 0.1146\n",
      "46/223, train_loss: 0.1143, step time: 0.0997\n",
      "47/223, train_loss: 0.1067, step time: 0.1007\n",
      "48/223, train_loss: 0.1201, step time: 0.1206\n",
      "49/223, train_loss: 0.1125, step time: 0.1034\n",
      "50/223, train_loss: 0.1082, step time: 0.1000\n",
      "51/223, train_loss: 0.1094, step time: 0.0999\n",
      "52/223, train_loss: 0.1040, step time: 0.1101\n",
      "53/223, train_loss: 0.1154, step time: 0.0998\n",
      "54/223, train_loss: 0.1210, step time: 0.1014\n",
      "55/223, train_loss: 0.1017, step time: 0.1014\n",
      "56/223, train_loss: 0.1178, step time: 0.1035\n",
      "57/223, train_loss: 0.1189, step time: 0.1013\n",
      "58/223, train_loss: 0.1162, step time: 0.1101\n",
      "59/223, train_loss: 0.1237, step time: 0.1113\n",
      "60/223, train_loss: 0.1244, step time: 0.1035\n",
      "61/223, train_loss: 0.3068, step time: 0.1085\n",
      "62/223, train_loss: 0.1379, step time: 0.1136\n",
      "63/223, train_loss: 0.1146, step time: 0.1007\n",
      "64/223, train_loss: 0.1244, step time: 0.1064\n",
      "65/223, train_loss: 0.1256, step time: 0.1129\n",
      "66/223, train_loss: 0.1415, step time: 0.1173\n",
      "67/223, train_loss: 0.1223, step time: 0.1281\n",
      "68/223, train_loss: 0.1256, step time: 0.1136\n",
      "69/223, train_loss: 0.1132, step time: 0.1405\n",
      "70/223, train_loss: 0.1338, step time: 0.1261\n",
      "71/223, train_loss: 0.1092, step time: 0.1078\n",
      "72/223, train_loss: 0.1096, step time: 0.1010\n",
      "73/223, train_loss: 0.1171, step time: 0.1229\n",
      "74/223, train_loss: 0.1130, step time: 0.1050\n",
      "75/223, train_loss: 0.1264, step time: 0.0997\n",
      "76/223, train_loss: 0.1140, step time: 0.1113\n",
      "77/223, train_loss: 0.1272, step time: 0.1302\n",
      "78/223, train_loss: 0.1127, step time: 0.1081\n",
      "79/223, train_loss: 0.1340, step time: 0.1091\n",
      "80/223, train_loss: 0.1117, step time: 0.1037\n",
      "81/223, train_loss: 0.1234, step time: 0.1132\n",
      "82/223, train_loss: 0.1257, step time: 0.0997\n",
      "83/223, train_loss: 0.1107, step time: 0.1011\n",
      "84/223, train_loss: 0.1111, step time: 0.1030\n",
      "85/223, train_loss: 0.1273, step time: 0.1054\n",
      "86/223, train_loss: 0.1082, step time: 0.0999\n",
      "87/223, train_loss: 0.1186, step time: 0.1084\n",
      "88/223, train_loss: 0.1345, step time: 0.1004\n",
      "89/223, train_loss: 0.1099, step time: 0.1047\n",
      "90/223, train_loss: 0.1259, step time: 0.1058\n",
      "91/223, train_loss: 0.1211, step time: 0.1113\n",
      "92/223, train_loss: 0.1364, step time: 0.1001\n",
      "93/223, train_loss: 0.1215, step time: 0.1002\n",
      "94/223, train_loss: 0.1322, step time: 0.1009\n",
      "95/223, train_loss: 0.1056, step time: 0.0998\n",
      "96/223, train_loss: 0.1145, step time: 0.1067\n",
      "97/223, train_loss: 0.1193, step time: 0.1100\n",
      "98/223, train_loss: 0.1251, step time: 0.1483\n",
      "99/223, train_loss: 0.1097, step time: 0.1265\n",
      "100/223, train_loss: 0.1084, step time: 0.1106\n",
      "101/223, train_loss: 0.1046, step time: 0.1211\n",
      "102/223, train_loss: 0.1179, step time: 0.1197\n",
      "103/223, train_loss: 0.1176, step time: 0.1027\n",
      "104/223, train_loss: 0.1035, step time: 0.1001\n",
      "105/223, train_loss: 0.1144, step time: 0.1063\n",
      "106/223, train_loss: 0.1279, step time: 0.1000\n",
      "107/223, train_loss: 0.1234, step time: 0.1007\n",
      "108/223, train_loss: 0.1228, step time: 0.1008\n",
      "109/223, train_loss: 0.1198, step time: 0.1081\n",
      "110/223, train_loss: 0.1265, step time: 0.0996\n",
      "111/223, train_loss: 0.1075, step time: 0.1208\n",
      "112/223, train_loss: 0.1271, step time: 0.1172\n",
      "113/223, train_loss: 0.1204, step time: 0.1204\n",
      "114/223, train_loss: 0.1088, step time: 0.1161\n",
      "115/223, train_loss: 0.1156, step time: 0.1055\n",
      "116/223, train_loss: 0.1090, step time: 0.1104\n",
      "117/223, train_loss: 0.1162, step time: 0.1202\n",
      "118/223, train_loss: 0.1282, step time: 0.1235\n",
      "119/223, train_loss: 0.1200, step time: 0.1066\n",
      "120/223, train_loss: 0.1159, step time: 0.1059\n",
      "121/223, train_loss: 0.1030, step time: 0.1272\n",
      "122/223, train_loss: 0.1241, step time: 0.1085\n",
      "123/223, train_loss: 0.1225, step time: 0.1033\n",
      "124/223, train_loss: 0.1175, step time: 0.1236\n",
      "125/223, train_loss: 0.1179, step time: 0.1064\n",
      "126/223, train_loss: 0.1278, step time: 0.1017\n",
      "127/223, train_loss: 0.1097, step time: 0.1331\n",
      "128/223, train_loss: 0.1173, step time: 0.1431\n",
      "129/223, train_loss: 0.1165, step time: 0.0995\n",
      "130/223, train_loss: 0.1082, step time: 0.0992\n",
      "131/223, train_loss: 0.1122, step time: 0.1003\n",
      "132/223, train_loss: 0.1221, step time: 0.1043\n",
      "133/223, train_loss: 0.1157, step time: 0.1184\n",
      "134/223, train_loss: 0.1119, step time: 0.1173\n",
      "135/223, train_loss: 0.1084, step time: 0.1111\n",
      "136/223, train_loss: 0.1209, step time: 0.1135\n",
      "137/223, train_loss: 0.1217, step time: 0.1001\n",
      "138/223, train_loss: 0.1217, step time: 0.1016\n",
      "139/223, train_loss: 0.1201, step time: 0.1020\n",
      "140/223, train_loss: 0.1108, step time: 0.1128\n",
      "141/223, train_loss: 0.1073, step time: 0.1007\n",
      "142/223, train_loss: 0.1115, step time: 0.1177\n",
      "143/223, train_loss: 0.1142, step time: 0.1234\n",
      "144/223, train_loss: 0.1095, step time: 0.1205\n",
      "145/223, train_loss: 0.1201, step time: 0.1121\n",
      "146/223, train_loss: 0.1191, step time: 0.1232\n",
      "147/223, train_loss: 0.1117, step time: 0.1097\n",
      "148/223, train_loss: 0.1101, step time: 0.1019\n",
      "149/223, train_loss: 0.1209, step time: 0.1026\n",
      "150/223, train_loss: 0.1155, step time: 0.1206\n",
      "151/223, train_loss: 0.1144, step time: 0.1067\n",
      "152/223, train_loss: 0.1185, step time: 0.0991\n",
      "153/223, train_loss: 0.1176, step time: 0.1111\n",
      "154/223, train_loss: 0.1087, step time: 0.1102\n",
      "155/223, train_loss: 0.1185, step time: 0.1136\n",
      "156/223, train_loss: 0.1175, step time: 0.0991\n",
      "157/223, train_loss: 0.1060, step time: 0.1047\n",
      "158/223, train_loss: 0.1309, step time: 0.1063\n",
      "159/223, train_loss: 0.1299, step time: 0.1151\n",
      "160/223, train_loss: 0.1124, step time: 0.1069\n",
      "161/223, train_loss: 0.1217, step time: 0.0994\n",
      "162/223, train_loss: 0.1129, step time: 0.1138\n",
      "163/223, train_loss: 0.1145, step time: 0.1335\n",
      "164/223, train_loss: 0.1249, step time: 0.1155\n",
      "165/223, train_loss: 0.1388, step time: 0.1028\n",
      "166/223, train_loss: 0.1250, step time: 0.0998\n",
      "167/223, train_loss: 0.1091, step time: 0.1051\n",
      "168/223, train_loss: 0.1138, step time: 0.1047\n",
      "169/223, train_loss: 0.1042, step time: 0.1042\n",
      "170/223, train_loss: 0.1259, step time: 0.1012\n",
      "171/223, train_loss: 0.1146, step time: 0.1138\n",
      "172/223, train_loss: 0.1174, step time: 0.1163\n",
      "173/223, train_loss: 0.1515, step time: 0.1086\n",
      "174/223, train_loss: 0.1258, step time: 0.1690\n",
      "175/223, train_loss: 0.1169, step time: 0.1008\n",
      "176/223, train_loss: 0.1180, step time: 0.1071\n",
      "177/223, train_loss: 0.1078, step time: 0.1059\n",
      "178/223, train_loss: 0.1111, step time: 0.1009\n",
      "179/223, train_loss: 0.1040, step time: 0.1076\n",
      "180/223, train_loss: 0.1204, step time: 0.1084\n",
      "181/223, train_loss: 0.1231, step time: 0.1181\n",
      "182/223, train_loss: 0.1354, step time: 0.1002\n",
      "183/223, train_loss: 0.1198, step time: 0.1077\n",
      "184/223, train_loss: 0.1142, step time: 0.1005\n",
      "185/223, train_loss: 0.1321, step time: 0.1007\n",
      "186/223, train_loss: 0.1173, step time: 0.1006\n",
      "187/223, train_loss: 0.1160, step time: 0.1000\n",
      "188/223, train_loss: 0.1190, step time: 0.1005\n",
      "189/223, train_loss: 0.1204, step time: 0.1003\n",
      "190/223, train_loss: 0.1228, step time: 0.0998\n",
      "191/223, train_loss: 0.1099, step time: 0.1011\n",
      "192/223, train_loss: 0.1071, step time: 0.0997\n",
      "193/223, train_loss: 0.1192, step time: 0.1059\n",
      "194/223, train_loss: 0.1232, step time: 0.0988\n",
      "195/223, train_loss: 0.1078, step time: 0.0987\n",
      "196/223, train_loss: 0.1157, step time: 0.1000\n",
      "197/223, train_loss: 0.1209, step time: 0.1061\n",
      "198/223, train_loss: 0.1148, step time: 0.0993\n",
      "199/223, train_loss: 0.1099, step time: 0.0994\n",
      "200/223, train_loss: 0.1130, step time: 0.0992\n",
      "201/223, train_loss: 0.1264, step time: 0.1014\n",
      "202/223, train_loss: 0.1085, step time: 0.0991\n",
      "203/223, train_loss: 0.1160, step time: 0.0995\n",
      "204/223, train_loss: 0.1383, step time: 0.1001\n",
      "205/223, train_loss: 0.1191, step time: 0.1013\n",
      "206/223, train_loss: 0.1062, step time: 0.0996\n",
      "207/223, train_loss: 0.1092, step time: 0.1007\n",
      "208/223, train_loss: 0.1023, step time: 0.0993\n",
      "209/223, train_loss: 0.1293, step time: 0.1080\n",
      "210/223, train_loss: 0.1052, step time: 0.1003\n",
      "211/223, train_loss: 0.1218, step time: 0.0993\n",
      "212/223, train_loss: 0.1146, step time: 0.1000\n",
      "213/223, train_loss: 0.1119, step time: 0.1100\n",
      "214/223, train_loss: 0.1088, step time: 0.1122\n",
      "215/223, train_loss: 0.1077, step time: 0.1050\n",
      "216/223, train_loss: 0.1171, step time: 0.1343\n",
      "217/223, train_loss: 0.1089, step time: 0.1012\n",
      "218/223, train_loss: 0.1286, step time: 0.1005\n",
      "219/223, train_loss: 0.1224, step time: 0.0998\n",
      "220/223, train_loss: 0.1093, step time: 0.0997\n",
      "221/223, train_loss: 0.1246, step time: 0.1070\n",
      "222/223, train_loss: 0.1164, step time: 0.0997\n",
      "223/223, train_loss: 0.1201, step time: 0.0992\n",
      "epoch 84 average loss: 0.1182\n",
      "time consuming of epoch 84 is: 94.2818\n",
      "----------\n",
      "epoch 85/300\n",
      "1/223, train_loss: 0.1118, step time: 0.1071\n",
      "2/223, train_loss: 0.1181, step time: 0.1057\n",
      "3/223, train_loss: 0.1110, step time: 0.1270\n",
      "4/223, train_loss: 0.1057, step time: 0.1014\n",
      "5/223, train_loss: 0.1146, step time: 0.1050\n",
      "6/223, train_loss: 0.1297, step time: 0.1120\n",
      "7/223, train_loss: 0.1191, step time: 0.1006\n",
      "8/223, train_loss: 0.1069, step time: 0.1121\n",
      "9/223, train_loss: 0.1263, step time: 0.1143\n",
      "10/223, train_loss: 0.1158, step time: 0.1219\n",
      "11/223, train_loss: 0.1278, step time: 0.1155\n",
      "12/223, train_loss: 0.1156, step time: 0.1068\n",
      "13/223, train_loss: 0.1166, step time: 0.1039\n",
      "14/223, train_loss: 0.1217, step time: 0.1308\n",
      "15/223, train_loss: 0.1183, step time: 0.1047\n",
      "16/223, train_loss: 0.1223, step time: 0.1003\n",
      "17/223, train_loss: 0.1124, step time: 0.1172\n",
      "18/223, train_loss: 0.1209, step time: 0.1132\n",
      "19/223, train_loss: 0.1171, step time: 0.1085\n",
      "20/223, train_loss: 0.1161, step time: 0.1278\n",
      "21/223, train_loss: 0.1199, step time: 0.1021\n",
      "22/223, train_loss: 0.1158, step time: 0.0998\n",
      "23/223, train_loss: 0.1134, step time: 0.1008\n",
      "24/223, train_loss: 0.1095, step time: 0.1230\n",
      "25/223, train_loss: 0.1139, step time: 0.1040\n",
      "26/223, train_loss: 0.1243, step time: 0.1289\n",
      "27/223, train_loss: 0.1172, step time: 0.1087\n",
      "28/223, train_loss: 0.1204, step time: 0.1056\n",
      "29/223, train_loss: 0.1070, step time: 0.1138\n",
      "30/223, train_loss: 0.1184, step time: 0.1191\n",
      "31/223, train_loss: 0.1152, step time: 0.0999\n",
      "32/223, train_loss: 0.1231, step time: 0.1012\n",
      "33/223, train_loss: 0.1045, step time: 0.1042\n",
      "34/223, train_loss: 0.1132, step time: 0.1068\n",
      "35/223, train_loss: 0.1144, step time: 0.0999\n",
      "36/223, train_loss: 0.1324, step time: 0.1064\n",
      "37/223, train_loss: 0.1290, step time: 0.1206\n",
      "38/223, train_loss: 0.1227, step time: 0.1159\n",
      "39/223, train_loss: 0.1094, step time: 0.1251\n",
      "40/223, train_loss: 0.1278, step time: 0.1220\n",
      "41/223, train_loss: 0.1099, step time: 0.1131\n",
      "42/223, train_loss: 0.1158, step time: 0.1073\n",
      "43/223, train_loss: 0.1091, step time: 0.1149\n",
      "44/223, train_loss: 0.1112, step time: 0.1010\n",
      "45/223, train_loss: 0.1253, step time: 0.1048\n",
      "46/223, train_loss: 0.1119, step time: 0.1244\n",
      "47/223, train_loss: 0.1152, step time: 0.1247\n",
      "48/223, train_loss: 0.1307, step time: 0.1306\n",
      "49/223, train_loss: 0.1249, step time: 0.1187\n",
      "50/223, train_loss: 0.1139, step time: 0.1159\n",
      "51/223, train_loss: 0.1075, step time: 0.1356\n",
      "52/223, train_loss: 0.1061, step time: 0.1123\n",
      "53/223, train_loss: 0.1192, step time: 0.1123\n",
      "54/223, train_loss: 0.1177, step time: 0.1054\n",
      "55/223, train_loss: 0.1100, step time: 0.1101\n",
      "56/223, train_loss: 0.1155, step time: 0.1003\n",
      "57/223, train_loss: 0.1120, step time: 0.1250\n",
      "58/223, train_loss: 0.1202, step time: 0.1205\n",
      "59/223, train_loss: 0.1220, step time: 0.1083\n",
      "60/223, train_loss: 0.1296, step time: 0.1009\n",
      "61/223, train_loss: 0.1141, step time: 0.1019\n",
      "62/223, train_loss: 0.1143, step time: 0.1029\n",
      "63/223, train_loss: 0.1180, step time: 0.1269\n",
      "64/223, train_loss: 0.1237, step time: 0.1221\n",
      "65/223, train_loss: 0.1127, step time: 0.1180\n",
      "66/223, train_loss: 0.1173, step time: 0.1014\n",
      "67/223, train_loss: 0.1089, step time: 0.1061\n",
      "68/223, train_loss: 0.1194, step time: 0.1191\n",
      "69/223, train_loss: 0.1263, step time: 0.1162\n",
      "70/223, train_loss: 0.1310, step time: 0.1175\n",
      "71/223, train_loss: 0.1106, step time: 0.1125\n",
      "72/223, train_loss: 0.1256, step time: 0.1119\n",
      "73/223, train_loss: 0.1234, step time: 0.1087\n",
      "74/223, train_loss: 0.1103, step time: 0.1115\n",
      "75/223, train_loss: 0.1148, step time: 0.1180\n",
      "76/223, train_loss: 0.1040, step time: 0.1114\n",
      "77/223, train_loss: 0.1083, step time: 0.1171\n",
      "78/223, train_loss: 0.1192, step time: 0.1150\n",
      "79/223, train_loss: 0.1225, step time: 0.1037\n",
      "80/223, train_loss: 0.1164, step time: 0.1216\n",
      "81/223, train_loss: 0.1335, step time: 0.1261\n",
      "82/223, train_loss: 0.1285, step time: 0.1326\n",
      "83/223, train_loss: 0.1481, step time: 0.1020\n",
      "84/223, train_loss: 0.1279, step time: 0.1114\n",
      "85/223, train_loss: 0.1117, step time: 0.1072\n",
      "86/223, train_loss: 0.1206, step time: 0.1194\n",
      "87/223, train_loss: 0.1278, step time: 0.1018\n",
      "88/223, train_loss: 0.1175, step time: 0.1001\n",
      "89/223, train_loss: 0.1163, step time: 0.1109\n",
      "90/223, train_loss: 0.1086, step time: 0.0999\n",
      "91/223, train_loss: 0.1158, step time: 0.1157\n",
      "92/223, train_loss: 0.1292, step time: 0.0996\n",
      "93/223, train_loss: 0.1105, step time: 0.1156\n",
      "94/223, train_loss: 0.1061, step time: 0.1009\n",
      "95/223, train_loss: 0.1126, step time: 0.1005\n",
      "96/223, train_loss: 0.1201, step time: 0.1013\n",
      "97/223, train_loss: 0.1217, step time: 0.1113\n",
      "98/223, train_loss: 0.1013, step time: 0.1054\n",
      "99/223, train_loss: 0.1427, step time: 0.1217\n",
      "100/223, train_loss: 0.1127, step time: 0.1009\n",
      "101/223, train_loss: 0.1042, step time: 0.1044\n",
      "102/223, train_loss: 0.1230, step time: 0.1003\n",
      "103/223, train_loss: 0.1210, step time: 0.1001\n",
      "104/223, train_loss: 0.1131, step time: 0.1237\n",
      "105/223, train_loss: 0.1137, step time: 0.1038\n",
      "106/223, train_loss: 0.1175, step time: 0.1109\n",
      "107/223, train_loss: 0.1247, step time: 0.1003\n",
      "108/223, train_loss: 0.1358, step time: 0.1019\n",
      "109/223, train_loss: 0.1277, step time: 0.1020\n",
      "110/223, train_loss: 0.1249, step time: 0.0995\n",
      "111/223, train_loss: 0.1346, step time: 0.1005\n",
      "112/223, train_loss: 0.1183, step time: 0.1023\n",
      "113/223, train_loss: 0.1146, step time: 0.0997\n",
      "114/223, train_loss: 0.1200, step time: 0.0988\n",
      "115/223, train_loss: 0.1239, step time: 0.0991\n",
      "116/223, train_loss: 0.1051, step time: 0.0999\n",
      "117/223, train_loss: 0.1188, step time: 0.1002\n",
      "118/223, train_loss: 0.1142, step time: 0.0996\n",
      "119/223, train_loss: 0.1124, step time: 0.0997\n",
      "120/223, train_loss: 0.1046, step time: 0.1014\n",
      "121/223, train_loss: 0.1156, step time: 0.1002\n",
      "122/223, train_loss: 0.1086, step time: 0.0995\n",
      "123/223, train_loss: 0.1143, step time: 0.0999\n",
      "124/223, train_loss: 0.1039, step time: 0.1047\n",
      "125/223, train_loss: 0.1160, step time: 0.1009\n",
      "126/223, train_loss: 0.1199, step time: 0.1004\n",
      "127/223, train_loss: 0.1061, step time: 0.1004\n",
      "128/223, train_loss: 0.1072, step time: 0.1067\n",
      "129/223, train_loss: 0.1101, step time: 0.1000\n",
      "130/223, train_loss: 0.1307, step time: 0.1066\n",
      "131/223, train_loss: 0.1197, step time: 0.1054\n",
      "132/223, train_loss: 0.1289, step time: 0.1043\n",
      "133/223, train_loss: 0.1081, step time: 0.1083\n",
      "134/223, train_loss: 0.1273, step time: 0.1043\n",
      "135/223, train_loss: 0.1175, step time: 0.1128\n",
      "136/223, train_loss: 0.1334, step time: 0.1080\n",
      "137/223, train_loss: 0.1070, step time: 0.0997\n",
      "138/223, train_loss: 0.1118, step time: 0.1194\n",
      "139/223, train_loss: 0.1087, step time: 0.0999\n",
      "140/223, train_loss: 0.1150, step time: 0.1089\n",
      "141/223, train_loss: 0.1133, step time: 0.1179\n",
      "142/223, train_loss: 0.1167, step time: 0.1104\n",
      "143/223, train_loss: 0.1086, step time: 0.0997\n",
      "144/223, train_loss: 0.1333, step time: 0.1020\n",
      "145/223, train_loss: 0.1108, step time: 0.1191\n",
      "146/223, train_loss: 0.1198, step time: 0.1098\n",
      "147/223, train_loss: 0.1187, step time: 0.1113\n",
      "148/223, train_loss: 0.1178, step time: 0.1010\n",
      "149/223, train_loss: 0.1225, step time: 0.1165\n",
      "150/223, train_loss: 0.1146, step time: 0.1069\n",
      "151/223, train_loss: 0.1032, step time: 0.1028\n",
      "152/223, train_loss: 0.1064, step time: 0.1032\n",
      "153/223, train_loss: 0.1076, step time: 0.1007\n",
      "154/223, train_loss: 0.1203, step time: 0.1016\n",
      "155/223, train_loss: 0.1175, step time: 0.1009\n",
      "156/223, train_loss: 0.1026, step time: 0.1293\n",
      "157/223, train_loss: 0.1069, step time: 0.1084\n",
      "158/223, train_loss: 0.1118, step time: 0.1004\n",
      "159/223, train_loss: 0.3218, step time: 0.1039\n",
      "160/223, train_loss: 0.1048, step time: 0.1000\n",
      "161/223, train_loss: 0.1026, step time: 0.1189\n",
      "162/223, train_loss: 0.1057, step time: 0.1000\n",
      "163/223, train_loss: 0.1168, step time: 0.1086\n",
      "164/223, train_loss: 0.1159, step time: 0.1008\n",
      "165/223, train_loss: 0.1104, step time: 0.1307\n",
      "166/223, train_loss: 0.1067, step time: 0.1113\n",
      "167/223, train_loss: 0.1168, step time: 0.1145\n",
      "168/223, train_loss: 0.1254, step time: 0.1403\n",
      "169/223, train_loss: 0.1306, step time: 0.1118\n",
      "170/223, train_loss: 0.1263, step time: 0.1001\n",
      "171/223, train_loss: 0.1144, step time: 0.1070\n",
      "172/223, train_loss: 0.1233, step time: 0.1312\n",
      "173/223, train_loss: 0.1171, step time: 0.1072\n",
      "174/223, train_loss: 0.1106, step time: 0.1141\n",
      "175/223, train_loss: 0.1176, step time: 0.1054\n",
      "176/223, train_loss: 0.1049, step time: 0.1092\n",
      "177/223, train_loss: 0.1074, step time: 0.1002\n",
      "178/223, train_loss: 0.1189, step time: 0.1134\n",
      "179/223, train_loss: 0.1238, step time: 0.1203\n",
      "180/223, train_loss: 0.1100, step time: 0.1020\n",
      "181/223, train_loss: 0.1086, step time: 0.1163\n",
      "182/223, train_loss: 0.1220, step time: 0.1088\n",
      "183/223, train_loss: 0.1077, step time: 0.1161\n",
      "184/223, train_loss: 0.1159, step time: 0.1140\n",
      "185/223, train_loss: 0.1418, step time: 0.1119\n",
      "186/223, train_loss: 0.1222, step time: 0.1187\n",
      "187/223, train_loss: 0.1152, step time: 0.1006\n",
      "188/223, train_loss: 0.1084, step time: 0.1129\n",
      "189/223, train_loss: 0.1317, step time: 0.1162\n",
      "190/223, train_loss: 0.1228, step time: 0.1120\n",
      "191/223, train_loss: 0.1121, step time: 0.1124\n",
      "192/223, train_loss: 0.1123, step time: 0.1043\n",
      "193/223, train_loss: 0.1138, step time: 0.1083\n",
      "194/223, train_loss: 0.1355, step time: 0.1003\n",
      "195/223, train_loss: 0.1332, step time: 0.1004\n",
      "196/223, train_loss: 0.1310, step time: 0.1025\n",
      "197/223, train_loss: 0.1294, step time: 0.1107\n",
      "198/223, train_loss: 0.1105, step time: 0.1317\n",
      "199/223, train_loss: 0.1249, step time: 0.1276\n",
      "200/223, train_loss: 0.1216, step time: 0.0998\n",
      "201/223, train_loss: 0.1192, step time: 0.1001\n",
      "202/223, train_loss: 0.1055, step time: 0.1049\n",
      "203/223, train_loss: 0.1295, step time: 0.1112\n",
      "204/223, train_loss: 0.1240, step time: 0.1026\n",
      "205/223, train_loss: 0.1144, step time: 0.1127\n",
      "206/223, train_loss: 0.1059, step time: 0.1074\n",
      "207/223, train_loss: 0.1166, step time: 0.1098\n",
      "208/223, train_loss: 0.1268, step time: 0.1096\n",
      "209/223, train_loss: 0.1256, step time: 0.1172\n",
      "210/223, train_loss: 0.1233, step time: 0.1006\n",
      "211/223, train_loss: 0.1352, step time: 0.0996\n",
      "212/223, train_loss: 0.1244, step time: 0.0999\n",
      "213/223, train_loss: 0.1178, step time: 0.1223\n",
      "214/223, train_loss: 0.1302, step time: 0.1151\n",
      "215/223, train_loss: 0.1217, step time: 0.1002\n",
      "216/223, train_loss: 0.1213, step time: 0.1007\n",
      "217/223, train_loss: 0.1208, step time: 0.1439\n",
      "218/223, train_loss: 0.1114, step time: 0.1087\n",
      "219/223, train_loss: 0.1216, step time: 0.1002\n",
      "220/223, train_loss: 0.1059, step time: 0.1003\n",
      "221/223, train_loss: 0.1196, step time: 0.1007\n",
      "222/223, train_loss: 0.1127, step time: 0.1003\n",
      "223/223, train_loss: 0.1164, step time: 0.0997\n",
      "epoch 85 average loss: 0.1185\n",
      "saved new best metric model\n",
      "current epoch: 85 current mean dice: 0.8453 tc: 0.9143 wt: 0.8569 et: 0.7647\n",
      "best mean dice: 0.8453 at epoch: 85\n",
      "time consuming of epoch 85 is: 92.6952\n",
      "----------\n",
      "epoch 86/300\n",
      "1/223, train_loss: 0.1166, step time: 0.1034\n",
      "2/223, train_loss: 0.1274, step time: 0.1029\n",
      "3/223, train_loss: 0.1130, step time: 0.1000\n",
      "4/223, train_loss: 0.1230, step time: 0.1012\n",
      "5/223, train_loss: 0.1153, step time: 0.0998\n",
      "6/223, train_loss: 0.1174, step time: 0.1001\n",
      "7/223, train_loss: 0.1058, step time: 0.1004\n",
      "8/223, train_loss: 0.1112, step time: 0.1022\n",
      "9/223, train_loss: 0.1173, step time: 0.1228\n",
      "10/223, train_loss: 0.1233, step time: 0.1218\n",
      "11/223, train_loss: 0.1130, step time: 0.1082\n",
      "12/223, train_loss: 0.1050, step time: 0.1179\n",
      "13/223, train_loss: 0.1116, step time: 0.1076\n",
      "14/223, train_loss: 0.1153, step time: 0.1089\n",
      "15/223, train_loss: 0.1031, step time: 0.1000\n",
      "16/223, train_loss: 0.1066, step time: 0.1084\n",
      "17/223, train_loss: 0.1081, step time: 0.1134\n",
      "18/223, train_loss: 0.1193, step time: 0.1099\n",
      "19/223, train_loss: 0.1272, step time: 0.0999\n",
      "20/223, train_loss: 0.1130, step time: 0.1167\n",
      "21/223, train_loss: 0.1084, step time: 0.1042\n",
      "22/223, train_loss: 0.1172, step time: 0.1002\n",
      "23/223, train_loss: 0.1334, step time: 0.1003\n",
      "24/223, train_loss: 0.1126, step time: 0.1000\n",
      "25/223, train_loss: 0.1180, step time: 0.1111\n",
      "26/223, train_loss: 0.1020, step time: 0.1057\n",
      "27/223, train_loss: 0.1060, step time: 0.1167\n",
      "28/223, train_loss: 0.1052, step time: 0.1011\n",
      "29/223, train_loss: 0.1259, step time: 0.1141\n",
      "30/223, train_loss: 0.1179, step time: 0.1007\n",
      "31/223, train_loss: 0.1174, step time: 0.1003\n",
      "32/223, train_loss: 0.1088, step time: 0.1002\n",
      "33/223, train_loss: 0.1179, step time: 0.1009\n",
      "34/223, train_loss: 0.1178, step time: 0.0997\n",
      "35/223, train_loss: 0.1218, step time: 0.1001\n",
      "36/223, train_loss: 0.1149, step time: 0.0998\n",
      "37/223, train_loss: 0.1159, step time: 0.1616\n",
      "38/223, train_loss: 0.1276, step time: 0.1005\n",
      "39/223, train_loss: 0.1230, step time: 0.0997\n",
      "40/223, train_loss: 0.1148, step time: 0.1006\n",
      "41/223, train_loss: 0.1267, step time: 0.1008\n",
      "42/223, train_loss: 0.1182, step time: 0.0987\n",
      "43/223, train_loss: 0.1090, step time: 0.0989\n",
      "44/223, train_loss: 0.1340, step time: 0.0994\n",
      "45/223, train_loss: 0.1211, step time: 0.1012\n",
      "46/223, train_loss: 0.1077, step time: 0.0999\n",
      "47/223, train_loss: 0.1200, step time: 0.0997\n",
      "48/223, train_loss: 0.1121, step time: 0.0997\n",
      "49/223, train_loss: 0.1230, step time: 0.1013\n",
      "50/223, train_loss: 0.1210, step time: 0.1011\n",
      "51/223, train_loss: 0.1149, step time: 0.1002\n",
      "52/223, train_loss: 0.1105, step time: 0.0999\n",
      "53/223, train_loss: 0.1205, step time: 0.1059\n",
      "54/223, train_loss: 0.1166, step time: 0.1182\n",
      "55/223, train_loss: 0.1332, step time: 0.1201\n",
      "56/223, train_loss: 0.1321, step time: 0.1237\n",
      "57/223, train_loss: 0.1108, step time: 0.1095\n",
      "58/223, train_loss: 0.1172, step time: 0.1012\n",
      "59/223, train_loss: 0.1249, step time: 0.1266\n",
      "60/223, train_loss: 0.1106, step time: 0.1138\n",
      "61/223, train_loss: 0.1181, step time: 0.1060\n",
      "62/223, train_loss: 0.1149, step time: 0.1004\n",
      "63/223, train_loss: 0.1123, step time: 0.1004\n",
      "64/223, train_loss: 0.1151, step time: 0.1006\n",
      "65/223, train_loss: 0.1247, step time: 0.1002\n",
      "66/223, train_loss: 0.1097, step time: 0.1006\n",
      "67/223, train_loss: 0.1081, step time: 0.1003\n",
      "68/223, train_loss: 0.1105, step time: 0.1044\n",
      "69/223, train_loss: 0.1217, step time: 0.1446\n",
      "70/223, train_loss: 0.1211, step time: 0.1556\n",
      "71/223, train_loss: 0.1139, step time: 0.1025\n",
      "72/223, train_loss: 0.1214, step time: 0.1004\n",
      "73/223, train_loss: 0.1196, step time: 0.1007\n",
      "74/223, train_loss: 0.0999, step time: 0.1222\n",
      "75/223, train_loss: 0.1152, step time: 0.1050\n",
      "76/223, train_loss: 0.1444, step time: 0.1135\n",
      "77/223, train_loss: 0.1231, step time: 0.1001\n",
      "78/223, train_loss: 0.1076, step time: 0.1006\n",
      "79/223, train_loss: 0.1137, step time: 0.1252\n",
      "80/223, train_loss: 0.1164, step time: 0.1188\n",
      "81/223, train_loss: 0.1215, step time: 0.1067\n",
      "82/223, train_loss: 0.1248, step time: 0.1120\n",
      "83/223, train_loss: 0.1087, step time: 0.1111\n",
      "84/223, train_loss: 0.1150, step time: 0.1245\n",
      "85/223, train_loss: 0.1195, step time: 0.1066\n",
      "86/223, train_loss: 0.1168, step time: 0.1146\n",
      "87/223, train_loss: 0.1147, step time: 0.1003\n",
      "88/223, train_loss: 0.1165, step time: 0.1007\n",
      "89/223, train_loss: 0.1133, step time: 0.0994\n",
      "90/223, train_loss: 0.1048, step time: 0.1396\n",
      "91/223, train_loss: 0.1058, step time: 0.1000\n",
      "92/223, train_loss: 0.1181, step time: 0.1009\n",
      "93/223, train_loss: 0.1211, step time: 0.1007\n",
      "94/223, train_loss: 0.1121, step time: 0.0996\n",
      "95/223, train_loss: 0.1162, step time: 0.1002\n",
      "96/223, train_loss: 0.1045, step time: 0.1005\n",
      "97/223, train_loss: 0.1122, step time: 0.1015\n",
      "98/223, train_loss: 0.1305, step time: 0.1438\n",
      "99/223, train_loss: 0.1200, step time: 0.1043\n",
      "100/223, train_loss: 0.1134, step time: 0.1010\n",
      "101/223, train_loss: 0.1238, step time: 0.1034\n",
      "102/223, train_loss: 0.1207, step time: 0.1084\n",
      "103/223, train_loss: 0.1248, step time: 0.1131\n",
      "104/223, train_loss: 0.1249, step time: 0.0999\n",
      "105/223, train_loss: 0.1251, step time: 0.1107\n",
      "106/223, train_loss: 0.1315, step time: 0.1073\n",
      "107/223, train_loss: 0.1203, step time: 0.1118\n",
      "108/223, train_loss: 0.1225, step time: 0.1178\n",
      "109/223, train_loss: 0.1141, step time: 0.0999\n",
      "110/223, train_loss: 0.1219, step time: 0.1269\n",
      "111/223, train_loss: 0.1211, step time: 0.1147\n",
      "112/223, train_loss: 0.1274, step time: 0.1149\n",
      "113/223, train_loss: 0.1175, step time: 0.1100\n",
      "114/223, train_loss: 0.1145, step time: 0.1125\n",
      "115/223, train_loss: 0.1234, step time: 0.1168\n",
      "116/223, train_loss: 0.1170, step time: 0.1110\n",
      "117/223, train_loss: 0.1065, step time: 0.1198\n",
      "118/223, train_loss: 0.1234, step time: 0.1052\n",
      "119/223, train_loss: 0.1199, step time: 0.1001\n",
      "120/223, train_loss: 0.1136, step time: 0.1010\n",
      "121/223, train_loss: 0.1180, step time: 0.1356\n",
      "122/223, train_loss: 0.1062, step time: 0.1071\n",
      "123/223, train_loss: 0.3164, step time: 0.1012\n",
      "124/223, train_loss: 0.1283, step time: 0.1287\n",
      "125/223, train_loss: 0.1106, step time: 0.1088\n",
      "126/223, train_loss: 0.1275, step time: 0.1169\n",
      "127/223, train_loss: 0.1039, step time: 0.1004\n",
      "128/223, train_loss: 0.1093, step time: 0.1166\n",
      "129/223, train_loss: 0.1104, step time: 0.1004\n",
      "130/223, train_loss: 0.1185, step time: 0.1002\n",
      "131/223, train_loss: 0.1187, step time: 0.0994\n",
      "132/223, train_loss: 0.1327, step time: 0.1038\n",
      "133/223, train_loss: 0.1219, step time: 0.1071\n",
      "134/223, train_loss: 0.1217, step time: 0.1082\n",
      "135/223, train_loss: 0.1130, step time: 0.1004\n",
      "136/223, train_loss: 0.1093, step time: 0.1002\n",
      "137/223, train_loss: 0.1141, step time: 0.1065\n",
      "138/223, train_loss: 0.1182, step time: 0.1005\n",
      "139/223, train_loss: 0.1387, step time: 0.1001\n",
      "140/223, train_loss: 0.1308, step time: 0.1165\n",
      "141/223, train_loss: 0.1147, step time: 0.1014\n",
      "142/223, train_loss: 0.1051, step time: 0.1257\n",
      "143/223, train_loss: 0.1116, step time: 0.1273\n",
      "144/223, train_loss: 0.1179, step time: 0.1240\n",
      "145/223, train_loss: 0.1299, step time: 0.1252\n",
      "146/223, train_loss: 0.1324, step time: 0.1159\n",
      "147/223, train_loss: 0.1032, step time: 0.1162\n",
      "148/223, train_loss: 0.1170, step time: 0.1031\n",
      "149/223, train_loss: 0.1094, step time: 0.1005\n",
      "150/223, train_loss: 0.1218, step time: 0.1001\n",
      "151/223, train_loss: 0.1102, step time: 0.0996\n",
      "152/223, train_loss: 0.1096, step time: 0.0997\n",
      "153/223, train_loss: 0.1196, step time: 0.1110\n",
      "154/223, train_loss: 0.1107, step time: 0.1121\n",
      "155/223, train_loss: 0.1207, step time: 0.1103\n",
      "156/223, train_loss: 0.1257, step time: 0.1530\n",
      "157/223, train_loss: 0.1225, step time: 0.1092\n",
      "158/223, train_loss: 0.0996, step time: 0.1048\n",
      "159/223, train_loss: 0.1231, step time: 0.1131\n",
      "160/223, train_loss: 0.1249, step time: 0.1007\n",
      "161/223, train_loss: 0.1064, step time: 0.1257\n",
      "162/223, train_loss: 0.1187, step time: 0.1015\n",
      "163/223, train_loss: 0.1374, step time: 0.0999\n",
      "164/223, train_loss: 0.1145, step time: 0.1007\n",
      "165/223, train_loss: 0.1218, step time: 0.1019\n",
      "166/223, train_loss: 0.1132, step time: 0.1065\n",
      "167/223, train_loss: 0.1179, step time: 0.1002\n",
      "168/223, train_loss: 0.1207, step time: 0.1005\n",
      "169/223, train_loss: 0.1224, step time: 0.1161\n",
      "170/223, train_loss: 0.1338, step time: 0.1107\n",
      "171/223, train_loss: 0.1230, step time: 0.1083\n",
      "172/223, train_loss: 0.1160, step time: 0.1229\n",
      "173/223, train_loss: 0.1179, step time: 0.1065\n",
      "174/223, train_loss: 0.1090, step time: 0.1144\n",
      "175/223, train_loss: 0.1228, step time: 0.1112\n",
      "176/223, train_loss: 0.1124, step time: 0.1144\n",
      "177/223, train_loss: 0.1049, step time: 0.1004\n",
      "178/223, train_loss: 0.1125, step time: 0.1165\n",
      "179/223, train_loss: 0.1179, step time: 0.0997\n",
      "180/223, train_loss: 0.1055, step time: 0.1003\n",
      "181/223, train_loss: 0.1224, step time: 0.1006\n",
      "182/223, train_loss: 0.1119, step time: 0.1073\n",
      "183/223, train_loss: 0.1141, step time: 0.1258\n",
      "184/223, train_loss: 0.1146, step time: 0.1009\n",
      "185/223, train_loss: 0.1245, step time: 0.1025\n",
      "186/223, train_loss: 0.1067, step time: 0.1000\n",
      "187/223, train_loss: 0.1240, step time: 0.1009\n",
      "188/223, train_loss: 0.1078, step time: 0.1000\n",
      "189/223, train_loss: 0.1159, step time: 0.1368\n",
      "190/223, train_loss: 0.1267, step time: 0.1046\n",
      "191/223, train_loss: 0.1100, step time: 0.1008\n",
      "192/223, train_loss: 0.1214, step time: 0.1019\n",
      "193/223, train_loss: 0.1186, step time: 0.1026\n",
      "194/223, train_loss: 0.1149, step time: 0.1052\n",
      "195/223, train_loss: 0.1236, step time: 0.1048\n",
      "196/223, train_loss: 0.1230, step time: 0.1040\n",
      "197/223, train_loss: 0.1097, step time: 0.1005\n",
      "198/223, train_loss: 0.1160, step time: 0.1090\n",
      "199/223, train_loss: 0.1077, step time: 0.1015\n",
      "200/223, train_loss: 0.1056, step time: 0.1016\n",
      "201/223, train_loss: 0.1127, step time: 0.1162\n",
      "202/223, train_loss: 0.1258, step time: 0.1106\n",
      "203/223, train_loss: 0.1329, step time: 0.1087\n",
      "204/223, train_loss: 0.1141, step time: 0.1245\n",
      "205/223, train_loss: 0.1118, step time: 0.1008\n",
      "206/223, train_loss: 0.1259, step time: 0.1279\n",
      "207/223, train_loss: 0.1114, step time: 0.1001\n",
      "208/223, train_loss: 0.1186, step time: 0.1007\n",
      "209/223, train_loss: 0.1201, step time: 0.1012\n",
      "210/223, train_loss: 0.1129, step time: 0.1184\n",
      "211/223, train_loss: 0.1148, step time: 0.1008\n",
      "212/223, train_loss: 0.1185, step time: 0.1008\n",
      "213/223, train_loss: 0.1158, step time: 0.1056\n",
      "214/223, train_loss: 0.1368, step time: 0.1071\n",
      "215/223, train_loss: 0.1187, step time: 0.1100\n",
      "216/223, train_loss: 0.1104, step time: 0.1001\n",
      "217/223, train_loss: 0.1208, step time: 0.1000\n",
      "218/223, train_loss: 0.1187, step time: 0.1005\n",
      "219/223, train_loss: 0.1199, step time: 0.0997\n",
      "220/223, train_loss: 0.1239, step time: 0.1004\n",
      "221/223, train_loss: 0.1111, step time: 0.0998\n",
      "222/223, train_loss: 0.1197, step time: 0.1001\n",
      "223/223, train_loss: 0.1066, step time: 0.1009\n",
      "epoch 86 average loss: 0.1181\n",
      "time consuming of epoch 86 is: 92.4808\n",
      "----------\n",
      "epoch 87/300\n",
      "1/223, train_loss: 0.1288, step time: 0.1061\n",
      "2/223, train_loss: 0.1161, step time: 0.1094\n",
      "3/223, train_loss: 0.1066, step time: 0.1026\n",
      "4/223, train_loss: 0.1181, step time: 0.1183\n",
      "5/223, train_loss: 0.1212, step time: 0.1089\n",
      "6/223, train_loss: 0.1325, step time: 0.1195\n",
      "7/223, train_loss: 0.1318, step time: 0.1104\n",
      "8/223, train_loss: 0.1118, step time: 0.1023\n",
      "9/223, train_loss: 0.1189, step time: 0.0999\n",
      "10/223, train_loss: 0.1062, step time: 0.1001\n",
      "11/223, train_loss: 0.1067, step time: 0.1001\n",
      "12/223, train_loss: 0.1204, step time: 0.1003\n",
      "13/223, train_loss: 0.1377, step time: 0.1013\n",
      "14/223, train_loss: 0.1104, step time: 0.1003\n",
      "15/223, train_loss: 0.1060, step time: 0.1008\n",
      "16/223, train_loss: 0.1144, step time: 0.1009\n",
      "17/223, train_loss: 0.1096, step time: 0.1164\n",
      "18/223, train_loss: 0.1168, step time: 0.1076\n",
      "19/223, train_loss: 0.1269, step time: 0.1189\n",
      "20/223, train_loss: 0.1181, step time: 0.1055\n",
      "21/223, train_loss: 0.1248, step time: 0.1122\n",
      "22/223, train_loss: 0.1240, step time: 0.0998\n",
      "23/223, train_loss: 0.1151, step time: 0.0999\n",
      "24/223, train_loss: 0.1191, step time: 0.1010\n",
      "25/223, train_loss: 0.1146, step time: 0.1131\n",
      "26/223, train_loss: 0.1376, step time: 0.1086\n",
      "27/223, train_loss: 0.1175, step time: 0.1183\n",
      "28/223, train_loss: 0.1191, step time: 0.1131\n",
      "29/223, train_loss: 0.1162, step time: 0.1009\n",
      "30/223, train_loss: 0.1204, step time: 0.1012\n",
      "31/223, train_loss: 0.1248, step time: 0.1345\n",
      "32/223, train_loss: 0.1192, step time: 0.1246\n",
      "33/223, train_loss: 0.1276, step time: 0.1126\n",
      "34/223, train_loss: 0.1179, step time: 0.1215\n",
      "35/223, train_loss: 0.1038, step time: 0.1317\n",
      "36/223, train_loss: 0.1203, step time: 0.1087\n",
      "37/223, train_loss: 0.1173, step time: 0.1039\n",
      "38/223, train_loss: 0.1320, step time: 0.1049\n",
      "39/223, train_loss: 0.1269, step time: 0.0989\n",
      "40/223, train_loss: 0.3220, step time: 0.0994\n",
      "41/223, train_loss: 0.1151, step time: 0.1568\n",
      "42/223, train_loss: 0.1294, step time: 0.0991\n",
      "43/223, train_loss: 0.1103, step time: 0.0993\n",
      "44/223, train_loss: 0.1333, step time: 0.0986\n",
      "45/223, train_loss: 0.1182, step time: 0.1030\n",
      "46/223, train_loss: 0.1282, step time: 0.1003\n",
      "47/223, train_loss: 0.1213, step time: 0.1007\n",
      "48/223, train_loss: 0.1319, step time: 0.1312\n",
      "49/223, train_loss: 0.1133, step time: 0.1358\n",
      "50/223, train_loss: 0.1056, step time: 0.1097\n",
      "51/223, train_loss: 0.1176, step time: 0.1121\n",
      "52/223, train_loss: 0.1251, step time: 0.1086\n",
      "53/223, train_loss: 0.1177, step time: 0.1314\n",
      "54/223, train_loss: 0.1219, step time: 0.0992\n",
      "55/223, train_loss: 0.1035, step time: 0.0998\n",
      "56/223, train_loss: 0.1082, step time: 0.1018\n",
      "57/223, train_loss: 0.1112, step time: 0.1158\n",
      "58/223, train_loss: 0.1183, step time: 0.1109\n",
      "59/223, train_loss: 0.1129, step time: 0.1090\n",
      "60/223, train_loss: 0.1145, step time: 0.1003\n",
      "61/223, train_loss: 0.1174, step time: 0.1005\n",
      "62/223, train_loss: 0.1184, step time: 0.1004\n",
      "63/223, train_loss: 0.1089, step time: 0.1002\n",
      "64/223, train_loss: 0.1091, step time: 0.1015\n",
      "65/223, train_loss: 0.1093, step time: 0.1003\n",
      "66/223, train_loss: 0.1305, step time: 0.1016\n",
      "67/223, train_loss: 0.1186, step time: 0.1032\n",
      "68/223, train_loss: 0.1260, step time: 0.1005\n",
      "69/223, train_loss: 0.1309, step time: 0.1003\n",
      "70/223, train_loss: 0.1205, step time: 0.1007\n",
      "71/223, train_loss: 0.1118, step time: 0.1011\n",
      "72/223, train_loss: 0.1235, step time: 0.1187\n",
      "73/223, train_loss: 0.1191, step time: 0.1027\n",
      "74/223, train_loss: 0.1124, step time: 0.1125\n",
      "75/223, train_loss: 0.1090, step time: 0.1184\n",
      "76/223, train_loss: 0.1157, step time: 0.1057\n",
      "77/223, train_loss: 0.1202, step time: 0.1000\n",
      "78/223, train_loss: 0.1274, step time: 0.1086\n",
      "79/223, train_loss: 0.1125, step time: 0.1007\n",
      "80/223, train_loss: 0.1369, step time: 0.1256\n",
      "81/223, train_loss: 0.1162, step time: 0.1230\n",
      "82/223, train_loss: 0.1109, step time: 0.1051\n",
      "83/223, train_loss: 0.1069, step time: 0.1177\n",
      "84/223, train_loss: 0.1143, step time: 0.0994\n",
      "85/223, train_loss: 0.1368, step time: 0.1130\n",
      "86/223, train_loss: 0.1233, step time: 0.1123\n",
      "87/223, train_loss: 0.1153, step time: 0.1127\n",
      "88/223, train_loss: 0.1215, step time: 0.1090\n",
      "89/223, train_loss: 0.1032, step time: 0.1398\n",
      "90/223, train_loss: 0.1266, step time: 0.1124\n",
      "91/223, train_loss: 0.1052, step time: 0.1053\n",
      "92/223, train_loss: 0.1194, step time: 0.1268\n",
      "93/223, train_loss: 0.1049, step time: 0.1049\n",
      "94/223, train_loss: 0.1218, step time: 0.1242\n",
      "95/223, train_loss: 0.1184, step time: 0.1190\n",
      "96/223, train_loss: 0.1207, step time: 0.1302\n",
      "97/223, train_loss: 0.1064, step time: 0.1059\n",
      "98/223, train_loss: 0.1143, step time: 0.0998\n",
      "99/223, train_loss: 0.1100, step time: 0.1001\n",
      "100/223, train_loss: 0.1080, step time: 0.1080\n",
      "101/223, train_loss: 0.1212, step time: 0.1136\n",
      "102/223, train_loss: 0.1244, step time: 0.1002\n",
      "103/223, train_loss: 0.1159, step time: 0.1004\n",
      "104/223, train_loss: 0.1227, step time: 0.1122\n",
      "105/223, train_loss: 0.1094, step time: 0.1052\n",
      "106/223, train_loss: 0.1191, step time: 0.1002\n",
      "107/223, train_loss: 0.1046, step time: 0.0996\n",
      "108/223, train_loss: 0.1042, step time: 0.1006\n",
      "109/223, train_loss: 0.1274, step time: 0.1150\n",
      "110/223, train_loss: 0.1141, step time: 0.1002\n",
      "111/223, train_loss: 0.1149, step time: 0.1005\n",
      "112/223, train_loss: 0.1292, step time: 0.1008\n",
      "113/223, train_loss: 0.1169, step time: 0.1006\n",
      "114/223, train_loss: 0.1177, step time: 0.1132\n",
      "115/223, train_loss: 0.1265, step time: 0.1054\n",
      "116/223, train_loss: 0.1186, step time: 0.1010\n",
      "117/223, train_loss: 0.1177, step time: 0.1007\n",
      "118/223, train_loss: 0.1083, step time: 0.1076\n",
      "119/223, train_loss: 0.1294, step time: 0.1345\n",
      "120/223, train_loss: 0.1254, step time: 0.1102\n",
      "121/223, train_loss: 0.1203, step time: 0.1003\n",
      "122/223, train_loss: 0.1229, step time: 0.1123\n",
      "123/223, train_loss: 0.1180, step time: 0.1013\n",
      "124/223, train_loss: 0.1065, step time: 0.1046\n",
      "125/223, train_loss: 0.1042, step time: 0.1077\n",
      "126/223, train_loss: 0.1144, step time: 0.1110\n",
      "127/223, train_loss: 0.1312, step time: 0.1002\n",
      "128/223, train_loss: 0.1093, step time: 0.1157\n",
      "129/223, train_loss: 0.1107, step time: 0.1023\n",
      "130/223, train_loss: 0.1106, step time: 0.1210\n",
      "131/223, train_loss: 0.1153, step time: 0.1340\n",
      "132/223, train_loss: 0.1054, step time: 0.1005\n",
      "133/223, train_loss: 0.1245, step time: 0.1071\n",
      "134/223, train_loss: 0.1167, step time: 0.1085\n",
      "135/223, train_loss: 0.1274, step time: 0.1219\n",
      "136/223, train_loss: 0.1138, step time: 0.1005\n",
      "137/223, train_loss: 0.1096, step time: 0.1226\n",
      "138/223, train_loss: 0.1168, step time: 0.1000\n",
      "139/223, train_loss: 0.1050, step time: 0.1083\n",
      "140/223, train_loss: 0.1145, step time: 0.1013\n",
      "141/223, train_loss: 0.1090, step time: 0.1184\n",
      "142/223, train_loss: 0.1201, step time: 0.1135\n",
      "143/223, train_loss: 0.1246, step time: 0.1138\n",
      "144/223, train_loss: 0.1132, step time: 0.1114\n",
      "145/223, train_loss: 0.1235, step time: 0.1135\n",
      "146/223, train_loss: 0.1077, step time: 0.1005\n",
      "147/223, train_loss: 0.1136, step time: 0.1205\n",
      "148/223, train_loss: 0.1301, step time: 0.1153\n",
      "149/223, train_loss: 0.1235, step time: 0.1004\n",
      "150/223, train_loss: 0.1227, step time: 0.1116\n",
      "151/223, train_loss: 0.1098, step time: 0.1130\n",
      "152/223, train_loss: 0.1132, step time: 0.1163\n",
      "153/223, train_loss: 0.1158, step time: 0.1253\n",
      "154/223, train_loss: 0.1105, step time: 0.1000\n",
      "155/223, train_loss: 0.1198, step time: 0.1153\n",
      "156/223, train_loss: 0.1250, step time: 0.1105\n",
      "157/223, train_loss: 0.1194, step time: 0.1213\n",
      "158/223, train_loss: 0.1133, step time: 0.1013\n",
      "159/223, train_loss: 0.1164, step time: 0.1169\n",
      "160/223, train_loss: 0.1359, step time: 0.1200\n",
      "161/223, train_loss: 0.1086, step time: 0.1193\n",
      "162/223, train_loss: 0.1189, step time: 0.0998\n",
      "163/223, train_loss: 0.1149, step time: 0.1006\n",
      "164/223, train_loss: 0.1190, step time: 0.1092\n",
      "165/223, train_loss: 0.1246, step time: 0.1051\n",
      "166/223, train_loss: 0.1283, step time: 0.1144\n",
      "167/223, train_loss: 0.1216, step time: 0.1053\n",
      "168/223, train_loss: 0.1170, step time: 0.1126\n",
      "169/223, train_loss: 0.1056, step time: 0.1121\n",
      "170/223, train_loss: 0.1093, step time: 0.1005\n",
      "171/223, train_loss: 0.1155, step time: 0.1000\n",
      "172/223, train_loss: 0.1132, step time: 0.1209\n",
      "173/223, train_loss: 0.1085, step time: 0.1226\n",
      "174/223, train_loss: 0.1170, step time: 0.1072\n",
      "175/223, train_loss: 0.1043, step time: 0.1146\n",
      "176/223, train_loss: 0.1135, step time: 0.1196\n",
      "177/223, train_loss: 0.1187, step time: 0.1220\n",
      "178/223, train_loss: 0.1221, step time: 0.1031\n",
      "179/223, train_loss: 0.1238, step time: 0.1049\n",
      "180/223, train_loss: 0.1247, step time: 0.1007\n",
      "181/223, train_loss: 0.1255, step time: 0.1210\n",
      "182/223, train_loss: 0.1209, step time: 0.1024\n",
      "183/223, train_loss: 0.1126, step time: 0.1141\n",
      "184/223, train_loss: 0.1214, step time: 0.1140\n",
      "185/223, train_loss: 0.1168, step time: 0.1186\n",
      "186/223, train_loss: 0.1256, step time: 0.1120\n",
      "187/223, train_loss: 0.1207, step time: 0.1124\n",
      "188/223, train_loss: 0.1150, step time: 0.1158\n",
      "189/223, train_loss: 0.1157, step time: 0.1084\n",
      "190/223, train_loss: 0.1122, step time: 0.1013\n",
      "191/223, train_loss: 0.1276, step time: 0.0998\n",
      "192/223, train_loss: 0.1244, step time: 0.1067\n",
      "193/223, train_loss: 0.1105, step time: 0.1166\n",
      "194/223, train_loss: 0.1143, step time: 0.1027\n",
      "195/223, train_loss: 0.1104, step time: 0.1499\n",
      "196/223, train_loss: 0.1092, step time: 0.1226\n",
      "197/223, train_loss: 0.1119, step time: 0.1141\n",
      "198/223, train_loss: 0.1347, step time: 0.1162\n",
      "199/223, train_loss: 0.1234, step time: 0.1002\n",
      "200/223, train_loss: 0.1286, step time: 0.1013\n",
      "201/223, train_loss: 0.1188, step time: 0.1066\n",
      "202/223, train_loss: 0.1111, step time: 0.1050\n",
      "203/223, train_loss: 0.1141, step time: 0.1001\n",
      "204/223, train_loss: 0.1126, step time: 0.1035\n",
      "205/223, train_loss: 0.1052, step time: 0.1074\n",
      "206/223, train_loss: 0.0999, step time: 0.1155\n",
      "207/223, train_loss: 0.1077, step time: 0.1161\n",
      "208/223, train_loss: 0.1294, step time: 0.1010\n",
      "209/223, train_loss: 0.1098, step time: 0.1008\n",
      "210/223, train_loss: 0.1091, step time: 0.1122\n",
      "211/223, train_loss: 0.1146, step time: 0.1259\n",
      "212/223, train_loss: 0.1128, step time: 0.1237\n",
      "213/223, train_loss: 0.1182, step time: 0.1176\n",
      "214/223, train_loss: 0.1244, step time: 0.1002\n",
      "215/223, train_loss: 0.1100, step time: 0.1006\n",
      "216/223, train_loss: 0.1060, step time: 0.1004\n",
      "217/223, train_loss: 0.1198, step time: 0.1012\n",
      "218/223, train_loss: 0.1149, step time: 0.0994\n",
      "219/223, train_loss: 0.1132, step time: 0.1009\n",
      "220/223, train_loss: 0.1084, step time: 0.0998\n",
      "221/223, train_loss: 0.1265, step time: 0.0998\n",
      "222/223, train_loss: 0.1037, step time: 0.1003\n",
      "223/223, train_loss: 0.1045, step time: 0.1001\n",
      "epoch 87 average loss: 0.1182\n",
      "time consuming of epoch 87 is: 88.8022\n",
      "----------\n",
      "epoch 88/300\n",
      "1/223, train_loss: 0.1151, step time: 0.1082\n",
      "2/223, train_loss: 0.1123, step time: 0.1056\n",
      "3/223, train_loss: 0.1104, step time: 0.1054\n",
      "4/223, train_loss: 0.1139, step time: 0.1004\n",
      "5/223, train_loss: 0.1135, step time: 0.1071\n",
      "6/223, train_loss: 0.1100, step time: 0.1009\n",
      "7/223, train_loss: 0.1134, step time: 0.1136\n",
      "8/223, train_loss: 0.1179, step time: 0.1114\n",
      "9/223, train_loss: 0.1035, step time: 0.1092\n",
      "10/223, train_loss: 0.1195, step time: 0.1018\n",
      "11/223, train_loss: 0.1325, step time: 0.1000\n",
      "12/223, train_loss: 0.1205, step time: 0.1000\n",
      "13/223, train_loss: 0.1146, step time: 0.1116\n",
      "14/223, train_loss: 0.1196, step time: 0.1144\n",
      "15/223, train_loss: 0.1179, step time: 0.1156\n",
      "16/223, train_loss: 0.1091, step time: 0.1185\n",
      "17/223, train_loss: 0.1152, step time: 0.1164\n",
      "18/223, train_loss: 0.1134, step time: 0.1143\n",
      "19/223, train_loss: 0.1335, step time: 0.1204\n",
      "20/223, train_loss: 0.1215, step time: 0.1003\n",
      "21/223, train_loss: 0.1194, step time: 0.1150\n",
      "22/223, train_loss: 0.1337, step time: 0.1119\n",
      "23/223, train_loss: 0.1108, step time: 0.1167\n",
      "24/223, train_loss: 0.1173, step time: 0.1001\n",
      "25/223, train_loss: 0.1216, step time: 0.1694\n",
      "26/223, train_loss: 0.1129, step time: 0.1088\n",
      "27/223, train_loss: 0.1137, step time: 0.1073\n",
      "28/223, train_loss: 0.1160, step time: 0.1058\n",
      "29/223, train_loss: 0.1127, step time: 0.1086\n",
      "30/223, train_loss: 0.1345, step time: 0.1157\n",
      "31/223, train_loss: 0.1126, step time: 0.1050\n",
      "32/223, train_loss: 0.1107, step time: 0.1005\n",
      "33/223, train_loss: 0.1163, step time: 0.0986\n",
      "34/223, train_loss: 0.1193, step time: 0.1120\n",
      "35/223, train_loss: 0.1018, step time: 0.1147\n",
      "36/223, train_loss: 0.1176, step time: 0.1098\n",
      "37/223, train_loss: 0.1172, step time: 0.1115\n",
      "38/223, train_loss: 0.1095, step time: 0.1094\n",
      "39/223, train_loss: 0.1161, step time: 0.1080\n",
      "40/223, train_loss: 0.1142, step time: 0.1007\n",
      "41/223, train_loss: 0.1073, step time: 0.1217\n",
      "42/223, train_loss: 0.1252, step time: 0.0990\n",
      "43/223, train_loss: 0.1439, step time: 0.1272\n",
      "44/223, train_loss: 0.1167, step time: 0.1110\n",
      "45/223, train_loss: 0.1206, step time: 0.0985\n",
      "46/223, train_loss: 0.1110, step time: 0.1098\n",
      "47/223, train_loss: 0.1181, step time: 0.1099\n",
      "48/223, train_loss: 0.1243, step time: 0.1009\n",
      "49/223, train_loss: 0.1159, step time: 0.1080\n",
      "50/223, train_loss: 0.1246, step time: 0.1002\n",
      "51/223, train_loss: 0.1039, step time: 0.1137\n",
      "52/223, train_loss: 0.1138, step time: 0.1040\n",
      "53/223, train_loss: 0.1161, step time: 0.1027\n",
      "54/223, train_loss: 0.1053, step time: 0.1107\n",
      "55/223, train_loss: 0.1366, step time: 0.1008\n",
      "56/223, train_loss: 0.1086, step time: 0.1002\n",
      "57/223, train_loss: 0.1112, step time: 0.1125\n",
      "58/223, train_loss: 0.1173, step time: 0.1039\n",
      "59/223, train_loss: 0.1109, step time: 0.1025\n",
      "60/223, train_loss: 0.1252, step time: 0.1094\n",
      "61/223, train_loss: 0.1106, step time: 0.1184\n",
      "62/223, train_loss: 0.1124, step time: 0.1107\n",
      "63/223, train_loss: 0.1247, step time: 0.1108\n",
      "64/223, train_loss: 0.1167, step time: 0.1088\n",
      "65/223, train_loss: 0.1271, step time: 0.1013\n",
      "66/223, train_loss: 0.1418, step time: 0.0999\n",
      "67/223, train_loss: 0.1191, step time: 0.1052\n",
      "68/223, train_loss: 0.1235, step time: 0.1140\n",
      "69/223, train_loss: 0.1025, step time: 0.1156\n",
      "70/223, train_loss: 0.1295, step time: 0.1006\n",
      "71/223, train_loss: 0.1092, step time: 0.1022\n",
      "72/223, train_loss: 0.1247, step time: 0.1063\n",
      "73/223, train_loss: 0.1087, step time: 0.1030\n",
      "74/223, train_loss: 0.1168, step time: 0.1104\n",
      "75/223, train_loss: 0.1096, step time: 0.1114\n",
      "76/223, train_loss: 0.1316, step time: 0.1022\n",
      "77/223, train_loss: 0.1198, step time: 0.1059\n",
      "78/223, train_loss: 0.1176, step time: 0.1156\n",
      "79/223, train_loss: 0.1109, step time: 0.1006\n",
      "80/223, train_loss: 0.1200, step time: 0.1095\n",
      "81/223, train_loss: 0.1243, step time: 0.1005\n",
      "82/223, train_loss: 0.1114, step time: 0.1049\n",
      "83/223, train_loss: 0.1131, step time: 0.1112\n",
      "84/223, train_loss: 0.1130, step time: 0.1266\n",
      "85/223, train_loss: 0.1215, step time: 0.1252\n",
      "86/223, train_loss: 0.1388, step time: 0.1049\n",
      "87/223, train_loss: 0.1193, step time: 0.1113\n",
      "88/223, train_loss: 0.1214, step time: 0.1067\n",
      "89/223, train_loss: 0.1112, step time: 0.1007\n",
      "90/223, train_loss: 0.1333, step time: 0.1009\n",
      "91/223, train_loss: 0.1072, step time: 0.1135\n",
      "92/223, train_loss: 0.1104, step time: 0.1010\n",
      "93/223, train_loss: 0.1290, step time: 0.1043\n",
      "94/223, train_loss: 0.1194, step time: 0.0997\n",
      "95/223, train_loss: 0.1167, step time: 0.1005\n",
      "96/223, train_loss: 0.1170, step time: 0.1081\n",
      "97/223, train_loss: 0.1090, step time: 0.1155\n",
      "98/223, train_loss: 0.1079, step time: 0.1100\n",
      "99/223, train_loss: 0.1269, step time: 0.1045\n",
      "100/223, train_loss: 0.1193, step time: 0.1013\n",
      "101/223, train_loss: 0.1182, step time: 0.1048\n",
      "102/223, train_loss: 0.1176, step time: 0.1116\n",
      "103/223, train_loss: 0.1101, step time: 0.1284\n",
      "104/223, train_loss: 0.1109, step time: 0.1003\n",
      "105/223, train_loss: 0.1124, step time: 0.1231\n",
      "106/223, train_loss: 0.1050, step time: 0.1069\n",
      "107/223, train_loss: 0.1218, step time: 0.1001\n",
      "108/223, train_loss: 0.1142, step time: 0.1151\n",
      "109/223, train_loss: 0.1091, step time: 0.1004\n",
      "110/223, train_loss: 0.1340, step time: 0.1008\n",
      "111/223, train_loss: 0.1175, step time: 0.1202\n",
      "112/223, train_loss: 0.1173, step time: 0.1126\n",
      "113/223, train_loss: 0.1036, step time: 0.1016\n",
      "114/223, train_loss: 0.1072, step time: 0.1047\n",
      "115/223, train_loss: 0.1263, step time: 0.1117\n",
      "116/223, train_loss: 0.1197, step time: 0.1006\n",
      "117/223, train_loss: 0.1113, step time: 0.1103\n",
      "118/223, train_loss: 0.1220, step time: 0.1104\n",
      "119/223, train_loss: 0.1139, step time: 0.1008\n",
      "120/223, train_loss: 0.1283, step time: 0.1000\n",
      "121/223, train_loss: 0.1108, step time: 0.1003\n",
      "122/223, train_loss: 0.1245, step time: 0.1111\n",
      "123/223, train_loss: 0.0964, step time: 0.1211\n",
      "124/223, train_loss: 0.1120, step time: 0.1027\n",
      "125/223, train_loss: 0.1121, step time: 0.1139\n",
      "126/223, train_loss: 0.1143, step time: 0.1001\n",
      "127/223, train_loss: 0.1282, step time: 0.1056\n",
      "128/223, train_loss: 0.0999, step time: 0.0987\n",
      "129/223, train_loss: 0.1193, step time: 0.0996\n",
      "130/223, train_loss: 0.1087, step time: 0.1122\n",
      "131/223, train_loss: 0.1057, step time: 0.1010\n",
      "132/223, train_loss: 0.1216, step time: 0.0999\n",
      "133/223, train_loss: 0.1320, step time: 0.1167\n",
      "134/223, train_loss: 0.1276, step time: 0.1004\n",
      "135/223, train_loss: 0.1164, step time: 0.1111\n",
      "136/223, train_loss: 0.1098, step time: 0.1189\n",
      "137/223, train_loss: 0.1286, step time: 0.1177\n",
      "138/223, train_loss: 0.1144, step time: 0.1109\n",
      "139/223, train_loss: 0.1086, step time: 0.1134\n",
      "140/223, train_loss: 0.1078, step time: 0.1121\n",
      "141/223, train_loss: 0.1282, step time: 0.1151\n",
      "142/223, train_loss: 0.1094, step time: 0.1017\n",
      "143/223, train_loss: 0.1241, step time: 0.1006\n",
      "144/223, train_loss: 0.1122, step time: 0.1010\n",
      "145/223, train_loss: 0.1020, step time: 0.1255\n",
      "146/223, train_loss: 0.1102, step time: 0.1063\n",
      "147/223, train_loss: 0.1307, step time: 0.1053\n",
      "148/223, train_loss: 0.1096, step time: 0.1128\n",
      "149/223, train_loss: 0.1246, step time: 0.1118\n",
      "150/223, train_loss: 0.1042, step time: 0.1014\n",
      "151/223, train_loss: 0.1101, step time: 0.0999\n",
      "152/223, train_loss: 0.1195, step time: 0.1004\n",
      "153/223, train_loss: 0.1087, step time: 0.1126\n",
      "154/223, train_loss: 0.1140, step time: 0.1109\n",
      "155/223, train_loss: 0.1117, step time: 0.1004\n",
      "156/223, train_loss: 0.3220, step time: 0.1006\n",
      "157/223, train_loss: 0.1175, step time: 0.1130\n",
      "158/223, train_loss: 0.1277, step time: 0.1043\n",
      "159/223, train_loss: 0.1164, step time: 0.1033\n",
      "160/223, train_loss: 0.1128, step time: 0.1004\n",
      "161/223, train_loss: 0.1099, step time: 0.1071\n",
      "162/223, train_loss: 0.1193, step time: 0.1039\n",
      "163/223, train_loss: 0.1224, step time: 0.1034\n",
      "164/223, train_loss: 0.1238, step time: 0.1006\n",
      "165/223, train_loss: 0.1157, step time: 0.1126\n",
      "166/223, train_loss: 0.1258, step time: 0.0999\n",
      "167/223, train_loss: 0.1089, step time: 0.0999\n",
      "168/223, train_loss: 0.1137, step time: 0.1002\n",
      "169/223, train_loss: 0.1152, step time: 0.1006\n",
      "170/223, train_loss: 0.1266, step time: 0.1065\n",
      "171/223, train_loss: 0.1072, step time: 0.1006\n",
      "172/223, train_loss: 0.1066, step time: 0.1117\n",
      "173/223, train_loss: 0.1216, step time: 0.1007\n",
      "174/223, train_loss: 0.1145, step time: 0.1101\n",
      "175/223, train_loss: 0.1205, step time: 0.1217\n",
      "176/223, train_loss: 0.1027, step time: 0.1082\n",
      "177/223, train_loss: 0.1128, step time: 0.1011\n",
      "178/223, train_loss: 0.1130, step time: 0.1125\n",
      "179/223, train_loss: 0.1167, step time: 0.1112\n",
      "180/223, train_loss: 0.1170, step time: 0.1018\n",
      "181/223, train_loss: 0.1097, step time: 0.1007\n",
      "182/223, train_loss: 0.1091, step time: 0.1001\n",
      "183/223, train_loss: 0.1281, step time: 0.1053\n",
      "184/223, train_loss: 0.1144, step time: 0.1002\n",
      "185/223, train_loss: 0.1209, step time: 0.1010\n",
      "186/223, train_loss: 0.1109, step time: 0.0994\n",
      "187/223, train_loss: 0.1060, step time: 0.1074\n",
      "188/223, train_loss: 0.0998, step time: 0.1000\n",
      "189/223, train_loss: 0.1150, step time: 0.0998\n",
      "190/223, train_loss: 0.1184, step time: 0.1012\n",
      "191/223, train_loss: 0.1151, step time: 0.1087\n",
      "192/223, train_loss: 0.1253, step time: 0.0996\n",
      "193/223, train_loss: 0.1163, step time: 0.0984\n",
      "194/223, train_loss: 0.1039, step time: 0.1048\n",
      "195/223, train_loss: 0.1090, step time: 0.1014\n",
      "196/223, train_loss: 0.1070, step time: 0.1005\n",
      "197/223, train_loss: 0.1061, step time: 0.1007\n",
      "198/223, train_loss: 0.1142, step time: 0.1010\n",
      "199/223, train_loss: 0.1133, step time: 0.1053\n",
      "200/223, train_loss: 0.1153, step time: 0.1008\n",
      "201/223, train_loss: 0.1098, step time: 0.1214\n",
      "202/223, train_loss: 0.1166, step time: 0.1002\n",
      "203/223, train_loss: 0.1302, step time: 0.1147\n",
      "204/223, train_loss: 0.1103, step time: 0.0986\n",
      "205/223, train_loss: 0.1136, step time: 0.0985\n",
      "206/223, train_loss: 0.1175, step time: 0.1015\n",
      "207/223, train_loss: 0.1225, step time: 0.1081\n",
      "208/223, train_loss: 0.1177, step time: 0.1049\n",
      "209/223, train_loss: 0.1348, step time: 0.1010\n",
      "210/223, train_loss: 0.1121, step time: 0.1077\n",
      "211/223, train_loss: 0.1239, step time: 0.1083\n",
      "212/223, train_loss: 0.1147, step time: 0.1057\n",
      "213/223, train_loss: 0.1122, step time: 0.1047\n",
      "214/223, train_loss: 0.1139, step time: 0.1003\n",
      "215/223, train_loss: 0.1128, step time: 0.1176\n",
      "216/223, train_loss: 0.1267, step time: 0.1093\n",
      "217/223, train_loss: 0.1140, step time: 0.1019\n",
      "218/223, train_loss: 0.1204, step time: 0.0990\n",
      "219/223, train_loss: 0.1126, step time: 0.0997\n",
      "220/223, train_loss: 0.1220, step time: 0.0990\n",
      "221/223, train_loss: 0.1049, step time: 0.0992\n",
      "222/223, train_loss: 0.1278, step time: 0.1003\n",
      "223/223, train_loss: 0.1036, step time: 0.0993\n",
      "epoch 88 average loss: 0.1173\n",
      "time consuming of epoch 88 is: 85.9087\n",
      "----------\n",
      "epoch 89/300\n",
      "1/223, train_loss: 0.1069, step time: 0.1008\n",
      "2/223, train_loss: 0.1202, step time: 0.1071\n",
      "3/223, train_loss: 0.1365, step time: 0.1004\n",
      "4/223, train_loss: 0.1077, step time: 0.1012\n",
      "5/223, train_loss: 0.1233, step time: 0.1063\n",
      "6/223, train_loss: 0.1253, step time: 0.1059\n",
      "7/223, train_loss: 0.1301, step time: 0.1094\n",
      "8/223, train_loss: 0.1148, step time: 0.0997\n",
      "9/223, train_loss: 0.1121, step time: 0.1051\n",
      "10/223, train_loss: 0.1226, step time: 0.1022\n",
      "11/223, train_loss: 0.1162, step time: 0.1354\n",
      "12/223, train_loss: 0.1180, step time: 0.1127\n",
      "13/223, train_loss: 0.1080, step time: 0.1054\n",
      "14/223, train_loss: 0.1142, step time: 0.1085\n",
      "15/223, train_loss: 0.1254, step time: 0.1254\n",
      "16/223, train_loss: 0.1294, step time: 0.1017\n",
      "17/223, train_loss: 0.1171, step time: 0.1102\n",
      "18/223, train_loss: 0.1073, step time: 0.1108\n",
      "19/223, train_loss: 0.1238, step time: 0.1282\n",
      "20/223, train_loss: 0.1208, step time: 0.1101\n",
      "21/223, train_loss: 0.1040, step time: 0.1025\n",
      "22/223, train_loss: 0.1254, step time: 0.1141\n",
      "23/223, train_loss: 0.1169, step time: 0.1127\n",
      "24/223, train_loss: 0.1264, step time: 0.1147\n",
      "25/223, train_loss: 0.1146, step time: 0.1117\n",
      "26/223, train_loss: 0.1062, step time: 0.1113\n",
      "27/223, train_loss: 0.1218, step time: 0.1009\n",
      "28/223, train_loss: 0.1056, step time: 0.1006\n",
      "29/223, train_loss: 0.1206, step time: 0.1001\n",
      "30/223, train_loss: 0.1123, step time: 0.1060\n",
      "31/223, train_loss: 0.1194, step time: 0.1014\n",
      "32/223, train_loss: 0.1121, step time: 0.1119\n",
      "33/223, train_loss: 0.1170, step time: 0.1024\n",
      "34/223, train_loss: 0.1228, step time: 0.1140\n",
      "35/223, train_loss: 0.1254, step time: 0.1185\n",
      "36/223, train_loss: 0.1030, step time: 0.1117\n",
      "37/223, train_loss: 0.1165, step time: 0.1129\n",
      "38/223, train_loss: 0.1056, step time: 0.1178\n",
      "39/223, train_loss: 0.1102, step time: 0.1117\n",
      "40/223, train_loss: 0.1117, step time: 0.1177\n",
      "41/223, train_loss: 0.1148, step time: 0.1010\n",
      "42/223, train_loss: 0.1078, step time: 0.1168\n",
      "43/223, train_loss: 0.1152, step time: 0.1008\n",
      "44/223, train_loss: 0.1208, step time: 0.1289\n",
      "45/223, train_loss: 0.1191, step time: 0.1218\n",
      "46/223, train_loss: 0.1130, step time: 0.1117\n",
      "47/223, train_loss: 0.1208, step time: 0.1106\n",
      "48/223, train_loss: 0.1144, step time: 0.1094\n",
      "49/223, train_loss: 0.1141, step time: 0.1059\n",
      "50/223, train_loss: 0.1197, step time: 0.1073\n",
      "51/223, train_loss: 0.1137, step time: 0.1006\n",
      "52/223, train_loss: 0.1187, step time: 0.1008\n",
      "53/223, train_loss: 0.1131, step time: 0.1268\n",
      "54/223, train_loss: 0.1164, step time: 0.1120\n",
      "55/223, train_loss: 0.1240, step time: 0.1000\n",
      "56/223, train_loss: 0.1187, step time: 0.1013\n",
      "57/223, train_loss: 0.1159, step time: 0.1010\n",
      "58/223, train_loss: 0.1121, step time: 0.1137\n",
      "59/223, train_loss: 0.1106, step time: 0.1005\n",
      "60/223, train_loss: 0.1256, step time: 0.1003\n",
      "61/223, train_loss: 0.1081, step time: 0.1007\n",
      "62/223, train_loss: 0.1261, step time: 0.1060\n",
      "63/223, train_loss: 0.1162, step time: 0.1128\n",
      "64/223, train_loss: 0.1145, step time: 0.1016\n",
      "65/223, train_loss: 0.1194, step time: 0.1134\n",
      "66/223, train_loss: 0.1043, step time: 0.1055\n",
      "67/223, train_loss: 0.1180, step time: 0.1101\n",
      "68/223, train_loss: 0.1077, step time: 0.1243\n",
      "69/223, train_loss: 0.1054, step time: 0.1079\n",
      "70/223, train_loss: 0.1143, step time: 0.1120\n",
      "71/223, train_loss: 0.1086, step time: 0.1001\n",
      "72/223, train_loss: 0.1085, step time: 0.1000\n",
      "73/223, train_loss: 0.1021, step time: 0.1057\n",
      "74/223, train_loss: 0.1363, step time: 0.1087\n",
      "75/223, train_loss: 0.1221, step time: 0.1092\n",
      "76/223, train_loss: 0.1222, step time: 0.0983\n",
      "77/223, train_loss: 0.1132, step time: 0.1115\n",
      "78/223, train_loss: 0.1245, step time: 0.1178\n",
      "79/223, train_loss: 0.1166, step time: 0.1417\n",
      "80/223, train_loss: 0.1187, step time: 0.1214\n",
      "81/223, train_loss: 0.1063, step time: 0.1000\n",
      "82/223, train_loss: 0.1178, step time: 0.1202\n",
      "83/223, train_loss: 0.1218, step time: 0.1092\n",
      "84/223, train_loss: 0.1142, step time: 0.1146\n",
      "85/223, train_loss: 0.1111, step time: 0.1047\n",
      "86/223, train_loss: 0.1150, step time: 0.1011\n",
      "87/223, train_loss: 0.1229, step time: 0.1001\n",
      "88/223, train_loss: 0.1286, step time: 0.0985\n",
      "89/223, train_loss: 0.1120, step time: 0.1263\n",
      "90/223, train_loss: 0.1127, step time: 0.1200\n",
      "91/223, train_loss: 0.1098, step time: 0.1006\n",
      "92/223, train_loss: 0.1324, step time: 0.1005\n",
      "93/223, train_loss: 0.1217, step time: 0.1009\n",
      "94/223, train_loss: 0.1164, step time: 0.1114\n",
      "95/223, train_loss: 0.1088, step time: 0.1101\n",
      "96/223, train_loss: 0.1187, step time: 0.1009\n",
      "97/223, train_loss: 0.1117, step time: 0.1004\n",
      "98/223, train_loss: 0.1220, step time: 0.0999\n",
      "99/223, train_loss: 0.1116, step time: 0.1068\n",
      "100/223, train_loss: 0.1142, step time: 0.1056\n",
      "101/223, train_loss: 0.1232, step time: 0.0997\n",
      "102/223, train_loss: 0.1133, step time: 0.0995\n",
      "103/223, train_loss: 0.1192, step time: 0.1000\n",
      "104/223, train_loss: 0.1037, step time: 0.1095\n",
      "105/223, train_loss: 0.1273, step time: 0.1866\n",
      "106/223, train_loss: 0.1237, step time: 0.1005\n",
      "107/223, train_loss: 0.1218, step time: 0.1003\n",
      "108/223, train_loss: 0.1220, step time: 0.1059\n",
      "109/223, train_loss: 0.1189, step time: 0.0998\n",
      "110/223, train_loss: 0.1175, step time: 0.0998\n",
      "111/223, train_loss: 0.1212, step time: 0.1001\n",
      "112/223, train_loss: 0.1121, step time: 0.1031\n",
      "113/223, train_loss: 0.1100, step time: 0.1008\n",
      "114/223, train_loss: 0.1187, step time: 0.1014\n",
      "115/223, train_loss: 0.1177, step time: 0.1084\n",
      "116/223, train_loss: 0.1169, step time: 0.1004\n",
      "117/223, train_loss: 0.1177, step time: 0.1122\n",
      "118/223, train_loss: 0.3097, step time: 0.1118\n",
      "119/223, train_loss: 0.1064, step time: 0.1266\n",
      "120/223, train_loss: 0.1054, step time: 0.1127\n",
      "121/223, train_loss: 0.1061, step time: 0.1159\n",
      "122/223, train_loss: 0.1047, step time: 0.1020\n",
      "123/223, train_loss: 0.1111, step time: 0.0999\n",
      "124/223, train_loss: 0.1189, step time: 0.1006\n",
      "125/223, train_loss: 0.1243, step time: 0.1045\n",
      "126/223, train_loss: 0.1081, step time: 0.1099\n",
      "127/223, train_loss: 0.1167, step time: 0.1029\n",
      "128/223, train_loss: 0.1135, step time: 0.1006\n",
      "129/223, train_loss: 0.1145, step time: 0.1056\n",
      "130/223, train_loss: 0.1202, step time: 0.1087\n",
      "131/223, train_loss: 0.1111, step time: 0.1153\n",
      "132/223, train_loss: 0.1197, step time: 0.1008\n",
      "133/223, train_loss: 0.1212, step time: 0.1125\n",
      "134/223, train_loss: 0.1242, step time: 0.1147\n",
      "135/223, train_loss: 0.1342, step time: 0.1239\n",
      "136/223, train_loss: 0.1233, step time: 0.1133\n",
      "137/223, train_loss: 0.1183, step time: 0.1043\n",
      "138/223, train_loss: 0.1144, step time: 0.1272\n",
      "139/223, train_loss: 0.1262, step time: 0.1196\n",
      "140/223, train_loss: 0.1348, step time: 0.1042\n",
      "141/223, train_loss: 0.1155, step time: 0.1173\n",
      "142/223, train_loss: 0.1256, step time: 0.1072\n",
      "143/223, train_loss: 0.1122, step time: 0.1130\n",
      "144/223, train_loss: 0.1211, step time: 0.1257\n",
      "145/223, train_loss: 0.1233, step time: 0.1131\n",
      "146/223, train_loss: 0.1124, step time: 0.1174\n",
      "147/223, train_loss: 0.1233, step time: 0.1248\n",
      "148/223, train_loss: 0.1248, step time: 0.1130\n",
      "149/223, train_loss: 0.1233, step time: 0.1048\n",
      "150/223, train_loss: 0.1227, step time: 0.1011\n",
      "151/223, train_loss: 0.1233, step time: 0.1012\n",
      "152/223, train_loss: 0.1093, step time: 0.1249\n",
      "153/223, train_loss: 0.1115, step time: 0.1171\n",
      "154/223, train_loss: 0.1068, step time: 0.1005\n",
      "155/223, train_loss: 0.1114, step time: 0.1001\n",
      "156/223, train_loss: 0.1199, step time: 0.1067\n",
      "157/223, train_loss: 0.1095, step time: 0.1075\n",
      "158/223, train_loss: 0.1139, step time: 0.1073\n",
      "159/223, train_loss: 0.1132, step time: 0.1319\n",
      "160/223, train_loss: 0.1365, step time: 0.1095\n",
      "161/223, train_loss: 0.1155, step time: 0.1130\n",
      "162/223, train_loss: 0.1167, step time: 0.1006\n",
      "163/223, train_loss: 0.1171, step time: 0.1006\n",
      "164/223, train_loss: 0.1095, step time: 0.1080\n",
      "165/223, train_loss: 0.1019, step time: 0.1001\n",
      "166/223, train_loss: 0.1117, step time: 0.1333\n",
      "167/223, train_loss: 0.1045, step time: 0.1008\n",
      "168/223, train_loss: 0.1178, step time: 0.1010\n",
      "169/223, train_loss: 0.1054, step time: 0.1083\n",
      "170/223, train_loss: 0.1167, step time: 0.0990\n",
      "171/223, train_loss: 0.1141, step time: 0.1260\n",
      "172/223, train_loss: 0.1144, step time: 0.1028\n",
      "173/223, train_loss: 0.1084, step time: 0.1062\n",
      "174/223, train_loss: 0.1056, step time: 0.0999\n",
      "175/223, train_loss: 0.1221, step time: 0.0990\n",
      "176/223, train_loss: 0.1117, step time: 0.1024\n",
      "177/223, train_loss: 0.1128, step time: 0.1203\n",
      "178/223, train_loss: 0.1250, step time: 0.1068\n",
      "179/223, train_loss: 0.1073, step time: 0.1003\n",
      "180/223, train_loss: 0.1145, step time: 0.1050\n",
      "181/223, train_loss: 0.1156, step time: 0.0996\n",
      "182/223, train_loss: 0.1146, step time: 0.1039\n",
      "183/223, train_loss: 0.1185, step time: 0.1001\n",
      "184/223, train_loss: 0.1322, step time: 0.1142\n",
      "185/223, train_loss: 0.1074, step time: 0.1090\n",
      "186/223, train_loss: 0.1108, step time: 0.1018\n",
      "187/223, train_loss: 0.1256, step time: 0.1006\n",
      "188/223, train_loss: 0.1232, step time: 0.1092\n",
      "189/223, train_loss: 0.1092, step time: 0.1068\n",
      "190/223, train_loss: 0.1106, step time: 0.1010\n",
      "191/223, train_loss: 0.1221, step time: 0.1009\n",
      "192/223, train_loss: 0.1166, step time: 0.1008\n",
      "193/223, train_loss: 0.1134, step time: 0.0984\n",
      "194/223, train_loss: 0.1084, step time: 0.1040\n",
      "195/223, train_loss: 0.1098, step time: 0.1044\n",
      "196/223, train_loss: 0.1195, step time: 0.1002\n",
      "197/223, train_loss: 0.1029, step time: 0.1066\n",
      "198/223, train_loss: 0.1106, step time: 0.1865\n",
      "199/223, train_loss: 0.1019, step time: 0.1138\n",
      "200/223, train_loss: 0.1168, step time: 0.1232\n",
      "201/223, train_loss: 0.1146, step time: 0.1243\n",
      "202/223, train_loss: 0.1124, step time: 0.1035\n",
      "203/223, train_loss: 0.1068, step time: 0.1161\n",
      "204/223, train_loss: 0.1274, step time: 0.1284\n",
      "205/223, train_loss: 0.1169, step time: 0.1069\n",
      "206/223, train_loss: 0.1063, step time: 0.1013\n",
      "207/223, train_loss: 0.1043, step time: 0.1193\n",
      "208/223, train_loss: 0.1156, step time: 0.0999\n",
      "209/223, train_loss: 0.1161, step time: 0.1204\n",
      "210/223, train_loss: 0.1095, step time: 0.1157\n",
      "211/223, train_loss: 0.1044, step time: 0.1269\n",
      "212/223, train_loss: 0.1130, step time: 0.1058\n",
      "213/223, train_loss: 0.1078, step time: 0.1149\n",
      "214/223, train_loss: 0.1207, step time: 0.1158\n",
      "215/223, train_loss: 0.1239, step time: 0.1194\n",
      "216/223, train_loss: 0.1080, step time: 0.1115\n",
      "217/223, train_loss: 0.1265, step time: 0.1235\n",
      "218/223, train_loss: 0.1178, step time: 0.1260\n",
      "219/223, train_loss: 0.1123, step time: 0.1206\n",
      "220/223, train_loss: 0.1125, step time: 0.1115\n",
      "221/223, train_loss: 0.1061, step time: 0.1025\n",
      "222/223, train_loss: 0.1376, step time: 0.1005\n",
      "223/223, train_loss: 0.1236, step time: 0.1008\n",
      "epoch 89 average loss: 0.1170\n",
      "time consuming of epoch 89 is: 89.5515\n",
      "----------\n",
      "epoch 90/300\n",
      "1/223, train_loss: 0.1208, step time: 0.1146\n",
      "2/223, train_loss: 0.1148, step time: 0.1047\n",
      "3/223, train_loss: 0.1045, step time: 0.1169\n",
      "4/223, train_loss: 0.1132, step time: 0.1161\n",
      "5/223, train_loss: 0.1160, step time: 0.1205\n",
      "6/223, train_loss: 0.1046, step time: 0.0997\n",
      "7/223, train_loss: 0.1262, step time: 0.1007\n",
      "8/223, train_loss: 0.1108, step time: 0.1050\n",
      "9/223, train_loss: 0.1027, step time: 0.1234\n",
      "10/223, train_loss: 0.1193, step time: 0.1167\n",
      "11/223, train_loss: 0.1093, step time: 0.1010\n",
      "12/223, train_loss: 0.1048, step time: 0.1065\n",
      "13/223, train_loss: 0.1067, step time: 0.1128\n",
      "14/223, train_loss: 0.1056, step time: 0.1094\n",
      "15/223, train_loss: 0.1093, step time: 0.1193\n",
      "16/223, train_loss: 0.1181, step time: 0.1256\n",
      "17/223, train_loss: 0.1087, step time: 0.1086\n",
      "18/223, train_loss: 0.1206, step time: 0.1047\n",
      "19/223, train_loss: 0.1174, step time: 0.1220\n",
      "20/223, train_loss: 0.1143, step time: 0.1243\n",
      "21/223, train_loss: 0.1044, step time: 0.1179\n",
      "22/223, train_loss: 0.1131, step time: 0.0985\n",
      "23/223, train_loss: 0.1126, step time: 0.0992\n",
      "24/223, train_loss: 0.1119, step time: 0.0986\n",
      "25/223, train_loss: 0.0994, step time: 0.1005\n",
      "26/223, train_loss: 0.1199, step time: 0.1140\n",
      "27/223, train_loss: 0.1249, step time: 0.1238\n",
      "28/223, train_loss: 0.1194, step time: 0.1251\n",
      "29/223, train_loss: 0.1085, step time: 0.1100\n",
      "30/223, train_loss: 0.1173, step time: 0.1045\n",
      "31/223, train_loss: 0.1315, step time: 0.1123\n",
      "32/223, train_loss: 0.1250, step time: 0.1151\n",
      "33/223, train_loss: 0.1086, step time: 0.1169\n",
      "34/223, train_loss: 0.1135, step time: 0.0987\n",
      "35/223, train_loss: 0.1183, step time: 0.1104\n",
      "36/223, train_loss: 0.1335, step time: 0.1081\n",
      "37/223, train_loss: 0.1123, step time: 0.1001\n",
      "38/223, train_loss: 0.1182, step time: 0.1064\n",
      "39/223, train_loss: 0.1167, step time: 0.1026\n",
      "40/223, train_loss: 0.1241, step time: 0.1043\n",
      "41/223, train_loss: 0.1184, step time: 0.1064\n",
      "42/223, train_loss: 0.1169, step time: 0.1175\n",
      "43/223, train_loss: 0.1135, step time: 0.1105\n",
      "44/223, train_loss: 0.1180, step time: 0.1001\n",
      "45/223, train_loss: 0.1240, step time: 0.1003\n",
      "46/223, train_loss: 0.1102, step time: 0.1126\n",
      "47/223, train_loss: 0.1128, step time: 0.1113\n",
      "48/223, train_loss: 0.1129, step time: 0.1008\n",
      "49/223, train_loss: 0.1142, step time: 0.1092\n",
      "50/223, train_loss: 0.1136, step time: 0.1050\n",
      "51/223, train_loss: 0.1209, step time: 0.0998\n",
      "52/223, train_loss: 0.1358, step time: 0.0997\n",
      "53/223, train_loss: 0.1161, step time: 0.0998\n",
      "54/223, train_loss: 0.1277, step time: 0.1095\n",
      "55/223, train_loss: 0.1190, step time: 0.1054\n",
      "56/223, train_loss: 0.1235, step time: 0.1001\n",
      "57/223, train_loss: 0.1124, step time: 0.1048\n",
      "58/223, train_loss: 0.1070, step time: 0.0997\n",
      "59/223, train_loss: 0.1220, step time: 0.1003\n",
      "60/223, train_loss: 0.1203, step time: 0.1007\n",
      "61/223, train_loss: 0.1263, step time: 0.1026\n",
      "62/223, train_loss: 0.1369, step time: 0.1002\n",
      "63/223, train_loss: 0.1066, step time: 0.1004\n",
      "64/223, train_loss: 0.3102, step time: 0.1093\n",
      "65/223, train_loss: 0.1438, step time: 0.1094\n",
      "66/223, train_loss: 0.1176, step time: 0.1006\n",
      "67/223, train_loss: 0.1258, step time: 0.0996\n",
      "68/223, train_loss: 0.1138, step time: 0.0997\n",
      "69/223, train_loss: 0.1136, step time: 0.1084\n",
      "70/223, train_loss: 0.1094, step time: 0.1042\n",
      "71/223, train_loss: 0.1337, step time: 0.1000\n",
      "72/223, train_loss: 0.1094, step time: 0.1007\n",
      "73/223, train_loss: 0.1176, step time: 0.1226\n",
      "74/223, train_loss: 0.1272, step time: 0.1102\n",
      "75/223, train_loss: 0.1194, step time: 0.0997\n",
      "76/223, train_loss: 0.1185, step time: 0.1008\n",
      "77/223, train_loss: 0.1210, step time: 0.1128\n",
      "78/223, train_loss: 0.1088, step time: 0.0997\n",
      "79/223, train_loss: 0.1234, step time: 0.1056\n",
      "80/223, train_loss: 0.1186, step time: 0.0994\n",
      "81/223, train_loss: 0.1145, step time: 0.1232\n",
      "82/223, train_loss: 0.1044, step time: 0.1420\n",
      "83/223, train_loss: 0.1188, step time: 0.1239\n",
      "84/223, train_loss: 0.1154, step time: 0.1004\n",
      "85/223, train_loss: 0.1249, step time: 0.1008\n",
      "86/223, train_loss: 0.1324, step time: 0.1548\n",
      "87/223, train_loss: 0.1108, step time: 0.1155\n",
      "88/223, train_loss: 0.1128, step time: 0.1137\n",
      "89/223, train_loss: 0.1192, step time: 0.1006\n",
      "90/223, train_loss: 0.1186, step time: 0.1098\n",
      "91/223, train_loss: 0.1026, step time: 0.1058\n",
      "92/223, train_loss: 0.1204, step time: 0.1127\n",
      "93/223, train_loss: 0.1199, step time: 0.1172\n",
      "94/223, train_loss: 0.1164, step time: 0.1176\n",
      "95/223, train_loss: 0.1269, step time: 0.1170\n",
      "96/223, train_loss: 0.1236, step time: 0.1295\n",
      "97/223, train_loss: 0.1084, step time: 0.1496\n",
      "98/223, train_loss: 0.1124, step time: 0.1161\n",
      "99/223, train_loss: 0.1208, step time: 0.1058\n",
      "100/223, train_loss: 0.1083, step time: 0.1053\n",
      "101/223, train_loss: 0.1228, step time: 0.1356\n",
      "102/223, train_loss: 0.1151, step time: 0.1041\n",
      "103/223, train_loss: 0.1070, step time: 0.1090\n",
      "104/223, train_loss: 0.1146, step time: 0.1458\n",
      "105/223, train_loss: 0.1192, step time: 0.1196\n",
      "106/223, train_loss: 0.1284, step time: 0.1149\n",
      "107/223, train_loss: 0.1085, step time: 0.1153\n",
      "108/223, train_loss: 0.1244, step time: 0.1090\n",
      "109/223, train_loss: 0.1350, step time: 0.1012\n",
      "110/223, train_loss: 0.1098, step time: 0.1122\n",
      "111/223, train_loss: 0.1123, step time: 0.1044\n",
      "112/223, train_loss: 0.1285, step time: 0.1102\n",
      "113/223, train_loss: 0.1003, step time: 0.1015\n",
      "114/223, train_loss: 0.1115, step time: 0.1085\n",
      "115/223, train_loss: 0.1183, step time: 0.1125\n",
      "116/223, train_loss: 0.1275, step time: 0.0998\n",
      "117/223, train_loss: 0.1172, step time: 0.1000\n",
      "118/223, train_loss: 0.1185, step time: 0.1148\n",
      "119/223, train_loss: 0.1162, step time: 0.1019\n",
      "120/223, train_loss: 0.1181, step time: 0.1007\n",
      "121/223, train_loss: 0.1097, step time: 0.1105\n",
      "122/223, train_loss: 0.1057, step time: 0.1362\n",
      "123/223, train_loss: 0.1221, step time: 0.1239\n",
      "124/223, train_loss: 0.1055, step time: 0.1119\n",
      "125/223, train_loss: 0.1021, step time: 0.1234\n",
      "126/223, train_loss: 0.1166, step time: 0.1070\n",
      "127/223, train_loss: 0.1117, step time: 0.1159\n",
      "128/223, train_loss: 0.1153, step time: 0.1054\n",
      "129/223, train_loss: 0.1041, step time: 0.1373\n",
      "130/223, train_loss: 0.1212, step time: 0.1236\n",
      "131/223, train_loss: 0.1189, step time: 0.1233\n",
      "132/223, train_loss: 0.1197, step time: 0.1147\n",
      "133/223, train_loss: 0.1127, step time: 0.1112\n",
      "134/223, train_loss: 0.1007, step time: 0.1098\n",
      "135/223, train_loss: 0.1217, step time: 0.1038\n",
      "136/223, train_loss: 0.1065, step time: 0.1071\n",
      "137/223, train_loss: 0.1163, step time: 0.1209\n",
      "138/223, train_loss: 0.1130, step time: 0.1010\n",
      "139/223, train_loss: 0.1162, step time: 0.1022\n",
      "140/223, train_loss: 0.1140, step time: 0.1011\n",
      "141/223, train_loss: 0.1148, step time: 0.1166\n",
      "142/223, train_loss: 0.1316, step time: 0.1086\n",
      "143/223, train_loss: 0.1148, step time: 0.1086\n",
      "144/223, train_loss: 0.1076, step time: 0.1130\n",
      "145/223, train_loss: 0.1074, step time: 0.1150\n",
      "146/223, train_loss: 0.1353, step time: 0.1027\n",
      "147/223, train_loss: 0.1352, step time: 0.1160\n",
      "148/223, train_loss: 0.1264, step time: 0.1160\n",
      "149/223, train_loss: 0.1144, step time: 0.1207\n",
      "150/223, train_loss: 0.1378, step time: 0.1376\n",
      "151/223, train_loss: 0.1165, step time: 0.1276\n",
      "152/223, train_loss: 0.1143, step time: 0.1017\n",
      "153/223, train_loss: 0.1196, step time: 0.1011\n",
      "154/223, train_loss: 0.1196, step time: 0.0993\n",
      "155/223, train_loss: 0.1187, step time: 0.0999\n",
      "156/223, train_loss: 0.1169, step time: 0.1004\n",
      "157/223, train_loss: 0.1289, step time: 0.1264\n",
      "158/223, train_loss: 0.1253, step time: 0.1008\n",
      "159/223, train_loss: 0.1201, step time: 0.1317\n",
      "160/223, train_loss: 0.1213, step time: 0.1002\n",
      "161/223, train_loss: 0.1348, step time: 0.1005\n",
      "162/223, train_loss: 0.1288, step time: 0.1136\n",
      "163/223, train_loss: 0.1239, step time: 0.1009\n",
      "164/223, train_loss: 0.1056, step time: 0.0996\n",
      "165/223, train_loss: 0.1132, step time: 0.1016\n",
      "166/223, train_loss: 0.1133, step time: 0.1150\n",
      "167/223, train_loss: 0.1167, step time: 0.0992\n",
      "168/223, train_loss: 0.1220, step time: 0.1009\n",
      "169/223, train_loss: 0.1148, step time: 0.1161\n",
      "170/223, train_loss: 0.1158, step time: 0.1108\n",
      "171/223, train_loss: 0.1178, step time: 0.1083\n",
      "172/223, train_loss: 0.1241, step time: 0.1453\n",
      "173/223, train_loss: 0.1180, step time: 0.1074\n",
      "174/223, train_loss: 0.1057, step time: 0.1009\n",
      "175/223, train_loss: 0.1133, step time: 0.1006\n",
      "176/223, train_loss: 0.1226, step time: 0.1003\n",
      "177/223, train_loss: 0.1260, step time: 0.1090\n",
      "178/223, train_loss: 0.1122, step time: 0.1054\n",
      "179/223, train_loss: 0.1024, step time: 0.1143\n",
      "180/223, train_loss: 0.1242, step time: 0.1197\n",
      "181/223, train_loss: 0.1107, step time: 0.1009\n",
      "182/223, train_loss: 0.1121, step time: 0.1001\n",
      "183/223, train_loss: 0.1082, step time: 0.1005\n",
      "184/223, train_loss: 0.1066, step time: 0.1164\n",
      "185/223, train_loss: 0.1206, step time: 0.1048\n",
      "186/223, train_loss: 0.0970, step time: 0.1093\n",
      "187/223, train_loss: 0.1066, step time: 0.1004\n",
      "188/223, train_loss: 0.1155, step time: 0.1331\n",
      "189/223, train_loss: 0.1140, step time: 0.1003\n",
      "190/223, train_loss: 0.1066, step time: 0.1189\n",
      "191/223, train_loss: 0.1129, step time: 0.1274\n",
      "192/223, train_loss: 0.1183, step time: 0.1127\n",
      "193/223, train_loss: 0.1232, step time: 0.1001\n",
      "194/223, train_loss: 0.1152, step time: 0.1021\n",
      "195/223, train_loss: 0.1106, step time: 0.1241\n",
      "196/223, train_loss: 0.1218, step time: 0.1143\n",
      "197/223, train_loss: 0.1072, step time: 0.1009\n",
      "198/223, train_loss: 0.1233, step time: 0.1103\n",
      "199/223, train_loss: 0.1198, step time: 0.1098\n",
      "200/223, train_loss: 0.1084, step time: 0.1015\n",
      "201/223, train_loss: 0.1142, step time: 0.1002\n",
      "202/223, train_loss: 0.1141, step time: 0.1004\n",
      "203/223, train_loss: 0.1025, step time: 0.1050\n",
      "204/223, train_loss: 0.1018, step time: 0.1171\n",
      "205/223, train_loss: 0.1154, step time: 0.1007\n",
      "206/223, train_loss: 0.1069, step time: 0.1078\n",
      "207/223, train_loss: 0.1236, step time: 0.1166\n",
      "208/223, train_loss: 0.1192, step time: 0.1145\n",
      "209/223, train_loss: 0.1215, step time: 0.1095\n",
      "210/223, train_loss: 0.1443, step time: 0.1116\n",
      "211/223, train_loss: 0.1074, step time: 0.1093\n",
      "212/223, train_loss: 0.1182, step time: 0.1131\n",
      "213/223, train_loss: 0.1145, step time: 0.1187\n",
      "214/223, train_loss: 0.1202, step time: 0.1031\n",
      "215/223, train_loss: 0.1317, step time: 0.0997\n",
      "216/223, train_loss: 0.1040, step time: 0.1000\n",
      "217/223, train_loss: 0.1185, step time: 0.1016\n",
      "218/223, train_loss: 0.1076, step time: 0.0999\n",
      "219/223, train_loss: 0.1049, step time: 0.0998\n",
      "220/223, train_loss: 0.1287, step time: 0.1142\n",
      "221/223, train_loss: 0.1179, step time: 0.1128\n",
      "222/223, train_loss: 0.1095, step time: 0.0996\n",
      "223/223, train_loss: 0.1149, step time: 0.1000\n",
      "epoch 90 average loss: 0.1174\n",
      "saved new best metric model\n",
      "current epoch: 90 current mean dice: 0.8485 tc: 0.9156 wt: 0.8591 et: 0.7708\n",
      "best mean dice: 0.8485 at epoch: 90\n",
      "time consuming of epoch 90 is: 91.3194\n",
      "----------\n",
      "epoch 91/300\n",
      "1/223, train_loss: 0.1161, step time: 0.1008\n",
      "2/223, train_loss: 0.1202, step time: 0.1010\n",
      "3/223, train_loss: 0.1145, step time: 0.1012\n",
      "4/223, train_loss: 0.1139, step time: 0.1004\n",
      "5/223, train_loss: 0.1062, step time: 0.1004\n",
      "6/223, train_loss: 0.1059, step time: 0.1074\n",
      "7/223, train_loss: 0.1107, step time: 0.1037\n",
      "8/223, train_loss: 0.1119, step time: 0.1251\n",
      "9/223, train_loss: 0.1170, step time: 0.1098\n",
      "10/223, train_loss: 0.1169, step time: 0.1014\n",
      "11/223, train_loss: 0.1116, step time: 0.1123\n",
      "12/223, train_loss: 0.1217, step time: 0.1190\n",
      "13/223, train_loss: 0.1106, step time: 0.1169\n",
      "14/223, train_loss: 0.1112, step time: 0.1102\n",
      "15/223, train_loss: 0.1309, step time: 0.1165\n",
      "16/223, train_loss: 0.1291, step time: 0.1110\n",
      "17/223, train_loss: 0.1030, step time: 0.1144\n",
      "18/223, train_loss: 0.1189, step time: 0.1163\n",
      "19/223, train_loss: 0.1102, step time: 0.1088\n",
      "20/223, train_loss: 0.1058, step time: 0.1208\n",
      "21/223, train_loss: 0.1040, step time: 0.1016\n",
      "22/223, train_loss: 0.1109, step time: 0.1136\n",
      "23/223, train_loss: 0.1128, step time: 0.1140\n",
      "24/223, train_loss: 0.1110, step time: 0.1005\n",
      "25/223, train_loss: 0.1212, step time: 0.1062\n",
      "26/223, train_loss: 0.1279, step time: 0.1118\n",
      "27/223, train_loss: 0.1235, step time: 0.1229\n",
      "28/223, train_loss: 0.1080, step time: 0.1133\n",
      "29/223, train_loss: 0.1169, step time: 0.0999\n",
      "30/223, train_loss: 0.1129, step time: 0.1163\n",
      "31/223, train_loss: 0.1289, step time: 0.1099\n",
      "32/223, train_loss: 0.1289, step time: 0.1090\n",
      "33/223, train_loss: 0.1360, step time: 0.1001\n",
      "34/223, train_loss: 0.1049, step time: 0.1042\n",
      "35/223, train_loss: 0.1234, step time: 0.1006\n",
      "36/223, train_loss: 0.0985, step time: 0.1132\n",
      "37/223, train_loss: 0.1345, step time: 0.1097\n",
      "38/223, train_loss: 0.1092, step time: 0.1140\n",
      "39/223, train_loss: 0.1198, step time: 0.1050\n",
      "40/223, train_loss: 0.1115, step time: 0.1126\n",
      "41/223, train_loss: 0.1216, step time: 0.1048\n",
      "42/223, train_loss: 0.1175, step time: 0.1006\n",
      "43/223, train_loss: 0.1065, step time: 0.1216\n",
      "44/223, train_loss: 0.1175, step time: 0.0990\n",
      "45/223, train_loss: 0.1101, step time: 0.0996\n",
      "46/223, train_loss: 0.1089, step time: 0.1254\n",
      "47/223, train_loss: 0.1418, step time: 0.1146\n",
      "48/223, train_loss: 0.1006, step time: 0.1108\n",
      "49/223, train_loss: 0.1215, step time: 0.1107\n",
      "50/223, train_loss: 0.1296, step time: 0.1050\n",
      "51/223, train_loss: 0.1144, step time: 0.0999\n",
      "52/223, train_loss: 0.1183, step time: 0.1180\n",
      "53/223, train_loss: 0.1425, step time: 0.1132\n",
      "54/223, train_loss: 0.1205, step time: 0.1179\n",
      "55/223, train_loss: 0.1134, step time: 0.1054\n",
      "56/223, train_loss: 0.1169, step time: 0.1355\n",
      "57/223, train_loss: 0.1081, step time: 0.1393\n",
      "58/223, train_loss: 0.1162, step time: 0.1228\n",
      "59/223, train_loss: 0.1295, step time: 0.1140\n",
      "60/223, train_loss: 0.1118, step time: 0.1006\n",
      "61/223, train_loss: 0.1228, step time: 0.1129\n",
      "62/223, train_loss: 0.1180, step time: 0.0997\n",
      "63/223, train_loss: 0.1145, step time: 0.1012\n",
      "64/223, train_loss: 0.1107, step time: 0.1152\n",
      "65/223, train_loss: 0.1232, step time: 0.1178\n",
      "66/223, train_loss: 0.1040, step time: 0.1053\n",
      "67/223, train_loss: 0.1207, step time: 0.1076\n",
      "68/223, train_loss: 0.1234, step time: 0.1121\n",
      "69/223, train_loss: 0.1030, step time: 0.1185\n",
      "70/223, train_loss: 0.1069, step time: 0.1045\n",
      "71/223, train_loss: 0.1092, step time: 0.1519\n",
      "72/223, train_loss: 0.1317, step time: 0.1084\n",
      "73/223, train_loss: 0.1047, step time: 0.1221\n",
      "74/223, train_loss: 0.1138, step time: 0.1077\n",
      "75/223, train_loss: 0.1202, step time: 0.1063\n",
      "76/223, train_loss: 0.1116, step time: 0.1106\n",
      "77/223, train_loss: 0.1205, step time: 0.1045\n",
      "78/223, train_loss: 0.1320, step time: 0.1309\n",
      "79/223, train_loss: 0.1114, step time: 0.1470\n",
      "80/223, train_loss: 0.1168, step time: 0.1118\n",
      "81/223, train_loss: 0.1126, step time: 0.1000\n",
      "82/223, train_loss: 0.1145, step time: 0.1067\n",
      "83/223, train_loss: 0.1168, step time: 0.1136\n",
      "84/223, train_loss: 0.1086, step time: 0.1226\n",
      "85/223, train_loss: 0.1102, step time: 0.1088\n",
      "86/223, train_loss: 0.1190, step time: 0.1179\n",
      "87/223, train_loss: 0.1215, step time: 0.1369\n",
      "88/223, train_loss: 0.1155, step time: 0.1094\n",
      "89/223, train_loss: 0.1163, step time: 0.1165\n",
      "90/223, train_loss: 0.1143, step time: 0.1119\n",
      "91/223, train_loss: 0.1120, step time: 0.1054\n",
      "92/223, train_loss: 0.1138, step time: 0.1139\n",
      "93/223, train_loss: 0.3158, step time: 0.1157\n",
      "94/223, train_loss: 0.1135, step time: 0.1034\n",
      "95/223, train_loss: 0.1149, step time: 0.1225\n",
      "96/223, train_loss: 0.1065, step time: 0.1011\n",
      "97/223, train_loss: 0.1136, step time: 0.1005\n",
      "98/223, train_loss: 0.1268, step time: 0.1006\n",
      "99/223, train_loss: 0.1126, step time: 0.1013\n",
      "100/223, train_loss: 0.1036, step time: 0.0999\n",
      "101/223, train_loss: 0.1225, step time: 0.1244\n",
      "102/223, train_loss: 0.1093, step time: 0.1112\n",
      "103/223, train_loss: 0.1122, step time: 0.1055\n",
      "104/223, train_loss: 0.1136, step time: 0.1001\n",
      "105/223, train_loss: 0.1018, step time: 0.1011\n",
      "106/223, train_loss: 0.1115, step time: 0.1019\n",
      "107/223, train_loss: 0.1150, step time: 0.1001\n",
      "108/223, train_loss: 0.1074, step time: 0.1139\n",
      "109/223, train_loss: 0.1224, step time: 0.0996\n",
      "110/223, train_loss: 0.1374, step time: 0.1039\n",
      "111/223, train_loss: 0.1042, step time: 0.1339\n",
      "112/223, train_loss: 0.1137, step time: 0.1056\n",
      "113/223, train_loss: 0.1083, step time: 0.1245\n",
      "114/223, train_loss: 0.1006, step time: 0.1056\n",
      "115/223, train_loss: 0.1306, step time: 0.1208\n",
      "116/223, train_loss: 0.1174, step time: 0.1035\n",
      "117/223, train_loss: 0.1182, step time: 0.1014\n",
      "118/223, train_loss: 0.1089, step time: 0.1173\n",
      "119/223, train_loss: 0.1236, step time: 0.1079\n",
      "120/223, train_loss: 0.1245, step time: 0.1014\n",
      "121/223, train_loss: 0.1153, step time: 0.1008\n",
      "122/223, train_loss: 0.1120, step time: 0.1150\n",
      "123/223, train_loss: 0.1227, step time: 0.1055\n",
      "124/223, train_loss: 0.1147, step time: 0.1013\n",
      "125/223, train_loss: 0.1025, step time: 0.1002\n",
      "126/223, train_loss: 0.1211, step time: 0.0990\n",
      "127/223, train_loss: 0.1076, step time: 0.0990\n",
      "128/223, train_loss: 0.1092, step time: 0.0997\n",
      "129/223, train_loss: 0.1077, step time: 0.1112\n",
      "130/223, train_loss: 0.1160, step time: 0.1002\n",
      "131/223, train_loss: 0.1129, step time: 0.1031\n",
      "132/223, train_loss: 0.1096, step time: 0.1256\n",
      "133/223, train_loss: 0.1132, step time: 0.1106\n",
      "134/223, train_loss: 0.1057, step time: 0.1076\n",
      "135/223, train_loss: 0.1209, step time: 0.1082\n",
      "136/223, train_loss: 0.1072, step time: 0.1157\n",
      "137/223, train_loss: 0.1201, step time: 0.1004\n",
      "138/223, train_loss: 0.1057, step time: 0.1028\n",
      "139/223, train_loss: 0.1020, step time: 0.1204\n",
      "140/223, train_loss: 0.1031, step time: 0.1006\n",
      "141/223, train_loss: 0.1059, step time: 0.1000\n",
      "142/223, train_loss: 0.1032, step time: 0.1214\n",
      "143/223, train_loss: 0.1299, step time: 0.1001\n",
      "144/223, train_loss: 0.1172, step time: 0.1027\n",
      "145/223, train_loss: 0.1340, step time: 0.1001\n",
      "146/223, train_loss: 0.1106, step time: 0.1035\n",
      "147/223, train_loss: 0.1143, step time: 0.1148\n",
      "148/223, train_loss: 0.1246, step time: 0.1028\n",
      "149/223, train_loss: 0.1227, step time: 0.1004\n",
      "150/223, train_loss: 0.1208, step time: 0.1078\n",
      "151/223, train_loss: 0.1202, step time: 0.1019\n",
      "152/223, train_loss: 0.1177, step time: 0.1004\n",
      "153/223, train_loss: 0.1059, step time: 0.1385\n",
      "154/223, train_loss: 0.1217, step time: 0.1023\n",
      "155/223, train_loss: 0.1245, step time: 0.0998\n",
      "156/223, train_loss: 0.1212, step time: 0.1008\n",
      "157/223, train_loss: 0.1280, step time: 0.1111\n",
      "158/223, train_loss: 0.1152, step time: 0.1100\n",
      "159/223, train_loss: 0.1031, step time: 0.1008\n",
      "160/223, train_loss: 0.1161, step time: 0.1000\n",
      "161/223, train_loss: 0.1105, step time: 0.0999\n",
      "162/223, train_loss: 0.1204, step time: 0.0995\n",
      "163/223, train_loss: 0.1192, step time: 0.1007\n",
      "164/223, train_loss: 0.1075, step time: 0.1170\n",
      "165/223, train_loss: 0.1109, step time: 0.1004\n",
      "166/223, train_loss: 0.1057, step time: 0.1004\n",
      "167/223, train_loss: 0.1206, step time: 0.1005\n",
      "168/223, train_loss: 0.1133, step time: 0.1007\n",
      "169/223, train_loss: 0.1112, step time: 0.1261\n",
      "170/223, train_loss: 0.1128, step time: 0.1065\n",
      "171/223, train_loss: 0.1147, step time: 0.1035\n",
      "172/223, train_loss: 0.1240, step time: 0.1009\n",
      "173/223, train_loss: 0.1290, step time: 0.1101\n",
      "174/223, train_loss: 0.1212, step time: 0.1008\n",
      "175/223, train_loss: 0.1237, step time: 0.0998\n",
      "176/223, train_loss: 0.1274, step time: 0.0990\n",
      "177/223, train_loss: 0.1162, step time: 0.0984\n",
      "178/223, train_loss: 0.1193, step time: 0.1395\n",
      "179/223, train_loss: 0.1054, step time: 0.1102\n",
      "180/223, train_loss: 0.1199, step time: 0.1264\n",
      "181/223, train_loss: 0.1142, step time: 0.1084\n",
      "182/223, train_loss: 0.1109, step time: 0.1114\n",
      "183/223, train_loss: 0.1074, step time: 0.1251\n",
      "184/223, train_loss: 0.1220, step time: 0.1005\n",
      "185/223, train_loss: 0.1266, step time: 0.0996\n",
      "186/223, train_loss: 0.1032, step time: 0.1171\n",
      "187/223, train_loss: 0.1104, step time: 0.1011\n",
      "188/223, train_loss: 0.1180, step time: 0.1084\n",
      "189/223, train_loss: 0.1147, step time: 0.1123\n",
      "190/223, train_loss: 0.1072, step time: 0.1132\n",
      "191/223, train_loss: 0.1136, step time: 0.1217\n",
      "192/223, train_loss: 0.1166, step time: 0.1480\n",
      "193/223, train_loss: 0.1308, step time: 0.1137\n",
      "194/223, train_loss: 0.1089, step time: 0.1108\n",
      "195/223, train_loss: 0.1146, step time: 0.1055\n",
      "196/223, train_loss: 0.1195, step time: 0.0997\n",
      "197/223, train_loss: 0.1300, step time: 0.0994\n",
      "198/223, train_loss: 0.1333, step time: 0.1094\n",
      "199/223, train_loss: 0.1075, step time: 0.1167\n",
      "200/223, train_loss: 0.1121, step time: 0.1038\n",
      "201/223, train_loss: 0.1086, step time: 0.1107\n",
      "202/223, train_loss: 0.1277, step time: 0.1142\n",
      "203/223, train_loss: 0.1193, step time: 0.1018\n",
      "204/223, train_loss: 0.1117, step time: 0.1053\n",
      "205/223, train_loss: 0.1246, step time: 0.1254\n",
      "206/223, train_loss: 0.1257, step time: 0.1019\n",
      "207/223, train_loss: 0.1058, step time: 0.1012\n",
      "208/223, train_loss: 0.1164, step time: 0.1156\n",
      "209/223, train_loss: 0.1135, step time: 0.1024\n",
      "210/223, train_loss: 0.1209, step time: 0.1061\n",
      "211/223, train_loss: 0.1230, step time: 0.1024\n",
      "212/223, train_loss: 0.1175, step time: 0.0996\n",
      "213/223, train_loss: 0.1175, step time: 0.1042\n",
      "214/223, train_loss: 0.1230, step time: 0.1083\n",
      "215/223, train_loss: 0.1282, step time: 0.1075\n",
      "216/223, train_loss: 0.1208, step time: 0.1277\n",
      "217/223, train_loss: 0.1285, step time: 0.1144\n",
      "218/223, train_loss: 0.1279, step time: 0.0995\n",
      "219/223, train_loss: 0.1187, step time: 0.0994\n",
      "220/223, train_loss: 0.1193, step time: 0.1023\n",
      "221/223, train_loss: 0.1212, step time: 0.1085\n",
      "222/223, train_loss: 0.1199, step time: 0.1000\n",
      "223/223, train_loss: 0.1116, step time: 0.0999\n",
      "epoch 91 average loss: 0.1170\n",
      "time consuming of epoch 91 is: 87.2125\n",
      "----------\n",
      "epoch 92/300\n",
      "1/223, train_loss: 0.1079, step time: 0.1010\n",
      "2/223, train_loss: 0.1209, step time: 0.0998\n",
      "3/223, train_loss: 0.1258, step time: 0.1010\n",
      "4/223, train_loss: 0.1126, step time: 0.1143\n",
      "5/223, train_loss: 0.1152, step time: 0.1283\n",
      "6/223, train_loss: 0.1061, step time: 0.1251\n",
      "7/223, train_loss: 0.1160, step time: 0.1098\n",
      "8/223, train_loss: 0.1246, step time: 0.1000\n",
      "9/223, train_loss: 0.1189, step time: 0.1246\n",
      "10/223, train_loss: 0.1089, step time: 0.1076\n",
      "11/223, train_loss: 0.1211, step time: 0.1213\n",
      "12/223, train_loss: 0.1133, step time: 0.1043\n",
      "13/223, train_loss: 0.1088, step time: 0.1004\n",
      "14/223, train_loss: 0.1196, step time: 0.1006\n",
      "15/223, train_loss: 0.1093, step time: 0.1246\n",
      "16/223, train_loss: 0.1052, step time: 0.1258\n",
      "17/223, train_loss: 0.1081, step time: 0.1008\n",
      "18/223, train_loss: 0.1211, step time: 0.1093\n",
      "19/223, train_loss: 0.1182, step time: 0.1193\n",
      "20/223, train_loss: 0.1259, step time: 0.1171\n",
      "21/223, train_loss: 0.1124, step time: 0.1004\n",
      "22/223, train_loss: 0.1115, step time: 0.1053\n",
      "23/223, train_loss: 0.1071, step time: 0.1079\n",
      "24/223, train_loss: 0.1113, step time: 0.1055\n",
      "25/223, train_loss: 0.1005, step time: 0.1045\n",
      "26/223, train_loss: 0.1169, step time: 0.1007\n",
      "27/223, train_loss: 0.1194, step time: 0.1004\n",
      "28/223, train_loss: 0.1110, step time: 0.1006\n",
      "29/223, train_loss: 0.1041, step time: 0.1201\n",
      "30/223, train_loss: 0.1043, step time: 0.0993\n",
      "31/223, train_loss: 0.1089, step time: 0.0985\n",
      "32/223, train_loss: 0.1057, step time: 0.1022\n",
      "33/223, train_loss: 0.1157, step time: 0.1110\n",
      "34/223, train_loss: 0.1104, step time: 0.1339\n",
      "35/223, train_loss: 0.1177, step time: 0.1091\n",
      "36/223, train_loss: 0.1283, step time: 0.1004\n",
      "37/223, train_loss: 0.1089, step time: 0.1419\n",
      "38/223, train_loss: 0.1172, step time: 0.1537\n",
      "39/223, train_loss: 0.1203, step time: 0.1015\n",
      "40/223, train_loss: 0.1119, step time: 0.0998\n",
      "41/223, train_loss: 0.1267, step time: 0.1208\n",
      "42/223, train_loss: 0.1156, step time: 0.1066\n",
      "43/223, train_loss: 0.1079, step time: 0.1182\n",
      "44/223, train_loss: 0.1146, step time: 0.0997\n",
      "45/223, train_loss: 0.1117, step time: 0.0995\n",
      "46/223, train_loss: 0.1243, step time: 0.1007\n",
      "47/223, train_loss: 0.1142, step time: 0.1436\n",
      "48/223, train_loss: 0.1164, step time: 0.0990\n",
      "49/223, train_loss: 0.1144, step time: 0.1081\n",
      "50/223, train_loss: 0.1213, step time: 0.1059\n",
      "51/223, train_loss: 0.1193, step time: 0.1051\n",
      "52/223, train_loss: 0.1160, step time: 0.1008\n",
      "53/223, train_loss: 0.1084, step time: 0.1019\n",
      "54/223, train_loss: 0.1268, step time: 0.1136\n",
      "55/223, train_loss: 0.1086, step time: 0.0991\n",
      "56/223, train_loss: 0.1206, step time: 0.0997\n",
      "57/223, train_loss: 0.1109, step time: 0.1129\n",
      "58/223, train_loss: 0.1111, step time: 0.1496\n",
      "59/223, train_loss: 0.1074, step time: 0.1013\n",
      "60/223, train_loss: 0.1292, step time: 0.1003\n",
      "61/223, train_loss: 0.1120, step time: 0.1108\n",
      "62/223, train_loss: 0.1152, step time: 0.1004\n",
      "63/223, train_loss: 0.1203, step time: 0.0995\n",
      "64/223, train_loss: 0.1206, step time: 0.1072\n",
      "65/223, train_loss: 0.1109, step time: 0.1208\n",
      "66/223, train_loss: 0.1285, step time: 0.1008\n",
      "67/223, train_loss: 0.0980, step time: 0.1001\n",
      "68/223, train_loss: 0.0981, step time: 0.1074\n",
      "69/223, train_loss: 0.1158, step time: 0.1031\n",
      "70/223, train_loss: 0.1190, step time: 0.1055\n",
      "71/223, train_loss: 0.1252, step time: 0.1251\n",
      "72/223, train_loss: 0.1091, step time: 0.1051\n",
      "73/223, train_loss: 0.1218, step time: 0.1144\n",
      "74/223, train_loss: 0.1090, step time: 0.1026\n",
      "75/223, train_loss: 0.1248, step time: 0.1178\n",
      "76/223, train_loss: 0.1339, step time: 0.1005\n",
      "77/223, train_loss: 0.1152, step time: 0.1026\n",
      "78/223, train_loss: 0.1157, step time: 0.1006\n",
      "79/223, train_loss: 0.1192, step time: 0.1054\n",
      "80/223, train_loss: 0.1189, step time: 0.1178\n",
      "81/223, train_loss: 0.1275, step time: 0.1036\n",
      "82/223, train_loss: 0.1119, step time: 0.1031\n",
      "83/223, train_loss: 0.1287, step time: 0.1103\n",
      "84/223, train_loss: 0.0992, step time: 0.1025\n",
      "85/223, train_loss: 0.1197, step time: 0.1000\n",
      "86/223, train_loss: 0.1097, step time: 0.1005\n",
      "87/223, train_loss: 0.1168, step time: 0.1000\n",
      "88/223, train_loss: 0.1197, step time: 0.0999\n",
      "89/223, train_loss: 0.1156, step time: 0.1189\n",
      "90/223, train_loss: 0.1179, step time: 0.1006\n",
      "91/223, train_loss: 0.1088, step time: 0.1000\n",
      "92/223, train_loss: 0.1221, step time: 0.1223\n",
      "93/223, train_loss: 0.1124, step time: 0.1039\n",
      "94/223, train_loss: 0.1250, step time: 0.1001\n",
      "95/223, train_loss: 0.1293, step time: 0.1011\n",
      "96/223, train_loss: 0.1382, step time: 0.1104\n",
      "97/223, train_loss: 0.1150, step time: 0.1131\n",
      "98/223, train_loss: 0.1158, step time: 0.1011\n",
      "99/223, train_loss: 0.1323, step time: 0.1723\n",
      "100/223, train_loss: 0.1059, step time: 0.1133\n",
      "101/223, train_loss: 0.1219, step time: 0.1345\n",
      "102/223, train_loss: 0.1234, step time: 0.1279\n",
      "103/223, train_loss: 0.1150, step time: 0.1105\n",
      "104/223, train_loss: 0.1095, step time: 0.1056\n",
      "105/223, train_loss: 0.1301, step time: 0.1093\n",
      "106/223, train_loss: 0.1066, step time: 0.0995\n",
      "107/223, train_loss: 0.1159, step time: 0.1072\n",
      "108/223, train_loss: 0.1084, step time: 0.1173\n",
      "109/223, train_loss: 0.1300, step time: 0.1111\n",
      "110/223, train_loss: 0.1095, step time: 0.1217\n",
      "111/223, train_loss: 0.1042, step time: 0.1134\n",
      "112/223, train_loss: 0.1163, step time: 0.0993\n",
      "113/223, train_loss: 0.1181, step time: 0.1173\n",
      "114/223, train_loss: 0.1131, step time: 0.1006\n",
      "115/223, train_loss: 0.1110, step time: 0.1042\n",
      "116/223, train_loss: 0.1115, step time: 0.1007\n",
      "117/223, train_loss: 0.1247, step time: 0.1100\n",
      "118/223, train_loss: 0.1113, step time: 0.1059\n",
      "119/223, train_loss: 0.1247, step time: 0.0994\n",
      "120/223, train_loss: 0.1211, step time: 0.1005\n",
      "121/223, train_loss: 0.1162, step time: 0.1142\n",
      "122/223, train_loss: 0.1124, step time: 0.1072\n",
      "123/223, train_loss: 0.1150, step time: 0.1002\n",
      "124/223, train_loss: 0.1275, step time: 0.0996\n",
      "125/223, train_loss: 0.1247, step time: 0.1391\n",
      "126/223, train_loss: 0.1099, step time: 0.1012\n",
      "127/223, train_loss: 0.1167, step time: 0.1003\n",
      "128/223, train_loss: 0.1228, step time: 0.1017\n",
      "129/223, train_loss: 0.1271, step time: 0.1002\n",
      "130/223, train_loss: 0.3127, step time: 0.1017\n",
      "131/223, train_loss: 0.1141, step time: 0.1081\n",
      "132/223, train_loss: 0.1082, step time: 0.1056\n",
      "133/223, train_loss: 0.1178, step time: 0.1221\n",
      "134/223, train_loss: 0.1089, step time: 0.1287\n",
      "135/223, train_loss: 0.1200, step time: 0.1174\n",
      "136/223, train_loss: 0.1181, step time: 0.1162\n",
      "137/223, train_loss: 0.1095, step time: 0.1003\n",
      "138/223, train_loss: 0.1053, step time: 0.1078\n",
      "139/223, train_loss: 0.1098, step time: 0.1208\n",
      "140/223, train_loss: 0.1109, step time: 0.1000\n",
      "141/223, train_loss: 0.1150, step time: 0.1122\n",
      "142/223, train_loss: 0.1127, step time: 0.1001\n",
      "143/223, train_loss: 0.1077, step time: 0.1012\n",
      "144/223, train_loss: 0.1102, step time: 0.1052\n",
      "145/223, train_loss: 0.1143, step time: 0.1137\n",
      "146/223, train_loss: 0.1120, step time: 0.1188\n",
      "147/223, train_loss: 0.1201, step time: 0.1011\n",
      "148/223, train_loss: 0.1153, step time: 0.1078\n",
      "149/223, train_loss: 0.1109, step time: 0.1127\n",
      "150/223, train_loss: 0.1232, step time: 0.1189\n",
      "151/223, train_loss: 0.1073, step time: 0.1003\n",
      "152/223, train_loss: 0.1095, step time: 0.1126\n",
      "153/223, train_loss: 0.1239, step time: 0.1221\n",
      "154/223, train_loss: 0.1066, step time: 0.1035\n",
      "155/223, train_loss: 0.1185, step time: 0.1058\n",
      "156/223, train_loss: 0.1222, step time: 0.1126\n",
      "157/223, train_loss: 0.1135, step time: 0.1083\n",
      "158/223, train_loss: 0.1255, step time: 0.1337\n",
      "159/223, train_loss: 0.1235, step time: 0.1155\n",
      "160/223, train_loss: 0.1202, step time: 0.1020\n",
      "161/223, train_loss: 0.1237, step time: 0.1016\n",
      "162/223, train_loss: 0.1188, step time: 0.1006\n",
      "163/223, train_loss: 0.1251, step time: 0.1005\n",
      "164/223, train_loss: 0.1186, step time: 0.1260\n",
      "165/223, train_loss: 0.1104, step time: 0.1002\n",
      "166/223, train_loss: 0.1155, step time: 0.0999\n",
      "167/223, train_loss: 0.1070, step time: 0.1003\n",
      "168/223, train_loss: 0.1102, step time: 0.1040\n",
      "169/223, train_loss: 0.1103, step time: 0.1172\n",
      "170/223, train_loss: 0.1043, step time: 0.1174\n",
      "171/223, train_loss: 0.1127, step time: 0.1144\n",
      "172/223, train_loss: 0.1036, step time: 0.1054\n",
      "173/223, train_loss: 0.1093, step time: 0.1173\n",
      "174/223, train_loss: 0.1244, step time: 0.1006\n",
      "175/223, train_loss: 0.1091, step time: 0.1029\n",
      "176/223, train_loss: 0.1068, step time: 0.1007\n",
      "177/223, train_loss: 0.1071, step time: 0.1104\n",
      "178/223, train_loss: 0.1241, step time: 0.1196\n",
      "179/223, train_loss: 0.1239, step time: 0.1310\n",
      "180/223, train_loss: 0.1130, step time: 0.1000\n",
      "181/223, train_loss: 0.1046, step time: 0.1276\n",
      "182/223, train_loss: 0.1201, step time: 0.1067\n",
      "183/223, train_loss: 0.1206, step time: 0.1020\n",
      "184/223, train_loss: 0.1286, step time: 0.1129\n",
      "185/223, train_loss: 0.1219, step time: 0.1106\n",
      "186/223, train_loss: 0.1111, step time: 0.1159\n",
      "187/223, train_loss: 0.1199, step time: 0.1152\n",
      "188/223, train_loss: 0.1008, step time: 0.1100\n",
      "189/223, train_loss: 0.1239, step time: 0.1232\n",
      "190/223, train_loss: 0.1090, step time: 0.1188\n",
      "191/223, train_loss: 0.1149, step time: 0.1166\n",
      "192/223, train_loss: 0.1187, step time: 0.1003\n",
      "193/223, train_loss: 0.1122, step time: 0.1106\n",
      "194/223, train_loss: 0.1165, step time: 0.0990\n",
      "195/223, train_loss: 0.1077, step time: 0.1292\n",
      "196/223, train_loss: 0.1082, step time: 0.0996\n",
      "197/223, train_loss: 0.1210, step time: 0.1054\n",
      "198/223, train_loss: 0.1287, step time: 0.1196\n",
      "199/223, train_loss: 0.1205, step time: 0.1046\n",
      "200/223, train_loss: 0.1168, step time: 0.0995\n",
      "201/223, train_loss: 0.1092, step time: 0.0995\n",
      "202/223, train_loss: 0.1318, step time: 0.1071\n",
      "203/223, train_loss: 0.1171, step time: 0.1260\n",
      "204/223, train_loss: 0.1321, step time: 0.1075\n",
      "205/223, train_loss: 0.1126, step time: 0.1149\n",
      "206/223, train_loss: 0.1266, step time: 0.1136\n",
      "207/223, train_loss: 0.1108, step time: 0.1173\n",
      "208/223, train_loss: 0.1060, step time: 0.1084\n",
      "209/223, train_loss: 0.1175, step time: 0.1140\n",
      "210/223, train_loss: 0.1126, step time: 0.1015\n",
      "211/223, train_loss: 0.1142, step time: 0.1141\n",
      "212/223, train_loss: 0.1067, step time: 0.1091\n",
      "213/223, train_loss: 0.1251, step time: 0.1165\n",
      "214/223, train_loss: 0.1183, step time: 0.1002\n",
      "215/223, train_loss: 0.1105, step time: 0.1098\n",
      "216/223, train_loss: 0.1152, step time: 0.1043\n",
      "217/223, train_loss: 0.1045, step time: 0.1008\n",
      "218/223, train_loss: 0.1129, step time: 0.1014\n",
      "219/223, train_loss: 0.1152, step time: 0.1036\n",
      "220/223, train_loss: 0.1317, step time: 0.1108\n",
      "221/223, train_loss: 0.1177, step time: 0.0991\n",
      "222/223, train_loss: 0.1147, step time: 0.0996\n",
      "223/223, train_loss: 0.1307, step time: 0.0999\n",
      "epoch 92 average loss: 0.1167\n",
      "time consuming of epoch 92 is: 87.6724\n",
      "----------\n",
      "epoch 93/300\n",
      "1/223, train_loss: 0.1052, step time: 0.1020\n",
      "2/223, train_loss: 0.1198, step time: 0.0998\n",
      "3/223, train_loss: 0.1107, step time: 0.1318\n",
      "4/223, train_loss: 0.1171, step time: 0.1177\n",
      "5/223, train_loss: 0.1079, step time: 0.1212\n",
      "6/223, train_loss: 0.1114, step time: 0.1001\n",
      "7/223, train_loss: 0.1051, step time: 0.1005\n",
      "8/223, train_loss: 0.1213, step time: 0.1262\n",
      "9/223, train_loss: 0.1079, step time: 0.1006\n",
      "10/223, train_loss: 0.1059, step time: 0.1009\n",
      "11/223, train_loss: 0.1109, step time: 0.1006\n",
      "12/223, train_loss: 0.1016, step time: 0.1186\n",
      "13/223, train_loss: 0.1182, step time: 0.1266\n",
      "14/223, train_loss: 0.1118, step time: 0.1080\n",
      "15/223, train_loss: 0.1106, step time: 0.1367\n",
      "16/223, train_loss: 0.1069, step time: 0.1063\n",
      "17/223, train_loss: 0.1135, step time: 0.1208\n",
      "18/223, train_loss: 0.1064, step time: 0.1104\n",
      "19/223, train_loss: 0.1112, step time: 0.1141\n",
      "20/223, train_loss: 0.1085, step time: 0.1012\n",
      "21/223, train_loss: 0.1182, step time: 0.1156\n",
      "22/223, train_loss: 0.1150, step time: 0.1105\n",
      "23/223, train_loss: 0.1084, step time: 0.1007\n",
      "24/223, train_loss: 0.1139, step time: 0.1145\n",
      "25/223, train_loss: 0.1175, step time: 0.1468\n",
      "26/223, train_loss: 0.0974, step time: 0.1005\n",
      "27/223, train_loss: 0.1255, step time: 0.1005\n",
      "28/223, train_loss: 0.1122, step time: 0.1037\n",
      "29/223, train_loss: 0.1022, step time: 0.1523\n",
      "30/223, train_loss: 0.1132, step time: 0.1123\n",
      "31/223, train_loss: 0.1125, step time: 0.1003\n",
      "32/223, train_loss: 0.1083, step time: 0.1127\n",
      "33/223, train_loss: 0.1068, step time: 0.1200\n",
      "34/223, train_loss: 0.1211, step time: 0.1310\n",
      "35/223, train_loss: 0.1211, step time: 0.1438\n",
      "36/223, train_loss: 0.1328, step time: 0.1139\n",
      "37/223, train_loss: 0.1331, step time: 0.1215\n",
      "38/223, train_loss: 0.1205, step time: 0.1007\n",
      "39/223, train_loss: 0.1230, step time: 0.1017\n",
      "40/223, train_loss: 0.1175, step time: 0.1172\n",
      "41/223, train_loss: 0.1195, step time: 0.1089\n",
      "42/223, train_loss: 0.1075, step time: 0.0994\n",
      "43/223, train_loss: 0.1144, step time: 0.1003\n",
      "44/223, train_loss: 0.1113, step time: 0.1197\n",
      "45/223, train_loss: 0.1226, step time: 0.1007\n",
      "46/223, train_loss: 0.1164, step time: 0.1010\n",
      "47/223, train_loss: 0.1170, step time: 0.1010\n",
      "48/223, train_loss: 0.1049, step time: 0.1118\n",
      "49/223, train_loss: 0.1220, step time: 0.1106\n",
      "50/223, train_loss: 0.1122, step time: 0.1034\n",
      "51/223, train_loss: 0.1048, step time: 0.1066\n",
      "52/223, train_loss: 0.1175, step time: 0.1040\n",
      "53/223, train_loss: 0.1214, step time: 0.1129\n",
      "54/223, train_loss: 0.1313, step time: 0.1066\n",
      "55/223, train_loss: 0.1055, step time: 0.1000\n",
      "56/223, train_loss: 0.1153, step time: 0.1004\n",
      "57/223, train_loss: 0.1070, step time: 0.1030\n",
      "58/223, train_loss: 0.1186, step time: 0.1116\n",
      "59/223, train_loss: 0.1197, step time: 0.1177\n",
      "60/223, train_loss: 0.1146, step time: 0.1067\n",
      "61/223, train_loss: 0.1247, step time: 0.1059\n",
      "62/223, train_loss: 0.1285, step time: 0.1175\n",
      "63/223, train_loss: 0.1225, step time: 0.1220\n",
      "64/223, train_loss: 0.1110, step time: 0.1224\n",
      "65/223, train_loss: 0.1091, step time: 0.1062\n",
      "66/223, train_loss: 0.1112, step time: 0.1002\n",
      "67/223, train_loss: 0.1109, step time: 0.1322\n",
      "68/223, train_loss: 0.1105, step time: 0.1112\n",
      "69/223, train_loss: 0.1066, step time: 0.1150\n",
      "70/223, train_loss: 0.1072, step time: 0.1096\n",
      "71/223, train_loss: 0.1117, step time: 0.1011\n",
      "72/223, train_loss: 0.1138, step time: 0.0997\n",
      "73/223, train_loss: 0.1310, step time: 0.1190\n",
      "74/223, train_loss: 0.1132, step time: 0.1125\n",
      "75/223, train_loss: 0.1016, step time: 0.1446\n",
      "76/223, train_loss: 0.1090, step time: 0.1135\n",
      "77/223, train_loss: 0.1260, step time: 0.1093\n",
      "78/223, train_loss: 0.1136, step time: 0.1103\n",
      "79/223, train_loss: 0.1199, step time: 0.1274\n",
      "80/223, train_loss: 0.1196, step time: 0.1046\n",
      "81/223, train_loss: 0.1197, step time: 0.1428\n",
      "82/223, train_loss: 0.1124, step time: 0.1250\n",
      "83/223, train_loss: 0.1213, step time: 0.1121\n",
      "84/223, train_loss: 0.1283, step time: 0.1227\n",
      "85/223, train_loss: 0.1167, step time: 0.1114\n",
      "86/223, train_loss: 0.1149, step time: 0.1093\n",
      "87/223, train_loss: 0.1058, step time: 0.1005\n",
      "88/223, train_loss: 0.1036, step time: 0.1121\n",
      "89/223, train_loss: 0.1054, step time: 0.1107\n",
      "90/223, train_loss: 0.1051, step time: 0.1289\n",
      "91/223, train_loss: 0.1133, step time: 0.1147\n",
      "92/223, train_loss: 0.1043, step time: 0.1040\n",
      "93/223, train_loss: 0.1136, step time: 0.1255\n",
      "94/223, train_loss: 0.1139, step time: 0.1017\n",
      "95/223, train_loss: 0.1186, step time: 0.1556\n",
      "96/223, train_loss: 0.1267, step time: 0.1070\n",
      "97/223, train_loss: 0.1188, step time: 0.1075\n",
      "98/223, train_loss: 0.1162, step time: 0.1074\n",
      "99/223, train_loss: 0.1277, step time: 0.1140\n",
      "100/223, train_loss: 0.1058, step time: 0.1077\n",
      "101/223, train_loss: 0.1263, step time: 0.1106\n",
      "102/223, train_loss: 0.1122, step time: 0.1010\n",
      "103/223, train_loss: 0.1146, step time: 0.1027\n",
      "104/223, train_loss: 0.1245, step time: 0.1003\n",
      "105/223, train_loss: 0.1062, step time: 0.1028\n",
      "106/223, train_loss: 0.1100, step time: 0.1054\n",
      "107/223, train_loss: 0.1032, step time: 0.1014\n",
      "108/223, train_loss: 0.1147, step time: 0.1010\n",
      "109/223, train_loss: 0.1063, step time: 0.1148\n",
      "110/223, train_loss: 0.1211, step time: 0.1150\n",
      "111/223, train_loss: 0.1088, step time: 0.1230\n",
      "112/223, train_loss: 0.1187, step time: 0.1010\n",
      "113/223, train_loss: 0.1175, step time: 0.1234\n",
      "114/223, train_loss: 0.1249, step time: 0.1099\n",
      "115/223, train_loss: 0.1012, step time: 0.1023\n",
      "116/223, train_loss: 0.1034, step time: 0.1121\n",
      "117/223, train_loss: 0.1102, step time: 0.0995\n",
      "118/223, train_loss: 0.1343, step time: 0.0998\n",
      "119/223, train_loss: 0.1092, step time: 0.1060\n",
      "120/223, train_loss: 0.1037, step time: 0.1255\n",
      "121/223, train_loss: 0.1253, step time: 0.1144\n",
      "122/223, train_loss: 0.1136, step time: 0.1145\n",
      "123/223, train_loss: 0.1071, step time: 0.1056\n",
      "124/223, train_loss: 0.1117, step time: 0.1017\n",
      "125/223, train_loss: 0.1261, step time: 0.1102\n",
      "126/223, train_loss: 0.1069, step time: 0.0998\n",
      "127/223, train_loss: 0.1185, step time: 0.1013\n",
      "128/223, train_loss: 0.1197, step time: 0.1048\n",
      "129/223, train_loss: 0.1061, step time: 0.1069\n",
      "130/223, train_loss: 0.1207, step time: 0.1007\n",
      "131/223, train_loss: 0.1158, step time: 0.1376\n",
      "132/223, train_loss: 0.1258, step time: 0.1151\n",
      "133/223, train_loss: 0.1267, step time: 0.1028\n",
      "134/223, train_loss: 0.1119, step time: 0.1048\n",
      "135/223, train_loss: 0.1152, step time: 0.1001\n",
      "136/223, train_loss: 0.1167, step time: 0.1254\n",
      "137/223, train_loss: 0.1091, step time: 0.0995\n",
      "138/223, train_loss: 0.1341, step time: 0.1280\n",
      "139/223, train_loss: 0.1215, step time: 0.1433\n",
      "140/223, train_loss: 0.1242, step time: 0.1022\n",
      "141/223, train_loss: 0.1134, step time: 0.1006\n",
      "142/223, train_loss: 0.1203, step time: 0.1084\n",
      "143/223, train_loss: 0.1074, step time: 0.0989\n",
      "144/223, train_loss: 0.1316, step time: 0.1240\n",
      "145/223, train_loss: 0.1057, step time: 0.1150\n",
      "146/223, train_loss: 0.1214, step time: 0.1112\n",
      "147/223, train_loss: 0.1161, step time: 0.1008\n",
      "148/223, train_loss: 0.1155, step time: 0.1021\n",
      "149/223, train_loss: 0.1335, step time: 0.0999\n",
      "150/223, train_loss: 0.1096, step time: 0.1024\n",
      "151/223, train_loss: 0.1069, step time: 0.1006\n",
      "152/223, train_loss: 0.1164, step time: 0.1022\n",
      "153/223, train_loss: 0.1249, step time: 0.1184\n",
      "154/223, train_loss: 0.1240, step time: 0.1024\n",
      "155/223, train_loss: 0.1158, step time: 0.1076\n",
      "156/223, train_loss: 0.1174, step time: 0.1202\n",
      "157/223, train_loss: 0.1279, step time: 0.1004\n",
      "158/223, train_loss: 0.1207, step time: 0.1089\n",
      "159/223, train_loss: 0.1114, step time: 0.1039\n",
      "160/223, train_loss: 0.1077, step time: 0.1257\n",
      "161/223, train_loss: 0.1099, step time: 0.0999\n",
      "162/223, train_loss: 0.1112, step time: 0.0994\n",
      "163/223, train_loss: 0.1129, step time: 0.0997\n",
      "164/223, train_loss: 0.1097, step time: 0.1041\n",
      "165/223, train_loss: 0.1104, step time: 0.1086\n",
      "166/223, train_loss: 0.1132, step time: 0.1024\n",
      "167/223, train_loss: 0.1140, step time: 0.1268\n",
      "168/223, train_loss: 0.1110, step time: 0.1052\n",
      "169/223, train_loss: 0.1177, step time: 0.1003\n",
      "170/223, train_loss: 0.1266, step time: 0.1007\n",
      "171/223, train_loss: 0.1068, step time: 0.1072\n",
      "172/223, train_loss: 0.1237, step time: 0.1005\n",
      "173/223, train_loss: 0.1147, step time: 0.1132\n",
      "174/223, train_loss: 0.1237, step time: 0.1138\n",
      "175/223, train_loss: 0.1094, step time: 0.1125\n",
      "176/223, train_loss: 0.1115, step time: 0.1146\n",
      "177/223, train_loss: 0.1271, step time: 0.1113\n",
      "178/223, train_loss: 0.1201, step time: 0.1260\n",
      "179/223, train_loss: 0.1259, step time: 0.1131\n",
      "180/223, train_loss: 0.1092, step time: 0.1096\n",
      "181/223, train_loss: 0.1163, step time: 0.1176\n",
      "182/223, train_loss: 0.1206, step time: 0.1133\n",
      "183/223, train_loss: 0.1144, step time: 0.1068\n",
      "184/223, train_loss: 0.1163, step time: 0.1355\n",
      "185/223, train_loss: 0.1157, step time: 0.1057\n",
      "186/223, train_loss: 0.1101, step time: 0.1004\n",
      "187/223, train_loss: 0.1110, step time: 0.1015\n",
      "188/223, train_loss: 0.1261, step time: 0.1001\n",
      "189/223, train_loss: 0.1017, step time: 0.1071\n",
      "190/223, train_loss: 0.1175, step time: 0.1215\n",
      "191/223, train_loss: 0.1151, step time: 0.1004\n",
      "192/223, train_loss: 0.1032, step time: 0.1035\n",
      "193/223, train_loss: 0.1138, step time: 0.1270\n",
      "194/223, train_loss: 0.3114, step time: 0.1067\n",
      "195/223, train_loss: 0.1033, step time: 0.1007\n",
      "196/223, train_loss: 0.1043, step time: 0.1011\n",
      "197/223, train_loss: 0.1259, step time: 0.1010\n",
      "198/223, train_loss: 0.1177, step time: 0.1041\n",
      "199/223, train_loss: 0.1078, step time: 0.0996\n",
      "200/223, train_loss: 0.1186, step time: 0.1133\n",
      "201/223, train_loss: 0.1102, step time: 0.1288\n",
      "202/223, train_loss: 0.1193, step time: 0.1115\n",
      "203/223, train_loss: 0.1186, step time: 0.1001\n",
      "204/223, train_loss: 0.1248, step time: 0.1001\n",
      "205/223, train_loss: 0.1381, step time: 0.1099\n",
      "206/223, train_loss: 0.1067, step time: 0.1008\n",
      "207/223, train_loss: 0.1297, step time: 0.1010\n",
      "208/223, train_loss: 0.1183, step time: 0.1038\n",
      "209/223, train_loss: 0.1168, step time: 0.0998\n",
      "210/223, train_loss: 0.1248, step time: 0.1032\n",
      "211/223, train_loss: 0.1135, step time: 0.1008\n",
      "212/223, train_loss: 0.1108, step time: 0.1010\n",
      "213/223, train_loss: 0.1058, step time: 0.1244\n",
      "214/223, train_loss: 0.1284, step time: 0.1060\n",
      "215/223, train_loss: 0.1212, step time: 0.1045\n",
      "216/223, train_loss: 0.1193, step time: 0.1004\n",
      "217/223, train_loss: 0.1088, step time: 0.1049\n",
      "218/223, train_loss: 0.1139, step time: 0.1003\n",
      "219/223, train_loss: 0.1116, step time: 0.1017\n",
      "220/223, train_loss: 0.1244, step time: 0.1007\n",
      "221/223, train_loss: 0.1116, step time: 0.0984\n",
      "222/223, train_loss: 0.1099, step time: 0.0991\n",
      "223/223, train_loss: 0.1197, step time: 0.0999\n",
      "epoch 93 average loss: 0.1161\n",
      "time consuming of epoch 93 is: 87.6673\n",
      "----------\n",
      "epoch 94/300\n",
      "1/223, train_loss: 0.1119, step time: 0.1065\n",
      "2/223, train_loss: 0.1124, step time: 0.1019\n",
      "3/223, train_loss: 0.1150, step time: 0.1352\n",
      "4/223, train_loss: 0.1194, step time: 0.1142\n",
      "5/223, train_loss: 0.1165, step time: 0.1082\n",
      "6/223, train_loss: 0.1235, step time: 0.1004\n",
      "7/223, train_loss: 0.1178, step time: 0.1113\n",
      "8/223, train_loss: 0.1113, step time: 0.1128\n",
      "9/223, train_loss: 0.1124, step time: 0.1015\n",
      "10/223, train_loss: 0.1213, step time: 0.1040\n",
      "11/223, train_loss: 0.1174, step time: 0.1056\n",
      "12/223, train_loss: 0.1129, step time: 0.1004\n",
      "13/223, train_loss: 0.1227, step time: 0.1107\n",
      "14/223, train_loss: 0.1156, step time: 0.1168\n",
      "15/223, train_loss: 0.1133, step time: 0.0997\n",
      "16/223, train_loss: 0.1136, step time: 0.0999\n",
      "17/223, train_loss: 0.1080, step time: 0.1135\n",
      "18/223, train_loss: 0.1050, step time: 0.1010\n",
      "19/223, train_loss: 0.1234, step time: 0.1037\n",
      "20/223, train_loss: 0.1064, step time: 0.1059\n",
      "21/223, train_loss: 0.1343, step time: 0.1018\n",
      "22/223, train_loss: 0.1140, step time: 0.1846\n",
      "23/223, train_loss: 0.1222, step time: 0.1005\n",
      "24/223, train_loss: 0.1034, step time: 0.1075\n",
      "25/223, train_loss: 0.1230, step time: 0.1010\n",
      "26/223, train_loss: 0.1246, step time: 0.1008\n",
      "27/223, train_loss: 0.1291, step time: 0.1011\n",
      "28/223, train_loss: 0.1199, step time: 0.1004\n",
      "29/223, train_loss: 0.1117, step time: 0.1055\n",
      "30/223, train_loss: 0.1173, step time: 0.1002\n",
      "31/223, train_loss: 0.1065, step time: 0.1001\n",
      "32/223, train_loss: 0.1072, step time: 0.1083\n",
      "33/223, train_loss: 0.1061, step time: 0.1130\n",
      "34/223, train_loss: 0.1219, step time: 0.1003\n",
      "35/223, train_loss: 0.1238, step time: 0.1063\n",
      "36/223, train_loss: 0.1071, step time: 0.0996\n",
      "37/223, train_loss: 0.1199, step time: 0.1210\n",
      "38/223, train_loss: 0.1089, step time: 0.1099\n",
      "39/223, train_loss: 0.1099, step time: 0.1005\n",
      "40/223, train_loss: 0.1089, step time: 0.1106\n",
      "41/223, train_loss: 0.1113, step time: 0.0999\n",
      "42/223, train_loss: 0.1225, step time: 0.0997\n",
      "43/223, train_loss: 0.1126, step time: 0.0998\n",
      "44/223, train_loss: 0.1183, step time: 0.0999\n",
      "45/223, train_loss: 0.1158, step time: 0.1017\n",
      "46/223, train_loss: 0.1140, step time: 0.0999\n",
      "47/223, train_loss: 0.1275, step time: 0.1005\n",
      "48/223, train_loss: 0.1092, step time: 0.1010\n",
      "49/223, train_loss: 0.1119, step time: 0.1116\n",
      "50/223, train_loss: 0.1199, step time: 0.1004\n",
      "51/223, train_loss: 0.1234, step time: 0.1030\n",
      "52/223, train_loss: 0.1201, step time: 0.1003\n",
      "53/223, train_loss: 0.1156, step time: 0.1072\n",
      "54/223, train_loss: 0.1104, step time: 0.1102\n",
      "55/223, train_loss: 0.1115, step time: 0.1054\n",
      "56/223, train_loss: 0.1094, step time: 0.1040\n",
      "57/223, train_loss: 0.1066, step time: 0.1198\n",
      "58/223, train_loss: 0.1019, step time: 0.1107\n",
      "59/223, train_loss: 0.1281, step time: 0.1041\n",
      "60/223, train_loss: 0.1090, step time: 0.1009\n",
      "61/223, train_loss: 0.1150, step time: 0.0997\n",
      "62/223, train_loss: 0.1128, step time: 0.1008\n",
      "63/223, train_loss: 0.1198, step time: 0.1600\n",
      "64/223, train_loss: 0.1162, step time: 0.1012\n",
      "65/223, train_loss: 0.1263, step time: 0.0999\n",
      "66/223, train_loss: 0.1166, step time: 0.1004\n",
      "67/223, train_loss: 0.1076, step time: 0.1002\n",
      "68/223, train_loss: 0.1122, step time: 0.1052\n",
      "69/223, train_loss: 0.1212, step time: 0.1007\n",
      "70/223, train_loss: 0.1266, step time: 0.1628\n",
      "71/223, train_loss: 0.1212, step time: 0.1407\n",
      "72/223, train_loss: 0.1076, step time: 0.1123\n",
      "73/223, train_loss: 0.1067, step time: 0.1210\n",
      "74/223, train_loss: 0.1136, step time: 0.1217\n",
      "75/223, train_loss: 0.1113, step time: 0.1214\n",
      "76/223, train_loss: 0.1156, step time: 0.1137\n",
      "77/223, train_loss: 0.1206, step time: 0.0998\n",
      "78/223, train_loss: 0.1019, step time: 0.1010\n",
      "79/223, train_loss: 0.1074, step time: 0.1109\n",
      "80/223, train_loss: 0.1118, step time: 0.1116\n",
      "81/223, train_loss: 0.1130, step time: 0.1174\n",
      "82/223, train_loss: 0.1183, step time: 0.1011\n",
      "83/223, train_loss: 0.1022, step time: 0.1107\n",
      "84/223, train_loss: 0.1114, step time: 0.1006\n",
      "85/223, train_loss: 0.1329, step time: 0.1006\n",
      "86/223, train_loss: 0.1046, step time: 0.0999\n",
      "87/223, train_loss: 0.1154, step time: 0.1003\n",
      "88/223, train_loss: 0.1120, step time: 0.1072\n",
      "89/223, train_loss: 0.1107, step time: 0.0999\n",
      "90/223, train_loss: 0.1107, step time: 0.0994\n",
      "91/223, train_loss: 0.1262, step time: 0.1243\n",
      "92/223, train_loss: 0.1022, step time: 0.1000\n",
      "93/223, train_loss: 0.1219, step time: 0.1073\n",
      "94/223, train_loss: 0.1050, step time: 0.1035\n",
      "95/223, train_loss: 0.1112, step time: 0.1293\n",
      "96/223, train_loss: 0.1257, step time: 0.1004\n",
      "97/223, train_loss: 0.1082, step time: 0.1088\n",
      "98/223, train_loss: 0.1435, step time: 0.1014\n",
      "99/223, train_loss: 0.1061, step time: 0.1005\n",
      "100/223, train_loss: 0.1269, step time: 0.1030\n",
      "101/223, train_loss: 0.1147, step time: 0.1026\n",
      "102/223, train_loss: 0.1121, step time: 0.1010\n",
      "103/223, train_loss: 0.1138, step time: 0.1012\n",
      "104/223, train_loss: 0.1290, step time: 0.1003\n",
      "105/223, train_loss: 0.1275, step time: 0.1279\n",
      "106/223, train_loss: 0.1127, step time: 0.1234\n",
      "107/223, train_loss: 0.1325, step time: 0.1225\n",
      "108/223, train_loss: 0.1183, step time: 0.1097\n",
      "109/223, train_loss: 0.1051, step time: 0.1096\n",
      "110/223, train_loss: 0.1132, step time: 0.1208\n",
      "111/223, train_loss: 0.1132, step time: 0.1139\n",
      "112/223, train_loss: 0.1150, step time: 0.1000\n",
      "113/223, train_loss: 0.1153, step time: 0.1184\n",
      "114/223, train_loss: 0.1145, step time: 0.1022\n",
      "115/223, train_loss: 0.1148, step time: 0.1080\n",
      "116/223, train_loss: 0.1175, step time: 0.1007\n",
      "117/223, train_loss: 0.1049, step time: 0.1135\n",
      "118/223, train_loss: 0.1180, step time: 0.1077\n",
      "119/223, train_loss: 0.1142, step time: 0.1265\n",
      "120/223, train_loss: 0.1091, step time: 0.1117\n",
      "121/223, train_loss: 0.1034, step time: 0.1251\n",
      "122/223, train_loss: 0.1083, step time: 0.1006\n",
      "123/223, train_loss: 0.1131, step time: 0.1200\n",
      "124/223, train_loss: 0.1290, step time: 0.1011\n",
      "125/223, train_loss: 0.1125, step time: 0.1439\n",
      "126/223, train_loss: 0.1309, step time: 0.1003\n",
      "127/223, train_loss: 0.1114, step time: 0.1092\n",
      "128/223, train_loss: 0.1198, step time: 0.1002\n",
      "129/223, train_loss: 0.1214, step time: 0.1084\n",
      "130/223, train_loss: 0.1083, step time: 0.1452\n",
      "131/223, train_loss: 0.1109, step time: 0.1170\n",
      "132/223, train_loss: 0.1156, step time: 0.1011\n",
      "133/223, train_loss: 0.1174, step time: 0.1017\n",
      "134/223, train_loss: 0.1125, step time: 0.1004\n",
      "135/223, train_loss: 0.1246, step time: 0.1006\n",
      "136/223, train_loss: 0.1098, step time: 0.1071\n",
      "137/223, train_loss: 0.1093, step time: 0.1109\n",
      "138/223, train_loss: 0.1061, step time: 0.1196\n",
      "139/223, train_loss: 0.1172, step time: 0.1013\n",
      "140/223, train_loss: 0.1180, step time: 0.1080\n",
      "141/223, train_loss: 0.1160, step time: 0.1002\n",
      "142/223, train_loss: 0.1135, step time: 0.0995\n",
      "143/223, train_loss: 0.1111, step time: 0.1005\n",
      "144/223, train_loss: 0.0959, step time: 0.1005\n",
      "145/223, train_loss: 0.1110, step time: 0.1009\n",
      "146/223, train_loss: 0.1241, step time: 0.1063\n",
      "147/223, train_loss: 0.1071, step time: 0.1002\n",
      "148/223, train_loss: 0.1135, step time: 0.1005\n",
      "149/223, train_loss: 0.1062, step time: 0.0999\n",
      "150/223, train_loss: 0.1089, step time: 0.0998\n",
      "151/223, train_loss: 0.1224, step time: 0.1005\n",
      "152/223, train_loss: 0.1014, step time: 0.1252\n",
      "153/223, train_loss: 0.1172, step time: 0.1036\n",
      "154/223, train_loss: 0.1069, step time: 0.1026\n",
      "155/223, train_loss: 0.1153, step time: 0.1019\n",
      "156/223, train_loss: 0.1131, step time: 0.1029\n",
      "157/223, train_loss: 0.1088, step time: 0.1002\n",
      "158/223, train_loss: 0.1220, step time: 0.1005\n",
      "159/223, train_loss: 0.3270, step time: 0.1003\n",
      "160/223, train_loss: 0.1111, step time: 0.1089\n",
      "161/223, train_loss: 0.1232, step time: 0.1001\n",
      "162/223, train_loss: 0.1253, step time: 0.1013\n",
      "163/223, train_loss: 0.1084, step time: 0.1184\n",
      "164/223, train_loss: 0.1138, step time: 0.1011\n",
      "165/223, train_loss: 0.1109, step time: 0.1264\n",
      "166/223, train_loss: 0.1086, step time: 0.1330\n",
      "167/223, train_loss: 0.1140, step time: 0.1025\n",
      "168/223, train_loss: 0.1139, step time: 0.1045\n",
      "169/223, train_loss: 0.1296, step time: 0.1098\n",
      "170/223, train_loss: 0.1279, step time: 0.1011\n",
      "171/223, train_loss: 0.1132, step time: 0.1023\n",
      "172/223, train_loss: 0.1225, step time: 0.1115\n",
      "173/223, train_loss: 0.1101, step time: 0.0995\n",
      "174/223, train_loss: 0.1250, step time: 0.1054\n",
      "175/223, train_loss: 0.1097, step time: 0.1070\n",
      "176/223, train_loss: 0.1040, step time: 0.1088\n",
      "177/223, train_loss: 0.1188, step time: 0.1264\n",
      "178/223, train_loss: 0.1075, step time: 0.1015\n",
      "179/223, train_loss: 0.1149, step time: 0.1001\n",
      "180/223, train_loss: 0.1125, step time: 0.1065\n",
      "181/223, train_loss: 0.1128, step time: 0.1259\n",
      "182/223, train_loss: 0.1101, step time: 0.1022\n",
      "183/223, train_loss: 0.1065, step time: 0.1357\n",
      "184/223, train_loss: 0.1015, step time: 0.1105\n",
      "185/223, train_loss: 0.1046, step time: 0.1089\n",
      "186/223, train_loss: 0.1186, step time: 0.1076\n",
      "187/223, train_loss: 0.1275, step time: 0.1064\n",
      "188/223, train_loss: 0.1121, step time: 0.1134\n",
      "189/223, train_loss: 0.1192, step time: 0.1104\n",
      "190/223, train_loss: 0.1010, step time: 0.1245\n",
      "191/223, train_loss: 0.1174, step time: 0.1178\n",
      "192/223, train_loss: 0.1074, step time: 0.1102\n",
      "193/223, train_loss: 0.1123, step time: 0.1053\n",
      "194/223, train_loss: 0.1186, step time: 0.1014\n",
      "195/223, train_loss: 0.1217, step time: 0.1050\n",
      "196/223, train_loss: 0.1295, step time: 0.1045\n",
      "197/223, train_loss: 0.1155, step time: 0.1168\n",
      "198/223, train_loss: 0.1067, step time: 0.1229\n",
      "199/223, train_loss: 0.1076, step time: 0.1140\n",
      "200/223, train_loss: 0.1144, step time: 0.1138\n",
      "201/223, train_loss: 0.1211, step time: 0.1147\n",
      "202/223, train_loss: 0.1270, step time: 0.1166\n",
      "203/223, train_loss: 0.1158, step time: 0.1370\n",
      "204/223, train_loss: 0.1204, step time: 0.1355\n",
      "205/223, train_loss: 0.1248, step time: 0.0995\n",
      "206/223, train_loss: 0.1159, step time: 0.0996\n",
      "207/223, train_loss: 0.1186, step time: 0.0996\n",
      "208/223, train_loss: 0.1121, step time: 0.1112\n",
      "209/223, train_loss: 0.1092, step time: 0.1000\n",
      "210/223, train_loss: 0.1235, step time: 0.0994\n",
      "211/223, train_loss: 0.1105, step time: 0.1005\n",
      "212/223, train_loss: 0.1230, step time: 0.1065\n",
      "213/223, train_loss: 0.1310, step time: 0.0999\n",
      "214/223, train_loss: 0.1176, step time: 0.1115\n",
      "215/223, train_loss: 0.1111, step time: 0.1082\n",
      "216/223, train_loss: 0.1115, step time: 0.1145\n",
      "217/223, train_loss: 0.1214, step time: 0.1018\n",
      "218/223, train_loss: 0.1305, step time: 0.1000\n",
      "219/223, train_loss: 0.1251, step time: 0.1034\n",
      "220/223, train_loss: 0.1339, step time: 0.1000\n",
      "221/223, train_loss: 0.1237, step time: 0.1002\n",
      "222/223, train_loss: 0.1091, step time: 0.0998\n",
      "223/223, train_loss: 0.1171, step time: 0.0994\n",
      "epoch 94 average loss: 0.1162\n",
      "time consuming of epoch 94 is: 91.8335\n",
      "----------\n",
      "epoch 95/300\n",
      "1/223, train_loss: 0.1176, step time: 0.1012\n",
      "2/223, train_loss: 0.1150, step time: 0.1002\n",
      "3/223, train_loss: 0.1076, step time: 0.1008\n",
      "4/223, train_loss: 0.1052, step time: 0.1013\n",
      "5/223, train_loss: 0.1131, step time: 0.1062\n",
      "6/223, train_loss: 0.1084, step time: 0.0997\n",
      "7/223, train_loss: 0.1137, step time: 0.1011\n",
      "8/223, train_loss: 0.1247, step time: 0.1166\n",
      "9/223, train_loss: 0.1204, step time: 0.1183\n",
      "10/223, train_loss: 0.1129, step time: 0.1119\n",
      "11/223, train_loss: 0.1048, step time: 0.1053\n",
      "12/223, train_loss: 0.1125, step time: 0.1003\n",
      "13/223, train_loss: 0.1213, step time: 0.1071\n",
      "14/223, train_loss: 0.1151, step time: 0.1347\n",
      "15/223, train_loss: 0.1235, step time: 0.1224\n",
      "16/223, train_loss: 0.1070, step time: 0.1282\n",
      "17/223, train_loss: 0.1141, step time: 0.1136\n",
      "18/223, train_loss: 0.1063, step time: 0.0997\n",
      "19/223, train_loss: 0.1123, step time: 0.1005\n",
      "20/223, train_loss: 0.1216, step time: 0.1010\n",
      "21/223, train_loss: 0.1215, step time: 0.1042\n",
      "22/223, train_loss: 0.1174, step time: 0.0998\n",
      "23/223, train_loss: 0.1205, step time: 0.1167\n",
      "24/223, train_loss: 0.1116, step time: 0.1006\n",
      "25/223, train_loss: 0.1116, step time: 0.1208\n",
      "26/223, train_loss: 0.1043, step time: 0.1005\n",
      "27/223, train_loss: 0.1090, step time: 0.1015\n",
      "28/223, train_loss: 0.1304, step time: 0.1000\n",
      "29/223, train_loss: 0.1192, step time: 0.1141\n",
      "30/223, train_loss: 0.1158, step time: 0.1063\n",
      "31/223, train_loss: 0.1114, step time: 0.1070\n",
      "32/223, train_loss: 0.1024, step time: 0.1038\n",
      "33/223, train_loss: 0.1165, step time: 0.1091\n",
      "34/223, train_loss: 0.1208, step time: 0.1560\n",
      "35/223, train_loss: 0.1077, step time: 0.1078\n",
      "36/223, train_loss: 0.1063, step time: 0.1103\n",
      "37/223, train_loss: 0.1094, step time: 0.1076\n",
      "38/223, train_loss: 0.1110, step time: 0.1001\n",
      "39/223, train_loss: 0.1265, step time: 0.1255\n",
      "40/223, train_loss: 0.1173, step time: 0.1094\n",
      "41/223, train_loss: 0.1057, step time: 0.1145\n",
      "42/223, train_loss: 0.1080, step time: 0.1020\n",
      "43/223, train_loss: 0.1023, step time: 0.1009\n",
      "44/223, train_loss: 0.1101, step time: 0.1008\n",
      "45/223, train_loss: 0.1162, step time: 0.1325\n",
      "46/223, train_loss: 0.1312, step time: 0.1059\n",
      "47/223, train_loss: 0.1091, step time: 0.1010\n",
      "48/223, train_loss: 0.1087, step time: 0.1228\n",
      "49/223, train_loss: 0.1140, step time: 0.1006\n",
      "50/223, train_loss: 0.1229, step time: 0.1003\n",
      "51/223, train_loss: 0.1150, step time: 0.0996\n",
      "52/223, train_loss: 0.1111, step time: 0.1095\n",
      "53/223, train_loss: 0.1074, step time: 0.1236\n",
      "54/223, train_loss: 0.1164, step time: 0.1131\n",
      "55/223, train_loss: 0.1228, step time: 0.1215\n",
      "56/223, train_loss: 0.1182, step time: 0.0998\n",
      "57/223, train_loss: 0.1000, step time: 0.1230\n",
      "58/223, train_loss: 0.1253, step time: 0.1326\n",
      "59/223, train_loss: 0.1019, step time: 0.1004\n",
      "60/223, train_loss: 0.1132, step time: 0.1014\n",
      "61/223, train_loss: 0.1118, step time: 0.0996\n",
      "62/223, train_loss: 0.1136, step time: 0.1008\n",
      "63/223, train_loss: 0.1232, step time: 0.1043\n",
      "64/223, train_loss: 0.1227, step time: 0.1037\n",
      "65/223, train_loss: 0.1134, step time: 0.0997\n",
      "66/223, train_loss: 0.1104, step time: 0.1005\n",
      "67/223, train_loss: 0.1204, step time: 0.1121\n",
      "68/223, train_loss: 0.1118, step time: 0.0999\n",
      "69/223, train_loss: 0.1157, step time: 0.1150\n",
      "70/223, train_loss: 0.1305, step time: 0.1102\n",
      "71/223, train_loss: 0.1306, step time: 0.1031\n",
      "72/223, train_loss: 0.1261, step time: 0.1008\n",
      "73/223, train_loss: 0.1095, step time: 0.1244\n",
      "74/223, train_loss: 0.1087, step time: 0.1012\n",
      "75/223, train_loss: 0.1131, step time: 0.1001\n",
      "76/223, train_loss: 0.1264, step time: 0.1074\n",
      "77/223, train_loss: 0.1176, step time: 0.1507\n",
      "78/223, train_loss: 0.1243, step time: 0.1003\n",
      "79/223, train_loss: 0.1171, step time: 0.1010\n",
      "80/223, train_loss: 0.1239, step time: 0.1007\n",
      "81/223, train_loss: 0.1223, step time: 0.1041\n",
      "82/223, train_loss: 0.1245, step time: 0.1052\n",
      "83/223, train_loss: 0.1498, step time: 0.1075\n",
      "84/223, train_loss: 0.1438, step time: 0.1061\n",
      "85/223, train_loss: 0.1122, step time: 0.1205\n",
      "86/223, train_loss: 0.1105, step time: 0.1033\n",
      "87/223, train_loss: 0.1187, step time: 0.1013\n",
      "88/223, train_loss: 0.1232, step time: 0.1063\n",
      "89/223, train_loss: 0.1163, step time: 0.1058\n",
      "90/223, train_loss: 0.0997, step time: 0.1129\n",
      "91/223, train_loss: 0.1235, step time: 0.1246\n",
      "92/223, train_loss: 0.1221, step time: 0.1264\n",
      "93/223, train_loss: 0.1134, step time: 0.1058\n",
      "94/223, train_loss: 0.1252, step time: 0.1162\n",
      "95/223, train_loss: 0.1231, step time: 0.1007\n",
      "96/223, train_loss: 0.1103, step time: 0.1010\n",
      "97/223, train_loss: 0.1223, step time: 0.1074\n",
      "98/223, train_loss: 0.1254, step time: 0.1118\n",
      "99/223, train_loss: 0.1098, step time: 0.1121\n",
      "100/223, train_loss: 0.1233, step time: 0.1006\n",
      "101/223, train_loss: 0.1208, step time: 0.1412\n",
      "102/223, train_loss: 0.1139, step time: 0.1135\n",
      "103/223, train_loss: 0.1270, step time: 0.0995\n",
      "104/223, train_loss: 0.1385, step time: 0.0997\n",
      "105/223, train_loss: 0.1070, step time: 0.1070\n",
      "106/223, train_loss: 0.1067, step time: 0.1169\n",
      "107/223, train_loss: 0.1129, step time: 0.0998\n",
      "108/223, train_loss: 0.1308, step time: 0.1007\n",
      "109/223, train_loss: 0.1238, step time: 0.1030\n",
      "110/223, train_loss: 0.1186, step time: 0.1060\n",
      "111/223, train_loss: 0.1155, step time: 0.1108\n",
      "112/223, train_loss: 0.1159, step time: 0.1005\n",
      "113/223, train_loss: 0.1231, step time: 0.1048\n",
      "114/223, train_loss: 0.1295, step time: 0.1217\n",
      "115/223, train_loss: 0.1116, step time: 0.1008\n",
      "116/223, train_loss: 0.1229, step time: 0.1007\n",
      "117/223, train_loss: 0.1199, step time: 0.1004\n",
      "118/223, train_loss: 0.1149, step time: 0.1142\n",
      "119/223, train_loss: 0.1221, step time: 0.1151\n",
      "120/223, train_loss: 0.1209, step time: 0.1004\n",
      "121/223, train_loss: 0.1101, step time: 0.1066\n",
      "122/223, train_loss: 0.1163, step time: 0.1002\n",
      "123/223, train_loss: 0.1281, step time: 0.1124\n",
      "124/223, train_loss: 0.1047, step time: 0.1026\n",
      "125/223, train_loss: 0.1130, step time: 0.0998\n",
      "126/223, train_loss: 0.1289, step time: 0.1005\n",
      "127/223, train_loss: 0.1205, step time: 0.1005\n",
      "128/223, train_loss: 0.1215, step time: 0.1011\n",
      "129/223, train_loss: 0.1179, step time: 0.1098\n",
      "130/223, train_loss: 0.1114, step time: 0.1000\n",
      "131/223, train_loss: 0.1028, step time: 0.1058\n",
      "132/223, train_loss: 0.1191, step time: 0.1005\n",
      "133/223, train_loss: 0.1075, step time: 0.1092\n",
      "134/223, train_loss: 0.1148, step time: 0.1029\n",
      "135/223, train_loss: 0.1092, step time: 0.1489\n",
      "136/223, train_loss: 0.1144, step time: 0.1005\n",
      "137/223, train_loss: 0.1188, step time: 0.0995\n",
      "138/223, train_loss: 0.1237, step time: 0.1007\n",
      "139/223, train_loss: 0.1232, step time: 0.1207\n",
      "140/223, train_loss: 0.1078, step time: 0.1081\n",
      "141/223, train_loss: 0.1148, step time: 0.1029\n",
      "142/223, train_loss: 0.1006, step time: 0.1178\n",
      "143/223, train_loss: 0.1208, step time: 0.1101\n",
      "144/223, train_loss: 0.1132, step time: 0.1013\n",
      "145/223, train_loss: 0.1094, step time: 0.1106\n",
      "146/223, train_loss: 0.1082, step time: 0.1106\n",
      "147/223, train_loss: 0.1154, step time: 0.1186\n",
      "148/223, train_loss: 0.1040, step time: 0.1010\n",
      "149/223, train_loss: 0.1223, step time: 0.1146\n",
      "150/223, train_loss: 0.1107, step time: 0.1023\n",
      "151/223, train_loss: 0.1077, step time: 0.1260\n",
      "152/223, train_loss: 0.1114, step time: 0.1136\n",
      "153/223, train_loss: 0.1009, step time: 0.1084\n",
      "154/223, train_loss: 0.1118, step time: 0.1007\n",
      "155/223, train_loss: 0.1159, step time: 0.1012\n",
      "156/223, train_loss: 0.1248, step time: 0.1087\n",
      "157/223, train_loss: 0.1138, step time: 0.1009\n",
      "158/223, train_loss: 0.1167, step time: 0.1013\n",
      "159/223, train_loss: 0.1164, step time: 0.1019\n",
      "160/223, train_loss: 0.1069, step time: 0.1175\n",
      "161/223, train_loss: 0.1179, step time: 0.1097\n",
      "162/223, train_loss: 0.1192, step time: 0.1362\n",
      "163/223, train_loss: 0.1122, step time: 0.1063\n",
      "164/223, train_loss: 0.1078, step time: 0.1017\n",
      "165/223, train_loss: 0.1185, step time: 0.1123\n",
      "166/223, train_loss: 0.1093, step time: 0.1109\n",
      "167/223, train_loss: 0.1115, step time: 0.0999\n",
      "168/223, train_loss: 0.1241, step time: 0.1002\n",
      "169/223, train_loss: 0.1208, step time: 0.1058\n",
      "170/223, train_loss: 0.1146, step time: 0.1183\n",
      "171/223, train_loss: 0.1085, step time: 0.1191\n",
      "172/223, train_loss: 0.1107, step time: 0.1001\n",
      "173/223, train_loss: 0.1080, step time: 0.1118\n",
      "174/223, train_loss: 0.1106, step time: 0.1001\n",
      "175/223, train_loss: 0.1107, step time: 0.1013\n",
      "176/223, train_loss: 0.3124, step time: 0.1032\n",
      "177/223, train_loss: 0.0997, step time: 0.1121\n",
      "178/223, train_loss: 0.1059, step time: 0.1037\n",
      "179/223, train_loss: 0.1212, step time: 0.1060\n",
      "180/223, train_loss: 0.1059, step time: 0.1268\n",
      "181/223, train_loss: 0.1169, step time: 0.1125\n",
      "182/223, train_loss: 0.1230, step time: 0.1005\n",
      "183/223, train_loss: 0.1112, step time: 0.1011\n",
      "184/223, train_loss: 0.1143, step time: 0.1025\n",
      "185/223, train_loss: 0.1069, step time: 0.1013\n",
      "186/223, train_loss: 0.1079, step time: 0.1044\n",
      "187/223, train_loss: 0.1079, step time: 0.1221\n",
      "188/223, train_loss: 0.1075, step time: 0.0998\n",
      "189/223, train_loss: 0.1109, step time: 0.1175\n",
      "190/223, train_loss: 0.1307, step time: 0.1125\n",
      "191/223, train_loss: 0.1140, step time: 0.1005\n",
      "192/223, train_loss: 0.1234, step time: 0.1010\n",
      "193/223, train_loss: 0.1092, step time: 0.1178\n",
      "194/223, train_loss: 0.1247, step time: 0.1000\n",
      "195/223, train_loss: 0.1169, step time: 0.1002\n",
      "196/223, train_loss: 0.1098, step time: 0.1187\n",
      "197/223, train_loss: 0.1179, step time: 0.1032\n",
      "198/223, train_loss: 0.1254, step time: 0.1118\n",
      "199/223, train_loss: 0.1238, step time: 0.1025\n",
      "200/223, train_loss: 0.1169, step time: 0.1124\n",
      "201/223, train_loss: 0.1113, step time: 0.1174\n",
      "202/223, train_loss: 0.1104, step time: 0.1224\n",
      "203/223, train_loss: 0.1103, step time: 0.1385\n",
      "204/223, train_loss: 0.1139, step time: 0.1169\n",
      "205/223, train_loss: 0.1231, step time: 0.1075\n",
      "206/223, train_loss: 0.1130, step time: 0.1373\n",
      "207/223, train_loss: 0.1155, step time: 0.1265\n",
      "208/223, train_loss: 0.1147, step time: 0.0993\n",
      "209/223, train_loss: 0.1160, step time: 0.1087\n",
      "210/223, train_loss: 0.1208, step time: 0.1031\n",
      "211/223, train_loss: 0.1113, step time: 0.1014\n",
      "212/223, train_loss: 0.1124, step time: 0.1076\n",
      "213/223, train_loss: 0.1441, step time: 0.1005\n",
      "214/223, train_loss: 0.1140, step time: 0.1005\n",
      "215/223, train_loss: 0.1284, step time: 0.1213\n",
      "216/223, train_loss: 0.1133, step time: 0.1161\n",
      "217/223, train_loss: 0.1305, step time: 0.1005\n",
      "218/223, train_loss: 0.1173, step time: 0.1010\n",
      "219/223, train_loss: 0.1172, step time: 0.1005\n",
      "220/223, train_loss: 0.1291, step time: 0.1059\n",
      "221/223, train_loss: 0.1155, step time: 0.0991\n",
      "222/223, train_loss: 0.1079, step time: 0.0990\n",
      "223/223, train_loss: 0.1243, step time: 0.1101\n",
      "epoch 95 average loss: 0.1168\n",
      "saved new best metric model\n",
      "current epoch: 95 current mean dice: 0.8490 tc: 0.9158 wt: 0.8598 et: 0.7714\n",
      "best mean dice: 0.8490 at epoch: 95\n",
      "time consuming of epoch 95 is: 91.1700\n",
      "----------\n",
      "epoch 96/300\n",
      "1/223, train_loss: 0.1162, step time: 0.1156\n",
      "2/223, train_loss: 0.1048, step time: 0.1056\n",
      "3/223, train_loss: 0.1044, step time: 0.1050\n",
      "4/223, train_loss: 0.1186, step time: 0.1161\n",
      "5/223, train_loss: 0.1196, step time: 0.1102\n",
      "6/223, train_loss: 0.1059, step time: 0.1002\n",
      "7/223, train_loss: 0.1244, step time: 0.1113\n",
      "8/223, train_loss: 0.1190, step time: 0.1170\n",
      "9/223, train_loss: 0.1187, step time: 0.1186\n",
      "10/223, train_loss: 0.1123, step time: 0.1110\n",
      "11/223, train_loss: 0.1259, step time: 0.1145\n",
      "12/223, train_loss: 0.1135, step time: 0.1164\n",
      "13/223, train_loss: 0.1267, step time: 0.0999\n",
      "14/223, train_loss: 0.1104, step time: 0.1070\n",
      "15/223, train_loss: 0.1211, step time: 0.1039\n",
      "16/223, train_loss: 0.1205, step time: 0.1029\n",
      "17/223, train_loss: 0.0981, step time: 0.1013\n",
      "18/223, train_loss: 0.1243, step time: 0.1078\n",
      "19/223, train_loss: 0.1122, step time: 0.0998\n",
      "20/223, train_loss: 0.1220, step time: 0.0996\n",
      "21/223, train_loss: 0.1095, step time: 0.1154\n",
      "22/223, train_loss: 0.1273, step time: 0.1099\n",
      "23/223, train_loss: 0.1168, step time: 0.1056\n",
      "24/223, train_loss: 0.1251, step time: 0.1021\n",
      "25/223, train_loss: 0.1121, step time: 0.1043\n",
      "26/223, train_loss: 0.1135, step time: 0.1206\n",
      "27/223, train_loss: 0.1137, step time: 0.1198\n",
      "28/223, train_loss: 0.1218, step time: 0.1007\n",
      "29/223, train_loss: 0.1074, step time: 0.1087\n",
      "30/223, train_loss: 0.1090, step time: 0.1149\n",
      "31/223, train_loss: 0.1061, step time: 0.1119\n",
      "32/223, train_loss: 0.1079, step time: 0.1095\n",
      "33/223, train_loss: 0.1039, step time: 0.1003\n",
      "34/223, train_loss: 0.1088, step time: 0.1022\n",
      "35/223, train_loss: 0.1158, step time: 0.1242\n",
      "36/223, train_loss: 0.1249, step time: 0.1147\n",
      "37/223, train_loss: 0.1052, step time: 0.1141\n",
      "38/223, train_loss: 0.1278, step time: 0.1257\n",
      "39/223, train_loss: 0.1184, step time: 0.1256\n",
      "40/223, train_loss: 0.1105, step time: 0.1018\n",
      "41/223, train_loss: 0.1171, step time: 0.1118\n",
      "42/223, train_loss: 0.1175, step time: 0.1000\n",
      "43/223, train_loss: 0.1144, step time: 0.0997\n",
      "44/223, train_loss: 0.1095, step time: 0.1003\n",
      "45/223, train_loss: 0.1093, step time: 0.1059\n",
      "46/223, train_loss: 0.1062, step time: 0.1108\n",
      "47/223, train_loss: 0.1127, step time: 0.1081\n",
      "48/223, train_loss: 0.1171, step time: 0.1146\n",
      "49/223, train_loss: 0.1071, step time: 0.1137\n",
      "50/223, train_loss: 0.1211, step time: 0.1001\n",
      "51/223, train_loss: 0.1275, step time: 0.1007\n",
      "52/223, train_loss: 0.1158, step time: 0.1136\n",
      "53/223, train_loss: 0.1125, step time: 0.0995\n",
      "54/223, train_loss: 0.1199, step time: 0.1071\n",
      "55/223, train_loss: 0.1163, step time: 0.1162\n",
      "56/223, train_loss: 0.1140, step time: 0.1109\n",
      "57/223, train_loss: 0.1206, step time: 0.1002\n",
      "58/223, train_loss: 0.1141, step time: 0.1000\n",
      "59/223, train_loss: 0.1120, step time: 0.1130\n",
      "60/223, train_loss: 0.1191, step time: 0.1026\n",
      "61/223, train_loss: 0.1072, step time: 0.1296\n",
      "62/223, train_loss: 0.1033, step time: 0.1145\n",
      "63/223, train_loss: 0.1060, step time: 0.0995\n",
      "64/223, train_loss: 0.1377, step time: 0.1264\n",
      "65/223, train_loss: 0.1043, step time: 0.1008\n",
      "66/223, train_loss: 0.1108, step time: 0.1110\n",
      "67/223, train_loss: 0.1088, step time: 0.1108\n",
      "68/223, train_loss: 0.1278, step time: 0.1057\n",
      "69/223, train_loss: 0.1106, step time: 0.1067\n",
      "70/223, train_loss: 0.1191, step time: 0.1024\n",
      "71/223, train_loss: 0.1056, step time: 0.1012\n",
      "72/223, train_loss: 0.1195, step time: 0.1096\n",
      "73/223, train_loss: 0.1163, step time: 0.1098\n",
      "74/223, train_loss: 0.1068, step time: 0.1044\n",
      "75/223, train_loss: 0.3133, step time: 0.1127\n",
      "76/223, train_loss: 0.1164, step time: 0.1080\n",
      "77/223, train_loss: 0.1144, step time: 0.1134\n",
      "78/223, train_loss: 0.1134, step time: 0.1063\n",
      "79/223, train_loss: 0.1233, step time: 0.1206\n",
      "80/223, train_loss: 0.1186, step time: 0.1053\n",
      "81/223, train_loss: 0.1105, step time: 0.1231\n",
      "82/223, train_loss: 0.1056, step time: 0.1088\n",
      "83/223, train_loss: 0.1180, step time: 0.1040\n",
      "84/223, train_loss: 0.1178, step time: 0.1181\n",
      "85/223, train_loss: 0.1190, step time: 0.1177\n",
      "86/223, train_loss: 0.1076, step time: 0.1059\n",
      "87/223, train_loss: 0.1169, step time: 0.1035\n",
      "88/223, train_loss: 0.1275, step time: 0.1021\n",
      "89/223, train_loss: 0.1155, step time: 0.1331\n",
      "90/223, train_loss: 0.1099, step time: 0.1401\n",
      "91/223, train_loss: 0.1113, step time: 0.1084\n",
      "92/223, train_loss: 0.1037, step time: 0.1185\n",
      "93/223, train_loss: 0.1235, step time: 0.1210\n",
      "94/223, train_loss: 0.1062, step time: 0.1089\n",
      "95/223, train_loss: 0.1049, step time: 0.1021\n",
      "96/223, train_loss: 0.1162, step time: 0.1049\n",
      "97/223, train_loss: 0.1123, step time: 0.1094\n",
      "98/223, train_loss: 0.1123, step time: 0.1230\n",
      "99/223, train_loss: 0.1202, step time: 0.1026\n",
      "100/223, train_loss: 0.1110, step time: 0.1018\n",
      "101/223, train_loss: 0.1138, step time: 0.1539\n",
      "102/223, train_loss: 0.1167, step time: 0.1200\n",
      "103/223, train_loss: 0.1198, step time: 0.1063\n",
      "104/223, train_loss: 0.1280, step time: 0.1269\n",
      "105/223, train_loss: 0.1324, step time: 0.1002\n",
      "106/223, train_loss: 0.1185, step time: 0.1006\n",
      "107/223, train_loss: 0.1019, step time: 0.1010\n",
      "108/223, train_loss: 0.1154, step time: 0.1077\n",
      "109/223, train_loss: 0.1043, step time: 0.1256\n",
      "110/223, train_loss: 0.1103, step time: 0.1282\n",
      "111/223, train_loss: 0.1213, step time: 0.1210\n",
      "112/223, train_loss: 0.1162, step time: 0.1165\n",
      "113/223, train_loss: 0.1055, step time: 0.1202\n",
      "114/223, train_loss: 0.1141, step time: 0.1335\n",
      "115/223, train_loss: 0.1135, step time: 0.1034\n",
      "116/223, train_loss: 0.1134, step time: 0.1312\n",
      "117/223, train_loss: 0.1144, step time: 0.1279\n",
      "118/223, train_loss: 0.1345, step time: 0.1053\n",
      "119/223, train_loss: 0.1232, step time: 0.1173\n",
      "120/223, train_loss: 0.1059, step time: 0.1050\n",
      "121/223, train_loss: 0.1078, step time: 0.1221\n",
      "122/223, train_loss: 0.1064, step time: 0.1356\n",
      "123/223, train_loss: 0.1262, step time: 0.1148\n",
      "124/223, train_loss: 0.1081, step time: 0.0985\n",
      "125/223, train_loss: 0.1213, step time: 0.0993\n",
      "126/223, train_loss: 0.1133, step time: 0.0988\n",
      "127/223, train_loss: 0.1251, step time: 0.1141\n",
      "128/223, train_loss: 0.1251, step time: 0.1011\n",
      "129/223, train_loss: 0.1132, step time: 0.1002\n",
      "130/223, train_loss: 0.1227, step time: 0.1004\n",
      "131/223, train_loss: 0.1162, step time: 0.1007\n",
      "132/223, train_loss: 0.1157, step time: 0.1011\n",
      "133/223, train_loss: 0.1056, step time: 0.0998\n",
      "134/223, train_loss: 0.1235, step time: 0.0996\n",
      "135/223, train_loss: 0.1127, step time: 0.1190\n",
      "136/223, train_loss: 0.1107, step time: 0.1260\n",
      "137/223, train_loss: 0.1052, step time: 0.1125\n",
      "138/223, train_loss: 0.1168, step time: 0.1005\n",
      "139/223, train_loss: 0.1134, step time: 0.1098\n",
      "140/223, train_loss: 0.1143, step time: 0.1018\n",
      "141/223, train_loss: 0.1118, step time: 0.1027\n",
      "142/223, train_loss: 0.1113, step time: 0.0994\n",
      "143/223, train_loss: 0.1039, step time: 0.1158\n",
      "144/223, train_loss: 0.1279, step time: 0.1134\n",
      "145/223, train_loss: 0.1201, step time: 0.1340\n",
      "146/223, train_loss: 0.1132, step time: 0.0995\n",
      "147/223, train_loss: 0.1103, step time: 0.1005\n",
      "148/223, train_loss: 0.1198, step time: 0.1001\n",
      "149/223, train_loss: 0.1310, step time: 0.1004\n",
      "150/223, train_loss: 0.1087, step time: 0.1084\n",
      "151/223, train_loss: 0.1107, step time: 0.1256\n",
      "152/223, train_loss: 0.1149, step time: 0.1007\n",
      "153/223, train_loss: 0.1085, step time: 0.1028\n",
      "154/223, train_loss: 0.1203, step time: 0.1006\n",
      "155/223, train_loss: 0.1118, step time: 0.1225\n",
      "156/223, train_loss: 0.1179, step time: 0.1001\n",
      "157/223, train_loss: 0.1313, step time: 0.1001\n",
      "158/223, train_loss: 0.1221, step time: 0.1108\n",
      "159/223, train_loss: 0.1026, step time: 0.1204\n",
      "160/223, train_loss: 0.1089, step time: 0.1014\n",
      "161/223, train_loss: 0.1282, step time: 0.0996\n",
      "162/223, train_loss: 0.1227, step time: 0.1105\n",
      "163/223, train_loss: 0.1328, step time: 0.1266\n",
      "164/223, train_loss: 0.1164, step time: 0.1090\n",
      "165/223, train_loss: 0.1064, step time: 0.1019\n",
      "166/223, train_loss: 0.1173, step time: 0.0997\n",
      "167/223, train_loss: 0.1210, step time: 0.1030\n",
      "168/223, train_loss: 0.1074, step time: 0.1048\n",
      "169/223, train_loss: 0.1075, step time: 0.1055\n",
      "170/223, train_loss: 0.1097, step time: 0.1013\n",
      "171/223, train_loss: 0.1127, step time: 0.1001\n",
      "172/223, train_loss: 0.1135, step time: 0.1184\n",
      "173/223, train_loss: 0.1156, step time: 0.1289\n",
      "174/223, train_loss: 0.1081, step time: 0.1101\n",
      "175/223, train_loss: 0.1019, step time: 0.0995\n",
      "176/223, train_loss: 0.1172, step time: 0.1112\n",
      "177/223, train_loss: 0.1381, step time: 0.1085\n",
      "178/223, train_loss: 0.1081, step time: 0.0996\n",
      "179/223, train_loss: 0.1053, step time: 0.1080\n",
      "180/223, train_loss: 0.1022, step time: 0.1209\n",
      "181/223, train_loss: 0.1219, step time: 0.1098\n",
      "182/223, train_loss: 0.1023, step time: 0.1010\n",
      "183/223, train_loss: 0.1081, step time: 0.1054\n",
      "184/223, train_loss: 0.1072, step time: 0.1005\n",
      "185/223, train_loss: 0.1082, step time: 0.1101\n",
      "186/223, train_loss: 0.1106, step time: 0.1069\n",
      "187/223, train_loss: 0.1147, step time: 0.1142\n",
      "188/223, train_loss: 0.1092, step time: 0.1046\n",
      "189/223, train_loss: 0.1081, step time: 0.1016\n",
      "190/223, train_loss: 0.0983, step time: 0.1006\n",
      "191/223, train_loss: 0.1236, step time: 0.1155\n",
      "192/223, train_loss: 0.1188, step time: 0.1241\n",
      "193/223, train_loss: 0.1063, step time: 0.1150\n",
      "194/223, train_loss: 0.1178, step time: 0.1050\n",
      "195/223, train_loss: 0.1248, step time: 0.1188\n",
      "196/223, train_loss: 0.1047, step time: 0.0994\n",
      "197/223, train_loss: 0.1234, step time: 0.1140\n",
      "198/223, train_loss: 0.1170, step time: 0.1035\n",
      "199/223, train_loss: 0.1122, step time: 0.1008\n",
      "200/223, train_loss: 0.1086, step time: 0.1185\n",
      "201/223, train_loss: 0.1022, step time: 0.1009\n",
      "202/223, train_loss: 0.1188, step time: 0.1009\n",
      "203/223, train_loss: 0.1219, step time: 0.1034\n",
      "204/223, train_loss: 0.0967, step time: 0.1010\n",
      "205/223, train_loss: 0.1218, step time: 0.1009\n",
      "206/223, train_loss: 0.1096, step time: 0.1004\n",
      "207/223, train_loss: 0.1014, step time: 0.1061\n",
      "208/223, train_loss: 0.1023, step time: 0.1007\n",
      "209/223, train_loss: 0.1180, step time: 0.1022\n",
      "210/223, train_loss: 0.1254, step time: 0.1006\n",
      "211/223, train_loss: 0.1082, step time: 0.1208\n",
      "212/223, train_loss: 0.1128, step time: 0.1076\n",
      "213/223, train_loss: 0.1062, step time: 0.1007\n",
      "214/223, train_loss: 0.1203, step time: 0.1000\n",
      "215/223, train_loss: 0.1115, step time: 0.1133\n",
      "216/223, train_loss: 0.1172, step time: 0.1003\n",
      "217/223, train_loss: 0.1160, step time: 0.1004\n",
      "218/223, train_loss: 0.1137, step time: 0.1068\n",
      "219/223, train_loss: 0.1146, step time: 0.0990\n",
      "220/223, train_loss: 0.1089, step time: 0.0998\n",
      "221/223, train_loss: 0.1242, step time: 0.1014\n",
      "222/223, train_loss: 0.1215, step time: 0.1004\n",
      "223/223, train_loss: 0.1084, step time: 0.0990\n",
      "epoch 96 average loss: 0.1154\n",
      "time consuming of epoch 96 is: 88.0591\n",
      "----------\n",
      "epoch 97/300\n",
      "1/223, train_loss: 0.1021, step time: 0.1042\n",
      "2/223, train_loss: 0.1041, step time: 0.1172\n",
      "3/223, train_loss: 0.1075, step time: 0.1068\n",
      "4/223, train_loss: 0.1049, step time: 0.0994\n",
      "5/223, train_loss: 0.1190, step time: 0.0996\n",
      "6/223, train_loss: 0.1058, step time: 0.1001\n",
      "7/223, train_loss: 0.1122, step time: 0.1011\n",
      "8/223, train_loss: 0.1163, step time: 0.1004\n",
      "9/223, train_loss: 0.1088, step time: 0.1145\n",
      "10/223, train_loss: 0.1215, step time: 0.1154\n",
      "11/223, train_loss: 0.1143, step time: 0.1104\n",
      "12/223, train_loss: 0.1157, step time: 0.1107\n",
      "13/223, train_loss: 0.1130, step time: 0.1048\n",
      "14/223, train_loss: 0.1185, step time: 0.1047\n",
      "15/223, train_loss: 0.1181, step time: 0.1131\n",
      "16/223, train_loss: 0.1069, step time: 0.1104\n",
      "17/223, train_loss: 0.1110, step time: 0.1116\n",
      "18/223, train_loss: 0.1035, step time: 0.1031\n",
      "19/223, train_loss: 0.1182, step time: 0.1186\n",
      "20/223, train_loss: 0.1128, step time: 0.1132\n",
      "21/223, train_loss: 0.1068, step time: 0.1029\n",
      "22/223, train_loss: 0.1121, step time: 0.1173\n",
      "23/223, train_loss: 0.1184, step time: 0.1001\n",
      "24/223, train_loss: 0.1255, step time: 0.1069\n",
      "25/223, train_loss: 0.1095, step time: 0.1017\n",
      "26/223, train_loss: 0.1169, step time: 0.1108\n",
      "27/223, train_loss: 0.1171, step time: 0.1126\n",
      "28/223, train_loss: 0.1083, step time: 0.1103\n",
      "29/223, train_loss: 0.1215, step time: 0.1036\n",
      "30/223, train_loss: 0.1167, step time: 0.1109\n",
      "31/223, train_loss: 0.1018, step time: 0.1148\n",
      "32/223, train_loss: 0.1136, step time: 0.1005\n",
      "33/223, train_loss: 0.1084, step time: 0.1229\n",
      "34/223, train_loss: 0.1098, step time: 0.1113\n",
      "35/223, train_loss: 0.1111, step time: 0.1176\n",
      "36/223, train_loss: 0.1183, step time: 0.1092\n",
      "37/223, train_loss: 0.1158, step time: 0.1006\n",
      "38/223, train_loss: 0.0995, step time: 0.1140\n",
      "39/223, train_loss: 0.1208, step time: 0.1113\n",
      "40/223, train_loss: 0.1227, step time: 0.1004\n",
      "41/223, train_loss: 0.1066, step time: 0.1191\n",
      "42/223, train_loss: 0.1110, step time: 0.1052\n",
      "43/223, train_loss: 0.1267, step time: 0.1033\n",
      "44/223, train_loss: 0.1080, step time: 0.1039\n",
      "45/223, train_loss: 0.1175, step time: 0.1165\n",
      "46/223, train_loss: 0.1173, step time: 0.1122\n",
      "47/223, train_loss: 0.1017, step time: 0.1090\n",
      "48/223, train_loss: 0.1094, step time: 0.1020\n",
      "49/223, train_loss: 0.1171, step time: 0.1144\n",
      "50/223, train_loss: 0.1076, step time: 0.1227\n",
      "51/223, train_loss: 0.1224, step time: 0.1298\n",
      "52/223, train_loss: 0.1113, step time: 0.1012\n",
      "53/223, train_loss: 0.1171, step time: 0.1172\n",
      "54/223, train_loss: 0.1292, step time: 0.1003\n",
      "55/223, train_loss: 0.1156, step time: 0.1012\n",
      "56/223, train_loss: 0.1035, step time: 0.1000\n",
      "57/223, train_loss: 0.1143, step time: 0.1211\n",
      "58/223, train_loss: 0.1099, step time: 0.1145\n",
      "59/223, train_loss: 0.1107, step time: 0.1116\n",
      "60/223, train_loss: 0.1112, step time: 0.1173\n",
      "61/223, train_loss: 0.1113, step time: 0.1100\n",
      "62/223, train_loss: 0.1098, step time: 0.1052\n",
      "63/223, train_loss: 0.1084, step time: 0.0994\n",
      "64/223, train_loss: 0.1110, step time: 0.0993\n",
      "65/223, train_loss: 0.1362, step time: 0.1050\n",
      "66/223, train_loss: 0.1219, step time: 0.1052\n",
      "67/223, train_loss: 0.1133, step time: 0.0990\n",
      "68/223, train_loss: 0.1026, step time: 0.0989\n",
      "69/223, train_loss: 0.1137, step time: 0.1073\n",
      "70/223, train_loss: 0.1102, step time: 0.1139\n",
      "71/223, train_loss: 0.0997, step time: 0.1005\n",
      "72/223, train_loss: 0.1210, step time: 0.1009\n",
      "73/223, train_loss: 0.1086, step time: 0.1003\n",
      "74/223, train_loss: 0.1165, step time: 0.1152\n",
      "75/223, train_loss: 0.1073, step time: 0.1198\n",
      "76/223, train_loss: 0.1305, step time: 0.1192\n",
      "77/223, train_loss: 0.1080, step time: 0.1048\n",
      "78/223, train_loss: 0.1051, step time: 0.1058\n",
      "79/223, train_loss: 0.1230, step time: 0.0994\n",
      "80/223, train_loss: 0.1176, step time: 0.1051\n",
      "81/223, train_loss: 0.1076, step time: 0.1124\n",
      "82/223, train_loss: 0.1130, step time: 0.1114\n",
      "83/223, train_loss: 0.1140, step time: 0.1088\n",
      "84/223, train_loss: 0.1251, step time: 0.1226\n",
      "85/223, train_loss: 0.1267, step time: 0.1217\n",
      "86/223, train_loss: 0.1208, step time: 0.1086\n",
      "87/223, train_loss: 0.1106, step time: 0.1055\n",
      "88/223, train_loss: 0.1092, step time: 0.1090\n",
      "89/223, train_loss: 0.1194, step time: 0.1001\n",
      "90/223, train_loss: 0.1214, step time: 0.1163\n",
      "91/223, train_loss: 0.1190, step time: 0.1003\n",
      "92/223, train_loss: 0.1140, step time: 0.1007\n",
      "93/223, train_loss: 0.1237, step time: 0.1004\n",
      "94/223, train_loss: 0.1140, step time: 0.0992\n",
      "95/223, train_loss: 0.1154, step time: 0.1001\n",
      "96/223, train_loss: 0.1093, step time: 0.1001\n",
      "97/223, train_loss: 0.1267, step time: 0.1027\n",
      "98/223, train_loss: 0.1147, step time: 0.1146\n",
      "99/223, train_loss: 0.1055, step time: 0.1086\n",
      "100/223, train_loss: 0.1053, step time: 0.1106\n",
      "101/223, train_loss: 0.1177, step time: 0.1134\n",
      "102/223, train_loss: 0.1011, step time: 0.1191\n",
      "103/223, train_loss: 0.1101, step time: 0.1096\n",
      "104/223, train_loss: 0.1252, step time: 0.1210\n",
      "105/223, train_loss: 0.1066, step time: 0.1105\n",
      "106/223, train_loss: 0.1066, step time: 0.1112\n",
      "107/223, train_loss: 0.1169, step time: 0.1044\n",
      "108/223, train_loss: 0.1076, step time: 0.1214\n",
      "109/223, train_loss: 0.1414, step time: 0.0999\n",
      "110/223, train_loss: 0.1156, step time: 0.1016\n",
      "111/223, train_loss: 0.1320, step time: 0.0991\n",
      "112/223, train_loss: 0.1022, step time: 0.1086\n",
      "113/223, train_loss: 0.1086, step time: 0.1051\n",
      "114/223, train_loss: 0.1019, step time: 0.1107\n",
      "115/223, train_loss: 0.1134, step time: 0.0998\n",
      "116/223, train_loss: 0.1106, step time: 0.1275\n",
      "117/223, train_loss: 0.1037, step time: 0.1007\n",
      "118/223, train_loss: 0.1211, step time: 0.1028\n",
      "119/223, train_loss: 0.1155, step time: 0.1002\n",
      "120/223, train_loss: 0.1232, step time: 0.0998\n",
      "121/223, train_loss: 0.1164, step time: 0.1003\n",
      "122/223, train_loss: 0.1063, step time: 0.1004\n",
      "123/223, train_loss: 0.1151, step time: 0.0998\n",
      "124/223, train_loss: 0.1126, step time: 0.1003\n",
      "125/223, train_loss: 0.1118, step time: 0.1084\n",
      "126/223, train_loss: 0.1113, step time: 0.1087\n",
      "127/223, train_loss: 0.0989, step time: 0.0999\n",
      "128/223, train_loss: 0.1144, step time: 0.1060\n",
      "129/223, train_loss: 0.1172, step time: 0.1074\n",
      "130/223, train_loss: 0.1074, step time: 0.1001\n",
      "131/223, train_loss: 0.1122, step time: 0.1087\n",
      "132/223, train_loss: 0.1136, step time: 0.1005\n",
      "133/223, train_loss: 0.1059, step time: 0.1003\n",
      "134/223, train_loss: 0.1225, step time: 0.1003\n",
      "135/223, train_loss: 0.1224, step time: 0.1062\n",
      "136/223, train_loss: 0.1049, step time: 0.1060\n",
      "137/223, train_loss: 0.1113, step time: 0.1041\n",
      "138/223, train_loss: 0.1183, step time: 0.1000\n",
      "139/223, train_loss: 0.1237, step time: 0.1004\n",
      "140/223, train_loss: 0.1054, step time: 0.1018\n",
      "141/223, train_loss: 0.1018, step time: 0.1106\n",
      "142/223, train_loss: 0.3120, step time: 0.1014\n",
      "143/223, train_loss: 0.1196, step time: 0.1014\n",
      "144/223, train_loss: 0.1138, step time: 0.1005\n",
      "145/223, train_loss: 0.1188, step time: 0.1054\n",
      "146/223, train_loss: 0.1250, step time: 0.0989\n",
      "147/223, train_loss: 0.1036, step time: 0.0992\n",
      "148/223, train_loss: 0.1160, step time: 0.1272\n",
      "149/223, train_loss: 0.1235, step time: 0.0998\n",
      "150/223, train_loss: 0.1145, step time: 0.1002\n",
      "151/223, train_loss: 0.1146, step time: 0.1013\n",
      "152/223, train_loss: 0.1158, step time: 0.1195\n",
      "153/223, train_loss: 0.1222, step time: 0.1010\n",
      "154/223, train_loss: 0.1086, step time: 0.1084\n",
      "155/223, train_loss: 0.1198, step time: 0.1002\n",
      "156/223, train_loss: 0.1128, step time: 0.1005\n",
      "157/223, train_loss: 0.1036, step time: 0.1154\n",
      "158/223, train_loss: 0.1003, step time: 0.1076\n",
      "159/223, train_loss: 0.1177, step time: 0.1005\n",
      "160/223, train_loss: 0.1027, step time: 0.0996\n",
      "161/223, train_loss: 0.1167, step time: 0.1201\n",
      "162/223, train_loss: 0.1302, step time: 0.1132\n",
      "163/223, train_loss: 0.1102, step time: 0.1248\n",
      "164/223, train_loss: 0.1081, step time: 0.1086\n",
      "165/223, train_loss: 0.1188, step time: 0.1252\n",
      "166/223, train_loss: 0.1220, step time: 0.0996\n",
      "167/223, train_loss: 0.1021, step time: 0.1094\n",
      "168/223, train_loss: 0.1106, step time: 0.1195\n",
      "169/223, train_loss: 0.1156, step time: 0.1007\n",
      "170/223, train_loss: 0.1236, step time: 0.1126\n",
      "171/223, train_loss: 0.1117, step time: 0.1115\n",
      "172/223, train_loss: 0.1189, step time: 0.1578\n",
      "173/223, train_loss: 0.1201, step time: 0.1069\n",
      "174/223, train_loss: 0.1113, step time: 0.0997\n",
      "175/223, train_loss: 0.1155, step time: 0.1094\n",
      "176/223, train_loss: 0.1133, step time: 0.1357\n",
      "177/223, train_loss: 0.1233, step time: 0.1207\n",
      "178/223, train_loss: 0.1113, step time: 0.1104\n",
      "179/223, train_loss: 0.1218, step time: 0.1122\n",
      "180/223, train_loss: 0.1083, step time: 0.1393\n",
      "181/223, train_loss: 0.1082, step time: 0.1049\n",
      "182/223, train_loss: 0.1324, step time: 0.1128\n",
      "183/223, train_loss: 0.1058, step time: 0.0993\n",
      "184/223, train_loss: 0.1033, step time: 0.1030\n",
      "185/223, train_loss: 0.1060, step time: 0.1078\n",
      "186/223, train_loss: 0.1120, step time: 0.1024\n",
      "187/223, train_loss: 0.1107, step time: 0.1241\n",
      "188/223, train_loss: 0.1216, step time: 0.1404\n",
      "189/223, train_loss: 0.1264, step time: 0.1077\n",
      "190/223, train_loss: 0.1252, step time: 0.1172\n",
      "191/223, train_loss: 0.1229, step time: 0.1201\n",
      "192/223, train_loss: 0.1077, step time: 0.1120\n",
      "193/223, train_loss: 0.1068, step time: 0.1017\n",
      "194/223, train_loss: 0.1143, step time: 0.1060\n",
      "195/223, train_loss: 0.1338, step time: 0.1282\n",
      "196/223, train_loss: 0.1254, step time: 0.1213\n",
      "197/223, train_loss: 0.1080, step time: 0.1065\n",
      "198/223, train_loss: 0.1216, step time: 0.1116\n",
      "199/223, train_loss: 0.1280, step time: 0.1302\n",
      "200/223, train_loss: 0.1074, step time: 0.1168\n",
      "201/223, train_loss: 0.1234, step time: 0.1123\n",
      "202/223, train_loss: 0.1163, step time: 0.1006\n",
      "203/223, train_loss: 0.1079, step time: 0.1014\n",
      "204/223, train_loss: 0.1137, step time: 0.1002\n",
      "205/223, train_loss: 0.1247, step time: 0.1182\n",
      "206/223, train_loss: 0.1145, step time: 0.1209\n",
      "207/223, train_loss: 0.1299, step time: 0.1104\n",
      "208/223, train_loss: 0.1062, step time: 0.1052\n",
      "209/223, train_loss: 0.1237, step time: 0.0997\n",
      "210/223, train_loss: 0.1157, step time: 0.1106\n",
      "211/223, train_loss: 0.1174, step time: 0.1267\n",
      "212/223, train_loss: 0.1257, step time: 0.1079\n",
      "213/223, train_loss: 0.1370, step time: 0.1078\n",
      "214/223, train_loss: 0.1119, step time: 0.1142\n",
      "215/223, train_loss: 0.1188, step time: 0.1115\n",
      "216/223, train_loss: 0.1172, step time: 0.1102\n",
      "217/223, train_loss: 0.1256, step time: 0.0995\n",
      "218/223, train_loss: 0.1254, step time: 0.0989\n",
      "219/223, train_loss: 0.1219, step time: 0.1002\n",
      "220/223, train_loss: 0.1168, step time: 0.1008\n",
      "221/223, train_loss: 0.1219, step time: 0.1010\n",
      "222/223, train_loss: 0.1151, step time: 0.0987\n",
      "223/223, train_loss: 0.1105, step time: 0.0999\n",
      "epoch 97 average loss: 0.1155\n",
      "time consuming of epoch 97 is: 85.8126\n",
      "----------\n",
      "epoch 98/300\n",
      "1/223, train_loss: 0.1297, step time: 0.1033\n",
      "2/223, train_loss: 0.1078, step time: 0.1001\n",
      "3/223, train_loss: 0.1023, step time: 0.1008\n",
      "4/223, train_loss: 0.1211, step time: 0.1008\n",
      "5/223, train_loss: 0.1029, step time: 0.1123\n",
      "6/223, train_loss: 0.1156, step time: 0.1131\n",
      "7/223, train_loss: 0.1088, step time: 0.1032\n",
      "8/223, train_loss: 0.1106, step time: 0.1008\n",
      "9/223, train_loss: 0.1040, step time: 0.1028\n",
      "10/223, train_loss: 0.1061, step time: 0.1160\n",
      "11/223, train_loss: 0.1006, step time: 0.1005\n",
      "12/223, train_loss: 0.1037, step time: 0.1005\n",
      "13/223, train_loss: 0.1102, step time: 0.1005\n",
      "14/223, train_loss: 0.1258, step time: 0.1273\n",
      "15/223, train_loss: 0.1093, step time: 0.1061\n",
      "16/223, train_loss: 0.1243, step time: 0.1034\n",
      "17/223, train_loss: 0.1066, step time: 0.1006\n",
      "18/223, train_loss: 0.1216, step time: 0.1140\n",
      "19/223, train_loss: 0.1134, step time: 0.1170\n",
      "20/223, train_loss: 0.1194, step time: 0.1042\n",
      "21/223, train_loss: 0.1210, step time: 0.1063\n",
      "22/223, train_loss: 0.1106, step time: 0.1176\n",
      "23/223, train_loss: 0.1133, step time: 0.1149\n",
      "24/223, train_loss: 0.1063, step time: 0.1184\n",
      "25/223, train_loss: 0.1120, step time: 0.1044\n",
      "26/223, train_loss: 0.1171, step time: 0.1005\n",
      "27/223, train_loss: 0.1180, step time: 0.1115\n",
      "28/223, train_loss: 0.1072, step time: 0.0998\n",
      "29/223, train_loss: 0.1103, step time: 0.1024\n",
      "30/223, train_loss: 0.1082, step time: 0.1187\n",
      "31/223, train_loss: 0.1184, step time: 0.0998\n",
      "32/223, train_loss: 0.1242, step time: 0.1088\n",
      "33/223, train_loss: 0.1198, step time: 0.1086\n",
      "34/223, train_loss: 0.1114, step time: 0.1199\n",
      "35/223, train_loss: 0.1148, step time: 0.1030\n",
      "36/223, train_loss: 0.1236, step time: 0.1033\n",
      "37/223, train_loss: 0.1189, step time: 0.1170\n",
      "38/223, train_loss: 0.1233, step time: 0.1181\n",
      "39/223, train_loss: 0.1015, step time: 0.1097\n",
      "40/223, train_loss: 0.1188, step time: 0.1006\n",
      "41/223, train_loss: 0.1172, step time: 0.0995\n",
      "42/223, train_loss: 0.1133, step time: 0.1095\n",
      "43/223, train_loss: 0.1030, step time: 0.1011\n",
      "44/223, train_loss: 0.1119, step time: 0.1081\n",
      "45/223, train_loss: 0.1200, step time: 0.1003\n",
      "46/223, train_loss: 0.1047, step time: 0.1007\n",
      "47/223, train_loss: 0.1104, step time: 0.0998\n",
      "48/223, train_loss: 0.1232, step time: 0.1000\n",
      "49/223, train_loss: 0.1098, step time: 0.1001\n",
      "50/223, train_loss: 0.1116, step time: 0.1080\n",
      "51/223, train_loss: 0.1093, step time: 0.1010\n",
      "52/223, train_loss: 0.1290, step time: 0.1108\n",
      "53/223, train_loss: 0.1103, step time: 0.0999\n",
      "54/223, train_loss: 0.1210, step time: 0.0999\n",
      "55/223, train_loss: 0.1064, step time: 0.1141\n",
      "56/223, train_loss: 0.1260, step time: 0.1057\n",
      "57/223, train_loss: 0.1245, step time: 0.1153\n",
      "58/223, train_loss: 0.1178, step time: 0.1039\n",
      "59/223, train_loss: 0.0998, step time: 0.1003\n",
      "60/223, train_loss: 0.1114, step time: 0.1008\n",
      "61/223, train_loss: 0.1105, step time: 0.1173\n",
      "62/223, train_loss: 0.0998, step time: 0.1017\n",
      "63/223, train_loss: 0.1062, step time: 0.1114\n",
      "64/223, train_loss: 0.1125, step time: 0.0993\n",
      "65/223, train_loss: 0.1264, step time: 0.0998\n",
      "66/223, train_loss: 0.1136, step time: 0.1161\n",
      "67/223, train_loss: 0.1166, step time: 0.0997\n",
      "68/223, train_loss: 0.1160, step time: 0.0986\n",
      "69/223, train_loss: 0.1187, step time: 0.1453\n",
      "70/223, train_loss: 0.1215, step time: 0.1077\n",
      "71/223, train_loss: 0.1142, step time: 0.1623\n",
      "72/223, train_loss: 0.1086, step time: 0.1329\n",
      "73/223, train_loss: 0.1022, step time: 0.1042\n",
      "74/223, train_loss: 0.1103, step time: 0.1178\n",
      "75/223, train_loss: 0.1113, step time: 0.1098\n",
      "76/223, train_loss: 0.1094, step time: 0.1182\n",
      "77/223, train_loss: 0.1083, step time: 0.1271\n",
      "78/223, train_loss: 0.1147, step time: 0.1064\n",
      "79/223, train_loss: 0.3074, step time: 0.1122\n",
      "80/223, train_loss: 0.1120, step time: 0.1047\n",
      "81/223, train_loss: 0.1049, step time: 0.1074\n",
      "82/223, train_loss: 0.1200, step time: 0.1151\n",
      "83/223, train_loss: 0.1074, step time: 0.1150\n",
      "84/223, train_loss: 0.1011, step time: 0.1188\n",
      "85/223, train_loss: 0.1021, step time: 0.1143\n",
      "86/223, train_loss: 0.1311, step time: 0.1291\n",
      "87/223, train_loss: 0.1031, step time: 0.0997\n",
      "88/223, train_loss: 0.1313, step time: 0.1181\n",
      "89/223, train_loss: 0.1198, step time: 0.1155\n",
      "90/223, train_loss: 0.1148, step time: 0.1000\n",
      "91/223, train_loss: 0.1096, step time: 0.1002\n",
      "92/223, train_loss: 0.1139, step time: 0.1001\n",
      "93/223, train_loss: 0.1245, step time: 0.1334\n",
      "94/223, train_loss: 0.1247, step time: 0.1150\n",
      "95/223, train_loss: 0.1140, step time: 0.1231\n",
      "96/223, train_loss: 0.1278, step time: 0.1217\n",
      "97/223, train_loss: 0.1135, step time: 0.1067\n",
      "98/223, train_loss: 0.1181, step time: 0.1012\n",
      "99/223, train_loss: 0.1131, step time: 0.1003\n",
      "100/223, train_loss: 0.1098, step time: 0.1006\n",
      "101/223, train_loss: 0.1098, step time: 0.1133\n",
      "102/223, train_loss: 0.1107, step time: 0.1124\n",
      "103/223, train_loss: 0.1279, step time: 0.1142\n",
      "104/223, train_loss: 0.1182, step time: 0.1301\n",
      "105/223, train_loss: 0.1140, step time: 0.1274\n",
      "106/223, train_loss: 0.1045, step time: 0.1189\n",
      "107/223, train_loss: 0.1047, step time: 0.1141\n",
      "108/223, train_loss: 0.1099, step time: 0.1244\n",
      "109/223, train_loss: 0.1048, step time: 0.1124\n",
      "110/223, train_loss: 0.1344, step time: 0.1301\n",
      "111/223, train_loss: 0.1347, step time: 0.1161\n",
      "112/223, train_loss: 0.1057, step time: 0.1060\n",
      "113/223, train_loss: 0.1131, step time: 0.1099\n",
      "114/223, train_loss: 0.1184, step time: 0.1195\n",
      "115/223, train_loss: 0.1213, step time: 0.1097\n",
      "116/223, train_loss: 0.1148, step time: 0.1158\n",
      "117/223, train_loss: 0.1186, step time: 0.1138\n",
      "118/223, train_loss: 0.1206, step time: 0.1134\n",
      "119/223, train_loss: 0.1126, step time: 0.1112\n",
      "120/223, train_loss: 0.1138, step time: 0.1134\n",
      "121/223, train_loss: 0.1171, step time: 0.1265\n",
      "122/223, train_loss: 0.1212, step time: 0.1024\n",
      "123/223, train_loss: 0.1227, step time: 0.1007\n",
      "124/223, train_loss: 0.1192, step time: 0.1001\n",
      "125/223, train_loss: 0.1146, step time: 0.1297\n",
      "126/223, train_loss: 0.1112, step time: 0.1201\n",
      "127/223, train_loss: 0.1271, step time: 0.1129\n",
      "128/223, train_loss: 0.1127, step time: 0.1090\n",
      "129/223, train_loss: 0.1027, step time: 0.1316\n",
      "130/223, train_loss: 0.1144, step time: 0.1136\n",
      "131/223, train_loss: 0.1096, step time: 0.1171\n",
      "132/223, train_loss: 0.1083, step time: 0.1231\n",
      "133/223, train_loss: 0.1123, step time: 0.0993\n",
      "134/223, train_loss: 0.1196, step time: 0.1137\n",
      "135/223, train_loss: 0.1100, step time: 0.0997\n",
      "136/223, train_loss: 0.1093, step time: 0.0989\n",
      "137/223, train_loss: 0.1104, step time: 0.0994\n",
      "138/223, train_loss: 0.1141, step time: 0.1101\n",
      "139/223, train_loss: 0.1096, step time: 0.1102\n",
      "140/223, train_loss: 0.1010, step time: 0.1103\n",
      "141/223, train_loss: 0.1186, step time: 0.0996\n",
      "142/223, train_loss: 0.1149, step time: 0.1012\n",
      "143/223, train_loss: 0.1195, step time: 0.0996\n",
      "144/223, train_loss: 0.1112, step time: 0.0992\n",
      "145/223, train_loss: 0.1309, step time: 0.1003\n",
      "146/223, train_loss: 0.1049, step time: 0.1228\n",
      "147/223, train_loss: 0.1088, step time: 0.1087\n",
      "148/223, train_loss: 0.1103, step time: 0.1002\n",
      "149/223, train_loss: 0.1186, step time: 0.1439\n",
      "150/223, train_loss: 0.1233, step time: 0.1186\n",
      "151/223, train_loss: 0.1094, step time: 0.1001\n",
      "152/223, train_loss: 0.1035, step time: 0.0998\n",
      "153/223, train_loss: 0.1179, step time: 0.1241\n",
      "154/223, train_loss: 0.1068, step time: 0.0996\n",
      "155/223, train_loss: 0.1197, step time: 0.0997\n",
      "156/223, train_loss: 0.1267, step time: 0.1032\n",
      "157/223, train_loss: 0.1219, step time: 0.1200\n",
      "158/223, train_loss: 0.1029, step time: 0.1128\n",
      "159/223, train_loss: 0.1039, step time: 0.1241\n",
      "160/223, train_loss: 0.1280, step time: 0.1002\n",
      "161/223, train_loss: 0.1302, step time: 0.1111\n",
      "162/223, train_loss: 0.1050, step time: 0.1105\n",
      "163/223, train_loss: 0.1175, step time: 0.1349\n",
      "164/223, train_loss: 0.1202, step time: 0.1078\n",
      "165/223, train_loss: 0.1048, step time: 0.1000\n",
      "166/223, train_loss: 0.1112, step time: 0.1092\n",
      "167/223, train_loss: 0.1090, step time: 0.1307\n",
      "168/223, train_loss: 0.1295, step time: 0.1266\n",
      "169/223, train_loss: 0.1114, step time: 0.1006\n",
      "170/223, train_loss: 0.1094, step time: 0.1135\n",
      "171/223, train_loss: 0.1077, step time: 0.0998\n",
      "172/223, train_loss: 0.0982, step time: 0.1029\n",
      "173/223, train_loss: 0.1225, step time: 0.1190\n",
      "174/223, train_loss: 0.1174, step time: 0.1118\n",
      "175/223, train_loss: 0.1306, step time: 0.1009\n",
      "176/223, train_loss: 0.1335, step time: 0.0995\n",
      "177/223, train_loss: 0.1173, step time: 0.1037\n",
      "178/223, train_loss: 0.1194, step time: 0.0997\n",
      "179/223, train_loss: 0.1155, step time: 0.1004\n",
      "180/223, train_loss: 0.1048, step time: 0.1003\n",
      "181/223, train_loss: 0.1217, step time: 0.1107\n",
      "182/223, train_loss: 0.1083, step time: 0.1000\n",
      "183/223, train_loss: 0.1181, step time: 0.1051\n",
      "184/223, train_loss: 0.1092, step time: 0.1166\n",
      "185/223, train_loss: 0.1117, step time: 0.1004\n",
      "186/223, train_loss: 0.1102, step time: 0.1120\n",
      "187/223, train_loss: 0.1141, step time: 0.1135\n",
      "188/223, train_loss: 0.1157, step time: 0.1004\n",
      "189/223, train_loss: 0.1130, step time: 0.1178\n",
      "190/223, train_loss: 0.1052, step time: 0.1022\n",
      "191/223, train_loss: 0.1143, step time: 0.0997\n",
      "192/223, train_loss: 0.1177, step time: 0.0999\n",
      "193/223, train_loss: 0.1132, step time: 0.1016\n",
      "194/223, train_loss: 0.1072, step time: 0.1086\n",
      "195/223, train_loss: 0.1152, step time: 0.1058\n",
      "196/223, train_loss: 0.1183, step time: 0.1108\n",
      "197/223, train_loss: 0.1178, step time: 0.1312\n",
      "198/223, train_loss: 0.1328, step time: 0.1080\n",
      "199/223, train_loss: 0.1358, step time: 0.1287\n",
      "200/223, train_loss: 0.1097, step time: 0.1216\n",
      "201/223, train_loss: 0.1151, step time: 0.1000\n",
      "202/223, train_loss: 0.1137, step time: 0.1043\n",
      "203/223, train_loss: 0.1031, step time: 0.0993\n",
      "204/223, train_loss: 0.1050, step time: 0.1000\n",
      "205/223, train_loss: 0.1094, step time: 0.1121\n",
      "206/223, train_loss: 0.1071, step time: 0.1110\n",
      "207/223, train_loss: 0.1128, step time: 0.1070\n",
      "208/223, train_loss: 0.1043, step time: 0.1299\n",
      "209/223, train_loss: 0.1205, step time: 0.0993\n",
      "210/223, train_loss: 0.1210, step time: 0.1003\n",
      "211/223, train_loss: 0.1205, step time: 0.0993\n",
      "212/223, train_loss: 0.1004, step time: 0.1555\n",
      "213/223, train_loss: 0.1079, step time: 0.1004\n",
      "214/223, train_loss: 0.1240, step time: 0.1144\n",
      "215/223, train_loss: 0.1310, step time: 0.1033\n",
      "216/223, train_loss: 0.1054, step time: 0.1225\n",
      "217/223, train_loss: 0.0941, step time: 0.1045\n",
      "218/223, train_loss: 0.1237, step time: 0.0990\n",
      "219/223, train_loss: 0.1113, step time: 0.1000\n",
      "220/223, train_loss: 0.1120, step time: 0.1002\n",
      "221/223, train_loss: 0.1204, step time: 0.0995\n",
      "222/223, train_loss: 0.1107, step time: 0.0989\n",
      "223/223, train_loss: 0.1169, step time: 0.0994\n",
      "epoch 98 average loss: 0.1151\n",
      "time consuming of epoch 98 is: 86.4610\n",
      "----------\n",
      "epoch 99/300\n",
      "1/223, train_loss: 0.1204, step time: 0.1089\n",
      "2/223, train_loss: 0.1143, step time: 0.0999\n",
      "3/223, train_loss: 0.1224, step time: 0.0998\n",
      "4/223, train_loss: 0.1106, step time: 0.1184\n",
      "5/223, train_loss: 0.1156, step time: 0.1128\n",
      "6/223, train_loss: 0.1122, step time: 0.1171\n",
      "7/223, train_loss: 0.1252, step time: 0.1146\n",
      "8/223, train_loss: 0.1201, step time: 0.1138\n",
      "9/223, train_loss: 0.1086, step time: 0.1036\n",
      "10/223, train_loss: 0.1069, step time: 0.1071\n",
      "11/223, train_loss: 0.1181, step time: 0.1191\n",
      "12/223, train_loss: 0.1091, step time: 0.1111\n",
      "13/223, train_loss: 0.1225, step time: 0.1148\n",
      "14/223, train_loss: 0.1087, step time: 0.1129\n",
      "15/223, train_loss: 0.1124, step time: 0.1004\n",
      "16/223, train_loss: 0.1117, step time: 0.1334\n",
      "17/223, train_loss: 0.1113, step time: 0.1139\n",
      "18/223, train_loss: 0.1135, step time: 0.1107\n",
      "19/223, train_loss: 0.1080, step time: 0.1135\n",
      "20/223, train_loss: 0.1198, step time: 0.1141\n",
      "21/223, train_loss: 0.1109, step time: 0.1090\n",
      "22/223, train_loss: 0.1040, step time: 0.1106\n",
      "23/223, train_loss: 0.1228, step time: 0.0999\n",
      "24/223, train_loss: 0.1045, step time: 0.1100\n",
      "25/223, train_loss: 0.1152, step time: 0.1229\n",
      "26/223, train_loss: 0.1071, step time: 0.1202\n",
      "27/223, train_loss: 0.1186, step time: 0.1009\n",
      "28/223, train_loss: 0.1170, step time: 0.1000\n",
      "29/223, train_loss: 0.1177, step time: 0.1001\n",
      "30/223, train_loss: 0.1275, step time: 0.1007\n",
      "31/223, train_loss: 0.1100, step time: 0.1057\n",
      "32/223, train_loss: 0.1214, step time: 0.0995\n",
      "33/223, train_loss: 0.1221, step time: 0.1088\n",
      "34/223, train_loss: 0.1044, step time: 0.0999\n",
      "35/223, train_loss: 0.1070, step time: 0.1002\n",
      "36/223, train_loss: 0.1094, step time: 0.1225\n",
      "37/223, train_loss: 0.1062, step time: 0.1068\n",
      "38/223, train_loss: 0.1259, step time: 0.1136\n",
      "39/223, train_loss: 0.0987, step time: 0.1138\n",
      "40/223, train_loss: 0.1264, step time: 0.1132\n",
      "41/223, train_loss: 0.1116, step time: 0.1149\n",
      "42/223, train_loss: 0.1088, step time: 0.1055\n",
      "43/223, train_loss: 0.1147, step time: 0.1091\n",
      "44/223, train_loss: 0.1230, step time: 0.1013\n",
      "45/223, train_loss: 0.1089, step time: 0.1120\n",
      "46/223, train_loss: 0.1116, step time: 0.1266\n",
      "47/223, train_loss: 0.1060, step time: 0.1018\n",
      "48/223, train_loss: 0.1181, step time: 0.1010\n",
      "49/223, train_loss: 0.1059, step time: 0.1028\n",
      "50/223, train_loss: 0.1290, step time: 0.1064\n",
      "51/223, train_loss: 0.1114, step time: 0.1000\n",
      "52/223, train_loss: 0.1236, step time: 0.1146\n",
      "53/223, train_loss: 0.1281, step time: 0.1002\n",
      "54/223, train_loss: 0.1020, step time: 0.1209\n",
      "55/223, train_loss: 0.1033, step time: 0.1227\n",
      "56/223, train_loss: 0.1173, step time: 0.1126\n",
      "57/223, train_loss: 0.1189, step time: 0.1278\n",
      "58/223, train_loss: 0.1124, step time: 0.1011\n",
      "59/223, train_loss: 0.1089, step time: 0.0995\n",
      "60/223, train_loss: 0.1225, step time: 0.1003\n",
      "61/223, train_loss: 0.1225, step time: 0.1002\n",
      "62/223, train_loss: 0.1092, step time: 0.1101\n",
      "63/223, train_loss: 0.1049, step time: 0.1014\n",
      "64/223, train_loss: 0.1127, step time: 0.1000\n",
      "65/223, train_loss: 0.0951, step time: 0.0999\n",
      "66/223, train_loss: 0.1234, step time: 0.1067\n",
      "67/223, train_loss: 0.1086, step time: 0.0995\n",
      "68/223, train_loss: 0.1175, step time: 0.1013\n",
      "69/223, train_loss: 0.1393, step time: 0.1070\n",
      "70/223, train_loss: 0.1163, step time: 0.1077\n",
      "71/223, train_loss: 0.1182, step time: 0.1003\n",
      "72/223, train_loss: 0.1127, step time: 0.1008\n",
      "73/223, train_loss: 0.1140, step time: 0.1221\n",
      "74/223, train_loss: 0.1054, step time: 0.1099\n",
      "75/223, train_loss: 0.1067, step time: 0.0996\n",
      "76/223, train_loss: 0.1144, step time: 0.1005\n",
      "77/223, train_loss: 0.1137, step time: 0.1017\n",
      "78/223, train_loss: 0.1090, step time: 0.1117\n",
      "79/223, train_loss: 0.1068, step time: 0.1008\n",
      "80/223, train_loss: 0.1120, step time: 0.1093\n",
      "81/223, train_loss: 0.3107, step time: 0.1291\n",
      "82/223, train_loss: 0.1028, step time: 0.1072\n",
      "83/223, train_loss: 0.1137, step time: 0.1155\n",
      "84/223, train_loss: 0.1195, step time: 0.1277\n",
      "85/223, train_loss: 0.1178, step time: 0.1013\n",
      "86/223, train_loss: 0.1143, step time: 0.1155\n",
      "87/223, train_loss: 0.1165, step time: 0.1070\n",
      "88/223, train_loss: 0.1016, step time: 0.1156\n",
      "89/223, train_loss: 0.1104, step time: 0.0995\n",
      "90/223, train_loss: 0.1205, step time: 0.1056\n",
      "91/223, train_loss: 0.1048, step time: 0.1158\n",
      "92/223, train_loss: 0.1170, step time: 0.1154\n",
      "93/223, train_loss: 0.1134, step time: 0.1059\n",
      "94/223, train_loss: 0.1231, step time: 0.1113\n",
      "95/223, train_loss: 0.1101, step time: 0.1091\n",
      "96/223, train_loss: 0.1183, step time: 0.1073\n",
      "97/223, train_loss: 0.1155, step time: 0.1077\n",
      "98/223, train_loss: 0.1089, step time: 0.1224\n",
      "99/223, train_loss: 0.1248, step time: 0.1130\n",
      "100/223, train_loss: 0.1221, step time: 0.1206\n",
      "101/223, train_loss: 0.1210, step time: 0.1153\n",
      "102/223, train_loss: 0.1262, step time: 0.1013\n",
      "103/223, train_loss: 0.1260, step time: 0.1160\n",
      "104/223, train_loss: 0.1080, step time: 0.1149\n",
      "105/223, train_loss: 0.1107, step time: 0.1238\n",
      "106/223, train_loss: 0.1223, step time: 0.1158\n",
      "107/223, train_loss: 0.1110, step time: 0.1086\n",
      "108/223, train_loss: 0.1014, step time: 0.1061\n",
      "109/223, train_loss: 0.1090, step time: 0.1012\n",
      "110/223, train_loss: 0.1161, step time: 0.1058\n",
      "111/223, train_loss: 0.1111, step time: 0.1117\n",
      "112/223, train_loss: 0.1091, step time: 0.1398\n",
      "113/223, train_loss: 0.0972, step time: 0.1069\n",
      "114/223, train_loss: 0.1092, step time: 0.1200\n",
      "115/223, train_loss: 0.1225, step time: 0.1418\n",
      "116/223, train_loss: 0.1244, step time: 0.1131\n",
      "117/223, train_loss: 0.1235, step time: 0.1006\n",
      "118/223, train_loss: 0.1012, step time: 0.1060\n",
      "119/223, train_loss: 0.1337, step time: 0.1143\n",
      "120/223, train_loss: 0.1085, step time: 0.1068\n",
      "121/223, train_loss: 0.1178, step time: 0.1002\n",
      "122/223, train_loss: 0.1050, step time: 0.1038\n",
      "123/223, train_loss: 0.1141, step time: 0.1003\n",
      "124/223, train_loss: 0.1117, step time: 0.0994\n",
      "125/223, train_loss: 0.1065, step time: 0.1000\n",
      "126/223, train_loss: 0.1061, step time: 0.1129\n",
      "127/223, train_loss: 0.1155, step time: 0.1005\n",
      "128/223, train_loss: 0.1171, step time: 0.1011\n",
      "129/223, train_loss: 0.1121, step time: 0.1167\n",
      "130/223, train_loss: 0.1082, step time: 0.1070\n",
      "131/223, train_loss: 0.1048, step time: 0.1021\n",
      "132/223, train_loss: 0.1092, step time: 0.1112\n",
      "133/223, train_loss: 0.1200, step time: 0.1132\n",
      "134/223, train_loss: 0.1123, step time: 0.1002\n",
      "135/223, train_loss: 0.1144, step time: 0.1021\n",
      "136/223, train_loss: 0.1075, step time: 0.1179\n",
      "137/223, train_loss: 0.1096, step time: 0.1041\n",
      "138/223, train_loss: 0.1143, step time: 0.0992\n",
      "139/223, train_loss: 0.1246, step time: 0.0993\n",
      "140/223, train_loss: 0.1241, step time: 0.0994\n",
      "141/223, train_loss: 0.1171, step time: 0.1002\n",
      "142/223, train_loss: 0.1137, step time: 0.1307\n",
      "143/223, train_loss: 0.1068, step time: 0.1009\n",
      "144/223, train_loss: 0.1207, step time: 0.1259\n",
      "145/223, train_loss: 0.1189, step time: 0.1086\n",
      "146/223, train_loss: 0.1231, step time: 0.1218\n",
      "147/223, train_loss: 0.1155, step time: 0.1192\n",
      "148/223, train_loss: 0.1058, step time: 0.1010\n",
      "149/223, train_loss: 0.0992, step time: 0.0997\n",
      "150/223, train_loss: 0.1190, step time: 0.1116\n",
      "151/223, train_loss: 0.1148, step time: 0.1252\n",
      "152/223, train_loss: 0.1161, step time: 0.1133\n",
      "153/223, train_loss: 0.1034, step time: 0.1218\n",
      "154/223, train_loss: 0.1150, step time: 0.1125\n",
      "155/223, train_loss: 0.1224, step time: 0.1312\n",
      "156/223, train_loss: 0.1234, step time: 0.1158\n",
      "157/223, train_loss: 0.1123, step time: 0.1137\n",
      "158/223, train_loss: 0.1155, step time: 0.1080\n",
      "159/223, train_loss: 0.1030, step time: 0.1063\n",
      "160/223, train_loss: 0.1204, step time: 0.1053\n",
      "161/223, train_loss: 0.1189, step time: 0.1165\n",
      "162/223, train_loss: 0.1121, step time: 0.1129\n",
      "163/223, train_loss: 0.1110, step time: 0.1168\n",
      "164/223, train_loss: 0.1284, step time: 0.1074\n",
      "165/223, train_loss: 0.1281, step time: 0.1082\n",
      "166/223, train_loss: 0.1013, step time: 0.1180\n",
      "167/223, train_loss: 0.1160, step time: 0.1133\n",
      "168/223, train_loss: 0.1122, step time: 0.1123\n",
      "169/223, train_loss: 0.1300, step time: 0.1005\n",
      "170/223, train_loss: 0.1086, step time: 0.1004\n",
      "171/223, train_loss: 0.1098, step time: 0.0994\n",
      "172/223, train_loss: 0.1217, step time: 0.1001\n",
      "173/223, train_loss: 0.1058, step time: 0.1072\n",
      "174/223, train_loss: 0.1253, step time: 0.1086\n",
      "175/223, train_loss: 0.1102, step time: 0.1063\n",
      "176/223, train_loss: 0.1067, step time: 0.1162\n",
      "177/223, train_loss: 0.1022, step time: 0.1068\n",
      "178/223, train_loss: 0.1033, step time: 0.1057\n",
      "179/223, train_loss: 0.1247, step time: 0.1174\n",
      "180/223, train_loss: 0.1090, step time: 0.1016\n",
      "181/223, train_loss: 0.1117, step time: 0.1081\n",
      "182/223, train_loss: 0.1067, step time: 0.1103\n",
      "183/223, train_loss: 0.1104, step time: 0.1004\n",
      "184/223, train_loss: 0.1002, step time: 0.1226\n",
      "185/223, train_loss: 0.1142, step time: 0.1007\n",
      "186/223, train_loss: 0.1167, step time: 0.1071\n",
      "187/223, train_loss: 0.1238, step time: 0.1112\n",
      "188/223, train_loss: 0.1148, step time: 0.1053\n",
      "189/223, train_loss: 0.1030, step time: 0.0997\n",
      "190/223, train_loss: 0.1134, step time: 0.1411\n",
      "191/223, train_loss: 0.1473, step time: 0.1196\n",
      "192/223, train_loss: 0.1106, step time: 0.1059\n",
      "193/223, train_loss: 0.1141, step time: 0.1003\n",
      "194/223, train_loss: 0.1051, step time: 0.1230\n",
      "195/223, train_loss: 0.1127, step time: 0.1217\n",
      "196/223, train_loss: 0.1092, step time: 0.1003\n",
      "197/223, train_loss: 0.1099, step time: 0.1008\n",
      "198/223, train_loss: 0.1156, step time: 0.1105\n",
      "199/223, train_loss: 0.1156, step time: 0.1001\n",
      "200/223, train_loss: 0.1050, step time: 0.1012\n",
      "201/223, train_loss: 0.1031, step time: 0.1138\n",
      "202/223, train_loss: 0.1172, step time: 0.1099\n",
      "203/223, train_loss: 0.1146, step time: 0.1000\n",
      "204/223, train_loss: 0.1205, step time: 0.1372\n",
      "205/223, train_loss: 0.1216, step time: 0.1099\n",
      "206/223, train_loss: 0.1196, step time: 0.1091\n",
      "207/223, train_loss: 0.1077, step time: 0.0994\n",
      "208/223, train_loss: 0.1158, step time: 0.1005\n",
      "209/223, train_loss: 0.1179, step time: 0.1029\n",
      "210/223, train_loss: 0.1058, step time: 0.1502\n",
      "211/223, train_loss: 0.1247, step time: 0.0996\n",
      "212/223, train_loss: 0.1065, step time: 0.0999\n",
      "213/223, train_loss: 0.1226, step time: 0.1030\n",
      "214/223, train_loss: 0.1145, step time: 0.1140\n",
      "215/223, train_loss: 0.1213, step time: 0.1001\n",
      "216/223, train_loss: 0.1110, step time: 0.1012\n",
      "217/223, train_loss: 0.1143, step time: 0.1048\n",
      "218/223, train_loss: 0.1336, step time: 0.1000\n",
      "219/223, train_loss: 0.1131, step time: 0.1003\n",
      "220/223, train_loss: 0.1100, step time: 0.1003\n",
      "221/223, train_loss: 0.1075, step time: 0.1000\n",
      "222/223, train_loss: 0.1155, step time: 0.0992\n",
      "223/223, train_loss: 0.1196, step time: 0.1003\n",
      "epoch 99 average loss: 0.1150\n",
      "time consuming of epoch 99 is: 87.2460\n",
      "----------\n",
      "epoch 100/300\n",
      "1/223, train_loss: 0.1130, step time: 0.1164\n",
      "2/223, train_loss: 0.1099, step time: 0.1064\n",
      "3/223, train_loss: 0.1080, step time: 0.1097\n",
      "4/223, train_loss: 0.1253, step time: 0.1130\n",
      "5/223, train_loss: 0.1143, step time: 0.0993\n",
      "6/223, train_loss: 0.1147, step time: 0.1005\n",
      "7/223, train_loss: 0.1041, step time: 0.1094\n",
      "8/223, train_loss: 0.1174, step time: 0.1209\n",
      "9/223, train_loss: 0.1064, step time: 0.1046\n",
      "10/223, train_loss: 0.1287, step time: 0.1171\n",
      "11/223, train_loss: 0.1081, step time: 0.1091\n",
      "12/223, train_loss: 0.1087, step time: 0.1322\n",
      "13/223, train_loss: 0.1261, step time: 0.1103\n",
      "14/223, train_loss: 0.1114, step time: 0.1007\n",
      "15/223, train_loss: 0.1088, step time: 0.1452\n",
      "16/223, train_loss: 0.1252, step time: 0.1229\n",
      "17/223, train_loss: 0.1026, step time: 0.1033\n",
      "18/223, train_loss: 0.1106, step time: 0.1006\n",
      "19/223, train_loss: 0.1082, step time: 0.1005\n",
      "20/223, train_loss: 0.1122, step time: 0.1042\n",
      "21/223, train_loss: 0.1262, step time: 0.1131\n",
      "22/223, train_loss: 0.1236, step time: 0.1048\n",
      "23/223, train_loss: 0.1110, step time: 0.1087\n",
      "24/223, train_loss: 0.1014, step time: 0.0999\n",
      "25/223, train_loss: 0.1227, step time: 0.1212\n",
      "26/223, train_loss: 0.1126, step time: 0.1193\n",
      "27/223, train_loss: 0.1106, step time: 0.1168\n",
      "28/223, train_loss: 0.1150, step time: 0.1112\n",
      "29/223, train_loss: 0.1320, step time: 0.1089\n",
      "30/223, train_loss: 0.1059, step time: 0.1152\n",
      "31/223, train_loss: 0.1120, step time: 0.1065\n",
      "32/223, train_loss: 0.1050, step time: 0.1174\n",
      "33/223, train_loss: 0.1181, step time: 0.1020\n",
      "34/223, train_loss: 0.1089, step time: 0.1192\n",
      "35/223, train_loss: 0.1232, step time: 0.1092\n",
      "36/223, train_loss: 0.1126, step time: 0.1121\n",
      "37/223, train_loss: 0.1076, step time: 0.1013\n",
      "38/223, train_loss: 0.1100, step time: 0.0999\n",
      "39/223, train_loss: 0.1233, step time: 0.1017\n",
      "40/223, train_loss: 0.1104, step time: 0.1054\n",
      "41/223, train_loss: 0.1103, step time: 0.1000\n",
      "42/223, train_loss: 0.1105, step time: 0.1255\n",
      "43/223, train_loss: 0.0983, step time: 0.1224\n",
      "44/223, train_loss: 0.1141, step time: 0.1101\n",
      "45/223, train_loss: 0.1388, step time: 0.1107\n",
      "46/223, train_loss: 0.0999, step time: 0.1097\n",
      "47/223, train_loss: 0.1084, step time: 0.1057\n",
      "48/223, train_loss: 0.1181, step time: 0.1193\n",
      "49/223, train_loss: 0.1237, step time: 0.1145\n",
      "50/223, train_loss: 0.1316, step time: 0.0994\n",
      "51/223, train_loss: 0.1189, step time: 0.1177\n",
      "52/223, train_loss: 0.1118, step time: 0.1047\n",
      "53/223, train_loss: 0.1074, step time: 0.1129\n",
      "54/223, train_loss: 0.1129, step time: 0.1107\n",
      "55/223, train_loss: 0.1076, step time: 0.1044\n",
      "56/223, train_loss: 0.1077, step time: 0.1077\n",
      "57/223, train_loss: 0.1043, step time: 0.1082\n",
      "58/223, train_loss: 0.1060, step time: 0.1075\n",
      "59/223, train_loss: 0.1061, step time: 0.1101\n",
      "60/223, train_loss: 0.1149, step time: 0.0991\n",
      "61/223, train_loss: 0.1173, step time: 0.1003\n",
      "62/223, train_loss: 0.1130, step time: 0.0997\n",
      "63/223, train_loss: 0.1213, step time: 0.1005\n",
      "64/223, train_loss: 0.1068, step time: 0.1142\n",
      "65/223, train_loss: 0.1114, step time: 0.1000\n",
      "66/223, train_loss: 0.1163, step time: 0.1106\n",
      "67/223, train_loss: 0.1255, step time: 0.1045\n",
      "68/223, train_loss: 0.1149, step time: 0.1016\n",
      "69/223, train_loss: 0.1188, step time: 0.1147\n",
      "70/223, train_loss: 0.1050, step time: 0.0999\n",
      "71/223, train_loss: 0.1168, step time: 0.1006\n",
      "72/223, train_loss: 0.1113, step time: 0.1004\n",
      "73/223, train_loss: 0.1074, step time: 0.1033\n",
      "74/223, train_loss: 0.1129, step time: 0.1008\n",
      "75/223, train_loss: 0.1169, step time: 0.1013\n",
      "76/223, train_loss: 0.1179, step time: 0.1001\n",
      "77/223, train_loss: 0.1306, step time: 0.1005\n",
      "78/223, train_loss: 0.1200, step time: 0.1008\n",
      "79/223, train_loss: 0.1182, step time: 0.1006\n",
      "80/223, train_loss: 0.1148, step time: 0.1006\n",
      "81/223, train_loss: 0.1111, step time: 0.1194\n",
      "82/223, train_loss: 0.1208, step time: 0.1006\n",
      "83/223, train_loss: 0.1086, step time: 0.1378\n",
      "84/223, train_loss: 0.1080, step time: 0.1089\n",
      "85/223, train_loss: 0.1114, step time: 0.1085\n",
      "86/223, train_loss: 0.1014, step time: 0.1008\n",
      "87/223, train_loss: 0.1045, step time: 0.1009\n",
      "88/223, train_loss: 0.1190, step time: 0.1104\n",
      "89/223, train_loss: 0.1030, step time: 0.1132\n",
      "90/223, train_loss: 0.1035, step time: 0.1148\n",
      "91/223, train_loss: 0.1165, step time: 0.1159\n",
      "92/223, train_loss: 0.1185, step time: 0.0999\n",
      "93/223, train_loss: 0.1135, step time: 0.1183\n",
      "94/223, train_loss: 0.1102, step time: 0.1095\n",
      "95/223, train_loss: 0.1177, step time: 0.1216\n",
      "96/223, train_loss: 0.1253, step time: 0.1005\n",
      "97/223, train_loss: 0.1140, step time: 0.1056\n",
      "98/223, train_loss: 0.1112, step time: 0.1341\n",
      "99/223, train_loss: 0.1136, step time: 0.1455\n",
      "100/223, train_loss: 0.1271, step time: 0.1149\n",
      "101/223, train_loss: 0.1167, step time: 0.1274\n",
      "102/223, train_loss: 0.1152, step time: 0.1068\n",
      "103/223, train_loss: 0.1139, step time: 0.1007\n",
      "104/223, train_loss: 0.1129, step time: 0.1067\n",
      "105/223, train_loss: 0.1108, step time: 0.1265\n",
      "106/223, train_loss: 0.1131, step time: 0.1303\n",
      "107/223, train_loss: 0.1023, step time: 0.1046\n",
      "108/223, train_loss: 0.1115, step time: 0.1100\n",
      "109/223, train_loss: 0.1145, step time: 0.1089\n",
      "110/223, train_loss: 0.1067, step time: 0.1113\n",
      "111/223, train_loss: 0.1260, step time: 0.1005\n",
      "112/223, train_loss: 0.1171, step time: 0.1332\n",
      "113/223, train_loss: 0.1138, step time: 0.1052\n",
      "114/223, train_loss: 0.1205, step time: 0.1012\n",
      "115/223, train_loss: 0.1043, step time: 0.1088\n",
      "116/223, train_loss: 0.1027, step time: 0.1130\n",
      "117/223, train_loss: 0.1262, step time: 0.1145\n",
      "118/223, train_loss: 0.1158, step time: 0.1122\n",
      "119/223, train_loss: 0.1313, step time: 0.1141\n",
      "120/223, train_loss: 0.1098, step time: 0.1132\n",
      "121/223, train_loss: 0.1131, step time: 0.1134\n",
      "122/223, train_loss: 0.1222, step time: 0.1140\n",
      "123/223, train_loss: 0.1144, step time: 0.1083\n",
      "124/223, train_loss: 0.1128, step time: 0.1083\n",
      "125/223, train_loss: 0.1076, step time: 0.1170\n",
      "126/223, train_loss: 0.1169, step time: 0.1391\n",
      "127/223, train_loss: 0.1149, step time: 0.0994\n",
      "128/223, train_loss: 0.0987, step time: 0.1111\n",
      "129/223, train_loss: 0.1129, step time: 0.1503\n",
      "130/223, train_loss: 0.1025, step time: 0.1182\n",
      "131/223, train_loss: 0.1271, step time: 0.1135\n",
      "132/223, train_loss: 0.1236, step time: 0.1102\n",
      "133/223, train_loss: 0.1159, step time: 0.1081\n",
      "134/223, train_loss: 0.1110, step time: 0.1149\n",
      "135/223, train_loss: 0.1198, step time: 0.1280\n",
      "136/223, train_loss: 0.0997, step time: 0.1564\n",
      "137/223, train_loss: 0.1246, step time: 0.1179\n",
      "138/223, train_loss: 0.1245, step time: 0.1201\n",
      "139/223, train_loss: 0.1077, step time: 0.1193\n",
      "140/223, train_loss: 0.1060, step time: 0.1246\n",
      "141/223, train_loss: 0.1086, step time: 0.1179\n",
      "142/223, train_loss: 0.1078, step time: 0.1101\n",
      "143/223, train_loss: 0.1118, step time: 0.1293\n",
      "144/223, train_loss: 0.1094, step time: 0.1117\n",
      "145/223, train_loss: 0.1073, step time: 0.1136\n",
      "146/223, train_loss: 0.1109, step time: 0.1143\n",
      "147/223, train_loss: 0.1043, step time: 0.1129\n",
      "148/223, train_loss: 0.1217, step time: 0.1305\n",
      "149/223, train_loss: 0.1078, step time: 0.0992\n",
      "150/223, train_loss: 0.1092, step time: 0.0991\n",
      "151/223, train_loss: 0.1032, step time: 0.0985\n",
      "152/223, train_loss: 0.1045, step time: 0.1311\n",
      "153/223, train_loss: 0.1221, step time: 0.1360\n",
      "154/223, train_loss: 0.1203, step time: 0.1010\n",
      "155/223, train_loss: 0.1219, step time: 0.1074\n",
      "156/223, train_loss: 0.1121, step time: 0.0993\n",
      "157/223, train_loss: 0.1015, step time: 0.0992\n",
      "158/223, train_loss: 0.1115, step time: 0.0985\n",
      "159/223, train_loss: 0.1083, step time: 0.1100\n",
      "160/223, train_loss: 0.1112, step time: 0.1318\n",
      "161/223, train_loss: 0.1068, step time: 0.1006\n",
      "162/223, train_loss: 0.1061, step time: 0.0999\n",
      "163/223, train_loss: 0.1182, step time: 0.1065\n",
      "164/223, train_loss: 0.1101, step time: 0.1164\n",
      "165/223, train_loss: 0.1104, step time: 0.1033\n",
      "166/223, train_loss: 0.1102, step time: 0.1074\n",
      "167/223, train_loss: 0.1151, step time: 0.1001\n",
      "168/223, train_loss: 0.1207, step time: 0.1411\n",
      "169/223, train_loss: 0.1169, step time: 0.1117\n",
      "170/223, train_loss: 0.1232, step time: 0.1168\n",
      "171/223, train_loss: 0.1182, step time: 0.1011\n",
      "172/223, train_loss: 0.1111, step time: 0.1092\n",
      "173/223, train_loss: 0.1181, step time: 0.1343\n",
      "174/223, train_loss: 0.1034, step time: 0.1085\n",
      "175/223, train_loss: 0.1126, step time: 0.1072\n",
      "176/223, train_loss: 0.1124, step time: 0.1230\n",
      "177/223, train_loss: 0.1045, step time: 0.1113\n",
      "178/223, train_loss: 0.3150, step time: 0.1087\n",
      "179/223, train_loss: 0.1078, step time: 0.1020\n",
      "180/223, train_loss: 0.1153, step time: 0.1142\n",
      "181/223, train_loss: 0.1142, step time: 0.1002\n",
      "182/223, train_loss: 0.1168, step time: 0.1014\n",
      "183/223, train_loss: 0.1193, step time: 0.1128\n",
      "184/223, train_loss: 0.1178, step time: 0.1072\n",
      "185/223, train_loss: 0.1078, step time: 0.1132\n",
      "186/223, train_loss: 0.1258, step time: 0.1110\n",
      "187/223, train_loss: 0.1140, step time: 0.0994\n",
      "188/223, train_loss: 0.1279, step time: 0.1250\n",
      "189/223, train_loss: 0.1222, step time: 0.1158\n",
      "190/223, train_loss: 0.0998, step time: 0.0998\n",
      "191/223, train_loss: 0.1170, step time: 0.1001\n",
      "192/223, train_loss: 0.1268, step time: 0.1027\n",
      "193/223, train_loss: 0.1257, step time: 0.1017\n",
      "194/223, train_loss: 0.1140, step time: 0.1014\n",
      "195/223, train_loss: 0.1209, step time: 0.1033\n",
      "196/223, train_loss: 0.1090, step time: 0.1170\n",
      "197/223, train_loss: 0.1307, step time: 0.1369\n",
      "198/223, train_loss: 0.1285, step time: 0.1000\n",
      "199/223, train_loss: 0.1223, step time: 0.1001\n",
      "200/223, train_loss: 0.1120, step time: 0.1193\n",
      "201/223, train_loss: 0.1321, step time: 0.1264\n",
      "202/223, train_loss: 0.1021, step time: 0.1073\n",
      "203/223, train_loss: 0.1086, step time: 0.1336\n",
      "204/223, train_loss: 0.1024, step time: 0.1109\n",
      "205/223, train_loss: 0.1129, step time: 0.1147\n",
      "206/223, train_loss: 0.1201, step time: 0.1011\n",
      "207/223, train_loss: 0.1215, step time: 0.1087\n",
      "208/223, train_loss: 0.1254, step time: 0.1133\n",
      "209/223, train_loss: 0.1139, step time: 0.1215\n",
      "210/223, train_loss: 0.1102, step time: 0.1301\n",
      "211/223, train_loss: 0.1084, step time: 0.1161\n",
      "212/223, train_loss: 0.1252, step time: 0.1306\n",
      "213/223, train_loss: 0.1158, step time: 0.1313\n",
      "214/223, train_loss: 0.1257, step time: 0.1163\n",
      "215/223, train_loss: 0.1309, step time: 0.1159\n",
      "216/223, train_loss: 0.1315, step time: 0.1001\n",
      "217/223, train_loss: 0.1193, step time: 0.1001\n",
      "218/223, train_loss: 0.1076, step time: 0.1001\n",
      "219/223, train_loss: 0.1084, step time: 0.1016\n",
      "220/223, train_loss: 0.1092, step time: 0.1002\n",
      "221/223, train_loss: 0.1214, step time: 0.0998\n",
      "222/223, train_loss: 0.1233, step time: 0.1007\n",
      "223/223, train_loss: 0.1118, step time: 0.1003\n",
      "epoch 100 average loss: 0.1151\n",
      "saved new best metric model\n",
      "current epoch: 100 current mean dice: 0.8507 tc: 0.9176 wt: 0.8617 et: 0.7728\n",
      "best mean dice: 0.8507 at epoch: 100\n",
      "time consuming of epoch 100 is: 89.0604\n",
      "----------\n",
      "epoch 101/300\n",
      "1/223, train_loss: 0.1141, step time: 0.1011\n",
      "2/223, train_loss: 0.1089, step time: 0.1002\n",
      "3/223, train_loss: 0.1160, step time: 0.1005\n",
      "4/223, train_loss: 0.1116, step time: 0.1017\n",
      "5/223, train_loss: 0.1138, step time: 0.1079\n",
      "6/223, train_loss: 0.1144, step time: 0.1004\n",
      "7/223, train_loss: 0.1215, step time: 0.1256\n",
      "8/223, train_loss: 0.1143, step time: 0.1154\n",
      "9/223, train_loss: 0.1074, step time: 0.1108\n",
      "10/223, train_loss: 0.1283, step time: 0.1006\n",
      "11/223, train_loss: 0.1284, step time: 0.1099\n",
      "12/223, train_loss: 0.1078, step time: 0.0997\n",
      "13/223, train_loss: 0.1163, step time: 0.1209\n",
      "14/223, train_loss: 0.1137, step time: 0.1325\n",
      "15/223, train_loss: 0.1268, step time: 0.1180\n",
      "16/223, train_loss: 0.1287, step time: 0.1103\n",
      "17/223, train_loss: 0.1033, step time: 0.1141\n",
      "18/223, train_loss: 0.1167, step time: 0.1285\n",
      "19/223, train_loss: 0.1082, step time: 0.1003\n",
      "20/223, train_loss: 0.1215, step time: 0.0997\n",
      "21/223, train_loss: 0.1133, step time: 0.1000\n",
      "22/223, train_loss: 0.1179, step time: 0.1006\n",
      "23/223, train_loss: 0.1053, step time: 0.1033\n",
      "24/223, train_loss: 0.1261, step time: 0.1017\n",
      "25/223, train_loss: 0.1012, step time: 0.1269\n",
      "26/223, train_loss: 0.1194, step time: 0.1165\n",
      "27/223, train_loss: 0.1128, step time: 0.1197\n",
      "28/223, train_loss: 0.1205, step time: 0.0997\n",
      "29/223, train_loss: 0.1133, step time: 0.1092\n",
      "30/223, train_loss: 0.1103, step time: 0.1154\n",
      "31/223, train_loss: 0.1272, step time: 0.0994\n",
      "32/223, train_loss: 0.1108, step time: 0.1053\n",
      "33/223, train_loss: 0.1225, step time: 0.0998\n",
      "34/223, train_loss: 0.1109, step time: 0.1094\n",
      "35/223, train_loss: 0.1200, step time: 0.1224\n",
      "36/223, train_loss: 0.1151, step time: 0.1161\n",
      "37/223, train_loss: 0.1085, step time: 0.1087\n",
      "38/223, train_loss: 0.1058, step time: 0.1222\n",
      "39/223, train_loss: 0.1086, step time: 0.1147\n",
      "40/223, train_loss: 0.1216, step time: 0.1020\n",
      "41/223, train_loss: 0.1090, step time: 0.1091\n",
      "42/223, train_loss: 0.1000, step time: 0.1021\n",
      "43/223, train_loss: 0.1086, step time: 0.1127\n",
      "44/223, train_loss: 0.1230, step time: 0.1044\n",
      "45/223, train_loss: 0.1099, step time: 0.1087\n",
      "46/223, train_loss: 0.1154, step time: 0.1015\n",
      "47/223, train_loss: 0.1179, step time: 0.1057\n",
      "48/223, train_loss: 0.1089, step time: 0.1175\n",
      "49/223, train_loss: 0.1169, step time: 0.1164\n",
      "50/223, train_loss: 0.1076, step time: 0.1481\n",
      "51/223, train_loss: 0.1104, step time: 0.1201\n",
      "52/223, train_loss: 0.1062, step time: 0.1176\n",
      "53/223, train_loss: 0.1403, step time: 0.1101\n",
      "54/223, train_loss: 0.1130, step time: 0.1017\n",
      "55/223, train_loss: 0.1026, step time: 0.0999\n",
      "56/223, train_loss: 0.1063, step time: 0.1227\n",
      "57/223, train_loss: 0.1203, step time: 0.1013\n",
      "58/223, train_loss: 0.1205, step time: 0.1086\n",
      "59/223, train_loss: 0.1187, step time: 0.1017\n",
      "60/223, train_loss: 0.1191, step time: 0.1008\n",
      "61/223, train_loss: 0.1199, step time: 0.0999\n",
      "62/223, train_loss: 0.1127, step time: 0.1006\n",
      "63/223, train_loss: 0.1221, step time: 0.1003\n",
      "64/223, train_loss: 0.1145, step time: 0.1356\n",
      "65/223, train_loss: 0.1162, step time: 0.1028\n",
      "66/223, train_loss: 0.1125, step time: 0.1048\n",
      "67/223, train_loss: 0.1150, step time: 0.1115\n",
      "68/223, train_loss: 0.1113, step time: 0.1024\n",
      "69/223, train_loss: 0.1257, step time: 0.1216\n",
      "70/223, train_loss: 0.1072, step time: 0.0997\n",
      "71/223, train_loss: 0.1059, step time: 0.1000\n",
      "72/223, train_loss: 0.1202, step time: 0.1078\n",
      "73/223, train_loss: 0.1056, step time: 0.1068\n",
      "74/223, train_loss: 0.1249, step time: 0.0997\n",
      "75/223, train_loss: 0.1133, step time: 0.1005\n",
      "76/223, train_loss: 0.1048, step time: 0.1131\n",
      "77/223, train_loss: 0.1126, step time: 0.1090\n",
      "78/223, train_loss: 0.1256, step time: 0.1163\n",
      "79/223, train_loss: 0.1209, step time: 0.1266\n",
      "80/223, train_loss: 0.1199, step time: 0.1002\n",
      "81/223, train_loss: 0.1169, step time: 0.1145\n",
      "82/223, train_loss: 0.1052, step time: 0.1012\n",
      "83/223, train_loss: 0.1023, step time: 0.1138\n",
      "84/223, train_loss: 0.1102, step time: 0.1009\n",
      "85/223, train_loss: 0.1210, step time: 0.1002\n",
      "86/223, train_loss: 0.1175, step time: 0.1003\n",
      "87/223, train_loss: 0.1064, step time: 0.1009\n",
      "88/223, train_loss: 0.1153, step time: 0.0998\n",
      "89/223, train_loss: 0.1175, step time: 0.1059\n",
      "90/223, train_loss: 0.1241, step time: 0.1179\n",
      "91/223, train_loss: 0.1040, step time: 0.1002\n",
      "92/223, train_loss: 0.1200, step time: 0.1004\n",
      "93/223, train_loss: 0.1072, step time: 0.1065\n",
      "94/223, train_loss: 0.1123, step time: 0.1011\n",
      "95/223, train_loss: 0.1049, step time: 0.1557\n",
      "96/223, train_loss: 0.1222, step time: 0.1106\n",
      "97/223, train_loss: 0.1207, step time: 0.1038\n",
      "98/223, train_loss: 0.1104, step time: 0.0996\n",
      "99/223, train_loss: 0.1226, step time: 0.0997\n",
      "100/223, train_loss: 0.1155, step time: 0.1100\n",
      "101/223, train_loss: 0.1223, step time: 0.1020\n",
      "102/223, train_loss: 0.1026, step time: 0.1006\n",
      "103/223, train_loss: 0.1121, step time: 0.1157\n",
      "104/223, train_loss: 0.1132, step time: 0.1006\n",
      "105/223, train_loss: 0.1242, step time: 0.1071\n",
      "106/223, train_loss: 0.1126, step time: 0.0999\n",
      "107/223, train_loss: 0.1175, step time: 0.1049\n",
      "108/223, train_loss: 0.1216, step time: 0.1228\n",
      "109/223, train_loss: 0.1062, step time: 0.1359\n",
      "110/223, train_loss: 0.1150, step time: 0.1216\n",
      "111/223, train_loss: 0.1141, step time: 0.1005\n",
      "112/223, train_loss: 0.1072, step time: 0.1208\n",
      "113/223, train_loss: 0.1217, step time: 0.1065\n",
      "114/223, train_loss: 0.1173, step time: 0.1111\n",
      "115/223, train_loss: 0.1098, step time: 0.1209\n",
      "116/223, train_loss: 0.1132, step time: 0.1122\n",
      "117/223, train_loss: 0.1128, step time: 0.1235\n",
      "118/223, train_loss: 0.1163, step time: 0.1321\n",
      "119/223, train_loss: 0.1127, step time: 0.1007\n",
      "120/223, train_loss: 0.1081, step time: 0.1009\n",
      "121/223, train_loss: 0.1075, step time: 0.1070\n",
      "122/223, train_loss: 0.3140, step time: 0.1028\n",
      "123/223, train_loss: 0.1175, step time: 0.1004\n",
      "124/223, train_loss: 0.1093, step time: 0.1002\n",
      "125/223, train_loss: 0.1166, step time: 0.1245\n",
      "126/223, train_loss: 0.1234, step time: 0.1204\n",
      "127/223, train_loss: 0.1126, step time: 0.1009\n",
      "128/223, train_loss: 0.1061, step time: 0.1005\n",
      "129/223, train_loss: 0.1023, step time: 0.1101\n",
      "130/223, train_loss: 0.1134, step time: 0.1012\n",
      "131/223, train_loss: 0.1189, step time: 0.1000\n",
      "132/223, train_loss: 0.1153, step time: 0.1009\n",
      "133/223, train_loss: 0.1062, step time: 0.1197\n",
      "134/223, train_loss: 0.1160, step time: 0.1105\n",
      "135/223, train_loss: 0.1206, step time: 0.1027\n",
      "136/223, train_loss: 0.1172, step time: 0.1059\n",
      "137/223, train_loss: 0.1141, step time: 0.1174\n",
      "138/223, train_loss: 0.1294, step time: 0.1072\n",
      "139/223, train_loss: 0.1131, step time: 0.1097\n",
      "140/223, train_loss: 0.1148, step time: 0.1007\n",
      "141/223, train_loss: 0.1217, step time: 0.1164\n",
      "142/223, train_loss: 0.1141, step time: 0.1186\n",
      "143/223, train_loss: 0.1009, step time: 0.1027\n",
      "144/223, train_loss: 0.1072, step time: 0.1007\n",
      "145/223, train_loss: 0.1262, step time: 0.0999\n",
      "146/223, train_loss: 0.1121, step time: 0.0999\n",
      "147/223, train_loss: 0.1088, step time: 0.0995\n",
      "148/223, train_loss: 0.1029, step time: 0.1145\n",
      "149/223, train_loss: 0.1180, step time: 0.1016\n",
      "150/223, train_loss: 0.1135, step time: 0.1102\n",
      "151/223, train_loss: 0.1319, step time: 0.1009\n",
      "152/223, train_loss: 0.1045, step time: 0.1081\n",
      "153/223, train_loss: 0.1212, step time: 0.1171\n",
      "154/223, train_loss: 0.1143, step time: 0.0995\n",
      "155/223, train_loss: 0.1239, step time: 0.1037\n",
      "156/223, train_loss: 0.1273, step time: 0.1090\n",
      "157/223, train_loss: 0.1159, step time: 0.1009\n",
      "158/223, train_loss: 0.1055, step time: 0.1113\n",
      "159/223, train_loss: 0.1136, step time: 0.1249\n",
      "160/223, train_loss: 0.1194, step time: 0.1097\n",
      "161/223, train_loss: 0.1239, step time: 0.1005\n",
      "162/223, train_loss: 0.1236, step time: 0.1105\n",
      "163/223, train_loss: 0.1128, step time: 0.1249\n",
      "164/223, train_loss: 0.1238, step time: 0.1068\n",
      "165/223, train_loss: 0.1086, step time: 0.1457\n",
      "166/223, train_loss: 0.1136, step time: 0.1000\n",
      "167/223, train_loss: 0.1236, step time: 0.1007\n",
      "168/223, train_loss: 0.1051, step time: 0.1011\n",
      "169/223, train_loss: 0.1207, step time: 0.1131\n",
      "170/223, train_loss: 0.1091, step time: 0.1013\n",
      "171/223, train_loss: 0.0978, step time: 0.0998\n",
      "172/223, train_loss: 0.1020, step time: 0.1018\n",
      "173/223, train_loss: 0.1204, step time: 0.1156\n",
      "174/223, train_loss: 0.1133, step time: 0.0985\n",
      "175/223, train_loss: 0.1260, step time: 0.0987\n",
      "176/223, train_loss: 0.0990, step time: 0.1021\n",
      "177/223, train_loss: 0.1148, step time: 0.0997\n",
      "178/223, train_loss: 0.1154, step time: 0.1051\n",
      "179/223, train_loss: 0.1091, step time: 0.1592\n",
      "180/223, train_loss: 0.1239, step time: 0.0992\n",
      "181/223, train_loss: 0.1139, step time: 0.1105\n",
      "182/223, train_loss: 0.1034, step time: 0.1168\n",
      "183/223, train_loss: 0.1084, step time: 0.1192\n",
      "184/223, train_loss: 0.1368, step time: 0.1004\n",
      "185/223, train_loss: 0.1158, step time: 0.1229\n",
      "186/223, train_loss: 0.1193, step time: 0.1407\n",
      "187/223, train_loss: 0.1307, step time: 0.1148\n",
      "188/223, train_loss: 0.1056, step time: 0.1123\n",
      "189/223, train_loss: 0.1205, step time: 0.1005\n",
      "190/223, train_loss: 0.0977, step time: 0.1063\n",
      "191/223, train_loss: 0.1034, step time: 0.1160\n",
      "192/223, train_loss: 0.1088, step time: 0.1180\n",
      "193/223, train_loss: 0.1154, step time: 0.1002\n",
      "194/223, train_loss: 0.1075, step time: 0.1020\n",
      "195/223, train_loss: 0.1210, step time: 0.1352\n",
      "196/223, train_loss: 0.1083, step time: 0.1056\n",
      "197/223, train_loss: 0.1053, step time: 0.1005\n",
      "198/223, train_loss: 0.1092, step time: 0.1007\n",
      "199/223, train_loss: 0.1189, step time: 0.1003\n",
      "200/223, train_loss: 0.1073, step time: 0.1060\n",
      "201/223, train_loss: 0.1129, step time: 0.1016\n",
      "202/223, train_loss: 0.1084, step time: 0.0999\n",
      "203/223, train_loss: 0.1092, step time: 0.1091\n",
      "204/223, train_loss: 0.1139, step time: 0.1284\n",
      "205/223, train_loss: 0.1092, step time: 0.1010\n",
      "206/223, train_loss: 0.1254, step time: 0.1008\n",
      "207/223, train_loss: 0.1171, step time: 0.1053\n",
      "208/223, train_loss: 0.1070, step time: 0.1010\n",
      "209/223, train_loss: 0.0993, step time: 0.1093\n",
      "210/223, train_loss: 0.1062, step time: 0.1064\n",
      "211/223, train_loss: 0.1195, step time: 0.1231\n",
      "212/223, train_loss: 0.0973, step time: 0.1156\n",
      "213/223, train_loss: 0.1035, step time: 0.0999\n",
      "214/223, train_loss: 0.1199, step time: 0.1007\n",
      "215/223, train_loss: 0.1170, step time: 0.1004\n",
      "216/223, train_loss: 0.1203, step time: 0.1157\n",
      "217/223, train_loss: 0.1110, step time: 0.1034\n",
      "218/223, train_loss: 0.1186, step time: 0.1002\n",
      "219/223, train_loss: 0.1179, step time: 0.1003\n",
      "220/223, train_loss: 0.1018, step time: 0.1001\n",
      "221/223, train_loss: 0.1208, step time: 0.0995\n",
      "222/223, train_loss: 0.1103, step time: 0.0990\n",
      "223/223, train_loss: 0.1125, step time: 0.0994\n",
      "epoch 101 average loss: 0.1151\n",
      "time consuming of epoch 101 is: 85.8280\n",
      "----------\n",
      "epoch 102/300\n",
      "1/223, train_loss: 0.1065, step time: 0.1010\n",
      "2/223, train_loss: 0.1124, step time: 0.1006\n",
      "3/223, train_loss: 0.0992, step time: 0.1000\n",
      "4/223, train_loss: 0.1134, step time: 0.1005\n",
      "5/223, train_loss: 0.1247, step time: 0.1160\n",
      "6/223, train_loss: 0.1268, step time: 0.1109\n",
      "7/223, train_loss: 0.1143, step time: 0.1336\n",
      "8/223, train_loss: 0.1176, step time: 0.1007\n",
      "9/223, train_loss: 0.1021, step time: 0.1069\n",
      "10/223, train_loss: 0.1034, step time: 0.1005\n",
      "11/223, train_loss: 0.1050, step time: 0.0999\n",
      "12/223, train_loss: 0.1185, step time: 0.1018\n",
      "13/223, train_loss: 0.1076, step time: 0.1097\n",
      "14/223, train_loss: 0.1078, step time: 0.1089\n",
      "15/223, train_loss: 0.1037, step time: 0.1013\n",
      "16/223, train_loss: 0.1163, step time: 0.1009\n",
      "17/223, train_loss: 0.1153, step time: 0.1000\n",
      "18/223, train_loss: 0.1105, step time: 0.1003\n",
      "19/223, train_loss: 0.1323, step time: 0.1142\n",
      "20/223, train_loss: 0.1218, step time: 0.1218\n",
      "21/223, train_loss: 0.1186, step time: 0.1010\n",
      "22/223, train_loss: 0.1132, step time: 0.1147\n",
      "23/223, train_loss: 0.1058, step time: 0.1116\n",
      "24/223, train_loss: 0.1088, step time: 0.1240\n",
      "25/223, train_loss: 0.1161, step time: 0.1165\n",
      "26/223, train_loss: 0.1021, step time: 0.1081\n",
      "27/223, train_loss: 0.1090, step time: 0.1090\n",
      "28/223, train_loss: 0.1204, step time: 0.1187\n",
      "29/223, train_loss: 0.1059, step time: 0.1079\n",
      "30/223, train_loss: 0.1035, step time: 0.1109\n",
      "31/223, train_loss: 0.1094, step time: 0.1004\n",
      "32/223, train_loss: 0.1120, step time: 0.0998\n",
      "33/223, train_loss: 0.1148, step time: 0.1133\n",
      "34/223, train_loss: 0.1039, step time: 0.1099\n",
      "35/223, train_loss: 0.1273, step time: 0.1005\n",
      "36/223, train_loss: 0.1266, step time: 0.1002\n",
      "37/223, train_loss: 0.1110, step time: 0.1001\n",
      "38/223, train_loss: 0.1028, step time: 0.1002\n",
      "39/223, train_loss: 0.1108, step time: 0.1008\n",
      "40/223, train_loss: 0.1012, step time: 0.1000\n",
      "41/223, train_loss: 0.1079, step time: 0.1058\n",
      "42/223, train_loss: 0.1144, step time: 0.0999\n",
      "43/223, train_loss: 0.1267, step time: 0.1000\n",
      "44/223, train_loss: 0.1030, step time: 0.1031\n",
      "45/223, train_loss: 0.1209, step time: 0.1121\n",
      "46/223, train_loss: 0.1046, step time: 0.1091\n",
      "47/223, train_loss: 0.1162, step time: 0.1052\n",
      "48/223, train_loss: 0.1106, step time: 0.1163\n",
      "49/223, train_loss: 0.1261, step time: 0.1006\n",
      "50/223, train_loss: 0.1212, step time: 0.1203\n",
      "51/223, train_loss: 0.1227, step time: 0.1022\n",
      "52/223, train_loss: 0.1168, step time: 0.1247\n",
      "53/223, train_loss: 0.1144, step time: 0.1106\n",
      "54/223, train_loss: 0.1088, step time: 0.1144\n",
      "55/223, train_loss: 0.1001, step time: 0.1086\n",
      "56/223, train_loss: 0.1029, step time: 0.1102\n",
      "57/223, train_loss: 0.1136, step time: 0.1001\n",
      "58/223, train_loss: 0.1156, step time: 0.1055\n",
      "59/223, train_loss: 0.1255, step time: 0.1043\n",
      "60/223, train_loss: 0.1179, step time: 0.1018\n",
      "61/223, train_loss: 0.1264, step time: 0.1004\n",
      "62/223, train_loss: 0.1161, step time: 0.1128\n",
      "63/223, train_loss: 0.1180, step time: 0.1165\n",
      "64/223, train_loss: 0.1046, step time: 0.1296\n",
      "65/223, train_loss: 0.1103, step time: 0.1024\n",
      "66/223, train_loss: 0.1031, step time: 0.1049\n",
      "67/223, train_loss: 0.1164, step time: 0.1288\n",
      "68/223, train_loss: 0.1083, step time: 0.1321\n",
      "69/223, train_loss: 0.1119, step time: 0.0999\n",
      "70/223, train_loss: 0.3033, step time: 0.1169\n",
      "71/223, train_loss: 0.1075, step time: 0.1113\n",
      "72/223, train_loss: 0.1081, step time: 0.1083\n",
      "73/223, train_loss: 0.1138, step time: 0.0999\n",
      "74/223, train_loss: 0.1277, step time: 0.1140\n",
      "75/223, train_loss: 0.1187, step time: 0.1182\n",
      "76/223, train_loss: 0.1091, step time: 0.1176\n",
      "77/223, train_loss: 0.1092, step time: 0.1010\n",
      "78/223, train_loss: 0.1001, step time: 0.1070\n",
      "79/223, train_loss: 0.1272, step time: 0.1223\n",
      "80/223, train_loss: 0.1174, step time: 0.1225\n",
      "81/223, train_loss: 0.1065, step time: 0.0996\n",
      "82/223, train_loss: 0.1072, step time: 0.1002\n",
      "83/223, train_loss: 0.1066, step time: 0.1035\n",
      "84/223, train_loss: 0.1220, step time: 0.1186\n",
      "85/223, train_loss: 0.1176, step time: 0.1041\n",
      "86/223, train_loss: 0.1079, step time: 0.1006\n",
      "87/223, train_loss: 0.1211, step time: 0.1035\n",
      "88/223, train_loss: 0.1353, step time: 0.1014\n",
      "89/223, train_loss: 0.1120, step time: 0.1118\n",
      "90/223, train_loss: 0.1113, step time: 0.1032\n",
      "91/223, train_loss: 0.1158, step time: 0.1001\n",
      "92/223, train_loss: 0.1164, step time: 0.1020\n",
      "93/223, train_loss: 0.1103, step time: 0.1085\n",
      "94/223, train_loss: 0.1116, step time: 0.1157\n",
      "95/223, train_loss: 0.1123, step time: 0.1033\n",
      "96/223, train_loss: 0.1273, step time: 0.1006\n",
      "97/223, train_loss: 0.1130, step time: 0.1087\n",
      "98/223, train_loss: 0.1172, step time: 0.0995\n",
      "99/223, train_loss: 0.1119, step time: 0.1007\n",
      "100/223, train_loss: 0.1270, step time: 0.1003\n",
      "101/223, train_loss: 0.1161, step time: 0.1127\n",
      "102/223, train_loss: 0.1158, step time: 0.1140\n",
      "103/223, train_loss: 0.1272, step time: 0.1117\n",
      "104/223, train_loss: 0.1088, step time: 0.1114\n",
      "105/223, train_loss: 0.1082, step time: 0.1117\n",
      "106/223, train_loss: 0.1102, step time: 0.1003\n",
      "107/223, train_loss: 0.1206, step time: 0.1004\n",
      "108/223, train_loss: 0.0994, step time: 0.1008\n",
      "109/223, train_loss: 0.1046, step time: 0.1114\n",
      "110/223, train_loss: 0.1179, step time: 0.1120\n",
      "111/223, train_loss: 0.1074, step time: 0.1144\n",
      "112/223, train_loss: 0.1164, step time: 0.1126\n",
      "113/223, train_loss: 0.1162, step time: 0.1043\n",
      "114/223, train_loss: 0.1133, step time: 0.1003\n",
      "115/223, train_loss: 0.1081, step time: 0.0997\n",
      "116/223, train_loss: 0.1254, step time: 0.1002\n",
      "117/223, train_loss: 0.1216, step time: 0.1045\n",
      "118/223, train_loss: 0.1083, step time: 0.1062\n",
      "119/223, train_loss: 0.1137, step time: 0.1003\n",
      "120/223, train_loss: 0.1099, step time: 0.1005\n",
      "121/223, train_loss: 0.1143, step time: 0.1226\n",
      "122/223, train_loss: 0.1146, step time: 0.1021\n",
      "123/223, train_loss: 0.1158, step time: 0.1004\n",
      "124/223, train_loss: 0.1098, step time: 0.1006\n",
      "125/223, train_loss: 0.1047, step time: 0.0986\n",
      "126/223, train_loss: 0.1315, step time: 0.1123\n",
      "127/223, train_loss: 0.1157, step time: 0.1146\n",
      "128/223, train_loss: 0.0979, step time: 0.1052\n",
      "129/223, train_loss: 0.1243, step time: 0.1000\n",
      "130/223, train_loss: 0.1166, step time: 0.1114\n",
      "131/223, train_loss: 0.1170, step time: 0.1229\n",
      "132/223, train_loss: 0.1065, step time: 0.1264\n",
      "133/223, train_loss: 0.1037, step time: 0.0993\n",
      "134/223, train_loss: 0.1147, step time: 0.1168\n",
      "135/223, train_loss: 0.1117, step time: 0.1001\n",
      "136/223, train_loss: 0.1080, step time: 0.1019\n",
      "137/223, train_loss: 0.1043, step time: 0.0997\n",
      "138/223, train_loss: 0.1070, step time: 0.1000\n",
      "139/223, train_loss: 0.1157, step time: 0.1007\n",
      "140/223, train_loss: 0.1084, step time: 0.1011\n",
      "141/223, train_loss: 0.1090, step time: 0.1164\n",
      "142/223, train_loss: 0.1231, step time: 0.1006\n",
      "143/223, train_loss: 0.1055, step time: 0.1005\n",
      "144/223, train_loss: 0.1081, step time: 0.1004\n",
      "145/223, train_loss: 0.1110, step time: 0.1095\n",
      "146/223, train_loss: 0.1006, step time: 0.1185\n",
      "147/223, train_loss: 0.1140, step time: 0.1386\n",
      "148/223, train_loss: 0.1195, step time: 0.1013\n",
      "149/223, train_loss: 0.1086, step time: 0.1002\n",
      "150/223, train_loss: 0.1160, step time: 0.1003\n",
      "151/223, train_loss: 0.1225, step time: 0.1006\n",
      "152/223, train_loss: 0.1003, step time: 0.1000\n",
      "153/223, train_loss: 0.0967, step time: 0.1134\n",
      "154/223, train_loss: 0.1138, step time: 0.1226\n",
      "155/223, train_loss: 0.1138, step time: 0.0998\n",
      "156/223, train_loss: 0.1159, step time: 0.0999\n",
      "157/223, train_loss: 0.1092, step time: 0.1148\n",
      "158/223, train_loss: 0.1149, step time: 0.1220\n",
      "159/223, train_loss: 0.1129, step time: 0.1102\n",
      "160/223, train_loss: 0.1129, step time: 0.1176\n",
      "161/223, train_loss: 0.1059, step time: 0.1083\n",
      "162/223, train_loss: 0.1229, step time: 0.1063\n",
      "163/223, train_loss: 0.1025, step time: 0.1006\n",
      "164/223, train_loss: 0.1074, step time: 0.1005\n",
      "165/223, train_loss: 0.1143, step time: 0.1107\n",
      "166/223, train_loss: 0.1125, step time: 0.1027\n",
      "167/223, train_loss: 0.1020, step time: 0.1086\n",
      "168/223, train_loss: 0.1165, step time: 0.1169\n",
      "169/223, train_loss: 0.1187, step time: 0.0997\n",
      "170/223, train_loss: 0.1212, step time: 0.1060\n",
      "171/223, train_loss: 0.1279, step time: 0.1352\n",
      "172/223, train_loss: 0.1255, step time: 0.1048\n",
      "173/223, train_loss: 0.1295, step time: 0.1000\n",
      "174/223, train_loss: 0.1174, step time: 0.1002\n",
      "175/223, train_loss: 0.1101, step time: 0.1007\n",
      "176/223, train_loss: 0.1128, step time: 0.0999\n",
      "177/223, train_loss: 0.1125, step time: 0.1093\n",
      "178/223, train_loss: 0.1197, step time: 0.1132\n",
      "179/223, train_loss: 0.1177, step time: 0.1147\n",
      "180/223, train_loss: 0.1089, step time: 0.1211\n",
      "181/223, train_loss: 0.1222, step time: 0.1146\n",
      "182/223, train_loss: 0.1080, step time: 0.1337\n",
      "183/223, train_loss: 0.1215, step time: 0.1142\n",
      "184/223, train_loss: 0.1133, step time: 0.1200\n",
      "185/223, train_loss: 0.1338, step time: 0.1052\n",
      "186/223, train_loss: 0.1157, step time: 0.1175\n",
      "187/223, train_loss: 0.1046, step time: 0.0998\n",
      "188/223, train_loss: 0.1060, step time: 0.1003\n",
      "189/223, train_loss: 0.1169, step time: 0.0998\n",
      "190/223, train_loss: 0.1180, step time: 0.1017\n",
      "191/223, train_loss: 0.1120, step time: 0.1000\n",
      "192/223, train_loss: 0.1066, step time: 0.1007\n",
      "193/223, train_loss: 0.1239, step time: 0.1192\n",
      "194/223, train_loss: 0.1117, step time: 0.1074\n",
      "195/223, train_loss: 0.1156, step time: 0.1191\n",
      "196/223, train_loss: 0.1199, step time: 0.1003\n",
      "197/223, train_loss: 0.1112, step time: 0.1005\n",
      "198/223, train_loss: 0.1106, step time: 0.1119\n",
      "199/223, train_loss: 0.1068, step time: 0.1088\n",
      "200/223, train_loss: 0.1092, step time: 0.1046\n",
      "201/223, train_loss: 0.1232, step time: 0.1200\n",
      "202/223, train_loss: 0.1159, step time: 0.1128\n",
      "203/223, train_loss: 0.1267, step time: 0.1072\n",
      "204/223, train_loss: 0.1080, step time: 0.1109\n",
      "205/223, train_loss: 0.1092, step time: 0.1126\n",
      "206/223, train_loss: 0.1199, step time: 0.1151\n",
      "207/223, train_loss: 0.1160, step time: 0.1101\n",
      "208/223, train_loss: 0.1068, step time: 0.1468\n",
      "209/223, train_loss: 0.1135, step time: 0.1075\n",
      "210/223, train_loss: 0.1112, step time: 0.1154\n",
      "211/223, train_loss: 0.1159, step time: 0.1041\n",
      "212/223, train_loss: 0.1066, step time: 0.0990\n",
      "213/223, train_loss: 0.1066, step time: 0.0982\n",
      "214/223, train_loss: 0.1049, step time: 0.1017\n",
      "215/223, train_loss: 0.1150, step time: 0.1219\n",
      "216/223, train_loss: 0.1285, step time: 0.1094\n",
      "217/223, train_loss: 0.1095, step time: 0.0987\n",
      "218/223, train_loss: 0.1195, step time: 0.1001\n",
      "219/223, train_loss: 0.1144, step time: 0.1009\n",
      "220/223, train_loss: 0.1042, step time: 0.1001\n",
      "221/223, train_loss: 0.1248, step time: 0.0999\n",
      "222/223, train_loss: 0.1190, step time: 0.1000\n",
      "223/223, train_loss: 0.1172, step time: 0.0996\n",
      "epoch 102 average loss: 0.1144\n",
      "time consuming of epoch 102 is: 86.3743\n",
      "----------\n",
      "epoch 103/300\n",
      "1/223, train_loss: 0.1105, step time: 0.1163\n",
      "2/223, train_loss: 0.1194, step time: 0.1099\n",
      "3/223, train_loss: 0.1189, step time: 0.1411\n",
      "4/223, train_loss: 0.1029, step time: 0.1154\n",
      "5/223, train_loss: 0.1123, step time: 0.1196\n",
      "6/223, train_loss: 0.1138, step time: 0.1179\n",
      "7/223, train_loss: 0.1165, step time: 0.1046\n",
      "8/223, train_loss: 0.1139, step time: 0.1226\n",
      "9/223, train_loss: 0.1114, step time: 0.1068\n",
      "10/223, train_loss: 0.1104, step time: 0.1165\n",
      "11/223, train_loss: 0.1088, step time: 0.1142\n",
      "12/223, train_loss: 0.1079, step time: 0.1020\n",
      "13/223, train_loss: 0.1267, step time: 0.1043\n",
      "14/223, train_loss: 0.1080, step time: 0.1104\n",
      "15/223, train_loss: 0.1045, step time: 0.1000\n",
      "16/223, train_loss: 0.1257, step time: 0.1040\n",
      "17/223, train_loss: 0.1208, step time: 0.1058\n",
      "18/223, train_loss: 0.1137, step time: 0.1064\n",
      "19/223, train_loss: 0.1152, step time: 0.1064\n",
      "20/223, train_loss: 0.1264, step time: 0.1197\n",
      "21/223, train_loss: 0.1092, step time: 0.1038\n",
      "22/223, train_loss: 0.3051, step time: 0.1133\n",
      "23/223, train_loss: 0.1067, step time: 0.1159\n",
      "24/223, train_loss: 0.1066, step time: 0.1265\n",
      "25/223, train_loss: 0.1093, step time: 0.1041\n",
      "26/223, train_loss: 0.1077, step time: 0.1104\n",
      "27/223, train_loss: 0.1183, step time: 0.1487\n",
      "28/223, train_loss: 0.1259, step time: 0.1066\n",
      "29/223, train_loss: 0.1211, step time: 0.1281\n",
      "30/223, train_loss: 0.1098, step time: 0.1119\n",
      "31/223, train_loss: 0.1211, step time: 0.1672\n",
      "32/223, train_loss: 0.1282, step time: 0.1123\n",
      "33/223, train_loss: 0.1003, step time: 0.1227\n",
      "34/223, train_loss: 0.1122, step time: 0.1095\n",
      "35/223, train_loss: 0.0979, step time: 0.1001\n",
      "36/223, train_loss: 0.1144, step time: 0.1003\n",
      "37/223, train_loss: 0.1156, step time: 0.1149\n",
      "38/223, train_loss: 0.1027, step time: 0.1043\n",
      "39/223, train_loss: 0.1074, step time: 0.1091\n",
      "40/223, train_loss: 0.1105, step time: 0.1085\n",
      "41/223, train_loss: 0.1033, step time: 0.1134\n",
      "42/223, train_loss: 0.1067, step time: 0.1225\n",
      "43/223, train_loss: 0.1219, step time: 0.1051\n",
      "44/223, train_loss: 0.1174, step time: 0.1010\n",
      "45/223, train_loss: 0.1110, step time: 0.1089\n",
      "46/223, train_loss: 0.1074, step time: 0.1187\n",
      "47/223, train_loss: 0.1162, step time: 0.1002\n",
      "48/223, train_loss: 0.1208, step time: 0.1055\n",
      "49/223, train_loss: 0.1177, step time: 0.1059\n",
      "50/223, train_loss: 0.1092, step time: 0.1099\n",
      "51/223, train_loss: 0.1121, step time: 0.0999\n",
      "52/223, train_loss: 0.1216, step time: 0.1338\n",
      "53/223, train_loss: 0.1128, step time: 0.1100\n",
      "54/223, train_loss: 0.1194, step time: 0.0999\n",
      "55/223, train_loss: 0.1166, step time: 0.1180\n",
      "56/223, train_loss: 0.1003, step time: 0.1091\n",
      "57/223, train_loss: 0.1286, step time: 0.1190\n",
      "58/223, train_loss: 0.1122, step time: 0.0994\n",
      "59/223, train_loss: 0.0973, step time: 0.1022\n",
      "60/223, train_loss: 0.1125, step time: 0.1067\n",
      "61/223, train_loss: 0.1175, step time: 0.1339\n",
      "62/223, train_loss: 0.1078, step time: 0.1139\n",
      "63/223, train_loss: 0.0996, step time: 0.1083\n",
      "64/223, train_loss: 0.1091, step time: 0.1134\n",
      "65/223, train_loss: 0.1076, step time: 0.1189\n",
      "66/223, train_loss: 0.1080, step time: 0.1116\n",
      "67/223, train_loss: 0.1142, step time: 0.1111\n",
      "68/223, train_loss: 0.1041, step time: 0.1141\n",
      "69/223, train_loss: 0.1086, step time: 0.1000\n",
      "70/223, train_loss: 0.1249, step time: 0.1050\n",
      "71/223, train_loss: 0.1159, step time: 0.1003\n",
      "72/223, train_loss: 0.1129, step time: 0.1096\n",
      "73/223, train_loss: 0.1128, step time: 0.1072\n",
      "74/223, train_loss: 0.1108, step time: 0.1093\n",
      "75/223, train_loss: 0.1147, step time: 0.1059\n",
      "76/223, train_loss: 0.1089, step time: 0.1260\n",
      "77/223, train_loss: 0.1224, step time: 0.1078\n",
      "78/223, train_loss: 0.1107, step time: 0.1083\n",
      "79/223, train_loss: 0.1078, step time: 0.1001\n",
      "80/223, train_loss: 0.1161, step time: 0.1719\n",
      "81/223, train_loss: 0.1156, step time: 0.1042\n",
      "82/223, train_loss: 0.1143, step time: 0.1118\n",
      "83/223, train_loss: 0.1181, step time: 0.1067\n",
      "84/223, train_loss: 0.1133, step time: 0.1155\n",
      "85/223, train_loss: 0.1064, step time: 0.1145\n",
      "86/223, train_loss: 0.1109, step time: 0.1147\n",
      "87/223, train_loss: 0.1069, step time: 0.1037\n",
      "88/223, train_loss: 0.1082, step time: 0.1037\n",
      "89/223, train_loss: 0.1120, step time: 0.1074\n",
      "90/223, train_loss: 0.1198, step time: 0.1177\n",
      "91/223, train_loss: 0.1199, step time: 0.1098\n",
      "92/223, train_loss: 0.1185, step time: 0.1061\n",
      "93/223, train_loss: 0.1114, step time: 0.1214\n",
      "94/223, train_loss: 0.1238, step time: 0.1072\n",
      "95/223, train_loss: 0.1261, step time: 0.1093\n",
      "96/223, train_loss: 0.1112, step time: 0.1085\n",
      "97/223, train_loss: 0.1254, step time: 0.1143\n",
      "98/223, train_loss: 0.1151, step time: 0.1063\n",
      "99/223, train_loss: 0.1126, step time: 0.1140\n",
      "100/223, train_loss: 0.1219, step time: 0.1044\n",
      "101/223, train_loss: 0.1065, step time: 0.1169\n",
      "102/223, train_loss: 0.1076, step time: 0.1113\n",
      "103/223, train_loss: 0.1231, step time: 0.1289\n",
      "104/223, train_loss: 0.1258, step time: 0.1154\n",
      "105/223, train_loss: 0.1002, step time: 0.1162\n",
      "106/223, train_loss: 0.1053, step time: 0.1003\n",
      "107/223, train_loss: 0.1145, step time: 0.1001\n",
      "108/223, train_loss: 0.1089, step time: 0.1074\n",
      "109/223, train_loss: 0.1074, step time: 0.1112\n",
      "110/223, train_loss: 0.1255, step time: 0.1030\n",
      "111/223, train_loss: 0.1133, step time: 0.1005\n",
      "112/223, train_loss: 0.1255, step time: 0.1249\n",
      "113/223, train_loss: 0.1190, step time: 0.1131\n",
      "114/223, train_loss: 0.1092, step time: 0.1007\n",
      "115/223, train_loss: 0.1104, step time: 0.1016\n",
      "116/223, train_loss: 0.1197, step time: 0.1002\n",
      "117/223, train_loss: 0.1173, step time: 0.1096\n",
      "118/223, train_loss: 0.1222, step time: 0.0998\n",
      "119/223, train_loss: 0.1219, step time: 0.1010\n",
      "120/223, train_loss: 0.1191, step time: 0.1004\n",
      "121/223, train_loss: 0.1096, step time: 0.1046\n",
      "122/223, train_loss: 0.1279, step time: 0.1002\n",
      "123/223, train_loss: 0.1193, step time: 0.0999\n",
      "124/223, train_loss: 0.1197, step time: 0.0999\n",
      "125/223, train_loss: 0.1214, step time: 0.1020\n",
      "126/223, train_loss: 0.1221, step time: 0.1007\n",
      "127/223, train_loss: 0.1152, step time: 0.1000\n",
      "128/223, train_loss: 0.1202, step time: 0.0998\n",
      "129/223, train_loss: 0.1158, step time: 0.1051\n",
      "130/223, train_loss: 0.1130, step time: 0.1060\n",
      "131/223, train_loss: 0.1131, step time: 0.1126\n",
      "132/223, train_loss: 0.1107, step time: 0.1114\n",
      "133/223, train_loss: 0.1165, step time: 0.0993\n",
      "134/223, train_loss: 0.1095, step time: 0.1069\n",
      "135/223, train_loss: 0.1170, step time: 0.0998\n",
      "136/223, train_loss: 0.1219, step time: 0.1001\n",
      "137/223, train_loss: 0.1149, step time: 0.1032\n",
      "138/223, train_loss: 0.1287, step time: 0.1006\n",
      "139/223, train_loss: 0.1089, step time: 0.1007\n",
      "140/223, train_loss: 0.1183, step time: 0.1014\n",
      "141/223, train_loss: 0.1205, step time: 0.1087\n",
      "142/223, train_loss: 0.1235, step time: 0.1006\n",
      "143/223, train_loss: 0.1166, step time: 0.1004\n",
      "144/223, train_loss: 0.1141, step time: 0.1004\n",
      "145/223, train_loss: 0.1222, step time: 0.1001\n",
      "146/223, train_loss: 0.1065, step time: 0.1148\n",
      "147/223, train_loss: 0.1195, step time: 0.1124\n",
      "148/223, train_loss: 0.1201, step time: 0.1007\n",
      "149/223, train_loss: 0.1203, step time: 0.1043\n",
      "150/223, train_loss: 0.1086, step time: 0.1047\n",
      "151/223, train_loss: 0.1143, step time: 0.1077\n",
      "152/223, train_loss: 0.1059, step time: 0.1162\n",
      "153/223, train_loss: 0.1044, step time: 0.1016\n",
      "154/223, train_loss: 0.1094, step time: 0.1020\n",
      "155/223, train_loss: 0.1084, step time: 0.1071\n",
      "156/223, train_loss: 0.1069, step time: 0.1179\n",
      "157/223, train_loss: 0.1180, step time: 0.1036\n",
      "158/223, train_loss: 0.0983, step time: 0.1028\n",
      "159/223, train_loss: 0.1122, step time: 0.1000\n",
      "160/223, train_loss: 0.1189, step time: 0.1081\n",
      "161/223, train_loss: 0.1036, step time: 0.1067\n",
      "162/223, train_loss: 0.1204, step time: 0.1004\n",
      "163/223, train_loss: 0.1200, step time: 0.1019\n",
      "164/223, train_loss: 0.1050, step time: 0.1078\n",
      "165/223, train_loss: 0.1051, step time: 0.1128\n",
      "166/223, train_loss: 0.1182, step time: 0.1075\n",
      "167/223, train_loss: 0.1383, step time: 0.1004\n",
      "168/223, train_loss: 0.1317, step time: 0.1051\n",
      "169/223, train_loss: 0.1075, step time: 0.1082\n",
      "170/223, train_loss: 0.1112, step time: 0.1115\n",
      "171/223, train_loss: 0.1152, step time: 0.1009\n",
      "172/223, train_loss: 0.1169, step time: 0.1109\n",
      "173/223, train_loss: 0.1021, step time: 0.1193\n",
      "174/223, train_loss: 0.1234, step time: 0.1091\n",
      "175/223, train_loss: 0.1217, step time: 0.1118\n",
      "176/223, train_loss: 0.1095, step time: 0.1175\n",
      "177/223, train_loss: 0.1069, step time: 0.1108\n",
      "178/223, train_loss: 0.1263, step time: 0.1100\n",
      "179/223, train_loss: 0.1155, step time: 0.1389\n",
      "180/223, train_loss: 0.1119, step time: 0.1006\n",
      "181/223, train_loss: 0.0995, step time: 0.0997\n",
      "182/223, train_loss: 0.1239, step time: 0.1005\n",
      "183/223, train_loss: 0.1087, step time: 0.1003\n",
      "184/223, train_loss: 0.1442, step time: 0.1107\n",
      "185/223, train_loss: 0.1167, step time: 0.1219\n",
      "186/223, train_loss: 0.1015, step time: 0.1157\n",
      "187/223, train_loss: 0.1273, step time: 0.1179\n",
      "188/223, train_loss: 0.1075, step time: 0.1084\n",
      "189/223, train_loss: 0.1145, step time: 0.1090\n",
      "190/223, train_loss: 0.1102, step time: 0.1006\n",
      "191/223, train_loss: 0.1184, step time: 0.1180\n",
      "192/223, train_loss: 0.1300, step time: 0.0995\n",
      "193/223, train_loss: 0.1094, step time: 0.1141\n",
      "194/223, train_loss: 0.1105, step time: 0.0989\n",
      "195/223, train_loss: 0.1151, step time: 0.0995\n",
      "196/223, train_loss: 0.1216, step time: 0.0986\n",
      "197/223, train_loss: 0.1313, step time: 0.1087\n",
      "198/223, train_loss: 0.1186, step time: 0.0984\n",
      "199/223, train_loss: 0.1232, step time: 0.0988\n",
      "200/223, train_loss: 0.1104, step time: 0.0994\n",
      "201/223, train_loss: 0.1307, step time: 0.1102\n",
      "202/223, train_loss: 0.1205, step time: 0.1273\n",
      "203/223, train_loss: 0.1037, step time: 0.0985\n",
      "204/223, train_loss: 0.1042, step time: 0.0982\n",
      "205/223, train_loss: 0.1126, step time: 0.1079\n",
      "206/223, train_loss: 0.1176, step time: 0.1117\n",
      "207/223, train_loss: 0.0999, step time: 0.1270\n",
      "208/223, train_loss: 0.1131, step time: 0.1185\n",
      "209/223, train_loss: 0.1058, step time: 0.1146\n",
      "210/223, train_loss: 0.1223, step time: 0.1065\n",
      "211/223, train_loss: 0.1049, step time: 0.1098\n",
      "212/223, train_loss: 0.1133, step time: 0.1068\n",
      "213/223, train_loss: 0.1141, step time: 0.1137\n",
      "214/223, train_loss: 0.1254, step time: 0.1117\n",
      "215/223, train_loss: 0.1111, step time: 0.1091\n",
      "216/223, train_loss: 0.1135, step time: 0.1032\n",
      "217/223, train_loss: 0.1113, step time: 0.1118\n",
      "218/223, train_loss: 0.1183, step time: 0.1008\n",
      "219/223, train_loss: 0.1105, step time: 0.1009\n",
      "220/223, train_loss: 0.1068, step time: 0.1007\n",
      "221/223, train_loss: 0.1167, step time: 0.0991\n",
      "222/223, train_loss: 0.1127, step time: 0.0992\n",
      "223/223, train_loss: 0.1242, step time: 0.1003\n",
      "epoch 103 average loss: 0.1153\n",
      "time consuming of epoch 103 is: 88.5127\n",
      "----------\n",
      "epoch 104/300\n",
      "1/223, train_loss: 0.1126, step time: 0.1012\n",
      "2/223, train_loss: 0.1092, step time: 0.1003\n",
      "3/223, train_loss: 0.1199, step time: 0.1021\n",
      "4/223, train_loss: 0.1174, step time: 0.0996\n",
      "5/223, train_loss: 0.1188, step time: 0.1058\n",
      "6/223, train_loss: 0.1270, step time: 0.1251\n",
      "7/223, train_loss: 0.1132, step time: 0.1219\n",
      "8/223, train_loss: 0.1106, step time: 0.1000\n",
      "9/223, train_loss: 0.1151, step time: 0.1354\n",
      "10/223, train_loss: 0.1239, step time: 0.1149\n",
      "11/223, train_loss: 0.1113, step time: 0.1067\n",
      "12/223, train_loss: 0.1035, step time: 0.1130\n",
      "13/223, train_loss: 0.1078, step time: 0.1166\n",
      "14/223, train_loss: 0.0985, step time: 0.1119\n",
      "15/223, train_loss: 0.1147, step time: 0.1216\n",
      "16/223, train_loss: 0.1159, step time: 0.1074\n",
      "17/223, train_loss: 0.1061, step time: 0.1078\n",
      "18/223, train_loss: 0.1092, step time: 0.1054\n",
      "19/223, train_loss: 0.1251, step time: 0.1025\n",
      "20/223, train_loss: 0.1221, step time: 0.1008\n",
      "21/223, train_loss: 0.1105, step time: 0.1119\n",
      "22/223, train_loss: 0.1100, step time: 0.1099\n",
      "23/223, train_loss: 0.1131, step time: 0.1293\n",
      "24/223, train_loss: 0.1141, step time: 0.1006\n",
      "25/223, train_loss: 0.1127, step time: 0.1222\n",
      "26/223, train_loss: 0.1077, step time: 0.1121\n",
      "27/223, train_loss: 0.1195, step time: 0.1003\n",
      "28/223, train_loss: 0.1231, step time: 0.1074\n",
      "29/223, train_loss: 0.1262, step time: 0.1104\n",
      "30/223, train_loss: 0.1134, step time: 0.1218\n",
      "31/223, train_loss: 0.1224, step time: 0.1000\n",
      "32/223, train_loss: 0.1073, step time: 0.1003\n",
      "33/223, train_loss: 0.1069, step time: 0.1002\n",
      "34/223, train_loss: 0.1250, step time: 0.1004\n",
      "35/223, train_loss: 0.1175, step time: 0.1002\n",
      "36/223, train_loss: 0.1116, step time: 0.1008\n",
      "37/223, train_loss: 0.1065, step time: 0.1366\n",
      "38/223, train_loss: 0.1112, step time: 0.1134\n",
      "39/223, train_loss: 0.1139, step time: 0.1016\n",
      "40/223, train_loss: 0.1118, step time: 0.1003\n",
      "41/223, train_loss: 0.1168, step time: 0.1004\n",
      "42/223, train_loss: 0.1033, step time: 0.0991\n",
      "43/223, train_loss: 0.1130, step time: 0.1000\n",
      "44/223, train_loss: 0.1011, step time: 0.1006\n",
      "45/223, train_loss: 0.1196, step time: 0.1009\n",
      "46/223, train_loss: 0.1117, step time: 0.1120\n",
      "47/223, train_loss: 0.1160, step time: 0.1118\n",
      "48/223, train_loss: 0.1083, step time: 0.1156\n",
      "49/223, train_loss: 0.1040, step time: 0.0987\n",
      "50/223, train_loss: 0.1188, step time: 0.1052\n",
      "51/223, train_loss: 0.1186, step time: 0.1061\n",
      "52/223, train_loss: 0.1244, step time: 0.1092\n",
      "53/223, train_loss: 0.1032, step time: 0.1016\n",
      "54/223, train_loss: 0.1140, step time: 0.1003\n",
      "55/223, train_loss: 0.1104, step time: 0.1067\n",
      "56/223, train_loss: 0.1137, step time: 0.1049\n",
      "57/223, train_loss: 0.1057, step time: 0.1088\n",
      "58/223, train_loss: 0.1124, step time: 0.1056\n",
      "59/223, train_loss: 0.1041, step time: 0.1016\n",
      "60/223, train_loss: 0.1111, step time: 0.1069\n",
      "61/223, train_loss: 0.1190, step time: 0.1044\n",
      "62/223, train_loss: 0.0998, step time: 0.1208\n",
      "63/223, train_loss: 0.1302, step time: 0.1053\n",
      "64/223, train_loss: 0.1121, step time: 0.1207\n",
      "65/223, train_loss: 0.1199, step time: 0.1055\n",
      "66/223, train_loss: 0.1160, step time: 0.1015\n",
      "67/223, train_loss: 0.1144, step time: 0.0999\n",
      "68/223, train_loss: 0.1047, step time: 0.0997\n",
      "69/223, train_loss: 0.1184, step time: 0.1265\n",
      "70/223, train_loss: 0.1057, step time: 0.1154\n",
      "71/223, train_loss: 0.1105, step time: 0.1048\n",
      "72/223, train_loss: 0.1139, step time: 0.0999\n",
      "73/223, train_loss: 0.1292, step time: 0.1011\n",
      "74/223, train_loss: 0.1075, step time: 0.1181\n",
      "75/223, train_loss: 0.1130, step time: 0.1150\n",
      "76/223, train_loss: 0.1219, step time: 0.1175\n",
      "77/223, train_loss: 0.1145, step time: 0.1104\n",
      "78/223, train_loss: 0.1004, step time: 0.1019\n",
      "79/223, train_loss: 0.1045, step time: 0.1057\n",
      "80/223, train_loss: 0.1097, step time: 0.1097\n",
      "81/223, train_loss: 0.1071, step time: 0.1036\n",
      "82/223, train_loss: 0.1314, step time: 0.1006\n",
      "83/223, train_loss: 0.1102, step time: 0.1312\n",
      "84/223, train_loss: 0.1138, step time: 0.1248\n",
      "85/223, train_loss: 0.0998, step time: 0.0991\n",
      "86/223, train_loss: 0.0988, step time: 0.1004\n",
      "87/223, train_loss: 0.1055, step time: 0.1141\n",
      "88/223, train_loss: 0.1212, step time: 0.1059\n",
      "89/223, train_loss: 0.1234, step time: 0.1015\n",
      "90/223, train_loss: 0.1189, step time: 0.1154\n",
      "91/223, train_loss: 0.1080, step time: 0.1123\n",
      "92/223, train_loss: 0.1305, step time: 0.1090\n",
      "93/223, train_loss: 0.1122, step time: 0.1129\n",
      "94/223, train_loss: 0.1029, step time: 0.1053\n",
      "95/223, train_loss: 0.1181, step time: 0.0999\n",
      "96/223, train_loss: 0.1061, step time: 0.1106\n",
      "97/223, train_loss: 0.1256, step time: 0.1111\n",
      "98/223, train_loss: 0.1068, step time: 0.1082\n",
      "99/223, train_loss: 0.1062, step time: 0.1374\n",
      "100/223, train_loss: 0.1011, step time: 0.1088\n",
      "101/223, train_loss: 0.1113, step time: 0.1185\n",
      "102/223, train_loss: 0.3048, step time: 0.1078\n",
      "103/223, train_loss: 0.1133, step time: 0.1078\n",
      "104/223, train_loss: 0.1099, step time: 0.1125\n",
      "105/223, train_loss: 0.1070, step time: 0.1177\n",
      "106/223, train_loss: 0.1066, step time: 0.1150\n",
      "107/223, train_loss: 0.1185, step time: 0.1130\n",
      "108/223, train_loss: 0.1019, step time: 0.1229\n",
      "109/223, train_loss: 0.1173, step time: 0.1062\n",
      "110/223, train_loss: 0.1114, step time: 0.1011\n",
      "111/223, train_loss: 0.0977, step time: 0.1096\n",
      "112/223, train_loss: 0.1311, step time: 0.1006\n",
      "113/223, train_loss: 0.1208, step time: 0.0999\n",
      "114/223, train_loss: 0.1265, step time: 0.1041\n",
      "115/223, train_loss: 0.1252, step time: 0.0996\n",
      "116/223, train_loss: 0.1213, step time: 0.1259\n",
      "117/223, train_loss: 0.1092, step time: 0.1128\n",
      "118/223, train_loss: 0.1073, step time: 0.1271\n",
      "119/223, train_loss: 0.1167, step time: 0.1152\n",
      "120/223, train_loss: 0.1144, step time: 0.1029\n",
      "121/223, train_loss: 0.1120, step time: 0.1176\n",
      "122/223, train_loss: 0.1008, step time: 0.1167\n",
      "123/223, train_loss: 0.1156, step time: 0.1159\n",
      "124/223, train_loss: 0.1097, step time: 0.1141\n",
      "125/223, train_loss: 0.1088, step time: 0.1222\n",
      "126/223, train_loss: 0.1150, step time: 0.1008\n",
      "127/223, train_loss: 0.1012, step time: 0.1101\n",
      "128/223, train_loss: 0.1219, step time: 0.1097\n",
      "129/223, train_loss: 0.1174, step time: 0.1044\n",
      "130/223, train_loss: 0.1272, step time: 0.1091\n",
      "131/223, train_loss: 0.1082, step time: 0.1065\n",
      "132/223, train_loss: 0.1080, step time: 0.1105\n",
      "133/223, train_loss: 0.1046, step time: 0.1001\n",
      "134/223, train_loss: 0.1187, step time: 0.0995\n",
      "135/223, train_loss: 0.1210, step time: 0.1101\n",
      "136/223, train_loss: 0.1021, step time: 0.1072\n",
      "137/223, train_loss: 0.1027, step time: 0.1177\n",
      "138/223, train_loss: 0.1211, step time: 0.1029\n",
      "139/223, train_loss: 0.1149, step time: 0.1225\n",
      "140/223, train_loss: 0.1117, step time: 0.1083\n",
      "141/223, train_loss: 0.1132, step time: 0.1025\n",
      "142/223, train_loss: 0.1203, step time: 0.1011\n",
      "143/223, train_loss: 0.1155, step time: 0.1142\n",
      "144/223, train_loss: 0.1237, step time: 0.1278\n",
      "145/223, train_loss: 0.1114, step time: 0.1195\n",
      "146/223, train_loss: 0.1177, step time: 0.1040\n",
      "147/223, train_loss: 0.1105, step time: 0.1259\n",
      "148/223, train_loss: 0.1087, step time: 0.1113\n",
      "149/223, train_loss: 0.1079, step time: 0.1031\n",
      "150/223, train_loss: 0.1239, step time: 0.1445\n",
      "151/223, train_loss: 0.1025, step time: 0.1065\n",
      "152/223, train_loss: 0.1249, step time: 0.1102\n",
      "153/223, train_loss: 0.1134, step time: 0.1067\n",
      "154/223, train_loss: 0.1203, step time: 0.1041\n",
      "155/223, train_loss: 0.1059, step time: 0.1201\n",
      "156/223, train_loss: 0.1119, step time: 0.1138\n",
      "157/223, train_loss: 0.1029, step time: 0.1060\n",
      "158/223, train_loss: 0.1230, step time: 0.1004\n",
      "159/223, train_loss: 0.1087, step time: 0.1057\n",
      "160/223, train_loss: 0.1091, step time: 0.1465\n",
      "161/223, train_loss: 0.1336, step time: 0.0999\n",
      "162/223, train_loss: 0.1151, step time: 0.1076\n",
      "163/223, train_loss: 0.1185, step time: 0.1065\n",
      "164/223, train_loss: 0.1108, step time: 0.1223\n",
      "165/223, train_loss: 0.1245, step time: 0.1360\n",
      "166/223, train_loss: 0.1048, step time: 0.1187\n",
      "167/223, train_loss: 0.1085, step time: 0.1169\n",
      "168/223, train_loss: 0.1096, step time: 0.1484\n",
      "169/223, train_loss: 0.1152, step time: 0.0998\n",
      "170/223, train_loss: 0.1173, step time: 0.1006\n",
      "171/223, train_loss: 0.1049, step time: 0.1113\n",
      "172/223, train_loss: 0.1054, step time: 0.1005\n",
      "173/223, train_loss: 0.1238, step time: 0.1065\n",
      "174/223, train_loss: 0.1081, step time: 0.1373\n",
      "175/223, train_loss: 0.1241, step time: 0.1337\n",
      "176/223, train_loss: 0.1188, step time: 0.1178\n",
      "177/223, train_loss: 0.1157, step time: 0.1210\n",
      "178/223, train_loss: 0.1102, step time: 0.1344\n",
      "179/223, train_loss: 0.1144, step time: 0.1064\n",
      "180/223, train_loss: 0.1164, step time: 0.1156\n",
      "181/223, train_loss: 0.1187, step time: 0.1285\n",
      "182/223, train_loss: 0.1128, step time: 0.1148\n",
      "183/223, train_loss: 0.1101, step time: 0.1003\n",
      "184/223, train_loss: 0.1146, step time: 0.1218\n",
      "185/223, train_loss: 0.1102, step time: 0.1276\n",
      "186/223, train_loss: 0.1159, step time: 0.1175\n",
      "187/223, train_loss: 0.0989, step time: 0.1325\n",
      "188/223, train_loss: 0.1062, step time: 0.1436\n",
      "189/223, train_loss: 0.1210, step time: 0.1124\n",
      "190/223, train_loss: 0.1217, step time: 0.1063\n",
      "191/223, train_loss: 0.1168, step time: 0.1104\n",
      "192/223, train_loss: 0.1196, step time: 0.1066\n",
      "193/223, train_loss: 0.1086, step time: 0.1003\n",
      "194/223, train_loss: 0.1206, step time: 0.1053\n",
      "195/223, train_loss: 0.1024, step time: 0.1135\n",
      "196/223, train_loss: 0.1122, step time: 0.1130\n",
      "197/223, train_loss: 0.1051, step time: 0.1495\n",
      "198/223, train_loss: 0.1178, step time: 0.1196\n",
      "199/223, train_loss: 0.1196, step time: 0.1136\n",
      "200/223, train_loss: 0.1158, step time: 0.1072\n",
      "201/223, train_loss: 0.1142, step time: 0.1004\n",
      "202/223, train_loss: 0.1060, step time: 0.1120\n",
      "203/223, train_loss: 0.1067, step time: 0.1376\n",
      "204/223, train_loss: 0.1043, step time: 0.1032\n",
      "205/223, train_loss: 0.1105, step time: 0.1157\n",
      "206/223, train_loss: 0.1047, step time: 0.1218\n",
      "207/223, train_loss: 0.1165, step time: 0.1328\n",
      "208/223, train_loss: 0.1062, step time: 0.1259\n",
      "209/223, train_loss: 0.1100, step time: 0.1089\n",
      "210/223, train_loss: 0.1093, step time: 0.1049\n",
      "211/223, train_loss: 0.1222, step time: 0.1051\n",
      "212/223, train_loss: 0.1141, step time: 0.1012\n",
      "213/223, train_loss: 0.1199, step time: 0.1002\n",
      "214/223, train_loss: 0.1181, step time: 0.1007\n",
      "215/223, train_loss: 0.1201, step time: 0.1006\n",
      "216/223, train_loss: 0.0992, step time: 0.0993\n",
      "217/223, train_loss: 0.1236, step time: 0.1000\n",
      "218/223, train_loss: 0.1137, step time: 0.1006\n",
      "219/223, train_loss: 0.1164, step time: 0.0996\n",
      "220/223, train_loss: 0.1146, step time: 0.1002\n",
      "221/223, train_loss: 0.1357, step time: 0.1007\n",
      "222/223, train_loss: 0.1152, step time: 0.1006\n",
      "223/223, train_loss: 0.1062, step time: 0.0990\n",
      "epoch 104 average loss: 0.1142\n",
      "time consuming of epoch 104 is: 89.7357\n",
      "----------\n",
      "epoch 105/300\n",
      "1/223, train_loss: 0.1143, step time: 0.1308\n",
      "2/223, train_loss: 0.1070, step time: 0.1005\n",
      "3/223, train_loss: 0.1127, step time: 0.1126\n",
      "4/223, train_loss: 0.1015, step time: 0.1190\n",
      "5/223, train_loss: 0.1182, step time: 0.1239\n",
      "6/223, train_loss: 0.1064, step time: 0.1085\n",
      "7/223, train_loss: 0.1135, step time: 0.1108\n",
      "8/223, train_loss: 0.1139, step time: 0.1005\n",
      "9/223, train_loss: 0.1191, step time: 0.1106\n",
      "10/223, train_loss: 0.1136, step time: 0.1176\n",
      "11/223, train_loss: 0.1102, step time: 0.1176\n",
      "12/223, train_loss: 0.1103, step time: 0.1009\n",
      "13/223, train_loss: 0.1137, step time: 0.1080\n",
      "14/223, train_loss: 0.1164, step time: 0.1104\n",
      "15/223, train_loss: 0.1403, step time: 0.1244\n",
      "16/223, train_loss: 0.1069, step time: 0.1168\n",
      "17/223, train_loss: 0.1055, step time: 0.1012\n",
      "18/223, train_loss: 0.1091, step time: 0.1018\n",
      "19/223, train_loss: 0.1084, step time: 0.1202\n",
      "20/223, train_loss: 0.1117, step time: 0.1206\n",
      "21/223, train_loss: 0.1147, step time: 0.1078\n",
      "22/223, train_loss: 0.1270, step time: 0.1007\n",
      "23/223, train_loss: 0.1081, step time: 0.1069\n",
      "24/223, train_loss: 0.1142, step time: 0.1060\n",
      "25/223, train_loss: 0.1120, step time: 0.1144\n",
      "26/223, train_loss: 0.1062, step time: 0.1069\n",
      "27/223, train_loss: 0.1220, step time: 0.1205\n",
      "28/223, train_loss: 0.3082, step time: 0.1103\n",
      "29/223, train_loss: 0.1167, step time: 0.1000\n",
      "30/223, train_loss: 0.1057, step time: 0.1277\n",
      "31/223, train_loss: 0.1101, step time: 0.1111\n",
      "32/223, train_loss: 0.1216, step time: 0.1007\n",
      "33/223, train_loss: 0.0953, step time: 0.1004\n",
      "34/223, train_loss: 0.1236, step time: 0.0999\n",
      "35/223, train_loss: 0.1115, step time: 0.1019\n",
      "36/223, train_loss: 0.1019, step time: 0.1004\n",
      "37/223, train_loss: 0.1151, step time: 0.1195\n",
      "38/223, train_loss: 0.1207, step time: 0.1296\n",
      "39/223, train_loss: 0.1078, step time: 0.1165\n",
      "40/223, train_loss: 0.1156, step time: 0.1216\n",
      "41/223, train_loss: 0.1078, step time: 0.1314\n",
      "42/223, train_loss: 0.1122, step time: 0.1023\n",
      "43/223, train_loss: 0.1196, step time: 0.1719\n",
      "44/223, train_loss: 0.1168, step time: 0.1004\n",
      "45/223, train_loss: 0.1108, step time: 0.1253\n",
      "46/223, train_loss: 0.1053, step time: 0.1006\n",
      "47/223, train_loss: 0.1191, step time: 0.1159\n",
      "48/223, train_loss: 0.1189, step time: 0.1165\n",
      "49/223, train_loss: 0.1081, step time: 0.1057\n",
      "50/223, train_loss: 0.1159, step time: 0.1094\n",
      "51/223, train_loss: 0.1051, step time: 0.1217\n",
      "52/223, train_loss: 0.1021, step time: 0.1002\n",
      "53/223, train_loss: 0.1149, step time: 0.1046\n",
      "54/223, train_loss: 0.1105, step time: 0.1290\n",
      "55/223, train_loss: 0.1162, step time: 0.1116\n",
      "56/223, train_loss: 0.1262, step time: 0.1000\n",
      "57/223, train_loss: 0.1123, step time: 0.1176\n",
      "58/223, train_loss: 0.1165, step time: 0.1057\n",
      "59/223, train_loss: 0.1022, step time: 0.0998\n",
      "60/223, train_loss: 0.1055, step time: 0.1005\n",
      "61/223, train_loss: 0.1226, step time: 0.1005\n",
      "62/223, train_loss: 0.1180, step time: 0.1002\n",
      "63/223, train_loss: 0.1134, step time: 0.1131\n",
      "64/223, train_loss: 0.1108, step time: 0.1178\n",
      "65/223, train_loss: 0.1198, step time: 0.1008\n",
      "66/223, train_loss: 0.1107, step time: 0.0991\n",
      "67/223, train_loss: 0.1152, step time: 0.1009\n",
      "68/223, train_loss: 0.1023, step time: 0.1000\n",
      "69/223, train_loss: 0.1160, step time: 0.1075\n",
      "70/223, train_loss: 0.1124, step time: 0.1176\n",
      "71/223, train_loss: 0.1076, step time: 0.1075\n",
      "72/223, train_loss: 0.1111, step time: 0.1002\n",
      "73/223, train_loss: 0.1120, step time: 0.1115\n",
      "74/223, train_loss: 0.1235, step time: 0.1321\n",
      "75/223, train_loss: 0.1069, step time: 0.1004\n",
      "76/223, train_loss: 0.1164, step time: 0.1014\n",
      "77/223, train_loss: 0.1184, step time: 0.1220\n",
      "78/223, train_loss: 0.1002, step time: 0.1110\n",
      "79/223, train_loss: 0.1047, step time: 0.1047\n",
      "80/223, train_loss: 0.1108, step time: 0.1050\n",
      "81/223, train_loss: 0.1172, step time: 0.1031\n",
      "82/223, train_loss: 0.1117, step time: 0.1216\n",
      "83/223, train_loss: 0.1143, step time: 0.1008\n",
      "84/223, train_loss: 0.1361, step time: 0.0995\n",
      "85/223, train_loss: 0.1175, step time: 0.0998\n",
      "86/223, train_loss: 0.1203, step time: 0.1324\n",
      "87/223, train_loss: 0.1020, step time: 0.1000\n",
      "88/223, train_loss: 0.1137, step time: 0.1008\n",
      "89/223, train_loss: 0.1151, step time: 0.1000\n",
      "90/223, train_loss: 0.1065, step time: 0.1349\n",
      "91/223, train_loss: 0.1011, step time: 0.1501\n",
      "92/223, train_loss: 0.1067, step time: 0.1097\n",
      "93/223, train_loss: 0.1076, step time: 0.1075\n",
      "94/223, train_loss: 0.1096, step time: 0.1021\n",
      "95/223, train_loss: 0.1143, step time: 0.1063\n",
      "96/223, train_loss: 0.1104, step time: 0.1004\n",
      "97/223, train_loss: 0.1181, step time: 0.1119\n",
      "98/223, train_loss: 0.1292, step time: 0.1141\n",
      "99/223, train_loss: 0.1170, step time: 0.1003\n",
      "100/223, train_loss: 0.1051, step time: 0.1062\n",
      "101/223, train_loss: 0.1132, step time: 0.1124\n",
      "102/223, train_loss: 0.1047, step time: 0.1073\n",
      "103/223, train_loss: 0.1115, step time: 0.1466\n",
      "104/223, train_loss: 0.0993, step time: 0.1072\n",
      "105/223, train_loss: 0.1227, step time: 0.1066\n",
      "106/223, train_loss: 0.1058, step time: 0.1076\n",
      "107/223, train_loss: 0.1247, step time: 0.1054\n",
      "108/223, train_loss: 0.1050, step time: 0.1263\n",
      "109/223, train_loss: 0.1192, step time: 0.1094\n",
      "110/223, train_loss: 0.1121, step time: 0.1064\n",
      "111/223, train_loss: 0.1221, step time: 0.1136\n",
      "112/223, train_loss: 0.1125, step time: 0.1081\n",
      "113/223, train_loss: 0.1018, step time: 0.1169\n",
      "114/223, train_loss: 0.1241, step time: 0.1014\n",
      "115/223, train_loss: 0.1113, step time: 0.1065\n",
      "116/223, train_loss: 0.1182, step time: 0.1006\n",
      "117/223, train_loss: 0.1102, step time: 0.1150\n",
      "118/223, train_loss: 0.1162, step time: 0.1030\n",
      "119/223, train_loss: 0.1060, step time: 0.1154\n",
      "120/223, train_loss: 0.1074, step time: 0.1011\n",
      "121/223, train_loss: 0.1102, step time: 0.1182\n",
      "122/223, train_loss: 0.1079, step time: 0.1152\n",
      "123/223, train_loss: 0.1171, step time: 0.1026\n",
      "124/223, train_loss: 0.1073, step time: 0.1128\n",
      "125/223, train_loss: 0.1230, step time: 0.1106\n",
      "126/223, train_loss: 0.1203, step time: 0.1269\n",
      "127/223, train_loss: 0.1196, step time: 0.1082\n",
      "128/223, train_loss: 0.1115, step time: 0.1202\n",
      "129/223, train_loss: 0.1393, step time: 0.1089\n",
      "130/223, train_loss: 0.1065, step time: 0.1168\n",
      "131/223, train_loss: 0.1126, step time: 0.1333\n",
      "132/223, train_loss: 0.1102, step time: 0.1328\n",
      "133/223, train_loss: 0.1096, step time: 0.1050\n",
      "134/223, train_loss: 0.1125, step time: 0.1008\n",
      "135/223, train_loss: 0.1083, step time: 0.1097\n",
      "136/223, train_loss: 0.1125, step time: 0.1316\n",
      "137/223, train_loss: 0.1107, step time: 0.1132\n",
      "138/223, train_loss: 0.1130, step time: 0.1165\n",
      "139/223, train_loss: 0.1199, step time: 0.1105\n",
      "140/223, train_loss: 0.1157, step time: 0.1003\n",
      "141/223, train_loss: 0.1026, step time: 0.1050\n",
      "142/223, train_loss: 0.1145, step time: 0.1266\n",
      "143/223, train_loss: 0.1284, step time: 0.1178\n",
      "144/223, train_loss: 0.1038, step time: 0.1069\n",
      "145/223, train_loss: 0.1080, step time: 0.1087\n",
      "146/223, train_loss: 0.1091, step time: 0.1089\n",
      "147/223, train_loss: 0.1081, step time: 0.1087\n",
      "148/223, train_loss: 0.1198, step time: 0.1101\n",
      "149/223, train_loss: 0.1036, step time: 0.0998\n",
      "150/223, train_loss: 0.1017, step time: 0.1003\n",
      "151/223, train_loss: 0.1160, step time: 0.1003\n",
      "152/223, train_loss: 0.1089, step time: 0.1198\n",
      "153/223, train_loss: 0.1079, step time: 0.1132\n",
      "154/223, train_loss: 0.1253, step time: 0.1013\n",
      "155/223, train_loss: 0.1181, step time: 0.1013\n",
      "156/223, train_loss: 0.1179, step time: 0.1235\n",
      "157/223, train_loss: 0.1219, step time: 0.1260\n",
      "158/223, train_loss: 0.1388, step time: 0.1123\n",
      "159/223, train_loss: 0.1170, step time: 0.1023\n",
      "160/223, train_loss: 0.1120, step time: 0.1086\n",
      "161/223, train_loss: 0.1075, step time: 0.1132\n",
      "162/223, train_loss: 0.1006, step time: 0.1373\n",
      "163/223, train_loss: 0.1028, step time: 0.1005\n",
      "164/223, train_loss: 0.1098, step time: 0.1002\n",
      "165/223, train_loss: 0.1100, step time: 0.1017\n",
      "166/223, train_loss: 0.1197, step time: 0.1030\n",
      "167/223, train_loss: 0.1206, step time: 0.1050\n",
      "168/223, train_loss: 0.1262, step time: 0.1008\n",
      "169/223, train_loss: 0.1146, step time: 0.1260\n",
      "170/223, train_loss: 0.1167, step time: 0.1185\n",
      "171/223, train_loss: 0.1131, step time: 0.1008\n",
      "172/223, train_loss: 0.1281, step time: 0.1041\n",
      "173/223, train_loss: 0.1152, step time: 0.1191\n",
      "174/223, train_loss: 0.1099, step time: 0.1114\n",
      "175/223, train_loss: 0.1117, step time: 0.1010\n",
      "176/223, train_loss: 0.1013, step time: 0.1148\n",
      "177/223, train_loss: 0.1175, step time: 0.1153\n",
      "178/223, train_loss: 0.1014, step time: 0.1153\n",
      "179/223, train_loss: 0.1050, step time: 0.1147\n",
      "180/223, train_loss: 0.1012, step time: 0.1136\n",
      "181/223, train_loss: 0.1052, step time: 0.1081\n",
      "182/223, train_loss: 0.1150, step time: 0.1046\n",
      "183/223, train_loss: 0.1126, step time: 0.1193\n",
      "184/223, train_loss: 0.1113, step time: 0.0999\n",
      "185/223, train_loss: 0.1285, step time: 0.1088\n",
      "186/223, train_loss: 0.1035, step time: 0.1200\n",
      "187/223, train_loss: 0.1065, step time: 0.1238\n",
      "188/223, train_loss: 0.1036, step time: 0.0997\n",
      "189/223, train_loss: 0.1215, step time: 0.1231\n",
      "190/223, train_loss: 0.1152, step time: 0.1002\n",
      "191/223, train_loss: 0.1075, step time: 0.1089\n",
      "192/223, train_loss: 0.1139, step time: 0.1154\n",
      "193/223, train_loss: 0.1201, step time: 0.1153\n",
      "194/223, train_loss: 0.1085, step time: 0.1681\n",
      "195/223, train_loss: 0.1080, step time: 0.1251\n",
      "196/223, train_loss: 0.0998, step time: 0.1053\n",
      "197/223, train_loss: 0.1065, step time: 0.1082\n",
      "198/223, train_loss: 0.1114, step time: 0.0998\n",
      "199/223, train_loss: 0.1173, step time: 0.1149\n",
      "200/223, train_loss: 0.1153, step time: 0.1226\n",
      "201/223, train_loss: 0.1112, step time: 0.1176\n",
      "202/223, train_loss: 0.1163, step time: 0.1041\n",
      "203/223, train_loss: 0.1073, step time: 0.1214\n",
      "204/223, train_loss: 0.1160, step time: 0.1037\n",
      "205/223, train_loss: 0.0964, step time: 0.1010\n",
      "206/223, train_loss: 0.1193, step time: 0.1068\n",
      "207/223, train_loss: 0.1153, step time: 0.1005\n",
      "208/223, train_loss: 0.1029, step time: 0.1012\n",
      "209/223, train_loss: 0.1074, step time: 0.0992\n",
      "210/223, train_loss: 0.1099, step time: 0.0994\n",
      "211/223, train_loss: 0.1328, step time: 0.1005\n",
      "212/223, train_loss: 0.0989, step time: 0.1018\n",
      "213/223, train_loss: 0.1260, step time: 0.0989\n",
      "214/223, train_loss: 0.1026, step time: 0.0994\n",
      "215/223, train_loss: 0.1132, step time: 0.0999\n",
      "216/223, train_loss: 0.1129, step time: 0.1022\n",
      "217/223, train_loss: 0.1112, step time: 0.0994\n",
      "218/223, train_loss: 0.1165, step time: 0.0997\n",
      "219/223, train_loss: 0.0955, step time: 0.0994\n",
      "220/223, train_loss: 0.1175, step time: 0.1009\n",
      "221/223, train_loss: 0.1104, step time: 0.0994\n",
      "222/223, train_loss: 0.1058, step time: 0.0990\n",
      "223/223, train_loss: 0.1120, step time: 0.1005\n",
      "epoch 105 average loss: 0.1136\n",
      "current epoch: 105 current mean dice: 0.8492 tc: 0.9153 wt: 0.8594 et: 0.7729\n",
      "best mean dice: 0.8507 at epoch: 100\n",
      "time consuming of epoch 105 is: 92.6722\n",
      "----------\n",
      "epoch 106/300\n",
      "1/223, train_loss: 0.1271, step time: 0.1017\n",
      "2/223, train_loss: 0.1082, step time: 0.1131\n",
      "3/223, train_loss: 0.1278, step time: 0.1118\n",
      "4/223, train_loss: 0.1027, step time: 0.1143\n",
      "5/223, train_loss: 0.1046, step time: 0.1089\n",
      "6/223, train_loss: 0.1280, step time: 0.1097\n",
      "7/223, train_loss: 0.1108, step time: 0.1142\n",
      "8/223, train_loss: 0.1076, step time: 0.1065\n",
      "9/223, train_loss: 0.1126, step time: 0.1167\n",
      "10/223, train_loss: 0.1136, step time: 0.1015\n",
      "11/223, train_loss: 0.1110, step time: 0.1238\n",
      "12/223, train_loss: 0.1113, step time: 0.1056\n",
      "13/223, train_loss: 0.1148, step time: 0.1118\n",
      "14/223, train_loss: 0.1001, step time: 0.1071\n",
      "15/223, train_loss: 0.1025, step time: 0.1068\n",
      "16/223, train_loss: 0.1035, step time: 0.0992\n",
      "17/223, train_loss: 0.1014, step time: 0.0996\n",
      "18/223, train_loss: 0.1106, step time: 0.0991\n",
      "19/223, train_loss: 0.1203, step time: 0.1012\n",
      "20/223, train_loss: 0.1197, step time: 0.1033\n",
      "21/223, train_loss: 0.1082, step time: 0.1205\n",
      "22/223, train_loss: 0.3067, step time: 0.1039\n",
      "23/223, train_loss: 0.1009, step time: 0.1002\n",
      "24/223, train_loss: 0.1195, step time: 0.1209\n",
      "25/223, train_loss: 0.1089, step time: 0.1037\n",
      "26/223, train_loss: 0.1063, step time: 0.1008\n",
      "27/223, train_loss: 0.1032, step time: 0.0998\n",
      "28/223, train_loss: 0.1132, step time: 0.1273\n",
      "29/223, train_loss: 0.1143, step time: 0.1141\n",
      "30/223, train_loss: 0.1071, step time: 0.1122\n",
      "31/223, train_loss: 0.1208, step time: 0.1309\n",
      "32/223, train_loss: 0.1104, step time: 0.1068\n",
      "33/223, train_loss: 0.1254, step time: 0.1182\n",
      "34/223, train_loss: 0.1134, step time: 0.1238\n",
      "35/223, train_loss: 0.1247, step time: 0.1101\n",
      "36/223, train_loss: 0.1132, step time: 0.1508\n",
      "37/223, train_loss: 0.1144, step time: 0.1172\n",
      "38/223, train_loss: 0.1098, step time: 0.1056\n",
      "39/223, train_loss: 0.1091, step time: 0.1174\n",
      "40/223, train_loss: 0.1080, step time: 0.1102\n",
      "41/223, train_loss: 0.1159, step time: 0.1002\n",
      "42/223, train_loss: 0.1144, step time: 0.1261\n",
      "43/223, train_loss: 0.1173, step time: 0.1422\n",
      "44/223, train_loss: 0.1160, step time: 0.1095\n",
      "45/223, train_loss: 0.1101, step time: 0.1079\n",
      "46/223, train_loss: 0.1047, step time: 0.1025\n",
      "47/223, train_loss: 0.1076, step time: 0.1366\n",
      "48/223, train_loss: 0.1202, step time: 0.1039\n",
      "49/223, train_loss: 0.1007, step time: 0.1016\n",
      "50/223, train_loss: 0.1235, step time: 0.1123\n",
      "51/223, train_loss: 0.1136, step time: 0.1262\n",
      "52/223, train_loss: 0.1193, step time: 0.1103\n",
      "53/223, train_loss: 0.1212, step time: 0.0999\n",
      "54/223, train_loss: 0.1225, step time: 0.1143\n",
      "55/223, train_loss: 0.1247, step time: 0.1117\n",
      "56/223, train_loss: 0.1154, step time: 0.0995\n",
      "57/223, train_loss: 0.1139, step time: 0.1062\n",
      "58/223, train_loss: 0.1165, step time: 0.1210\n",
      "59/223, train_loss: 0.1150, step time: 0.1087\n",
      "60/223, train_loss: 0.1140, step time: 0.1008\n",
      "61/223, train_loss: 0.1123, step time: 0.1136\n",
      "62/223, train_loss: 0.1114, step time: 0.1075\n",
      "63/223, train_loss: 0.1084, step time: 0.1139\n",
      "64/223, train_loss: 0.1007, step time: 0.1243\n",
      "65/223, train_loss: 0.1130, step time: 0.1230\n",
      "66/223, train_loss: 0.1222, step time: 0.1071\n",
      "67/223, train_loss: 0.1220, step time: 0.1005\n",
      "68/223, train_loss: 0.1067, step time: 0.1037\n",
      "69/223, train_loss: 0.1152, step time: 0.1085\n",
      "70/223, train_loss: 0.1008, step time: 0.1058\n",
      "71/223, train_loss: 0.1056, step time: 0.1033\n",
      "72/223, train_loss: 0.1197, step time: 0.0996\n",
      "73/223, train_loss: 0.1039, step time: 0.1104\n",
      "74/223, train_loss: 0.1102, step time: 0.1228\n",
      "75/223, train_loss: 0.1094, step time: 0.1170\n",
      "76/223, train_loss: 0.1074, step time: 0.1102\n",
      "77/223, train_loss: 0.1061, step time: 0.1100\n",
      "78/223, train_loss: 0.1166, step time: 0.1271\n",
      "79/223, train_loss: 0.1055, step time: 0.1157\n",
      "80/223, train_loss: 0.1133, step time: 0.1003\n",
      "81/223, train_loss: 0.1083, step time: 0.1068\n",
      "82/223, train_loss: 0.1045, step time: 0.1101\n",
      "83/223, train_loss: 0.1015, step time: 0.1099\n",
      "84/223, train_loss: 0.1023, step time: 0.1008\n",
      "85/223, train_loss: 0.1126, step time: 0.1099\n",
      "86/223, train_loss: 0.1043, step time: 0.1119\n",
      "87/223, train_loss: 0.1065, step time: 0.1144\n",
      "88/223, train_loss: 0.1065, step time: 0.1102\n",
      "89/223, train_loss: 0.1123, step time: 0.1061\n",
      "90/223, train_loss: 0.1223, step time: 0.1391\n",
      "91/223, train_loss: 0.1270, step time: 0.1041\n",
      "92/223, train_loss: 0.1171, step time: 0.1137\n",
      "93/223, train_loss: 0.1079, step time: 0.1147\n",
      "94/223, train_loss: 0.1278, step time: 0.1087\n",
      "95/223, train_loss: 0.1180, step time: 0.1001\n",
      "96/223, train_loss: 0.1110, step time: 0.1128\n",
      "97/223, train_loss: 0.1247, step time: 0.1327\n",
      "98/223, train_loss: 0.1019, step time: 0.1029\n",
      "99/223, train_loss: 0.1317, step time: 0.1002\n",
      "100/223, train_loss: 0.1084, step time: 0.0993\n",
      "101/223, train_loss: 0.1163, step time: 0.1074\n",
      "102/223, train_loss: 0.1093, step time: 0.1005\n",
      "103/223, train_loss: 0.1083, step time: 0.1132\n",
      "104/223, train_loss: 0.1221, step time: 0.1041\n",
      "105/223, train_loss: 0.1094, step time: 0.1091\n",
      "106/223, train_loss: 0.1135, step time: 0.1024\n",
      "107/223, train_loss: 0.1186, step time: 0.1047\n",
      "108/223, train_loss: 0.1086, step time: 0.1136\n",
      "109/223, train_loss: 0.1084, step time: 0.1135\n",
      "110/223, train_loss: 0.1171, step time: 0.1003\n",
      "111/223, train_loss: 0.1255, step time: 0.1006\n",
      "112/223, train_loss: 0.1133, step time: 0.1000\n",
      "113/223, train_loss: 0.1062, step time: 0.1103\n",
      "114/223, train_loss: 0.1038, step time: 0.1044\n",
      "115/223, train_loss: 0.1074, step time: 0.1080\n",
      "116/223, train_loss: 0.1397, step time: 0.1030\n",
      "117/223, train_loss: 0.1078, step time: 0.1098\n",
      "118/223, train_loss: 0.1185, step time: 0.1001\n",
      "119/223, train_loss: 0.1102, step time: 0.1047\n",
      "120/223, train_loss: 0.0992, step time: 0.1041\n",
      "121/223, train_loss: 0.1069, step time: 0.1035\n",
      "122/223, train_loss: 0.1052, step time: 0.1037\n",
      "123/223, train_loss: 0.1109, step time: 0.1068\n",
      "124/223, train_loss: 0.0988, step time: 0.1019\n",
      "125/223, train_loss: 0.1115, step time: 0.1290\n",
      "126/223, train_loss: 0.1076, step time: 0.1095\n",
      "127/223, train_loss: 0.1088, step time: 0.1189\n",
      "128/223, train_loss: 0.1122, step time: 0.1011\n",
      "129/223, train_loss: 0.1287, step time: 0.1122\n",
      "130/223, train_loss: 0.1031, step time: 0.1269\n",
      "131/223, train_loss: 0.1090, step time: 0.1103\n",
      "132/223, train_loss: 0.1101, step time: 0.1122\n",
      "133/223, train_loss: 0.1117, step time: 0.1293\n",
      "134/223, train_loss: 0.0971, step time: 0.1465\n",
      "135/223, train_loss: 0.1269, step time: 0.1218\n",
      "136/223, train_loss: 0.1164, step time: 0.1125\n",
      "137/223, train_loss: 0.1132, step time: 0.1125\n",
      "138/223, train_loss: 0.1128, step time: 0.1058\n",
      "139/223, train_loss: 0.1279, step time: 0.1067\n",
      "140/223, train_loss: 0.1091, step time: 0.1012\n",
      "141/223, train_loss: 0.1048, step time: 0.1120\n",
      "142/223, train_loss: 0.1226, step time: 0.1025\n",
      "143/223, train_loss: 0.1231, step time: 0.1163\n",
      "144/223, train_loss: 0.1028, step time: 0.1328\n",
      "145/223, train_loss: 0.1253, step time: 0.1001\n",
      "146/223, train_loss: 0.1081, step time: 0.0993\n",
      "147/223, train_loss: 0.1070, step time: 0.1007\n",
      "148/223, train_loss: 0.1018, step time: 0.1008\n",
      "149/223, train_loss: 0.1228, step time: 0.1076\n",
      "150/223, train_loss: 0.1196, step time: 0.1002\n",
      "151/223, train_loss: 0.1116, step time: 0.0996\n",
      "152/223, train_loss: 0.1070, step time: 0.1127\n",
      "153/223, train_loss: 0.1092, step time: 0.1136\n",
      "154/223, train_loss: 0.1128, step time: 0.1117\n",
      "155/223, train_loss: 0.1220, step time: 0.1159\n",
      "156/223, train_loss: 0.1164, step time: 0.1097\n",
      "157/223, train_loss: 0.1145, step time: 0.1066\n",
      "158/223, train_loss: 0.1033, step time: 0.1107\n",
      "159/223, train_loss: 0.1083, step time: 0.1270\n",
      "160/223, train_loss: 0.1093, step time: 0.1026\n",
      "161/223, train_loss: 0.1141, step time: 0.1136\n",
      "162/223, train_loss: 0.1170, step time: 0.1100\n",
      "163/223, train_loss: 0.1135, step time: 0.1228\n",
      "164/223, train_loss: 0.0983, step time: 0.1166\n",
      "165/223, train_loss: 0.1115, step time: 0.1102\n",
      "166/223, train_loss: 0.1059, step time: 0.1008\n",
      "167/223, train_loss: 0.1181, step time: 0.1029\n",
      "168/223, train_loss: 0.1197, step time: 0.1004\n",
      "169/223, train_loss: 0.1085, step time: 0.1059\n",
      "170/223, train_loss: 0.1166, step time: 0.1101\n",
      "171/223, train_loss: 0.1140, step time: 0.1110\n",
      "172/223, train_loss: 0.1121, step time: 0.1107\n",
      "173/223, train_loss: 0.1021, step time: 0.1007\n",
      "174/223, train_loss: 0.1259, step time: 0.0999\n",
      "175/223, train_loss: 0.1114, step time: 0.1010\n",
      "176/223, train_loss: 0.1165, step time: 0.1027\n",
      "177/223, train_loss: 0.1112, step time: 0.1082\n",
      "178/223, train_loss: 0.1148, step time: 0.1002\n",
      "179/223, train_loss: 0.1098, step time: 0.1001\n",
      "180/223, train_loss: 0.1186, step time: 0.1005\n",
      "181/223, train_loss: 0.1109, step time: 0.1031\n",
      "182/223, train_loss: 0.1071, step time: 0.1004\n",
      "183/223, train_loss: 0.1104, step time: 0.1013\n",
      "184/223, train_loss: 0.1148, step time: 0.1007\n",
      "185/223, train_loss: 0.1206, step time: 0.1107\n",
      "186/223, train_loss: 0.1070, step time: 0.1010\n",
      "187/223, train_loss: 0.1282, step time: 0.1005\n",
      "188/223, train_loss: 0.1036, step time: 0.1334\n",
      "189/223, train_loss: 0.1120, step time: 0.1102\n",
      "190/223, train_loss: 0.1253, step time: 0.1003\n",
      "191/223, train_loss: 0.1029, step time: 0.1003\n",
      "192/223, train_loss: 0.1177, step time: 0.1062\n",
      "193/223, train_loss: 0.1108, step time: 0.1294\n",
      "194/223, train_loss: 0.1291, step time: 0.1015\n",
      "195/223, train_loss: 0.1192, step time: 0.1642\n",
      "196/223, train_loss: 0.1251, step time: 0.1236\n",
      "197/223, train_loss: 0.1135, step time: 0.1146\n",
      "198/223, train_loss: 0.1054, step time: 0.1004\n",
      "199/223, train_loss: 0.1153, step time: 0.1009\n",
      "200/223, train_loss: 0.1100, step time: 0.1185\n",
      "201/223, train_loss: 0.1147, step time: 0.1004\n",
      "202/223, train_loss: 0.1184, step time: 0.1003\n",
      "203/223, train_loss: 0.1083, step time: 0.0998\n",
      "204/223, train_loss: 0.1057, step time: 0.1043\n",
      "205/223, train_loss: 0.1005, step time: 0.1020\n",
      "206/223, train_loss: 0.1106, step time: 0.1005\n",
      "207/223, train_loss: 0.1046, step time: 0.1053\n",
      "208/223, train_loss: 0.1100, step time: 0.1027\n",
      "209/223, train_loss: 0.1205, step time: 0.1124\n",
      "210/223, train_loss: 0.1304, step time: 0.1083\n",
      "211/223, train_loss: 0.1095, step time: 0.1136\n",
      "212/223, train_loss: 0.1043, step time: 0.0995\n",
      "213/223, train_loss: 0.1022, step time: 0.1079\n",
      "214/223, train_loss: 0.1174, step time: 0.1060\n",
      "215/223, train_loss: 0.1056, step time: 0.1183\n",
      "216/223, train_loss: 0.1193, step time: 0.1031\n",
      "217/223, train_loss: 0.1241, step time: 0.1196\n",
      "218/223, train_loss: 0.1112, step time: 0.1042\n",
      "219/223, train_loss: 0.1121, step time: 0.1149\n",
      "220/223, train_loss: 0.1151, step time: 0.1161\n",
      "221/223, train_loss: 0.1159, step time: 0.0992\n",
      "222/223, train_loss: 0.1159, step time: 0.0999\n",
      "223/223, train_loss: 0.1270, step time: 0.1006\n",
      "epoch 106 average loss: 0.1137\n",
      "time consuming of epoch 106 is: 87.2880\n",
      "----------\n",
      "epoch 107/300\n",
      "1/223, train_loss: 0.1069, step time: 0.1005\n",
      "2/223, train_loss: 0.1089, step time: 0.0995\n",
      "3/223, train_loss: 0.1118, step time: 0.0997\n",
      "4/223, train_loss: 0.1198, step time: 0.0995\n",
      "5/223, train_loss: 0.1120, step time: 0.1064\n",
      "6/223, train_loss: 0.1144, step time: 0.1000\n",
      "7/223, train_loss: 0.1034, step time: 0.1003\n",
      "8/223, train_loss: 0.1220, step time: 0.1016\n",
      "9/223, train_loss: 0.1131, step time: 0.1124\n",
      "10/223, train_loss: 0.1069, step time: 0.1429\n",
      "11/223, train_loss: 0.1114, step time: 0.1057\n",
      "12/223, train_loss: 0.1045, step time: 0.1003\n",
      "13/223, train_loss: 0.1046, step time: 0.1187\n",
      "14/223, train_loss: 0.1270, step time: 0.1238\n",
      "15/223, train_loss: 0.1059, step time: 0.1238\n",
      "16/223, train_loss: 0.1003, step time: 0.1130\n",
      "17/223, train_loss: 0.1174, step time: 0.1091\n",
      "18/223, train_loss: 0.1108, step time: 0.1055\n",
      "19/223, train_loss: 0.1014, step time: 0.1154\n",
      "20/223, train_loss: 0.1064, step time: 0.1202\n",
      "21/223, train_loss: 0.1072, step time: 0.1265\n",
      "22/223, train_loss: 0.1097, step time: 0.1094\n",
      "23/223, train_loss: 0.1091, step time: 0.1031\n",
      "24/223, train_loss: 0.1022, step time: 0.0996\n",
      "25/223, train_loss: 0.1085, step time: 0.1214\n",
      "26/223, train_loss: 0.1176, step time: 0.1188\n",
      "27/223, train_loss: 0.1150, step time: 0.0992\n",
      "28/223, train_loss: 0.1076, step time: 0.1036\n",
      "29/223, train_loss: 0.1029, step time: 0.1147\n",
      "30/223, train_loss: 0.1077, step time: 0.1355\n",
      "31/223, train_loss: 0.1102, step time: 0.1076\n",
      "32/223, train_loss: 0.1177, step time: 0.1146\n",
      "33/223, train_loss: 0.1125, step time: 0.1050\n",
      "34/223, train_loss: 0.1030, step time: 0.1048\n",
      "35/223, train_loss: 0.1095, step time: 0.1517\n",
      "36/223, train_loss: 0.1103, step time: 0.1133\n",
      "37/223, train_loss: 0.1020, step time: 0.1127\n",
      "38/223, train_loss: 0.1113, step time: 0.1124\n",
      "39/223, train_loss: 0.1186, step time: 0.1254\n",
      "40/223, train_loss: 0.1118, step time: 0.1204\n",
      "41/223, train_loss: 0.1183, step time: 0.1158\n",
      "42/223, train_loss: 0.1151, step time: 0.1035\n",
      "43/223, train_loss: 0.1218, step time: 0.1077\n",
      "44/223, train_loss: 0.1024, step time: 0.1073\n",
      "45/223, train_loss: 0.1273, step time: 0.1042\n",
      "46/223, train_loss: 0.1140, step time: 0.1038\n",
      "47/223, train_loss: 0.1059, step time: 0.1205\n",
      "48/223, train_loss: 0.1221, step time: 0.0997\n",
      "49/223, train_loss: 0.0983, step time: 0.1001\n",
      "50/223, train_loss: 0.1127, step time: 0.1004\n",
      "51/223, train_loss: 0.1143, step time: 0.1003\n",
      "52/223, train_loss: 0.1034, step time: 0.1004\n",
      "53/223, train_loss: 0.1177, step time: 0.1002\n",
      "54/223, train_loss: 0.1124, step time: 0.1004\n",
      "55/223, train_loss: 0.1071, step time: 0.1198\n",
      "56/223, train_loss: 0.1184, step time: 0.1111\n",
      "57/223, train_loss: 0.1043, step time: 0.1223\n",
      "58/223, train_loss: 0.1286, step time: 0.1101\n",
      "59/223, train_loss: 0.1048, step time: 0.1230\n",
      "60/223, train_loss: 0.1097, step time: 0.1000\n",
      "61/223, train_loss: 0.1186, step time: 0.1072\n",
      "62/223, train_loss: 0.1084, step time: 0.1143\n",
      "63/223, train_loss: 0.1072, step time: 0.1259\n",
      "64/223, train_loss: 0.3053, step time: 0.1227\n",
      "65/223, train_loss: 0.1254, step time: 0.1039\n",
      "66/223, train_loss: 0.1004, step time: 0.1031\n",
      "67/223, train_loss: 0.1164, step time: 0.1160\n",
      "68/223, train_loss: 0.1051, step time: 0.1039\n",
      "69/223, train_loss: 0.1099, step time: 0.1290\n",
      "70/223, train_loss: 0.1156, step time: 0.1070\n",
      "71/223, train_loss: 0.1139, step time: 0.1167\n",
      "72/223, train_loss: 0.1011, step time: 0.1025\n",
      "73/223, train_loss: 0.1232, step time: 0.1166\n",
      "74/223, train_loss: 0.1067, step time: 0.1002\n",
      "75/223, train_loss: 0.1204, step time: 0.1072\n",
      "76/223, train_loss: 0.1152, step time: 0.1165\n",
      "77/223, train_loss: 0.1324, step time: 0.1059\n",
      "78/223, train_loss: 0.1254, step time: 0.1005\n",
      "79/223, train_loss: 0.1043, step time: 0.1120\n",
      "80/223, train_loss: 0.1069, step time: 0.1004\n",
      "81/223, train_loss: 0.1128, step time: 0.1238\n",
      "82/223, train_loss: 0.1155, step time: 0.1130\n",
      "83/223, train_loss: 0.1172, step time: 0.1190\n",
      "84/223, train_loss: 0.1068, step time: 0.1001\n",
      "85/223, train_loss: 0.0982, step time: 0.1215\n",
      "86/223, train_loss: 0.1196, step time: 0.1104\n",
      "87/223, train_loss: 0.1095, step time: 0.1198\n",
      "88/223, train_loss: 0.1186, step time: 0.1003\n",
      "89/223, train_loss: 0.1175, step time: 0.1109\n",
      "90/223, train_loss: 0.1190, step time: 0.1004\n",
      "91/223, train_loss: 0.1206, step time: 0.1228\n",
      "92/223, train_loss: 0.1090, step time: 0.1095\n",
      "93/223, train_loss: 0.1078, step time: 0.1145\n",
      "94/223, train_loss: 0.1037, step time: 0.1032\n",
      "95/223, train_loss: 0.1170, step time: 0.1286\n",
      "96/223, train_loss: 0.1000, step time: 0.1083\n",
      "97/223, train_loss: 0.1163, step time: 0.1118\n",
      "98/223, train_loss: 0.1110, step time: 0.1155\n",
      "99/223, train_loss: 0.1097, step time: 0.1166\n",
      "100/223, train_loss: 0.1193, step time: 0.1123\n",
      "101/223, train_loss: 0.1071, step time: 0.1056\n",
      "102/223, train_loss: 0.1260, step time: 0.1174\n",
      "103/223, train_loss: 0.1121, step time: 0.1191\n",
      "104/223, train_loss: 0.1004, step time: 0.1165\n",
      "105/223, train_loss: 0.1213, step time: 0.0989\n",
      "106/223, train_loss: 0.1091, step time: 0.0989\n",
      "107/223, train_loss: 0.1175, step time: 0.1008\n",
      "108/223, train_loss: 0.1004, step time: 0.1138\n",
      "109/223, train_loss: 0.1146, step time: 0.0999\n",
      "110/223, train_loss: 0.1348, step time: 0.0994\n",
      "111/223, train_loss: 0.1203, step time: 0.1002\n",
      "112/223, train_loss: 0.1033, step time: 0.1127\n",
      "113/223, train_loss: 0.1156, step time: 0.1004\n",
      "114/223, train_loss: 0.1134, step time: 0.1102\n",
      "115/223, train_loss: 0.1071, step time: 0.0996\n",
      "116/223, train_loss: 0.1121, step time: 0.0999\n",
      "117/223, train_loss: 0.1205, step time: 0.1223\n",
      "118/223, train_loss: 0.1129, step time: 0.1031\n",
      "119/223, train_loss: 0.1212, step time: 0.1003\n",
      "120/223, train_loss: 0.1147, step time: 0.1038\n",
      "121/223, train_loss: 0.1123, step time: 0.1126\n",
      "122/223, train_loss: 0.1006, step time: 0.1118\n",
      "123/223, train_loss: 0.1183, step time: 0.1019\n",
      "124/223, train_loss: 0.1156, step time: 0.1016\n",
      "125/223, train_loss: 0.1077, step time: 0.1016\n",
      "126/223, train_loss: 0.1061, step time: 0.1339\n",
      "127/223, train_loss: 0.1209, step time: 0.1110\n",
      "128/223, train_loss: 0.1055, step time: 0.1232\n",
      "129/223, train_loss: 0.1154, step time: 0.1147\n",
      "130/223, train_loss: 0.1191, step time: 0.1001\n",
      "131/223, train_loss: 0.1089, step time: 0.1294\n",
      "132/223, train_loss: 0.1163, step time: 0.1077\n",
      "133/223, train_loss: 0.1086, step time: 0.1239\n",
      "134/223, train_loss: 0.1071, step time: 0.1107\n",
      "135/223, train_loss: 0.1104, step time: 0.1191\n",
      "136/223, train_loss: 0.1079, step time: 0.1210\n",
      "137/223, train_loss: 0.1249, step time: 0.1176\n",
      "138/223, train_loss: 0.1033, step time: 0.1052\n",
      "139/223, train_loss: 0.1222, step time: 0.1065\n",
      "140/223, train_loss: 0.1146, step time: 0.1196\n",
      "141/223, train_loss: 0.1087, step time: 0.1021\n",
      "142/223, train_loss: 0.1096, step time: 0.1006\n",
      "143/223, train_loss: 0.1079, step time: 0.1001\n",
      "144/223, train_loss: 0.1170, step time: 0.1009\n",
      "145/223, train_loss: 0.1049, step time: 0.1001\n",
      "146/223, train_loss: 0.1136, step time: 0.1016\n",
      "147/223, train_loss: 0.1061, step time: 0.1077\n",
      "148/223, train_loss: 0.1056, step time: 0.1145\n",
      "149/223, train_loss: 0.1246, step time: 0.1169\n",
      "150/223, train_loss: 0.1181, step time: 0.1005\n",
      "151/223, train_loss: 0.1054, step time: 0.1015\n",
      "152/223, train_loss: 0.1245, step time: 0.1121\n",
      "153/223, train_loss: 0.1206, step time: 0.1004\n",
      "154/223, train_loss: 0.1283, step time: 0.1004\n",
      "155/223, train_loss: 0.1085, step time: 0.1007\n",
      "156/223, train_loss: 0.1053, step time: 0.1002\n",
      "157/223, train_loss: 0.1223, step time: 0.1121\n",
      "158/223, train_loss: 0.1034, step time: 0.1133\n",
      "159/223, train_loss: 0.1099, step time: 0.1018\n",
      "160/223, train_loss: 0.1039, step time: 0.1122\n",
      "161/223, train_loss: 0.1138, step time: 0.1237\n",
      "162/223, train_loss: 0.1173, step time: 0.1004\n",
      "163/223, train_loss: 0.1137, step time: 0.1003\n",
      "164/223, train_loss: 0.1224, step time: 0.1015\n",
      "165/223, train_loss: 0.1112, step time: 0.1068\n",
      "166/223, train_loss: 0.1057, step time: 0.1008\n",
      "167/223, train_loss: 0.1092, step time: 0.1011\n",
      "168/223, train_loss: 0.1028, step time: 0.1143\n",
      "169/223, train_loss: 0.1062, step time: 0.1106\n",
      "170/223, train_loss: 0.1037, step time: 0.1005\n",
      "171/223, train_loss: 0.1084, step time: 0.0999\n",
      "172/223, train_loss: 0.1273, step time: 0.1176\n",
      "173/223, train_loss: 0.1079, step time: 0.1137\n",
      "174/223, train_loss: 0.1136, step time: 0.1009\n",
      "175/223, train_loss: 0.1173, step time: 0.1002\n",
      "176/223, train_loss: 0.1050, step time: 0.1121\n",
      "177/223, train_loss: 0.1342, step time: 0.1023\n",
      "178/223, train_loss: 0.1221, step time: 0.1001\n",
      "179/223, train_loss: 0.1252, step time: 0.0995\n",
      "180/223, train_loss: 0.1207, step time: 0.1147\n",
      "181/223, train_loss: 0.1140, step time: 0.1027\n",
      "182/223, train_loss: 0.1124, step time: 0.1005\n",
      "183/223, train_loss: 0.0992, step time: 0.1326\n",
      "184/223, train_loss: 0.1126, step time: 0.1004\n",
      "185/223, train_loss: 0.1158, step time: 0.1200\n",
      "186/223, train_loss: 0.1073, step time: 0.1166\n",
      "187/223, train_loss: 0.1168, step time: 0.1282\n",
      "188/223, train_loss: 0.1098, step time: 0.1083\n",
      "189/223, train_loss: 0.1138, step time: 0.1149\n",
      "190/223, train_loss: 0.1229, step time: 0.1054\n",
      "191/223, train_loss: 0.1289, step time: 0.0996\n",
      "192/223, train_loss: 0.1159, step time: 0.1152\n",
      "193/223, train_loss: 0.1062, step time: 0.1002\n",
      "194/223, train_loss: 0.1264, step time: 0.1126\n",
      "195/223, train_loss: 0.1142, step time: 0.1001\n",
      "196/223, train_loss: 0.1195, step time: 0.1098\n",
      "197/223, train_loss: 0.1161, step time: 0.1175\n",
      "198/223, train_loss: 0.1060, step time: 0.1011\n",
      "199/223, train_loss: 0.1029, step time: 0.1148\n",
      "200/223, train_loss: 0.1099, step time: 0.1091\n",
      "201/223, train_loss: 0.1280, step time: 0.1021\n",
      "202/223, train_loss: 0.1145, step time: 0.1136\n",
      "203/223, train_loss: 0.1090, step time: 0.1050\n",
      "204/223, train_loss: 0.1167, step time: 0.0999\n",
      "205/223, train_loss: 0.1247, step time: 0.1102\n",
      "206/223, train_loss: 0.1168, step time: 0.1011\n",
      "207/223, train_loss: 0.1098, step time: 0.1008\n",
      "208/223, train_loss: 0.0977, step time: 0.1055\n",
      "209/223, train_loss: 0.1084, step time: 0.1011\n",
      "210/223, train_loss: 0.1041, step time: 0.1003\n",
      "211/223, train_loss: 0.1051, step time: 0.1012\n",
      "212/223, train_loss: 0.1194, step time: 0.1007\n",
      "213/223, train_loss: 0.1064, step time: 0.1002\n",
      "214/223, train_loss: 0.1147, step time: 0.1012\n",
      "215/223, train_loss: 0.1328, step time: 0.1169\n",
      "216/223, train_loss: 0.1150, step time: 0.1004\n",
      "217/223, train_loss: 0.1038, step time: 0.1079\n",
      "218/223, train_loss: 0.1162, step time: 0.1001\n",
      "219/223, train_loss: 0.1147, step time: 0.1008\n",
      "220/223, train_loss: 0.1246, step time: 0.1001\n",
      "221/223, train_loss: 0.1216, step time: 0.0992\n",
      "222/223, train_loss: 0.1134, step time: 0.0998\n",
      "223/223, train_loss: 0.1190, step time: 0.0997\n",
      "epoch 107 average loss: 0.1137\n",
      "time consuming of epoch 107 is: 90.1727\n",
      "----------\n",
      "epoch 108/300\n",
      "1/223, train_loss: 0.1104, step time: 0.1135\n",
      "2/223, train_loss: 0.1117, step time: 0.1044\n",
      "3/223, train_loss: 0.1190, step time: 0.1003\n",
      "4/223, train_loss: 0.1252, step time: 0.1003\n",
      "5/223, train_loss: 0.1166, step time: 0.1004\n",
      "6/223, train_loss: 0.1056, step time: 0.1001\n",
      "7/223, train_loss: 0.1145, step time: 0.0998\n",
      "8/223, train_loss: 0.1194, step time: 0.1001\n",
      "9/223, train_loss: 0.1151, step time: 0.1126\n",
      "10/223, train_loss: 0.1103, step time: 0.1135\n",
      "11/223, train_loss: 0.1056, step time: 0.1143\n",
      "12/223, train_loss: 0.0990, step time: 0.1005\n",
      "13/223, train_loss: 0.1239, step time: 0.1103\n",
      "14/223, train_loss: 0.1056, step time: 0.1102\n",
      "15/223, train_loss: 0.1139, step time: 0.1182\n",
      "16/223, train_loss: 0.1038, step time: 0.1151\n",
      "17/223, train_loss: 0.1126, step time: 0.1104\n",
      "18/223, train_loss: 0.1101, step time: 0.1205\n",
      "19/223, train_loss: 0.1144, step time: 0.1078\n",
      "20/223, train_loss: 0.1053, step time: 0.1087\n",
      "21/223, train_loss: 0.1298, step time: 0.1056\n",
      "22/223, train_loss: 0.0984, step time: 0.1085\n",
      "23/223, train_loss: 0.1163, step time: 0.1014\n",
      "24/223, train_loss: 0.1050, step time: 0.0996\n",
      "25/223, train_loss: 0.1098, step time: 0.0999\n",
      "26/223, train_loss: 0.1038, step time: 0.1086\n",
      "27/223, train_loss: 0.1141, step time: 0.1020\n",
      "28/223, train_loss: 0.1075, step time: 0.1041\n",
      "29/223, train_loss: 0.1160, step time: 0.1246\n",
      "30/223, train_loss: 0.1172, step time: 0.1078\n",
      "31/223, train_loss: 0.1144, step time: 0.1082\n",
      "32/223, train_loss: 0.1117, step time: 0.1034\n",
      "33/223, train_loss: 0.1237, step time: 0.1123\n",
      "34/223, train_loss: 0.1144, step time: 0.1174\n",
      "35/223, train_loss: 0.1149, step time: 0.1165\n",
      "36/223, train_loss: 0.1094, step time: 0.1009\n",
      "37/223, train_loss: 0.1180, step time: 0.1007\n",
      "38/223, train_loss: 0.1095, step time: 0.1091\n",
      "39/223, train_loss: 0.1145, step time: 0.1001\n",
      "40/223, train_loss: 0.0970, step time: 0.1001\n",
      "41/223, train_loss: 0.1162, step time: 0.1072\n",
      "42/223, train_loss: 0.1123, step time: 0.1078\n",
      "43/223, train_loss: 0.1065, step time: 0.0997\n",
      "44/223, train_loss: 0.0996, step time: 0.1007\n",
      "45/223, train_loss: 0.1206, step time: 0.1121\n",
      "46/223, train_loss: 0.1064, step time: 0.1068\n",
      "47/223, train_loss: 0.1213, step time: 0.1001\n",
      "48/223, train_loss: 0.1102, step time: 0.1053\n",
      "49/223, train_loss: 0.1071, step time: 0.1047\n",
      "50/223, train_loss: 0.1070, step time: 0.0994\n",
      "51/223, train_loss: 0.1050, step time: 0.1004\n",
      "52/223, train_loss: 0.1114, step time: 0.1011\n",
      "53/223, train_loss: 0.1112, step time: 0.1167\n",
      "54/223, train_loss: 0.3009, step time: 0.1097\n",
      "55/223, train_loss: 0.1050, step time: 0.1068\n",
      "56/223, train_loss: 0.1069, step time: 0.1012\n",
      "57/223, train_loss: 0.1152, step time: 0.1016\n",
      "58/223, train_loss: 0.1310, step time: 0.1006\n",
      "59/223, train_loss: 0.1001, step time: 0.0995\n",
      "60/223, train_loss: 0.1135, step time: 0.1249\n",
      "61/223, train_loss: 0.0998, step time: 0.1046\n",
      "62/223, train_loss: 0.1108, step time: 0.1107\n",
      "63/223, train_loss: 0.1234, step time: 0.1137\n",
      "64/223, train_loss: 0.1080, step time: 0.1010\n",
      "65/223, train_loss: 0.1261, step time: 0.1314\n",
      "66/223, train_loss: 0.1218, step time: 0.1047\n",
      "67/223, train_loss: 0.0998, step time: 0.1059\n",
      "68/223, train_loss: 0.1084, step time: 0.0985\n",
      "69/223, train_loss: 0.1063, step time: 0.1286\n",
      "70/223, train_loss: 0.1129, step time: 0.1033\n",
      "71/223, train_loss: 0.1041, step time: 0.1043\n",
      "72/223, train_loss: 0.1200, step time: 0.1330\n",
      "73/223, train_loss: 0.1077, step time: 0.1310\n",
      "74/223, train_loss: 0.1202, step time: 0.0996\n",
      "75/223, train_loss: 0.1128, step time: 0.1017\n",
      "76/223, train_loss: 0.1227, step time: 0.1326\n",
      "77/223, train_loss: 0.1172, step time: 0.1093\n",
      "78/223, train_loss: 0.1170, step time: 0.1253\n",
      "79/223, train_loss: 0.1031, step time: 0.1185\n",
      "80/223, train_loss: 0.1200, step time: 0.1008\n",
      "81/223, train_loss: 0.1108, step time: 0.1008\n",
      "82/223, train_loss: 0.1158, step time: 0.1257\n",
      "83/223, train_loss: 0.1070, step time: 0.1106\n",
      "84/223, train_loss: 0.1182, step time: 0.1016\n",
      "85/223, train_loss: 0.1271, step time: 0.1139\n",
      "86/223, train_loss: 0.1105, step time: 0.1055\n",
      "87/223, train_loss: 0.1239, step time: 0.1231\n",
      "88/223, train_loss: 0.1103, step time: 0.1288\n",
      "89/223, train_loss: 0.1166, step time: 0.1172\n",
      "90/223, train_loss: 0.1231, step time: 0.1142\n",
      "91/223, train_loss: 0.1061, step time: 0.1053\n",
      "92/223, train_loss: 0.1231, step time: 0.1304\n",
      "93/223, train_loss: 0.1232, step time: 0.1084\n",
      "94/223, train_loss: 0.1103, step time: 0.1005\n",
      "95/223, train_loss: 0.1003, step time: 0.0997\n",
      "96/223, train_loss: 0.1152, step time: 0.1000\n",
      "97/223, train_loss: 0.1023, step time: 0.1057\n",
      "98/223, train_loss: 0.1295, step time: 0.1264\n",
      "99/223, train_loss: 0.1142, step time: 0.1292\n",
      "100/223, train_loss: 0.1124, step time: 0.1062\n",
      "101/223, train_loss: 0.1130, step time: 0.1082\n",
      "102/223, train_loss: 0.1107, step time: 0.1121\n",
      "103/223, train_loss: 0.1053, step time: 0.1041\n",
      "104/223, train_loss: 0.1087, step time: 0.1055\n",
      "105/223, train_loss: 0.1057, step time: 0.1137\n",
      "106/223, train_loss: 0.1103, step time: 0.1201\n",
      "107/223, train_loss: 0.1161, step time: 0.1321\n",
      "108/223, train_loss: 0.1173, step time: 0.1024\n",
      "109/223, train_loss: 0.1137, step time: 0.0985\n",
      "110/223, train_loss: 0.1068, step time: 0.1059\n",
      "111/223, train_loss: 0.1106, step time: 0.1147\n",
      "112/223, train_loss: 0.1087, step time: 0.1064\n",
      "113/223, train_loss: 0.1238, step time: 0.1064\n",
      "114/223, train_loss: 0.1030, step time: 0.1118\n",
      "115/223, train_loss: 0.1154, step time: 0.1243\n",
      "116/223, train_loss: 0.1192, step time: 0.1228\n",
      "117/223, train_loss: 0.1169, step time: 0.1088\n",
      "118/223, train_loss: 0.1059, step time: 0.1323\n",
      "119/223, train_loss: 0.1061, step time: 0.1098\n",
      "120/223, train_loss: 0.1124, step time: 0.1111\n",
      "121/223, train_loss: 0.1277, step time: 0.0999\n",
      "122/223, train_loss: 0.1046, step time: 0.1111\n",
      "123/223, train_loss: 0.1022, step time: 0.1127\n",
      "124/223, train_loss: 0.1047, step time: 0.1151\n",
      "125/223, train_loss: 0.1080, step time: 0.1113\n",
      "126/223, train_loss: 0.1205, step time: 0.0997\n",
      "127/223, train_loss: 0.1202, step time: 0.1081\n",
      "128/223, train_loss: 0.1043, step time: 0.1132\n",
      "129/223, train_loss: 0.1265, step time: 0.1194\n",
      "130/223, train_loss: 0.1170, step time: 0.1079\n",
      "131/223, train_loss: 0.1196, step time: 0.1118\n",
      "132/223, train_loss: 0.0950, step time: 0.1399\n",
      "133/223, train_loss: 0.1116, step time: 0.1191\n",
      "134/223, train_loss: 0.1074, step time: 0.1096\n",
      "135/223, train_loss: 0.1071, step time: 0.1232\n",
      "136/223, train_loss: 0.1162, step time: 0.1061\n",
      "137/223, train_loss: 0.1039, step time: 0.1303\n",
      "138/223, train_loss: 0.1046, step time: 0.1058\n",
      "139/223, train_loss: 0.1140, step time: 0.1013\n",
      "140/223, train_loss: 0.1095, step time: 0.1010\n",
      "141/223, train_loss: 0.1028, step time: 0.1360\n",
      "142/223, train_loss: 0.1167, step time: 0.1111\n",
      "143/223, train_loss: 0.1141, step time: 0.1231\n",
      "144/223, train_loss: 0.1096, step time: 0.1079\n",
      "145/223, train_loss: 0.1215, step time: 0.1270\n",
      "146/223, train_loss: 0.1164, step time: 0.1016\n",
      "147/223, train_loss: 0.1128, step time: 0.1004\n",
      "148/223, train_loss: 0.1303, step time: 0.1179\n",
      "149/223, train_loss: 0.1199, step time: 0.1029\n",
      "150/223, train_loss: 0.1151, step time: 0.1061\n",
      "151/223, train_loss: 0.1258, step time: 0.1061\n",
      "152/223, train_loss: 0.1067, step time: 0.1016\n",
      "153/223, train_loss: 0.1214, step time: 0.1003\n",
      "154/223, train_loss: 0.1082, step time: 0.1138\n",
      "155/223, train_loss: 0.1148, step time: 0.1238\n",
      "156/223, train_loss: 0.1270, step time: 0.1255\n",
      "157/223, train_loss: 0.1083, step time: 0.1178\n",
      "158/223, train_loss: 0.1069, step time: 0.1223\n",
      "159/223, train_loss: 0.1145, step time: 0.1032\n",
      "160/223, train_loss: 0.1158, step time: 0.1124\n",
      "161/223, train_loss: 0.1085, step time: 0.1126\n",
      "162/223, train_loss: 0.1282, step time: 0.1430\n",
      "163/223, train_loss: 0.1110, step time: 0.1150\n",
      "164/223, train_loss: 0.1218, step time: 0.1008\n",
      "165/223, train_loss: 0.1170, step time: 0.1004\n",
      "166/223, train_loss: 0.1111, step time: 0.1133\n",
      "167/223, train_loss: 0.1105, step time: 0.1342\n",
      "168/223, train_loss: 0.1244, step time: 0.1002\n",
      "169/223, train_loss: 0.1167, step time: 0.1005\n",
      "170/223, train_loss: 0.1056, step time: 0.1234\n",
      "171/223, train_loss: 0.1016, step time: 0.1103\n",
      "172/223, train_loss: 0.1237, step time: 0.1049\n",
      "173/223, train_loss: 0.1106, step time: 0.1049\n",
      "174/223, train_loss: 0.1174, step time: 0.1005\n",
      "175/223, train_loss: 0.1220, step time: 0.1017\n",
      "176/223, train_loss: 0.1164, step time: 0.1129\n",
      "177/223, train_loss: 0.1220, step time: 0.1016\n",
      "178/223, train_loss: 0.1122, step time: 0.1249\n",
      "179/223, train_loss: 0.1137, step time: 0.1089\n",
      "180/223, train_loss: 0.1032, step time: 0.0994\n",
      "181/223, train_loss: 0.1118, step time: 0.0997\n",
      "182/223, train_loss: 0.1190, step time: 0.1238\n",
      "183/223, train_loss: 0.1156, step time: 0.1044\n",
      "184/223, train_loss: 0.1125, step time: 0.1007\n",
      "185/223, train_loss: 0.1097, step time: 0.1277\n",
      "186/223, train_loss: 0.1157, step time: 0.0985\n",
      "187/223, train_loss: 0.1000, step time: 0.1049\n",
      "188/223, train_loss: 0.1132, step time: 0.1183\n",
      "189/223, train_loss: 0.1200, step time: 0.1544\n",
      "190/223, train_loss: 0.1247, step time: 0.1103\n",
      "191/223, train_loss: 0.1079, step time: 0.1000\n",
      "192/223, train_loss: 0.1150, step time: 0.0986\n",
      "193/223, train_loss: 0.1148, step time: 0.1343\n",
      "194/223, train_loss: 0.1077, step time: 0.1033\n",
      "195/223, train_loss: 0.1110, step time: 0.0994\n",
      "196/223, train_loss: 0.1057, step time: 0.0987\n",
      "197/223, train_loss: 0.1058, step time: 0.1208\n",
      "198/223, train_loss: 0.1286, step time: 0.1134\n",
      "199/223, train_loss: 0.1124, step time: 0.1073\n",
      "200/223, train_loss: 0.1066, step time: 0.1115\n",
      "201/223, train_loss: 0.1083, step time: 0.1102\n",
      "202/223, train_loss: 0.1044, step time: 0.1060\n",
      "203/223, train_loss: 0.1039, step time: 0.1228\n",
      "204/223, train_loss: 0.1023, step time: 0.1021\n",
      "205/223, train_loss: 0.1153, step time: 0.1203\n",
      "206/223, train_loss: 0.1153, step time: 0.1092\n",
      "207/223, train_loss: 0.1217, step time: 0.1123\n",
      "208/223, train_loss: 0.1000, step time: 0.1115\n",
      "209/223, train_loss: 0.1056, step time: 0.1117\n",
      "210/223, train_loss: 0.1102, step time: 0.1101\n",
      "211/223, train_loss: 0.1073, step time: 0.0997\n",
      "212/223, train_loss: 0.1154, step time: 0.1131\n",
      "213/223, train_loss: 0.1277, step time: 0.1070\n",
      "214/223, train_loss: 0.1037, step time: 0.1129\n",
      "215/223, train_loss: 0.1122, step time: 0.1008\n",
      "216/223, train_loss: 0.1076, step time: 0.1145\n",
      "217/223, train_loss: 0.1084, step time: 0.1063\n",
      "218/223, train_loss: 0.1022, step time: 0.1015\n",
      "219/223, train_loss: 0.1165, step time: 0.1000\n",
      "220/223, train_loss: 0.1220, step time: 0.0998\n",
      "221/223, train_loss: 0.1136, step time: 0.0991\n",
      "222/223, train_loss: 0.1130, step time: 0.1008\n",
      "223/223, train_loss: 0.1074, step time: 0.0991\n",
      "epoch 108 average loss: 0.1135\n",
      "time consuming of epoch 108 is: 87.3480\n",
      "----------\n",
      "epoch 109/300\n",
      "1/223, train_loss: 0.1126, step time: 0.1122\n",
      "2/223, train_loss: 0.1097, step time: 0.1003\n",
      "3/223, train_loss: 0.1161, step time: 0.1158\n",
      "4/223, train_loss: 0.1052, step time: 0.1008\n",
      "5/223, train_loss: 0.1034, step time: 0.1096\n",
      "6/223, train_loss: 0.0986, step time: 0.1102\n",
      "7/223, train_loss: 0.1135, step time: 0.1150\n",
      "8/223, train_loss: 0.1032, step time: 0.1255\n",
      "9/223, train_loss: 0.1173, step time: 0.1016\n",
      "10/223, train_loss: 0.1134, step time: 0.1098\n",
      "11/223, train_loss: 0.1107, step time: 0.1088\n",
      "12/223, train_loss: 0.1171, step time: 0.1122\n",
      "13/223, train_loss: 0.1114, step time: 0.1076\n",
      "14/223, train_loss: 0.1049, step time: 0.1173\n",
      "15/223, train_loss: 0.1148, step time: 0.1043\n",
      "16/223, train_loss: 0.1122, step time: 0.1024\n",
      "17/223, train_loss: 0.1166, step time: 0.1205\n",
      "18/223, train_loss: 0.1282, step time: 0.1149\n",
      "19/223, train_loss: 0.1289, step time: 0.1122\n",
      "20/223, train_loss: 0.1170, step time: 0.1003\n",
      "21/223, train_loss: 0.1199, step time: 0.1313\n",
      "22/223, train_loss: 0.1221, step time: 0.1108\n",
      "23/223, train_loss: 0.1146, step time: 0.1008\n",
      "24/223, train_loss: 0.1189, step time: 0.1298\n",
      "25/223, train_loss: 0.1150, step time: 0.1111\n",
      "26/223, train_loss: 0.1252, step time: 0.1011\n",
      "27/223, train_loss: 0.1119, step time: 0.1000\n",
      "28/223, train_loss: 0.0998, step time: 0.1026\n",
      "29/223, train_loss: 0.1100, step time: 0.1267\n",
      "30/223, train_loss: 0.1089, step time: 0.1063\n",
      "31/223, train_loss: 0.1118, step time: 0.1350\n",
      "32/223, train_loss: 0.1031, step time: 0.1170\n",
      "33/223, train_loss: 0.1308, step time: 0.1149\n",
      "34/223, train_loss: 0.1072, step time: 0.1150\n",
      "35/223, train_loss: 0.1204, step time: 0.1145\n",
      "36/223, train_loss: 0.1211, step time: 0.1170\n",
      "37/223, train_loss: 0.1265, step time: 0.1013\n",
      "38/223, train_loss: 0.1081, step time: 0.1005\n",
      "39/223, train_loss: 0.1034, step time: 0.1001\n",
      "40/223, train_loss: 0.1153, step time: 0.1174\n",
      "41/223, train_loss: 0.1057, step time: 0.1151\n",
      "42/223, train_loss: 0.1041, step time: 0.1339\n",
      "43/223, train_loss: 0.1178, step time: 0.1154\n",
      "44/223, train_loss: 0.1010, step time: 0.1005\n",
      "45/223, train_loss: 0.1215, step time: 0.1143\n",
      "46/223, train_loss: 0.1043, step time: 0.1005\n",
      "47/223, train_loss: 0.1088, step time: 0.1015\n",
      "48/223, train_loss: 0.1228, step time: 0.1002\n",
      "49/223, train_loss: 0.0998, step time: 0.1160\n",
      "50/223, train_loss: 0.1275, step time: 0.1030\n",
      "51/223, train_loss: 0.1065, step time: 0.1106\n",
      "52/223, train_loss: 0.1048, step time: 0.1004\n",
      "53/223, train_loss: 0.1083, step time: 0.1066\n",
      "54/223, train_loss: 0.1080, step time: 0.0997\n",
      "55/223, train_loss: 0.1057, step time: 0.1062\n",
      "56/223, train_loss: 0.1245, step time: 0.1390\n",
      "57/223, train_loss: 0.1109, step time: 0.1149\n",
      "58/223, train_loss: 0.1134, step time: 0.1223\n",
      "59/223, train_loss: 0.0987, step time: 0.1164\n",
      "60/223, train_loss: 0.1115, step time: 0.1146\n",
      "61/223, train_loss: 0.1129, step time: 0.0997\n",
      "62/223, train_loss: 0.1303, step time: 0.1059\n",
      "63/223, train_loss: 0.1106, step time: 0.1195\n",
      "64/223, train_loss: 0.0968, step time: 0.1168\n",
      "65/223, train_loss: 0.1108, step time: 0.1046\n",
      "66/223, train_loss: 0.1224, step time: 0.1321\n",
      "67/223, train_loss: 0.1111, step time: 0.1355\n",
      "68/223, train_loss: 0.1003, step time: 0.1004\n",
      "69/223, train_loss: 0.1131, step time: 0.1206\n",
      "70/223, train_loss: 0.1100, step time: 0.1317\n",
      "71/223, train_loss: 0.1021, step time: 0.1105\n",
      "72/223, train_loss: 0.3087, step time: 0.1004\n",
      "73/223, train_loss: 0.1084, step time: 0.1082\n",
      "74/223, train_loss: 0.1104, step time: 0.1015\n",
      "75/223, train_loss: 0.1005, step time: 0.1078\n",
      "76/223, train_loss: 0.1043, step time: 0.1114\n",
      "77/223, train_loss: 0.1126, step time: 0.1095\n",
      "78/223, train_loss: 0.1104, step time: 0.1254\n",
      "79/223, train_loss: 0.1143, step time: 0.1037\n",
      "80/223, train_loss: 0.1031, step time: 0.1000\n",
      "81/223, train_loss: 0.1214, step time: 0.1134\n",
      "82/223, train_loss: 0.1214, step time: 0.1007\n",
      "83/223, train_loss: 0.1072, step time: 0.1026\n",
      "84/223, train_loss: 0.1120, step time: 0.1034\n",
      "85/223, train_loss: 0.1073, step time: 0.1445\n",
      "86/223, train_loss: 0.1156, step time: 0.1161\n",
      "87/223, train_loss: 0.1107, step time: 0.1154\n",
      "88/223, train_loss: 0.1220, step time: 0.1057\n",
      "89/223, train_loss: 0.1106, step time: 0.1161\n",
      "90/223, train_loss: 0.1055, step time: 0.1052\n",
      "91/223, train_loss: 0.1145, step time: 0.1003\n",
      "92/223, train_loss: 0.1315, step time: 0.1241\n",
      "93/223, train_loss: 0.1121, step time: 0.1248\n",
      "94/223, train_loss: 0.1096, step time: 0.1112\n",
      "95/223, train_loss: 0.1067, step time: 0.1185\n",
      "96/223, train_loss: 0.1129, step time: 0.1020\n",
      "97/223, train_loss: 0.1149, step time: 0.1006\n",
      "98/223, train_loss: 0.1262, step time: 0.1018\n",
      "99/223, train_loss: 0.1094, step time: 0.1005\n",
      "100/223, train_loss: 0.1090, step time: 0.1063\n",
      "101/223, train_loss: 0.1060, step time: 0.1083\n",
      "102/223, train_loss: 0.1071, step time: 0.1017\n",
      "103/223, train_loss: 0.1030, step time: 0.1056\n",
      "104/223, train_loss: 0.1188, step time: 0.1063\n",
      "105/223, train_loss: 0.1041, step time: 0.1062\n",
      "106/223, train_loss: 0.1110, step time: 0.1003\n",
      "107/223, train_loss: 0.1097, step time: 0.1220\n",
      "108/223, train_loss: 0.1199, step time: 0.1001\n",
      "109/223, train_loss: 0.1051, step time: 0.1010\n",
      "110/223, train_loss: 0.1118, step time: 0.0997\n",
      "111/223, train_loss: 0.1026, step time: 0.1006\n",
      "112/223, train_loss: 0.0998, step time: 0.1001\n",
      "113/223, train_loss: 0.1073, step time: 0.1163\n",
      "114/223, train_loss: 0.1165, step time: 0.1029\n",
      "115/223, train_loss: 0.1213, step time: 0.1143\n",
      "116/223, train_loss: 0.1088, step time: 0.1270\n",
      "117/223, train_loss: 0.1121, step time: 0.1105\n",
      "118/223, train_loss: 0.1082, step time: 0.1001\n",
      "119/223, train_loss: 0.1254, step time: 0.1178\n",
      "120/223, train_loss: 0.1166, step time: 0.1004\n",
      "121/223, train_loss: 0.1107, step time: 0.1136\n",
      "122/223, train_loss: 0.0987, step time: 0.1066\n",
      "123/223, train_loss: 0.1045, step time: 0.1002\n",
      "124/223, train_loss: 0.1240, step time: 0.1008\n",
      "125/223, train_loss: 0.1201, step time: 0.1185\n",
      "126/223, train_loss: 0.1109, step time: 0.1087\n",
      "127/223, train_loss: 0.1037, step time: 0.1196\n",
      "128/223, train_loss: 0.1177, step time: 0.1004\n",
      "129/223, train_loss: 0.1207, step time: 0.1142\n",
      "130/223, train_loss: 0.1038, step time: 0.1095\n",
      "131/223, train_loss: 0.1132, step time: 0.1012\n",
      "132/223, train_loss: 0.1298, step time: 0.1055\n",
      "133/223, train_loss: 0.1075, step time: 0.1164\n",
      "134/223, train_loss: 0.1120, step time: 0.1206\n",
      "135/223, train_loss: 0.1051, step time: 0.1218\n",
      "136/223, train_loss: 0.1088, step time: 0.1046\n",
      "137/223, train_loss: 0.1047, step time: 0.1180\n",
      "138/223, train_loss: 0.1220, step time: 0.1124\n",
      "139/223, train_loss: 0.1032, step time: 0.1075\n",
      "140/223, train_loss: 0.1171, step time: 0.0999\n",
      "141/223, train_loss: 0.1166, step time: 0.1075\n",
      "142/223, train_loss: 0.1108, step time: 0.1033\n",
      "143/223, train_loss: 0.1162, step time: 0.1001\n",
      "144/223, train_loss: 0.0977, step time: 0.0995\n",
      "145/223, train_loss: 0.1142, step time: 0.1275\n",
      "146/223, train_loss: 0.1141, step time: 0.1051\n",
      "147/223, train_loss: 0.1041, step time: 0.1167\n",
      "148/223, train_loss: 0.1080, step time: 0.1005\n",
      "149/223, train_loss: 0.1133, step time: 0.1010\n",
      "150/223, train_loss: 0.1064, step time: 0.0997\n",
      "151/223, train_loss: 0.1078, step time: 0.0991\n",
      "152/223, train_loss: 0.1299, step time: 0.1013\n",
      "153/223, train_loss: 0.1062, step time: 0.0995\n",
      "154/223, train_loss: 0.1099, step time: 0.1006\n",
      "155/223, train_loss: 0.1159, step time: 0.1187\n",
      "156/223, train_loss: 0.1113, step time: 0.1002\n",
      "157/223, train_loss: 0.1122, step time: 0.0999\n",
      "158/223, train_loss: 0.1076, step time: 0.1126\n",
      "159/223, train_loss: 0.0967, step time: 0.1047\n",
      "160/223, train_loss: 0.1233, step time: 0.1072\n",
      "161/223, train_loss: 0.1078, step time: 0.1001\n",
      "162/223, train_loss: 0.1219, step time: 0.0997\n",
      "163/223, train_loss: 0.1162, step time: 0.0991\n",
      "164/223, train_loss: 0.1161, step time: 0.0999\n",
      "165/223, train_loss: 0.1051, step time: 0.1079\n",
      "166/223, train_loss: 0.1029, step time: 0.1178\n",
      "167/223, train_loss: 0.1070, step time: 0.1260\n",
      "168/223, train_loss: 0.1097, step time: 0.1067\n",
      "169/223, train_loss: 0.1154, step time: 0.0998\n",
      "170/223, train_loss: 0.1065, step time: 0.1070\n",
      "171/223, train_loss: 0.1100, step time: 0.1004\n",
      "172/223, train_loss: 0.1131, step time: 0.1028\n",
      "173/223, train_loss: 0.1084, step time: 0.1144\n",
      "174/223, train_loss: 0.1128, step time: 0.1158\n",
      "175/223, train_loss: 0.1163, step time: 0.1014\n",
      "176/223, train_loss: 0.1050, step time: 0.1030\n",
      "177/223, train_loss: 0.1193, step time: 0.1199\n",
      "178/223, train_loss: 0.1070, step time: 0.1051\n",
      "179/223, train_loss: 0.1165, step time: 0.1008\n",
      "180/223, train_loss: 0.1142, step time: 0.1475\n",
      "181/223, train_loss: 0.1009, step time: 0.1138\n",
      "182/223, train_loss: 0.1232, step time: 0.0999\n",
      "183/223, train_loss: 0.1121, step time: 0.1260\n",
      "184/223, train_loss: 0.1123, step time: 0.1127\n",
      "185/223, train_loss: 0.1090, step time: 0.1138\n",
      "186/223, train_loss: 0.1229, step time: 0.1028\n",
      "187/223, train_loss: 0.1252, step time: 0.1097\n",
      "188/223, train_loss: 0.1049, step time: 0.1006\n",
      "189/223, train_loss: 0.1194, step time: 0.1004\n",
      "190/223, train_loss: 0.1148, step time: 0.0999\n",
      "191/223, train_loss: 0.1200, step time: 0.1000\n",
      "192/223, train_loss: 0.1218, step time: 0.1031\n",
      "193/223, train_loss: 0.0976, step time: 0.1046\n",
      "194/223, train_loss: 0.1136, step time: 0.1010\n",
      "195/223, train_loss: 0.1163, step time: 0.1050\n",
      "196/223, train_loss: 0.1188, step time: 0.1002\n",
      "197/223, train_loss: 0.1128, step time: 0.1004\n",
      "198/223, train_loss: 0.1034, step time: 0.0995\n",
      "199/223, train_loss: 0.1193, step time: 0.0999\n",
      "200/223, train_loss: 0.1144, step time: 0.1005\n",
      "201/223, train_loss: 0.1084, step time: 0.1116\n",
      "202/223, train_loss: 0.1041, step time: 0.1098\n",
      "203/223, train_loss: 0.1023, step time: 0.1269\n",
      "204/223, train_loss: 0.1210, step time: 0.1029\n",
      "205/223, train_loss: 0.1171, step time: 0.1109\n",
      "206/223, train_loss: 0.1037, step time: 0.1001\n",
      "207/223, train_loss: 0.1247, step time: 0.1001\n",
      "208/223, train_loss: 0.1132, step time: 0.1133\n",
      "209/223, train_loss: 0.1062, step time: 0.1024\n",
      "210/223, train_loss: 0.1201, step time: 0.1004\n",
      "211/223, train_loss: 0.1044, step time: 0.1004\n",
      "212/223, train_loss: 0.1119, step time: 0.1056\n",
      "213/223, train_loss: 0.1135, step time: 0.1165\n",
      "214/223, train_loss: 0.1206, step time: 0.0999\n",
      "215/223, train_loss: 0.1174, step time: 0.1224\n",
      "216/223, train_loss: 0.1060, step time: 0.1210\n",
      "217/223, train_loss: 0.1094, step time: 0.0998\n",
      "218/223, train_loss: 0.1321, step time: 0.1002\n",
      "219/223, train_loss: 0.1039, step time: 0.1006\n",
      "220/223, train_loss: 0.1180, step time: 0.1033\n",
      "221/223, train_loss: 0.1189, step time: 0.0996\n",
      "222/223, train_loss: 0.1150, step time: 0.0992\n",
      "223/223, train_loss: 0.1161, step time: 0.0999\n",
      "epoch 109 average loss: 0.1131\n",
      "time consuming of epoch 109 is: 90.7373\n",
      "----------\n",
      "epoch 110/300\n",
      "1/223, train_loss: 0.1084, step time: 0.1015\n",
      "2/223, train_loss: 0.1120, step time: 0.1008\n",
      "3/223, train_loss: 0.1239, step time: 0.1008\n",
      "4/223, train_loss: 0.1118, step time: 0.1001\n",
      "5/223, train_loss: 0.1189, step time: 0.1045\n",
      "6/223, train_loss: 0.1085, step time: 0.1014\n",
      "7/223, train_loss: 0.1205, step time: 0.1000\n",
      "8/223, train_loss: 0.1192, step time: 0.0998\n",
      "9/223, train_loss: 0.1181, step time: 0.0992\n",
      "10/223, train_loss: 0.1093, step time: 0.1116\n",
      "11/223, train_loss: 0.1121, step time: 0.1001\n",
      "12/223, train_loss: 0.0942, step time: 0.0996\n",
      "13/223, train_loss: 0.1097, step time: 0.1016\n",
      "14/223, train_loss: 0.1226, step time: 0.1052\n",
      "15/223, train_loss: 0.1147, step time: 0.1125\n",
      "16/223, train_loss: 0.1184, step time: 0.1100\n",
      "17/223, train_loss: 0.1162, step time: 0.1510\n",
      "18/223, train_loss: 0.1032, step time: 0.1004\n",
      "19/223, train_loss: 0.1070, step time: 0.1078\n",
      "20/223, train_loss: 0.1159, step time: 0.1210\n",
      "21/223, train_loss: 0.1190, step time: 0.1071\n",
      "22/223, train_loss: 0.1175, step time: 0.1366\n",
      "23/223, train_loss: 0.1013, step time: 0.1100\n",
      "24/223, train_loss: 0.1136, step time: 0.1125\n",
      "25/223, train_loss: 0.1048, step time: 0.0996\n",
      "26/223, train_loss: 0.1218, step time: 0.1031\n",
      "27/223, train_loss: 0.1078, step time: 0.1142\n",
      "28/223, train_loss: 0.1168, step time: 0.1002\n",
      "29/223, train_loss: 0.1064, step time: 0.0997\n",
      "30/223, train_loss: 0.1073, step time: 0.1003\n",
      "31/223, train_loss: 0.1113, step time: 0.1247\n",
      "32/223, train_loss: 0.1300, step time: 0.1076\n",
      "33/223, train_loss: 0.1113, step time: 0.1131\n",
      "34/223, train_loss: 0.1059, step time: 0.1165\n",
      "35/223, train_loss: 0.1072, step time: 0.1070\n",
      "36/223, train_loss: 0.1058, step time: 0.1004\n",
      "37/223, train_loss: 0.1106, step time: 0.1190\n",
      "38/223, train_loss: 0.1098, step time: 0.1282\n",
      "39/223, train_loss: 0.1084, step time: 0.1181\n",
      "40/223, train_loss: 0.1035, step time: 0.1117\n",
      "41/223, train_loss: 0.1042, step time: 0.0998\n",
      "42/223, train_loss: 0.0989, step time: 0.1112\n",
      "43/223, train_loss: 0.1211, step time: 0.1166\n",
      "44/223, train_loss: 0.1117, step time: 0.0997\n",
      "45/223, train_loss: 0.1181, step time: 0.0998\n",
      "46/223, train_loss: 0.1070, step time: 0.1137\n",
      "47/223, train_loss: 0.1118, step time: 0.1155\n",
      "48/223, train_loss: 0.1197, step time: 0.1207\n",
      "49/223, train_loss: 0.1253, step time: 0.1134\n",
      "50/223, train_loss: 0.1005, step time: 0.1048\n",
      "51/223, train_loss: 0.1107, step time: 0.1214\n",
      "52/223, train_loss: 0.1096, step time: 0.1093\n",
      "53/223, train_loss: 0.1215, step time: 0.1113\n",
      "54/223, train_loss: 0.1086, step time: 0.1097\n",
      "55/223, train_loss: 0.1104, step time: 0.1149\n",
      "56/223, train_loss: 0.1256, step time: 0.1344\n",
      "57/223, train_loss: 0.1119, step time: 0.1027\n",
      "58/223, train_loss: 0.1172, step time: 0.1057\n",
      "59/223, train_loss: 0.1048, step time: 0.1216\n",
      "60/223, train_loss: 0.1018, step time: 0.1143\n",
      "61/223, train_loss: 0.0966, step time: 0.1199\n",
      "62/223, train_loss: 0.1236, step time: 0.1050\n",
      "63/223, train_loss: 0.1077, step time: 0.1131\n",
      "64/223, train_loss: 0.1027, step time: 0.1257\n",
      "65/223, train_loss: 0.1019, step time: 0.1059\n",
      "66/223, train_loss: 0.1167, step time: 0.1015\n",
      "67/223, train_loss: 0.1065, step time: 0.1341\n",
      "68/223, train_loss: 0.1116, step time: 0.1006\n",
      "69/223, train_loss: 0.1158, step time: 0.1011\n",
      "70/223, train_loss: 0.1133, step time: 0.1006\n",
      "71/223, train_loss: 0.1025, step time: 0.1092\n",
      "72/223, train_loss: 0.1180, step time: 0.1007\n",
      "73/223, train_loss: 0.1184, step time: 0.1009\n",
      "74/223, train_loss: 0.1132, step time: 0.1026\n",
      "75/223, train_loss: 0.1089, step time: 0.1083\n",
      "76/223, train_loss: 0.1129, step time: 0.1205\n",
      "77/223, train_loss: 0.1115, step time: 0.1687\n",
      "78/223, train_loss: 0.1272, step time: 0.1005\n",
      "79/223, train_loss: 0.1230, step time: 0.1070\n",
      "80/223, train_loss: 0.1143, step time: 0.1296\n",
      "81/223, train_loss: 0.1146, step time: 0.1406\n",
      "82/223, train_loss: 0.1208, step time: 0.1110\n",
      "83/223, train_loss: 0.1130, step time: 0.1008\n",
      "84/223, train_loss: 0.1043, step time: 0.1044\n",
      "85/223, train_loss: 0.1167, step time: 0.1114\n",
      "86/223, train_loss: 0.1150, step time: 0.1191\n",
      "87/223, train_loss: 0.1052, step time: 0.1009\n",
      "88/223, train_loss: 0.1029, step time: 0.1003\n",
      "89/223, train_loss: 0.1046, step time: 0.1007\n",
      "90/223, train_loss: 0.1202, step time: 0.1007\n",
      "91/223, train_loss: 0.1081, step time: 0.1015\n",
      "92/223, train_loss: 0.1114, step time: 0.1275\n",
      "93/223, train_loss: 0.1130, step time: 0.1026\n",
      "94/223, train_loss: 0.1102, step time: 0.1176\n",
      "95/223, train_loss: 0.1216, step time: 0.1179\n",
      "96/223, train_loss: 0.1108, step time: 0.1096\n",
      "97/223, train_loss: 0.1048, step time: 0.1121\n",
      "98/223, train_loss: 0.1194, step time: 0.1088\n",
      "99/223, train_loss: 0.1295, step time: 0.1064\n",
      "100/223, train_loss: 0.0979, step time: 0.1110\n",
      "101/223, train_loss: 0.1114, step time: 0.1066\n",
      "102/223, train_loss: 0.1100, step time: 0.1111\n",
      "103/223, train_loss: 0.1139, step time: 0.1125\n",
      "104/223, train_loss: 0.1044, step time: 0.1085\n",
      "105/223, train_loss: 0.1037, step time: 0.1009\n",
      "106/223, train_loss: 0.1017, step time: 0.1004\n",
      "107/223, train_loss: 0.1162, step time: 0.1376\n",
      "108/223, train_loss: 0.1181, step time: 0.1012\n",
      "109/223, train_loss: 0.1194, step time: 0.0993\n",
      "110/223, train_loss: 0.1208, step time: 0.1006\n",
      "111/223, train_loss: 0.1249, step time: 0.1067\n",
      "112/223, train_loss: 0.1228, step time: 0.1225\n",
      "113/223, train_loss: 0.1162, step time: 0.1129\n",
      "114/223, train_loss: 0.1081, step time: 0.1166\n",
      "115/223, train_loss: 0.1139, step time: 0.1221\n",
      "116/223, train_loss: 0.1069, step time: 0.1045\n",
      "117/223, train_loss: 0.0944, step time: 0.1052\n",
      "118/223, train_loss: 0.1145, step time: 0.1135\n",
      "119/223, train_loss: 0.1041, step time: 0.1129\n",
      "120/223, train_loss: 0.1024, step time: 0.1120\n",
      "121/223, train_loss: 0.1087, step time: 0.1264\n",
      "122/223, train_loss: 0.1156, step time: 0.1041\n",
      "123/223, train_loss: 0.1262, step time: 0.1097\n",
      "124/223, train_loss: 0.1333, step time: 0.1103\n",
      "125/223, train_loss: 0.1008, step time: 0.1084\n",
      "126/223, train_loss: 0.1062, step time: 0.1089\n",
      "127/223, train_loss: 0.1030, step time: 0.1131\n",
      "128/223, train_loss: 0.1106, step time: 0.1111\n",
      "129/223, train_loss: 0.1012, step time: 0.1072\n",
      "130/223, train_loss: 0.1068, step time: 0.1055\n",
      "131/223, train_loss: 0.1079, step time: 0.1082\n",
      "132/223, train_loss: 0.1068, step time: 0.1477\n",
      "133/223, train_loss: 0.1019, step time: 0.1006\n",
      "134/223, train_loss: 0.1105, step time: 0.1011\n",
      "135/223, train_loss: 0.1192, step time: 0.1123\n",
      "136/223, train_loss: 0.0978, step time: 0.1008\n",
      "137/223, train_loss: 0.1069, step time: 0.1002\n",
      "138/223, train_loss: 0.1126, step time: 0.1007\n",
      "139/223, train_loss: 0.1337, step time: 0.1115\n",
      "140/223, train_loss: 0.1031, step time: 0.1009\n",
      "141/223, train_loss: 0.1023, step time: 0.1157\n",
      "142/223, train_loss: 0.1121, step time: 0.1148\n",
      "143/223, train_loss: 0.1130, step time: 0.1146\n",
      "144/223, train_loss: 0.1218, step time: 0.1194\n",
      "145/223, train_loss: 0.1078, step time: 0.1081\n",
      "146/223, train_loss: 0.1170, step time: 0.1130\n",
      "147/223, train_loss: 0.1011, step time: 0.1156\n",
      "148/223, train_loss: 0.1132, step time: 0.1000\n",
      "149/223, train_loss: 0.1106, step time: 0.0995\n",
      "150/223, train_loss: 0.1071, step time: 0.1036\n",
      "151/223, train_loss: 0.1113, step time: 0.1285\n",
      "152/223, train_loss: 0.1087, step time: 0.1007\n",
      "153/223, train_loss: 0.1113, step time: 0.1000\n",
      "154/223, train_loss: 0.1117, step time: 0.1014\n",
      "155/223, train_loss: 0.1245, step time: 0.1227\n",
      "156/223, train_loss: 0.1154, step time: 0.1119\n",
      "157/223, train_loss: 0.1065, step time: 0.1359\n",
      "158/223, train_loss: 0.1222, step time: 0.1003\n",
      "159/223, train_loss: 0.1073, step time: 0.1081\n",
      "160/223, train_loss: 0.1142, step time: 0.0997\n",
      "161/223, train_loss: 0.1034, step time: 0.1001\n",
      "162/223, train_loss: 0.1153, step time: 0.1228\n",
      "163/223, train_loss: 0.1081, step time: 0.1136\n",
      "164/223, train_loss: 0.0993, step time: 0.1012\n",
      "165/223, train_loss: 0.1160, step time: 0.1012\n",
      "166/223, train_loss: 0.1045, step time: 0.1241\n",
      "167/223, train_loss: 0.1236, step time: 0.1136\n",
      "168/223, train_loss: 0.1049, step time: 0.1093\n",
      "169/223, train_loss: 0.1218, step time: 0.1029\n",
      "170/223, train_loss: 0.1101, step time: 0.1097\n",
      "171/223, train_loss: 0.1150, step time: 0.1094\n",
      "172/223, train_loss: 0.1051, step time: 0.0998\n",
      "173/223, train_loss: 0.1077, step time: 0.1007\n",
      "174/223, train_loss: 0.1287, step time: 0.1078\n",
      "175/223, train_loss: 0.1168, step time: 0.1060\n",
      "176/223, train_loss: 0.1031, step time: 0.1043\n",
      "177/223, train_loss: 0.1078, step time: 0.1152\n",
      "178/223, train_loss: 0.1110, step time: 0.1201\n",
      "179/223, train_loss: 0.1134, step time: 0.1060\n",
      "180/223, train_loss: 0.3112, step time: 0.1090\n",
      "181/223, train_loss: 0.1020, step time: 0.1352\n",
      "182/223, train_loss: 0.1015, step time: 0.1281\n",
      "183/223, train_loss: 0.1141, step time: 0.1076\n",
      "184/223, train_loss: 0.1072, step time: 0.1002\n",
      "185/223, train_loss: 0.1007, step time: 0.1100\n",
      "186/223, train_loss: 0.1031, step time: 0.1031\n",
      "187/223, train_loss: 0.1062, step time: 0.1147\n",
      "188/223, train_loss: 0.1129, step time: 0.1181\n",
      "189/223, train_loss: 0.1026, step time: 0.1263\n",
      "190/223, train_loss: 0.1101, step time: 0.1268\n",
      "191/223, train_loss: 0.1084, step time: 0.1305\n",
      "192/223, train_loss: 0.1062, step time: 0.1208\n",
      "193/223, train_loss: 0.1169, step time: 0.1097\n",
      "194/223, train_loss: 0.1151, step time: 0.1008\n",
      "195/223, train_loss: 0.1187, step time: 0.1136\n",
      "196/223, train_loss: 0.1064, step time: 0.1327\n",
      "197/223, train_loss: 0.1105, step time: 0.1019\n",
      "198/223, train_loss: 0.1063, step time: 0.1136\n",
      "199/223, train_loss: 0.1157, step time: 0.1077\n",
      "200/223, train_loss: 0.1087, step time: 0.1084\n",
      "201/223, train_loss: 0.1094, step time: 0.1041\n",
      "202/223, train_loss: 0.1133, step time: 0.1246\n",
      "203/223, train_loss: 0.1138, step time: 0.1218\n",
      "204/223, train_loss: 0.1065, step time: 0.1219\n",
      "205/223, train_loss: 0.1126, step time: 0.1313\n",
      "206/223, train_loss: 0.1029, step time: 0.1213\n",
      "207/223, train_loss: 0.1110, step time: 0.1181\n",
      "208/223, train_loss: 0.1234, step time: 0.1103\n",
      "209/223, train_loss: 0.1101, step time: 0.1056\n",
      "210/223, train_loss: 0.1093, step time: 0.1061\n",
      "211/223, train_loss: 0.1066, step time: 0.1176\n",
      "212/223, train_loss: 0.1061, step time: 0.1345\n",
      "213/223, train_loss: 0.1146, step time: 0.1195\n",
      "214/223, train_loss: 0.1060, step time: 0.1121\n",
      "215/223, train_loss: 0.1166, step time: 0.1192\n",
      "216/223, train_loss: 0.1130, step time: 0.1149\n",
      "217/223, train_loss: 0.1094, step time: 0.1166\n",
      "218/223, train_loss: 0.1261, step time: 0.1139\n",
      "219/223, train_loss: 0.1179, step time: 0.1002\n",
      "220/223, train_loss: 0.1087, step time: 0.1036\n",
      "221/223, train_loss: 0.1154, step time: 0.1185\n",
      "222/223, train_loss: 0.1108, step time: 0.0999\n",
      "223/223, train_loss: 0.1275, step time: 0.0999\n",
      "epoch 110 average loss: 0.1125\n",
      "saved new best metric model\n",
      "current epoch: 110 current mean dice: 0.8515 tc: 0.9176 wt: 0.8620 et: 0.7749\n",
      "best mean dice: 0.8515 at epoch: 110\n",
      "time consuming of epoch 110 is: 92.7236\n",
      "----------\n",
      "epoch 111/300\n",
      "1/223, train_loss: 0.1024, step time: 0.1026\n",
      "2/223, train_loss: 0.1088, step time: 0.1003\n",
      "3/223, train_loss: 0.1066, step time: 0.0999\n",
      "4/223, train_loss: 0.1082, step time: 0.1047\n",
      "5/223, train_loss: 0.1000, step time: 0.1131\n",
      "6/223, train_loss: 0.1031, step time: 0.1153\n",
      "7/223, train_loss: 0.1250, step time: 0.1110\n",
      "8/223, train_loss: 0.1150, step time: 0.1012\n",
      "9/223, train_loss: 0.1131, step time: 0.1269\n",
      "10/223, train_loss: 0.1203, step time: 0.1057\n",
      "11/223, train_loss: 0.1146, step time: 0.1227\n",
      "12/223, train_loss: 0.1209, step time: 0.1013\n",
      "13/223, train_loss: 0.1019, step time: 0.1149\n",
      "14/223, train_loss: 0.1082, step time: 0.1222\n",
      "15/223, train_loss: 0.1046, step time: 0.1124\n",
      "16/223, train_loss: 0.1002, step time: 0.1008\n",
      "17/223, train_loss: 0.1179, step time: 0.1108\n",
      "18/223, train_loss: 0.1110, step time: 0.1222\n",
      "19/223, train_loss: 0.1054, step time: 0.1157\n",
      "20/223, train_loss: 0.1139, step time: 0.1145\n",
      "21/223, train_loss: 0.1097, step time: 0.1144\n",
      "22/223, train_loss: 0.1321, step time: 0.1210\n",
      "23/223, train_loss: 0.1074, step time: 0.1174\n",
      "24/223, train_loss: 0.1135, step time: 0.1075\n",
      "25/223, train_loss: 0.1186, step time: 0.1277\n",
      "26/223, train_loss: 0.1099, step time: 0.1447\n",
      "27/223, train_loss: 0.1203, step time: 0.1168\n",
      "28/223, train_loss: 0.1217, step time: 0.1048\n",
      "29/223, train_loss: 0.1108, step time: 0.1182\n",
      "30/223, train_loss: 0.1141, step time: 0.1058\n",
      "31/223, train_loss: 0.1091, step time: 0.1283\n",
      "32/223, train_loss: 0.1093, step time: 0.0993\n",
      "33/223, train_loss: 0.0988, step time: 0.1112\n",
      "34/223, train_loss: 0.1072, step time: 0.1216\n",
      "35/223, train_loss: 0.1328, step time: 0.1182\n",
      "36/223, train_loss: 0.1091, step time: 0.1034\n",
      "37/223, train_loss: 0.1065, step time: 0.1346\n",
      "38/223, train_loss: 0.1009, step time: 0.1078\n",
      "39/223, train_loss: 0.1163, step time: 0.1126\n",
      "40/223, train_loss: 0.1074, step time: 0.1019\n",
      "41/223, train_loss: 0.1182, step time: 0.1435\n",
      "42/223, train_loss: 0.1095, step time: 0.1063\n",
      "43/223, train_loss: 0.1236, step time: 0.1116\n",
      "44/223, train_loss: 0.0939, step time: 0.1063\n",
      "45/223, train_loss: 0.1041, step time: 0.1089\n",
      "46/223, train_loss: 0.1102, step time: 0.1146\n",
      "47/223, train_loss: 0.1034, step time: 0.1032\n",
      "48/223, train_loss: 0.1256, step time: 0.1185\n",
      "49/223, train_loss: 0.1148, step time: 0.1010\n",
      "50/223, train_loss: 0.1055, step time: 0.1022\n",
      "51/223, train_loss: 0.1169, step time: 0.1275\n",
      "52/223, train_loss: 0.0984, step time: 0.1129\n",
      "53/223, train_loss: 0.1186, step time: 0.1246\n",
      "54/223, train_loss: 0.1056, step time: 0.1113\n",
      "55/223, train_loss: 0.1096, step time: 0.1699\n",
      "56/223, train_loss: 0.1210, step time: 0.1038\n",
      "57/223, train_loss: 0.1065, step time: 0.1120\n",
      "58/223, train_loss: 0.1093, step time: 0.1224\n",
      "59/223, train_loss: 0.1023, step time: 0.1035\n",
      "60/223, train_loss: 0.1149, step time: 0.1059\n",
      "61/223, train_loss: 0.1066, step time: 0.1033\n",
      "62/223, train_loss: 0.1203, step time: 0.0998\n",
      "63/223, train_loss: 0.1114, step time: 0.1010\n",
      "64/223, train_loss: 0.1249, step time: 0.1092\n",
      "65/223, train_loss: 0.1071, step time: 0.1085\n",
      "66/223, train_loss: 0.1148, step time: 0.1016\n",
      "67/223, train_loss: 0.1120, step time: 0.1114\n",
      "68/223, train_loss: 0.1062, step time: 0.1206\n",
      "69/223, train_loss: 0.1110, step time: 0.1150\n",
      "70/223, train_loss: 0.3063, step time: 0.1032\n",
      "71/223, train_loss: 0.1086, step time: 0.1062\n",
      "72/223, train_loss: 0.1208, step time: 0.1177\n",
      "73/223, train_loss: 0.1052, step time: 0.1285\n",
      "74/223, train_loss: 0.1191, step time: 0.1321\n",
      "75/223, train_loss: 0.1316, step time: 0.1153\n",
      "76/223, train_loss: 0.1077, step time: 0.1181\n",
      "77/223, train_loss: 0.1047, step time: 0.1186\n",
      "78/223, train_loss: 0.1216, step time: 0.1068\n",
      "79/223, train_loss: 0.1165, step time: 0.1297\n",
      "80/223, train_loss: 0.1101, step time: 0.1157\n",
      "81/223, train_loss: 0.1092, step time: 0.1016\n",
      "82/223, train_loss: 0.1236, step time: 0.1003\n",
      "83/223, train_loss: 0.1111, step time: 0.0999\n",
      "84/223, train_loss: 0.1252, step time: 0.1146\n",
      "85/223, train_loss: 0.1116, step time: 0.1000\n",
      "86/223, train_loss: 0.1249, step time: 0.1008\n",
      "87/223, train_loss: 0.1214, step time: 0.1005\n",
      "88/223, train_loss: 0.1059, step time: 0.1059\n",
      "89/223, train_loss: 0.1069, step time: 0.1475\n",
      "90/223, train_loss: 0.1025, step time: 0.1057\n",
      "91/223, train_loss: 0.1078, step time: 0.1143\n",
      "92/223, train_loss: 0.1063, step time: 0.1061\n",
      "93/223, train_loss: 0.1087, step time: 0.1032\n",
      "94/223, train_loss: 0.1057, step time: 0.1016\n",
      "95/223, train_loss: 0.1057, step time: 0.1134\n",
      "96/223, train_loss: 0.1039, step time: 0.1050\n",
      "97/223, train_loss: 0.1123, step time: 0.1075\n",
      "98/223, train_loss: 0.1268, step time: 0.1142\n",
      "99/223, train_loss: 0.1033, step time: 0.1209\n",
      "100/223, train_loss: 0.1166, step time: 0.1080\n",
      "101/223, train_loss: 0.1030, step time: 0.1067\n",
      "102/223, train_loss: 0.1045, step time: 0.1051\n",
      "103/223, train_loss: 0.1056, step time: 0.1242\n",
      "104/223, train_loss: 0.1058, step time: 0.1119\n",
      "105/223, train_loss: 0.1138, step time: 0.1005\n",
      "106/223, train_loss: 0.1138, step time: 0.1011\n",
      "107/223, train_loss: 0.1039, step time: 0.1006\n",
      "108/223, train_loss: 0.1075, step time: 0.1083\n",
      "109/223, train_loss: 0.1279, step time: 0.1072\n",
      "110/223, train_loss: 0.1136, step time: 0.1101\n",
      "111/223, train_loss: 0.1096, step time: 0.1005\n",
      "112/223, train_loss: 0.0991, step time: 0.1000\n",
      "113/223, train_loss: 0.1061, step time: 0.1122\n",
      "114/223, train_loss: 0.1049, step time: 0.1125\n",
      "115/223, train_loss: 0.1209, step time: 0.1159\n",
      "116/223, train_loss: 0.1104, step time: 0.1135\n",
      "117/223, train_loss: 0.1046, step time: 0.1077\n",
      "118/223, train_loss: 0.1103, step time: 0.1222\n",
      "119/223, train_loss: 0.1192, step time: 0.1373\n",
      "120/223, train_loss: 0.1108, step time: 0.1033\n",
      "121/223, train_loss: 0.0964, step time: 0.1103\n",
      "122/223, train_loss: 0.1077, step time: 0.1130\n",
      "123/223, train_loss: 0.1096, step time: 0.1254\n",
      "124/223, train_loss: 0.1110, step time: 0.1281\n",
      "125/223, train_loss: 0.1275, step time: 0.1150\n",
      "126/223, train_loss: 0.1200, step time: 0.1097\n",
      "127/223, train_loss: 0.1082, step time: 0.1158\n",
      "128/223, train_loss: 0.1166, step time: 0.1455\n",
      "129/223, train_loss: 0.1101, step time: 0.1072\n",
      "130/223, train_loss: 0.1126, step time: 0.1067\n",
      "131/223, train_loss: 0.1033, step time: 0.1140\n",
      "132/223, train_loss: 0.1205, step time: 0.1092\n",
      "133/223, train_loss: 0.1185, step time: 0.1103\n",
      "134/223, train_loss: 0.1150, step time: 0.1214\n",
      "135/223, train_loss: 0.1046, step time: 0.1107\n",
      "136/223, train_loss: 0.1042, step time: 0.1123\n",
      "137/223, train_loss: 0.1261, step time: 0.1245\n",
      "138/223, train_loss: 0.1205, step time: 0.1288\n",
      "139/223, train_loss: 0.1156, step time: 0.1585\n",
      "140/223, train_loss: 0.1131, step time: 0.1055\n",
      "141/223, train_loss: 0.1059, step time: 0.0996\n",
      "142/223, train_loss: 0.1098, step time: 0.0999\n",
      "143/223, train_loss: 0.1003, step time: 0.1002\n",
      "144/223, train_loss: 0.1163, step time: 0.1035\n",
      "145/223, train_loss: 0.1122, step time: 0.0997\n",
      "146/223, train_loss: 0.1044, step time: 0.0998\n",
      "147/223, train_loss: 0.1021, step time: 0.1006\n",
      "148/223, train_loss: 0.1134, step time: 0.1005\n",
      "149/223, train_loss: 0.1091, step time: 0.1007\n",
      "150/223, train_loss: 0.1089, step time: 0.0997\n",
      "151/223, train_loss: 0.1049, step time: 0.1004\n",
      "152/223, train_loss: 0.1087, step time: 0.1034\n",
      "153/223, train_loss: 0.1185, step time: 0.1004\n",
      "154/223, train_loss: 0.1171, step time: 0.1014\n",
      "155/223, train_loss: 0.1225, step time: 0.1095\n",
      "156/223, train_loss: 0.1095, step time: 0.1684\n",
      "157/223, train_loss: 0.1140, step time: 0.1001\n",
      "158/223, train_loss: 0.1236, step time: 0.1239\n",
      "159/223, train_loss: 0.1027, step time: 0.1287\n",
      "160/223, train_loss: 0.1107, step time: 0.1010\n",
      "161/223, train_loss: 0.1253, step time: 0.0989\n",
      "162/223, train_loss: 0.1041, step time: 0.1225\n",
      "163/223, train_loss: 0.1125, step time: 0.0988\n",
      "164/223, train_loss: 0.1017, step time: 0.0995\n",
      "165/223, train_loss: 0.1163, step time: 0.1054\n",
      "166/223, train_loss: 0.1112, step time: 0.1234\n",
      "167/223, train_loss: 0.1109, step time: 0.1000\n",
      "168/223, train_loss: 0.1089, step time: 0.0995\n",
      "169/223, train_loss: 0.1022, step time: 0.1010\n",
      "170/223, train_loss: 0.1140, step time: 0.1021\n",
      "171/223, train_loss: 0.1118, step time: 0.1152\n",
      "172/223, train_loss: 0.1095, step time: 0.1003\n",
      "173/223, train_loss: 0.1034, step time: 0.1163\n",
      "174/223, train_loss: 0.1171, step time: 0.1179\n",
      "175/223, train_loss: 0.1178, step time: 0.1429\n",
      "176/223, train_loss: 0.1064, step time: 0.1024\n",
      "177/223, train_loss: 0.1071, step time: 0.1001\n",
      "178/223, train_loss: 0.1236, step time: 0.1001\n",
      "179/223, train_loss: 0.1172, step time: 0.1009\n",
      "180/223, train_loss: 0.1193, step time: 0.1250\n",
      "181/223, train_loss: 0.1047, step time: 0.1101\n",
      "182/223, train_loss: 0.1137, step time: 0.1055\n",
      "183/223, train_loss: 0.1187, step time: 0.1276\n",
      "184/223, train_loss: 0.1058, step time: 0.1247\n",
      "185/223, train_loss: 0.0992, step time: 0.1005\n",
      "186/223, train_loss: 0.1160, step time: 0.1048\n",
      "187/223, train_loss: 0.1245, step time: 0.1041\n",
      "188/223, train_loss: 0.1206, step time: 0.1084\n",
      "189/223, train_loss: 0.1161, step time: 0.1001\n",
      "190/223, train_loss: 0.1109, step time: 0.1164\n",
      "191/223, train_loss: 0.1188, step time: 0.1189\n",
      "192/223, train_loss: 0.1226, step time: 0.1116\n",
      "193/223, train_loss: 0.1103, step time: 0.1222\n",
      "194/223, train_loss: 0.1105, step time: 0.1162\n",
      "195/223, train_loss: 0.1104, step time: 0.1004\n",
      "196/223, train_loss: 0.1120, step time: 0.1010\n",
      "197/223, train_loss: 0.1102, step time: 0.1077\n",
      "198/223, train_loss: 0.1160, step time: 0.1168\n",
      "199/223, train_loss: 0.1252, step time: 0.1102\n",
      "200/223, train_loss: 0.1009, step time: 0.1150\n",
      "201/223, train_loss: 0.1049, step time: 0.1093\n",
      "202/223, train_loss: 0.1137, step time: 0.1127\n",
      "203/223, train_loss: 0.1081, step time: 0.1156\n",
      "204/223, train_loss: 0.1242, step time: 0.1186\n",
      "205/223, train_loss: 0.1104, step time: 0.1203\n",
      "206/223, train_loss: 0.1267, step time: 0.1066\n",
      "207/223, train_loss: 0.1206, step time: 0.1193\n",
      "208/223, train_loss: 0.1069, step time: 0.1129\n",
      "209/223, train_loss: 0.1185, step time: 0.1009\n",
      "210/223, train_loss: 0.1088, step time: 0.1012\n",
      "211/223, train_loss: 0.1053, step time: 0.1029\n",
      "212/223, train_loss: 0.1186, step time: 0.1003\n",
      "213/223, train_loss: 0.1150, step time: 0.0988\n",
      "214/223, train_loss: 0.1093, step time: 0.1000\n",
      "215/223, train_loss: 0.1153, step time: 0.1007\n",
      "216/223, train_loss: 0.1115, step time: 0.1010\n",
      "217/223, train_loss: 0.1158, step time: 0.0994\n",
      "218/223, train_loss: 0.1093, step time: 0.1000\n",
      "219/223, train_loss: 0.1086, step time: 0.1003\n",
      "220/223, train_loss: 0.1148, step time: 0.1003\n",
      "221/223, train_loss: 0.1050, step time: 0.0991\n",
      "222/223, train_loss: 0.1148, step time: 0.0989\n",
      "223/223, train_loss: 0.1093, step time: 0.1003\n",
      "epoch 111 average loss: 0.1128\n",
      "time consuming of epoch 111 is: 91.0054\n",
      "----------\n",
      "epoch 112/300\n",
      "1/223, train_loss: 0.1074, step time: 0.1029\n",
      "2/223, train_loss: 0.1277, step time: 0.1001\n",
      "3/223, train_loss: 0.1067, step time: 0.1075\n",
      "4/223, train_loss: 0.1017, step time: 0.1310\n",
      "5/223, train_loss: 0.1069, step time: 0.1084\n",
      "6/223, train_loss: 0.1022, step time: 0.1103\n",
      "7/223, train_loss: 0.1142, step time: 0.1158\n",
      "8/223, train_loss: 0.1105, step time: 0.1137\n",
      "9/223, train_loss: 0.1102, step time: 0.0992\n",
      "10/223, train_loss: 0.1120, step time: 0.1084\n",
      "11/223, train_loss: 0.1196, step time: 0.1136\n",
      "12/223, train_loss: 0.1035, step time: 0.1108\n",
      "13/223, train_loss: 0.1013, step time: 0.1091\n",
      "14/223, train_loss: 0.1052, step time: 0.1060\n",
      "15/223, train_loss: 0.1145, step time: 0.1172\n",
      "16/223, train_loss: 0.1108, step time: 0.1047\n",
      "17/223, train_loss: 0.1124, step time: 0.1074\n",
      "18/223, train_loss: 0.1110, step time: 0.1143\n",
      "19/223, train_loss: 0.0986, step time: 0.1184\n",
      "20/223, train_loss: 0.1025, step time: 0.1047\n",
      "21/223, train_loss: 0.1010, step time: 0.1109\n",
      "22/223, train_loss: 0.1133, step time: 0.1033\n",
      "23/223, train_loss: 0.1021, step time: 0.0994\n",
      "24/223, train_loss: 0.1088, step time: 0.1010\n",
      "25/223, train_loss: 0.1056, step time: 0.1101\n",
      "26/223, train_loss: 0.1072, step time: 0.1091\n",
      "27/223, train_loss: 0.1013, step time: 0.1002\n",
      "28/223, train_loss: 0.1091, step time: 0.1003\n",
      "29/223, train_loss: 0.1225, step time: 0.1014\n",
      "30/223, train_loss: 0.1042, step time: 0.1117\n",
      "31/223, train_loss: 0.1204, step time: 0.1197\n",
      "32/223, train_loss: 0.1109, step time: 0.1203\n",
      "33/223, train_loss: 0.1148, step time: 0.1154\n",
      "34/223, train_loss: 0.0972, step time: 0.1006\n",
      "35/223, train_loss: 0.1194, step time: 0.1210\n",
      "36/223, train_loss: 0.1056, step time: 0.1216\n",
      "37/223, train_loss: 0.1124, step time: 0.1203\n",
      "38/223, train_loss: 0.1002, step time: 0.1120\n",
      "39/223, train_loss: 0.1179, step time: 0.1265\n",
      "40/223, train_loss: 0.1101, step time: 0.1285\n",
      "41/223, train_loss: 0.1247, step time: 0.0997\n",
      "42/223, train_loss: 0.1062, step time: 0.0995\n",
      "43/223, train_loss: 0.1228, step time: 0.1109\n",
      "44/223, train_loss: 0.1003, step time: 0.1079\n",
      "45/223, train_loss: 0.1216, step time: 0.1529\n",
      "46/223, train_loss: 0.1224, step time: 0.1009\n",
      "47/223, train_loss: 0.1095, step time: 0.1007\n",
      "48/223, train_loss: 0.1191, step time: 0.1010\n",
      "49/223, train_loss: 0.1169, step time: 0.1005\n",
      "50/223, train_loss: 0.1121, step time: 0.1006\n",
      "51/223, train_loss: 0.1191, step time: 0.1314\n",
      "52/223, train_loss: 0.1197, step time: 0.1476\n",
      "53/223, train_loss: 0.1185, step time: 0.1027\n",
      "54/223, train_loss: 0.1229, step time: 0.1005\n",
      "55/223, train_loss: 0.1183, step time: 0.1001\n",
      "56/223, train_loss: 0.1054, step time: 0.1058\n",
      "57/223, train_loss: 0.0982, step time: 0.1004\n",
      "58/223, train_loss: 0.1011, step time: 0.1000\n",
      "59/223, train_loss: 0.1125, step time: 0.0998\n",
      "60/223, train_loss: 0.1131, step time: 0.0996\n",
      "61/223, train_loss: 0.1009, step time: 0.0991\n",
      "62/223, train_loss: 0.1091, step time: 0.1003\n",
      "63/223, train_loss: 0.1132, step time: 0.1010\n",
      "64/223, train_loss: 0.1188, step time: 0.1003\n",
      "65/223, train_loss: 0.1158, step time: 0.0991\n",
      "66/223, train_loss: 0.1086, step time: 0.1106\n",
      "67/223, train_loss: 0.1193, step time: 0.1128\n",
      "68/223, train_loss: 0.1029, step time: 0.1173\n",
      "69/223, train_loss: 0.1316, step time: 0.1011\n",
      "70/223, train_loss: 0.1054, step time: 0.1073\n",
      "71/223, train_loss: 0.1084, step time: 0.1066\n",
      "72/223, train_loss: 0.1076, step time: 0.1095\n",
      "73/223, train_loss: 0.1237, step time: 0.1004\n",
      "74/223, train_loss: 0.1055, step time: 0.1118\n",
      "75/223, train_loss: 0.1157, step time: 0.1075\n",
      "76/223, train_loss: 0.1144, step time: 0.1021\n",
      "77/223, train_loss: 0.1133, step time: 0.1080\n",
      "78/223, train_loss: 0.1193, step time: 0.1593\n",
      "79/223, train_loss: 0.1247, step time: 0.1154\n",
      "80/223, train_loss: 0.1055, step time: 0.1005\n",
      "81/223, train_loss: 0.1242, step time: 0.1006\n",
      "82/223, train_loss: 0.1014, step time: 0.1007\n",
      "83/223, train_loss: 0.1112, step time: 0.1085\n",
      "84/223, train_loss: 0.1037, step time: 0.0995\n",
      "85/223, train_loss: 0.1168, step time: 0.1006\n",
      "86/223, train_loss: 0.1032, step time: 0.1118\n",
      "87/223, train_loss: 0.1090, step time: 0.1080\n",
      "88/223, train_loss: 0.1109, step time: 0.1079\n",
      "89/223, train_loss: 0.1025, step time: 0.1113\n",
      "90/223, train_loss: 0.1091, step time: 0.1113\n",
      "91/223, train_loss: 0.1077, step time: 0.1095\n",
      "92/223, train_loss: 0.1145, step time: 0.1225\n",
      "93/223, train_loss: 0.3026, step time: 0.1183\n",
      "94/223, train_loss: 0.1067, step time: 0.1096\n",
      "95/223, train_loss: 0.0991, step time: 0.1169\n",
      "96/223, train_loss: 0.1087, step time: 0.1134\n",
      "97/223, train_loss: 0.1251, step time: 0.1451\n",
      "98/223, train_loss: 0.0986, step time: 0.1275\n",
      "99/223, train_loss: 0.1038, step time: 0.1098\n",
      "100/223, train_loss: 0.1174, step time: 0.1295\n",
      "101/223, train_loss: 0.1176, step time: 0.1412\n",
      "102/223, train_loss: 0.1037, step time: 0.1125\n",
      "103/223, train_loss: 0.1233, step time: 0.1219\n",
      "104/223, train_loss: 0.1287, step time: 0.1094\n",
      "105/223, train_loss: 0.1201, step time: 0.1520\n",
      "106/223, train_loss: 0.1067, step time: 0.1212\n",
      "107/223, train_loss: 0.1023, step time: 0.1285\n",
      "108/223, train_loss: 0.1103, step time: 0.1569\n",
      "109/223, train_loss: 0.1104, step time: 0.1150\n",
      "110/223, train_loss: 0.1045, step time: 0.1012\n",
      "111/223, train_loss: 0.1191, step time: 0.1013\n",
      "112/223, train_loss: 0.1285, step time: 0.1252\n",
      "113/223, train_loss: 0.1304, step time: 0.1231\n",
      "114/223, train_loss: 0.1148, step time: 0.1266\n",
      "115/223, train_loss: 0.1168, step time: 0.1283\n",
      "116/223, train_loss: 0.1039, step time: 0.1053\n",
      "117/223, train_loss: 0.1034, step time: 0.1153\n",
      "118/223, train_loss: 0.1105, step time: 0.1151\n",
      "119/223, train_loss: 0.1102, step time: 0.1002\n",
      "120/223, train_loss: 0.1063, step time: 0.1218\n",
      "121/223, train_loss: 0.1021, step time: 0.1022\n",
      "122/223, train_loss: 0.0988, step time: 0.1088\n",
      "123/223, train_loss: 0.1066, step time: 0.1023\n",
      "124/223, train_loss: 0.1138, step time: 0.1035\n",
      "125/223, train_loss: 0.1140, step time: 0.1157\n",
      "126/223, train_loss: 0.1167, step time: 0.1090\n",
      "127/223, train_loss: 0.1195, step time: 0.1107\n",
      "128/223, train_loss: 0.1148, step time: 0.1108\n",
      "129/223, train_loss: 0.1100, step time: 0.0992\n",
      "130/223, train_loss: 0.1156, step time: 0.1192\n",
      "131/223, train_loss: 0.1096, step time: 0.1023\n",
      "132/223, train_loss: 0.1101, step time: 0.1108\n",
      "133/223, train_loss: 0.1140, step time: 0.1258\n",
      "134/223, train_loss: 0.1103, step time: 0.1027\n",
      "135/223, train_loss: 0.1137, step time: 0.1008\n",
      "136/223, train_loss: 0.1057, step time: 0.1084\n",
      "137/223, train_loss: 0.1176, step time: 0.1061\n",
      "138/223, train_loss: 0.1117, step time: 0.1098\n",
      "139/223, train_loss: 0.1242, step time: 0.1007\n",
      "140/223, train_loss: 0.1063, step time: 0.1055\n",
      "141/223, train_loss: 0.1355, step time: 0.1001\n",
      "142/223, train_loss: 0.1012, step time: 0.1016\n",
      "143/223, train_loss: 0.1136, step time: 0.1075\n",
      "144/223, train_loss: 0.1134, step time: 0.1233\n",
      "145/223, train_loss: 0.1201, step time: 0.1079\n",
      "146/223, train_loss: 0.1040, step time: 0.1041\n",
      "147/223, train_loss: 0.1113, step time: 0.1031\n",
      "148/223, train_loss: 0.1001, step time: 0.1189\n",
      "149/223, train_loss: 0.1074, step time: 0.1006\n",
      "150/223, train_loss: 0.1287, step time: 0.1094\n",
      "151/223, train_loss: 0.1185, step time: 0.1070\n",
      "152/223, train_loss: 0.0950, step time: 0.1108\n",
      "153/223, train_loss: 0.1311, step time: 0.1143\n",
      "154/223, train_loss: 0.1163, step time: 0.1126\n",
      "155/223, train_loss: 0.1171, step time: 0.1007\n",
      "156/223, train_loss: 0.1066, step time: 0.1001\n",
      "157/223, train_loss: 0.1055, step time: 0.1002\n",
      "158/223, train_loss: 0.1182, step time: 0.1022\n",
      "159/223, train_loss: 0.1198, step time: 0.1260\n",
      "160/223, train_loss: 0.1048, step time: 0.1005\n",
      "161/223, train_loss: 0.1110, step time: 0.1132\n",
      "162/223, train_loss: 0.1042, step time: 0.1195\n",
      "163/223, train_loss: 0.1128, step time: 0.1134\n",
      "164/223, train_loss: 0.1042, step time: 0.1117\n",
      "165/223, train_loss: 0.1117, step time: 0.1080\n",
      "166/223, train_loss: 0.1207, step time: 0.1002\n",
      "167/223, train_loss: 0.1196, step time: 0.1003\n",
      "168/223, train_loss: 0.1174, step time: 0.1037\n",
      "169/223, train_loss: 0.1140, step time: 0.1254\n",
      "170/223, train_loss: 0.1103, step time: 0.1012\n",
      "171/223, train_loss: 0.1164, step time: 0.1001\n",
      "172/223, train_loss: 0.1068, step time: 0.1141\n",
      "173/223, train_loss: 0.1083, step time: 0.1003\n",
      "174/223, train_loss: 0.1006, step time: 0.1008\n",
      "175/223, train_loss: 0.1135, step time: 0.1099\n",
      "176/223, train_loss: 0.1060, step time: 0.1075\n",
      "177/223, train_loss: 0.1073, step time: 0.1114\n",
      "178/223, train_loss: 0.1147, step time: 0.1028\n",
      "179/223, train_loss: 0.1049, step time: 0.1012\n",
      "180/223, train_loss: 0.1099, step time: 0.1015\n",
      "181/223, train_loss: 0.1062, step time: 0.1204\n",
      "182/223, train_loss: 0.1015, step time: 0.1032\n",
      "183/223, train_loss: 0.0949, step time: 0.1016\n",
      "184/223, train_loss: 0.1195, step time: 0.1054\n",
      "185/223, train_loss: 0.1189, step time: 0.1227\n",
      "186/223, train_loss: 0.1152, step time: 0.1241\n",
      "187/223, train_loss: 0.1151, step time: 0.1127\n",
      "188/223, train_loss: 0.1070, step time: 0.1131\n",
      "189/223, train_loss: 0.1222, step time: 0.1383\n",
      "190/223, train_loss: 0.1090, step time: 0.1179\n",
      "191/223, train_loss: 0.1044, step time: 0.1059\n",
      "192/223, train_loss: 0.1155, step time: 0.1019\n",
      "193/223, train_loss: 0.1261, step time: 0.1201\n",
      "194/223, train_loss: 0.1241, step time: 0.1182\n",
      "195/223, train_loss: 0.1019, step time: 0.1050\n",
      "196/223, train_loss: 0.1072, step time: 0.1309\n",
      "197/223, train_loss: 0.1000, step time: 0.1151\n",
      "198/223, train_loss: 0.1146, step time: 0.1711\n",
      "199/223, train_loss: 0.1125, step time: 0.1221\n",
      "200/223, train_loss: 0.1053, step time: 0.1179\n",
      "201/223, train_loss: 0.1089, step time: 0.1263\n",
      "202/223, train_loss: 0.1071, step time: 0.1013\n",
      "203/223, train_loss: 0.1182, step time: 0.1150\n",
      "204/223, train_loss: 0.1162, step time: 0.1146\n",
      "205/223, train_loss: 0.1071, step time: 0.1182\n",
      "206/223, train_loss: 0.1042, step time: 0.1084\n",
      "207/223, train_loss: 0.1033, step time: 0.0996\n",
      "208/223, train_loss: 0.1104, step time: 0.1208\n",
      "209/223, train_loss: 0.1153, step time: 0.1274\n",
      "210/223, train_loss: 0.1115, step time: 0.1119\n",
      "211/223, train_loss: 0.1212, step time: 0.1315\n",
      "212/223, train_loss: 0.1183, step time: 0.1117\n",
      "213/223, train_loss: 0.1068, step time: 0.1001\n",
      "214/223, train_loss: 0.1026, step time: 0.1199\n",
      "215/223, train_loss: 0.1074, step time: 0.1069\n",
      "216/223, train_loss: 0.1045, step time: 0.1151\n",
      "217/223, train_loss: 0.1146, step time: 0.1022\n",
      "218/223, train_loss: 0.1082, step time: 0.1003\n",
      "219/223, train_loss: 0.1035, step time: 0.1011\n",
      "220/223, train_loss: 0.1099, step time: 0.0993\n",
      "221/223, train_loss: 0.1116, step time: 0.0998\n",
      "222/223, train_loss: 0.1194, step time: 0.0998\n",
      "223/223, train_loss: 0.1106, step time: 0.1001\n",
      "epoch 112 average loss: 0.1123\n",
      "time consuming of epoch 112 is: 87.6200\n",
      "----------\n",
      "epoch 113/300\n",
      "1/223, train_loss: 0.1097, step time: 0.1094\n",
      "2/223, train_loss: 0.1120, step time: 0.1030\n",
      "3/223, train_loss: 0.1154, step time: 0.1117\n",
      "4/223, train_loss: 0.1124, step time: 0.1153\n",
      "5/223, train_loss: 0.1071, step time: 0.1164\n",
      "6/223, train_loss: 0.1084, step time: 0.1033\n",
      "7/223, train_loss: 0.1187, step time: 0.1004\n",
      "8/223, train_loss: 0.1008, step time: 0.1059\n",
      "9/223, train_loss: 0.1036, step time: 0.1004\n",
      "10/223, train_loss: 0.1179, step time: 0.1204\n",
      "11/223, train_loss: 0.1268, step time: 0.1029\n",
      "12/223, train_loss: 0.1069, step time: 0.1003\n",
      "13/223, train_loss: 0.1184, step time: 0.1180\n",
      "14/223, train_loss: 0.1191, step time: 0.1217\n",
      "15/223, train_loss: 0.1033, step time: 0.0995\n",
      "16/223, train_loss: 0.1138, step time: 0.1010\n",
      "17/223, train_loss: 0.1009, step time: 0.1091\n",
      "18/223, train_loss: 0.1136, step time: 0.1080\n",
      "19/223, train_loss: 0.1092, step time: 0.1011\n",
      "20/223, train_loss: 0.1168, step time: 0.1066\n",
      "21/223, train_loss: 0.1159, step time: 0.1243\n",
      "22/223, train_loss: 0.1172, step time: 0.0999\n",
      "23/223, train_loss: 0.1039, step time: 0.0997\n",
      "24/223, train_loss: 0.1034, step time: 0.1009\n",
      "25/223, train_loss: 0.1116, step time: 0.1042\n",
      "26/223, train_loss: 0.1137, step time: 0.1047\n",
      "27/223, train_loss: 0.0985, step time: 0.1074\n",
      "28/223, train_loss: 0.1113, step time: 0.1045\n",
      "29/223, train_loss: 0.1038, step time: 0.1026\n",
      "30/223, train_loss: 0.1048, step time: 0.1168\n",
      "31/223, train_loss: 0.1201, step time: 0.1369\n",
      "32/223, train_loss: 0.1135, step time: 0.1139\n",
      "33/223, train_loss: 0.1060, step time: 0.1156\n",
      "34/223, train_loss: 0.0989, step time: 0.1148\n",
      "35/223, train_loss: 0.1066, step time: 0.1001\n",
      "36/223, train_loss: 0.1000, step time: 0.1061\n",
      "37/223, train_loss: 0.1112, step time: 0.1385\n",
      "38/223, train_loss: 0.1174, step time: 0.1001\n",
      "39/223, train_loss: 0.1106, step time: 0.1005\n",
      "40/223, train_loss: 0.1220, step time: 0.1020\n",
      "41/223, train_loss: 0.1027, step time: 0.1158\n",
      "42/223, train_loss: 0.1082, step time: 0.1243\n",
      "43/223, train_loss: 0.0992, step time: 0.1107\n",
      "44/223, train_loss: 0.1209, step time: 0.1004\n",
      "45/223, train_loss: 0.1135, step time: 0.1023\n",
      "46/223, train_loss: 0.1397, step time: 0.1121\n",
      "47/223, train_loss: 0.1107, step time: 0.1345\n",
      "48/223, train_loss: 0.1092, step time: 0.1026\n",
      "49/223, train_loss: 0.1083, step time: 0.1042\n",
      "50/223, train_loss: 0.1172, step time: 0.1150\n",
      "51/223, train_loss: 0.1160, step time: 0.1123\n",
      "52/223, train_loss: 0.1089, step time: 0.1164\n",
      "53/223, train_loss: 0.1133, step time: 0.1024\n",
      "54/223, train_loss: 0.1017, step time: 0.1408\n",
      "55/223, train_loss: 0.1008, step time: 0.1352\n",
      "56/223, train_loss: 0.1076, step time: 0.1236\n",
      "57/223, train_loss: 0.1177, step time: 0.1112\n",
      "58/223, train_loss: 0.1204, step time: 0.1090\n",
      "59/223, train_loss: 0.1209, step time: 0.1188\n",
      "60/223, train_loss: 0.1142, step time: 0.1185\n",
      "61/223, train_loss: 0.1083, step time: 0.1005\n",
      "62/223, train_loss: 0.1192, step time: 0.1062\n",
      "63/223, train_loss: 0.1098, step time: 0.1098\n",
      "64/223, train_loss: 0.1325, step time: 0.1051\n",
      "65/223, train_loss: 0.1033, step time: 0.1222\n",
      "66/223, train_loss: 0.1130, step time: 0.1012\n",
      "67/223, train_loss: 0.1193, step time: 0.1004\n",
      "68/223, train_loss: 0.1160, step time: 0.1093\n",
      "69/223, train_loss: 0.1231, step time: 0.1131\n",
      "70/223, train_loss: 0.1181, step time: 0.1266\n",
      "71/223, train_loss: 0.1109, step time: 0.1009\n",
      "72/223, train_loss: 0.1173, step time: 0.1002\n",
      "73/223, train_loss: 0.1162, step time: 0.1230\n",
      "74/223, train_loss: 0.1021, step time: 0.1169\n",
      "75/223, train_loss: 0.1162, step time: 0.1006\n",
      "76/223, train_loss: 0.1035, step time: 0.1059\n",
      "77/223, train_loss: 0.1195, step time: 0.1057\n",
      "78/223, train_loss: 0.1206, step time: 0.1003\n",
      "79/223, train_loss: 0.1273, step time: 0.1010\n",
      "80/223, train_loss: 0.1148, step time: 0.1251\n",
      "81/223, train_loss: 0.1079, step time: 0.1022\n",
      "82/223, train_loss: 0.1141, step time: 0.1021\n",
      "83/223, train_loss: 0.1114, step time: 0.1228\n",
      "84/223, train_loss: 0.1123, step time: 0.1000\n",
      "85/223, train_loss: 0.1215, step time: 0.1047\n",
      "86/223, train_loss: 0.1121, step time: 0.1131\n",
      "87/223, train_loss: 0.1113, step time: 0.1047\n",
      "88/223, train_loss: 0.1017, step time: 0.1008\n",
      "89/223, train_loss: 0.1029, step time: 0.1129\n",
      "90/223, train_loss: 0.1132, step time: 0.1145\n",
      "91/223, train_loss: 0.1146, step time: 0.1121\n",
      "92/223, train_loss: 0.1107, step time: 0.1002\n",
      "93/223, train_loss: 0.1141, step time: 0.1017\n",
      "94/223, train_loss: 0.1085, step time: 0.1061\n",
      "95/223, train_loss: 0.1008, step time: 0.1020\n",
      "96/223, train_loss: 0.1190, step time: 0.1005\n",
      "97/223, train_loss: 0.1086, step time: 0.1105\n",
      "98/223, train_loss: 0.1227, step time: 0.1095\n",
      "99/223, train_loss: 0.1115, step time: 0.1006\n",
      "100/223, train_loss: 0.1141, step time: 0.1011\n",
      "101/223, train_loss: 0.1232, step time: 0.1104\n",
      "102/223, train_loss: 0.1087, step time: 0.1104\n",
      "103/223, train_loss: 0.0927, step time: 0.1023\n",
      "104/223, train_loss: 0.1103, step time: 0.1010\n",
      "105/223, train_loss: 0.1010, step time: 0.1097\n",
      "106/223, train_loss: 0.1193, step time: 0.1169\n",
      "107/223, train_loss: 0.1113, step time: 0.1317\n",
      "108/223, train_loss: 0.0986, step time: 0.1245\n",
      "109/223, train_loss: 0.1050, step time: 0.1047\n",
      "110/223, train_loss: 0.1190, step time: 0.1093\n",
      "111/223, train_loss: 0.1047, step time: 0.1206\n",
      "112/223, train_loss: 0.1007, step time: 0.1082\n",
      "113/223, train_loss: 0.1044, step time: 0.1049\n",
      "114/223, train_loss: 0.1009, step time: 0.1047\n",
      "115/223, train_loss: 0.1307, step time: 0.1134\n",
      "116/223, train_loss: 0.1050, step time: 0.1208\n",
      "117/223, train_loss: 0.1033, step time: 0.1170\n",
      "118/223, train_loss: 0.1140, step time: 0.1245\n",
      "119/223, train_loss: 0.1137, step time: 0.1231\n",
      "120/223, train_loss: 0.1076, step time: 0.1105\n",
      "121/223, train_loss: 0.1200, step time: 0.1258\n",
      "122/223, train_loss: 0.1011, step time: 0.1216\n",
      "123/223, train_loss: 0.1068, step time: 0.1116\n",
      "124/223, train_loss: 0.1070, step time: 0.1046\n",
      "125/223, train_loss: 0.1258, step time: 0.0999\n",
      "126/223, train_loss: 0.1087, step time: 0.0996\n",
      "127/223, train_loss: 0.1148, step time: 0.1194\n",
      "128/223, train_loss: 0.1117, step time: 0.1246\n",
      "129/223, train_loss: 0.1160, step time: 0.1005\n",
      "130/223, train_loss: 0.1085, step time: 0.1005\n",
      "131/223, train_loss: 0.1103, step time: 0.1007\n",
      "132/223, train_loss: 0.1194, step time: 0.1002\n",
      "133/223, train_loss: 0.1198, step time: 0.1011\n",
      "134/223, train_loss: 0.1149, step time: 0.1005\n",
      "135/223, train_loss: 0.1190, step time: 0.1005\n",
      "136/223, train_loss: 0.1096, step time: 0.1031\n",
      "137/223, train_loss: 0.1244, step time: 0.1067\n",
      "138/223, train_loss: 0.1028, step time: 0.1133\n",
      "139/223, train_loss: 0.1117, step time: 0.1019\n",
      "140/223, train_loss: 0.1255, step time: 0.1089\n",
      "141/223, train_loss: 0.1108, step time: 0.1014\n",
      "142/223, train_loss: 0.1040, step time: 0.1297\n",
      "143/223, train_loss: 0.1126, step time: 0.1052\n",
      "144/223, train_loss: 0.1134, step time: 0.1133\n",
      "145/223, train_loss: 0.1014, step time: 0.1191\n",
      "146/223, train_loss: 0.1240, step time: 0.1216\n",
      "147/223, train_loss: 0.1078, step time: 0.0993\n",
      "148/223, train_loss: 0.1149, step time: 0.1073\n",
      "149/223, train_loss: 0.1099, step time: 0.1104\n",
      "150/223, train_loss: 0.1110, step time: 0.1275\n",
      "151/223, train_loss: 0.1129, step time: 0.1008\n",
      "152/223, train_loss: 0.1110, step time: 0.1082\n",
      "153/223, train_loss: 0.1100, step time: 0.1183\n",
      "154/223, train_loss: 0.1212, step time: 0.1443\n",
      "155/223, train_loss: 0.1101, step time: 0.1007\n",
      "156/223, train_loss: 0.1076, step time: 0.1215\n",
      "157/223, train_loss: 0.1111, step time: 0.1011\n",
      "158/223, train_loss: 0.1033, step time: 0.1009\n",
      "159/223, train_loss: 0.1070, step time: 0.1527\n",
      "160/223, train_loss: 0.1012, step time: 0.1085\n",
      "161/223, train_loss: 0.1187, step time: 0.0999\n",
      "162/223, train_loss: 0.1213, step time: 0.0998\n",
      "163/223, train_loss: 0.1108, step time: 0.1230\n",
      "164/223, train_loss: 0.3106, step time: 0.1002\n",
      "165/223, train_loss: 0.1077, step time: 0.1556\n",
      "166/223, train_loss: 0.1170, step time: 0.1245\n",
      "167/223, train_loss: 0.1135, step time: 0.1008\n",
      "168/223, train_loss: 0.1041, step time: 0.1267\n",
      "169/223, train_loss: 0.1040, step time: 0.1003\n",
      "170/223, train_loss: 0.1008, step time: 0.1018\n",
      "171/223, train_loss: 0.1162, step time: 0.1023\n",
      "172/223, train_loss: 0.1146, step time: 0.1002\n",
      "173/223, train_loss: 0.1132, step time: 0.1025\n",
      "174/223, train_loss: 0.0957, step time: 0.0998\n",
      "175/223, train_loss: 0.1105, step time: 0.1064\n",
      "176/223, train_loss: 0.1048, step time: 0.1103\n",
      "177/223, train_loss: 0.1204, step time: 0.1082\n",
      "178/223, train_loss: 0.1061, step time: 0.1288\n",
      "179/223, train_loss: 0.1073, step time: 0.1115\n",
      "180/223, train_loss: 0.1091, step time: 0.1104\n",
      "181/223, train_loss: 0.1203, step time: 0.1099\n",
      "182/223, train_loss: 0.1086, step time: 0.1618\n",
      "183/223, train_loss: 0.1062, step time: 0.1171\n",
      "184/223, train_loss: 0.1202, step time: 0.1100\n",
      "185/223, train_loss: 0.1002, step time: 0.1123\n",
      "186/223, train_loss: 0.1110, step time: 0.1090\n",
      "187/223, train_loss: 0.1011, step time: 0.1003\n",
      "188/223, train_loss: 0.1155, step time: 0.1090\n",
      "189/223, train_loss: 0.1157, step time: 0.1048\n",
      "190/223, train_loss: 0.1068, step time: 0.1188\n",
      "191/223, train_loss: 0.1148, step time: 0.1152\n",
      "192/223, train_loss: 0.1153, step time: 0.1202\n",
      "193/223, train_loss: 0.1160, step time: 0.1137\n",
      "194/223, train_loss: 0.1146, step time: 0.1041\n",
      "195/223, train_loss: 0.1217, step time: 0.0998\n",
      "196/223, train_loss: 0.1097, step time: 0.1004\n",
      "197/223, train_loss: 0.1019, step time: 0.1157\n",
      "198/223, train_loss: 0.1033, step time: 0.1067\n",
      "199/223, train_loss: 0.1017, step time: 0.1014\n",
      "200/223, train_loss: 0.0979, step time: 0.1139\n",
      "201/223, train_loss: 0.1036, step time: 0.1210\n",
      "202/223, train_loss: 0.1090, step time: 0.1003\n",
      "203/223, train_loss: 0.1081, step time: 0.1008\n",
      "204/223, train_loss: 0.1177, step time: 0.1089\n",
      "205/223, train_loss: 0.1134, step time: 0.1025\n",
      "206/223, train_loss: 0.1256, step time: 0.1069\n",
      "207/223, train_loss: 0.1097, step time: 0.1054\n",
      "208/223, train_loss: 0.0990, step time: 0.1172\n",
      "209/223, train_loss: 0.1086, step time: 0.1069\n",
      "210/223, train_loss: 0.1250, step time: 0.1109\n",
      "211/223, train_loss: 0.1085, step time: 0.1063\n",
      "212/223, train_loss: 0.1102, step time: 0.1126\n",
      "213/223, train_loss: 0.1112, step time: 0.1024\n",
      "214/223, train_loss: 0.0989, step time: 0.1025\n",
      "215/223, train_loss: 0.1025, step time: 0.1009\n",
      "216/223, train_loss: 0.1000, step time: 0.1362\n",
      "217/223, train_loss: 0.1122, step time: 0.1137\n",
      "218/223, train_loss: 0.1174, step time: 0.1136\n",
      "219/223, train_loss: 0.1180, step time: 0.1253\n",
      "220/223, train_loss: 0.1257, step time: 0.0999\n",
      "221/223, train_loss: 0.1120, step time: 0.0996\n",
      "222/223, train_loss: 0.1117, step time: 0.0997\n",
      "223/223, train_loss: 0.1244, step time: 0.1002\n",
      "epoch 113 average loss: 0.1124\n",
      "time consuming of epoch 113 is: 88.2913\n",
      "----------\n",
      "epoch 114/300\n",
      "1/223, train_loss: 0.1067, step time: 0.1005\n",
      "2/223, train_loss: 0.1029, step time: 0.1005\n",
      "3/223, train_loss: 0.1039, step time: 0.0994\n",
      "4/223, train_loss: 0.0985, step time: 0.1065\n",
      "5/223, train_loss: 0.1190, step time: 0.0999\n",
      "6/223, train_loss: 0.1034, step time: 0.1004\n",
      "7/223, train_loss: 0.1118, step time: 0.1006\n",
      "8/223, train_loss: 0.0984, step time: 0.1013\n",
      "9/223, train_loss: 0.1304, step time: 0.1051\n",
      "10/223, train_loss: 0.1029, step time: 0.1033\n",
      "11/223, train_loss: 0.1011, step time: 0.1038\n",
      "12/223, train_loss: 0.1072, step time: 0.1014\n",
      "13/223, train_loss: 0.1120, step time: 0.1005\n",
      "14/223, train_loss: 0.1094, step time: 0.1225\n",
      "15/223, train_loss: 0.1209, step time: 0.1021\n",
      "16/223, train_loss: 0.1025, step time: 0.1045\n",
      "17/223, train_loss: 0.1262, step time: 0.1006\n",
      "18/223, train_loss: 0.1097, step time: 0.0998\n",
      "19/223, train_loss: 0.1111, step time: 0.1006\n",
      "20/223, train_loss: 0.1152, step time: 0.1015\n",
      "21/223, train_loss: 0.0994, step time: 0.1017\n",
      "22/223, train_loss: 0.1157, step time: 0.1003\n",
      "23/223, train_loss: 0.1122, step time: 0.1003\n",
      "24/223, train_loss: 0.1123, step time: 0.1010\n",
      "25/223, train_loss: 0.1030, step time: 0.1275\n",
      "26/223, train_loss: 0.1184, step time: 0.1185\n",
      "27/223, train_loss: 0.1139, step time: 0.1082\n",
      "28/223, train_loss: 0.1084, step time: 0.1007\n",
      "29/223, train_loss: 0.1172, step time: 0.1050\n",
      "30/223, train_loss: 0.1085, step time: 0.1570\n",
      "31/223, train_loss: 0.0982, step time: 0.1220\n",
      "32/223, train_loss: 0.1075, step time: 0.1005\n",
      "33/223, train_loss: 0.1129, step time: 0.0993\n",
      "34/223, train_loss: 0.1183, step time: 0.0998\n",
      "35/223, train_loss: 0.1129, step time: 0.1070\n",
      "36/223, train_loss: 0.1099, step time: 0.1028\n",
      "37/223, train_loss: 0.1186, step time: 0.1062\n",
      "38/223, train_loss: 0.1148, step time: 0.1044\n",
      "39/223, train_loss: 0.1253, step time: 0.1231\n",
      "40/223, train_loss: 0.1251, step time: 0.1090\n",
      "41/223, train_loss: 0.1230, step time: 0.1129\n",
      "42/223, train_loss: 0.1214, step time: 0.1415\n",
      "43/223, train_loss: 0.1176, step time: 0.1526\n",
      "44/223, train_loss: 0.1070, step time: 0.1005\n",
      "45/223, train_loss: 0.1048, step time: 0.1181\n",
      "46/223, train_loss: 0.1126, step time: 0.1123\n",
      "47/223, train_loss: 0.1088, step time: 0.1124\n",
      "48/223, train_loss: 0.1216, step time: 0.1091\n",
      "49/223, train_loss: 0.1060, step time: 0.1396\n",
      "50/223, train_loss: 0.1051, step time: 0.1168\n",
      "51/223, train_loss: 0.1088, step time: 0.1228\n",
      "52/223, train_loss: 0.1057, step time: 0.1009\n",
      "53/223, train_loss: 0.1070, step time: 0.1598\n",
      "54/223, train_loss: 0.1167, step time: 0.1132\n",
      "55/223, train_loss: 0.1126, step time: 0.1082\n",
      "56/223, train_loss: 0.1212, step time: 0.1000\n",
      "57/223, train_loss: 0.1066, step time: 0.1273\n",
      "58/223, train_loss: 0.1075, step time: 0.1106\n",
      "59/223, train_loss: 0.1145, step time: 0.1218\n",
      "60/223, train_loss: 0.1005, step time: 0.1026\n",
      "61/223, train_loss: 0.1219, step time: 0.1225\n",
      "62/223, train_loss: 0.1022, step time: 0.1088\n",
      "63/223, train_loss: 0.1064, step time: 0.1404\n",
      "64/223, train_loss: 0.1190, step time: 0.1020\n",
      "65/223, train_loss: 0.1068, step time: 0.1248\n",
      "66/223, train_loss: 0.1329, step time: 0.1125\n",
      "67/223, train_loss: 0.1162, step time: 0.1167\n",
      "68/223, train_loss: 0.1134, step time: 0.1191\n",
      "69/223, train_loss: 0.1009, step time: 0.1119\n",
      "70/223, train_loss: 0.1005, step time: 0.1072\n",
      "71/223, train_loss: 0.1114, step time: 0.1001\n",
      "72/223, train_loss: 0.1089, step time: 0.1010\n",
      "73/223, train_loss: 0.1147, step time: 0.1062\n",
      "74/223, train_loss: 0.1055, step time: 0.1210\n",
      "75/223, train_loss: 0.1116, step time: 0.1237\n",
      "76/223, train_loss: 0.1284, step time: 0.1009\n",
      "77/223, train_loss: 0.1261, step time: 0.1328\n",
      "78/223, train_loss: 0.1110, step time: 0.1114\n",
      "79/223, train_loss: 0.1095, step time: 0.1122\n",
      "80/223, train_loss: 0.1050, step time: 0.1264\n",
      "81/223, train_loss: 0.1074, step time: 0.1128\n",
      "82/223, train_loss: 0.1134, step time: 0.1303\n",
      "83/223, train_loss: 0.1101, step time: 0.1053\n",
      "84/223, train_loss: 0.1113, step time: 0.0996\n",
      "85/223, train_loss: 0.1180, step time: 0.1102\n",
      "86/223, train_loss: 0.1169, step time: 0.1120\n",
      "87/223, train_loss: 0.1093, step time: 0.1128\n",
      "88/223, train_loss: 0.1105, step time: 0.1214\n",
      "89/223, train_loss: 0.1060, step time: 0.1134\n",
      "90/223, train_loss: 0.1047, step time: 0.1007\n",
      "91/223, train_loss: 0.1096, step time: 0.1007\n",
      "92/223, train_loss: 0.1105, step time: 0.1037\n",
      "93/223, train_loss: 0.1204, step time: 0.1086\n",
      "94/223, train_loss: 0.1091, step time: 0.1223\n",
      "95/223, train_loss: 0.1237, step time: 0.1150\n",
      "96/223, train_loss: 0.1002, step time: 0.1003\n",
      "97/223, train_loss: 0.1042, step time: 0.1144\n",
      "98/223, train_loss: 0.1052, step time: 0.1004\n",
      "99/223, train_loss: 0.1144, step time: 0.1067\n",
      "100/223, train_loss: 0.1175, step time: 0.1037\n",
      "101/223, train_loss: 0.1169, step time: 0.1113\n",
      "102/223, train_loss: 0.1186, step time: 0.1259\n",
      "103/223, train_loss: 0.1269, step time: 0.1166\n",
      "104/223, train_loss: 0.1130, step time: 0.1030\n",
      "105/223, train_loss: 0.1212, step time: 0.1073\n",
      "106/223, train_loss: 0.1162, step time: 0.1218\n",
      "107/223, train_loss: 0.1213, step time: 0.1199\n",
      "108/223, train_loss: 0.1093, step time: 0.1146\n",
      "109/223, train_loss: 0.1131, step time: 0.1037\n",
      "110/223, train_loss: 0.1118, step time: 0.1004\n",
      "111/223, train_loss: 0.1216, step time: 0.1363\n",
      "112/223, train_loss: 0.1058, step time: 0.1146\n",
      "113/223, train_loss: 0.1183, step time: 0.1000\n",
      "114/223, train_loss: 0.1192, step time: 0.1004\n",
      "115/223, train_loss: 0.1161, step time: 0.1004\n",
      "116/223, train_loss: 0.1097, step time: 0.1051\n",
      "117/223, train_loss: 0.1062, step time: 0.1161\n",
      "118/223, train_loss: 0.1179, step time: 0.1003\n",
      "119/223, train_loss: 0.1134, step time: 0.0999\n",
      "120/223, train_loss: 0.1112, step time: 0.1002\n",
      "121/223, train_loss: 0.1221, step time: 0.1110\n",
      "122/223, train_loss: 0.1085, step time: 0.1033\n",
      "123/223, train_loss: 0.0993, step time: 0.1250\n",
      "124/223, train_loss: 0.1098, step time: 0.1217\n",
      "125/223, train_loss: 0.1123, step time: 0.1008\n",
      "126/223, train_loss: 0.1056, step time: 0.1011\n",
      "127/223, train_loss: 0.1093, step time: 0.1011\n",
      "128/223, train_loss: 0.1102, step time: 0.1060\n",
      "129/223, train_loss: 0.0994, step time: 0.1181\n",
      "130/223, train_loss: 0.0968, step time: 0.1081\n",
      "131/223, train_loss: 0.1005, step time: 0.1125\n",
      "132/223, train_loss: 0.1112, step time: 0.1062\n",
      "133/223, train_loss: 0.0997, step time: 0.1130\n",
      "134/223, train_loss: 0.1154, step time: 0.1005\n",
      "135/223, train_loss: 0.1130, step time: 0.1002\n",
      "136/223, train_loss: 0.1363, step time: 0.1105\n",
      "137/223, train_loss: 0.1208, step time: 0.1059\n",
      "138/223, train_loss: 0.1089, step time: 0.1108\n",
      "139/223, train_loss: 0.1046, step time: 0.1131\n",
      "140/223, train_loss: 0.1026, step time: 0.1478\n",
      "141/223, train_loss: 0.1033, step time: 0.1178\n",
      "142/223, train_loss: 0.1156, step time: 0.1121\n",
      "143/223, train_loss: 0.1062, step time: 0.1075\n",
      "144/223, train_loss: 0.3117, step time: 0.1088\n",
      "145/223, train_loss: 0.1038, step time: 0.1163\n",
      "146/223, train_loss: 0.1156, step time: 0.1120\n",
      "147/223, train_loss: 0.1141, step time: 0.1030\n",
      "148/223, train_loss: 0.1254, step time: 0.1108\n",
      "149/223, train_loss: 0.1245, step time: 0.1097\n",
      "150/223, train_loss: 0.1204, step time: 0.1050\n",
      "151/223, train_loss: 0.1248, step time: 0.1282\n",
      "152/223, train_loss: 0.1039, step time: 0.1343\n",
      "153/223, train_loss: 0.1089, step time: 0.1197\n",
      "154/223, train_loss: 0.1112, step time: 0.1262\n",
      "155/223, train_loss: 0.1000, step time: 0.1159\n",
      "156/223, train_loss: 0.1056, step time: 0.1131\n",
      "157/223, train_loss: 0.1118, step time: 0.1138\n",
      "158/223, train_loss: 0.1042, step time: 0.1061\n",
      "159/223, train_loss: 0.1237, step time: 0.1084\n",
      "160/223, train_loss: 0.1106, step time: 0.1126\n",
      "161/223, train_loss: 0.1218, step time: 0.1140\n",
      "162/223, train_loss: 0.1239, step time: 0.1068\n",
      "163/223, train_loss: 0.0998, step time: 0.1021\n",
      "164/223, train_loss: 0.1077, step time: 0.1008\n",
      "165/223, train_loss: 0.1034, step time: 0.1752\n",
      "166/223, train_loss: 0.1153, step time: 0.1000\n",
      "167/223, train_loss: 0.1046, step time: 0.0998\n",
      "168/223, train_loss: 0.1160, step time: 0.1086\n",
      "169/223, train_loss: 0.1062, step time: 0.1167\n",
      "170/223, train_loss: 0.1064, step time: 0.1007\n",
      "171/223, train_loss: 0.1076, step time: 0.1133\n",
      "172/223, train_loss: 0.0996, step time: 0.1010\n",
      "173/223, train_loss: 0.1063, step time: 0.1124\n",
      "174/223, train_loss: 0.1090, step time: 0.1063\n",
      "175/223, train_loss: 0.1020, step time: 0.1020\n",
      "176/223, train_loss: 0.0940, step time: 0.1022\n",
      "177/223, train_loss: 0.0998, step time: 0.1044\n",
      "178/223, train_loss: 0.1182, step time: 0.1294\n",
      "179/223, train_loss: 0.1105, step time: 0.1398\n",
      "180/223, train_loss: 0.1162, step time: 0.1050\n",
      "181/223, train_loss: 0.1115, step time: 0.1144\n",
      "182/223, train_loss: 0.1310, step time: 0.1402\n",
      "183/223, train_loss: 0.1169, step time: 0.1001\n",
      "184/223, train_loss: 0.1135, step time: 0.1009\n",
      "185/223, train_loss: 0.1016, step time: 0.0993\n",
      "186/223, train_loss: 0.1260, step time: 0.0996\n",
      "187/223, train_loss: 0.1086, step time: 0.1001\n",
      "188/223, train_loss: 0.1082, step time: 0.1012\n",
      "189/223, train_loss: 0.1041, step time: 0.1006\n",
      "190/223, train_loss: 0.1040, step time: 0.1005\n",
      "191/223, train_loss: 0.1207, step time: 0.0998\n",
      "192/223, train_loss: 0.1276, step time: 0.1011\n",
      "193/223, train_loss: 0.1210, step time: 0.1009\n",
      "194/223, train_loss: 0.0999, step time: 0.1129\n",
      "195/223, train_loss: 0.1100, step time: 0.1341\n",
      "196/223, train_loss: 0.1061, step time: 0.1010\n",
      "197/223, train_loss: 0.1112, step time: 0.1001\n",
      "198/223, train_loss: 0.1060, step time: 0.1037\n",
      "199/223, train_loss: 0.1179, step time: 0.1134\n",
      "200/223, train_loss: 0.1105, step time: 0.1017\n",
      "201/223, train_loss: 0.1094, step time: 0.1126\n",
      "202/223, train_loss: 0.0993, step time: 0.1096\n",
      "203/223, train_loss: 0.1126, step time: 0.1178\n",
      "204/223, train_loss: 0.1053, step time: 0.1143\n",
      "205/223, train_loss: 0.1067, step time: 0.1005\n",
      "206/223, train_loss: 0.1276, step time: 0.1162\n",
      "207/223, train_loss: 0.1109, step time: 0.1286\n",
      "208/223, train_loss: 0.1063, step time: 0.1079\n",
      "209/223, train_loss: 0.1140, step time: 0.1040\n",
      "210/223, train_loss: 0.1068, step time: 0.1097\n",
      "211/223, train_loss: 0.1187, step time: 0.1145\n",
      "212/223, train_loss: 0.1192, step time: 0.1108\n",
      "213/223, train_loss: 0.1156, step time: 0.0999\n",
      "214/223, train_loss: 0.1155, step time: 0.1004\n",
      "215/223, train_loss: 0.1118, step time: 0.1003\n",
      "216/223, train_loss: 0.0962, step time: 0.1100\n",
      "217/223, train_loss: 0.1067, step time: 0.1002\n",
      "218/223, train_loss: 0.1142, step time: 0.0999\n",
      "219/223, train_loss: 0.1094, step time: 0.0995\n",
      "220/223, train_loss: 0.1042, step time: 0.1135\n",
      "221/223, train_loss: 0.1034, step time: 0.0990\n",
      "222/223, train_loss: 0.1117, step time: 0.0991\n",
      "223/223, train_loss: 0.1106, step time: 0.1000\n",
      "epoch 114 average loss: 0.1123\n",
      "time consuming of epoch 114 is: 90.8718\n",
      "----------\n",
      "epoch 115/300\n",
      "1/223, train_loss: 0.1041, step time: 0.1010\n",
      "2/223, train_loss: 0.1011, step time: 0.1009\n",
      "3/223, train_loss: 0.0963, step time: 0.1051\n",
      "4/223, train_loss: 0.1081, step time: 0.1099\n",
      "5/223, train_loss: 0.1005, step time: 0.1193\n",
      "6/223, train_loss: 0.1116, step time: 0.1028\n",
      "7/223, train_loss: 0.1044, step time: 0.1145\n",
      "8/223, train_loss: 0.0967, step time: 0.1003\n",
      "9/223, train_loss: 0.1095, step time: 0.1063\n",
      "10/223, train_loss: 0.1261, step time: 0.1004\n",
      "11/223, train_loss: 0.1124, step time: 0.1146\n",
      "12/223, train_loss: 0.1009, step time: 0.1244\n",
      "13/223, train_loss: 0.1177, step time: 0.1151\n",
      "14/223, train_loss: 0.1023, step time: 0.1206\n",
      "15/223, train_loss: 0.1029, step time: 0.1026\n",
      "16/223, train_loss: 0.1041, step time: 0.1129\n",
      "17/223, train_loss: 0.1067, step time: 0.1114\n",
      "18/223, train_loss: 0.1009, step time: 0.1153\n",
      "19/223, train_loss: 0.1046, step time: 0.1234\n",
      "20/223, train_loss: 0.1080, step time: 0.1287\n",
      "21/223, train_loss: 0.1001, step time: 0.1002\n",
      "22/223, train_loss: 0.1114, step time: 0.1115\n",
      "23/223, train_loss: 0.1050, step time: 0.1076\n",
      "24/223, train_loss: 0.1100, step time: 0.1063\n",
      "25/223, train_loss: 0.1084, step time: 0.1080\n",
      "26/223, train_loss: 0.1033, step time: 0.1098\n",
      "27/223, train_loss: 0.1087, step time: 0.1296\n",
      "28/223, train_loss: 0.1179, step time: 0.1014\n",
      "29/223, train_loss: 0.1439, step time: 0.1323\n",
      "30/223, train_loss: 0.1093, step time: 0.1106\n",
      "31/223, train_loss: 0.1193, step time: 0.1303\n",
      "32/223, train_loss: 0.1053, step time: 0.1136\n",
      "33/223, train_loss: 0.1058, step time: 0.1004\n",
      "34/223, train_loss: 0.1088, step time: 0.1434\n",
      "35/223, train_loss: 0.1122, step time: 0.1001\n",
      "36/223, train_loss: 0.1084, step time: 0.1018\n",
      "37/223, train_loss: 0.1037, step time: 0.1145\n",
      "38/223, train_loss: 0.1055, step time: 0.1380\n",
      "39/223, train_loss: 0.1106, step time: 0.1151\n",
      "40/223, train_loss: 0.1057, step time: 0.1178\n",
      "41/223, train_loss: 0.0988, step time: 0.1067\n",
      "42/223, train_loss: 0.1067, step time: 0.1209\n",
      "43/223, train_loss: 0.1186, step time: 0.1035\n",
      "44/223, train_loss: 0.3029, step time: 0.1146\n",
      "45/223, train_loss: 0.1024, step time: 0.1137\n",
      "46/223, train_loss: 0.1112, step time: 0.1107\n",
      "47/223, train_loss: 0.1101, step time: 0.1105\n",
      "48/223, train_loss: 0.1128, step time: 0.1117\n",
      "49/223, train_loss: 0.1044, step time: 0.1108\n",
      "50/223, train_loss: 0.0987, step time: 0.1006\n",
      "51/223, train_loss: 0.1216, step time: 0.1009\n",
      "52/223, train_loss: 0.1044, step time: 0.1209\n",
      "53/223, train_loss: 0.1137, step time: 0.0999\n",
      "54/223, train_loss: 0.1158, step time: 0.1029\n",
      "55/223, train_loss: 0.1117, step time: 0.1010\n",
      "56/223, train_loss: 0.1052, step time: 0.1104\n",
      "57/223, train_loss: 0.1028, step time: 0.1185\n",
      "58/223, train_loss: 0.1162, step time: 0.1160\n",
      "59/223, train_loss: 0.1088, step time: 0.1134\n",
      "60/223, train_loss: 0.1087, step time: 0.1270\n",
      "61/223, train_loss: 0.1133, step time: 0.0996\n",
      "62/223, train_loss: 0.1202, step time: 0.1149\n",
      "63/223, train_loss: 0.1089, step time: 0.1147\n",
      "64/223, train_loss: 0.1212, step time: 0.1213\n",
      "65/223, train_loss: 0.1131, step time: 0.1228\n",
      "66/223, train_loss: 0.1019, step time: 0.1194\n",
      "67/223, train_loss: 0.1105, step time: 0.1077\n",
      "68/223, train_loss: 0.1255, step time: 0.1003\n",
      "69/223, train_loss: 0.1141, step time: 0.1216\n",
      "70/223, train_loss: 0.1150, step time: 0.1003\n",
      "71/223, train_loss: 0.1121, step time: 0.1155\n",
      "72/223, train_loss: 0.1129, step time: 0.1026\n",
      "73/223, train_loss: 0.1127, step time: 0.0999\n",
      "74/223, train_loss: 0.1178, step time: 0.1057\n",
      "75/223, train_loss: 0.1193, step time: 0.1151\n",
      "76/223, train_loss: 0.1079, step time: 0.1191\n",
      "77/223, train_loss: 0.1125, step time: 0.1142\n",
      "78/223, train_loss: 0.1075, step time: 0.1226\n",
      "79/223, train_loss: 0.1155, step time: 0.1068\n",
      "80/223, train_loss: 0.1196, step time: 0.1040\n",
      "81/223, train_loss: 0.1098, step time: 0.1118\n",
      "82/223, train_loss: 0.1050, step time: 0.1027\n",
      "83/223, train_loss: 0.1248, step time: 0.1033\n",
      "84/223, train_loss: 0.1039, step time: 0.1021\n",
      "85/223, train_loss: 0.1152, step time: 0.1065\n",
      "86/223, train_loss: 0.1085, step time: 0.1108\n",
      "87/223, train_loss: 0.1054, step time: 0.1000\n",
      "88/223, train_loss: 0.1117, step time: 0.1010\n",
      "89/223, train_loss: 0.1272, step time: 0.1218\n",
      "90/223, train_loss: 0.1083, step time: 0.1135\n",
      "91/223, train_loss: 0.0994, step time: 0.1341\n",
      "92/223, train_loss: 0.1183, step time: 0.0994\n",
      "93/223, train_loss: 0.1207, step time: 0.1166\n",
      "94/223, train_loss: 0.1209, step time: 0.1247\n",
      "95/223, train_loss: 0.1153, step time: 0.1250\n",
      "96/223, train_loss: 0.1201, step time: 0.1078\n",
      "97/223, train_loss: 0.1190, step time: 0.1253\n",
      "98/223, train_loss: 0.1228, step time: 0.1221\n",
      "99/223, train_loss: 0.1090, step time: 0.1064\n",
      "100/223, train_loss: 0.1010, step time: 0.1471\n",
      "101/223, train_loss: 0.1138, step time: 0.1078\n",
      "102/223, train_loss: 0.1036, step time: 0.1098\n",
      "103/223, train_loss: 0.1112, step time: 0.1312\n",
      "104/223, train_loss: 0.1097, step time: 0.1356\n",
      "105/223, train_loss: 0.1015, step time: 0.1121\n",
      "106/223, train_loss: 0.1110, step time: 0.1116\n",
      "107/223, train_loss: 0.0981, step time: 0.1117\n",
      "108/223, train_loss: 0.1183, step time: 0.1015\n",
      "109/223, train_loss: 0.1265, step time: 0.1129\n",
      "110/223, train_loss: 0.1103, step time: 0.1116\n",
      "111/223, train_loss: 0.1216, step time: 0.1009\n",
      "112/223, train_loss: 0.1036, step time: 0.1075\n",
      "113/223, train_loss: 0.1022, step time: 0.1192\n",
      "114/223, train_loss: 0.1110, step time: 0.1117\n",
      "115/223, train_loss: 0.1195, step time: 0.1559\n",
      "116/223, train_loss: 0.1106, step time: 0.1206\n",
      "117/223, train_loss: 0.1087, step time: 0.1115\n",
      "118/223, train_loss: 0.1187, step time: 0.1010\n",
      "119/223, train_loss: 0.1111, step time: 0.1006\n",
      "120/223, train_loss: 0.1173, step time: 0.1012\n",
      "121/223, train_loss: 0.1059, step time: 0.1133\n",
      "122/223, train_loss: 0.1119, step time: 0.1115\n",
      "123/223, train_loss: 0.1072, step time: 0.1157\n",
      "124/223, train_loss: 0.1055, step time: 0.1111\n",
      "125/223, train_loss: 0.1218, step time: 0.1000\n",
      "126/223, train_loss: 0.1206, step time: 0.1019\n",
      "127/223, train_loss: 0.1069, step time: 0.1158\n",
      "128/223, train_loss: 0.1095, step time: 0.1006\n",
      "129/223, train_loss: 0.1140, step time: 0.1176\n",
      "130/223, train_loss: 0.1210, step time: 0.1016\n",
      "131/223, train_loss: 0.1078, step time: 0.1064\n",
      "132/223, train_loss: 0.1031, step time: 0.1043\n",
      "133/223, train_loss: 0.1219, step time: 0.1297\n",
      "134/223, train_loss: 0.1125, step time: 0.1077\n",
      "135/223, train_loss: 0.1105, step time: 0.1004\n",
      "136/223, train_loss: 0.1117, step time: 0.1011\n",
      "137/223, train_loss: 0.1079, step time: 0.1067\n",
      "138/223, train_loss: 0.0993, step time: 0.1006\n",
      "139/223, train_loss: 0.1213, step time: 0.1008\n",
      "140/223, train_loss: 0.1185, step time: 0.1006\n",
      "141/223, train_loss: 0.1129, step time: 0.1105\n",
      "142/223, train_loss: 0.1086, step time: 0.1006\n",
      "143/223, train_loss: 0.1155, step time: 0.1003\n",
      "144/223, train_loss: 0.1008, step time: 0.1004\n",
      "145/223, train_loss: 0.1124, step time: 0.1040\n",
      "146/223, train_loss: 0.1002, step time: 0.1045\n",
      "147/223, train_loss: 0.1153, step time: 0.1191\n",
      "148/223, train_loss: 0.1142, step time: 0.1217\n",
      "149/223, train_loss: 0.1114, step time: 0.1039\n",
      "150/223, train_loss: 0.1112, step time: 0.1013\n",
      "151/223, train_loss: 0.1118, step time: 0.1013\n",
      "152/223, train_loss: 0.1262, step time: 0.1016\n",
      "153/223, train_loss: 0.1138, step time: 0.1117\n",
      "154/223, train_loss: 0.1097, step time: 0.1174\n",
      "155/223, train_loss: 0.1183, step time: 0.1112\n",
      "156/223, train_loss: 0.1118, step time: 0.1087\n",
      "157/223, train_loss: 0.1220, step time: 0.1081\n",
      "158/223, train_loss: 0.1136, step time: 0.1064\n",
      "159/223, train_loss: 0.1075, step time: 0.1341\n",
      "160/223, train_loss: 0.1113, step time: 0.1036\n",
      "161/223, train_loss: 0.1191, step time: 0.1002\n",
      "162/223, train_loss: 0.1020, step time: 0.1082\n",
      "163/223, train_loss: 0.1021, step time: 0.1148\n",
      "164/223, train_loss: 0.1017, step time: 0.1159\n",
      "165/223, train_loss: 0.1108, step time: 0.1071\n",
      "166/223, train_loss: 0.1071, step time: 0.1361\n",
      "167/223, train_loss: 0.1145, step time: 0.1064\n",
      "168/223, train_loss: 0.1105, step time: 0.1205\n",
      "169/223, train_loss: 0.1082, step time: 0.1176\n",
      "170/223, train_loss: 0.1090, step time: 0.1176\n",
      "171/223, train_loss: 0.1119, step time: 0.1159\n",
      "172/223, train_loss: 0.1060, step time: 0.1109\n",
      "173/223, train_loss: 0.1060, step time: 0.1054\n",
      "174/223, train_loss: 0.1049, step time: 0.1277\n",
      "175/223, train_loss: 0.1116, step time: 0.1142\n",
      "176/223, train_loss: 0.1060, step time: 0.1001\n",
      "177/223, train_loss: 0.1148, step time: 0.1062\n",
      "178/223, train_loss: 0.1123, step time: 0.1213\n",
      "179/223, train_loss: 0.1152, step time: 0.1406\n",
      "180/223, train_loss: 0.1191, step time: 0.1044\n",
      "181/223, train_loss: 0.1179, step time: 0.1151\n",
      "182/223, train_loss: 0.1247, step time: 0.1046\n",
      "183/223, train_loss: 0.1035, step time: 0.1195\n",
      "184/223, train_loss: 0.1080, step time: 0.1004\n",
      "185/223, train_loss: 0.1072, step time: 0.1151\n",
      "186/223, train_loss: 0.1039, step time: 0.1116\n",
      "187/223, train_loss: 0.1229, step time: 0.1401\n",
      "188/223, train_loss: 0.0992, step time: 0.1147\n",
      "189/223, train_loss: 0.1260, step time: 0.1143\n",
      "190/223, train_loss: 0.1089, step time: 0.1008\n",
      "191/223, train_loss: 0.1153, step time: 0.1153\n",
      "192/223, train_loss: 0.1202, step time: 0.1023\n",
      "193/223, train_loss: 0.1109, step time: 0.1200\n",
      "194/223, train_loss: 0.1026, step time: 0.1093\n",
      "195/223, train_loss: 0.1062, step time: 0.1226\n",
      "196/223, train_loss: 0.1071, step time: 0.1220\n",
      "197/223, train_loss: 0.1134, step time: 0.1091\n",
      "198/223, train_loss: 0.1030, step time: 0.1120\n",
      "199/223, train_loss: 0.1057, step time: 0.1077\n",
      "200/223, train_loss: 0.1251, step time: 0.1173\n",
      "201/223, train_loss: 0.1237, step time: 0.1215\n",
      "202/223, train_loss: 0.1102, step time: 0.0998\n",
      "203/223, train_loss: 0.1136, step time: 0.1102\n",
      "204/223, train_loss: 0.1153, step time: 0.1006\n",
      "205/223, train_loss: 0.1145, step time: 0.1116\n",
      "206/223, train_loss: 0.0949, step time: 0.1158\n",
      "207/223, train_loss: 0.1015, step time: 0.1190\n",
      "208/223, train_loss: 0.1063, step time: 0.1047\n",
      "209/223, train_loss: 0.1122, step time: 0.1263\n",
      "210/223, train_loss: 0.0997, step time: 0.1259\n",
      "211/223, train_loss: 0.1047, step time: 0.1270\n",
      "212/223, train_loss: 0.1154, step time: 0.1019\n",
      "213/223, train_loss: 0.0986, step time: 0.1276\n",
      "214/223, train_loss: 0.1009, step time: 0.1105\n",
      "215/223, train_loss: 0.1134, step time: 0.1014\n",
      "216/223, train_loss: 0.1067, step time: 0.1022\n",
      "217/223, train_loss: 0.1075, step time: 0.1070\n",
      "218/223, train_loss: 0.1062, step time: 0.1105\n",
      "219/223, train_loss: 0.1066, step time: 0.1001\n",
      "220/223, train_loss: 0.1070, step time: 0.0995\n",
      "221/223, train_loss: 0.1256, step time: 0.0990\n",
      "222/223, train_loss: 0.1006, step time: 0.0995\n",
      "223/223, train_loss: 0.1021, step time: 0.1005\n",
      "epoch 115 average loss: 0.1115\n",
      "saved new best metric model\n",
      "current epoch: 115 current mean dice: 0.8527 tc: 0.9191 wt: 0.8642 et: 0.7747\n",
      "best mean dice: 0.8527 at epoch: 115\n",
      "time consuming of epoch 115 is: 91.3621\n",
      "----------\n",
      "epoch 116/300\n",
      "1/223, train_loss: 0.1128, step time: 0.1063\n",
      "2/223, train_loss: 0.1147, step time: 0.1005\n",
      "3/223, train_loss: 0.1115, step time: 0.1009\n",
      "4/223, train_loss: 0.1272, step time: 0.1015\n",
      "5/223, train_loss: 0.1110, step time: 0.1188\n",
      "6/223, train_loss: 0.1078, step time: 0.1024\n",
      "7/223, train_loss: 0.1026, step time: 0.1076\n",
      "8/223, train_loss: 0.1186, step time: 0.1181\n",
      "9/223, train_loss: 0.1038, step time: 0.1093\n",
      "10/223, train_loss: 0.1100, step time: 0.1001\n",
      "11/223, train_loss: 0.1153, step time: 0.1161\n",
      "12/223, train_loss: 0.0997, step time: 0.1018\n",
      "13/223, train_loss: 0.1182, step time: 0.1148\n",
      "14/223, train_loss: 0.1090, step time: 0.1051\n",
      "15/223, train_loss: 0.1016, step time: 0.1135\n",
      "16/223, train_loss: 0.1101, step time: 0.1050\n",
      "17/223, train_loss: 0.1078, step time: 0.1052\n",
      "18/223, train_loss: 0.1166, step time: 0.1002\n",
      "19/223, train_loss: 0.1100, step time: 0.1140\n",
      "20/223, train_loss: 0.1122, step time: 0.1028\n",
      "21/223, train_loss: 0.1060, step time: 0.1240\n",
      "22/223, train_loss: 0.1049, step time: 0.0997\n",
      "23/223, train_loss: 0.1097, step time: 0.1069\n",
      "24/223, train_loss: 0.1130, step time: 0.1001\n",
      "25/223, train_loss: 0.1006, step time: 0.1324\n",
      "26/223, train_loss: 0.1130, step time: 0.1007\n",
      "27/223, train_loss: 0.1142, step time: 0.1174\n",
      "28/223, train_loss: 0.1179, step time: 0.1067\n",
      "29/223, train_loss: 0.1103, step time: 0.1094\n",
      "30/223, train_loss: 0.0936, step time: 0.1167\n",
      "31/223, train_loss: 0.1031, step time: 0.1164\n",
      "32/223, train_loss: 0.1070, step time: 0.1110\n",
      "33/223, train_loss: 0.1152, step time: 0.1167\n",
      "34/223, train_loss: 0.1013, step time: 0.1166\n",
      "35/223, train_loss: 0.1152, step time: 0.1110\n",
      "36/223, train_loss: 0.1137, step time: 0.1260\n",
      "37/223, train_loss: 0.1182, step time: 0.1114\n",
      "38/223, train_loss: 0.1037, step time: 0.1011\n",
      "39/223, train_loss: 0.1090, step time: 0.1061\n",
      "40/223, train_loss: 0.1067, step time: 0.1299\n",
      "41/223, train_loss: 0.1246, step time: 0.1142\n",
      "42/223, train_loss: 0.1015, step time: 0.1080\n",
      "43/223, train_loss: 0.1028, step time: 0.1094\n",
      "44/223, train_loss: 0.1143, step time: 0.1020\n",
      "45/223, train_loss: 0.1076, step time: 0.1177\n",
      "46/223, train_loss: 0.1107, step time: 0.1115\n",
      "47/223, train_loss: 0.1086, step time: 0.1000\n",
      "48/223, train_loss: 0.1071, step time: 0.1002\n",
      "49/223, train_loss: 0.1128, step time: 0.1191\n",
      "50/223, train_loss: 0.1042, step time: 0.1053\n",
      "51/223, train_loss: 0.1076, step time: 0.1621\n",
      "52/223, train_loss: 0.1194, step time: 0.1006\n",
      "53/223, train_loss: 0.1054, step time: 0.1193\n",
      "54/223, train_loss: 0.1104, step time: 0.1107\n",
      "55/223, train_loss: 0.1014, step time: 0.1124\n",
      "56/223, train_loss: 0.0991, step time: 0.1007\n",
      "57/223, train_loss: 0.1189, step time: 0.1268\n",
      "58/223, train_loss: 0.1091, step time: 0.1026\n",
      "59/223, train_loss: 0.1093, step time: 0.1136\n",
      "60/223, train_loss: 0.1223, step time: 0.1097\n",
      "61/223, train_loss: 0.1205, step time: 0.1010\n",
      "62/223, train_loss: 0.1160, step time: 0.1161\n",
      "63/223, train_loss: 0.0919, step time: 0.1071\n",
      "64/223, train_loss: 0.1178, step time: 0.1115\n",
      "65/223, train_loss: 0.1186, step time: 0.1078\n",
      "66/223, train_loss: 0.1142, step time: 0.1528\n",
      "67/223, train_loss: 0.1044, step time: 0.1078\n",
      "68/223, train_loss: 0.1056, step time: 0.0990\n",
      "69/223, train_loss: 0.1033, step time: 0.1006\n",
      "70/223, train_loss: 0.1100, step time: 0.1070\n",
      "71/223, train_loss: 0.3116, step time: 0.1238\n",
      "72/223, train_loss: 0.1132, step time: 0.1009\n",
      "73/223, train_loss: 0.1149, step time: 0.1135\n",
      "74/223, train_loss: 0.1103, step time: 0.1145\n",
      "75/223, train_loss: 0.1139, step time: 0.1009\n",
      "76/223, train_loss: 0.1045, step time: 0.1001\n",
      "77/223, train_loss: 0.1174, step time: 0.1189\n",
      "78/223, train_loss: 0.1066, step time: 0.1343\n",
      "79/223, train_loss: 0.1157, step time: 0.1009\n",
      "80/223, train_loss: 0.1079, step time: 0.1010\n",
      "81/223, train_loss: 0.1051, step time: 0.1132\n",
      "82/223, train_loss: 0.1100, step time: 0.1154\n",
      "83/223, train_loss: 0.1092, step time: 0.1058\n",
      "84/223, train_loss: 0.1075, step time: 0.1108\n",
      "85/223, train_loss: 0.1137, step time: 0.1048\n",
      "86/223, train_loss: 0.1026, step time: 0.1000\n",
      "87/223, train_loss: 0.1136, step time: 0.1104\n",
      "88/223, train_loss: 0.1246, step time: 0.1066\n",
      "89/223, train_loss: 0.1156, step time: 0.1120\n",
      "90/223, train_loss: 0.1101, step time: 0.1109\n",
      "91/223, train_loss: 0.1202, step time: 0.1120\n",
      "92/223, train_loss: 0.0953, step time: 0.1023\n",
      "93/223, train_loss: 0.1053, step time: 0.1013\n",
      "94/223, train_loss: 0.1067, step time: 0.1108\n",
      "95/223, train_loss: 0.1068, step time: 0.1036\n",
      "96/223, train_loss: 0.1186, step time: 0.1016\n",
      "97/223, train_loss: 0.1084, step time: 0.1036\n",
      "98/223, train_loss: 0.1403, step time: 0.1003\n",
      "99/223, train_loss: 0.1257, step time: 0.1004\n",
      "100/223, train_loss: 0.1062, step time: 0.1012\n",
      "101/223, train_loss: 0.1099, step time: 0.1003\n",
      "102/223, train_loss: 0.1155, step time: 0.1008\n",
      "103/223, train_loss: 0.1017, step time: 0.1014\n",
      "104/223, train_loss: 0.1086, step time: 0.0998\n",
      "105/223, train_loss: 0.1159, step time: 0.1015\n",
      "106/223, train_loss: 0.1065, step time: 0.0997\n",
      "107/223, train_loss: 0.0986, step time: 0.1011\n",
      "108/223, train_loss: 0.1056, step time: 0.1009\n",
      "109/223, train_loss: 0.1143, step time: 0.1005\n",
      "110/223, train_loss: 0.1209, step time: 0.1004\n",
      "111/223, train_loss: 0.1155, step time: 0.1006\n",
      "112/223, train_loss: 0.1006, step time: 0.1006\n",
      "113/223, train_loss: 0.1374, step time: 0.1003\n",
      "114/223, train_loss: 0.1198, step time: 0.0989\n",
      "115/223, train_loss: 0.1193, step time: 0.1007\n",
      "116/223, train_loss: 0.0975, step time: 0.1014\n",
      "117/223, train_loss: 0.1074, step time: 0.1230\n",
      "118/223, train_loss: 0.1020, step time: 0.1440\n",
      "119/223, train_loss: 0.1098, step time: 0.1074\n",
      "120/223, train_loss: 0.1075, step time: 0.1206\n",
      "121/223, train_loss: 0.1029, step time: 0.1077\n",
      "122/223, train_loss: 0.1182, step time: 0.1249\n",
      "123/223, train_loss: 0.1036, step time: 0.1645\n",
      "124/223, train_loss: 0.1299, step time: 0.1313\n",
      "125/223, train_loss: 0.1131, step time: 0.1187\n",
      "126/223, train_loss: 0.1060, step time: 0.1126\n",
      "127/223, train_loss: 0.1157, step time: 0.1058\n",
      "128/223, train_loss: 0.1150, step time: 0.1004\n",
      "129/223, train_loss: 0.1087, step time: 0.1193\n",
      "130/223, train_loss: 0.1254, step time: 0.1081\n",
      "131/223, train_loss: 0.1126, step time: 0.1250\n",
      "132/223, train_loss: 0.1154, step time: 0.1039\n",
      "133/223, train_loss: 0.1111, step time: 0.1088\n",
      "134/223, train_loss: 0.1056, step time: 0.1025\n",
      "135/223, train_loss: 0.0972, step time: 0.1046\n",
      "136/223, train_loss: 0.1172, step time: 0.1104\n",
      "137/223, train_loss: 0.1179, step time: 0.1313\n",
      "138/223, train_loss: 0.0990, step time: 0.1123\n",
      "139/223, train_loss: 0.1126, step time: 0.1372\n",
      "140/223, train_loss: 0.0982, step time: 0.1183\n",
      "141/223, train_loss: 0.1084, step time: 0.1122\n",
      "142/223, train_loss: 0.1071, step time: 0.1157\n",
      "143/223, train_loss: 0.1005, step time: 0.1051\n",
      "144/223, train_loss: 0.1135, step time: 0.1202\n",
      "145/223, train_loss: 0.1005, step time: 0.1013\n",
      "146/223, train_loss: 0.1351, step time: 0.1032\n",
      "147/223, train_loss: 0.1127, step time: 0.1035\n",
      "148/223, train_loss: 0.1046, step time: 0.1009\n",
      "149/223, train_loss: 0.1115, step time: 0.1006\n",
      "150/223, train_loss: 0.1028, step time: 0.1056\n",
      "151/223, train_loss: 0.1137, step time: 0.1004\n",
      "152/223, train_loss: 0.1105, step time: 0.1010\n",
      "153/223, train_loss: 0.1214, step time: 0.1011\n",
      "154/223, train_loss: 0.1059, step time: 0.1100\n",
      "155/223, train_loss: 0.1178, step time: 0.1170\n",
      "156/223, train_loss: 0.1121, step time: 0.1066\n",
      "157/223, train_loss: 0.1049, step time: 0.1122\n",
      "158/223, train_loss: 0.1004, step time: 0.1110\n",
      "159/223, train_loss: 0.1128, step time: 0.1009\n",
      "160/223, train_loss: 0.1320, step time: 0.1147\n",
      "161/223, train_loss: 0.1244, step time: 0.1476\n",
      "162/223, train_loss: 0.1119, step time: 0.1002\n",
      "163/223, train_loss: 0.1153, step time: 0.0997\n",
      "164/223, train_loss: 0.1116, step time: 0.0998\n",
      "165/223, train_loss: 0.1178, step time: 0.1007\n",
      "166/223, train_loss: 0.1274, step time: 0.1005\n",
      "167/223, train_loss: 0.1115, step time: 0.1000\n",
      "168/223, train_loss: 0.1046, step time: 0.1005\n",
      "169/223, train_loss: 0.1021, step time: 0.1013\n",
      "170/223, train_loss: 0.1076, step time: 0.1184\n",
      "171/223, train_loss: 0.1075, step time: 0.1172\n",
      "172/223, train_loss: 0.1069, step time: 0.1163\n",
      "173/223, train_loss: 0.1007, step time: 0.1172\n",
      "174/223, train_loss: 0.1068, step time: 0.1161\n",
      "175/223, train_loss: 0.1100, step time: 0.1140\n",
      "176/223, train_loss: 0.1029, step time: 0.1058\n",
      "177/223, train_loss: 0.1061, step time: 0.1034\n",
      "178/223, train_loss: 0.1181, step time: 0.1560\n",
      "179/223, train_loss: 0.1117, step time: 0.1347\n",
      "180/223, train_loss: 0.1248, step time: 0.1006\n",
      "181/223, train_loss: 0.1127, step time: 0.1022\n",
      "182/223, train_loss: 0.1242, step time: 0.1147\n",
      "183/223, train_loss: 0.1100, step time: 0.1086\n",
      "184/223, train_loss: 0.1156, step time: 0.1142\n",
      "185/223, train_loss: 0.1105, step time: 0.1068\n",
      "186/223, train_loss: 0.1037, step time: 0.1183\n",
      "187/223, train_loss: 0.0981, step time: 0.1147\n",
      "188/223, train_loss: 0.1091, step time: 0.1046\n",
      "189/223, train_loss: 0.1144, step time: 0.1130\n",
      "190/223, train_loss: 0.1088, step time: 0.1084\n",
      "191/223, train_loss: 0.1129, step time: 0.1157\n",
      "192/223, train_loss: 0.1081, step time: 0.1087\n",
      "193/223, train_loss: 0.1118, step time: 0.1151\n",
      "194/223, train_loss: 0.1124, step time: 0.1165\n",
      "195/223, train_loss: 0.1204, step time: 0.1068\n",
      "196/223, train_loss: 0.1144, step time: 0.1204\n",
      "197/223, train_loss: 0.1089, step time: 0.1330\n",
      "198/223, train_loss: 0.1033, step time: 0.1126\n",
      "199/223, train_loss: 0.1043, step time: 0.1216\n",
      "200/223, train_loss: 0.1120, step time: 0.1081\n",
      "201/223, train_loss: 0.0978, step time: 0.1223\n",
      "202/223, train_loss: 0.1079, step time: 0.1100\n",
      "203/223, train_loss: 0.1062, step time: 0.1131\n",
      "204/223, train_loss: 0.1099, step time: 0.1261\n",
      "205/223, train_loss: 0.1064, step time: 0.1148\n",
      "206/223, train_loss: 0.1141, step time: 0.1131\n",
      "207/223, train_loss: 0.1113, step time: 0.1033\n",
      "208/223, train_loss: 0.1222, step time: 0.1099\n",
      "209/223, train_loss: 0.1081, step time: 0.1008\n",
      "210/223, train_loss: 0.1080, step time: 0.1015\n",
      "211/223, train_loss: 0.1247, step time: 0.1029\n",
      "212/223, train_loss: 0.1080, step time: 0.1084\n",
      "213/223, train_loss: 0.1130, step time: 0.1058\n",
      "214/223, train_loss: 0.1081, step time: 0.1194\n",
      "215/223, train_loss: 0.1243, step time: 0.1069\n",
      "216/223, train_loss: 0.1138, step time: 0.1004\n",
      "217/223, train_loss: 0.1063, step time: 0.0995\n",
      "218/223, train_loss: 0.1077, step time: 0.0996\n",
      "219/223, train_loss: 0.1193, step time: 0.1006\n",
      "220/223, train_loss: 0.1066, step time: 0.0996\n",
      "221/223, train_loss: 0.1066, step time: 0.1021\n",
      "222/223, train_loss: 0.1000, step time: 0.1001\n",
      "223/223, train_loss: 0.1224, step time: 0.0998\n",
      "epoch 116 average loss: 0.1118\n",
      "time consuming of epoch 116 is: 89.8351\n",
      "----------\n",
      "epoch 117/300\n",
      "1/223, train_loss: 0.1031, step time: 0.1160\n",
      "2/223, train_loss: 0.1092, step time: 0.1310\n",
      "3/223, train_loss: 0.1155, step time: 0.1089\n",
      "4/223, train_loss: 0.1153, step time: 0.1257\n",
      "5/223, train_loss: 0.1077, step time: 0.1113\n",
      "6/223, train_loss: 0.1178, step time: 0.1000\n",
      "7/223, train_loss: 0.1042, step time: 0.1024\n",
      "8/223, train_loss: 0.1212, step time: 0.1196\n",
      "9/223, train_loss: 0.1037, step time: 0.1077\n",
      "10/223, train_loss: 0.1049, step time: 0.1085\n",
      "11/223, train_loss: 0.1166, step time: 0.1193\n",
      "12/223, train_loss: 0.0935, step time: 0.1085\n",
      "13/223, train_loss: 0.1078, step time: 0.0997\n",
      "14/223, train_loss: 0.1079, step time: 0.1097\n",
      "15/223, train_loss: 0.1078, step time: 0.1143\n",
      "16/223, train_loss: 0.1045, step time: 0.1233\n",
      "17/223, train_loss: 0.1008, step time: 0.1005\n",
      "18/223, train_loss: 0.1123, step time: 0.1101\n",
      "19/223, train_loss: 0.1005, step time: 0.1057\n",
      "20/223, train_loss: 0.1105, step time: 0.1141\n",
      "21/223, train_loss: 0.1080, step time: 0.1057\n",
      "22/223, train_loss: 0.1075, step time: 0.1215\n",
      "23/223, train_loss: 0.1214, step time: 0.1096\n",
      "24/223, train_loss: 0.1062, step time: 0.1003\n",
      "25/223, train_loss: 0.1250, step time: 0.1215\n",
      "26/223, train_loss: 0.1099, step time: 0.1244\n",
      "27/223, train_loss: 0.1186, step time: 0.1174\n",
      "28/223, train_loss: 0.1160, step time: 0.1058\n",
      "29/223, train_loss: 0.1104, step time: 0.1107\n",
      "30/223, train_loss: 0.0977, step time: 0.1194\n",
      "31/223, train_loss: 0.1281, step time: 0.1328\n",
      "32/223, train_loss: 0.1154, step time: 0.1074\n",
      "33/223, train_loss: 0.1110, step time: 0.1009\n",
      "34/223, train_loss: 0.1235, step time: 0.1020\n",
      "35/223, train_loss: 0.1163, step time: 0.1000\n",
      "36/223, train_loss: 0.1022, step time: 0.1005\n",
      "37/223, train_loss: 0.1023, step time: 0.1287\n",
      "38/223, train_loss: 0.1038, step time: 0.1084\n",
      "39/223, train_loss: 0.1196, step time: 0.1060\n",
      "40/223, train_loss: 0.1185, step time: 0.1080\n",
      "41/223, train_loss: 0.1030, step time: 0.1050\n",
      "42/223, train_loss: 0.1038, step time: 0.1190\n",
      "43/223, train_loss: 0.1042, step time: 0.1196\n",
      "44/223, train_loss: 0.1060, step time: 0.1013\n",
      "45/223, train_loss: 0.1047, step time: 0.1113\n",
      "46/223, train_loss: 0.1153, step time: 0.1144\n",
      "47/223, train_loss: 0.1211, step time: 0.1321\n",
      "48/223, train_loss: 0.1221, step time: 0.1004\n",
      "49/223, train_loss: 0.1013, step time: 0.0997\n",
      "50/223, train_loss: 0.1136, step time: 0.1012\n",
      "51/223, train_loss: 0.1008, step time: 0.1019\n",
      "52/223, train_loss: 0.1121, step time: 0.0995\n",
      "53/223, train_loss: 0.1146, step time: 0.1075\n",
      "54/223, train_loss: 0.1025, step time: 0.1126\n",
      "55/223, train_loss: 0.1157, step time: 0.1097\n",
      "56/223, train_loss: 0.1116, step time: 0.1072\n",
      "57/223, train_loss: 0.1088, step time: 0.1288\n",
      "58/223, train_loss: 0.1148, step time: 0.1181\n",
      "59/223, train_loss: 0.1141, step time: 0.1026\n",
      "60/223, train_loss: 0.1108, step time: 0.1077\n",
      "61/223, train_loss: 0.1068, step time: 0.0994\n",
      "62/223, train_loss: 0.1168, step time: 0.1198\n",
      "63/223, train_loss: 0.1036, step time: 0.1298\n",
      "64/223, train_loss: 0.1100, step time: 0.1136\n",
      "65/223, train_loss: 0.1149, step time: 0.1004\n",
      "66/223, train_loss: 0.1064, step time: 0.1011\n",
      "67/223, train_loss: 0.1252, step time: 0.1008\n",
      "68/223, train_loss: 0.3049, step time: 0.1178\n",
      "69/223, train_loss: 0.1143, step time: 0.1386\n",
      "70/223, train_loss: 0.1138, step time: 0.1061\n",
      "71/223, train_loss: 0.1038, step time: 0.1013\n",
      "72/223, train_loss: 0.1127, step time: 0.1074\n",
      "73/223, train_loss: 0.1050, step time: 0.1063\n",
      "74/223, train_loss: 0.0948, step time: 0.1023\n",
      "75/223, train_loss: 0.0983, step time: 0.1020\n",
      "76/223, train_loss: 0.1124, step time: 0.1014\n",
      "77/223, train_loss: 0.1148, step time: 0.1097\n",
      "78/223, train_loss: 0.1144, step time: 0.1083\n",
      "79/223, train_loss: 0.1277, step time: 0.1269\n",
      "80/223, train_loss: 0.1069, step time: 0.1118\n",
      "81/223, train_loss: 0.1066, step time: 0.1051\n",
      "82/223, train_loss: 0.1171, step time: 0.1270\n",
      "83/223, train_loss: 0.1129, step time: 0.1276\n",
      "84/223, train_loss: 0.1065, step time: 0.1153\n",
      "85/223, train_loss: 0.1145, step time: 0.1001\n",
      "86/223, train_loss: 0.1088, step time: 0.1041\n",
      "87/223, train_loss: 0.1192, step time: 0.1134\n",
      "88/223, train_loss: 0.1016, step time: 0.0997\n",
      "89/223, train_loss: 0.1068, step time: 0.1070\n",
      "90/223, train_loss: 0.1022, step time: 0.1019\n",
      "91/223, train_loss: 0.1263, step time: 0.1059\n",
      "92/223, train_loss: 0.1204, step time: 0.1002\n",
      "93/223, train_loss: 0.1076, step time: 0.1079\n",
      "94/223, train_loss: 0.1094, step time: 0.1456\n",
      "95/223, train_loss: 0.1021, step time: 0.1163\n",
      "96/223, train_loss: 0.1042, step time: 0.1205\n",
      "97/223, train_loss: 0.1156, step time: 0.1212\n",
      "98/223, train_loss: 0.1110, step time: 0.1100\n",
      "99/223, train_loss: 0.1197, step time: 0.1130\n",
      "100/223, train_loss: 0.1177, step time: 0.1182\n",
      "101/223, train_loss: 0.1063, step time: 0.1042\n",
      "102/223, train_loss: 0.1095, step time: 0.1159\n",
      "103/223, train_loss: 0.1077, step time: 0.1208\n",
      "104/223, train_loss: 0.1185, step time: 0.1008\n",
      "105/223, train_loss: 0.1104, step time: 0.1085\n",
      "106/223, train_loss: 0.1155, step time: 0.1101\n",
      "107/223, train_loss: 0.1170, step time: 0.1099\n",
      "108/223, train_loss: 0.1088, step time: 0.1033\n",
      "109/223, train_loss: 0.1213, step time: 0.1055\n",
      "110/223, train_loss: 0.1104, step time: 0.1005\n",
      "111/223, train_loss: 0.1212, step time: 0.1355\n",
      "112/223, train_loss: 0.1087, step time: 0.1023\n",
      "113/223, train_loss: 0.1048, step time: 0.1004\n",
      "114/223, train_loss: 0.1185, step time: 0.1045\n",
      "115/223, train_loss: 0.1041, step time: 0.1161\n",
      "116/223, train_loss: 0.1141, step time: 0.1039\n",
      "117/223, train_loss: 0.1070, step time: 0.1003\n",
      "118/223, train_loss: 0.1188, step time: 0.1011\n",
      "119/223, train_loss: 0.1192, step time: 0.1282\n",
      "120/223, train_loss: 0.1116, step time: 0.1250\n",
      "121/223, train_loss: 0.1193, step time: 0.1002\n",
      "122/223, train_loss: 0.1106, step time: 0.1000\n",
      "123/223, train_loss: 0.1019, step time: 0.1273\n",
      "124/223, train_loss: 0.1191, step time: 0.1121\n",
      "125/223, train_loss: 0.1110, step time: 0.1039\n",
      "126/223, train_loss: 0.1080, step time: 0.1004\n",
      "127/223, train_loss: 0.1132, step time: 0.1307\n",
      "128/223, train_loss: 0.0975, step time: 0.1006\n",
      "129/223, train_loss: 0.0960, step time: 0.1171\n",
      "130/223, train_loss: 0.1270, step time: 0.1004\n",
      "131/223, train_loss: 0.1112, step time: 0.1008\n",
      "132/223, train_loss: 0.1111, step time: 0.1212\n",
      "133/223, train_loss: 0.1018, step time: 0.1009\n",
      "134/223, train_loss: 0.1033, step time: 0.0998\n",
      "135/223, train_loss: 0.1067, step time: 0.1014\n",
      "136/223, train_loss: 0.1148, step time: 0.1004\n",
      "137/223, train_loss: 0.1099, step time: 0.1092\n",
      "138/223, train_loss: 0.1185, step time: 0.1037\n",
      "139/223, train_loss: 0.1147, step time: 0.1200\n",
      "140/223, train_loss: 0.1207, step time: 0.1194\n",
      "141/223, train_loss: 0.1125, step time: 0.0999\n",
      "142/223, train_loss: 0.1045, step time: 0.1006\n",
      "143/223, train_loss: 0.0996, step time: 0.1073\n",
      "144/223, train_loss: 0.1070, step time: 0.1003\n",
      "145/223, train_loss: 0.1149, step time: 0.1009\n",
      "146/223, train_loss: 0.1083, step time: 0.1086\n",
      "147/223, train_loss: 0.1159, step time: 0.1079\n",
      "148/223, train_loss: 0.1169, step time: 0.1001\n",
      "149/223, train_loss: 0.1069, step time: 0.1035\n",
      "150/223, train_loss: 0.1186, step time: 0.1005\n",
      "151/223, train_loss: 0.1017, step time: 0.1134\n",
      "152/223, train_loss: 0.1116, step time: 0.1013\n",
      "153/223, train_loss: 0.1145, step time: 0.1222\n",
      "154/223, train_loss: 0.1105, step time: 0.1006\n",
      "155/223, train_loss: 0.1041, step time: 0.1012\n",
      "156/223, train_loss: 0.1077, step time: 0.1006\n",
      "157/223, train_loss: 0.1066, step time: 0.1145\n",
      "158/223, train_loss: 0.1143, step time: 0.1147\n",
      "159/223, train_loss: 0.1211, step time: 0.1001\n",
      "160/223, train_loss: 0.1121, step time: 0.1010\n",
      "161/223, train_loss: 0.1089, step time: 0.1165\n",
      "162/223, train_loss: 0.1164, step time: 0.1086\n",
      "163/223, train_loss: 0.1177, step time: 0.1203\n",
      "164/223, train_loss: 0.1175, step time: 0.1006\n",
      "165/223, train_loss: 0.1009, step time: 0.1208\n",
      "166/223, train_loss: 0.1187, step time: 0.1009\n",
      "167/223, train_loss: 0.1084, step time: 0.1008\n",
      "168/223, train_loss: 0.1128, step time: 0.1009\n",
      "169/223, train_loss: 0.1076, step time: 0.1312\n",
      "170/223, train_loss: 0.1095, step time: 0.1423\n",
      "171/223, train_loss: 0.1182, step time: 0.1162\n",
      "172/223, train_loss: 0.1141, step time: 0.1125\n",
      "173/223, train_loss: 0.1128, step time: 0.1031\n",
      "174/223, train_loss: 0.1151, step time: 0.1127\n",
      "175/223, train_loss: 0.1127, step time: 0.1207\n",
      "176/223, train_loss: 0.1263, step time: 0.1011\n",
      "177/223, train_loss: 0.1111, step time: 0.1017\n",
      "178/223, train_loss: 0.1053, step time: 0.0998\n",
      "179/223, train_loss: 0.1196, step time: 0.1001\n",
      "180/223, train_loss: 0.1228, step time: 0.1062\n",
      "181/223, train_loss: 0.1082, step time: 0.1133\n",
      "182/223, train_loss: 0.1131, step time: 0.1063\n",
      "183/223, train_loss: 0.1161, step time: 0.0997\n",
      "184/223, train_loss: 0.1053, step time: 0.1094\n",
      "185/223, train_loss: 0.1013, step time: 0.1089\n",
      "186/223, train_loss: 0.1000, step time: 0.1000\n",
      "187/223, train_loss: 0.1219, step time: 0.1077\n",
      "188/223, train_loss: 0.1078, step time: 0.1267\n",
      "189/223, train_loss: 0.1077, step time: 0.1204\n",
      "190/223, train_loss: 0.1164, step time: 0.1024\n",
      "191/223, train_loss: 0.1097, step time: 0.1351\n",
      "192/223, train_loss: 0.1010, step time: 0.1093\n",
      "193/223, train_loss: 0.0980, step time: 0.1208\n",
      "194/223, train_loss: 0.1117, step time: 0.1064\n",
      "195/223, train_loss: 0.1043, step time: 0.1043\n",
      "196/223, train_loss: 0.1024, step time: 0.1099\n",
      "197/223, train_loss: 0.1280, step time: 0.1275\n",
      "198/223, train_loss: 0.1026, step time: 0.0998\n",
      "199/223, train_loss: 0.1148, step time: 0.1069\n",
      "200/223, train_loss: 0.1123, step time: 0.1169\n",
      "201/223, train_loss: 0.1067, step time: 0.1052\n",
      "202/223, train_loss: 0.1169, step time: 0.1142\n",
      "203/223, train_loss: 0.1025, step time: 0.1159\n",
      "204/223, train_loss: 0.1142, step time: 0.1061\n",
      "205/223, train_loss: 0.1180, step time: 0.1178\n",
      "206/223, train_loss: 0.1097, step time: 0.1150\n",
      "207/223, train_loss: 0.1086, step time: 0.1198\n",
      "208/223, train_loss: 0.1084, step time: 0.1052\n",
      "209/223, train_loss: 0.1181, step time: 0.1153\n",
      "210/223, train_loss: 0.1132, step time: 0.1047\n",
      "211/223, train_loss: 0.1096, step time: 0.1170\n",
      "212/223, train_loss: 0.1129, step time: 0.0993\n",
      "213/223, train_loss: 0.1012, step time: 0.1138\n",
      "214/223, train_loss: 0.1129, step time: 0.1038\n",
      "215/223, train_loss: 0.1061, step time: 0.1048\n",
      "216/223, train_loss: 0.1079, step time: 0.0985\n",
      "217/223, train_loss: 0.1003, step time: 0.0992\n",
      "218/223, train_loss: 0.1013, step time: 0.1001\n",
      "219/223, train_loss: 0.1085, step time: 0.0988\n",
      "220/223, train_loss: 0.1204, step time: 0.0992\n",
      "221/223, train_loss: 0.1095, step time: 0.0998\n",
      "222/223, train_loss: 0.1205, step time: 0.0998\n",
      "223/223, train_loss: 0.1032, step time: 0.0997\n",
      "epoch 117 average loss: 0.1118\n",
      "time consuming of epoch 117 is: 85.6805\n",
      "----------\n",
      "epoch 118/300\n",
      "1/223, train_loss: 0.1083, step time: 0.1038\n",
      "2/223, train_loss: 0.1232, step time: 0.1161\n",
      "3/223, train_loss: 0.1222, step time: 0.1286\n",
      "4/223, train_loss: 0.1109, step time: 0.1060\n",
      "5/223, train_loss: 0.1082, step time: 0.1150\n",
      "6/223, train_loss: 0.1083, step time: 0.1214\n",
      "7/223, train_loss: 0.1188, step time: 0.1306\n",
      "8/223, train_loss: 0.1167, step time: 0.1237\n",
      "9/223, train_loss: 0.1037, step time: 0.0994\n",
      "10/223, train_loss: 0.1115, step time: 0.0989\n",
      "11/223, train_loss: 0.1057, step time: 0.1080\n",
      "12/223, train_loss: 0.1021, step time: 0.1105\n",
      "13/223, train_loss: 0.1245, step time: 0.1174\n",
      "14/223, train_loss: 0.1065, step time: 0.1018\n",
      "15/223, train_loss: 0.1158, step time: 0.1041\n",
      "16/223, train_loss: 0.1120, step time: 0.1135\n",
      "17/223, train_loss: 0.1072, step time: 0.1296\n",
      "18/223, train_loss: 0.0971, step time: 0.1199\n",
      "19/223, train_loss: 0.1192, step time: 0.1386\n",
      "20/223, train_loss: 0.1097, step time: 0.1134\n",
      "21/223, train_loss: 0.1151, step time: 0.1078\n",
      "22/223, train_loss: 0.1092, step time: 0.1113\n",
      "23/223, train_loss: 0.1087, step time: 0.1002\n",
      "24/223, train_loss: 0.1089, step time: 0.1762\n",
      "25/223, train_loss: 0.1089, step time: 0.1044\n",
      "26/223, train_loss: 0.1217, step time: 0.1270\n",
      "27/223, train_loss: 0.1149, step time: 0.1095\n",
      "28/223, train_loss: 0.1018, step time: 0.1002\n",
      "29/223, train_loss: 0.1109, step time: 0.1286\n",
      "30/223, train_loss: 0.1129, step time: 0.1006\n",
      "31/223, train_loss: 0.1043, step time: 0.1003\n",
      "32/223, train_loss: 0.0991, step time: 0.1011\n",
      "33/223, train_loss: 0.1147, step time: 0.1220\n",
      "34/223, train_loss: 0.1096, step time: 0.1061\n",
      "35/223, train_loss: 0.1063, step time: 0.1007\n",
      "36/223, train_loss: 0.0954, step time: 0.1006\n",
      "37/223, train_loss: 0.1263, step time: 0.1632\n",
      "38/223, train_loss: 0.0983, step time: 0.1570\n",
      "39/223, train_loss: 0.1149, step time: 0.0997\n",
      "40/223, train_loss: 0.1153, step time: 0.1002\n",
      "41/223, train_loss: 0.1015, step time: 0.1131\n",
      "42/223, train_loss: 0.1148, step time: 0.1305\n",
      "43/223, train_loss: 0.1153, step time: 0.1093\n",
      "44/223, train_loss: 0.1008, step time: 0.1024\n",
      "45/223, train_loss: 0.1147, step time: 0.1123\n",
      "46/223, train_loss: 0.1129, step time: 0.1077\n",
      "47/223, train_loss: 0.1077, step time: 0.1221\n",
      "48/223, train_loss: 0.1083, step time: 0.1128\n",
      "49/223, train_loss: 0.1032, step time: 0.1007\n",
      "50/223, train_loss: 0.1167, step time: 0.1015\n",
      "51/223, train_loss: 0.1108, step time: 0.1004\n",
      "52/223, train_loss: 0.0938, step time: 0.1010\n",
      "53/223, train_loss: 0.1067, step time: 0.1046\n",
      "54/223, train_loss: 0.0982, step time: 0.1006\n",
      "55/223, train_loss: 0.1060, step time: 0.1011\n",
      "56/223, train_loss: 0.1132, step time: 0.1036\n",
      "57/223, train_loss: 0.1104, step time: 0.1050\n",
      "58/223, train_loss: 0.1110, step time: 0.1362\n",
      "59/223, train_loss: 0.1084, step time: 0.1240\n",
      "60/223, train_loss: 0.1070, step time: 0.1003\n",
      "61/223, train_loss: 0.1139, step time: 0.1277\n",
      "62/223, train_loss: 0.1110, step time: 0.1092\n",
      "63/223, train_loss: 0.1183, step time: 0.1057\n",
      "64/223, train_loss: 0.1050, step time: 0.1002\n",
      "65/223, train_loss: 0.1120, step time: 0.1010\n",
      "66/223, train_loss: 0.1149, step time: 0.1253\n",
      "67/223, train_loss: 0.1067, step time: 0.1008\n",
      "68/223, train_loss: 0.1073, step time: 0.1081\n",
      "69/223, train_loss: 0.1049, step time: 0.1008\n",
      "70/223, train_loss: 0.1028, step time: 0.1195\n",
      "71/223, train_loss: 0.1014, step time: 0.1001\n",
      "72/223, train_loss: 0.1090, step time: 0.1085\n",
      "73/223, train_loss: 0.1136, step time: 0.1036\n",
      "74/223, train_loss: 0.1093, step time: 0.1128\n",
      "75/223, train_loss: 0.1038, step time: 0.1092\n",
      "76/223, train_loss: 0.1119, step time: 0.1146\n",
      "77/223, train_loss: 0.1056, step time: 0.1146\n",
      "78/223, train_loss: 0.1040, step time: 0.1108\n",
      "79/223, train_loss: 0.1327, step time: 0.1315\n",
      "80/223, train_loss: 0.1040, step time: 0.1005\n",
      "81/223, train_loss: 0.1155, step time: 0.0991\n",
      "82/223, train_loss: 0.1022, step time: 0.0996\n",
      "83/223, train_loss: 0.1114, step time: 0.0991\n",
      "84/223, train_loss: 0.0963, step time: 0.1012\n",
      "85/223, train_loss: 0.1216, step time: 0.1055\n",
      "86/223, train_loss: 0.1100, step time: 0.1026\n",
      "87/223, train_loss: 0.1185, step time: 0.1058\n",
      "88/223, train_loss: 0.1087, step time: 0.1244\n",
      "89/223, train_loss: 0.1172, step time: 0.1292\n",
      "90/223, train_loss: 0.1028, step time: 0.1222\n",
      "91/223, train_loss: 0.1267, step time: 0.1077\n",
      "92/223, train_loss: 0.1050, step time: 0.1226\n",
      "93/223, train_loss: 0.1049, step time: 0.1116\n",
      "94/223, train_loss: 0.1212, step time: 0.1184\n",
      "95/223, train_loss: 0.1067, step time: 0.1099\n",
      "96/223, train_loss: 0.1207, step time: 0.1141\n",
      "97/223, train_loss: 0.0970, step time: 0.1077\n",
      "98/223, train_loss: 0.1085, step time: 0.1139\n",
      "99/223, train_loss: 0.1282, step time: 0.1211\n",
      "100/223, train_loss: 0.1128, step time: 0.1006\n",
      "101/223, train_loss: 0.1190, step time: 0.1155\n",
      "102/223, train_loss: 0.1019, step time: 0.1099\n",
      "103/223, train_loss: 0.1106, step time: 0.1108\n",
      "104/223, train_loss: 0.1145, step time: 0.1005\n",
      "105/223, train_loss: 0.1085, step time: 0.1146\n",
      "106/223, train_loss: 0.1015, step time: 0.1073\n",
      "107/223, train_loss: 0.1138, step time: 0.1056\n",
      "108/223, train_loss: 0.1046, step time: 0.1128\n",
      "109/223, train_loss: 0.1036, step time: 0.1111\n",
      "110/223, train_loss: 0.1138, step time: 0.1007\n",
      "111/223, train_loss: 0.1056, step time: 0.1161\n",
      "112/223, train_loss: 0.1143, step time: 0.1292\n",
      "113/223, train_loss: 0.1033, step time: 0.1156\n",
      "114/223, train_loss: 0.1229, step time: 0.1238\n",
      "115/223, train_loss: 0.1067, step time: 0.1008\n",
      "116/223, train_loss: 0.1181, step time: 0.1011\n",
      "117/223, train_loss: 0.1112, step time: 0.1180\n",
      "118/223, train_loss: 0.1030, step time: 0.1081\n",
      "119/223, train_loss: 0.1079, step time: 0.1436\n",
      "120/223, train_loss: 0.1024, step time: 0.1002\n",
      "121/223, train_loss: 0.1169, step time: 0.1010\n",
      "122/223, train_loss: 0.1137, step time: 0.1017\n",
      "123/223, train_loss: 0.1136, step time: 0.1010\n",
      "124/223, train_loss: 0.1126, step time: 0.1157\n",
      "125/223, train_loss: 0.0998, step time: 0.1183\n",
      "126/223, train_loss: 0.1048, step time: 0.1216\n",
      "127/223, train_loss: 0.1151, step time: 0.1135\n",
      "128/223, train_loss: 0.0992, step time: 0.0996\n",
      "129/223, train_loss: 0.1083, step time: 0.1149\n",
      "130/223, train_loss: 0.0987, step time: 0.1123\n",
      "131/223, train_loss: 0.1093, step time: 0.1066\n",
      "132/223, train_loss: 0.1187, step time: 0.1006\n",
      "133/223, train_loss: 0.1143, step time: 0.1148\n",
      "134/223, train_loss: 0.1059, step time: 0.1029\n",
      "135/223, train_loss: 0.1052, step time: 0.1147\n",
      "136/223, train_loss: 0.1065, step time: 0.1006\n",
      "137/223, train_loss: 0.1148, step time: 0.1177\n",
      "138/223, train_loss: 0.1068, step time: 0.1075\n",
      "139/223, train_loss: 0.1115, step time: 0.1181\n",
      "140/223, train_loss: 0.1035, step time: 0.1065\n",
      "141/223, train_loss: 0.3086, step time: 0.1124\n",
      "142/223, train_loss: 0.1064, step time: 0.1135\n",
      "143/223, train_loss: 0.1070, step time: 0.1078\n",
      "144/223, train_loss: 0.1094, step time: 0.1005\n",
      "145/223, train_loss: 0.0997, step time: 0.1233\n",
      "146/223, train_loss: 0.0988, step time: 0.1167\n",
      "147/223, train_loss: 0.1223, step time: 0.1047\n",
      "148/223, train_loss: 0.0963, step time: 0.1015\n",
      "149/223, train_loss: 0.1239, step time: 0.1010\n",
      "150/223, train_loss: 0.1130, step time: 0.1005\n",
      "151/223, train_loss: 0.1002, step time: 0.1008\n",
      "152/223, train_loss: 0.1132, step time: 0.1106\n",
      "153/223, train_loss: 0.1070, step time: 0.1154\n",
      "154/223, train_loss: 0.1066, step time: 0.1436\n",
      "155/223, train_loss: 0.1063, step time: 0.1572\n",
      "156/223, train_loss: 0.1265, step time: 0.1005\n",
      "157/223, train_loss: 0.1118, step time: 0.1115\n",
      "158/223, train_loss: 0.1071, step time: 0.1098\n",
      "159/223, train_loss: 0.1285, step time: 0.1524\n",
      "160/223, train_loss: 0.0985, step time: 0.1246\n",
      "161/223, train_loss: 0.1102, step time: 0.1137\n",
      "162/223, train_loss: 0.1070, step time: 0.1078\n",
      "163/223, train_loss: 0.1143, step time: 0.1361\n",
      "164/223, train_loss: 0.1048, step time: 0.1033\n",
      "165/223, train_loss: 0.1124, step time: 0.1117\n",
      "166/223, train_loss: 0.1283, step time: 0.1050\n",
      "167/223, train_loss: 0.1070, step time: 0.1102\n",
      "168/223, train_loss: 0.1095, step time: 0.1110\n",
      "169/223, train_loss: 0.1182, step time: 0.1196\n",
      "170/223, train_loss: 0.1132, step time: 0.1146\n",
      "171/223, train_loss: 0.1219, step time: 0.1090\n",
      "172/223, train_loss: 0.1082, step time: 0.1002\n",
      "173/223, train_loss: 0.1064, step time: 0.1024\n",
      "174/223, train_loss: 0.1046, step time: 0.1287\n",
      "175/223, train_loss: 0.1104, step time: 0.1252\n",
      "176/223, train_loss: 0.1076, step time: 0.1074\n",
      "177/223, train_loss: 0.1232, step time: 0.1012\n",
      "178/223, train_loss: 0.1058, step time: 0.0997\n",
      "179/223, train_loss: 0.1046, step time: 0.1009\n",
      "180/223, train_loss: 0.1033, step time: 0.1078\n",
      "181/223, train_loss: 0.1073, step time: 0.1001\n",
      "182/223, train_loss: 0.1115, step time: 0.1009\n",
      "183/223, train_loss: 0.1209, step time: 0.1024\n",
      "184/223, train_loss: 0.1130, step time: 0.1193\n",
      "185/223, train_loss: 0.1078, step time: 0.1067\n",
      "186/223, train_loss: 0.1147, step time: 0.0994\n",
      "187/223, train_loss: 0.1150, step time: 0.0993\n",
      "188/223, train_loss: 0.0985, step time: 0.1033\n",
      "189/223, train_loss: 0.1238, step time: 0.1171\n",
      "190/223, train_loss: 0.1108, step time: 0.1030\n",
      "191/223, train_loss: 0.1056, step time: 0.1086\n",
      "192/223, train_loss: 0.1117, step time: 0.1037\n",
      "193/223, train_loss: 0.1122, step time: 0.1096\n",
      "194/223, train_loss: 0.1178, step time: 0.1002\n",
      "195/223, train_loss: 0.1181, step time: 0.1297\n",
      "196/223, train_loss: 0.1135, step time: 0.1034\n",
      "197/223, train_loss: 0.1080, step time: 0.1131\n",
      "198/223, train_loss: 0.1025, step time: 0.1211\n",
      "199/223, train_loss: 0.1067, step time: 0.1097\n",
      "200/223, train_loss: 0.1048, step time: 0.1194\n",
      "201/223, train_loss: 0.1153, step time: 0.1082\n",
      "202/223, train_loss: 0.1243, step time: 0.1110\n",
      "203/223, train_loss: 0.1220, step time: 0.1274\n",
      "204/223, train_loss: 0.1093, step time: 0.0997\n",
      "205/223, train_loss: 0.1185, step time: 0.1042\n",
      "206/223, train_loss: 0.1115, step time: 0.1128\n",
      "207/223, train_loss: 0.1146, step time: 0.1256\n",
      "208/223, train_loss: 0.1123, step time: 0.1053\n",
      "209/223, train_loss: 0.1014, step time: 0.1003\n",
      "210/223, train_loss: 0.1051, step time: 0.1023\n",
      "211/223, train_loss: 0.1124, step time: 0.1049\n",
      "212/223, train_loss: 0.1042, step time: 0.1077\n",
      "213/223, train_loss: 0.0991, step time: 0.1238\n",
      "214/223, train_loss: 0.1143, step time: 0.1060\n",
      "215/223, train_loss: 0.1114, step time: 0.1146\n",
      "216/223, train_loss: 0.1127, step time: 0.1001\n",
      "217/223, train_loss: 0.1127, step time: 0.1010\n",
      "218/223, train_loss: 0.1069, step time: 0.1185\n",
      "219/223, train_loss: 0.1145, step time: 0.1026\n",
      "220/223, train_loss: 0.1041, step time: 0.1013\n",
      "221/223, train_loss: 0.1019, step time: 0.0989\n",
      "222/223, train_loss: 0.1087, step time: 0.0994\n",
      "223/223, train_loss: 0.1184, step time: 0.1006\n",
      "epoch 118 average loss: 0.1111\n",
      "time consuming of epoch 118 is: 88.9406\n",
      "----------\n",
      "epoch 119/300\n",
      "1/223, train_loss: 0.1105, step time: 0.1027\n",
      "2/223, train_loss: 0.1094, step time: 0.1017\n",
      "3/223, train_loss: 0.1216, step time: 0.0989\n",
      "4/223, train_loss: 0.1092, step time: 0.0987\n",
      "5/223, train_loss: 0.1141, step time: 0.1001\n",
      "6/223, train_loss: 0.1018, step time: 0.1114\n",
      "7/223, train_loss: 0.1033, step time: 0.1182\n",
      "8/223, train_loss: 0.1113, step time: 0.1008\n",
      "9/223, train_loss: 0.0998, step time: 0.1167\n",
      "10/223, train_loss: 0.0978, step time: 0.1165\n",
      "11/223, train_loss: 0.1096, step time: 0.1117\n",
      "12/223, train_loss: 0.1041, step time: 0.1106\n",
      "13/223, train_loss: 0.1188, step time: 0.1140\n",
      "14/223, train_loss: 0.1058, step time: 0.1082\n",
      "15/223, train_loss: 0.1101, step time: 0.1087\n",
      "16/223, train_loss: 0.1137, step time: 0.1115\n",
      "17/223, train_loss: 0.1075, step time: 0.1003\n",
      "18/223, train_loss: 0.1045, step time: 0.1248\n",
      "19/223, train_loss: 0.1144, step time: 0.1088\n",
      "20/223, train_loss: 0.1153, step time: 0.1286\n",
      "21/223, train_loss: 0.1305, step time: 0.1204\n",
      "22/223, train_loss: 0.1223, step time: 0.1119\n",
      "23/223, train_loss: 0.1196, step time: 0.1062\n",
      "24/223, train_loss: 0.1098, step time: 0.1123\n",
      "25/223, train_loss: 0.1068, step time: 0.1072\n",
      "26/223, train_loss: 0.1113, step time: 0.1004\n",
      "27/223, train_loss: 0.1094, step time: 0.1041\n",
      "28/223, train_loss: 0.1004, step time: 0.1227\n",
      "29/223, train_loss: 0.1039, step time: 0.1073\n",
      "30/223, train_loss: 0.1079, step time: 0.1062\n",
      "31/223, train_loss: 0.1233, step time: 0.1108\n",
      "32/223, train_loss: 0.1094, step time: 0.1004\n",
      "33/223, train_loss: 0.1214, step time: 0.1094\n",
      "34/223, train_loss: 0.1185, step time: 0.1210\n",
      "35/223, train_loss: 0.1093, step time: 0.1064\n",
      "36/223, train_loss: 0.1058, step time: 0.1066\n",
      "37/223, train_loss: 0.1069, step time: 0.1031\n",
      "38/223, train_loss: 0.1014, step time: 0.1053\n",
      "39/223, train_loss: 0.1053, step time: 0.1002\n",
      "40/223, train_loss: 0.1119, step time: 0.0998\n",
      "41/223, train_loss: 0.1172, step time: 0.1220\n",
      "42/223, train_loss: 0.1118, step time: 0.1170\n",
      "43/223, train_loss: 0.1008, step time: 0.0998\n",
      "44/223, train_loss: 0.1164, step time: 0.1060\n",
      "45/223, train_loss: 0.1024, step time: 0.1022\n",
      "46/223, train_loss: 0.1148, step time: 0.1042\n",
      "47/223, train_loss: 0.1142, step time: 0.1154\n",
      "48/223, train_loss: 0.1248, step time: 0.1123\n",
      "49/223, train_loss: 0.1090, step time: 0.1030\n",
      "50/223, train_loss: 0.1159, step time: 0.1081\n",
      "51/223, train_loss: 0.1070, step time: 0.1062\n",
      "52/223, train_loss: 0.1181, step time: 0.1161\n",
      "53/223, train_loss: 0.1068, step time: 0.1177\n",
      "54/223, train_loss: 0.1125, step time: 0.1031\n",
      "55/223, train_loss: 0.1173, step time: 0.1103\n",
      "56/223, train_loss: 0.1113, step time: 0.1013\n",
      "57/223, train_loss: 0.1128, step time: 0.1153\n",
      "58/223, train_loss: 0.1102, step time: 0.1151\n",
      "59/223, train_loss: 0.1131, step time: 0.1008\n",
      "60/223, train_loss: 0.1199, step time: 0.1294\n",
      "61/223, train_loss: 0.1267, step time: 0.1083\n",
      "62/223, train_loss: 0.1077, step time: 0.1058\n",
      "63/223, train_loss: 0.1051, step time: 0.1122\n",
      "64/223, train_loss: 0.0985, step time: 0.1100\n",
      "65/223, train_loss: 0.1076, step time: 0.1094\n",
      "66/223, train_loss: 0.1012, step time: 0.1048\n",
      "67/223, train_loss: 0.0984, step time: 0.0995\n",
      "68/223, train_loss: 0.1055, step time: 0.0999\n",
      "69/223, train_loss: 0.1146, step time: 0.1052\n",
      "70/223, train_loss: 0.1069, step time: 0.1098\n",
      "71/223, train_loss: 0.1180, step time: 0.0997\n",
      "72/223, train_loss: 0.1182, step time: 0.0990\n",
      "73/223, train_loss: 0.1124, step time: 0.1021\n",
      "74/223, train_loss: 0.1282, step time: 0.1006\n",
      "75/223, train_loss: 0.1126, step time: 0.1001\n",
      "76/223, train_loss: 0.1231, step time: 0.1004\n",
      "77/223, train_loss: 0.1057, step time: 0.1101\n",
      "78/223, train_loss: 0.1121, step time: 0.1182\n",
      "79/223, train_loss: 0.1008, step time: 0.1134\n",
      "80/223, train_loss: 0.1158, step time: 0.1002\n",
      "81/223, train_loss: 0.1135, step time: 0.1171\n",
      "82/223, train_loss: 0.1125, step time: 0.1130\n",
      "83/223, train_loss: 0.1057, step time: 0.1000\n",
      "84/223, train_loss: 0.1079, step time: 0.1002\n",
      "85/223, train_loss: 0.1117, step time: 0.1210\n",
      "86/223, train_loss: 0.1093, step time: 0.0997\n",
      "87/223, train_loss: 0.1022, step time: 0.1003\n",
      "88/223, train_loss: 0.1033, step time: 0.1035\n",
      "89/223, train_loss: 0.0973, step time: 0.1165\n",
      "90/223, train_loss: 0.1076, step time: 0.1211\n",
      "91/223, train_loss: 0.1077, step time: 0.1075\n",
      "92/223, train_loss: 0.1139, step time: 0.1010\n",
      "93/223, train_loss: 0.1142, step time: 0.1021\n",
      "94/223, train_loss: 0.1128, step time: 0.1043\n",
      "95/223, train_loss: 0.1187, step time: 0.1095\n",
      "96/223, train_loss: 0.1194, step time: 0.1083\n",
      "97/223, train_loss: 0.1201, step time: 0.1112\n",
      "98/223, train_loss: 0.1129, step time: 0.1013\n",
      "99/223, train_loss: 0.1051, step time: 0.1162\n",
      "100/223, train_loss: 0.1207, step time: 0.1575\n",
      "101/223, train_loss: 0.1170, step time: 0.1268\n",
      "102/223, train_loss: 0.1098, step time: 0.1106\n",
      "103/223, train_loss: 0.1179, step time: 0.1055\n",
      "104/223, train_loss: 0.1033, step time: 0.1381\n",
      "105/223, train_loss: 0.1118, step time: 0.1289\n",
      "106/223, train_loss: 0.1040, step time: 0.1199\n",
      "107/223, train_loss: 0.1133, step time: 0.1147\n",
      "108/223, train_loss: 0.1052, step time: 0.1212\n",
      "109/223, train_loss: 0.1069, step time: 0.1184\n",
      "110/223, train_loss: 0.1065, step time: 0.1154\n",
      "111/223, train_loss: 0.1098, step time: 0.1141\n",
      "112/223, train_loss: 0.1043, step time: 0.1153\n",
      "113/223, train_loss: 0.1125, step time: 0.1033\n",
      "114/223, train_loss: 0.1076, step time: 0.1162\n",
      "115/223, train_loss: 0.1196, step time: 0.1122\n",
      "116/223, train_loss: 0.1268, step time: 0.1014\n",
      "117/223, train_loss: 0.1123, step time: 0.1057\n",
      "118/223, train_loss: 0.1105, step time: 0.0999\n",
      "119/223, train_loss: 0.1228, step time: 0.1004\n",
      "120/223, train_loss: 0.1028, step time: 0.1095\n",
      "121/223, train_loss: 0.3133, step time: 0.1277\n",
      "122/223, train_loss: 0.1397, step time: 0.1159\n",
      "123/223, train_loss: 0.1412, step time: 0.1044\n",
      "124/223, train_loss: 0.1171, step time: 0.1163\n",
      "125/223, train_loss: 0.1248, step time: 0.1146\n",
      "126/223, train_loss: 0.1418, step time: 0.1095\n",
      "127/223, train_loss: 0.1430, step time: 0.1282\n",
      "128/223, train_loss: 0.1225, step time: 0.1022\n",
      "129/223, train_loss: 0.1180, step time: 0.1325\n",
      "130/223, train_loss: 0.1132, step time: 0.0992\n",
      "131/223, train_loss: 0.0976, step time: 0.0986\n",
      "132/223, train_loss: 0.1350, step time: 0.0999\n",
      "133/223, train_loss: 0.1156, step time: 0.1124\n",
      "134/223, train_loss: 0.1211, step time: 0.1159\n",
      "135/223, train_loss: 0.1242, step time: 0.1280\n",
      "136/223, train_loss: 0.1041, step time: 0.0993\n",
      "137/223, train_loss: 0.1208, step time: 0.0998\n",
      "138/223, train_loss: 0.1130, step time: 0.1069\n",
      "139/223, train_loss: 0.1377, step time: 0.1093\n",
      "140/223, train_loss: 0.1095, step time: 0.1202\n",
      "141/223, train_loss: 0.1054, step time: 0.1059\n",
      "142/223, train_loss: 0.1449, step time: 0.1061\n",
      "143/223, train_loss: 0.1050, step time: 0.1064\n",
      "144/223, train_loss: 0.1169, step time: 0.1186\n",
      "145/223, train_loss: 0.1126, step time: 0.0986\n",
      "146/223, train_loss: 0.1239, step time: 0.1397\n",
      "147/223, train_loss: 0.1184, step time: 0.1707\n",
      "148/223, train_loss: 0.1401, step time: 0.1007\n",
      "149/223, train_loss: 0.1114, step time: 0.1005\n",
      "150/223, train_loss: 0.1177, step time: 0.1002\n",
      "151/223, train_loss: 0.1134, step time: 0.1001\n",
      "152/223, train_loss: 0.1378, step time: 0.1083\n",
      "153/223, train_loss: 0.1374, step time: 0.1011\n",
      "154/223, train_loss: 0.1345, step time: 0.1176\n",
      "155/223, train_loss: 0.1089, step time: 0.1374\n",
      "156/223, train_loss: 0.1184, step time: 0.1006\n",
      "157/223, train_loss: 0.1291, step time: 0.1002\n",
      "158/223, train_loss: 0.1154, step time: 0.1072\n",
      "159/223, train_loss: 0.1295, step time: 0.1088\n",
      "160/223, train_loss: 0.1375, step time: 0.1084\n",
      "161/223, train_loss: 0.1410, step time: 0.1015\n",
      "162/223, train_loss: 0.1515, step time: 0.1300\n",
      "163/223, train_loss: 0.1178, step time: 0.1148\n",
      "164/223, train_loss: 0.1365, step time: 0.1009\n",
      "165/223, train_loss: 0.1273, step time: 0.1001\n",
      "166/223, train_loss: 0.1203, step time: 0.1214\n",
      "167/223, train_loss: 0.1393, step time: 0.1088\n",
      "168/223, train_loss: 0.1219, step time: 0.1014\n",
      "169/223, train_loss: 0.1278, step time: 0.1002\n",
      "170/223, train_loss: 0.1355, step time: 0.1085\n",
      "171/223, train_loss: 0.1613, step time: 0.1093\n",
      "172/223, train_loss: 0.1440, step time: 0.1007\n",
      "173/223, train_loss: 0.1127, step time: 0.0997\n",
      "174/223, train_loss: 0.1199, step time: 0.1060\n",
      "175/223, train_loss: 0.1137, step time: 0.1059\n",
      "176/223, train_loss: 0.1357, step time: 0.1016\n",
      "177/223, train_loss: 0.1332, step time: 0.1124\n",
      "178/223, train_loss: 0.1267, step time: 0.1182\n",
      "179/223, train_loss: 0.1129, step time: 0.1100\n",
      "180/223, train_loss: 0.1043, step time: 0.1007\n",
      "181/223, train_loss: 0.1152, step time: 0.1007\n",
      "182/223, train_loss: 0.1204, step time: 0.1102\n",
      "183/223, train_loss: 0.1453, step time: 0.1341\n",
      "184/223, train_loss: 0.1250, step time: 0.1068\n",
      "185/223, train_loss: 0.1289, step time: 0.1004\n",
      "186/223, train_loss: 0.1135, step time: 0.1260\n",
      "187/223, train_loss: 0.1164, step time: 0.1006\n",
      "188/223, train_loss: 0.1539, step time: 0.1004\n",
      "189/223, train_loss: 0.1112, step time: 0.1012\n",
      "190/223, train_loss: 0.1209, step time: 0.1008\n",
      "191/223, train_loss: 0.1156, step time: 0.1000\n",
      "192/223, train_loss: 0.1114, step time: 0.1000\n",
      "193/223, train_loss: 0.1235, step time: 0.1049\n",
      "194/223, train_loss: 0.1297, step time: 0.1434\n",
      "195/223, train_loss: 0.1160, step time: 0.1193\n",
      "196/223, train_loss: 0.1186, step time: 0.1094\n",
      "197/223, train_loss: 0.1150, step time: 0.1155\n",
      "198/223, train_loss: 0.1223, step time: 0.1632\n",
      "199/223, train_loss: 0.1065, step time: 0.1288\n",
      "200/223, train_loss: 0.1077, step time: 0.1105\n",
      "201/223, train_loss: 0.1197, step time: 0.1180\n",
      "202/223, train_loss: 0.1436, step time: 0.1193\n",
      "203/223, train_loss: 0.1152, step time: 0.1094\n",
      "204/223, train_loss: 0.1135, step time: 0.1115\n",
      "205/223, train_loss: 0.1264, step time: 0.1021\n",
      "206/223, train_loss: 0.1376, step time: 0.1184\n",
      "207/223, train_loss: 0.1190, step time: 0.1077\n",
      "208/223, train_loss: 0.1457, step time: 0.1007\n",
      "209/223, train_loss: 0.1177, step time: 0.0999\n",
      "210/223, train_loss: 0.1217, step time: 0.0991\n",
      "211/223, train_loss: 0.1049, step time: 0.1008\n",
      "212/223, train_loss: 0.1067, step time: 0.1025\n",
      "213/223, train_loss: 0.1077, step time: 0.1066\n",
      "214/223, train_loss: 0.1352, step time: 0.0995\n",
      "215/223, train_loss: 0.1311, step time: 0.1057\n",
      "216/223, train_loss: 0.1178, step time: 0.1257\n",
      "217/223, train_loss: 0.1200, step time: 0.1000\n",
      "218/223, train_loss: 0.1190, step time: 0.0997\n",
      "219/223, train_loss: 0.1110, step time: 0.0993\n",
      "220/223, train_loss: 0.1254, step time: 0.0997\n",
      "221/223, train_loss: 0.1170, step time: 0.1004\n",
      "222/223, train_loss: 0.1067, step time: 0.0987\n",
      "223/223, train_loss: 0.1058, step time: 0.1003\n",
      "epoch 119 average loss: 0.1175\n",
      "time consuming of epoch 119 is: 87.4831\n",
      "----------\n",
      "epoch 120/300\n",
      "1/223, train_loss: 0.1062, step time: 0.1003\n",
      "2/223, train_loss: 0.1224, step time: 0.1073\n",
      "3/223, train_loss: 0.1049, step time: 0.1092\n",
      "4/223, train_loss: 0.1101, step time: 0.1099\n",
      "5/223, train_loss: 0.1268, step time: 0.0998\n",
      "6/223, train_loss: 0.1141, step time: 0.1010\n",
      "7/223, train_loss: 0.1098, step time: 0.0997\n",
      "8/223, train_loss: 0.1156, step time: 0.1033\n",
      "9/223, train_loss: 0.1151, step time: 0.1116\n",
      "10/223, train_loss: 0.1163, step time: 0.0998\n",
      "11/223, train_loss: 0.1101, step time: 0.1046\n",
      "12/223, train_loss: 0.1088, step time: 0.1118\n",
      "13/223, train_loss: 0.1134, step time: 0.1145\n",
      "14/223, train_loss: 0.1102, step time: 0.1123\n",
      "15/223, train_loss: 0.1139, step time: 0.1208\n",
      "16/223, train_loss: 0.1145, step time: 0.1156\n",
      "17/223, train_loss: 0.1150, step time: 0.1093\n",
      "18/223, train_loss: 0.1105, step time: 0.1147\n",
      "19/223, train_loss: 0.1202, step time: 0.1279\n",
      "20/223, train_loss: 0.1285, step time: 0.1195\n",
      "21/223, train_loss: 0.1225, step time: 0.0991\n",
      "22/223, train_loss: 0.1090, step time: 0.1091\n",
      "23/223, train_loss: 0.1136, step time: 0.1280\n",
      "24/223, train_loss: 0.1318, step time: 0.1087\n",
      "25/223, train_loss: 0.1078, step time: 0.1109\n",
      "26/223, train_loss: 0.1038, step time: 0.1043\n",
      "27/223, train_loss: 0.3129, step time: 0.1159\n",
      "28/223, train_loss: 0.1098, step time: 0.1159\n",
      "29/223, train_loss: 0.1078, step time: 0.1060\n",
      "30/223, train_loss: 0.1169, step time: 0.1274\n",
      "31/223, train_loss: 0.1058, step time: 0.1240\n",
      "32/223, train_loss: 0.1160, step time: 0.0994\n",
      "33/223, train_loss: 0.1061, step time: 0.1040\n",
      "34/223, train_loss: 0.1098, step time: 0.1110\n",
      "35/223, train_loss: 0.1228, step time: 0.1133\n",
      "36/223, train_loss: 0.1278, step time: 0.1133\n",
      "37/223, train_loss: 0.1173, step time: 0.1103\n",
      "38/223, train_loss: 0.1057, step time: 0.1198\n",
      "39/223, train_loss: 0.1104, step time: 0.1000\n",
      "40/223, train_loss: 0.1068, step time: 0.1038\n",
      "41/223, train_loss: 0.1098, step time: 0.1144\n",
      "42/223, train_loss: 0.1095, step time: 0.1319\n",
      "43/223, train_loss: 0.1067, step time: 0.1156\n",
      "44/223, train_loss: 0.1246, step time: 0.1165\n",
      "45/223, train_loss: 0.1063, step time: 0.1216\n",
      "46/223, train_loss: 0.1206, step time: 0.1243\n",
      "47/223, train_loss: 0.1138, step time: 0.1156\n",
      "48/223, train_loss: 0.1108, step time: 0.1171\n",
      "49/223, train_loss: 0.0999, step time: 0.1078\n",
      "50/223, train_loss: 0.1121, step time: 0.1241\n",
      "51/223, train_loss: 0.1280, step time: 0.1124\n",
      "52/223, train_loss: 0.1074, step time: 0.1180\n",
      "53/223, train_loss: 0.1077, step time: 0.1199\n",
      "54/223, train_loss: 0.1056, step time: 0.1068\n",
      "55/223, train_loss: 0.1011, step time: 0.1313\n",
      "56/223, train_loss: 0.1100, step time: 0.1066\n",
      "57/223, train_loss: 0.1028, step time: 0.1000\n",
      "58/223, train_loss: 0.0991, step time: 0.1066\n",
      "59/223, train_loss: 0.1040, step time: 0.1091\n",
      "60/223, train_loss: 0.1210, step time: 0.1054\n",
      "61/223, train_loss: 0.1214, step time: 0.1710\n",
      "62/223, train_loss: 0.1130, step time: 0.1090\n",
      "63/223, train_loss: 0.1192, step time: 0.1173\n",
      "64/223, train_loss: 0.1182, step time: 0.1004\n",
      "65/223, train_loss: 0.1241, step time: 0.1203\n",
      "66/223, train_loss: 0.1171, step time: 0.1114\n",
      "67/223, train_loss: 0.1164, step time: 0.1100\n",
      "68/223, train_loss: 0.1317, step time: 0.1075\n",
      "69/223, train_loss: 0.1109, step time: 0.1054\n",
      "70/223, train_loss: 0.1098, step time: 0.1046\n",
      "71/223, train_loss: 0.1295, step time: 0.1007\n",
      "72/223, train_loss: 0.1159, step time: 0.1005\n",
      "73/223, train_loss: 0.1171, step time: 0.1044\n",
      "74/223, train_loss: 0.1106, step time: 0.1289\n",
      "75/223, train_loss: 0.1113, step time: 0.1109\n",
      "76/223, train_loss: 0.1123, step time: 0.1147\n",
      "77/223, train_loss: 0.1240, step time: 0.1118\n",
      "78/223, train_loss: 0.1087, step time: 0.1002\n",
      "79/223, train_loss: 0.1072, step time: 0.1130\n",
      "80/223, train_loss: 0.1054, step time: 0.1283\n",
      "81/223, train_loss: 0.1108, step time: 0.1082\n",
      "82/223, train_loss: 0.1064, step time: 0.1058\n",
      "83/223, train_loss: 0.1064, step time: 0.1286\n",
      "84/223, train_loss: 0.1155, step time: 0.1177\n",
      "85/223, train_loss: 0.1061, step time: 0.1118\n",
      "86/223, train_loss: 0.1099, step time: 0.1021\n",
      "87/223, train_loss: 0.1278, step time: 0.1153\n",
      "88/223, train_loss: 0.0965, step time: 0.1109\n",
      "89/223, train_loss: 0.1179, step time: 0.1293\n",
      "90/223, train_loss: 0.1098, step time: 0.1016\n",
      "91/223, train_loss: 0.1096, step time: 0.1000\n",
      "92/223, train_loss: 0.1120, step time: 0.1168\n",
      "93/223, train_loss: 0.1120, step time: 0.1083\n",
      "94/223, train_loss: 0.1052, step time: 0.1185\n",
      "95/223, train_loss: 0.1070, step time: 0.1001\n",
      "96/223, train_loss: 0.1082, step time: 0.1007\n",
      "97/223, train_loss: 0.1047, step time: 0.0997\n",
      "98/223, train_loss: 0.1155, step time: 0.0989\n",
      "99/223, train_loss: 0.1097, step time: 0.1255\n",
      "100/223, train_loss: 0.1245, step time: 0.0995\n",
      "101/223, train_loss: 0.1084, step time: 0.1133\n",
      "102/223, train_loss: 0.1084, step time: 0.1058\n",
      "103/223, train_loss: 0.1140, step time: 0.0991\n",
      "104/223, train_loss: 0.1125, step time: 0.1053\n",
      "105/223, train_loss: 0.0977, step time: 0.1112\n",
      "106/223, train_loss: 0.1183, step time: 0.1146\n",
      "107/223, train_loss: 0.1300, step time: 0.1441\n",
      "108/223, train_loss: 0.1045, step time: 0.1045\n",
      "109/223, train_loss: 0.1053, step time: 0.1025\n",
      "110/223, train_loss: 0.1089, step time: 0.1135\n",
      "111/223, train_loss: 0.1070, step time: 0.1267\n",
      "112/223, train_loss: 0.1110, step time: 0.1118\n",
      "113/223, train_loss: 0.1114, step time: 0.1010\n",
      "114/223, train_loss: 0.1113, step time: 0.1038\n",
      "115/223, train_loss: 0.1229, step time: 0.1114\n",
      "116/223, train_loss: 0.1197, step time: 0.1495\n",
      "117/223, train_loss: 0.1018, step time: 0.1024\n",
      "118/223, train_loss: 0.1070, step time: 0.1034\n",
      "119/223, train_loss: 0.1087, step time: 0.1409\n",
      "120/223, train_loss: 0.1181, step time: 0.1005\n",
      "121/223, train_loss: 0.1123, step time: 0.1149\n",
      "122/223, train_loss: 0.1259, step time: 0.1114\n",
      "123/223, train_loss: 0.1111, step time: 0.1271\n",
      "124/223, train_loss: 0.1202, step time: 0.1004\n",
      "125/223, train_loss: 0.1128, step time: 0.1198\n",
      "126/223, train_loss: 0.1168, step time: 0.1185\n",
      "127/223, train_loss: 0.1092, step time: 0.1026\n",
      "128/223, train_loss: 0.0943, step time: 0.1000\n",
      "129/223, train_loss: 0.1019, step time: 0.1167\n",
      "130/223, train_loss: 0.1172, step time: 0.1210\n",
      "131/223, train_loss: 0.1124, step time: 0.1270\n",
      "132/223, train_loss: 0.1209, step time: 0.1067\n",
      "133/223, train_loss: 0.1111, step time: 0.1013\n",
      "134/223, train_loss: 0.1088, step time: 0.1067\n",
      "135/223, train_loss: 0.1175, step time: 0.1154\n",
      "136/223, train_loss: 0.1232, step time: 0.1068\n",
      "137/223, train_loss: 0.1044, step time: 0.1002\n",
      "138/223, train_loss: 0.1108, step time: 0.1139\n",
      "139/223, train_loss: 0.1107, step time: 0.1098\n",
      "140/223, train_loss: 0.1085, step time: 0.1240\n",
      "141/223, train_loss: 0.1138, step time: 0.1059\n",
      "142/223, train_loss: 0.1153, step time: 0.1252\n",
      "143/223, train_loss: 0.1056, step time: 0.1211\n",
      "144/223, train_loss: 0.1147, step time: 0.1088\n",
      "145/223, train_loss: 0.1184, step time: 0.1242\n",
      "146/223, train_loss: 0.1177, step time: 0.1093\n",
      "147/223, train_loss: 0.1093, step time: 0.1279\n",
      "148/223, train_loss: 0.1139, step time: 0.1011\n",
      "149/223, train_loss: 0.1140, step time: 0.1052\n",
      "150/223, train_loss: 0.0990, step time: 0.1008\n",
      "151/223, train_loss: 0.1130, step time: 0.1051\n",
      "152/223, train_loss: 0.1146, step time: 0.1115\n",
      "153/223, train_loss: 0.1117, step time: 0.1106\n",
      "154/223, train_loss: 0.1109, step time: 0.1253\n",
      "155/223, train_loss: 0.1093, step time: 0.1079\n",
      "156/223, train_loss: 0.0999, step time: 0.1030\n",
      "157/223, train_loss: 0.1128, step time: 0.1004\n",
      "158/223, train_loss: 0.0961, step time: 0.1002\n",
      "159/223, train_loss: 0.0989, step time: 0.1011\n",
      "160/223, train_loss: 0.1220, step time: 0.1078\n",
      "161/223, train_loss: 0.1128, step time: 0.1023\n",
      "162/223, train_loss: 0.0999, step time: 0.1214\n",
      "163/223, train_loss: 0.1061, step time: 0.1142\n",
      "164/223, train_loss: 0.1202, step time: 0.1019\n",
      "165/223, train_loss: 0.1061, step time: 0.1084\n",
      "166/223, train_loss: 0.1051, step time: 0.1069\n",
      "167/223, train_loss: 0.1209, step time: 0.1162\n",
      "168/223, train_loss: 0.1064, step time: 0.1059\n",
      "169/223, train_loss: 0.1075, step time: 0.1480\n",
      "170/223, train_loss: 0.0972, step time: 0.1230\n",
      "171/223, train_loss: 0.1102, step time: 0.1083\n",
      "172/223, train_loss: 0.1238, step time: 0.1004\n",
      "173/223, train_loss: 0.1066, step time: 0.1188\n",
      "174/223, train_loss: 0.1062, step time: 0.1312\n",
      "175/223, train_loss: 0.1019, step time: 0.1120\n",
      "176/223, train_loss: 0.1143, step time: 0.1010\n",
      "177/223, train_loss: 0.0995, step time: 0.1242\n",
      "178/223, train_loss: 0.1059, step time: 0.1003\n",
      "179/223, train_loss: 0.1200, step time: 0.1000\n",
      "180/223, train_loss: 0.0988, step time: 0.1015\n",
      "181/223, train_loss: 0.1154, step time: 0.1063\n",
      "182/223, train_loss: 0.1159, step time: 0.1062\n",
      "183/223, train_loss: 0.1258, step time: 0.1134\n",
      "184/223, train_loss: 0.1031, step time: 0.1001\n",
      "185/223, train_loss: 0.1197, step time: 0.1002\n",
      "186/223, train_loss: 0.1221, step time: 0.1066\n",
      "187/223, train_loss: 0.1021, step time: 0.1390\n",
      "188/223, train_loss: 0.0994, step time: 0.1001\n",
      "189/223, train_loss: 0.1069, step time: 0.1057\n",
      "190/223, train_loss: 0.1025, step time: 0.1001\n",
      "191/223, train_loss: 0.1061, step time: 0.1009\n",
      "192/223, train_loss: 0.1123, step time: 0.0997\n",
      "193/223, train_loss: 0.1036, step time: 0.1109\n",
      "194/223, train_loss: 0.1109, step time: 0.1119\n",
      "195/223, train_loss: 0.1112, step time: 0.0995\n",
      "196/223, train_loss: 0.1136, step time: 0.1044\n",
      "197/223, train_loss: 0.1145, step time: 0.1157\n",
      "198/223, train_loss: 0.1156, step time: 0.1281\n",
      "199/223, train_loss: 0.1191, step time: 0.1023\n",
      "200/223, train_loss: 0.1091, step time: 0.1006\n",
      "201/223, train_loss: 0.1278, step time: 0.1151\n",
      "202/223, train_loss: 0.0955, step time: 0.0995\n",
      "203/223, train_loss: 0.1012, step time: 0.0994\n",
      "204/223, train_loss: 0.1099, step time: 0.1040\n",
      "205/223, train_loss: 0.0999, step time: 0.1059\n",
      "206/223, train_loss: 0.1088, step time: 0.1136\n",
      "207/223, train_loss: 0.1151, step time: 0.1045\n",
      "208/223, train_loss: 0.0982, step time: 0.0991\n",
      "209/223, train_loss: 0.1117, step time: 0.1386\n",
      "210/223, train_loss: 0.1054, step time: 0.1046\n",
      "211/223, train_loss: 0.1110, step time: 0.1087\n",
      "212/223, train_loss: 0.1178, step time: 0.0996\n",
      "213/223, train_loss: 0.1042, step time: 0.1059\n",
      "214/223, train_loss: 0.1024, step time: 0.1005\n",
      "215/223, train_loss: 0.1044, step time: 0.1013\n",
      "216/223, train_loss: 0.1128, step time: 0.1013\n",
      "217/223, train_loss: 0.1146, step time: 0.1136\n",
      "218/223, train_loss: 0.1060, step time: 0.1006\n",
      "219/223, train_loss: 0.1139, step time: 0.0990\n",
      "220/223, train_loss: 0.1078, step time: 0.1008\n",
      "221/223, train_loss: 0.1056, step time: 0.1002\n",
      "222/223, train_loss: 0.1105, step time: 0.1006\n",
      "223/223, train_loss: 0.1165, step time: 0.1075\n",
      "epoch 120 average loss: 0.1125\n",
      "saved new best metric model\n",
      "current epoch: 120 current mean dice: 0.8530 tc: 0.9175 wt: 0.8632 et: 0.7782\n",
      "best mean dice: 0.8530 at epoch: 120\n",
      "time consuming of epoch 120 is: 91.4716\n",
      "----------\n",
      "epoch 121/300\n",
      "1/223, train_loss: 0.1156, step time: 0.1134\n",
      "2/223, train_loss: 0.1042, step time: 0.0987\n",
      "3/223, train_loss: 0.1162, step time: 0.0988\n",
      "4/223, train_loss: 0.1048, step time: 0.0991\n",
      "5/223, train_loss: 0.1077, step time: 0.1002\n",
      "6/223, train_loss: 0.1047, step time: 0.1027\n",
      "7/223, train_loss: 0.1046, step time: 0.1030\n",
      "8/223, train_loss: 0.1069, step time: 0.1081\n",
      "9/223, train_loss: 0.1200, step time: 0.1080\n",
      "10/223, train_loss: 0.1129, step time: 0.1156\n",
      "11/223, train_loss: 0.1076, step time: 0.1106\n",
      "12/223, train_loss: 0.1025, step time: 0.1104\n",
      "13/223, train_loss: 0.1263, step time: 0.1032\n",
      "14/223, train_loss: 0.1096, step time: 0.1055\n",
      "15/223, train_loss: 0.1010, step time: 0.1067\n",
      "16/223, train_loss: 0.1016, step time: 0.1080\n",
      "17/223, train_loss: 0.1096, step time: 0.1028\n",
      "18/223, train_loss: 0.1184, step time: 0.1210\n",
      "19/223, train_loss: 0.1174, step time: 0.1034\n",
      "20/223, train_loss: 0.1138, step time: 0.1152\n",
      "21/223, train_loss: 0.1091, step time: 0.1064\n",
      "22/223, train_loss: 0.1012, step time: 0.1063\n",
      "23/223, train_loss: 0.0999, step time: 0.1047\n",
      "24/223, train_loss: 0.1128, step time: 0.1110\n",
      "25/223, train_loss: 0.1061, step time: 0.0994\n",
      "26/223, train_loss: 0.1192, step time: 0.1161\n",
      "27/223, train_loss: 0.1136, step time: 0.1032\n",
      "28/223, train_loss: 0.1169, step time: 0.1016\n",
      "29/223, train_loss: 0.1194, step time: 0.1095\n",
      "30/223, train_loss: 0.1221, step time: 0.1114\n",
      "31/223, train_loss: 0.1182, step time: 0.1237\n",
      "32/223, train_loss: 0.1213, step time: 0.1094\n",
      "33/223, train_loss: 0.1056, step time: 0.1092\n",
      "34/223, train_loss: 0.1214, step time: 0.1113\n",
      "35/223, train_loss: 0.1250, step time: 0.1318\n",
      "36/223, train_loss: 0.1156, step time: 0.1167\n",
      "37/223, train_loss: 0.1080, step time: 0.1185\n",
      "38/223, train_loss: 0.1045, step time: 0.1110\n",
      "39/223, train_loss: 0.1019, step time: 0.1440\n",
      "40/223, train_loss: 0.1047, step time: 0.1352\n",
      "41/223, train_loss: 0.1121, step time: 0.1041\n",
      "42/223, train_loss: 0.1061, step time: 0.1054\n",
      "43/223, train_loss: 0.1016, step time: 0.1015\n",
      "44/223, train_loss: 0.1107, step time: 0.1119\n",
      "45/223, train_loss: 0.0969, step time: 0.1001\n",
      "46/223, train_loss: 0.1079, step time: 0.1004\n",
      "47/223, train_loss: 0.1130, step time: 0.1001\n",
      "48/223, train_loss: 0.1114, step time: 0.1006\n",
      "49/223, train_loss: 0.1050, step time: 0.1003\n",
      "50/223, train_loss: 0.1066, step time: 0.1177\n",
      "51/223, train_loss: 0.1077, step time: 0.1097\n",
      "52/223, train_loss: 0.1064, step time: 0.1015\n",
      "53/223, train_loss: 0.1079, step time: 0.1058\n",
      "54/223, train_loss: 0.1037, step time: 0.0998\n",
      "55/223, train_loss: 0.1168, step time: 0.1062\n",
      "56/223, train_loss: 0.1100, step time: 0.1015\n",
      "57/223, train_loss: 0.1065, step time: 0.1053\n",
      "58/223, train_loss: 0.1146, step time: 0.1005\n",
      "59/223, train_loss: 0.1088, step time: 0.1012\n",
      "60/223, train_loss: 0.1049, step time: 0.1005\n",
      "61/223, train_loss: 0.1112, step time: 0.1079\n",
      "62/223, train_loss: 0.1071, step time: 0.1001\n",
      "63/223, train_loss: 0.1123, step time: 0.1115\n",
      "64/223, train_loss: 0.1059, step time: 0.1073\n",
      "65/223, train_loss: 0.1060, step time: 0.1003\n",
      "66/223, train_loss: 0.1133, step time: 0.1241\n",
      "67/223, train_loss: 0.1100, step time: 0.1260\n",
      "68/223, train_loss: 0.0974, step time: 0.1129\n",
      "69/223, train_loss: 0.1097, step time: 0.1114\n",
      "70/223, train_loss: 0.0957, step time: 0.1083\n",
      "71/223, train_loss: 0.1092, step time: 0.1109\n",
      "72/223, train_loss: 0.1041, step time: 0.1155\n",
      "73/223, train_loss: 0.1069, step time: 0.1057\n",
      "74/223, train_loss: 0.1143, step time: 0.1118\n",
      "75/223, train_loss: 0.1137, step time: 0.1157\n",
      "76/223, train_loss: 0.1150, step time: 0.1213\n",
      "77/223, train_loss: 0.1022, step time: 0.1003\n",
      "78/223, train_loss: 0.1102, step time: 0.1049\n",
      "79/223, train_loss: 0.1141, step time: 0.1230\n",
      "80/223, train_loss: 0.1027, step time: 0.1220\n",
      "81/223, train_loss: 0.1251, step time: 0.1005\n",
      "82/223, train_loss: 0.3164, step time: 0.1026\n",
      "83/223, train_loss: 0.1074, step time: 0.1002\n",
      "84/223, train_loss: 0.1218, step time: 0.1003\n",
      "85/223, train_loss: 0.0968, step time: 0.1042\n",
      "86/223, train_loss: 0.1211, step time: 0.1002\n",
      "87/223, train_loss: 0.1132, step time: 0.1038\n",
      "88/223, train_loss: 0.0984, step time: 0.1122\n",
      "89/223, train_loss: 0.1150, step time: 0.0998\n",
      "90/223, train_loss: 0.1182, step time: 0.1004\n",
      "91/223, train_loss: 0.1141, step time: 0.1627\n",
      "92/223, train_loss: 0.1129, step time: 0.0992\n",
      "93/223, train_loss: 0.1111, step time: 0.1003\n",
      "94/223, train_loss: 0.1129, step time: 0.1102\n",
      "95/223, train_loss: 0.1056, step time: 0.1257\n",
      "96/223, train_loss: 0.1208, step time: 0.1170\n",
      "97/223, train_loss: 0.1223, step time: 0.1001\n",
      "98/223, train_loss: 0.1033, step time: 0.0995\n",
      "99/223, train_loss: 0.1250, step time: 0.1044\n",
      "100/223, train_loss: 0.1076, step time: 0.1003\n",
      "101/223, train_loss: 0.1211, step time: 0.1138\n",
      "102/223, train_loss: 0.1110, step time: 0.1006\n",
      "103/223, train_loss: 0.1155, step time: 0.1001\n",
      "104/223, train_loss: 0.1194, step time: 0.1197\n",
      "105/223, train_loss: 0.1187, step time: 0.1345\n",
      "106/223, train_loss: 0.1067, step time: 0.1021\n",
      "107/223, train_loss: 0.1080, step time: 0.1006\n",
      "108/223, train_loss: 0.1103, step time: 0.1111\n",
      "109/223, train_loss: 0.1159, step time: 0.1174\n",
      "110/223, train_loss: 0.1029, step time: 0.1065\n",
      "111/223, train_loss: 0.1123, step time: 0.1122\n",
      "112/223, train_loss: 0.1100, step time: 0.1003\n",
      "113/223, train_loss: 0.1012, step time: 0.1374\n",
      "114/223, train_loss: 0.1057, step time: 0.1016\n",
      "115/223, train_loss: 0.1119, step time: 0.1090\n",
      "116/223, train_loss: 0.1257, step time: 0.1012\n",
      "117/223, train_loss: 0.1047, step time: 0.1176\n",
      "118/223, train_loss: 0.0999, step time: 0.1084\n",
      "119/223, train_loss: 0.1034, step time: 0.1005\n",
      "120/223, train_loss: 0.1123, step time: 0.1011\n",
      "121/223, train_loss: 0.1246, step time: 0.1100\n",
      "122/223, train_loss: 0.1142, step time: 0.1297\n",
      "123/223, train_loss: 0.1060, step time: 0.1540\n",
      "124/223, train_loss: 0.1250, step time: 0.1009\n",
      "125/223, train_loss: 0.1064, step time: 0.1344\n",
      "126/223, train_loss: 0.1089, step time: 0.1142\n",
      "127/223, train_loss: 0.1077, step time: 0.1187\n",
      "128/223, train_loss: 0.1082, step time: 0.0991\n",
      "129/223, train_loss: 0.0995, step time: 0.1043\n",
      "130/223, train_loss: 0.1139, step time: 0.1179\n",
      "131/223, train_loss: 0.1021, step time: 0.1001\n",
      "132/223, train_loss: 0.1065, step time: 0.0995\n",
      "133/223, train_loss: 0.1019, step time: 0.1260\n",
      "134/223, train_loss: 0.1031, step time: 0.1102\n",
      "135/223, train_loss: 0.1017, step time: 0.1050\n",
      "136/223, train_loss: 0.1107, step time: 0.1086\n",
      "137/223, train_loss: 0.1101, step time: 0.1005\n",
      "138/223, train_loss: 0.1047, step time: 0.1014\n",
      "139/223, train_loss: 0.1158, step time: 0.1690\n",
      "140/223, train_loss: 0.1056, step time: 0.1130\n",
      "141/223, train_loss: 0.1228, step time: 0.1076\n",
      "142/223, train_loss: 0.1169, step time: 0.1193\n",
      "143/223, train_loss: 0.1032, step time: 0.1154\n",
      "144/223, train_loss: 0.0985, step time: 0.1042\n",
      "145/223, train_loss: 0.1038, step time: 0.0996\n",
      "146/223, train_loss: 0.1081, step time: 0.1321\n",
      "147/223, train_loss: 0.1036, step time: 0.1188\n",
      "148/223, train_loss: 0.1071, step time: 0.1018\n",
      "149/223, train_loss: 0.1093, step time: 0.1031\n",
      "150/223, train_loss: 0.1020, step time: 0.1087\n",
      "151/223, train_loss: 0.1038, step time: 0.1059\n",
      "152/223, train_loss: 0.1001, step time: 0.1180\n",
      "153/223, train_loss: 0.1198, step time: 0.1129\n",
      "154/223, train_loss: 0.1042, step time: 0.1092\n",
      "155/223, train_loss: 0.1150, step time: 0.1022\n",
      "156/223, train_loss: 0.1043, step time: 0.0996\n",
      "157/223, train_loss: 0.1195, step time: 0.1026\n",
      "158/223, train_loss: 0.1136, step time: 0.1035\n",
      "159/223, train_loss: 0.1125, step time: 0.1101\n",
      "160/223, train_loss: 0.1073, step time: 0.0997\n",
      "161/223, train_loss: 0.1048, step time: 0.1175\n",
      "162/223, train_loss: 0.1093, step time: 0.1135\n",
      "163/223, train_loss: 0.1227, step time: 0.1006\n",
      "164/223, train_loss: 0.1044, step time: 0.1061\n",
      "165/223, train_loss: 0.1030, step time: 0.1011\n",
      "166/223, train_loss: 0.1146, step time: 0.0988\n",
      "167/223, train_loss: 0.1062, step time: 0.0995\n",
      "168/223, train_loss: 0.1070, step time: 0.0991\n",
      "169/223, train_loss: 0.1083, step time: 0.1117\n",
      "170/223, train_loss: 0.1098, step time: 0.1400\n",
      "171/223, train_loss: 0.1040, step time: 0.0998\n",
      "172/223, train_loss: 0.1022, step time: 0.1162\n",
      "173/223, train_loss: 0.0978, step time: 0.1148\n",
      "174/223, train_loss: 0.1107, step time: 0.1188\n",
      "175/223, train_loss: 0.0990, step time: 0.1112\n",
      "176/223, train_loss: 0.1146, step time: 0.0996\n",
      "177/223, train_loss: 0.1090, step time: 0.1044\n",
      "178/223, train_loss: 0.1193, step time: 0.0987\n",
      "179/223, train_loss: 0.1129, step time: 0.0999\n",
      "180/223, train_loss: 0.0974, step time: 0.1088\n",
      "181/223, train_loss: 0.1287, step time: 0.1080\n",
      "182/223, train_loss: 0.1176, step time: 0.1078\n",
      "183/223, train_loss: 0.1111, step time: 0.1083\n",
      "184/223, train_loss: 0.1086, step time: 0.1074\n",
      "185/223, train_loss: 0.1068, step time: 0.0984\n",
      "186/223, train_loss: 0.1195, step time: 0.1015\n",
      "187/223, train_loss: 0.1111, step time: 0.0990\n",
      "188/223, train_loss: 0.1084, step time: 0.1070\n",
      "189/223, train_loss: 0.1205, step time: 0.1128\n",
      "190/223, train_loss: 0.1037, step time: 0.1194\n",
      "191/223, train_loss: 0.1044, step time: 0.1244\n",
      "192/223, train_loss: 0.1032, step time: 0.1172\n",
      "193/223, train_loss: 0.1071, step time: 0.1194\n",
      "194/223, train_loss: 0.1031, step time: 0.1098\n",
      "195/223, train_loss: 0.1140, step time: 0.1164\n",
      "196/223, train_loss: 0.1143, step time: 0.1253\n",
      "197/223, train_loss: 0.1151, step time: 0.1143\n",
      "198/223, train_loss: 0.1101, step time: 0.1092\n",
      "199/223, train_loss: 0.1052, step time: 0.1186\n",
      "200/223, train_loss: 0.1017, step time: 0.1147\n",
      "201/223, train_loss: 0.1072, step time: 0.1055\n",
      "202/223, train_loss: 0.1045, step time: 0.1015\n",
      "203/223, train_loss: 0.1103, step time: 0.1091\n",
      "204/223, train_loss: 0.1000, step time: 0.1165\n",
      "205/223, train_loss: 0.0988, step time: 0.1071\n",
      "206/223, train_loss: 0.1212, step time: 0.1007\n",
      "207/223, train_loss: 0.1161, step time: 0.1010\n",
      "208/223, train_loss: 0.1253, step time: 0.1005\n",
      "209/223, train_loss: 0.1067, step time: 0.1171\n",
      "210/223, train_loss: 0.0992, step time: 0.1035\n",
      "211/223, train_loss: 0.1058, step time: 0.1256\n",
      "212/223, train_loss: 0.1049, step time: 0.1155\n",
      "213/223, train_loss: 0.1145, step time: 0.1010\n",
      "214/223, train_loss: 0.1084, step time: 0.1007\n",
      "215/223, train_loss: 0.1129, step time: 0.1001\n",
      "216/223, train_loss: 0.1142, step time: 0.1104\n",
      "217/223, train_loss: 0.1211, step time: 0.0999\n",
      "218/223, train_loss: 0.1099, step time: 0.0994\n",
      "219/223, train_loss: 0.1199, step time: 0.0999\n",
      "220/223, train_loss: 0.1321, step time: 0.1002\n",
      "221/223, train_loss: 0.1130, step time: 0.1002\n",
      "222/223, train_loss: 0.1268, step time: 0.1001\n",
      "223/223, train_loss: 0.1006, step time: 0.0998\n",
      "epoch 121 average loss: 0.1110\n",
      "time consuming of epoch 121 is: 87.6845\n",
      "----------\n",
      "epoch 122/300\n",
      "1/223, train_loss: 0.1141, step time: 0.1104\n",
      "2/223, train_loss: 0.1051, step time: 0.1137\n",
      "3/223, train_loss: 0.1038, step time: 0.1230\n",
      "4/223, train_loss: 0.1054, step time: 0.1230\n",
      "5/223, train_loss: 0.0961, step time: 0.1141\n",
      "6/223, train_loss: 0.1086, step time: 0.1024\n",
      "7/223, train_loss: 0.1111, step time: 0.1165\n",
      "8/223, train_loss: 0.1141, step time: 0.1197\n",
      "9/223, train_loss: 0.1160, step time: 0.0992\n",
      "10/223, train_loss: 0.1156, step time: 0.0988\n",
      "11/223, train_loss: 0.1169, step time: 0.0983\n",
      "12/223, train_loss: 0.1103, step time: 0.1083\n",
      "13/223, train_loss: 0.1104, step time: 0.1121\n",
      "14/223, train_loss: 0.1087, step time: 0.1000\n",
      "15/223, train_loss: 0.1083, step time: 0.1000\n",
      "16/223, train_loss: 0.1275, step time: 0.1399\n",
      "17/223, train_loss: 0.1099, step time: 0.1159\n",
      "18/223, train_loss: 0.0993, step time: 0.1127\n",
      "19/223, train_loss: 0.1090, step time: 0.1069\n",
      "20/223, train_loss: 0.1045, step time: 0.1103\n",
      "21/223, train_loss: 0.1040, step time: 0.1205\n",
      "22/223, train_loss: 0.1074, step time: 0.1198\n",
      "23/223, train_loss: 0.1037, step time: 0.0996\n",
      "24/223, train_loss: 0.1145, step time: 0.0994\n",
      "25/223, train_loss: 0.1176, step time: 0.1392\n",
      "26/223, train_loss: 0.1141, step time: 0.1237\n",
      "27/223, train_loss: 0.1181, step time: 0.1264\n",
      "28/223, train_loss: 0.1149, step time: 0.0991\n",
      "29/223, train_loss: 0.1155, step time: 0.1231\n",
      "30/223, train_loss: 0.1087, step time: 0.1455\n",
      "31/223, train_loss: 0.1172, step time: 0.1179\n",
      "32/223, train_loss: 0.1053, step time: 0.1015\n",
      "33/223, train_loss: 0.1013, step time: 0.1181\n",
      "34/223, train_loss: 0.1104, step time: 0.0996\n",
      "35/223, train_loss: 0.1011, step time: 0.1173\n",
      "36/223, train_loss: 0.1109, step time: 0.1277\n",
      "37/223, train_loss: 0.1111, step time: 0.1114\n",
      "38/223, train_loss: 0.1086, step time: 0.1060\n",
      "39/223, train_loss: 0.1202, step time: 0.1138\n",
      "40/223, train_loss: 0.1288, step time: 0.1140\n",
      "41/223, train_loss: 0.1241, step time: 0.1104\n",
      "42/223, train_loss: 0.1018, step time: 0.1491\n",
      "43/223, train_loss: 0.1022, step time: 0.1202\n",
      "44/223, train_loss: 0.1049, step time: 0.1108\n",
      "45/223, train_loss: 0.1030, step time: 0.1219\n",
      "46/223, train_loss: 0.1119, step time: 0.1045\n",
      "47/223, train_loss: 0.1137, step time: 0.1266\n",
      "48/223, train_loss: 0.1150, step time: 0.1067\n",
      "49/223, train_loss: 0.1032, step time: 0.1066\n",
      "50/223, train_loss: 0.1263, step time: 0.1008\n",
      "51/223, train_loss: 0.0952, step time: 0.1080\n",
      "52/223, train_loss: 0.1117, step time: 0.1078\n",
      "53/223, train_loss: 0.1146, step time: 0.1137\n",
      "54/223, train_loss: 0.1098, step time: 0.1224\n",
      "55/223, train_loss: 0.1048, step time: 0.1025\n",
      "56/223, train_loss: 0.1037, step time: 0.1011\n",
      "57/223, train_loss: 0.1096, step time: 0.1057\n",
      "58/223, train_loss: 0.1022, step time: 0.1078\n",
      "59/223, train_loss: 0.1090, step time: 0.1065\n",
      "60/223, train_loss: 0.1087, step time: 0.1089\n",
      "61/223, train_loss: 0.1205, step time: 0.1389\n",
      "62/223, train_loss: 0.1226, step time: 0.1190\n",
      "63/223, train_loss: 0.1041, step time: 0.1187\n",
      "64/223, train_loss: 0.1133, step time: 0.1146\n",
      "65/223, train_loss: 0.1038, step time: 0.1221\n",
      "66/223, train_loss: 0.1051, step time: 0.1115\n",
      "67/223, train_loss: 0.1190, step time: 0.1060\n",
      "68/223, train_loss: 0.1097, step time: 0.1218\n",
      "69/223, train_loss: 0.1086, step time: 0.1011\n",
      "70/223, train_loss: 0.1206, step time: 0.1217\n",
      "71/223, train_loss: 0.1218, step time: 0.1219\n",
      "72/223, train_loss: 0.1187, step time: 0.1091\n",
      "73/223, train_loss: 0.1115, step time: 0.1008\n",
      "74/223, train_loss: 0.1145, step time: 0.1080\n",
      "75/223, train_loss: 0.1074, step time: 0.1085\n",
      "76/223, train_loss: 0.1221, step time: 0.1005\n",
      "77/223, train_loss: 0.1157, step time: 0.1016\n",
      "78/223, train_loss: 0.1135, step time: 0.1256\n",
      "79/223, train_loss: 0.1092, step time: 0.1001\n",
      "80/223, train_loss: 0.1169, step time: 0.1013\n",
      "81/223, train_loss: 0.1038, step time: 0.1583\n",
      "82/223, train_loss: 0.1176, step time: 0.1038\n",
      "83/223, train_loss: 0.1138, step time: 0.0992\n",
      "84/223, train_loss: 0.1145, step time: 0.1122\n",
      "85/223, train_loss: 0.1224, step time: 0.1106\n",
      "86/223, train_loss: 0.1259, step time: 0.1054\n",
      "87/223, train_loss: 0.1142, step time: 0.1066\n",
      "88/223, train_loss: 0.1114, step time: 0.1004\n",
      "89/223, train_loss: 0.1028, step time: 0.1010\n",
      "90/223, train_loss: 0.1116, step time: 0.1026\n",
      "91/223, train_loss: 0.1093, step time: 0.1109\n",
      "92/223, train_loss: 0.1023, step time: 0.1091\n",
      "93/223, train_loss: 0.1181, step time: 0.1099\n",
      "94/223, train_loss: 0.1064, step time: 0.1001\n",
      "95/223, train_loss: 0.1090, step time: 0.1015\n",
      "96/223, train_loss: 0.0968, step time: 0.1019\n",
      "97/223, train_loss: 0.1175, step time: 0.1288\n",
      "98/223, train_loss: 0.1015, step time: 0.1142\n",
      "99/223, train_loss: 0.1100, step time: 0.1146\n",
      "100/223, train_loss: 0.1128, step time: 0.1033\n",
      "101/223, train_loss: 0.1095, step time: 0.1424\n",
      "102/223, train_loss: 0.1179, step time: 0.1242\n",
      "103/223, train_loss: 0.1196, step time: 0.1047\n",
      "104/223, train_loss: 0.1012, step time: 0.1012\n",
      "105/223, train_loss: 0.1063, step time: 0.1005\n",
      "106/223, train_loss: 0.1279, step time: 0.0995\n",
      "107/223, train_loss: 0.1228, step time: 0.1004\n",
      "108/223, train_loss: 0.1033, step time: 0.1057\n",
      "109/223, train_loss: 0.1117, step time: 0.1095\n",
      "110/223, train_loss: 0.1074, step time: 0.1002\n",
      "111/223, train_loss: 0.1083, step time: 0.0996\n",
      "112/223, train_loss: 0.1124, step time: 0.1009\n",
      "113/223, train_loss: 0.1107, step time: 0.1060\n",
      "114/223, train_loss: 0.1106, step time: 0.1060\n",
      "115/223, train_loss: 0.1078, step time: 0.1071\n",
      "116/223, train_loss: 0.0974, step time: 0.1037\n",
      "117/223, train_loss: 0.0969, step time: 0.1049\n",
      "118/223, train_loss: 0.1066, step time: 0.1150\n",
      "119/223, train_loss: 0.1092, step time: 0.1256\n",
      "120/223, train_loss: 0.1110, step time: 0.1121\n",
      "121/223, train_loss: 0.1098, step time: 0.0995\n",
      "122/223, train_loss: 0.0982, step time: 0.1336\n",
      "123/223, train_loss: 0.1034, step time: 0.1239\n",
      "124/223, train_loss: 0.1127, step time: 0.1060\n",
      "125/223, train_loss: 0.0974, step time: 0.1284\n",
      "126/223, train_loss: 0.1153, step time: 0.1260\n",
      "127/223, train_loss: 0.1164, step time: 0.1131\n",
      "128/223, train_loss: 0.1027, step time: 0.1124\n",
      "129/223, train_loss: 0.1129, step time: 0.1084\n",
      "130/223, train_loss: 0.1072, step time: 0.1344\n",
      "131/223, train_loss: 0.1047, step time: 0.1005\n",
      "132/223, train_loss: 0.1095, step time: 0.1100\n",
      "133/223, train_loss: 0.1121, step time: 0.1055\n",
      "134/223, train_loss: 0.1084, step time: 0.1002\n",
      "135/223, train_loss: 0.1069, step time: 0.1045\n",
      "136/223, train_loss: 0.1079, step time: 0.1089\n",
      "137/223, train_loss: 0.1021, step time: 0.1031\n",
      "138/223, train_loss: 0.1141, step time: 0.1078\n",
      "139/223, train_loss: 0.1256, step time: 0.1088\n",
      "140/223, train_loss: 0.0980, step time: 0.1038\n",
      "141/223, train_loss: 0.1070, step time: 0.1135\n",
      "142/223, train_loss: 0.1050, step time: 0.1149\n",
      "143/223, train_loss: 0.1061, step time: 0.1345\n",
      "144/223, train_loss: 0.1086, step time: 0.1258\n",
      "145/223, train_loss: 0.1235, step time: 0.1002\n",
      "146/223, train_loss: 0.1210, step time: 0.1107\n",
      "147/223, train_loss: 0.0994, step time: 0.1347\n",
      "148/223, train_loss: 0.1144, step time: 0.1076\n",
      "149/223, train_loss: 0.1112, step time: 0.1043\n",
      "150/223, train_loss: 0.1011, step time: 0.1156\n",
      "151/223, train_loss: 0.1033, step time: 0.1181\n",
      "152/223, train_loss: 0.1039, step time: 0.1170\n",
      "153/223, train_loss: 0.1233, step time: 0.1085\n",
      "154/223, train_loss: 0.1084, step time: 0.1146\n",
      "155/223, train_loss: 0.1198, step time: 0.1081\n",
      "156/223, train_loss: 0.1053, step time: 0.1124\n",
      "157/223, train_loss: 0.1321, step time: 0.1272\n",
      "158/223, train_loss: 0.1048, step time: 0.1118\n",
      "159/223, train_loss: 0.1125, step time: 0.1132\n",
      "160/223, train_loss: 0.1123, step time: 0.1125\n",
      "161/223, train_loss: 0.1038, step time: 0.1129\n",
      "162/223, train_loss: 0.1004, step time: 0.1407\n",
      "163/223, train_loss: 0.0955, step time: 0.1128\n",
      "164/223, train_loss: 0.1205, step time: 0.1137\n",
      "165/223, train_loss: 0.1122, step time: 0.1171\n",
      "166/223, train_loss: 0.1205, step time: 0.1034\n",
      "167/223, train_loss: 0.1239, step time: 0.1101\n",
      "168/223, train_loss: 0.1076, step time: 0.0999\n",
      "169/223, train_loss: 0.1107, step time: 0.1134\n",
      "170/223, train_loss: 0.1026, step time: 0.1099\n",
      "171/223, train_loss: 0.1209, step time: 0.1067\n",
      "172/223, train_loss: 0.1103, step time: 0.1046\n",
      "173/223, train_loss: 0.1171, step time: 0.1210\n",
      "174/223, train_loss: 0.1041, step time: 0.1002\n",
      "175/223, train_loss: 0.1165, step time: 0.1168\n",
      "176/223, train_loss: 0.0999, step time: 0.1003\n",
      "177/223, train_loss: 0.1067, step time: 0.1005\n",
      "178/223, train_loss: 0.1060, step time: 0.1052\n",
      "179/223, train_loss: 0.1058, step time: 0.0994\n",
      "180/223, train_loss: 0.1005, step time: 0.0993\n",
      "181/223, train_loss: 0.1071, step time: 0.1139\n",
      "182/223, train_loss: 0.1078, step time: 0.1143\n",
      "183/223, train_loss: 0.1138, step time: 0.1273\n",
      "184/223, train_loss: 0.1062, step time: 0.1147\n",
      "185/223, train_loss: 0.1145, step time: 0.1065\n",
      "186/223, train_loss: 0.1153, step time: 0.1129\n",
      "187/223, train_loss: 0.1001, step time: 0.1423\n",
      "188/223, train_loss: 0.1043, step time: 0.0995\n",
      "189/223, train_loss: 0.1069, step time: 0.1130\n",
      "190/223, train_loss: 0.1045, step time: 0.1206\n",
      "191/223, train_loss: 0.1019, step time: 0.1180\n",
      "192/223, train_loss: 0.1111, step time: 0.1059\n",
      "193/223, train_loss: 0.1166, step time: 0.1081\n",
      "194/223, train_loss: 0.1050, step time: 0.1156\n",
      "195/223, train_loss: 0.1028, step time: 0.1154\n",
      "196/223, train_loss: 0.3037, step time: 0.1051\n",
      "197/223, train_loss: 0.0977, step time: 0.1120\n",
      "198/223, train_loss: 0.1063, step time: 0.1130\n",
      "199/223, train_loss: 0.1114, step time: 0.1128\n",
      "200/223, train_loss: 0.1250, step time: 0.1214\n",
      "201/223, train_loss: 0.1094, step time: 0.1081\n",
      "202/223, train_loss: 0.0995, step time: 0.1105\n",
      "203/223, train_loss: 0.1114, step time: 0.1020\n",
      "204/223, train_loss: 0.1056, step time: 0.1125\n",
      "205/223, train_loss: 0.1124, step time: 0.0995\n",
      "206/223, train_loss: 0.1108, step time: 0.1005\n",
      "207/223, train_loss: 0.1022, step time: 0.1014\n",
      "208/223, train_loss: 0.1047, step time: 0.0993\n",
      "209/223, train_loss: 0.0994, step time: 0.0994\n",
      "210/223, train_loss: 0.1137, step time: 0.1450\n",
      "211/223, train_loss: 0.1020, step time: 0.1152\n",
      "212/223, train_loss: 0.1161, step time: 0.1042\n",
      "213/223, train_loss: 0.1080, step time: 0.1009\n",
      "214/223, train_loss: 0.1117, step time: 0.1248\n",
      "215/223, train_loss: 0.1074, step time: 0.1064\n",
      "216/223, train_loss: 0.1203, step time: 0.1139\n",
      "217/223, train_loss: 0.0981, step time: 0.1016\n",
      "218/223, train_loss: 0.1138, step time: 0.0996\n",
      "219/223, train_loss: 0.1173, step time: 0.1001\n",
      "220/223, train_loss: 0.1205, step time: 0.1001\n",
      "221/223, train_loss: 0.1091, step time: 0.0983\n",
      "222/223, train_loss: 0.1058, step time: 0.0983\n",
      "223/223, train_loss: 0.1081, step time: 0.1003\n",
      "epoch 122 average loss: 0.1110\n",
      "time consuming of epoch 122 is: 88.8729\n",
      "----------\n",
      "epoch 123/300\n",
      "1/223, train_loss: 0.1116, step time: 0.1018\n",
      "2/223, train_loss: 0.1085, step time: 0.1022\n",
      "3/223, train_loss: 0.1238, step time: 0.1175\n",
      "4/223, train_loss: 0.1133, step time: 0.1092\n",
      "5/223, train_loss: 0.1119, step time: 0.1070\n",
      "6/223, train_loss: 0.0940, step time: 0.1129\n",
      "7/223, train_loss: 0.1199, step time: 0.1128\n",
      "8/223, train_loss: 0.1099, step time: 0.1112\n",
      "9/223, train_loss: 0.1095, step time: 0.1316\n",
      "10/223, train_loss: 0.1184, step time: 0.1121\n",
      "11/223, train_loss: 0.0996, step time: 0.1024\n",
      "12/223, train_loss: 0.1133, step time: 0.1158\n",
      "13/223, train_loss: 0.1046, step time: 0.1136\n",
      "14/223, train_loss: 0.1162, step time: 0.1694\n",
      "15/223, train_loss: 0.1159, step time: 0.1473\n",
      "16/223, train_loss: 0.1192, step time: 0.1151\n",
      "17/223, train_loss: 0.1104, step time: 0.1449\n",
      "18/223, train_loss: 0.1092, step time: 0.1125\n",
      "19/223, train_loss: 0.1045, step time: 0.1160\n",
      "20/223, train_loss: 0.1067, step time: 0.1004\n",
      "21/223, train_loss: 0.1079, step time: 0.1133\n",
      "22/223, train_loss: 0.1070, step time: 0.1132\n",
      "23/223, train_loss: 0.1150, step time: 0.1001\n",
      "24/223, train_loss: 0.1104, step time: 0.1012\n",
      "25/223, train_loss: 0.1137, step time: 0.1089\n",
      "26/223, train_loss: 0.1121, step time: 0.0996\n",
      "27/223, train_loss: 0.1065, step time: 0.1004\n",
      "28/223, train_loss: 0.1073, step time: 0.1005\n",
      "29/223, train_loss: 0.1058, step time: 0.1006\n",
      "30/223, train_loss: 0.1122, step time: 0.1007\n",
      "31/223, train_loss: 0.1086, step time: 0.1432\n",
      "32/223, train_loss: 0.1081, step time: 0.1089\n",
      "33/223, train_loss: 0.1176, step time: 0.1123\n",
      "34/223, train_loss: 0.1150, step time: 0.1055\n",
      "35/223, train_loss: 0.1062, step time: 0.1163\n",
      "36/223, train_loss: 0.1002, step time: 0.1147\n",
      "37/223, train_loss: 0.1116, step time: 0.1045\n",
      "38/223, train_loss: 0.1173, step time: 0.1100\n",
      "39/223, train_loss: 0.1004, step time: 0.1184\n",
      "40/223, train_loss: 0.1211, step time: 0.1162\n",
      "41/223, train_loss: 0.1042, step time: 0.1064\n",
      "42/223, train_loss: 0.1154, step time: 0.1207\n",
      "43/223, train_loss: 0.1074, step time: 0.1209\n",
      "44/223, train_loss: 0.1248, step time: 0.1124\n",
      "45/223, train_loss: 0.1109, step time: 0.1002\n",
      "46/223, train_loss: 0.1159, step time: 0.1337\n",
      "47/223, train_loss: 0.1193, step time: 0.1010\n",
      "48/223, train_loss: 0.1110, step time: 0.1004\n",
      "49/223, train_loss: 0.1028, step time: 0.1106\n",
      "50/223, train_loss: 0.1196, step time: 0.1005\n",
      "51/223, train_loss: 0.1131, step time: 0.1088\n",
      "52/223, train_loss: 0.1029, step time: 0.1232\n",
      "53/223, train_loss: 0.1025, step time: 0.1210\n",
      "54/223, train_loss: 0.1011, step time: 0.1357\n",
      "55/223, train_loss: 0.1064, step time: 0.1006\n",
      "56/223, train_loss: 0.1035, step time: 0.1012\n",
      "57/223, train_loss: 0.1038, step time: 0.1006\n",
      "58/223, train_loss: 0.1136, step time: 0.1084\n",
      "59/223, train_loss: 0.1092, step time: 0.1136\n",
      "60/223, train_loss: 0.1127, step time: 0.0996\n",
      "61/223, train_loss: 0.1030, step time: 0.1105\n",
      "62/223, train_loss: 0.1175, step time: 0.1003\n",
      "63/223, train_loss: 0.1172, step time: 0.1022\n",
      "64/223, train_loss: 0.1061, step time: 0.1010\n",
      "65/223, train_loss: 0.1068, step time: 0.1167\n",
      "66/223, train_loss: 0.1034, step time: 0.1023\n",
      "67/223, train_loss: 0.1218, step time: 0.1070\n",
      "68/223, train_loss: 0.1183, step time: 0.1083\n",
      "69/223, train_loss: 0.1079, step time: 0.1099\n",
      "70/223, train_loss: 0.0986, step time: 0.1092\n",
      "71/223, train_loss: 0.1059, step time: 0.1026\n",
      "72/223, train_loss: 0.1179, step time: 0.1008\n",
      "73/223, train_loss: 0.1147, step time: 0.1049\n",
      "74/223, train_loss: 0.1223, step time: 0.0991\n",
      "75/223, train_loss: 0.1161, step time: 0.1033\n",
      "76/223, train_loss: 0.1086, step time: 0.1149\n",
      "77/223, train_loss: 0.1128, step time: 0.1118\n",
      "78/223, train_loss: 0.1248, step time: 0.1170\n",
      "79/223, train_loss: 0.1061, step time: 0.1196\n",
      "80/223, train_loss: 0.1025, step time: 0.1005\n",
      "81/223, train_loss: 0.1144, step time: 0.1173\n",
      "82/223, train_loss: 0.1110, step time: 0.1091\n",
      "83/223, train_loss: 0.1020, step time: 0.1335\n",
      "84/223, train_loss: 0.1081, step time: 0.1134\n",
      "85/223, train_loss: 0.1064, step time: 0.1122\n",
      "86/223, train_loss: 0.1239, step time: 0.1040\n",
      "87/223, train_loss: 0.1262, step time: 0.1277\n",
      "88/223, train_loss: 0.0991, step time: 0.1011\n",
      "89/223, train_loss: 0.3054, step time: 0.1017\n",
      "90/223, train_loss: 0.1073, step time: 0.1496\n",
      "91/223, train_loss: 0.1158, step time: 0.1274\n",
      "92/223, train_loss: 0.1023, step time: 0.0995\n",
      "93/223, train_loss: 0.1057, step time: 0.1175\n",
      "94/223, train_loss: 0.1248, step time: 0.0991\n",
      "95/223, train_loss: 0.1158, step time: 0.1278\n",
      "96/223, train_loss: 0.1096, step time: 0.1013\n",
      "97/223, train_loss: 0.1223, step time: 0.1031\n",
      "98/223, train_loss: 0.1107, step time: 0.1391\n",
      "99/223, train_loss: 0.1086, step time: 0.1001\n",
      "100/223, train_loss: 0.1087, step time: 0.1004\n",
      "101/223, train_loss: 0.1231, step time: 0.1200\n",
      "102/223, train_loss: 0.1016, step time: 0.0997\n",
      "103/223, train_loss: 0.1033, step time: 0.1003\n",
      "104/223, train_loss: 0.1037, step time: 0.1258\n",
      "105/223, train_loss: 0.1146, step time: 0.1168\n",
      "106/223, train_loss: 0.1047, step time: 0.1053\n",
      "107/223, train_loss: 0.1088, step time: 0.1020\n",
      "108/223, train_loss: 0.1177, step time: 0.1025\n",
      "109/223, train_loss: 0.1049, step time: 0.1154\n",
      "110/223, train_loss: 0.1060, step time: 0.1004\n",
      "111/223, train_loss: 0.1120, step time: 0.1004\n",
      "112/223, train_loss: 0.1047, step time: 0.1014\n",
      "113/223, train_loss: 0.1018, step time: 0.1001\n",
      "114/223, train_loss: 0.1140, step time: 0.1013\n",
      "115/223, train_loss: 0.1043, step time: 0.1002\n",
      "116/223, train_loss: 0.1244, step time: 0.1758\n",
      "117/223, train_loss: 0.1013, step time: 0.1063\n",
      "118/223, train_loss: 0.1061, step time: 0.1010\n",
      "119/223, train_loss: 0.1122, step time: 0.1013\n",
      "120/223, train_loss: 0.1019, step time: 0.1034\n",
      "121/223, train_loss: 0.1121, step time: 0.1150\n",
      "122/223, train_loss: 0.1065, step time: 0.1017\n",
      "123/223, train_loss: 0.1033, step time: 0.1024\n",
      "124/223, train_loss: 0.1099, step time: 0.1062\n",
      "125/223, train_loss: 0.1094, step time: 0.1133\n",
      "126/223, train_loss: 0.1111, step time: 0.1115\n",
      "127/223, train_loss: 0.1075, step time: 0.1015\n",
      "128/223, train_loss: 0.1112, step time: 0.1050\n",
      "129/223, train_loss: 0.1167, step time: 0.1542\n",
      "130/223, train_loss: 0.1130, step time: 0.1299\n",
      "131/223, train_loss: 0.1065, step time: 0.1011\n",
      "132/223, train_loss: 0.1114, step time: 0.1001\n",
      "133/223, train_loss: 0.1037, step time: 0.1117\n",
      "134/223, train_loss: 0.1078, step time: 0.1032\n",
      "135/223, train_loss: 0.1134, step time: 0.1006\n",
      "136/223, train_loss: 0.1036, step time: 0.1028\n",
      "137/223, train_loss: 0.1177, step time: 0.1133\n",
      "138/223, train_loss: 0.1130, step time: 0.1155\n",
      "139/223, train_loss: 0.1210, step time: 0.1711\n",
      "140/223, train_loss: 0.1011, step time: 0.1099\n",
      "141/223, train_loss: 0.1084, step time: 0.1062\n",
      "142/223, train_loss: 0.1059, step time: 0.1175\n",
      "143/223, train_loss: 0.1102, step time: 0.1132\n",
      "144/223, train_loss: 0.1035, step time: 0.1131\n",
      "145/223, train_loss: 0.1051, step time: 0.1178\n",
      "146/223, train_loss: 0.1090, step time: 0.1058\n",
      "147/223, train_loss: 0.1268, step time: 0.1045\n",
      "148/223, train_loss: 0.1063, step time: 0.1006\n",
      "149/223, train_loss: 0.1081, step time: 0.1451\n",
      "150/223, train_loss: 0.1095, step time: 0.1042\n",
      "151/223, train_loss: 0.1052, step time: 0.1017\n",
      "152/223, train_loss: 0.1125, step time: 0.1073\n",
      "153/223, train_loss: 0.1045, step time: 0.1006\n",
      "154/223, train_loss: 0.1186, step time: 0.1008\n",
      "155/223, train_loss: 0.1029, step time: 0.1006\n",
      "156/223, train_loss: 0.1194, step time: 0.1018\n",
      "157/223, train_loss: 0.1209, step time: 0.1188\n",
      "158/223, train_loss: 0.1045, step time: 0.1105\n",
      "159/223, train_loss: 0.1042, step time: 0.1227\n",
      "160/223, train_loss: 0.1046, step time: 0.1137\n",
      "161/223, train_loss: 0.1308, step time: 0.1014\n",
      "162/223, train_loss: 0.1036, step time: 0.1075\n",
      "163/223, train_loss: 0.1121, step time: 0.1125\n",
      "164/223, train_loss: 0.1147, step time: 0.0997\n",
      "165/223, train_loss: 0.1060, step time: 0.1012\n",
      "166/223, train_loss: 0.0942, step time: 0.1241\n",
      "167/223, train_loss: 0.1125, step time: 0.1198\n",
      "168/223, train_loss: 0.0951, step time: 0.1015\n",
      "169/223, train_loss: 0.1119, step time: 0.1120\n",
      "170/223, train_loss: 0.1021, step time: 0.1045\n",
      "171/223, train_loss: 0.1129, step time: 0.1008\n",
      "172/223, train_loss: 0.1000, step time: 0.1034\n",
      "173/223, train_loss: 0.1081, step time: 0.1009\n",
      "174/223, train_loss: 0.1145, step time: 0.1245\n",
      "175/223, train_loss: 0.1104, step time: 0.1181\n",
      "176/223, train_loss: 0.0989, step time: 0.1006\n",
      "177/223, train_loss: 0.1001, step time: 0.1109\n",
      "178/223, train_loss: 0.1020, step time: 0.1132\n",
      "179/223, train_loss: 0.0938, step time: 0.1192\n",
      "180/223, train_loss: 0.1157, step time: 0.1315\n",
      "181/223, train_loss: 0.0998, step time: 0.1125\n",
      "182/223, train_loss: 0.1126, step time: 0.1436\n",
      "183/223, train_loss: 0.1034, step time: 0.1006\n",
      "184/223, train_loss: 0.1038, step time: 0.1004\n",
      "185/223, train_loss: 0.1050, step time: 0.1194\n",
      "186/223, train_loss: 0.1176, step time: 0.1013\n",
      "187/223, train_loss: 0.1252, step time: 0.1071\n",
      "188/223, train_loss: 0.1130, step time: 0.1061\n",
      "189/223, train_loss: 0.1161, step time: 0.1225\n",
      "190/223, train_loss: 0.1009, step time: 0.1152\n",
      "191/223, train_loss: 0.1115, step time: 0.1006\n",
      "192/223, train_loss: 0.1111, step time: 0.1007\n",
      "193/223, train_loss: 0.1066, step time: 0.1313\n",
      "194/223, train_loss: 0.1143, step time: 0.1004\n",
      "195/223, train_loss: 0.1225, step time: 0.1013\n",
      "196/223, train_loss: 0.1100, step time: 0.1036\n",
      "197/223, train_loss: 0.0990, step time: 0.1063\n",
      "198/223, train_loss: 0.1151, step time: 0.1060\n",
      "199/223, train_loss: 0.1153, step time: 0.1026\n",
      "200/223, train_loss: 0.1095, step time: 0.1006\n",
      "201/223, train_loss: 0.1059, step time: 0.1199\n",
      "202/223, train_loss: 0.1054, step time: 0.1142\n",
      "203/223, train_loss: 0.1139, step time: 0.1024\n",
      "204/223, train_loss: 0.1023, step time: 0.1115\n",
      "205/223, train_loss: 0.1018, step time: 0.1192\n",
      "206/223, train_loss: 0.1074, step time: 0.1088\n",
      "207/223, train_loss: 0.1087, step time: 0.1262\n",
      "208/223, train_loss: 0.1118, step time: 0.1104\n",
      "209/223, train_loss: 0.1193, step time: 0.1119\n",
      "210/223, train_loss: 0.1071, step time: 0.1261\n",
      "211/223, train_loss: 0.1047, step time: 0.1047\n",
      "212/223, train_loss: 0.1101, step time: 0.1039\n",
      "213/223, train_loss: 0.1033, step time: 0.1142\n",
      "214/223, train_loss: 0.1012, step time: 0.1087\n",
      "215/223, train_loss: 0.1077, step time: 0.1257\n",
      "216/223, train_loss: 0.1156, step time: 0.1010\n",
      "217/223, train_loss: 0.1070, step time: 0.1071\n",
      "218/223, train_loss: 0.0968, step time: 0.1094\n",
      "219/223, train_loss: 0.0973, step time: 0.1014\n",
      "220/223, train_loss: 0.1075, step time: 0.1018\n",
      "221/223, train_loss: 0.1250, step time: 0.0999\n",
      "222/223, train_loss: 0.1072, step time: 0.1007\n",
      "223/223, train_loss: 0.1113, step time: 0.1019\n",
      "epoch 123 average loss: 0.1107\n",
      "time consuming of epoch 123 is: 89.5640\n",
      "----------\n",
      "epoch 124/300\n",
      "1/223, train_loss: 0.1126, step time: 0.1014\n",
      "2/223, train_loss: 0.1107, step time: 0.1042\n",
      "3/223, train_loss: 0.1215, step time: 0.1138\n",
      "4/223, train_loss: 0.1060, step time: 0.1006\n",
      "5/223, train_loss: 0.1018, step time: 0.1215\n",
      "6/223, train_loss: 0.1113, step time: 0.1103\n",
      "7/223, train_loss: 0.1097, step time: 0.1003\n",
      "8/223, train_loss: 0.1173, step time: 0.1003\n",
      "9/223, train_loss: 0.0992, step time: 0.1065\n",
      "10/223, train_loss: 0.1046, step time: 0.1330\n",
      "11/223, train_loss: 0.0968, step time: 0.1005\n",
      "12/223, train_loss: 0.0979, step time: 0.1002\n",
      "13/223, train_loss: 0.1178, step time: 0.1148\n",
      "14/223, train_loss: 0.0992, step time: 0.1003\n",
      "15/223, train_loss: 0.1096, step time: 0.0993\n",
      "16/223, train_loss: 0.1030, step time: 0.0998\n",
      "17/223, train_loss: 0.1044, step time: 0.1067\n",
      "18/223, train_loss: 0.1276, step time: 0.1104\n",
      "19/223, train_loss: 0.1025, step time: 0.1216\n",
      "20/223, train_loss: 0.1104, step time: 0.1046\n",
      "21/223, train_loss: 0.0968, step time: 0.1000\n",
      "22/223, train_loss: 0.1084, step time: 0.1112\n",
      "23/223, train_loss: 0.1157, step time: 0.1003\n",
      "24/223, train_loss: 0.1092, step time: 0.1110\n",
      "25/223, train_loss: 0.1184, step time: 0.1094\n",
      "26/223, train_loss: 0.1123, step time: 0.0987\n",
      "27/223, train_loss: 0.1077, step time: 0.0994\n",
      "28/223, train_loss: 0.1125, step time: 0.0991\n",
      "29/223, train_loss: 0.1111, step time: 0.1221\n",
      "30/223, train_loss: 0.1094, step time: 0.1078\n",
      "31/223, train_loss: 0.1022, step time: 0.1118\n",
      "32/223, train_loss: 0.1101, step time: 0.1020\n",
      "33/223, train_loss: 0.1156, step time: 0.1117\n",
      "34/223, train_loss: 0.1104, step time: 0.1175\n",
      "35/223, train_loss: 0.1122, step time: 0.1132\n",
      "36/223, train_loss: 0.1120, step time: 0.1130\n",
      "37/223, train_loss: 0.1087, step time: 0.1049\n",
      "38/223, train_loss: 0.1058, step time: 0.1360\n",
      "39/223, train_loss: 0.1142, step time: 0.1214\n",
      "40/223, train_loss: 0.1171, step time: 0.1030\n",
      "41/223, train_loss: 0.1159, step time: 0.1082\n",
      "42/223, train_loss: 0.1009, step time: 0.1066\n",
      "43/223, train_loss: 0.1073, step time: 0.0993\n",
      "44/223, train_loss: 0.1035, step time: 0.1123\n",
      "45/223, train_loss: 0.1084, step time: 0.1396\n",
      "46/223, train_loss: 0.1091, step time: 0.1016\n",
      "47/223, train_loss: 0.1095, step time: 0.1000\n",
      "48/223, train_loss: 0.1140, step time: 0.1119\n",
      "49/223, train_loss: 0.1064, step time: 0.1091\n",
      "50/223, train_loss: 0.1045, step time: 0.1141\n",
      "51/223, train_loss: 0.1068, step time: 0.1124\n",
      "52/223, train_loss: 0.1123, step time: 0.1055\n",
      "53/223, train_loss: 0.1018, step time: 0.1110\n",
      "54/223, train_loss: 0.1028, step time: 0.1097\n",
      "55/223, train_loss: 0.0978, step time: 0.1530\n",
      "56/223, train_loss: 0.1009, step time: 0.0995\n",
      "57/223, train_loss: 0.1215, step time: 0.1110\n",
      "58/223, train_loss: 0.1176, step time: 0.1182\n",
      "59/223, train_loss: 0.1050, step time: 0.1169\n",
      "60/223, train_loss: 0.1157, step time: 0.1183\n",
      "61/223, train_loss: 0.1138, step time: 0.1577\n",
      "62/223, train_loss: 0.1146, step time: 0.1336\n",
      "63/223, train_loss: 0.1001, step time: 0.1226\n",
      "64/223, train_loss: 0.1062, step time: 0.1209\n",
      "65/223, train_loss: 0.1073, step time: 0.1045\n",
      "66/223, train_loss: 0.1047, step time: 0.1001\n",
      "67/223, train_loss: 0.0999, step time: 0.1319\n",
      "68/223, train_loss: 0.1068, step time: 0.1020\n",
      "69/223, train_loss: 0.1033, step time: 0.1207\n",
      "70/223, train_loss: 0.1002, step time: 0.1006\n",
      "71/223, train_loss: 0.1069, step time: 0.1262\n",
      "72/223, train_loss: 0.1173, step time: 0.1124\n",
      "73/223, train_loss: 0.1055, step time: 0.1034\n",
      "74/223, train_loss: 0.1037, step time: 0.1000\n",
      "75/223, train_loss: 0.1127, step time: 0.1000\n",
      "76/223, train_loss: 0.1110, step time: 0.1346\n",
      "77/223, train_loss: 0.1044, step time: 0.1157\n",
      "78/223, train_loss: 0.0969, step time: 0.1115\n",
      "79/223, train_loss: 0.1089, step time: 0.1253\n",
      "80/223, train_loss: 0.0981, step time: 0.1196\n",
      "81/223, train_loss: 0.1113, step time: 0.1061\n",
      "82/223, train_loss: 0.1105, step time: 0.1090\n",
      "83/223, train_loss: 0.1114, step time: 0.1000\n",
      "84/223, train_loss: 0.0991, step time: 0.0999\n",
      "85/223, train_loss: 0.1310, step time: 0.1102\n",
      "86/223, train_loss: 0.1011, step time: 0.1408\n",
      "87/223, train_loss: 0.1002, step time: 0.1340\n",
      "88/223, train_loss: 0.1187, step time: 0.1016\n",
      "89/223, train_loss: 0.1025, step time: 0.1150\n",
      "90/223, train_loss: 0.1102, step time: 0.1580\n",
      "91/223, train_loss: 0.1229, step time: 0.1001\n",
      "92/223, train_loss: 0.1169, step time: 0.1016\n",
      "93/223, train_loss: 0.1075, step time: 0.1233\n",
      "94/223, train_loss: 0.1088, step time: 0.1139\n",
      "95/223, train_loss: 0.1121, step time: 0.1132\n",
      "96/223, train_loss: 0.1101, step time: 0.0989\n",
      "97/223, train_loss: 0.3100, step time: 0.1046\n",
      "98/223, train_loss: 0.1158, step time: 0.1000\n",
      "99/223, train_loss: 0.1012, step time: 0.0993\n",
      "100/223, train_loss: 0.1294, step time: 0.1071\n",
      "101/223, train_loss: 0.1174, step time: 0.1189\n",
      "102/223, train_loss: 0.1011, step time: 0.1001\n",
      "103/223, train_loss: 0.1123, step time: 0.1013\n",
      "104/223, train_loss: 0.1113, step time: 0.1108\n",
      "105/223, train_loss: 0.1201, step time: 0.1129\n",
      "106/223, train_loss: 0.1021, step time: 0.0998\n",
      "107/223, train_loss: 0.1110, step time: 0.1068\n",
      "108/223, train_loss: 0.1218, step time: 0.1066\n",
      "109/223, train_loss: 0.0965, step time: 0.1066\n",
      "110/223, train_loss: 0.1010, step time: 0.1546\n",
      "111/223, train_loss: 0.1024, step time: 0.1150\n",
      "112/223, train_loss: 0.1166, step time: 0.1007\n",
      "113/223, train_loss: 0.1085, step time: 0.1036\n",
      "114/223, train_loss: 0.1138, step time: 0.1209\n",
      "115/223, train_loss: 0.1014, step time: 0.0996\n",
      "116/223, train_loss: 0.1081, step time: 0.0999\n",
      "117/223, train_loss: 0.1208, step time: 0.1084\n",
      "118/223, train_loss: 0.1239, step time: 0.0988\n",
      "119/223, train_loss: 0.1012, step time: 0.0999\n",
      "120/223, train_loss: 0.1029, step time: 0.1565\n",
      "121/223, train_loss: 0.1280, step time: 0.1245\n",
      "122/223, train_loss: 0.1201, step time: 0.1000\n",
      "123/223, train_loss: 0.1128, step time: 0.0984\n",
      "124/223, train_loss: 0.1188, step time: 0.0989\n",
      "125/223, train_loss: 0.1231, step time: 0.1202\n",
      "126/223, train_loss: 0.1165, step time: 0.1094\n",
      "127/223, train_loss: 0.1075, step time: 0.1104\n",
      "128/223, train_loss: 0.1195, step time: 0.1034\n",
      "129/223, train_loss: 0.0974, step time: 0.1219\n",
      "130/223, train_loss: 0.0987, step time: 0.1142\n",
      "131/223, train_loss: 0.1114, step time: 0.1061\n",
      "132/223, train_loss: 0.1001, step time: 0.1258\n",
      "133/223, train_loss: 0.1047, step time: 0.1458\n",
      "134/223, train_loss: 0.1133, step time: 0.1028\n",
      "135/223, train_loss: 0.1009, step time: 0.1181\n",
      "136/223, train_loss: 0.1083, step time: 0.1137\n",
      "137/223, train_loss: 0.1023, step time: 0.1116\n",
      "138/223, train_loss: 0.1056, step time: 0.1723\n",
      "139/223, train_loss: 0.1152, step time: 0.1186\n",
      "140/223, train_loss: 0.1085, step time: 0.1081\n",
      "141/223, train_loss: 0.1003, step time: 0.1165\n",
      "142/223, train_loss: 0.1260, step time: 0.1015\n",
      "143/223, train_loss: 0.0996, step time: 0.1141\n",
      "144/223, train_loss: 0.1199, step time: 0.0997\n",
      "145/223, train_loss: 0.1182, step time: 0.1059\n",
      "146/223, train_loss: 0.1040, step time: 0.1100\n",
      "147/223, train_loss: 0.1141, step time: 0.1125\n",
      "148/223, train_loss: 0.0987, step time: 0.1058\n",
      "149/223, train_loss: 0.1017, step time: 0.1087\n",
      "150/223, train_loss: 0.1134, step time: 0.1163\n",
      "151/223, train_loss: 0.1147, step time: 0.1207\n",
      "152/223, train_loss: 0.1153, step time: 0.1009\n",
      "153/223, train_loss: 0.1110, step time: 0.1298\n",
      "154/223, train_loss: 0.1149, step time: 0.1061\n",
      "155/223, train_loss: 0.1053, step time: 0.1614\n",
      "156/223, train_loss: 0.1088, step time: 0.1012\n",
      "157/223, train_loss: 0.0978, step time: 0.1260\n",
      "158/223, train_loss: 0.1045, step time: 0.1270\n",
      "159/223, train_loss: 0.1111, step time: 0.1125\n",
      "160/223, train_loss: 0.1089, step time: 0.1011\n",
      "161/223, train_loss: 0.1092, step time: 0.1015\n",
      "162/223, train_loss: 0.1084, step time: 0.1016\n",
      "163/223, train_loss: 0.1132, step time: 0.1020\n",
      "164/223, train_loss: 0.1085, step time: 0.1127\n",
      "165/223, train_loss: 0.1091, step time: 0.1251\n",
      "166/223, train_loss: 0.1144, step time: 0.1076\n",
      "167/223, train_loss: 0.1160, step time: 0.1161\n",
      "168/223, train_loss: 0.1051, step time: 0.1010\n",
      "169/223, train_loss: 0.1159, step time: 0.1193\n",
      "170/223, train_loss: 0.1085, step time: 0.1094\n",
      "171/223, train_loss: 0.0983, step time: 0.1006\n",
      "172/223, train_loss: 0.1046, step time: 0.1007\n",
      "173/223, train_loss: 0.1172, step time: 0.1160\n",
      "174/223, train_loss: 0.1083, step time: 0.1124\n",
      "175/223, train_loss: 0.1104, step time: 0.1077\n",
      "176/223, train_loss: 0.1117, step time: 0.1159\n",
      "177/223, train_loss: 0.0997, step time: 0.1128\n",
      "178/223, train_loss: 0.1084, step time: 0.0999\n",
      "179/223, train_loss: 0.1036, step time: 0.1007\n",
      "180/223, train_loss: 0.1254, step time: 0.1242\n",
      "181/223, train_loss: 0.1099, step time: 0.0998\n",
      "182/223, train_loss: 0.1188, step time: 0.1041\n",
      "183/223, train_loss: 0.1192, step time: 0.1440\n",
      "184/223, train_loss: 0.1145, step time: 0.0997\n",
      "185/223, train_loss: 0.1009, step time: 0.0998\n",
      "186/223, train_loss: 0.1180, step time: 0.1006\n",
      "187/223, train_loss: 0.1039, step time: 0.1005\n",
      "188/223, train_loss: 0.1085, step time: 0.1036\n",
      "189/223, train_loss: 0.1061, step time: 0.1018\n",
      "190/223, train_loss: 0.0989, step time: 0.1526\n",
      "191/223, train_loss: 0.1036, step time: 0.1335\n",
      "192/223, train_loss: 0.1018, step time: 0.1016\n",
      "193/223, train_loss: 0.1113, step time: 0.1007\n",
      "194/223, train_loss: 0.1002, step time: 0.1002\n",
      "195/223, train_loss: 0.1077, step time: 0.1004\n",
      "196/223, train_loss: 0.1036, step time: 0.1059\n",
      "197/223, train_loss: 0.1035, step time: 0.1091\n",
      "198/223, train_loss: 0.1071, step time: 0.1147\n",
      "199/223, train_loss: 0.1029, step time: 0.1258\n",
      "200/223, train_loss: 0.1193, step time: 0.1125\n",
      "201/223, train_loss: 0.1054, step time: 0.1001\n",
      "202/223, train_loss: 0.1010, step time: 0.1017\n",
      "203/223, train_loss: 0.1112, step time: 0.1154\n",
      "204/223, train_loss: 0.1088, step time: 0.1114\n",
      "205/223, train_loss: 0.1127, step time: 0.1555\n",
      "206/223, train_loss: 0.1122, step time: 0.1163\n",
      "207/223, train_loss: 0.1115, step time: 0.1107\n",
      "208/223, train_loss: 0.1281, step time: 0.1125\n",
      "209/223, train_loss: 0.1284, step time: 0.1073\n",
      "210/223, train_loss: 0.1110, step time: 0.1008\n",
      "211/223, train_loss: 0.1105, step time: 0.1451\n",
      "212/223, train_loss: 0.1141, step time: 0.1012\n",
      "213/223, train_loss: 0.1233, step time: 0.1129\n",
      "214/223, train_loss: 0.1125, step time: 0.1005\n",
      "215/223, train_loss: 0.1099, step time: 0.1372\n",
      "216/223, train_loss: 0.1014, step time: 0.1012\n",
      "217/223, train_loss: 0.1025, step time: 0.1007\n",
      "218/223, train_loss: 0.1040, step time: 0.1011\n",
      "219/223, train_loss: 0.1008, step time: 0.1169\n",
      "220/223, train_loss: 0.1215, step time: 0.1106\n",
      "221/223, train_loss: 0.1178, step time: 0.1003\n",
      "222/223, train_loss: 0.1082, step time: 0.0996\n",
      "223/223, train_loss: 0.1017, step time: 0.1002\n",
      "epoch 124 average loss: 0.1103\n",
      "time consuming of epoch 124 is: 88.1981\n",
      "----------\n",
      "epoch 125/300\n",
      "1/223, train_loss: 0.1064, step time: 0.1031\n",
      "2/223, train_loss: 0.1160, step time: 0.1000\n",
      "3/223, train_loss: 0.1096, step time: 0.1000\n",
      "4/223, train_loss: 0.0993, step time: 0.1030\n",
      "5/223, train_loss: 0.0980, step time: 0.1227\n",
      "6/223, train_loss: 0.0987, step time: 0.1051\n",
      "7/223, train_loss: 0.1075, step time: 0.1019\n",
      "8/223, train_loss: 0.1030, step time: 0.1004\n",
      "9/223, train_loss: 0.1098, step time: 0.1177\n",
      "10/223, train_loss: 0.1080, step time: 0.1252\n",
      "11/223, train_loss: 0.1164, step time: 0.1392\n",
      "12/223, train_loss: 0.1096, step time: 0.1041\n",
      "13/223, train_loss: 0.1009, step time: 0.1433\n",
      "14/223, train_loss: 0.1133, step time: 0.1081\n",
      "15/223, train_loss: 0.1033, step time: 0.1007\n",
      "16/223, train_loss: 0.0975, step time: 0.1021\n",
      "17/223, train_loss: 0.1135, step time: 0.1331\n",
      "18/223, train_loss: 0.1145, step time: 0.1004\n",
      "19/223, train_loss: 0.1107, step time: 0.1070\n",
      "20/223, train_loss: 0.1150, step time: 0.1008\n",
      "21/223, train_loss: 0.0979, step time: 0.1183\n",
      "22/223, train_loss: 0.1071, step time: 0.1006\n",
      "23/223, train_loss: 0.1209, step time: 0.1180\n",
      "24/223, train_loss: 0.1125, step time: 0.1002\n",
      "25/223, train_loss: 0.1111, step time: 0.1003\n",
      "26/223, train_loss: 0.0986, step time: 0.1001\n",
      "27/223, train_loss: 0.1058, step time: 0.1006\n",
      "28/223, train_loss: 0.1178, step time: 0.1020\n",
      "29/223, train_loss: 0.1169, step time: 0.1063\n",
      "30/223, train_loss: 0.1133, step time: 0.1088\n",
      "31/223, train_loss: 0.1012, step time: 0.1222\n",
      "32/223, train_loss: 0.1017, step time: 0.1090\n",
      "33/223, train_loss: 0.1021, step time: 0.1211\n",
      "34/223, train_loss: 0.1081, step time: 0.1261\n",
      "35/223, train_loss: 0.1056, step time: 0.1142\n",
      "36/223, train_loss: 0.0972, step time: 0.1024\n",
      "37/223, train_loss: 0.0976, step time: 0.1130\n",
      "38/223, train_loss: 0.1040, step time: 0.1078\n",
      "39/223, train_loss: 0.1088, step time: 0.1393\n",
      "40/223, train_loss: 0.1133, step time: 0.1476\n",
      "41/223, train_loss: 0.1026, step time: 0.1190\n",
      "42/223, train_loss: 0.1013, step time: 0.1233\n",
      "43/223, train_loss: 0.1247, step time: 0.1006\n",
      "44/223, train_loss: 0.1102, step time: 0.1001\n",
      "45/223, train_loss: 0.1060, step time: 0.1090\n",
      "46/223, train_loss: 0.1260, step time: 0.1002\n",
      "47/223, train_loss: 0.1085, step time: 0.1002\n",
      "48/223, train_loss: 0.1159, step time: 0.1134\n",
      "49/223, train_loss: 0.1128, step time: 0.1120\n",
      "50/223, train_loss: 0.0991, step time: 0.1198\n",
      "51/223, train_loss: 0.1157, step time: 0.1313\n",
      "52/223, train_loss: 0.0983, step time: 0.1096\n",
      "53/223, train_loss: 0.1256, step time: 0.1048\n",
      "54/223, train_loss: 0.1125, step time: 0.1021\n",
      "55/223, train_loss: 0.1083, step time: 0.1181\n",
      "56/223, train_loss: 0.1106, step time: 0.1157\n",
      "57/223, train_loss: 0.1003, step time: 0.1139\n",
      "58/223, train_loss: 0.1033, step time: 0.1141\n",
      "59/223, train_loss: 0.1038, step time: 0.1029\n",
      "60/223, train_loss: 0.1173, step time: 0.1128\n",
      "61/223, train_loss: 0.1006, step time: 0.1059\n",
      "62/223, train_loss: 0.1059, step time: 0.1183\n",
      "63/223, train_loss: 0.1114, step time: 0.1065\n",
      "64/223, train_loss: 0.1127, step time: 0.1161\n",
      "65/223, train_loss: 0.1001, step time: 0.1130\n",
      "66/223, train_loss: 0.1068, step time: 0.1173\n",
      "67/223, train_loss: 0.0987, step time: 0.1148\n",
      "68/223, train_loss: 0.1084, step time: 0.1139\n",
      "69/223, train_loss: 0.1143, step time: 0.1010\n",
      "70/223, train_loss: 0.1097, step time: 0.1111\n",
      "71/223, train_loss: 0.1146, step time: 0.1139\n",
      "72/223, train_loss: 0.1006, step time: 0.1128\n",
      "73/223, train_loss: 0.1078, step time: 0.1222\n",
      "74/223, train_loss: 0.1021, step time: 0.1132\n",
      "75/223, train_loss: 0.0978, step time: 0.1132\n",
      "76/223, train_loss: 0.1041, step time: 0.0998\n",
      "77/223, train_loss: 0.1260, step time: 0.1157\n",
      "78/223, train_loss: 0.1076, step time: 0.0999\n",
      "79/223, train_loss: 0.1000, step time: 0.1264\n",
      "80/223, train_loss: 0.1007, step time: 0.1136\n",
      "81/223, train_loss: 0.3160, step time: 0.1185\n",
      "82/223, train_loss: 0.1141, step time: 0.1125\n",
      "83/223, train_loss: 0.1272, step time: 0.1194\n",
      "84/223, train_loss: 0.1086, step time: 0.1096\n",
      "85/223, train_loss: 0.1046, step time: 0.1208\n",
      "86/223, train_loss: 0.1101, step time: 0.1075\n",
      "87/223, train_loss: 0.1102, step time: 0.0998\n",
      "88/223, train_loss: 0.1096, step time: 0.1023\n",
      "89/223, train_loss: 0.0969, step time: 0.1306\n",
      "90/223, train_loss: 0.1178, step time: 0.1168\n",
      "91/223, train_loss: 0.1088, step time: 0.1087\n",
      "92/223, train_loss: 0.1143, step time: 0.1170\n",
      "93/223, train_loss: 0.1061, step time: 0.0994\n",
      "94/223, train_loss: 0.1162, step time: 0.1103\n",
      "95/223, train_loss: 0.1140, step time: 0.0998\n",
      "96/223, train_loss: 0.1230, step time: 0.1087\n",
      "97/223, train_loss: 0.1205, step time: 0.1112\n",
      "98/223, train_loss: 0.0992, step time: 0.1111\n",
      "99/223, train_loss: 0.1084, step time: 0.1008\n",
      "100/223, train_loss: 0.1159, step time: 0.1287\n",
      "101/223, train_loss: 0.1082, step time: 0.1243\n",
      "102/223, train_loss: 0.1132, step time: 0.1109\n",
      "103/223, train_loss: 0.0999, step time: 0.1124\n",
      "104/223, train_loss: 0.1156, step time: 0.1016\n",
      "105/223, train_loss: 0.1029, step time: 0.1178\n",
      "106/223, train_loss: 0.1048, step time: 0.0999\n",
      "107/223, train_loss: 0.1161, step time: 0.1014\n",
      "108/223, train_loss: 0.1027, step time: 0.1013\n",
      "109/223, train_loss: 0.1190, step time: 0.1045\n",
      "110/223, train_loss: 0.1071, step time: 0.1154\n",
      "111/223, train_loss: 0.1201, step time: 0.1147\n",
      "112/223, train_loss: 0.1119, step time: 0.1005\n",
      "113/223, train_loss: 0.1067, step time: 0.1205\n",
      "114/223, train_loss: 0.1138, step time: 0.1074\n",
      "115/223, train_loss: 0.1241, step time: 0.0996\n",
      "116/223, train_loss: 0.0958, step time: 0.1150\n",
      "117/223, train_loss: 0.1073, step time: 0.1013\n",
      "118/223, train_loss: 0.1106, step time: 0.1045\n",
      "119/223, train_loss: 0.1091, step time: 0.1005\n",
      "120/223, train_loss: 0.1098, step time: 0.1112\n",
      "121/223, train_loss: 0.1077, step time: 0.1121\n",
      "122/223, train_loss: 0.1134, step time: 0.1061\n",
      "123/223, train_loss: 0.1143, step time: 0.1125\n",
      "124/223, train_loss: 0.1051, step time: 0.1128\n",
      "125/223, train_loss: 0.1003, step time: 0.1508\n",
      "126/223, train_loss: 0.1093, step time: 0.1036\n",
      "127/223, train_loss: 0.1047, step time: 0.1199\n",
      "128/223, train_loss: 0.1136, step time: 0.1005\n",
      "129/223, train_loss: 0.1031, step time: 0.1001\n",
      "130/223, train_loss: 0.1098, step time: 0.1019\n",
      "131/223, train_loss: 0.1029, step time: 0.1005\n",
      "132/223, train_loss: 0.1143, step time: 0.1238\n",
      "133/223, train_loss: 0.1121, step time: 0.1005\n",
      "134/223, train_loss: 0.1089, step time: 0.1123\n",
      "135/223, train_loss: 0.1129, step time: 0.1000\n",
      "136/223, train_loss: 0.1089, step time: 0.1006\n",
      "137/223, train_loss: 0.1186, step time: 0.1074\n",
      "138/223, train_loss: 0.1057, step time: 0.1051\n",
      "139/223, train_loss: 0.0977, step time: 0.1010\n",
      "140/223, train_loss: 0.0977, step time: 0.1008\n",
      "141/223, train_loss: 0.0966, step time: 0.1102\n",
      "142/223, train_loss: 0.1082, step time: 0.1112\n",
      "143/223, train_loss: 0.1122, step time: 0.1297\n",
      "144/223, train_loss: 0.1073, step time: 0.1020\n",
      "145/223, train_loss: 0.1070, step time: 0.1108\n",
      "146/223, train_loss: 0.1066, step time: 0.1177\n",
      "147/223, train_loss: 0.0997, step time: 0.1147\n",
      "148/223, train_loss: 0.1098, step time: 0.1232\n",
      "149/223, train_loss: 0.1038, step time: 0.1043\n",
      "150/223, train_loss: 0.1068, step time: 0.1616\n",
      "151/223, train_loss: 0.1312, step time: 0.1207\n",
      "152/223, train_loss: 0.1201, step time: 0.1156\n",
      "153/223, train_loss: 0.1100, step time: 0.0998\n",
      "154/223, train_loss: 0.1158, step time: 0.1104\n",
      "155/223, train_loss: 0.1064, step time: 0.1314\n",
      "156/223, train_loss: 0.1094, step time: 0.1052\n",
      "157/223, train_loss: 0.1270, step time: 0.1102\n",
      "158/223, train_loss: 0.1104, step time: 0.1048\n",
      "159/223, train_loss: 0.0988, step time: 0.1266\n",
      "160/223, train_loss: 0.1032, step time: 0.1093\n",
      "161/223, train_loss: 0.1044, step time: 0.1014\n",
      "162/223, train_loss: 0.1057, step time: 0.1074\n",
      "163/223, train_loss: 0.1153, step time: 0.1055\n",
      "164/223, train_loss: 0.1159, step time: 0.1077\n",
      "165/223, train_loss: 0.1074, step time: 0.1008\n",
      "166/223, train_loss: 0.1027, step time: 0.1059\n",
      "167/223, train_loss: 0.1032, step time: 0.1101\n",
      "168/223, train_loss: 0.1096, step time: 0.1174\n",
      "169/223, train_loss: 0.1029, step time: 0.1239\n",
      "170/223, train_loss: 0.1205, step time: 0.1074\n",
      "171/223, train_loss: 0.1066, step time: 0.1001\n",
      "172/223, train_loss: 0.1188, step time: 0.1016\n",
      "173/223, train_loss: 0.1142, step time: 0.1126\n",
      "174/223, train_loss: 0.1172, step time: 0.1017\n",
      "175/223, train_loss: 0.1035, step time: 0.1031\n",
      "176/223, train_loss: 0.0983, step time: 0.1180\n",
      "177/223, train_loss: 0.1067, step time: 0.1193\n",
      "178/223, train_loss: 0.1088, step time: 0.1177\n",
      "179/223, train_loss: 0.1096, step time: 0.1275\n",
      "180/223, train_loss: 0.1053, step time: 0.1125\n",
      "181/223, train_loss: 0.1026, step time: 0.1003\n",
      "182/223, train_loss: 0.1141, step time: 0.1123\n",
      "183/223, train_loss: 0.1136, step time: 0.1001\n",
      "184/223, train_loss: 0.1041, step time: 0.1184\n",
      "185/223, train_loss: 0.1108, step time: 0.1143\n",
      "186/223, train_loss: 0.1114, step time: 0.0995\n",
      "187/223, train_loss: 0.1048, step time: 0.1089\n",
      "188/223, train_loss: 0.0979, step time: 0.1323\n",
      "189/223, train_loss: 0.1089, step time: 0.1121\n",
      "190/223, train_loss: 0.1155, step time: 0.1170\n",
      "191/223, train_loss: 0.1110, step time: 0.1159\n",
      "192/223, train_loss: 0.1217, step time: 0.1066\n",
      "193/223, train_loss: 0.1094, step time: 0.1051\n",
      "194/223, train_loss: 0.0987, step time: 0.1011\n",
      "195/223, train_loss: 0.1114, step time: 0.1067\n",
      "196/223, train_loss: 0.1175, step time: 0.1286\n",
      "197/223, train_loss: 0.0960, step time: 0.1078\n",
      "198/223, train_loss: 0.0941, step time: 0.1040\n",
      "199/223, train_loss: 0.1176, step time: 0.1220\n",
      "200/223, train_loss: 0.1031, step time: 0.1179\n",
      "201/223, train_loss: 0.1132, step time: 0.1067\n",
      "202/223, train_loss: 0.1032, step time: 0.1053\n",
      "203/223, train_loss: 0.1145, step time: 0.1188\n",
      "204/223, train_loss: 0.1137, step time: 0.1124\n",
      "205/223, train_loss: 0.1147, step time: 0.1007\n",
      "206/223, train_loss: 0.1145, step time: 0.1092\n",
      "207/223, train_loss: 0.1098, step time: 0.1007\n",
      "208/223, train_loss: 0.1097, step time: 0.1094\n",
      "209/223, train_loss: 0.1026, step time: 0.1012\n",
      "210/223, train_loss: 0.1075, step time: 0.1102\n",
      "211/223, train_loss: 0.1099, step time: 0.1197\n",
      "212/223, train_loss: 0.0958, step time: 0.1048\n",
      "213/223, train_loss: 0.1103, step time: 0.1000\n",
      "214/223, train_loss: 0.1069, step time: 0.1418\n",
      "215/223, train_loss: 0.1092, step time: 0.1087\n",
      "216/223, train_loss: 0.1234, step time: 0.1200\n",
      "217/223, train_loss: 0.1104, step time: 0.1002\n",
      "218/223, train_loss: 0.1096, step time: 0.0996\n",
      "219/223, train_loss: 0.1033, step time: 0.0991\n",
      "220/223, train_loss: 0.0988, step time: 0.1004\n",
      "221/223, train_loss: 0.1131, step time: 0.1017\n",
      "222/223, train_loss: 0.1174, step time: 0.0990\n",
      "223/223, train_loss: 0.1063, step time: 0.0988\n",
      "epoch 125 average loss: 0.1098\n",
      "saved new best metric model\n",
      "current epoch: 125 current mean dice: 0.8551 tc: 0.9188 wt: 0.8651 et: 0.7813\n",
      "best mean dice: 0.8551 at epoch: 125\n",
      "time consuming of epoch 125 is: 91.5904\n",
      "----------\n",
      "epoch 126/300\n",
      "1/223, train_loss: 0.0973, step time: 0.1081\n",
      "2/223, train_loss: 0.1035, step time: 0.1161\n",
      "3/223, train_loss: 0.1086, step time: 0.1015\n",
      "4/223, train_loss: 0.0985, step time: 0.1201\n",
      "5/223, train_loss: 0.1117, step time: 0.1503\n",
      "6/223, train_loss: 0.1053, step time: 0.1179\n",
      "7/223, train_loss: 0.1007, step time: 0.1021\n",
      "8/223, train_loss: 0.1120, step time: 0.1111\n",
      "9/223, train_loss: 0.1132, step time: 0.1129\n",
      "10/223, train_loss: 0.1063, step time: 0.1151\n",
      "11/223, train_loss: 0.1187, step time: 0.1083\n",
      "12/223, train_loss: 0.1036, step time: 0.1257\n",
      "13/223, train_loss: 0.1044, step time: 0.0995\n",
      "14/223, train_loss: 0.1068, step time: 0.0991\n",
      "15/223, train_loss: 0.1099, step time: 0.1138\n",
      "16/223, train_loss: 0.0976, step time: 0.0997\n",
      "17/223, train_loss: 0.1115, step time: 0.0997\n",
      "18/223, train_loss: 0.1082, step time: 0.1423\n",
      "19/223, train_loss: 0.1066, step time: 0.1044\n",
      "20/223, train_loss: 0.1061, step time: 0.1006\n",
      "21/223, train_loss: 0.1120, step time: 0.1048\n",
      "22/223, train_loss: 0.0977, step time: 0.1004\n",
      "23/223, train_loss: 0.0988, step time: 0.1460\n",
      "24/223, train_loss: 0.0999, step time: 0.1007\n",
      "25/223, train_loss: 0.1029, step time: 0.1015\n",
      "26/223, train_loss: 0.1047, step time: 0.1206\n",
      "27/223, train_loss: 0.1292, step time: 0.1023\n",
      "28/223, train_loss: 0.1157, step time: 0.1088\n",
      "29/223, train_loss: 0.1071, step time: 0.1082\n",
      "30/223, train_loss: 0.1081, step time: 0.1000\n",
      "31/223, train_loss: 0.1240, step time: 0.1146\n",
      "32/223, train_loss: 0.1049, step time: 0.1048\n",
      "33/223, train_loss: 0.1135, step time: 0.1114\n",
      "34/223, train_loss: 0.1012, step time: 0.1002\n",
      "35/223, train_loss: 0.1131, step time: 0.1186\n",
      "36/223, train_loss: 0.1108, step time: 0.1023\n",
      "37/223, train_loss: 0.1089, step time: 0.1058\n",
      "38/223, train_loss: 0.1018, step time: 0.0999\n",
      "39/223, train_loss: 0.1062, step time: 0.1011\n",
      "40/223, train_loss: 0.1079, step time: 0.1173\n",
      "41/223, train_loss: 0.1234, step time: 0.1011\n",
      "42/223, train_loss: 0.1147, step time: 0.1015\n",
      "43/223, train_loss: 0.1050, step time: 0.1064\n",
      "44/223, train_loss: 0.1073, step time: 0.1027\n",
      "45/223, train_loss: 0.0948, step time: 0.1159\n",
      "46/223, train_loss: 0.1151, step time: 0.1201\n",
      "47/223, train_loss: 0.1124, step time: 0.1010\n",
      "48/223, train_loss: 0.1091, step time: 0.1009\n",
      "49/223, train_loss: 0.1124, step time: 0.1153\n",
      "50/223, train_loss: 0.0983, step time: 0.1009\n",
      "51/223, train_loss: 0.1203, step time: 0.1003\n",
      "52/223, train_loss: 0.1080, step time: 0.1012\n",
      "53/223, train_loss: 0.1107, step time: 0.1438\n",
      "54/223, train_loss: 0.1063, step time: 0.1229\n",
      "55/223, train_loss: 0.1040, step time: 0.1218\n",
      "56/223, train_loss: 0.1055, step time: 0.1080\n",
      "57/223, train_loss: 0.1052, step time: 0.1251\n",
      "58/223, train_loss: 0.1006, step time: 0.1119\n",
      "59/223, train_loss: 0.1161, step time: 0.1428\n",
      "60/223, train_loss: 0.0986, step time: 0.1008\n",
      "61/223, train_loss: 0.1078, step time: 0.1625\n",
      "62/223, train_loss: 0.1024, step time: 0.1136\n",
      "63/223, train_loss: 0.1103, step time: 0.1004\n",
      "64/223, train_loss: 0.1112, step time: 0.1003\n",
      "65/223, train_loss: 0.0985, step time: 0.1237\n",
      "66/223, train_loss: 0.1039, step time: 0.1095\n",
      "67/223, train_loss: 0.0999, step time: 0.1132\n",
      "68/223, train_loss: 0.1110, step time: 0.1028\n",
      "69/223, train_loss: 0.1170, step time: 0.1140\n",
      "70/223, train_loss: 0.1013, step time: 0.1002\n",
      "71/223, train_loss: 0.1054, step time: 0.1007\n",
      "72/223, train_loss: 0.1011, step time: 0.1004\n",
      "73/223, train_loss: 0.1185, step time: 0.1248\n",
      "74/223, train_loss: 0.1052, step time: 0.1181\n",
      "75/223, train_loss: 0.1032, step time: 0.1007\n",
      "76/223, train_loss: 0.1145, step time: 0.1003\n",
      "77/223, train_loss: 0.1172, step time: 0.1247\n",
      "78/223, train_loss: 0.1154, step time: 0.1103\n",
      "79/223, train_loss: 0.1086, step time: 0.1019\n",
      "80/223, train_loss: 0.1106, step time: 0.1017\n",
      "81/223, train_loss: 0.1096, step time: 0.1362\n",
      "82/223, train_loss: 0.1212, step time: 0.0995\n",
      "83/223, train_loss: 0.1008, step time: 0.1000\n",
      "84/223, train_loss: 0.1071, step time: 0.1018\n",
      "85/223, train_loss: 0.1032, step time: 0.1016\n",
      "86/223, train_loss: 0.1211, step time: 0.1007\n",
      "87/223, train_loss: 0.1046, step time: 0.1004\n",
      "88/223, train_loss: 0.1087, step time: 0.0999\n",
      "89/223, train_loss: 0.1258, step time: 0.1015\n",
      "90/223, train_loss: 0.0990, step time: 0.1002\n",
      "91/223, train_loss: 0.1093, step time: 0.1004\n",
      "92/223, train_loss: 0.1157, step time: 0.0992\n",
      "93/223, train_loss: 0.1057, step time: 0.1108\n",
      "94/223, train_loss: 0.1055, step time: 0.1004\n",
      "95/223, train_loss: 0.1086, step time: 0.0998\n",
      "96/223, train_loss: 0.1058, step time: 0.0997\n",
      "97/223, train_loss: 0.0946, step time: 0.1064\n",
      "98/223, train_loss: 0.1086, step time: 0.0994\n",
      "99/223, train_loss: 0.1113, step time: 0.0997\n",
      "100/223, train_loss: 0.1003, step time: 0.0998\n",
      "101/223, train_loss: 0.1127, step time: 0.1009\n",
      "102/223, train_loss: 0.1257, step time: 0.0994\n",
      "103/223, train_loss: 0.1152, step time: 0.1003\n",
      "104/223, train_loss: 0.1090, step time: 0.1005\n",
      "105/223, train_loss: 0.1006, step time: 0.1003\n",
      "106/223, train_loss: 0.1088, step time: 0.1112\n",
      "107/223, train_loss: 0.1088, step time: 0.1202\n",
      "108/223, train_loss: 0.1017, step time: 0.1145\n",
      "109/223, train_loss: 0.1090, step time: 0.1056\n",
      "110/223, train_loss: 0.1067, step time: 0.1147\n",
      "111/223, train_loss: 0.1096, step time: 0.1208\n",
      "112/223, train_loss: 0.1023, step time: 0.1006\n",
      "113/223, train_loss: 0.1182, step time: 0.1238\n",
      "114/223, train_loss: 0.1117, step time: 0.1155\n",
      "115/223, train_loss: 0.1175, step time: 0.1071\n",
      "116/223, train_loss: 0.1189, step time: 0.1029\n",
      "117/223, train_loss: 0.1103, step time: 0.1327\n",
      "118/223, train_loss: 0.1145, step time: 0.1096\n",
      "119/223, train_loss: 0.1160, step time: 0.0998\n",
      "120/223, train_loss: 0.1034, step time: 0.0992\n",
      "121/223, train_loss: 0.1052, step time: 0.0991\n",
      "122/223, train_loss: 0.1088, step time: 0.0988\n",
      "123/223, train_loss: 0.0976, step time: 0.0982\n",
      "124/223, train_loss: 0.1101, step time: 0.0987\n",
      "125/223, train_loss: 0.1185, step time: 0.1004\n",
      "126/223, train_loss: 0.1152, step time: 0.1138\n",
      "127/223, train_loss: 0.1101, step time: 0.1146\n",
      "128/223, train_loss: 0.1196, step time: 0.1003\n",
      "129/223, train_loss: 0.1235, step time: 0.1005\n",
      "130/223, train_loss: 0.1138, step time: 0.1075\n",
      "131/223, train_loss: 0.1083, step time: 0.1041\n",
      "132/223, train_loss: 0.1130, step time: 0.1178\n",
      "133/223, train_loss: 0.1120, step time: 0.1014\n",
      "134/223, train_loss: 0.1156, step time: 0.1314\n",
      "135/223, train_loss: 0.0978, step time: 0.1488\n",
      "136/223, train_loss: 0.1152, step time: 0.1005\n",
      "137/223, train_loss: 0.1115, step time: 0.1014\n",
      "138/223, train_loss: 0.0997, step time: 0.1187\n",
      "139/223, train_loss: 0.1229, step time: 0.1005\n",
      "140/223, train_loss: 0.1062, step time: 0.1023\n",
      "141/223, train_loss: 0.1150, step time: 0.1107\n",
      "142/223, train_loss: 0.1028, step time: 0.1059\n",
      "143/223, train_loss: 0.1104, step time: 0.1000\n",
      "144/223, train_loss: 0.1088, step time: 0.1148\n",
      "145/223, train_loss: 0.0992, step time: 0.1005\n",
      "146/223, train_loss: 0.1192, step time: 0.1008\n",
      "147/223, train_loss: 0.1048, step time: 0.1000\n",
      "148/223, train_loss: 0.1123, step time: 0.1000\n",
      "149/223, train_loss: 0.1166, step time: 0.1025\n",
      "150/223, train_loss: 0.1082, step time: 0.1044\n",
      "151/223, train_loss: 0.1075, step time: 0.1108\n",
      "152/223, train_loss: 0.1137, step time: 0.1114\n",
      "153/223, train_loss: 0.1252, step time: 0.1149\n",
      "154/223, train_loss: 0.1095, step time: 0.1177\n",
      "155/223, train_loss: 0.1218, step time: 0.1053\n",
      "156/223, train_loss: 0.1137, step time: 0.1126\n",
      "157/223, train_loss: 0.1014, step time: 0.1005\n",
      "158/223, train_loss: 0.1188, step time: 0.1050\n",
      "159/223, train_loss: 0.1116, step time: 0.1000\n",
      "160/223, train_loss: 0.1107, step time: 0.1226\n",
      "161/223, train_loss: 0.1058, step time: 0.1324\n",
      "162/223, train_loss: 0.1098, step time: 0.1005\n",
      "163/223, train_loss: 0.1192, step time: 0.1109\n",
      "164/223, train_loss: 0.3132, step time: 0.1179\n",
      "165/223, train_loss: 0.1163, step time: 0.1264\n",
      "166/223, train_loss: 0.1171, step time: 0.1107\n",
      "167/223, train_loss: 0.1022, step time: 0.1092\n",
      "168/223, train_loss: 0.1031, step time: 0.1161\n",
      "169/223, train_loss: 0.1059, step time: 0.1275\n",
      "170/223, train_loss: 0.0993, step time: 0.1198\n",
      "171/223, train_loss: 0.1127, step time: 0.1327\n",
      "172/223, train_loss: 0.1119, step time: 0.1172\n",
      "173/223, train_loss: 0.1066, step time: 0.1158\n",
      "174/223, train_loss: 0.1086, step time: 0.1048\n",
      "175/223, train_loss: 0.1034, step time: 0.1224\n",
      "176/223, train_loss: 0.0983, step time: 0.1020\n",
      "177/223, train_loss: 0.1032, step time: 0.1567\n",
      "178/223, train_loss: 0.1045, step time: 0.1091\n",
      "179/223, train_loss: 0.1190, step time: 0.1229\n",
      "180/223, train_loss: 0.1064, step time: 0.1304\n",
      "181/223, train_loss: 0.1094, step time: 0.1242\n",
      "182/223, train_loss: 0.1106, step time: 0.1155\n",
      "183/223, train_loss: 0.1199, step time: 0.1154\n",
      "184/223, train_loss: 0.1081, step time: 0.1127\n",
      "185/223, train_loss: 0.1139, step time: 0.1244\n",
      "186/223, train_loss: 0.1159, step time: 0.1361\n",
      "187/223, train_loss: 0.1135, step time: 0.1055\n",
      "188/223, train_loss: 0.1028, step time: 0.1186\n",
      "189/223, train_loss: 0.1187, step time: 0.0994\n",
      "190/223, train_loss: 0.1035, step time: 0.1151\n",
      "191/223, train_loss: 0.1054, step time: 0.1111\n",
      "192/223, train_loss: 0.1251, step time: 0.1009\n",
      "193/223, train_loss: 0.1080, step time: 0.1015\n",
      "194/223, train_loss: 0.1119, step time: 0.1016\n",
      "195/223, train_loss: 0.1087, step time: 0.1132\n",
      "196/223, train_loss: 0.1137, step time: 0.1008\n",
      "197/223, train_loss: 0.1072, step time: 0.1010\n",
      "198/223, train_loss: 0.1222, step time: 0.1112\n",
      "199/223, train_loss: 0.1139, step time: 0.1103\n",
      "200/223, train_loss: 0.1023, step time: 0.1261\n",
      "201/223, train_loss: 0.1128, step time: 0.1114\n",
      "202/223, train_loss: 0.1076, step time: 0.1247\n",
      "203/223, train_loss: 0.1176, step time: 0.1051\n",
      "204/223, train_loss: 0.1091, step time: 0.0991\n",
      "205/223, train_loss: 0.1159, step time: 0.1099\n",
      "206/223, train_loss: 0.0997, step time: 0.1010\n",
      "207/223, train_loss: 0.1135, step time: 0.1365\n",
      "208/223, train_loss: 0.1068, step time: 0.1091\n",
      "209/223, train_loss: 0.1193, step time: 0.1189\n",
      "210/223, train_loss: 0.1011, step time: 0.1301\n",
      "211/223, train_loss: 0.1036, step time: 0.1114\n",
      "212/223, train_loss: 0.1069, step time: 0.1131\n",
      "213/223, train_loss: 0.1261, step time: 0.1003\n",
      "214/223, train_loss: 0.1094, step time: 0.1144\n",
      "215/223, train_loss: 0.1100, step time: 0.1039\n",
      "216/223, train_loss: 0.1038, step time: 0.1178\n",
      "217/223, train_loss: 0.1162, step time: 0.1001\n",
      "218/223, train_loss: 0.0954, step time: 0.0996\n",
      "219/223, train_loss: 0.1114, step time: 0.0993\n",
      "220/223, train_loss: 0.1024, step time: 0.1004\n",
      "221/223, train_loss: 0.1180, step time: 0.0999\n",
      "222/223, train_loss: 0.0965, step time: 0.0994\n",
      "223/223, train_loss: 0.1036, step time: 0.0998\n",
      "epoch 126 average loss: 0.1102\n",
      "time consuming of epoch 126 is: 92.6580\n",
      "----------\n",
      "epoch 127/300\n",
      "1/223, train_loss: 0.1129, step time: 0.1065\n",
      "2/223, train_loss: 0.1136, step time: 0.1065\n",
      "3/223, train_loss: 0.1052, step time: 0.1173\n",
      "4/223, train_loss: 0.1172, step time: 0.1179\n",
      "5/223, train_loss: 0.1121, step time: 0.1094\n",
      "6/223, train_loss: 0.1074, step time: 0.1272\n",
      "7/223, train_loss: 0.1142, step time: 0.1235\n",
      "8/223, train_loss: 0.1181, step time: 0.1070\n",
      "9/223, train_loss: 0.1189, step time: 0.1021\n",
      "10/223, train_loss: 0.1060, step time: 0.1000\n",
      "11/223, train_loss: 0.1081, step time: 0.0998\n",
      "12/223, train_loss: 0.1215, step time: 0.0998\n",
      "13/223, train_loss: 0.0925, step time: 0.1138\n",
      "14/223, train_loss: 0.1162, step time: 0.0999\n",
      "15/223, train_loss: 0.1048, step time: 0.0999\n",
      "16/223, train_loss: 0.1199, step time: 0.0998\n",
      "17/223, train_loss: 0.1051, step time: 0.1149\n",
      "18/223, train_loss: 0.1135, step time: 0.1054\n",
      "19/223, train_loss: 0.1121, step time: 0.1000\n",
      "20/223, train_loss: 0.1138, step time: 0.1171\n",
      "21/223, train_loss: 0.1121, step time: 0.1032\n",
      "22/223, train_loss: 0.1061, step time: 0.1053\n",
      "23/223, train_loss: 0.1049, step time: 0.1000\n",
      "24/223, train_loss: 0.1071, step time: 0.1004\n",
      "25/223, train_loss: 0.1095, step time: 0.1254\n",
      "26/223, train_loss: 0.0995, step time: 0.1082\n",
      "27/223, train_loss: 0.1153, step time: 0.0997\n",
      "28/223, train_loss: 0.1135, step time: 0.1005\n",
      "29/223, train_loss: 0.0972, step time: 0.1105\n",
      "30/223, train_loss: 0.1112, step time: 0.1138\n",
      "31/223, train_loss: 0.1085, step time: 0.1248\n",
      "32/223, train_loss: 0.1081, step time: 0.1249\n",
      "33/223, train_loss: 0.1042, step time: 0.1187\n",
      "34/223, train_loss: 0.0964, step time: 0.1034\n",
      "35/223, train_loss: 0.0966, step time: 0.1154\n",
      "36/223, train_loss: 0.0978, step time: 0.1197\n",
      "37/223, train_loss: 0.1199, step time: 0.1216\n",
      "38/223, train_loss: 0.1153, step time: 0.1156\n",
      "39/223, train_loss: 0.1181, step time: 0.1047\n",
      "40/223, train_loss: 0.1176, step time: 0.1098\n",
      "41/223, train_loss: 0.1198, step time: 0.1063\n",
      "42/223, train_loss: 0.1110, step time: 0.1143\n",
      "43/223, train_loss: 0.1110, step time: 0.1069\n",
      "44/223, train_loss: 0.1008, step time: 0.1019\n",
      "45/223, train_loss: 0.1033, step time: 0.1010\n",
      "46/223, train_loss: 0.1148, step time: 0.1067\n",
      "47/223, train_loss: 0.1134, step time: 0.1001\n",
      "48/223, train_loss: 0.1220, step time: 0.1218\n",
      "49/223, train_loss: 0.1137, step time: 0.1150\n",
      "50/223, train_loss: 0.1074, step time: 0.1025\n",
      "51/223, train_loss: 0.1080, step time: 0.1005\n",
      "52/223, train_loss: 0.0970, step time: 0.1005\n",
      "53/223, train_loss: 0.1049, step time: 0.1107\n",
      "54/223, train_loss: 0.1198, step time: 0.1087\n",
      "55/223, train_loss: 0.1103, step time: 0.1001\n",
      "56/223, train_loss: 0.1128, step time: 0.1003\n",
      "57/223, train_loss: 0.1103, step time: 0.1122\n",
      "58/223, train_loss: 0.1082, step time: 0.0996\n",
      "59/223, train_loss: 0.1055, step time: 0.1178\n",
      "60/223, train_loss: 0.1087, step time: 0.1010\n",
      "61/223, train_loss: 0.1159, step time: 0.1213\n",
      "62/223, train_loss: 0.1212, step time: 0.1000\n",
      "63/223, train_loss: 0.0953, step time: 0.1125\n",
      "64/223, train_loss: 0.1124, step time: 0.1140\n",
      "65/223, train_loss: 0.1118, step time: 0.1253\n",
      "66/223, train_loss: 0.1070, step time: 0.0997\n",
      "67/223, train_loss: 0.1143, step time: 0.1144\n",
      "68/223, train_loss: 0.1022, step time: 0.1011\n",
      "69/223, train_loss: 0.1147, step time: 0.1079\n",
      "70/223, train_loss: 0.1136, step time: 0.1127\n",
      "71/223, train_loss: 0.1092, step time: 0.1020\n",
      "72/223, train_loss: 0.1019, step time: 0.1143\n",
      "73/223, train_loss: 0.1020, step time: 0.1089\n",
      "74/223, train_loss: 0.1173, step time: 0.1078\n",
      "75/223, train_loss: 0.1203, step time: 0.1056\n",
      "76/223, train_loss: 0.1161, step time: 0.1008\n",
      "77/223, train_loss: 0.1028, step time: 0.1151\n",
      "78/223, train_loss: 0.1115, step time: 0.1124\n",
      "79/223, train_loss: 0.1071, step time: 0.1099\n",
      "80/223, train_loss: 0.1087, step time: 0.1001\n",
      "81/223, train_loss: 0.1052, step time: 0.1155\n",
      "82/223, train_loss: 0.1061, step time: 0.1018\n",
      "83/223, train_loss: 0.1071, step time: 0.0996\n",
      "84/223, train_loss: 0.0999, step time: 0.1217\n",
      "85/223, train_loss: 0.0967, step time: 0.1097\n",
      "86/223, train_loss: 0.1025, step time: 0.1123\n",
      "87/223, train_loss: 0.1079, step time: 0.1068\n",
      "88/223, train_loss: 0.1112, step time: 0.1144\n",
      "89/223, train_loss: 0.1186, step time: 0.1123\n",
      "90/223, train_loss: 0.3135, step time: 0.1218\n",
      "91/223, train_loss: 0.0971, step time: 0.1031\n",
      "92/223, train_loss: 0.1091, step time: 0.1144\n",
      "93/223, train_loss: 0.1057, step time: 0.1106\n",
      "94/223, train_loss: 0.0983, step time: 0.1132\n",
      "95/223, train_loss: 0.1123, step time: 0.1209\n",
      "96/223, train_loss: 0.0997, step time: 0.0996\n",
      "97/223, train_loss: 0.1118, step time: 0.1069\n",
      "98/223, train_loss: 0.1083, step time: 0.1069\n",
      "99/223, train_loss: 0.1015, step time: 0.1060\n",
      "100/223, train_loss: 0.0981, step time: 0.1173\n",
      "101/223, train_loss: 0.1208, step time: 0.1050\n",
      "102/223, train_loss: 0.1025, step time: 0.1082\n",
      "103/223, train_loss: 0.1227, step time: 0.1003\n",
      "104/223, train_loss: 0.1093, step time: 0.1094\n",
      "105/223, train_loss: 0.1028, step time: 0.1007\n",
      "106/223, train_loss: 0.1103, step time: 0.1174\n",
      "107/223, train_loss: 0.1159, step time: 0.1096\n",
      "108/223, train_loss: 0.1186, step time: 0.1005\n",
      "109/223, train_loss: 0.1148, step time: 0.1209\n",
      "110/223, train_loss: 0.1195, step time: 0.1226\n",
      "111/223, train_loss: 0.1298, step time: 0.1081\n",
      "112/223, train_loss: 0.1134, step time: 0.1025\n",
      "113/223, train_loss: 0.1184, step time: 0.1014\n",
      "114/223, train_loss: 0.0999, step time: 0.1359\n",
      "115/223, train_loss: 0.1143, step time: 0.1094\n",
      "116/223, train_loss: 0.1045, step time: 0.1055\n",
      "117/223, train_loss: 0.1092, step time: 0.1001\n",
      "118/223, train_loss: 0.1025, step time: 0.1026\n",
      "119/223, train_loss: 0.1035, step time: 0.1002\n",
      "120/223, train_loss: 0.1068, step time: 0.1013\n",
      "121/223, train_loss: 0.1049, step time: 0.1129\n",
      "122/223, train_loss: 0.1015, step time: 0.1014\n",
      "123/223, train_loss: 0.1158, step time: 0.1036\n",
      "124/223, train_loss: 0.1168, step time: 0.1160\n",
      "125/223, train_loss: 0.1008, step time: 0.1006\n",
      "126/223, train_loss: 0.1072, step time: 0.1006\n",
      "127/223, train_loss: 0.1035, step time: 0.1040\n",
      "128/223, train_loss: 0.1046, step time: 0.1152\n",
      "129/223, train_loss: 0.0977, step time: 0.1157\n",
      "130/223, train_loss: 0.1014, step time: 0.1209\n",
      "131/223, train_loss: 0.1073, step time: 0.1200\n",
      "132/223, train_loss: 0.1099, step time: 0.1043\n",
      "133/223, train_loss: 0.1050, step time: 0.1013\n",
      "134/223, train_loss: 0.1113, step time: 0.1000\n",
      "135/223, train_loss: 0.1096, step time: 0.1065\n",
      "136/223, train_loss: 0.1081, step time: 0.1007\n",
      "137/223, train_loss: 0.0984, step time: 0.1156\n",
      "138/223, train_loss: 0.1005, step time: 0.1388\n",
      "139/223, train_loss: 0.1110, step time: 0.1021\n",
      "140/223, train_loss: 0.1075, step time: 0.1047\n",
      "141/223, train_loss: 0.0970, step time: 0.1164\n",
      "142/223, train_loss: 0.1127, step time: 0.1030\n",
      "143/223, train_loss: 0.1129, step time: 0.0998\n",
      "144/223, train_loss: 0.1000, step time: 0.1062\n",
      "145/223, train_loss: 0.1113, step time: 0.1080\n",
      "146/223, train_loss: 0.1080, step time: 0.1132\n",
      "147/223, train_loss: 0.1174, step time: 0.1006\n",
      "148/223, train_loss: 0.1141, step time: 0.1005\n",
      "149/223, train_loss: 0.1010, step time: 0.1131\n",
      "150/223, train_loss: 0.1163, step time: 0.1006\n",
      "151/223, train_loss: 0.1074, step time: 0.1130\n",
      "152/223, train_loss: 0.1187, step time: 0.1144\n",
      "153/223, train_loss: 0.0953, step time: 0.1113\n",
      "154/223, train_loss: 0.1054, step time: 0.1003\n",
      "155/223, train_loss: 0.1161, step time: 0.1135\n",
      "156/223, train_loss: 0.1139, step time: 0.1109\n",
      "157/223, train_loss: 0.1059, step time: 0.1093\n",
      "158/223, train_loss: 0.1098, step time: 0.1144\n",
      "159/223, train_loss: 0.1067, step time: 0.1166\n",
      "160/223, train_loss: 0.0931, step time: 0.1220\n",
      "161/223, train_loss: 0.0982, step time: 0.1061\n",
      "162/223, train_loss: 0.1098, step time: 0.1282\n",
      "163/223, train_loss: 0.1129, step time: 0.1004\n",
      "164/223, train_loss: 0.1066, step time: 0.1003\n",
      "165/223, train_loss: 0.1049, step time: 0.1178\n",
      "166/223, train_loss: 0.1131, step time: 0.1146\n",
      "167/223, train_loss: 0.1007, step time: 0.1056\n",
      "168/223, train_loss: 0.1265, step time: 0.1004\n",
      "169/223, train_loss: 0.1194, step time: 0.1110\n",
      "170/223, train_loss: 0.1183, step time: 0.1004\n",
      "171/223, train_loss: 0.1015, step time: 0.1006\n",
      "172/223, train_loss: 0.1141, step time: 0.1002\n",
      "173/223, train_loss: 0.1052, step time: 0.1089\n",
      "174/223, train_loss: 0.1000, step time: 0.1272\n",
      "175/223, train_loss: 0.1107, step time: 0.1163\n",
      "176/223, train_loss: 0.1079, step time: 0.0999\n",
      "177/223, train_loss: 0.1214, step time: 0.1149\n",
      "178/223, train_loss: 0.1214, step time: 0.1000\n",
      "179/223, train_loss: 0.0989, step time: 0.0998\n",
      "180/223, train_loss: 0.1083, step time: 0.1009\n",
      "181/223, train_loss: 0.1025, step time: 0.1271\n",
      "182/223, train_loss: 0.1048, step time: 0.1103\n",
      "183/223, train_loss: 0.1103, step time: 0.1027\n",
      "184/223, train_loss: 0.1089, step time: 0.1104\n",
      "185/223, train_loss: 0.1211, step time: 0.1151\n",
      "186/223, train_loss: 0.1070, step time: 0.1005\n",
      "187/223, train_loss: 0.1289, step time: 0.1127\n",
      "188/223, train_loss: 0.1106, step time: 0.1077\n",
      "189/223, train_loss: 0.1012, step time: 0.1118\n",
      "190/223, train_loss: 0.1219, step time: 0.1227\n",
      "191/223, train_loss: 0.1064, step time: 0.1156\n",
      "192/223, train_loss: 0.1183, step time: 0.1235\n",
      "193/223, train_loss: 0.1015, step time: 0.1173\n",
      "194/223, train_loss: 0.1019, step time: 0.1012\n",
      "195/223, train_loss: 0.1097, step time: 0.1000\n",
      "196/223, train_loss: 0.1128, step time: 0.1117\n",
      "197/223, train_loss: 0.1039, step time: 0.1119\n",
      "198/223, train_loss: 0.1173, step time: 0.1044\n",
      "199/223, train_loss: 0.1057, step time: 0.1224\n",
      "200/223, train_loss: 0.1058, step time: 0.1007\n",
      "201/223, train_loss: 0.1079, step time: 0.0999\n",
      "202/223, train_loss: 0.1023, step time: 0.1044\n",
      "203/223, train_loss: 0.1082, step time: 0.1170\n",
      "204/223, train_loss: 0.1084, step time: 0.1172\n",
      "205/223, train_loss: 0.1147, step time: 0.1143\n",
      "206/223, train_loss: 0.0987, step time: 0.1358\n",
      "207/223, train_loss: 0.1129, step time: 0.1269\n",
      "208/223, train_loss: 0.1148, step time: 0.1029\n",
      "209/223, train_loss: 0.0996, step time: 0.1002\n",
      "210/223, train_loss: 0.1088, step time: 0.1083\n",
      "211/223, train_loss: 0.1063, step time: 0.1094\n",
      "212/223, train_loss: 0.1052, step time: 0.1244\n",
      "213/223, train_loss: 0.1102, step time: 0.1097\n",
      "214/223, train_loss: 0.1136, step time: 0.1018\n",
      "215/223, train_loss: 0.1072, step time: 0.1013\n",
      "216/223, train_loss: 0.1052, step time: 0.1009\n",
      "217/223, train_loss: 0.0973, step time: 0.1007\n",
      "218/223, train_loss: 0.1121, step time: 0.1018\n",
      "219/223, train_loss: 0.1043, step time: 0.0995\n",
      "220/223, train_loss: 0.1106, step time: 0.0986\n",
      "221/223, train_loss: 0.1013, step time: 0.1053\n",
      "222/223, train_loss: 0.1068, step time: 0.1040\n",
      "223/223, train_loss: 0.1168, step time: 0.1010\n",
      "epoch 127 average loss: 0.1099\n",
      "time consuming of epoch 127 is: 87.2534\n",
      "----------\n",
      "epoch 128/300\n",
      "1/223, train_loss: 0.1145, step time: 0.1047\n",
      "2/223, train_loss: 0.1099, step time: 0.0990\n",
      "3/223, train_loss: 0.1055, step time: 0.1037\n",
      "4/223, train_loss: 0.1180, step time: 0.1008\n",
      "5/223, train_loss: 0.1069, step time: 0.1299\n",
      "6/223, train_loss: 0.1002, step time: 0.1270\n",
      "7/223, train_loss: 0.1220, step time: 0.1175\n",
      "8/223, train_loss: 0.1028, step time: 0.1177\n",
      "9/223, train_loss: 0.1054, step time: 0.1068\n",
      "10/223, train_loss: 0.1064, step time: 0.1047\n",
      "11/223, train_loss: 0.1005, step time: 0.1018\n",
      "12/223, train_loss: 0.1137, step time: 0.1009\n",
      "13/223, train_loss: 0.0994, step time: 0.1307\n",
      "14/223, train_loss: 0.1099, step time: 0.1370\n",
      "15/223, train_loss: 0.1203, step time: 0.1007\n",
      "16/223, train_loss: 0.1116, step time: 0.1007\n",
      "17/223, train_loss: 0.1173, step time: 0.1060\n",
      "18/223, train_loss: 0.1107, step time: 0.1185\n",
      "19/223, train_loss: 0.1065, step time: 0.1065\n",
      "20/223, train_loss: 0.1117, step time: 0.1019\n",
      "21/223, train_loss: 0.1000, step time: 0.1336\n",
      "22/223, train_loss: 0.1048, step time: 0.1166\n",
      "23/223, train_loss: 0.1057, step time: 0.1133\n",
      "24/223, train_loss: 0.1196, step time: 0.1114\n",
      "25/223, train_loss: 0.1056, step time: 0.1035\n",
      "26/223, train_loss: 0.1127, step time: 0.1071\n",
      "27/223, train_loss: 0.1269, step time: 0.1207\n",
      "28/223, train_loss: 0.1321, step time: 0.1282\n",
      "29/223, train_loss: 0.1212, step time: 0.1075\n",
      "30/223, train_loss: 0.1086, step time: 0.1031\n",
      "31/223, train_loss: 0.1015, step time: 0.1188\n",
      "32/223, train_loss: 0.1188, step time: 0.1216\n",
      "33/223, train_loss: 0.1104, step time: 0.1003\n",
      "34/223, train_loss: 0.1129, step time: 0.1221\n",
      "35/223, train_loss: 0.1049, step time: 0.1178\n",
      "36/223, train_loss: 0.1091, step time: 0.1029\n",
      "37/223, train_loss: 0.1099, step time: 0.1052\n",
      "38/223, train_loss: 0.1060, step time: 0.1004\n",
      "39/223, train_loss: 0.1065, step time: 0.1004\n",
      "40/223, train_loss: 0.1042, step time: 0.1219\n",
      "41/223, train_loss: 0.1090, step time: 0.1118\n",
      "42/223, train_loss: 0.0997, step time: 0.1218\n",
      "43/223, train_loss: 0.1218, step time: 0.1006\n",
      "44/223, train_loss: 0.1082, step time: 0.1048\n",
      "45/223, train_loss: 0.1135, step time: 0.1006\n",
      "46/223, train_loss: 0.1285, step time: 0.1067\n",
      "47/223, train_loss: 0.1127, step time: 0.1303\n",
      "48/223, train_loss: 0.1067, step time: 0.0995\n",
      "49/223, train_loss: 0.1031, step time: 0.0997\n",
      "50/223, train_loss: 0.1052, step time: 0.2231\n",
      "51/223, train_loss: 0.1097, step time: 0.1006\n",
      "52/223, train_loss: 0.1019, step time: 0.0997\n",
      "53/223, train_loss: 0.1137, step time: 0.1011\n",
      "54/223, train_loss: 0.1136, step time: 0.1061\n",
      "55/223, train_loss: 0.1151, step time: 0.1116\n",
      "56/223, train_loss: 0.1063, step time: 0.1127\n",
      "57/223, train_loss: 0.1143, step time: 0.1134\n",
      "58/223, train_loss: 0.1029, step time: 0.1006\n",
      "59/223, train_loss: 0.0966, step time: 0.1048\n",
      "60/223, train_loss: 0.1158, step time: 0.1268\n",
      "61/223, train_loss: 0.1117, step time: 0.1046\n",
      "62/223, train_loss: 0.1203, step time: 0.1012\n",
      "63/223, train_loss: 0.1044, step time: 0.1108\n",
      "64/223, train_loss: 0.1052, step time: 0.1173\n",
      "65/223, train_loss: 0.1104, step time: 0.1005\n",
      "66/223, train_loss: 0.1207, step time: 0.1002\n",
      "67/223, train_loss: 0.1130, step time: 0.1111\n",
      "68/223, train_loss: 0.1054, step time: 0.1118\n",
      "69/223, train_loss: 0.1011, step time: 0.1004\n",
      "70/223, train_loss: 0.1137, step time: 0.1012\n",
      "71/223, train_loss: 0.1031, step time: 0.1003\n",
      "72/223, train_loss: 0.1148, step time: 0.1000\n",
      "73/223, train_loss: 0.0977, step time: 0.0998\n",
      "74/223, train_loss: 0.1114, step time: 0.0999\n",
      "75/223, train_loss: 0.1102, step time: 0.1068\n",
      "76/223, train_loss: 0.1076, step time: 0.1123\n",
      "77/223, train_loss: 0.1227, step time: 0.1117\n",
      "78/223, train_loss: 0.1146, step time: 0.1071\n",
      "79/223, train_loss: 0.1086, step time: 0.0996\n",
      "80/223, train_loss: 0.1086, step time: 0.0996\n",
      "81/223, train_loss: 0.1145, step time: 0.1254\n",
      "82/223, train_loss: 0.1109, step time: 0.1022\n",
      "83/223, train_loss: 0.1249, step time: 0.1170\n",
      "84/223, train_loss: 0.1177, step time: 0.1174\n",
      "85/223, train_loss: 0.0998, step time: 0.1044\n",
      "86/223, train_loss: 0.0959, step time: 0.1007\n",
      "87/223, train_loss: 0.1001, step time: 0.1000\n",
      "88/223, train_loss: 0.0953, step time: 0.1009\n",
      "89/223, train_loss: 0.1018, step time: 0.1030\n",
      "90/223, train_loss: 0.1390, step time: 0.1004\n",
      "91/223, train_loss: 0.1059, step time: 0.1017\n",
      "92/223, train_loss: 0.1102, step time: 0.1011\n",
      "93/223, train_loss: 0.1065, step time: 0.1289\n",
      "94/223, train_loss: 0.1109, step time: 0.1006\n",
      "95/223, train_loss: 0.1206, step time: 0.1002\n",
      "96/223, train_loss: 0.1141, step time: 0.1012\n",
      "97/223, train_loss: 0.0962, step time: 0.1161\n",
      "98/223, train_loss: 0.1066, step time: 0.1015\n",
      "99/223, train_loss: 0.1085, step time: 0.1006\n",
      "100/223, train_loss: 0.1033, step time: 0.1004\n",
      "101/223, train_loss: 0.1203, step time: 0.0995\n",
      "102/223, train_loss: 0.1014, step time: 0.1009\n",
      "103/223, train_loss: 0.1250, step time: 0.0998\n",
      "104/223, train_loss: 0.1186, step time: 0.1002\n",
      "105/223, train_loss: 0.1219, step time: 0.1000\n",
      "106/223, train_loss: 0.1263, step time: 0.1009\n",
      "107/223, train_loss: 0.0983, step time: 0.0998\n",
      "108/223, train_loss: 0.1030, step time: 0.0994\n",
      "109/223, train_loss: 0.1144, step time: 0.1000\n",
      "110/223, train_loss: 0.1084, step time: 0.1007\n",
      "111/223, train_loss: 0.1042, step time: 0.1010\n",
      "112/223, train_loss: 0.1082, step time: 0.0996\n",
      "113/223, train_loss: 0.1036, step time: 0.1000\n",
      "114/223, train_loss: 0.1018, step time: 0.1007\n",
      "115/223, train_loss: 0.1123, step time: 0.1004\n",
      "116/223, train_loss: 0.1043, step time: 0.1089\n",
      "117/223, train_loss: 0.1131, step time: 0.1007\n",
      "118/223, train_loss: 0.1100, step time: 0.1206\n",
      "119/223, train_loss: 0.0984, step time: 0.1282\n",
      "120/223, train_loss: 0.1206, step time: 0.0995\n",
      "121/223, train_loss: 0.1109, step time: 0.1114\n",
      "122/223, train_loss: 0.1184, step time: 0.1041\n",
      "123/223, train_loss: 0.1019, step time: 0.1003\n",
      "124/223, train_loss: 0.1031, step time: 0.1145\n",
      "125/223, train_loss: 0.1168, step time: 0.1222\n",
      "126/223, train_loss: 0.1053, step time: 0.1064\n",
      "127/223, train_loss: 0.0993, step time: 0.1002\n",
      "128/223, train_loss: 0.1080, step time: 0.1232\n",
      "129/223, train_loss: 0.1019, step time: 0.1170\n",
      "130/223, train_loss: 0.1123, step time: 0.1118\n",
      "131/223, train_loss: 0.1010, step time: 0.1002\n",
      "132/223, train_loss: 0.1022, step time: 0.1229\n",
      "133/223, train_loss: 0.1009, step time: 0.1104\n",
      "134/223, train_loss: 0.1251, step time: 0.1014\n",
      "135/223, train_loss: 0.1010, step time: 0.1066\n",
      "136/223, train_loss: 0.0941, step time: 0.1011\n",
      "137/223, train_loss: 0.1240, step time: 0.1004\n",
      "138/223, train_loss: 0.0994, step time: 0.1055\n",
      "139/223, train_loss: 0.1167, step time: 0.1103\n",
      "140/223, train_loss: 0.1111, step time: 0.1158\n",
      "141/223, train_loss: 0.1056, step time: 0.1018\n",
      "142/223, train_loss: 0.0988, step time: 0.1229\n",
      "143/223, train_loss: 0.1209, step time: 0.1205\n",
      "144/223, train_loss: 0.1097, step time: 0.1176\n",
      "145/223, train_loss: 0.1014, step time: 0.1158\n",
      "146/223, train_loss: 0.1076, step time: 0.1140\n",
      "147/223, train_loss: 0.0989, step time: 0.1094\n",
      "148/223, train_loss: 0.0947, step time: 0.1160\n",
      "149/223, train_loss: 0.1104, step time: 0.1241\n",
      "150/223, train_loss: 0.1080, step time: 0.1135\n",
      "151/223, train_loss: 0.1262, step time: 0.1192\n",
      "152/223, train_loss: 0.1062, step time: 0.1173\n",
      "153/223, train_loss: 0.1080, step time: 0.1208\n",
      "154/223, train_loss: 0.1125, step time: 0.1180\n",
      "155/223, train_loss: 0.1073, step time: 0.1111\n",
      "156/223, train_loss: 0.1194, step time: 0.1192\n",
      "157/223, train_loss: 0.1097, step time: 0.1032\n",
      "158/223, train_loss: 0.1160, step time: 0.1034\n",
      "159/223, train_loss: 0.1061, step time: 0.1274\n",
      "160/223, train_loss: 0.1066, step time: 0.1207\n",
      "161/223, train_loss: 0.1196, step time: 0.1297\n",
      "162/223, train_loss: 0.0962, step time: 0.1084\n",
      "163/223, train_loss: 0.1055, step time: 0.1330\n",
      "164/223, train_loss: 0.0978, step time: 0.1270\n",
      "165/223, train_loss: 0.1092, step time: 0.1043\n",
      "166/223, train_loss: 0.1093, step time: 0.0999\n",
      "167/223, train_loss: 0.1058, step time: 0.1160\n",
      "168/223, train_loss: 0.1047, step time: 0.1021\n",
      "169/223, train_loss: 0.1001, step time: 0.1040\n",
      "170/223, train_loss: 0.1126, step time: 0.1155\n",
      "171/223, train_loss: 0.1119, step time: 0.1354\n",
      "172/223, train_loss: 0.1213, step time: 0.1298\n",
      "173/223, train_loss: 0.1057, step time: 0.1556\n",
      "174/223, train_loss: 0.1185, step time: 0.1007\n",
      "175/223, train_loss: 0.1080, step time: 0.1049\n",
      "176/223, train_loss: 0.1181, step time: 0.1064\n",
      "177/223, train_loss: 0.1104, step time: 0.1098\n",
      "178/223, train_loss: 0.1188, step time: 0.1098\n",
      "179/223, train_loss: 0.1125, step time: 0.1005\n",
      "180/223, train_loss: 0.1033, step time: 0.1165\n",
      "181/223, train_loss: 0.1044, step time: 0.1009\n",
      "182/223, train_loss: 0.1061, step time: 0.0999\n",
      "183/223, train_loss: 0.1078, step time: 0.1000\n",
      "184/223, train_loss: 0.1144, step time: 0.0995\n",
      "185/223, train_loss: 0.1172, step time: 0.1002\n",
      "186/223, train_loss: 0.1107, step time: 0.0993\n",
      "187/223, train_loss: 0.1085, step time: 0.0995\n",
      "188/223, train_loss: 0.1020, step time: 0.0991\n",
      "189/223, train_loss: 0.0967, step time: 0.1002\n",
      "190/223, train_loss: 0.1176, step time: 0.0995\n",
      "191/223, train_loss: 0.1043, step time: 0.1002\n",
      "192/223, train_loss: 0.1024, step time: 0.0994\n",
      "193/223, train_loss: 0.1049, step time: 0.1059\n",
      "194/223, train_loss: 0.3075, step time: 0.1238\n",
      "195/223, train_loss: 0.1136, step time: 0.1138\n",
      "196/223, train_loss: 0.1006, step time: 0.1087\n",
      "197/223, train_loss: 0.1148, step time: 0.1724\n",
      "198/223, train_loss: 0.1065, step time: 0.1080\n",
      "199/223, train_loss: 0.1119, step time: 0.1007\n",
      "200/223, train_loss: 0.1024, step time: 0.1200\n",
      "201/223, train_loss: 0.1064, step time: 0.1291\n",
      "202/223, train_loss: 0.1060, step time: 0.1160\n",
      "203/223, train_loss: 0.1164, step time: 0.1203\n",
      "204/223, train_loss: 0.1173, step time: 0.1070\n",
      "205/223, train_loss: 0.1051, step time: 0.1170\n",
      "206/223, train_loss: 0.1124, step time: 0.1148\n",
      "207/223, train_loss: 0.0992, step time: 0.1164\n",
      "208/223, train_loss: 0.0991, step time: 0.1125\n",
      "209/223, train_loss: 0.1011, step time: 0.1172\n",
      "210/223, train_loss: 0.1102, step time: 0.1184\n",
      "211/223, train_loss: 0.1077, step time: 0.1198\n",
      "212/223, train_loss: 0.1067, step time: 0.1042\n",
      "213/223, train_loss: 0.1051, step time: 0.1041\n",
      "214/223, train_loss: 0.1144, step time: 0.1236\n",
      "215/223, train_loss: 0.1026, step time: 0.1111\n",
      "216/223, train_loss: 0.1062, step time: 0.1205\n",
      "217/223, train_loss: 0.1192, step time: 0.1133\n",
      "218/223, train_loss: 0.1037, step time: 0.0987\n",
      "219/223, train_loss: 0.1161, step time: 0.0989\n",
      "220/223, train_loss: 0.0983, step time: 0.0992\n",
      "221/223, train_loss: 0.1203, step time: 0.0990\n",
      "222/223, train_loss: 0.1096, step time: 0.0992\n",
      "223/223, train_loss: 0.1101, step time: 0.0998\n",
      "epoch 128 average loss: 0.1102\n",
      "time consuming of epoch 128 is: 92.1823\n",
      "----------\n",
      "epoch 129/300\n",
      "1/223, train_loss: 0.1091, step time: 0.1135\n",
      "2/223, train_loss: 0.1101, step time: 0.1004\n",
      "3/223, train_loss: 0.1031, step time: 0.1143\n",
      "4/223, train_loss: 0.1210, step time: 0.1023\n",
      "5/223, train_loss: 0.0955, step time: 0.1192\n",
      "6/223, train_loss: 0.1261, step time: 0.1049\n",
      "7/223, train_loss: 0.1072, step time: 0.1811\n",
      "8/223, train_loss: 0.1166, step time: 0.1394\n",
      "9/223, train_loss: 0.1022, step time: 0.1075\n",
      "10/223, train_loss: 0.1105, step time: 0.1099\n",
      "11/223, train_loss: 0.1046, step time: 0.1144\n",
      "12/223, train_loss: 0.1209, step time: 0.1201\n",
      "13/223, train_loss: 0.1092, step time: 0.1125\n",
      "14/223, train_loss: 0.1216, step time: 0.1019\n",
      "15/223, train_loss: 0.1110, step time: 0.1066\n",
      "16/223, train_loss: 0.1157, step time: 0.1065\n",
      "17/223, train_loss: 0.1147, step time: 0.1014\n",
      "18/223, train_loss: 0.1366, step time: 0.1019\n",
      "19/223, train_loss: 0.1166, step time: 0.1246\n",
      "20/223, train_loss: 0.0985, step time: 0.1219\n",
      "21/223, train_loss: 0.1046, step time: 0.1083\n",
      "22/223, train_loss: 0.1123, step time: 0.1036\n",
      "23/223, train_loss: 0.1020, step time: 0.1063\n",
      "24/223, train_loss: 0.0997, step time: 0.1076\n",
      "25/223, train_loss: 0.0950, step time: 0.1025\n",
      "26/223, train_loss: 0.1096, step time: 0.1112\n",
      "27/223, train_loss: 0.1099, step time: 0.1069\n",
      "28/223, train_loss: 0.1084, step time: 0.1017\n",
      "29/223, train_loss: 0.0957, step time: 0.1217\n",
      "30/223, train_loss: 0.1118, step time: 0.1184\n",
      "31/223, train_loss: 0.1098, step time: 0.1012\n",
      "32/223, train_loss: 0.1093, step time: 0.1011\n",
      "33/223, train_loss: 0.1075, step time: 0.1168\n",
      "34/223, train_loss: 0.1077, step time: 0.1115\n",
      "35/223, train_loss: 0.1090, step time: 0.1098\n",
      "36/223, train_loss: 0.1022, step time: 0.1167\n",
      "37/223, train_loss: 0.1051, step time: 0.1086\n",
      "38/223, train_loss: 0.0997, step time: 0.1037\n",
      "39/223, train_loss: 0.1027, step time: 0.1008\n",
      "40/223, train_loss: 0.1145, step time: 0.1014\n",
      "41/223, train_loss: 0.1216, step time: 0.1115\n",
      "42/223, train_loss: 0.1072, step time: 0.1077\n",
      "43/223, train_loss: 0.1130, step time: 0.1007\n",
      "44/223, train_loss: 0.1143, step time: 0.1010\n",
      "45/223, train_loss: 0.1027, step time: 0.1038\n",
      "46/223, train_loss: 0.1118, step time: 0.1218\n",
      "47/223, train_loss: 0.0975, step time: 0.1008\n",
      "48/223, train_loss: 0.1104, step time: 0.0994\n",
      "49/223, train_loss: 0.1086, step time: 0.1249\n",
      "50/223, train_loss: 0.1021, step time: 0.1140\n",
      "51/223, train_loss: 0.1083, step time: 0.1004\n",
      "52/223, train_loss: 0.1038, step time: 0.1009\n",
      "53/223, train_loss: 0.1131, step time: 0.0997\n",
      "54/223, train_loss: 0.1093, step time: 0.1006\n",
      "55/223, train_loss: 0.1046, step time: 0.1010\n",
      "56/223, train_loss: 0.1107, step time: 0.1078\n",
      "57/223, train_loss: 0.1096, step time: 0.1116\n",
      "58/223, train_loss: 0.1104, step time: 0.1006\n",
      "59/223, train_loss: 0.1202, step time: 0.1001\n",
      "60/223, train_loss: 0.0987, step time: 0.1323\n",
      "61/223, train_loss: 0.1109, step time: 0.1004\n",
      "62/223, train_loss: 0.1134, step time: 0.1112\n",
      "63/223, train_loss: 0.0982, step time: 0.0999\n",
      "64/223, train_loss: 0.1064, step time: 0.1007\n",
      "65/223, train_loss: 0.1096, step time: 0.1042\n",
      "66/223, train_loss: 0.3116, step time: 0.1098\n",
      "67/223, train_loss: 0.1049, step time: 0.1009\n",
      "68/223, train_loss: 0.1055, step time: 0.1055\n",
      "69/223, train_loss: 0.1116, step time: 0.1076\n",
      "70/223, train_loss: 0.1320, step time: 0.0996\n",
      "71/223, train_loss: 0.1121, step time: 0.1008\n",
      "72/223, train_loss: 0.1048, step time: 0.1003\n",
      "73/223, train_loss: 0.1034, step time: 0.1093\n",
      "74/223, train_loss: 0.1166, step time: 0.1073\n",
      "75/223, train_loss: 0.1057, step time: 0.1256\n",
      "76/223, train_loss: 0.1059, step time: 0.1172\n",
      "77/223, train_loss: 0.1114, step time: 0.1254\n",
      "78/223, train_loss: 0.1087, step time: 0.1004\n",
      "79/223, train_loss: 0.1003, step time: 0.1004\n",
      "80/223, train_loss: 0.1006, step time: 0.1000\n",
      "81/223, train_loss: 0.1025, step time: 0.1086\n",
      "82/223, train_loss: 0.1246, step time: 0.1000\n",
      "83/223, train_loss: 0.1203, step time: 0.1141\n",
      "84/223, train_loss: 0.1080, step time: 0.1128\n",
      "85/223, train_loss: 0.1210, step time: 0.1046\n",
      "86/223, train_loss: 0.0926, step time: 0.1000\n",
      "87/223, train_loss: 0.1063, step time: 0.0997\n",
      "88/223, train_loss: 0.0968, step time: 0.1010\n",
      "89/223, train_loss: 0.1114, step time: 0.1003\n",
      "90/223, train_loss: 0.1140, step time: 0.1004\n",
      "91/223, train_loss: 0.1128, step time: 0.1008\n",
      "92/223, train_loss: 0.0950, step time: 0.1007\n",
      "93/223, train_loss: 0.1134, step time: 0.1046\n",
      "94/223, train_loss: 0.1083, step time: 0.0999\n",
      "95/223, train_loss: 0.0932, step time: 0.1000\n",
      "96/223, train_loss: 0.1090, step time: 0.1004\n",
      "97/223, train_loss: 0.1095, step time: 0.1005\n",
      "98/223, train_loss: 0.1234, step time: 0.0995\n",
      "99/223, train_loss: 0.1041, step time: 0.0998\n",
      "100/223, train_loss: 0.0954, step time: 0.1007\n",
      "101/223, train_loss: 0.1023, step time: 0.1066\n",
      "102/223, train_loss: 0.1088, step time: 0.0996\n",
      "103/223, train_loss: 0.1110, step time: 0.0993\n",
      "104/223, train_loss: 0.1019, step time: 0.1003\n",
      "105/223, train_loss: 0.0952, step time: 0.1019\n",
      "106/223, train_loss: 0.1070, step time: 0.0994\n",
      "107/223, train_loss: 0.1110, step time: 0.0995\n",
      "108/223, train_loss: 0.1100, step time: 0.1003\n",
      "109/223, train_loss: 0.1066, step time: 0.1006\n",
      "110/223, train_loss: 0.1167, step time: 0.1007\n",
      "111/223, train_loss: 0.0992, step time: 0.0997\n",
      "112/223, train_loss: 0.1009, step time: 0.1002\n",
      "113/223, train_loss: 0.0957, step time: 0.1119\n",
      "114/223, train_loss: 0.0962, step time: 0.1001\n",
      "115/223, train_loss: 0.1158, step time: 0.0997\n",
      "116/223, train_loss: 0.1135, step time: 0.1380\n",
      "117/223, train_loss: 0.1021, step time: 0.1139\n",
      "118/223, train_loss: 0.1235, step time: 0.1103\n",
      "119/223, train_loss: 0.1037, step time: 0.1007\n",
      "120/223, train_loss: 0.1270, step time: 0.1144\n",
      "121/223, train_loss: 0.1099, step time: 0.1557\n",
      "122/223, train_loss: 0.1159, step time: 0.1128\n",
      "123/223, train_loss: 0.1137, step time: 0.1041\n",
      "124/223, train_loss: 0.1096, step time: 0.1012\n",
      "125/223, train_loss: 0.1090, step time: 0.1043\n",
      "126/223, train_loss: 0.1132, step time: 0.1146\n",
      "127/223, train_loss: 0.1132, step time: 0.1144\n",
      "128/223, train_loss: 0.1091, step time: 0.1100\n",
      "129/223, train_loss: 0.1153, step time: 0.1041\n",
      "130/223, train_loss: 0.1159, step time: 0.1177\n",
      "131/223, train_loss: 0.1251, step time: 0.1042\n",
      "132/223, train_loss: 0.1115, step time: 0.1068\n",
      "133/223, train_loss: 0.1164, step time: 0.1058\n",
      "134/223, train_loss: 0.1153, step time: 0.1126\n",
      "135/223, train_loss: 0.1049, step time: 0.1096\n",
      "136/223, train_loss: 0.1011, step time: 0.1127\n",
      "137/223, train_loss: 0.0998, step time: 0.1095\n",
      "138/223, train_loss: 0.1112, step time: 0.1417\n",
      "139/223, train_loss: 0.1074, step time: 0.0999\n",
      "140/223, train_loss: 0.0932, step time: 0.1140\n",
      "141/223, train_loss: 0.1015, step time: 0.0999\n",
      "142/223, train_loss: 0.1158, step time: 0.1010\n",
      "143/223, train_loss: 0.1137, step time: 0.1170\n",
      "144/223, train_loss: 0.1065, step time: 0.1203\n",
      "145/223, train_loss: 0.1106, step time: 0.1407\n",
      "146/223, train_loss: 0.1055, step time: 0.1003\n",
      "147/223, train_loss: 0.0992, step time: 0.1053\n",
      "148/223, train_loss: 0.1066, step time: 0.1000\n",
      "149/223, train_loss: 0.1170, step time: 0.1158\n",
      "150/223, train_loss: 0.1023, step time: 0.1307\n",
      "151/223, train_loss: 0.1120, step time: 0.1113\n",
      "152/223, train_loss: 0.1156, step time: 0.1124\n",
      "153/223, train_loss: 0.1187, step time: 0.1150\n",
      "154/223, train_loss: 0.1095, step time: 0.1316\n",
      "155/223, train_loss: 0.0993, step time: 0.1163\n",
      "156/223, train_loss: 0.1060, step time: 0.1137\n",
      "157/223, train_loss: 0.0993, step time: 0.1323\n",
      "158/223, train_loss: 0.1170, step time: 0.1180\n",
      "159/223, train_loss: 0.1150, step time: 0.1221\n",
      "160/223, train_loss: 0.1109, step time: 0.1025\n",
      "161/223, train_loss: 0.1133, step time: 0.1255\n",
      "162/223, train_loss: 0.1051, step time: 0.1090\n",
      "163/223, train_loss: 0.1092, step time: 0.1127\n",
      "164/223, train_loss: 0.0928, step time: 0.1427\n",
      "165/223, train_loss: 0.1208, step time: 0.1166\n",
      "166/223, train_loss: 0.1083, step time: 0.1003\n",
      "167/223, train_loss: 0.1167, step time: 0.1231\n",
      "168/223, train_loss: 0.1119, step time: 0.1033\n",
      "169/223, train_loss: 0.1041, step time: 0.1551\n",
      "170/223, train_loss: 0.1316, step time: 0.1002\n",
      "171/223, train_loss: 0.1176, step time: 0.1077\n",
      "172/223, train_loss: 0.1172, step time: 0.1186\n",
      "173/223, train_loss: 0.1110, step time: 0.1169\n",
      "174/223, train_loss: 0.1174, step time: 0.1249\n",
      "175/223, train_loss: 0.0966, step time: 0.1181\n",
      "176/223, train_loss: 0.1181, step time: 0.1128\n",
      "177/223, train_loss: 0.1109, step time: 0.1150\n",
      "178/223, train_loss: 0.1079, step time: 0.1243\n",
      "179/223, train_loss: 0.1135, step time: 0.1010\n",
      "180/223, train_loss: 0.1052, step time: 0.1120\n",
      "181/223, train_loss: 0.1230, step time: 0.1182\n",
      "182/223, train_loss: 0.1121, step time: 0.1183\n",
      "183/223, train_loss: 0.1019, step time: 0.1018\n",
      "184/223, train_loss: 0.1024, step time: 0.1151\n",
      "185/223, train_loss: 0.1160, step time: 0.1010\n",
      "186/223, train_loss: 0.1067, step time: 0.1029\n",
      "187/223, train_loss: 0.1106, step time: 0.1143\n",
      "188/223, train_loss: 0.1143, step time: 0.1248\n",
      "189/223, train_loss: 0.1087, step time: 0.1085\n",
      "190/223, train_loss: 0.1008, step time: 0.1181\n",
      "191/223, train_loss: 0.0996, step time: 0.1158\n",
      "192/223, train_loss: 0.1127, step time: 0.1074\n",
      "193/223, train_loss: 0.1033, step time: 0.1206\n",
      "194/223, train_loss: 0.1146, step time: 0.1113\n",
      "195/223, train_loss: 0.1046, step time: 0.1003\n",
      "196/223, train_loss: 0.1062, step time: 0.1150\n",
      "197/223, train_loss: 0.1058, step time: 0.1056\n",
      "198/223, train_loss: 0.1092, step time: 0.1059\n",
      "199/223, train_loss: 0.1064, step time: 0.1004\n",
      "200/223, train_loss: 0.1163, step time: 0.1019\n",
      "201/223, train_loss: 0.1019, step time: 0.1057\n",
      "202/223, train_loss: 0.1066, step time: 0.1232\n",
      "203/223, train_loss: 0.1024, step time: 0.1146\n",
      "204/223, train_loss: 0.1097, step time: 0.1084\n",
      "205/223, train_loss: 0.1101, step time: 0.1183\n",
      "206/223, train_loss: 0.1161, step time: 0.1171\n",
      "207/223, train_loss: 0.1090, step time: 0.1045\n",
      "208/223, train_loss: 0.1023, step time: 0.1091\n",
      "209/223, train_loss: 0.1093, step time: 0.1037\n",
      "210/223, train_loss: 0.1066, step time: 0.0991\n",
      "211/223, train_loss: 0.1052, step time: 0.1261\n",
      "212/223, train_loss: 0.1104, step time: 0.1113\n",
      "213/223, train_loss: 0.1043, step time: 0.1035\n",
      "214/223, train_loss: 0.1141, step time: 0.1000\n",
      "215/223, train_loss: 0.0950, step time: 0.1007\n",
      "216/223, train_loss: 0.1027, step time: 0.1003\n",
      "217/223, train_loss: 0.1096, step time: 0.1004\n",
      "218/223, train_loss: 0.1179, step time: 0.0994\n",
      "219/223, train_loss: 0.1077, step time: 0.1012\n",
      "220/223, train_loss: 0.1002, step time: 0.0998\n",
      "221/223, train_loss: 0.1105, step time: 0.0993\n",
      "222/223, train_loss: 0.1142, step time: 0.0986\n",
      "223/223, train_loss: 0.1064, step time: 0.1000\n",
      "epoch 129 average loss: 0.1099\n",
      "time consuming of epoch 129 is: 91.9709\n",
      "----------\n",
      "epoch 130/300\n",
      "1/223, train_loss: 0.0970, step time: 0.1438\n",
      "2/223, train_loss: 0.1070, step time: 0.1168\n",
      "3/223, train_loss: 0.1063, step time: 0.1083\n",
      "4/223, train_loss: 0.1033, step time: 0.1127\n",
      "5/223, train_loss: 0.1147, step time: 0.0999\n",
      "6/223, train_loss: 0.1177, step time: 0.1130\n",
      "7/223, train_loss: 0.0982, step time: 0.1004\n",
      "8/223, train_loss: 0.1091, step time: 0.0997\n",
      "9/223, train_loss: 0.1029, step time: 0.1002\n",
      "10/223, train_loss: 0.1205, step time: 0.1136\n",
      "11/223, train_loss: 0.1043, step time: 0.1143\n",
      "12/223, train_loss: 0.1046, step time: 0.1214\n",
      "13/223, train_loss: 0.1053, step time: 0.1004\n",
      "14/223, train_loss: 0.0995, step time: 0.1230\n",
      "15/223, train_loss: 0.1100, step time: 0.1156\n",
      "16/223, train_loss: 0.1157, step time: 0.1098\n",
      "17/223, train_loss: 0.1078, step time: 0.1263\n",
      "18/223, train_loss: 0.1063, step time: 0.1184\n",
      "19/223, train_loss: 0.1138, step time: 0.1125\n",
      "20/223, train_loss: 0.1134, step time: 0.1012\n",
      "21/223, train_loss: 0.1152, step time: 0.1196\n",
      "22/223, train_loss: 0.1057, step time: 0.1071\n",
      "23/223, train_loss: 0.1277, step time: 0.1141\n",
      "24/223, train_loss: 0.1073, step time: 0.1146\n",
      "25/223, train_loss: 0.0970, step time: 0.1004\n",
      "26/223, train_loss: 0.1172, step time: 0.1102\n",
      "27/223, train_loss: 0.1050, step time: 0.1096\n",
      "28/223, train_loss: 0.3062, step time: 0.1190\n",
      "29/223, train_loss: 0.1035, step time: 0.1002\n",
      "30/223, train_loss: 0.1039, step time: 0.1236\n",
      "31/223, train_loss: 0.1121, step time: 0.1188\n",
      "32/223, train_loss: 0.0992, step time: 0.1015\n",
      "33/223, train_loss: 0.1019, step time: 0.1066\n",
      "34/223, train_loss: 0.1106, step time: 0.1353\n",
      "35/223, train_loss: 0.1084, step time: 0.1001\n",
      "36/223, train_loss: 0.0964, step time: 0.1097\n",
      "37/223, train_loss: 0.1118, step time: 0.1024\n",
      "38/223, train_loss: 0.1070, step time: 0.1137\n",
      "39/223, train_loss: 0.0993, step time: 0.1009\n",
      "40/223, train_loss: 0.1119, step time: 0.1008\n",
      "41/223, train_loss: 0.1042, step time: 0.1072\n",
      "42/223, train_loss: 0.1146, step time: 0.1239\n",
      "43/223, train_loss: 0.1110, step time: 0.1301\n",
      "44/223, train_loss: 0.1093, step time: 0.1261\n",
      "45/223, train_loss: 0.1079, step time: 0.1038\n",
      "46/223, train_loss: 0.1032, step time: 0.1159\n",
      "47/223, train_loss: 0.1122, step time: 0.1000\n",
      "48/223, train_loss: 0.1185, step time: 0.1004\n",
      "49/223, train_loss: 0.1187, step time: 0.1027\n",
      "50/223, train_loss: 0.1149, step time: 0.1138\n",
      "51/223, train_loss: 0.1156, step time: 0.1105\n",
      "52/223, train_loss: 0.1126, step time: 0.1017\n",
      "53/223, train_loss: 0.1076, step time: 0.1053\n",
      "54/223, train_loss: 0.1037, step time: 0.0998\n",
      "55/223, train_loss: 0.1060, step time: 0.0998\n",
      "56/223, train_loss: 0.1117, step time: 0.1053\n",
      "57/223, train_loss: 0.1092, step time: 0.1148\n",
      "58/223, train_loss: 0.1248, step time: 0.1011\n",
      "59/223, train_loss: 0.1042, step time: 0.1046\n",
      "60/223, train_loss: 0.1080, step time: 0.1178\n",
      "61/223, train_loss: 0.1124, step time: 0.1197\n",
      "62/223, train_loss: 0.1090, step time: 0.1181\n",
      "63/223, train_loss: 0.0988, step time: 0.1157\n",
      "64/223, train_loss: 0.1116, step time: 0.1252\n",
      "65/223, train_loss: 0.1053, step time: 0.1092\n",
      "66/223, train_loss: 0.1216, step time: 0.1110\n",
      "67/223, train_loss: 0.1110, step time: 0.1080\n",
      "68/223, train_loss: 0.1099, step time: 0.0990\n",
      "69/223, train_loss: 0.1083, step time: 0.1007\n",
      "70/223, train_loss: 0.1086, step time: 0.1059\n",
      "71/223, train_loss: 0.0996, step time: 0.1043\n",
      "72/223, train_loss: 0.1181, step time: 0.1220\n",
      "73/223, train_loss: 0.1051, step time: 0.1131\n",
      "74/223, train_loss: 0.1108, step time: 0.1201\n",
      "75/223, train_loss: 0.0927, step time: 0.1012\n",
      "76/223, train_loss: 0.1167, step time: 0.1001\n",
      "77/223, train_loss: 0.1193, step time: 0.1064\n",
      "78/223, train_loss: 0.1019, step time: 0.1046\n",
      "79/223, train_loss: 0.1046, step time: 0.1190\n",
      "80/223, train_loss: 0.1079, step time: 0.1138\n",
      "81/223, train_loss: 0.1024, step time: 0.1003\n",
      "82/223, train_loss: 0.1150, step time: 0.1144\n",
      "83/223, train_loss: 0.0964, step time: 0.1106\n",
      "84/223, train_loss: 0.1066, step time: 0.1356\n",
      "85/223, train_loss: 0.1073, step time: 0.1085\n",
      "86/223, train_loss: 0.1283, step time: 0.1006\n",
      "87/223, train_loss: 0.1094, step time: 0.1210\n",
      "88/223, train_loss: 0.1101, step time: 0.1064\n",
      "89/223, train_loss: 0.1055, step time: 0.1005\n",
      "90/223, train_loss: 0.1217, step time: 0.1144\n",
      "91/223, train_loss: 0.1035, step time: 0.1241\n",
      "92/223, train_loss: 0.1096, step time: 0.1148\n",
      "93/223, train_loss: 0.1149, step time: 0.1050\n",
      "94/223, train_loss: 0.1029, step time: 0.1151\n",
      "95/223, train_loss: 0.1095, step time: 0.1204\n",
      "96/223, train_loss: 0.1054, step time: 0.1165\n",
      "97/223, train_loss: 0.1070, step time: 0.1213\n",
      "98/223, train_loss: 0.1067, step time: 0.1243\n",
      "99/223, train_loss: 0.1007, step time: 0.1297\n",
      "100/223, train_loss: 0.1103, step time: 0.1005\n",
      "101/223, train_loss: 0.0958, step time: 0.1030\n",
      "102/223, train_loss: 0.0967, step time: 0.1124\n",
      "103/223, train_loss: 0.1081, step time: 0.1213\n",
      "104/223, train_loss: 0.1031, step time: 0.1219\n",
      "105/223, train_loss: 0.1026, step time: 0.1034\n",
      "106/223, train_loss: 0.1053, step time: 0.1113\n",
      "107/223, train_loss: 0.1052, step time: 0.1204\n",
      "108/223, train_loss: 0.0976, step time: 0.1034\n",
      "109/223, train_loss: 0.1142, step time: 0.1118\n",
      "110/223, train_loss: 0.1077, step time: 0.1077\n",
      "111/223, train_loss: 0.0951, step time: 0.1116\n",
      "112/223, train_loss: 0.0978, step time: 0.1082\n",
      "113/223, train_loss: 0.1091, step time: 0.1058\n",
      "114/223, train_loss: 0.1074, step time: 0.1227\n",
      "115/223, train_loss: 0.1018, step time: 0.1177\n",
      "116/223, train_loss: 0.1118, step time: 0.1112\n",
      "117/223, train_loss: 0.1226, step time: 0.1055\n",
      "118/223, train_loss: 0.1056, step time: 0.1164\n",
      "119/223, train_loss: 0.1111, step time: 0.1175\n",
      "120/223, train_loss: 0.1097, step time: 0.1067\n",
      "121/223, train_loss: 0.1074, step time: 0.1256\n",
      "122/223, train_loss: 0.1082, step time: 0.1059\n",
      "123/223, train_loss: 0.1116, step time: 0.0994\n",
      "124/223, train_loss: 0.1202, step time: 0.0993\n",
      "125/223, train_loss: 0.1181, step time: 0.1695\n",
      "126/223, train_loss: 0.1245, step time: 0.1162\n",
      "127/223, train_loss: 0.1154, step time: 0.0999\n",
      "128/223, train_loss: 0.1011, step time: 0.1190\n",
      "129/223, train_loss: 0.1063, step time: 0.0990\n",
      "130/223, train_loss: 0.1076, step time: 0.1407\n",
      "131/223, train_loss: 0.0988, step time: 0.1173\n",
      "132/223, train_loss: 0.1017, step time: 0.1023\n",
      "133/223, train_loss: 0.1072, step time: 0.0993\n",
      "134/223, train_loss: 0.1080, step time: 0.1070\n",
      "135/223, train_loss: 0.0967, step time: 0.1215\n",
      "136/223, train_loss: 0.1240, step time: 0.1452\n",
      "137/223, train_loss: 0.1156, step time: 0.1010\n",
      "138/223, train_loss: 0.1167, step time: 0.1019\n",
      "139/223, train_loss: 0.1048, step time: 0.1025\n",
      "140/223, train_loss: 0.1107, step time: 0.1002\n",
      "141/223, train_loss: 0.1104, step time: 0.1003\n",
      "142/223, train_loss: 0.1058, step time: 0.1077\n",
      "143/223, train_loss: 0.1110, step time: 0.1005\n",
      "144/223, train_loss: 0.1010, step time: 0.1216\n",
      "145/223, train_loss: 0.1138, step time: 0.1222\n",
      "146/223, train_loss: 0.0961, step time: 0.1115\n",
      "147/223, train_loss: 0.1140, step time: 0.0999\n",
      "148/223, train_loss: 0.1015, step time: 0.1073\n",
      "149/223, train_loss: 0.1219, step time: 0.0998\n",
      "150/223, train_loss: 0.1143, step time: 0.1242\n",
      "151/223, train_loss: 0.1123, step time: 0.1052\n",
      "152/223, train_loss: 0.1081, step time: 0.1006\n",
      "153/223, train_loss: 0.1158, step time: 0.0999\n",
      "154/223, train_loss: 0.1056, step time: 0.0996\n",
      "155/223, train_loss: 0.1017, step time: 0.1158\n",
      "156/223, train_loss: 0.1023, step time: 0.1188\n",
      "157/223, train_loss: 0.1089, step time: 0.1014\n",
      "158/223, train_loss: 0.1116, step time: 0.1205\n",
      "159/223, train_loss: 0.1051, step time: 0.1088\n",
      "160/223, train_loss: 0.1113, step time: 0.1113\n",
      "161/223, train_loss: 0.1032, step time: 0.1100\n",
      "162/223, train_loss: 0.1068, step time: 0.1309\n",
      "163/223, train_loss: 0.1131, step time: 0.1327\n",
      "164/223, train_loss: 0.1044, step time: 0.1031\n",
      "165/223, train_loss: 0.1155, step time: 0.1002\n",
      "166/223, train_loss: 0.1002, step time: 0.1163\n",
      "167/223, train_loss: 0.1163, step time: 0.1040\n",
      "168/223, train_loss: 0.1074, step time: 0.1150\n",
      "169/223, train_loss: 0.1060, step time: 0.1222\n",
      "170/223, train_loss: 0.1156, step time: 0.1043\n",
      "171/223, train_loss: 0.0993, step time: 0.1061\n",
      "172/223, train_loss: 0.1097, step time: 0.1212\n",
      "173/223, train_loss: 0.1010, step time: 0.0991\n",
      "174/223, train_loss: 0.1189, step time: 0.1034\n",
      "175/223, train_loss: 0.1125, step time: 0.1010\n",
      "176/223, train_loss: 0.0999, step time: 0.1034\n",
      "177/223, train_loss: 0.1126, step time: 0.1014\n",
      "178/223, train_loss: 0.1059, step time: 0.1102\n",
      "179/223, train_loss: 0.1135, step time: 0.1362\n",
      "180/223, train_loss: 0.1104, step time: 0.1272\n",
      "181/223, train_loss: 0.1246, step time: 0.1051\n",
      "182/223, train_loss: 0.1064, step time: 0.1079\n",
      "183/223, train_loss: 0.1018, step time: 0.1055\n",
      "184/223, train_loss: 0.0978, step time: 0.1098\n",
      "185/223, train_loss: 0.1109, step time: 0.1275\n",
      "186/223, train_loss: 0.1076, step time: 0.1171\n",
      "187/223, train_loss: 0.1113, step time: 0.1419\n",
      "188/223, train_loss: 0.1125, step time: 0.1102\n",
      "189/223, train_loss: 0.1168, step time: 0.1230\n",
      "190/223, train_loss: 0.1106, step time: 0.1010\n",
      "191/223, train_loss: 0.1199, step time: 0.1053\n",
      "192/223, train_loss: 0.1121, step time: 0.1078\n",
      "193/223, train_loss: 0.1132, step time: 0.1008\n",
      "194/223, train_loss: 0.1058, step time: 0.1205\n",
      "195/223, train_loss: 0.1257, step time: 0.1556\n",
      "196/223, train_loss: 0.1059, step time: 0.1169\n",
      "197/223, train_loss: 0.1349, step time: 0.1009\n",
      "198/223, train_loss: 0.1116, step time: 0.1625\n",
      "199/223, train_loss: 0.1122, step time: 0.1000\n",
      "200/223, train_loss: 0.1145, step time: 0.1216\n",
      "201/223, train_loss: 0.1329, step time: 0.1130\n",
      "202/223, train_loss: 0.1069, step time: 0.1453\n",
      "203/223, train_loss: 0.1027, step time: 0.1007\n",
      "204/223, train_loss: 0.0974, step time: 0.1004\n",
      "205/223, train_loss: 0.1204, step time: 0.1087\n",
      "206/223, train_loss: 0.1047, step time: 0.1208\n",
      "207/223, train_loss: 0.1205, step time: 0.1135\n",
      "208/223, train_loss: 0.1101, step time: 0.1202\n",
      "209/223, train_loss: 0.1061, step time: 0.1004\n",
      "210/223, train_loss: 0.0950, step time: 0.1003\n",
      "211/223, train_loss: 0.1091, step time: 0.1087\n",
      "212/223, train_loss: 0.1079, step time: 0.1168\n",
      "213/223, train_loss: 0.1003, step time: 0.0998\n",
      "214/223, train_loss: 0.1082, step time: 0.1088\n",
      "215/223, train_loss: 0.1051, step time: 0.1066\n",
      "216/223, train_loss: 0.1012, step time: 0.1112\n",
      "217/223, train_loss: 0.1157, step time: 0.1272\n",
      "218/223, train_loss: 0.1125, step time: 0.1006\n",
      "219/223, train_loss: 0.1145, step time: 0.1011\n",
      "220/223, train_loss: 0.1229, step time: 0.0998\n",
      "221/223, train_loss: 0.1029, step time: 0.0998\n",
      "222/223, train_loss: 0.1005, step time: 0.0992\n",
      "223/223, train_loss: 0.1141, step time: 0.0992\n",
      "epoch 130 average loss: 0.1098\n",
      "current epoch: 130 current mean dice: 0.8540 tc: 0.9186 wt: 0.8641 et: 0.7792\n",
      "best mean dice: 0.8551 at epoch: 125\n",
      "time consuming of epoch 130 is: 89.6063\n",
      "----------\n",
      "epoch 131/300\n",
      "1/223, train_loss: 0.1030, step time: 0.1117\n",
      "2/223, train_loss: 0.0949, step time: 0.1004\n",
      "3/223, train_loss: 0.1235, step time: 0.1015\n",
      "4/223, train_loss: 0.1248, step time: 0.1010\n",
      "5/223, train_loss: 0.0996, step time: 0.1059\n",
      "6/223, train_loss: 0.1213, step time: 0.1000\n",
      "7/223, train_loss: 0.1019, step time: 0.1822\n",
      "8/223, train_loss: 0.1031, step time: 0.1132\n",
      "9/223, train_loss: 0.1127, step time: 0.1131\n",
      "10/223, train_loss: 0.1131, step time: 0.1137\n",
      "11/223, train_loss: 0.1021, step time: 0.1206\n",
      "12/223, train_loss: 0.1052, step time: 0.1244\n",
      "13/223, train_loss: 0.0997, step time: 0.1082\n",
      "14/223, train_loss: 0.1113, step time: 0.1004\n",
      "15/223, train_loss: 0.1055, step time: 0.1060\n",
      "16/223, train_loss: 0.1118, step time: 0.0999\n",
      "17/223, train_loss: 0.1132, step time: 0.1049\n",
      "18/223, train_loss: 0.1069, step time: 0.0999\n",
      "19/223, train_loss: 0.1104, step time: 0.1314\n",
      "20/223, train_loss: 0.1078, step time: 0.1001\n",
      "21/223, train_loss: 0.1029, step time: 0.0999\n",
      "22/223, train_loss: 0.1096, step time: 0.1109\n",
      "23/223, train_loss: 0.1003, step time: 0.1193\n",
      "24/223, train_loss: 0.1240, step time: 0.1272\n",
      "25/223, train_loss: 0.1153, step time: 0.1069\n",
      "26/223, train_loss: 0.1133, step time: 0.1190\n",
      "27/223, train_loss: 0.1057, step time: 0.1100\n",
      "28/223, train_loss: 0.1229, step time: 0.0999\n",
      "29/223, train_loss: 0.1085, step time: 0.0999\n",
      "30/223, train_loss: 0.1028, step time: 0.1158\n",
      "31/223, train_loss: 0.1032, step time: 0.1002\n",
      "32/223, train_loss: 0.1117, step time: 0.1008\n",
      "33/223, train_loss: 0.1163, step time: 0.1006\n",
      "34/223, train_loss: 0.1038, step time: 0.1562\n",
      "35/223, train_loss: 0.1013, step time: 0.1032\n",
      "36/223, train_loss: 0.1260, step time: 0.1002\n",
      "37/223, train_loss: 0.1184, step time: 0.1000\n",
      "38/223, train_loss: 0.1086, step time: 0.1347\n",
      "39/223, train_loss: 0.1032, step time: 0.1125\n",
      "40/223, train_loss: 0.0956, step time: 0.1048\n",
      "41/223, train_loss: 0.1268, step time: 0.1002\n",
      "42/223, train_loss: 0.1222, step time: 0.1007\n",
      "43/223, train_loss: 0.0979, step time: 0.1416\n",
      "44/223, train_loss: 0.1069, step time: 0.1031\n",
      "45/223, train_loss: 0.1086, step time: 0.1025\n",
      "46/223, train_loss: 0.1141, step time: 0.1020\n",
      "47/223, train_loss: 0.1185, step time: 0.1049\n",
      "48/223, train_loss: 0.1146, step time: 0.1002\n",
      "49/223, train_loss: 0.1036, step time: 0.1007\n",
      "50/223, train_loss: 0.1053, step time: 0.1057\n",
      "51/223, train_loss: 0.1109, step time: 0.1367\n",
      "52/223, train_loss: 0.1129, step time: 0.1391\n",
      "53/223, train_loss: 0.1025, step time: 0.1132\n",
      "54/223, train_loss: 0.1044, step time: 0.1008\n",
      "55/223, train_loss: 0.1100, step time: 0.1111\n",
      "56/223, train_loss: 0.1004, step time: 0.1119\n",
      "57/223, train_loss: 0.1025, step time: 0.1106\n",
      "58/223, train_loss: 0.1047, step time: 0.1008\n",
      "59/223, train_loss: 0.1035, step time: 0.1021\n",
      "60/223, train_loss: 0.1006, step time: 0.1046\n",
      "61/223, train_loss: 0.1112, step time: 0.1160\n",
      "62/223, train_loss: 0.1195, step time: 0.1001\n",
      "63/223, train_loss: 0.1061, step time: 0.1000\n",
      "64/223, train_loss: 0.1159, step time: 0.1172\n",
      "65/223, train_loss: 0.1118, step time: 0.1146\n",
      "66/223, train_loss: 0.1014, step time: 0.1075\n",
      "67/223, train_loss: 0.1298, step time: 0.1186\n",
      "68/223, train_loss: 0.1004, step time: 0.1130\n",
      "69/223, train_loss: 0.1267, step time: 0.1119\n",
      "70/223, train_loss: 0.1109, step time: 0.1152\n",
      "71/223, train_loss: 0.0976, step time: 0.1083\n",
      "72/223, train_loss: 0.1113, step time: 0.1189\n",
      "73/223, train_loss: 0.1117, step time: 0.1079\n",
      "74/223, train_loss: 0.1015, step time: 0.1056\n",
      "75/223, train_loss: 0.1054, step time: 0.1159\n",
      "76/223, train_loss: 0.1016, step time: 0.1448\n",
      "77/223, train_loss: 0.1031, step time: 0.0997\n",
      "78/223, train_loss: 0.1163, step time: 0.1013\n",
      "79/223, train_loss: 0.1096, step time: 0.1001\n",
      "80/223, train_loss: 0.1187, step time: 0.1188\n",
      "81/223, train_loss: 0.1096, step time: 0.1275\n",
      "82/223, train_loss: 0.1042, step time: 0.1139\n",
      "83/223, train_loss: 0.1018, step time: 0.1167\n",
      "84/223, train_loss: 0.1056, step time: 0.1144\n",
      "85/223, train_loss: 0.1056, step time: 0.0995\n",
      "86/223, train_loss: 0.1193, step time: 0.1209\n",
      "87/223, train_loss: 0.1118, step time: 0.1059\n",
      "88/223, train_loss: 0.1073, step time: 0.1184\n",
      "89/223, train_loss: 0.1078, step time: 0.1004\n",
      "90/223, train_loss: 0.1108, step time: 0.1146\n",
      "91/223, train_loss: 0.1094, step time: 0.1022\n",
      "92/223, train_loss: 0.1177, step time: 0.1144\n",
      "93/223, train_loss: 0.1036, step time: 0.1059\n",
      "94/223, train_loss: 0.1084, step time: 0.1159\n",
      "95/223, train_loss: 0.1033, step time: 0.1047\n",
      "96/223, train_loss: 0.1201, step time: 0.1142\n",
      "97/223, train_loss: 0.1073, step time: 0.1099\n",
      "98/223, train_loss: 0.1108, step time: 0.1271\n",
      "99/223, train_loss: 0.1106, step time: 0.1116\n",
      "100/223, train_loss: 0.1160, step time: 0.1156\n",
      "101/223, train_loss: 0.1065, step time: 0.1226\n",
      "102/223, train_loss: 0.1142, step time: 0.1059\n",
      "103/223, train_loss: 0.1044, step time: 0.1060\n",
      "104/223, train_loss: 0.1031, step time: 0.1262\n",
      "105/223, train_loss: 0.0991, step time: 0.1151\n",
      "106/223, train_loss: 0.1042, step time: 0.1148\n",
      "107/223, train_loss: 0.0955, step time: 0.1060\n",
      "108/223, train_loss: 0.0965, step time: 0.1165\n",
      "109/223, train_loss: 0.0976, step time: 0.1039\n",
      "110/223, train_loss: 0.1082, step time: 0.1147\n",
      "111/223, train_loss: 0.1130, step time: 0.1220\n",
      "112/223, train_loss: 0.1153, step time: 0.1475\n",
      "113/223, train_loss: 0.1225, step time: 0.1223\n",
      "114/223, train_loss: 0.1063, step time: 0.1001\n",
      "115/223, train_loss: 0.1165, step time: 0.0997\n",
      "116/223, train_loss: 0.1012, step time: 0.1001\n",
      "117/223, train_loss: 0.1269, step time: 0.1008\n",
      "118/223, train_loss: 0.1038, step time: 0.1001\n",
      "119/223, train_loss: 0.1146, step time: 0.0994\n",
      "120/223, train_loss: 0.1017, step time: 0.0996\n",
      "121/223, train_loss: 0.1012, step time: 0.1127\n",
      "122/223, train_loss: 0.0933, step time: 0.1001\n",
      "123/223, train_loss: 0.0994, step time: 0.1001\n",
      "124/223, train_loss: 0.1116, step time: 0.0994\n",
      "125/223, train_loss: 0.0976, step time: 0.1122\n",
      "126/223, train_loss: 0.1070, step time: 0.1031\n",
      "127/223, train_loss: 0.1029, step time: 0.1166\n",
      "128/223, train_loss: 0.1121, step time: 0.1058\n",
      "129/223, train_loss: 0.1051, step time: 0.1157\n",
      "130/223, train_loss: 0.1088, step time: 0.1146\n",
      "131/223, train_loss: 0.0995, step time: 0.1008\n",
      "132/223, train_loss: 0.1085, step time: 0.1179\n",
      "133/223, train_loss: 0.1213, step time: 0.1017\n",
      "134/223, train_loss: 0.1032, step time: 0.1175\n",
      "135/223, train_loss: 0.1114, step time: 0.1248\n",
      "136/223, train_loss: 0.1020, step time: 0.1054\n",
      "137/223, train_loss: 0.0952, step time: 0.1004\n",
      "138/223, train_loss: 0.1123, step time: 0.1096\n",
      "139/223, train_loss: 0.1168, step time: 0.1312\n",
      "140/223, train_loss: 0.1138, step time: 0.1029\n",
      "141/223, train_loss: 0.1064, step time: 0.1004\n",
      "142/223, train_loss: 0.1097, step time: 0.1100\n",
      "143/223, train_loss: 0.1066, step time: 0.1002\n",
      "144/223, train_loss: 0.1051, step time: 0.1006\n",
      "145/223, train_loss: 0.1047, step time: 0.1102\n",
      "146/223, train_loss: 0.0998, step time: 0.1087\n",
      "147/223, train_loss: 0.1112, step time: 0.1009\n",
      "148/223, train_loss: 0.1069, step time: 0.0996\n",
      "149/223, train_loss: 0.1139, step time: 0.1208\n",
      "150/223, train_loss: 0.1128, step time: 0.0989\n",
      "151/223, train_loss: 0.1127, step time: 0.1011\n",
      "152/223, train_loss: 0.1150, step time: 0.1066\n",
      "153/223, train_loss: 0.1090, step time: 0.1037\n",
      "154/223, train_loss: 0.0989, step time: 0.0987\n",
      "155/223, train_loss: 0.1051, step time: 0.0991\n",
      "156/223, train_loss: 0.1041, step time: 0.1046\n",
      "157/223, train_loss: 0.1100, step time: 0.1064\n",
      "158/223, train_loss: 0.1214, step time: 0.0995\n",
      "159/223, train_loss: 0.1041, step time: 0.0988\n",
      "160/223, train_loss: 0.1102, step time: 0.0994\n",
      "161/223, train_loss: 0.1048, step time: 0.0985\n",
      "162/223, train_loss: 0.1065, step time: 0.1282\n",
      "163/223, train_loss: 0.1102, step time: 0.1460\n",
      "164/223, train_loss: 0.1053, step time: 0.1047\n",
      "165/223, train_loss: 0.1056, step time: 0.1010\n",
      "166/223, train_loss: 0.1118, step time: 0.1184\n",
      "167/223, train_loss: 0.1115, step time: 0.1207\n",
      "168/223, train_loss: 0.1179, step time: 0.1044\n",
      "169/223, train_loss: 0.1182, step time: 0.1066\n",
      "170/223, train_loss: 0.1064, step time: 0.1142\n",
      "171/223, train_loss: 0.1051, step time: 0.1118\n",
      "172/223, train_loss: 0.1124, step time: 0.1410\n",
      "173/223, train_loss: 0.1081, step time: 0.1064\n",
      "174/223, train_loss: 0.1028, step time: 0.1016\n",
      "175/223, train_loss: 0.1023, step time: 0.1146\n",
      "176/223, train_loss: 0.1230, step time: 0.0995\n",
      "177/223, train_loss: 0.1137, step time: 0.1176\n",
      "178/223, train_loss: 0.1119, step time: 0.1041\n",
      "179/223, train_loss: 0.1170, step time: 0.1094\n",
      "180/223, train_loss: 0.1175, step time: 0.1448\n",
      "181/223, train_loss: 0.1041, step time: 0.1326\n",
      "182/223, train_loss: 0.1043, step time: 0.1043\n",
      "183/223, train_loss: 0.1166, step time: 0.1114\n",
      "184/223, train_loss: 0.1126, step time: 0.1121\n",
      "185/223, train_loss: 0.1055, step time: 0.1215\n",
      "186/223, train_loss: 0.1081, step time: 0.0999\n",
      "187/223, train_loss: 0.0997, step time: 0.1254\n",
      "188/223, train_loss: 0.1003, step time: 0.1336\n",
      "189/223, train_loss: 0.1014, step time: 0.1056\n",
      "190/223, train_loss: 0.1078, step time: 0.1205\n",
      "191/223, train_loss: 0.0946, step time: 0.1090\n",
      "192/223, train_loss: 0.1008, step time: 0.0994\n",
      "193/223, train_loss: 0.1149, step time: 0.1122\n",
      "194/223, train_loss: 0.1093, step time: 0.1237\n",
      "195/223, train_loss: 0.0977, step time: 0.1203\n",
      "196/223, train_loss: 0.1116, step time: 0.1100\n",
      "197/223, train_loss: 0.1142, step time: 0.1128\n",
      "198/223, train_loss: 0.1135, step time: 0.1033\n",
      "199/223, train_loss: 0.1062, step time: 0.1024\n",
      "200/223, train_loss: 0.1150, step time: 0.1000\n",
      "201/223, train_loss: 0.1104, step time: 0.1046\n",
      "202/223, train_loss: 0.1125, step time: 0.1000\n",
      "203/223, train_loss: 0.1060, step time: 0.1008\n",
      "204/223, train_loss: 0.1036, step time: 0.1020\n",
      "205/223, train_loss: 0.1106, step time: 0.1005\n",
      "206/223, train_loss: 0.1135, step time: 0.0988\n",
      "207/223, train_loss: 0.1047, step time: 0.0990\n",
      "208/223, train_loss: 0.1013, step time: 0.1010\n",
      "209/223, train_loss: 0.1063, step time: 0.1224\n",
      "210/223, train_loss: 0.1084, step time: 0.0999\n",
      "211/223, train_loss: 0.1182, step time: 0.1358\n",
      "212/223, train_loss: 0.1114, step time: 0.1106\n",
      "213/223, train_loss: 0.3037, step time: 0.1001\n",
      "214/223, train_loss: 0.1095, step time: 0.0996\n",
      "215/223, train_loss: 0.0991, step time: 0.1011\n",
      "216/223, train_loss: 0.1040, step time: 0.1042\n",
      "217/223, train_loss: 0.1097, step time: 0.1009\n",
      "218/223, train_loss: 0.0993, step time: 0.0991\n",
      "219/223, train_loss: 0.1057, step time: 0.0996\n",
      "220/223, train_loss: 0.1175, step time: 0.1003\n",
      "221/223, train_loss: 0.1051, step time: 0.1020\n",
      "222/223, train_loss: 0.1082, step time: 0.0992\n",
      "223/223, train_loss: 0.1082, step time: 0.0995\n",
      "epoch 131 average loss: 0.1094\n",
      "time consuming of epoch 131 is: 89.3428\n",
      "----------\n",
      "epoch 132/300\n",
      "1/223, train_loss: 0.1044, step time: 0.1017\n",
      "2/223, train_loss: 0.0962, step time: 0.0997\n",
      "3/223, train_loss: 0.1099, step time: 0.1001\n",
      "4/223, train_loss: 0.1063, step time: 0.1149\n",
      "5/223, train_loss: 0.1074, step time: 0.1278\n",
      "6/223, train_loss: 0.1130, step time: 0.1076\n",
      "7/223, train_loss: 0.1017, step time: 0.1126\n",
      "8/223, train_loss: 0.0990, step time: 0.1012\n",
      "9/223, train_loss: 0.1181, step time: 0.1108\n",
      "10/223, train_loss: 0.1042, step time: 0.1109\n",
      "11/223, train_loss: 0.1026, step time: 0.1022\n",
      "12/223, train_loss: 0.1162, step time: 0.1148\n",
      "13/223, train_loss: 0.1056, step time: 0.1291\n",
      "14/223, train_loss: 0.1149, step time: 0.1248\n",
      "15/223, train_loss: 0.0961, step time: 0.1086\n",
      "16/223, train_loss: 0.1108, step time: 0.1237\n",
      "17/223, train_loss: 0.1078, step time: 0.1098\n",
      "18/223, train_loss: 0.1175, step time: 0.1064\n",
      "19/223, train_loss: 0.1113, step time: 0.1158\n",
      "20/223, train_loss: 0.1016, step time: 0.1208\n",
      "21/223, train_loss: 0.1163, step time: 0.1094\n",
      "22/223, train_loss: 0.1017, step time: 0.1264\n",
      "23/223, train_loss: 0.1035, step time: 0.1302\n",
      "24/223, train_loss: 0.1073, step time: 0.1127\n",
      "25/223, train_loss: 0.1082, step time: 0.1210\n",
      "26/223, train_loss: 0.1058, step time: 0.1107\n",
      "27/223, train_loss: 0.1011, step time: 0.1006\n",
      "28/223, train_loss: 0.1075, step time: 0.1026\n",
      "29/223, train_loss: 0.0969, step time: 0.1094\n",
      "30/223, train_loss: 0.1163, step time: 0.1327\n",
      "31/223, train_loss: 0.1105, step time: 0.1015\n",
      "32/223, train_loss: 0.1089, step time: 0.1147\n",
      "33/223, train_loss: 0.1048, step time: 0.1070\n",
      "34/223, train_loss: 0.0975, step time: 0.1126\n",
      "35/223, train_loss: 0.1068, step time: 0.1118\n",
      "36/223, train_loss: 0.1029, step time: 0.1014\n",
      "37/223, train_loss: 0.1040, step time: 0.1334\n",
      "38/223, train_loss: 0.1163, step time: 0.0997\n",
      "39/223, train_loss: 0.1216, step time: 0.1005\n",
      "40/223, train_loss: 0.1003, step time: 0.1272\n",
      "41/223, train_loss: 0.1206, step time: 0.1100\n",
      "42/223, train_loss: 0.1098, step time: 0.1253\n",
      "43/223, train_loss: 0.1079, step time: 0.1027\n",
      "44/223, train_loss: 0.1128, step time: 0.1001\n",
      "45/223, train_loss: 0.1036, step time: 0.1167\n",
      "46/223, train_loss: 0.0940, step time: 0.1183\n",
      "47/223, train_loss: 0.1006, step time: 0.1009\n",
      "48/223, train_loss: 0.1094, step time: 0.1017\n",
      "49/223, train_loss: 0.1158, step time: 0.1237\n",
      "50/223, train_loss: 0.3081, step time: 0.1001\n",
      "51/223, train_loss: 0.0988, step time: 0.1005\n",
      "52/223, train_loss: 0.1003, step time: 0.1010\n",
      "53/223, train_loss: 0.1112, step time: 0.1213\n",
      "54/223, train_loss: 0.1082, step time: 0.1052\n",
      "55/223, train_loss: 0.1021, step time: 0.1172\n",
      "56/223, train_loss: 0.1073, step time: 0.1085\n",
      "57/223, train_loss: 0.1053, step time: 0.1005\n",
      "58/223, train_loss: 0.1088, step time: 0.1114\n",
      "59/223, train_loss: 0.1044, step time: 0.1013\n",
      "60/223, train_loss: 0.1165, step time: 0.1004\n",
      "61/223, train_loss: 0.0937, step time: 0.1197\n",
      "62/223, train_loss: 0.1006, step time: 0.1089\n",
      "63/223, train_loss: 0.1091, step time: 0.1103\n",
      "64/223, train_loss: 0.1073, step time: 0.1057\n",
      "65/223, train_loss: 0.1033, step time: 0.1082\n",
      "66/223, train_loss: 0.1116, step time: 0.1036\n",
      "67/223, train_loss: 0.1008, step time: 0.1113\n",
      "68/223, train_loss: 0.1032, step time: 0.1003\n",
      "69/223, train_loss: 0.1047, step time: 0.1126\n",
      "70/223, train_loss: 0.1165, step time: 0.1034\n",
      "71/223, train_loss: 0.1105, step time: 0.1009\n",
      "72/223, train_loss: 0.1035, step time: 0.1024\n",
      "73/223, train_loss: 0.1004, step time: 0.1098\n",
      "74/223, train_loss: 0.0999, step time: 0.1066\n",
      "75/223, train_loss: 0.1002, step time: 0.1289\n",
      "76/223, train_loss: 0.1228, step time: 0.1095\n",
      "77/223, train_loss: 0.1122, step time: 0.1037\n",
      "78/223, train_loss: 0.1082, step time: 0.1003\n",
      "79/223, train_loss: 0.1036, step time: 0.1004\n",
      "80/223, train_loss: 0.0996, step time: 0.1040\n",
      "81/223, train_loss: 0.1243, step time: 0.1119\n",
      "82/223, train_loss: 0.1143, step time: 0.1094\n",
      "83/223, train_loss: 0.1069, step time: 0.1172\n",
      "84/223, train_loss: 0.1094, step time: 0.1096\n",
      "85/223, train_loss: 0.1044, step time: 0.1181\n",
      "86/223, train_loss: 0.1178, step time: 0.1061\n",
      "87/223, train_loss: 0.1090, step time: 0.1198\n",
      "88/223, train_loss: 0.1124, step time: 0.1054\n",
      "89/223, train_loss: 0.1017, step time: 0.1097\n",
      "90/223, train_loss: 0.1081, step time: 0.1534\n",
      "91/223, train_loss: 0.1080, step time: 0.1003\n",
      "92/223, train_loss: 0.1140, step time: 0.1002\n",
      "93/223, train_loss: 0.1084, step time: 0.1059\n",
      "94/223, train_loss: 0.1116, step time: 0.1476\n",
      "95/223, train_loss: 0.1304, step time: 0.1114\n",
      "96/223, train_loss: 0.1086, step time: 0.1206\n",
      "97/223, train_loss: 0.1164, step time: 0.1005\n",
      "98/223, train_loss: 0.1064, step time: 0.1019\n",
      "99/223, train_loss: 0.1143, step time: 0.1137\n",
      "100/223, train_loss: 0.1126, step time: 0.1106\n",
      "101/223, train_loss: 0.1142, step time: 0.1163\n",
      "102/223, train_loss: 0.1047, step time: 0.1109\n",
      "103/223, train_loss: 0.1099, step time: 0.1153\n",
      "104/223, train_loss: 0.1072, step time: 0.1166\n",
      "105/223, train_loss: 0.1071, step time: 0.1133\n",
      "106/223, train_loss: 0.1077, step time: 0.1140\n",
      "107/223, train_loss: 0.1045, step time: 0.1145\n",
      "108/223, train_loss: 0.1104, step time: 0.1741\n",
      "109/223, train_loss: 0.1166, step time: 0.1161\n",
      "110/223, train_loss: 0.1204, step time: 0.1302\n",
      "111/223, train_loss: 0.1003, step time: 0.1194\n",
      "112/223, train_loss: 0.1155, step time: 0.1049\n",
      "113/223, train_loss: 0.1079, step time: 0.1076\n",
      "114/223, train_loss: 0.1039, step time: 0.1039\n",
      "115/223, train_loss: 0.1107, step time: 0.1242\n",
      "116/223, train_loss: 0.1001, step time: 0.1114\n",
      "117/223, train_loss: 0.1099, step time: 0.1220\n",
      "118/223, train_loss: 0.1147, step time: 0.1483\n",
      "119/223, train_loss: 0.1099, step time: 0.1011\n",
      "120/223, train_loss: 0.1083, step time: 0.1012\n",
      "121/223, train_loss: 0.0978, step time: 0.0996\n",
      "122/223, train_loss: 0.1188, step time: 0.0992\n",
      "123/223, train_loss: 0.1184, step time: 0.0999\n",
      "124/223, train_loss: 0.1060, step time: 0.0998\n",
      "125/223, train_loss: 0.1178, step time: 0.1486\n",
      "126/223, train_loss: 0.1172, step time: 0.1193\n",
      "127/223, train_loss: 0.0969, step time: 0.0997\n",
      "128/223, train_loss: 0.1033, step time: 0.1005\n",
      "129/223, train_loss: 0.0973, step time: 0.1076\n",
      "130/223, train_loss: 0.1155, step time: 0.1006\n",
      "131/223, train_loss: 0.0994, step time: 0.1002\n",
      "132/223, train_loss: 0.1089, step time: 0.1064\n",
      "133/223, train_loss: 0.1055, step time: 0.1121\n",
      "134/223, train_loss: 0.1054, step time: 0.1008\n",
      "135/223, train_loss: 0.1174, step time: 0.1054\n",
      "136/223, train_loss: 0.1102, step time: 0.0994\n",
      "137/223, train_loss: 0.1164, step time: 0.0992\n",
      "138/223, train_loss: 0.1045, step time: 0.1005\n",
      "139/223, train_loss: 0.1089, step time: 0.1004\n",
      "140/223, train_loss: 0.1167, step time: 0.1148\n",
      "141/223, train_loss: 0.1046, step time: 0.1068\n",
      "142/223, train_loss: 0.1102, step time: 0.1004\n",
      "143/223, train_loss: 0.1135, step time: 0.1314\n",
      "144/223, train_loss: 0.1111, step time: 0.1006\n",
      "145/223, train_loss: 0.1056, step time: 0.1063\n",
      "146/223, train_loss: 0.1072, step time: 0.1154\n",
      "147/223, train_loss: 0.1075, step time: 0.1000\n",
      "148/223, train_loss: 0.1050, step time: 0.1036\n",
      "149/223, train_loss: 0.1093, step time: 0.1001\n",
      "150/223, train_loss: 0.1115, step time: 0.0998\n",
      "151/223, train_loss: 0.1169, step time: 0.1002\n",
      "152/223, train_loss: 0.1165, step time: 0.1061\n",
      "153/223, train_loss: 0.1000, step time: 0.1132\n",
      "154/223, train_loss: 0.1015, step time: 0.1126\n",
      "155/223, train_loss: 0.1113, step time: 0.1099\n",
      "156/223, train_loss: 0.1115, step time: 0.1142\n",
      "157/223, train_loss: 0.1054, step time: 0.1176\n",
      "158/223, train_loss: 0.1101, step time: 0.1028\n",
      "159/223, train_loss: 0.1017, step time: 0.1061\n",
      "160/223, train_loss: 0.0985, step time: 0.1142\n",
      "161/223, train_loss: 0.1126, step time: 0.1188\n",
      "162/223, train_loss: 0.1109, step time: 0.1076\n",
      "163/223, train_loss: 0.0960, step time: 0.1117\n",
      "164/223, train_loss: 0.1157, step time: 0.1041\n",
      "165/223, train_loss: 0.0979, step time: 0.1243\n",
      "166/223, train_loss: 0.1151, step time: 0.1000\n",
      "167/223, train_loss: 0.1286, step time: 0.1172\n",
      "168/223, train_loss: 0.1099, step time: 0.1172\n",
      "169/223, train_loss: 0.1145, step time: 0.1137\n",
      "170/223, train_loss: 0.1182, step time: 0.1152\n",
      "171/223, train_loss: 0.1156, step time: 0.1125\n",
      "172/223, train_loss: 0.1042, step time: 0.1051\n",
      "173/223, train_loss: 0.1079, step time: 0.1071\n",
      "174/223, train_loss: 0.1134, step time: 0.1131\n",
      "175/223, train_loss: 0.1040, step time: 0.1131\n",
      "176/223, train_loss: 0.1082, step time: 0.1223\n",
      "177/223, train_loss: 0.1090, step time: 0.1072\n",
      "178/223, train_loss: 0.1220, step time: 0.1060\n",
      "179/223, train_loss: 0.1063, step time: 0.2284\n",
      "180/223, train_loss: 0.1037, step time: 0.1240\n",
      "181/223, train_loss: 0.1135, step time: 0.1114\n",
      "182/223, train_loss: 0.1070, step time: 0.1273\n",
      "183/223, train_loss: 0.1110, step time: 0.1308\n",
      "184/223, train_loss: 0.1061, step time: 0.1186\n",
      "185/223, train_loss: 0.1014, step time: 0.1085\n",
      "186/223, train_loss: 0.1058, step time: 0.1200\n",
      "187/223, train_loss: 0.0965, step time: 0.1378\n",
      "188/223, train_loss: 0.1115, step time: 0.1123\n",
      "189/223, train_loss: 0.1115, step time: 0.1083\n",
      "190/223, train_loss: 0.1129, step time: 0.1261\n",
      "191/223, train_loss: 0.1071, step time: 0.1248\n",
      "192/223, train_loss: 0.1014, step time: 0.1026\n",
      "193/223, train_loss: 0.1163, step time: 0.1094\n",
      "194/223, train_loss: 0.0994, step time: 0.1339\n",
      "195/223, train_loss: 0.1220, step time: 0.1566\n",
      "196/223, train_loss: 0.1088, step time: 0.1146\n",
      "197/223, train_loss: 0.1053, step time: 0.1120\n",
      "198/223, train_loss: 0.1087, step time: 0.1202\n",
      "199/223, train_loss: 0.1079, step time: 0.1073\n",
      "200/223, train_loss: 0.0978, step time: 0.1046\n",
      "201/223, train_loss: 0.1054, step time: 0.1179\n",
      "202/223, train_loss: 0.1138, step time: 0.1172\n",
      "203/223, train_loss: 0.1249, step time: 0.1003\n",
      "204/223, train_loss: 0.1125, step time: 0.0997\n",
      "205/223, train_loss: 0.1174, step time: 0.1288\n",
      "206/223, train_loss: 0.0989, step time: 0.1265\n",
      "207/223, train_loss: 0.1120, step time: 0.1001\n",
      "208/223, train_loss: 0.1111, step time: 0.1018\n",
      "209/223, train_loss: 0.1062, step time: 0.1038\n",
      "210/223, train_loss: 0.1123, step time: 0.1121\n",
      "211/223, train_loss: 0.1169, step time: 0.0999\n",
      "212/223, train_loss: 0.0989, step time: 0.1079\n",
      "213/223, train_loss: 0.1141, step time: 0.1235\n",
      "214/223, train_loss: 0.1140, step time: 0.0999\n",
      "215/223, train_loss: 0.1095, step time: 0.0992\n",
      "216/223, train_loss: 0.1058, step time: 0.1131\n",
      "217/223, train_loss: 0.1113, step time: 0.1054\n",
      "218/223, train_loss: 0.1071, step time: 0.1000\n",
      "219/223, train_loss: 0.1232, step time: 0.1007\n",
      "220/223, train_loss: 0.1140, step time: 0.1025\n",
      "221/223, train_loss: 0.1107, step time: 0.0993\n",
      "222/223, train_loss: 0.0988, step time: 0.0998\n",
      "223/223, train_loss: 0.1171, step time: 0.0996\n",
      "epoch 132 average loss: 0.1095\n",
      "time consuming of epoch 132 is: 87.7893\n",
      "----------\n",
      "epoch 133/300\n",
      "1/223, train_loss: 0.1090, step time: 0.1021\n",
      "2/223, train_loss: 0.1002, step time: 0.0992\n",
      "3/223, train_loss: 0.0990, step time: 0.0999\n",
      "4/223, train_loss: 0.0988, step time: 0.1009\n",
      "5/223, train_loss: 0.1090, step time: 0.1174\n",
      "6/223, train_loss: 0.1146, step time: 0.1086\n",
      "7/223, train_loss: 0.1018, step time: 0.1055\n",
      "8/223, train_loss: 0.1052, step time: 0.1002\n",
      "9/223, train_loss: 0.1163, step time: 0.0997\n",
      "10/223, train_loss: 0.1148, step time: 0.1003\n",
      "11/223, train_loss: 0.1060, step time: 0.1042\n",
      "12/223, train_loss: 0.1111, step time: 0.1118\n",
      "13/223, train_loss: 0.1106, step time: 0.0997\n",
      "14/223, train_loss: 0.1185, step time: 0.0997\n",
      "15/223, train_loss: 0.1003, step time: 0.0985\n",
      "16/223, train_loss: 0.1105, step time: 0.1000\n",
      "17/223, train_loss: 0.1057, step time: 0.1045\n",
      "18/223, train_loss: 0.1050, step time: 0.1004\n",
      "19/223, train_loss: 0.1155, step time: 0.0999\n",
      "20/223, train_loss: 0.1023, step time: 0.1003\n",
      "21/223, train_loss: 0.1076, step time: 0.1008\n",
      "22/223, train_loss: 0.1191, step time: 0.0995\n",
      "23/223, train_loss: 0.1076, step time: 0.0993\n",
      "24/223, train_loss: 0.1158, step time: 0.0995\n",
      "25/223, train_loss: 0.1132, step time: 0.1047\n",
      "26/223, train_loss: 0.0990, step time: 0.0998\n",
      "27/223, train_loss: 0.1042, step time: 0.0991\n",
      "28/223, train_loss: 0.1010, step time: 0.0989\n",
      "29/223, train_loss: 0.1096, step time: 0.1087\n",
      "30/223, train_loss: 0.0986, step time: 0.0995\n",
      "31/223, train_loss: 0.1121, step time: 0.0988\n",
      "32/223, train_loss: 0.1076, step time: 0.0991\n",
      "33/223, train_loss: 0.1084, step time: 0.1026\n",
      "34/223, train_loss: 0.1000, step time: 0.1002\n",
      "35/223, train_loss: 0.1158, step time: 0.1006\n",
      "36/223, train_loss: 0.1133, step time: 0.1002\n",
      "37/223, train_loss: 0.1129, step time: 0.1316\n",
      "38/223, train_loss: 0.1091, step time: 0.1027\n",
      "39/223, train_loss: 0.1049, step time: 0.1034\n",
      "40/223, train_loss: 0.1192, step time: 0.1408\n",
      "41/223, train_loss: 0.0957, step time: 0.1336\n",
      "42/223, train_loss: 0.1187, step time: 0.1028\n",
      "43/223, train_loss: 0.1089, step time: 0.1176\n",
      "44/223, train_loss: 0.1001, step time: 0.1107\n",
      "45/223, train_loss: 0.1040, step time: 0.1109\n",
      "46/223, train_loss: 0.1188, step time: 0.1035\n",
      "47/223, train_loss: 0.1083, step time: 0.1328\n",
      "48/223, train_loss: 0.1058, step time: 0.1222\n",
      "49/223, train_loss: 0.1053, step time: 0.1092\n",
      "50/223, train_loss: 0.1015, step time: 0.1132\n",
      "51/223, train_loss: 0.0988, step time: 0.1121\n",
      "52/223, train_loss: 0.1127, step time: 0.1049\n",
      "53/223, train_loss: 0.1127, step time: 0.1132\n",
      "54/223, train_loss: 0.1056, step time: 0.1148\n",
      "55/223, train_loss: 0.1076, step time: 0.1208\n",
      "56/223, train_loss: 0.1016, step time: 0.1003\n",
      "57/223, train_loss: 0.1078, step time: 0.1065\n",
      "58/223, train_loss: 0.1279, step time: 0.1136\n",
      "59/223, train_loss: 0.1161, step time: 0.1226\n",
      "60/223, train_loss: 0.1168, step time: 0.1097\n",
      "61/223, train_loss: 0.1059, step time: 0.1006\n",
      "62/223, train_loss: 0.0998, step time: 0.1067\n",
      "63/223, train_loss: 0.0980, step time: 0.1114\n",
      "64/223, train_loss: 0.1061, step time: 0.1004\n",
      "65/223, train_loss: 0.1114, step time: 0.1007\n",
      "66/223, train_loss: 0.1216, step time: 0.1091\n",
      "67/223, train_loss: 0.1089, step time: 0.1213\n",
      "68/223, train_loss: 0.1000, step time: 0.1026\n",
      "69/223, train_loss: 0.1005, step time: 0.1142\n",
      "70/223, train_loss: 0.0994, step time: 0.1193\n",
      "71/223, train_loss: 0.1071, step time: 0.1329\n",
      "72/223, train_loss: 0.1026, step time: 0.1294\n",
      "73/223, train_loss: 0.1063, step time: 0.1096\n",
      "74/223, train_loss: 0.1049, step time: 0.1052\n",
      "75/223, train_loss: 0.1127, step time: 0.0995\n",
      "76/223, train_loss: 0.1069, step time: 0.1052\n",
      "77/223, train_loss: 0.1100, step time: 0.1093\n",
      "78/223, train_loss: 0.1087, step time: 0.1092\n",
      "79/223, train_loss: 0.1160, step time: 0.1040\n",
      "80/223, train_loss: 0.1072, step time: 0.1048\n",
      "81/223, train_loss: 0.1109, step time: 0.1208\n",
      "82/223, train_loss: 0.1050, step time: 0.1126\n",
      "83/223, train_loss: 0.1034, step time: 0.1081\n",
      "84/223, train_loss: 0.1216, step time: 0.1019\n",
      "85/223, train_loss: 0.1205, step time: 0.1144\n",
      "86/223, train_loss: 0.1128, step time: 0.1196\n",
      "87/223, train_loss: 0.1123, step time: 0.1041\n",
      "88/223, train_loss: 0.1156, step time: 0.1287\n",
      "89/223, train_loss: 0.1065, step time: 0.1294\n",
      "90/223, train_loss: 0.1079, step time: 0.0997\n",
      "91/223, train_loss: 0.1113, step time: 0.0998\n",
      "92/223, train_loss: 0.1045, step time: 0.0996\n",
      "93/223, train_loss: 0.0986, step time: 0.1008\n",
      "94/223, train_loss: 0.1149, step time: 0.0992\n",
      "95/223, train_loss: 0.1024, step time: 0.0998\n",
      "96/223, train_loss: 0.1001, step time: 0.1003\n",
      "97/223, train_loss: 0.1044, step time: 0.1208\n",
      "98/223, train_loss: 0.1013, step time: 0.0998\n",
      "99/223, train_loss: 0.0964, step time: 0.0995\n",
      "100/223, train_loss: 0.1023, step time: 0.1000\n",
      "101/223, train_loss: 0.1233, step time: 0.1001\n",
      "102/223, train_loss: 0.1017, step time: 0.0997\n",
      "103/223, train_loss: 0.1097, step time: 0.0994\n",
      "104/223, train_loss: 0.1090, step time: 0.0991\n",
      "105/223, train_loss: 0.1097, step time: 0.1119\n",
      "106/223, train_loss: 0.1078, step time: 0.1004\n",
      "107/223, train_loss: 0.1092, step time: 0.0997\n",
      "108/223, train_loss: 0.1145, step time: 0.1001\n",
      "109/223, train_loss: 0.1077, step time: 0.1155\n",
      "110/223, train_loss: 0.0975, step time: 0.1089\n",
      "111/223, train_loss: 0.1091, step time: 0.1247\n",
      "112/223, train_loss: 0.1150, step time: 0.1110\n",
      "113/223, train_loss: 0.1146, step time: 0.1241\n",
      "114/223, train_loss: 0.0990, step time: 0.1044\n",
      "115/223, train_loss: 0.1008, step time: 0.1016\n",
      "116/223, train_loss: 0.1001, step time: 0.1041\n",
      "117/223, train_loss: 0.1008, step time: 0.1109\n",
      "118/223, train_loss: 0.1174, step time: 0.1135\n",
      "119/223, train_loss: 0.1118, step time: 0.1003\n",
      "120/223, train_loss: 0.1091, step time: 0.0999\n",
      "121/223, train_loss: 0.1058, step time: 0.1008\n",
      "122/223, train_loss: 0.1010, step time: 0.1110\n",
      "123/223, train_loss: 0.1055, step time: 0.1005\n",
      "124/223, train_loss: 0.1089, step time: 0.1145\n",
      "125/223, train_loss: 0.1076, step time: 0.1240\n",
      "126/223, train_loss: 0.1095, step time: 0.1113\n",
      "127/223, train_loss: 0.1011, step time: 0.1027\n",
      "128/223, train_loss: 0.1110, step time: 0.1118\n",
      "129/223, train_loss: 0.0958, step time: 0.1213\n",
      "130/223, train_loss: 0.1074, step time: 0.1037\n",
      "131/223, train_loss: 0.1074, step time: 0.1116\n",
      "132/223, train_loss: 0.1121, step time: 0.1206\n",
      "133/223, train_loss: 0.1047, step time: 0.1129\n",
      "134/223, train_loss: 0.1007, step time: 0.1306\n",
      "135/223, train_loss: 0.1052, step time: 0.1038\n",
      "136/223, train_loss: 0.1028, step time: 0.1037\n",
      "137/223, train_loss: 0.0969, step time: 0.1008\n",
      "138/223, train_loss: 0.1237, step time: 0.1151\n",
      "139/223, train_loss: 0.1036, step time: 0.1045\n",
      "140/223, train_loss: 0.1038, step time: 0.1024\n",
      "141/223, train_loss: 0.0918, step time: 0.1012\n",
      "142/223, train_loss: 0.1121, step time: 0.1081\n",
      "143/223, train_loss: 0.1190, step time: 0.1003\n",
      "144/223, train_loss: 0.1055, step time: 0.1501\n",
      "145/223, train_loss: 0.1021, step time: 0.1079\n",
      "146/223, train_loss: 0.1047, step time: 0.1155\n",
      "147/223, train_loss: 0.0991, step time: 0.1000\n",
      "148/223, train_loss: 0.1040, step time: 0.1133\n",
      "149/223, train_loss: 0.1012, step time: 0.1325\n",
      "150/223, train_loss: 0.1084, step time: 0.1090\n",
      "151/223, train_loss: 0.1148, step time: 0.1495\n",
      "152/223, train_loss: 0.1085, step time: 0.1171\n",
      "153/223, train_loss: 0.1082, step time: 0.1058\n",
      "154/223, train_loss: 0.1121, step time: 0.1028\n",
      "155/223, train_loss: 0.1057, step time: 0.1128\n",
      "156/223, train_loss: 0.1039, step time: 0.1245\n",
      "157/223, train_loss: 0.1107, step time: 0.1138\n",
      "158/223, train_loss: 0.0945, step time: 0.1098\n",
      "159/223, train_loss: 0.1157, step time: 0.1301\n",
      "160/223, train_loss: 0.1148, step time: 0.1153\n",
      "161/223, train_loss: 0.1227, step time: 0.1043\n",
      "162/223, train_loss: 0.1166, step time: 0.1132\n",
      "163/223, train_loss: 0.1196, step time: 0.1232\n",
      "164/223, train_loss: 0.1006, step time: 0.1134\n",
      "165/223, train_loss: 0.1067, step time: 0.1064\n",
      "166/223, train_loss: 0.0967, step time: 0.1142\n",
      "167/223, train_loss: 0.1053, step time: 0.1237\n",
      "168/223, train_loss: 0.1079, step time: 0.1129\n",
      "169/223, train_loss: 0.1239, step time: 0.1345\n",
      "170/223, train_loss: 0.1140, step time: 0.1222\n",
      "171/223, train_loss: 0.0957, step time: 0.1177\n",
      "172/223, train_loss: 0.1236, step time: 0.1184\n",
      "173/223, train_loss: 0.0979, step time: 0.1128\n",
      "174/223, train_loss: 0.1299, step time: 0.1084\n",
      "175/223, train_loss: 0.1022, step time: 0.1124\n",
      "176/223, train_loss: 0.1001, step time: 0.1117\n",
      "177/223, train_loss: 0.1042, step time: 0.1227\n",
      "178/223, train_loss: 0.0992, step time: 0.1174\n",
      "179/223, train_loss: 0.1343, step time: 0.1062\n",
      "180/223, train_loss: 0.1052, step time: 0.1230\n",
      "181/223, train_loss: 0.0987, step time: 0.1205\n",
      "182/223, train_loss: 0.1262, step time: 0.1015\n",
      "183/223, train_loss: 0.1178, step time: 0.0993\n",
      "184/223, train_loss: 0.1107, step time: 0.1000\n",
      "185/223, train_loss: 0.1136, step time: 0.1271\n",
      "186/223, train_loss: 0.1012, step time: 0.1003\n",
      "187/223, train_loss: 0.1181, step time: 0.0991\n",
      "188/223, train_loss: 0.1195, step time: 0.0997\n",
      "189/223, train_loss: 0.1128, step time: 0.1190\n",
      "190/223, train_loss: 0.1105, step time: 0.1018\n",
      "191/223, train_loss: 0.1103, step time: 0.1077\n",
      "192/223, train_loss: 0.0999, step time: 0.1213\n",
      "193/223, train_loss: 0.1150, step time: 0.1137\n",
      "194/223, train_loss: 0.1199, step time: 0.1167\n",
      "195/223, train_loss: 0.1002, step time: 0.1213\n",
      "196/223, train_loss: 0.1104, step time: 0.1150\n",
      "197/223, train_loss: 0.1282, step time: 0.1005\n",
      "198/223, train_loss: 0.1019, step time: 0.1082\n",
      "199/223, train_loss: 0.1167, step time: 0.1228\n",
      "200/223, train_loss: 0.0994, step time: 0.1011\n",
      "201/223, train_loss: 0.1246, step time: 0.0998\n",
      "202/223, train_loss: 0.3125, step time: 0.1021\n",
      "203/223, train_loss: 0.0900, step time: 0.1012\n",
      "204/223, train_loss: 0.1097, step time: 0.1188\n",
      "205/223, train_loss: 0.1057, step time: 0.0993\n",
      "206/223, train_loss: 0.1034, step time: 0.1117\n",
      "207/223, train_loss: 0.0971, step time: 0.1004\n",
      "208/223, train_loss: 0.1150, step time: 0.1012\n",
      "209/223, train_loss: 0.0984, step time: 0.1049\n",
      "210/223, train_loss: 0.1035, step time: 0.1186\n",
      "211/223, train_loss: 0.1004, step time: 0.1003\n",
      "212/223, train_loss: 0.1030, step time: 0.1197\n",
      "213/223, train_loss: 0.1017, step time: 0.1096\n",
      "214/223, train_loss: 0.1132, step time: 0.1013\n",
      "215/223, train_loss: 0.1036, step time: 0.0998\n",
      "216/223, train_loss: 0.1091, step time: 0.1026\n",
      "217/223, train_loss: 0.1073, step time: 0.1166\n",
      "218/223, train_loss: 0.1027, step time: 0.0991\n",
      "219/223, train_loss: 0.1161, step time: 0.1000\n",
      "220/223, train_loss: 0.1174, step time: 0.1003\n",
      "221/223, train_loss: 0.1081, step time: 0.1003\n",
      "222/223, train_loss: 0.1212, step time: 0.0991\n",
      "223/223, train_loss: 0.1213, step time: 0.1001\n",
      "epoch 133 average loss: 0.1091\n",
      "time consuming of epoch 133 is: 97.6205\n",
      "----------\n",
      "epoch 134/300\n",
      "1/223, train_loss: 0.1171, step time: 0.1084\n",
      "2/223, train_loss: 0.1087, step time: 0.1207\n",
      "3/223, train_loss: 0.1099, step time: 0.1128\n",
      "4/223, train_loss: 0.1056, step time: 0.1150\n",
      "5/223, train_loss: 0.1071, step time: 0.1004\n",
      "6/223, train_loss: 0.1205, step time: 0.1130\n",
      "7/223, train_loss: 0.1136, step time: 0.1232\n",
      "8/223, train_loss: 0.1156, step time: 0.1200\n",
      "9/223, train_loss: 0.1036, step time: 0.1017\n",
      "10/223, train_loss: 0.0989, step time: 0.1149\n",
      "11/223, train_loss: 0.1095, step time: 0.1045\n",
      "12/223, train_loss: 0.1083, step time: 0.1009\n",
      "13/223, train_loss: 0.1107, step time: 0.1125\n",
      "14/223, train_loss: 0.1053, step time: 0.1165\n",
      "15/223, train_loss: 0.1082, step time: 0.1137\n",
      "16/223, train_loss: 0.1003, step time: 0.1104\n",
      "17/223, train_loss: 0.0981, step time: 0.1159\n",
      "18/223, train_loss: 0.1079, step time: 0.1153\n",
      "19/223, train_loss: 0.1049, step time: 0.1200\n",
      "20/223, train_loss: 0.1025, step time: 0.1138\n",
      "21/223, train_loss: 0.1010, step time: 0.1130\n",
      "22/223, train_loss: 0.1097, step time: 0.1160\n",
      "23/223, train_loss: 0.1002, step time: 0.1139\n",
      "24/223, train_loss: 0.1166, step time: 0.1058\n",
      "25/223, train_loss: 0.1035, step time: 0.0999\n",
      "26/223, train_loss: 0.1059, step time: 0.1160\n",
      "27/223, train_loss: 0.1029, step time: 0.1157\n",
      "28/223, train_loss: 0.1014, step time: 0.1280\n",
      "29/223, train_loss: 0.1039, step time: 0.1186\n",
      "30/223, train_loss: 0.0954, step time: 0.1181\n",
      "31/223, train_loss: 0.1181, step time: 0.1274\n",
      "32/223, train_loss: 0.1162, step time: 0.1076\n",
      "33/223, train_loss: 0.1000, step time: 0.1083\n",
      "34/223, train_loss: 0.1063, step time: 0.0991\n",
      "35/223, train_loss: 0.1121, step time: 0.1003\n",
      "36/223, train_loss: 0.1060, step time: 0.1164\n",
      "37/223, train_loss: 0.1093, step time: 0.1070\n",
      "38/223, train_loss: 0.1035, step time: 0.1119\n",
      "39/223, train_loss: 0.1164, step time: 0.1562\n",
      "40/223, train_loss: 0.1037, step time: 0.1135\n",
      "41/223, train_loss: 0.1140, step time: 0.1070\n",
      "42/223, train_loss: 0.0987, step time: 0.1008\n",
      "43/223, train_loss: 0.1135, step time: 0.1140\n",
      "44/223, train_loss: 0.1090, step time: 0.1183\n",
      "45/223, train_loss: 0.1071, step time: 0.1056\n",
      "46/223, train_loss: 0.1196, step time: 0.1074\n",
      "47/223, train_loss: 0.1010, step time: 0.1135\n",
      "48/223, train_loss: 0.0954, step time: 0.1115\n",
      "49/223, train_loss: 0.1092, step time: 0.1099\n",
      "50/223, train_loss: 0.1114, step time: 0.1020\n",
      "51/223, train_loss: 0.1140, step time: 0.1221\n",
      "52/223, train_loss: 0.1105, step time: 0.1167\n",
      "53/223, train_loss: 0.1077, step time: 0.1098\n",
      "54/223, train_loss: 0.1104, step time: 0.1009\n",
      "55/223, train_loss: 0.1182, step time: 0.1159\n",
      "56/223, train_loss: 0.1081, step time: 0.1092\n",
      "57/223, train_loss: 0.1100, step time: 0.0992\n",
      "58/223, train_loss: 0.1000, step time: 0.1073\n",
      "59/223, train_loss: 0.0968, step time: 0.1274\n",
      "60/223, train_loss: 0.1039, step time: 0.1131\n",
      "61/223, train_loss: 0.1057, step time: 0.1106\n",
      "62/223, train_loss: 0.1156, step time: 0.1090\n",
      "63/223, train_loss: 0.1162, step time: 0.1111\n",
      "64/223, train_loss: 0.1064, step time: 0.1010\n",
      "65/223, train_loss: 0.1082, step time: 0.1006\n",
      "66/223, train_loss: 0.1013, step time: 0.1033\n",
      "67/223, train_loss: 0.1273, step time: 0.1086\n",
      "68/223, train_loss: 0.0983, step time: 0.1226\n",
      "69/223, train_loss: 0.1079, step time: 0.1001\n",
      "70/223, train_loss: 0.1061, step time: 0.1068\n",
      "71/223, train_loss: 0.0959, step time: 0.1233\n",
      "72/223, train_loss: 0.1167, step time: 0.1043\n",
      "73/223, train_loss: 0.1048, step time: 0.1157\n",
      "74/223, train_loss: 0.1120, step time: 0.1119\n",
      "75/223, train_loss: 0.1021, step time: 0.1056\n",
      "76/223, train_loss: 0.1063, step time: 0.1042\n",
      "77/223, train_loss: 0.1023, step time: 0.1015\n",
      "78/223, train_loss: 0.1092, step time: 0.1049\n",
      "79/223, train_loss: 0.2973, step time: 0.1006\n",
      "80/223, train_loss: 0.1044, step time: 0.1002\n",
      "81/223, train_loss: 0.1097, step time: 0.1098\n",
      "82/223, train_loss: 0.1213, step time: 0.1169\n",
      "83/223, train_loss: 0.1276, step time: 0.1023\n",
      "84/223, train_loss: 0.1086, step time: 0.1006\n",
      "85/223, train_loss: 0.1073, step time: 0.1088\n",
      "86/223, train_loss: 0.0922, step time: 0.1201\n",
      "87/223, train_loss: 0.0984, step time: 0.1126\n",
      "88/223, train_loss: 0.1168, step time: 0.1234\n",
      "89/223, train_loss: 0.1018, step time: 0.0998\n",
      "90/223, train_loss: 0.1216, step time: 0.1079\n",
      "91/223, train_loss: 0.1089, step time: 0.1120\n",
      "92/223, train_loss: 0.0929, step time: 0.1007\n",
      "93/223, train_loss: 0.0934, step time: 0.1000\n",
      "94/223, train_loss: 0.1023, step time: 0.1271\n",
      "95/223, train_loss: 0.1002, step time: 0.1308\n",
      "96/223, train_loss: 0.1169, step time: 0.1003\n",
      "97/223, train_loss: 0.1137, step time: 0.1010\n",
      "98/223, train_loss: 0.1013, step time: 0.1171\n",
      "99/223, train_loss: 0.0959, step time: 0.1104\n",
      "100/223, train_loss: 0.1051, step time: 0.1009\n",
      "101/223, train_loss: 0.1122, step time: 0.0996\n",
      "102/223, train_loss: 0.1200, step time: 0.1006\n",
      "103/223, train_loss: 0.1105, step time: 0.1154\n",
      "104/223, train_loss: 0.0995, step time: 0.1090\n",
      "105/223, train_loss: 0.1045, step time: 0.1034\n",
      "106/223, train_loss: 0.1118, step time: 0.1003\n",
      "107/223, train_loss: 0.1056, step time: 0.1133\n",
      "108/223, train_loss: 0.1020, step time: 0.1068\n",
      "109/223, train_loss: 0.0964, step time: 0.1128\n",
      "110/223, train_loss: 0.1153, step time: 0.1130\n",
      "111/223, train_loss: 0.1130, step time: 0.1039\n",
      "112/223, train_loss: 0.1121, step time: 0.1016\n",
      "113/223, train_loss: 0.1280, step time: 0.1310\n",
      "114/223, train_loss: 0.1239, step time: 0.1102\n",
      "115/223, train_loss: 0.1166, step time: 0.0998\n",
      "116/223, train_loss: 0.1056, step time: 0.1006\n",
      "117/223, train_loss: 0.1019, step time: 0.1005\n",
      "118/223, train_loss: 0.1208, step time: 0.0994\n",
      "119/223, train_loss: 0.0997, step time: 0.1081\n",
      "120/223, train_loss: 0.1085, step time: 0.1034\n",
      "121/223, train_loss: 0.1093, step time: 0.1011\n",
      "122/223, train_loss: 0.0977, step time: 0.1039\n",
      "123/223, train_loss: 0.1171, step time: 0.1142\n",
      "124/223, train_loss: 0.0986, step time: 0.1179\n",
      "125/223, train_loss: 0.1184, step time: 0.1222\n",
      "126/223, train_loss: 0.1034, step time: 0.1042\n",
      "127/223, train_loss: 0.1108, step time: 0.1246\n",
      "128/223, train_loss: 0.1050, step time: 0.1167\n",
      "129/223, train_loss: 0.1086, step time: 0.1292\n",
      "130/223, train_loss: 0.1087, step time: 0.0996\n",
      "131/223, train_loss: 0.1149, step time: 0.1009\n",
      "132/223, train_loss: 0.1040, step time: 0.1010\n",
      "133/223, train_loss: 0.1216, step time: 0.1008\n",
      "134/223, train_loss: 0.1004, step time: 0.1052\n",
      "135/223, train_loss: 0.1039, step time: 0.1005\n",
      "136/223, train_loss: 0.1002, step time: 0.1001\n",
      "137/223, train_loss: 0.1118, step time: 0.1012\n",
      "138/223, train_loss: 0.1044, step time: 0.1035\n",
      "139/223, train_loss: 0.1046, step time: 0.1132\n",
      "140/223, train_loss: 0.1155, step time: 0.1218\n",
      "141/223, train_loss: 0.1093, step time: 0.1058\n",
      "142/223, train_loss: 0.1143, step time: 0.1177\n",
      "143/223, train_loss: 0.1037, step time: 0.1053\n",
      "144/223, train_loss: 0.1154, step time: 0.1245\n",
      "145/223, train_loss: 0.1021, step time: 0.1001\n",
      "146/223, train_loss: 0.1013, step time: 0.1399\n",
      "147/223, train_loss: 0.1163, step time: 0.1491\n",
      "148/223, train_loss: 0.1270, step time: 0.1231\n",
      "149/223, train_loss: 0.1103, step time: 0.1135\n",
      "150/223, train_loss: 0.1120, step time: 0.1188\n",
      "151/223, train_loss: 0.1080, step time: 0.1095\n",
      "152/223, train_loss: 0.1225, step time: 0.1265\n",
      "153/223, train_loss: 0.1169, step time: 0.1138\n",
      "154/223, train_loss: 0.1226, step time: 0.1320\n",
      "155/223, train_loss: 0.1125, step time: 0.1368\n",
      "156/223, train_loss: 0.1151, step time: 0.1155\n",
      "157/223, train_loss: 0.1158, step time: 0.1005\n",
      "158/223, train_loss: 0.1136, step time: 0.1138\n",
      "159/223, train_loss: 0.1061, step time: 0.1101\n",
      "160/223, train_loss: 0.1036, step time: 0.1247\n",
      "161/223, train_loss: 0.1065, step time: 0.1103\n",
      "162/223, train_loss: 0.1135, step time: 0.1083\n",
      "163/223, train_loss: 0.1133, step time: 0.1186\n",
      "164/223, train_loss: 0.1093, step time: 0.1294\n",
      "165/223, train_loss: 0.1076, step time: 0.1100\n",
      "166/223, train_loss: 0.1034, step time: 0.1304\n",
      "167/223, train_loss: 0.1180, step time: 0.1095\n",
      "168/223, train_loss: 0.1001, step time: 0.1100\n",
      "169/223, train_loss: 0.1075, step time: 0.1034\n",
      "170/223, train_loss: 0.1135, step time: 0.1104\n",
      "171/223, train_loss: 0.1060, step time: 0.1080\n",
      "172/223, train_loss: 0.1165, step time: 0.1380\n",
      "173/223, train_loss: 0.1030, step time: 0.1124\n",
      "174/223, train_loss: 0.1048, step time: 0.1044\n",
      "175/223, train_loss: 0.1060, step time: 0.1320\n",
      "176/223, train_loss: 0.1035, step time: 0.1019\n",
      "177/223, train_loss: 0.1128, step time: 0.1029\n",
      "178/223, train_loss: 0.1166, step time: 0.1182\n",
      "179/223, train_loss: 0.0967, step time: 0.1066\n",
      "180/223, train_loss: 0.1010, step time: 0.1279\n",
      "181/223, train_loss: 0.1023, step time: 0.0995\n",
      "182/223, train_loss: 0.1107, step time: 0.1338\n",
      "183/223, train_loss: 0.1127, step time: 0.1155\n",
      "184/223, train_loss: 0.1000, step time: 0.1145\n",
      "185/223, train_loss: 0.1043, step time: 0.0998\n",
      "186/223, train_loss: 0.1121, step time: 0.1172\n",
      "187/223, train_loss: 0.1130, step time: 0.1109\n",
      "188/223, train_loss: 0.0943, step time: 0.1178\n",
      "189/223, train_loss: 0.1121, step time: 0.1100\n",
      "190/223, train_loss: 0.0998, step time: 0.1120\n",
      "191/223, train_loss: 0.0945, step time: 0.1098\n",
      "192/223, train_loss: 0.1080, step time: 0.1163\n",
      "193/223, train_loss: 0.1079, step time: 0.1118\n",
      "194/223, train_loss: 0.1074, step time: 0.1075\n",
      "195/223, train_loss: 0.1141, step time: 0.0997\n",
      "196/223, train_loss: 0.1091, step time: 0.0998\n",
      "197/223, train_loss: 0.1127, step time: 0.1121\n",
      "198/223, train_loss: 0.1080, step time: 0.1060\n",
      "199/223, train_loss: 0.1211, step time: 0.1058\n",
      "200/223, train_loss: 0.1020, step time: 0.1020\n",
      "201/223, train_loss: 0.1002, step time: 0.1045\n",
      "202/223, train_loss: 0.1070, step time: 0.1071\n",
      "203/223, train_loss: 0.1041, step time: 0.1238\n",
      "204/223, train_loss: 0.0996, step time: 0.0995\n",
      "205/223, train_loss: 0.1050, step time: 0.1005\n",
      "206/223, train_loss: 0.1176, step time: 0.1115\n",
      "207/223, train_loss: 0.1112, step time: 0.1000\n",
      "208/223, train_loss: 0.1167, step time: 0.1045\n",
      "209/223, train_loss: 0.1241, step time: 0.1058\n",
      "210/223, train_loss: 0.0983, step time: 0.1082\n",
      "211/223, train_loss: 0.1017, step time: 0.1402\n",
      "212/223, train_loss: 0.1034, step time: 0.1006\n",
      "213/223, train_loss: 0.1182, step time: 0.1040\n",
      "214/223, train_loss: 0.1160, step time: 0.1244\n",
      "215/223, train_loss: 0.1092, step time: 0.1028\n",
      "216/223, train_loss: 0.1140, step time: 0.1049\n",
      "217/223, train_loss: 0.1065, step time: 0.1008\n",
      "218/223, train_loss: 0.1083, step time: 0.1001\n",
      "219/223, train_loss: 0.1020, step time: 0.1001\n",
      "220/223, train_loss: 0.1206, step time: 0.0996\n",
      "221/223, train_loss: 0.1038, step time: 0.1002\n",
      "222/223, train_loss: 0.0992, step time: 0.1000\n",
      "223/223, train_loss: 0.1022, step time: 0.0995\n",
      "epoch 134 average loss: 0.1091\n",
      "time consuming of epoch 134 is: 86.8571\n",
      "----------\n",
      "epoch 135/300\n",
      "1/223, train_loss: 0.1101, step time: 0.1011\n",
      "2/223, train_loss: 0.1034, step time: 0.1003\n",
      "3/223, train_loss: 0.1156, step time: 0.1012\n",
      "4/223, train_loss: 0.0999, step time: 0.1003\n",
      "5/223, train_loss: 0.0962, step time: 0.1061\n",
      "6/223, train_loss: 0.1029, step time: 0.1167\n",
      "7/223, train_loss: 0.1104, step time: 0.1226\n",
      "8/223, train_loss: 0.1101, step time: 0.1073\n",
      "9/223, train_loss: 0.1037, step time: 0.1111\n",
      "10/223, train_loss: 0.1042, step time: 0.1147\n",
      "11/223, train_loss: 0.0967, step time: 0.1136\n",
      "12/223, train_loss: 0.1011, step time: 0.1093\n",
      "13/223, train_loss: 0.0986, step time: 0.1063\n",
      "14/223, train_loss: 0.0990, step time: 0.1002\n",
      "15/223, train_loss: 0.1207, step time: 0.1016\n",
      "16/223, train_loss: 0.1033, step time: 0.1004\n",
      "17/223, train_loss: 0.1071, step time: 0.1164\n",
      "18/223, train_loss: 0.1036, step time: 0.1025\n",
      "19/223, train_loss: 0.1051, step time: 0.1054\n",
      "20/223, train_loss: 0.1037, step time: 0.1009\n",
      "21/223, train_loss: 0.1156, step time: 0.1001\n",
      "22/223, train_loss: 0.1149, step time: 0.1149\n",
      "23/223, train_loss: 0.1301, step time: 0.1014\n",
      "24/223, train_loss: 0.1019, step time: 0.1007\n",
      "25/223, train_loss: 0.1194, step time: 0.1067\n",
      "26/223, train_loss: 0.1007, step time: 0.1186\n",
      "27/223, train_loss: 0.1039, step time: 0.1002\n",
      "28/223, train_loss: 0.1073, step time: 0.1234\n",
      "29/223, train_loss: 0.1217, step time: 0.1184\n",
      "30/223, train_loss: 0.1047, step time: 0.1174\n",
      "31/223, train_loss: 0.0939, step time: 0.1053\n",
      "32/223, train_loss: 0.1146, step time: 0.1062\n",
      "33/223, train_loss: 0.1168, step time: 0.1004\n",
      "34/223, train_loss: 0.1070, step time: 0.1033\n",
      "35/223, train_loss: 0.0999, step time: 0.0995\n",
      "36/223, train_loss: 0.1070, step time: 0.0998\n",
      "37/223, train_loss: 0.1188, step time: 0.1008\n",
      "38/223, train_loss: 0.1148, step time: 0.1038\n",
      "39/223, train_loss: 0.1007, step time: 0.1211\n",
      "40/223, train_loss: 0.1047, step time: 0.1069\n",
      "41/223, train_loss: 0.1169, step time: 0.1086\n",
      "42/223, train_loss: 0.0977, step time: 0.1112\n",
      "43/223, train_loss: 0.1071, step time: 0.1004\n",
      "44/223, train_loss: 0.1323, step time: 0.1030\n",
      "45/223, train_loss: 0.1108, step time: 0.1217\n",
      "46/223, train_loss: 0.1012, step time: 0.1124\n",
      "47/223, train_loss: 0.1084, step time: 0.1020\n",
      "48/223, train_loss: 0.1186, step time: 0.1115\n",
      "49/223, train_loss: 0.1132, step time: 0.1010\n",
      "50/223, train_loss: 0.1024, step time: 0.1164\n",
      "51/223, train_loss: 0.1106, step time: 0.1304\n",
      "52/223, train_loss: 0.1091, step time: 0.1035\n",
      "53/223, train_loss: 0.1090, step time: 0.1080\n",
      "54/223, train_loss: 0.1116, step time: 0.1018\n",
      "55/223, train_loss: 0.1057, step time: 0.1110\n",
      "56/223, train_loss: 0.1025, step time: 0.1002\n",
      "57/223, train_loss: 0.0967, step time: 0.1004\n",
      "58/223, train_loss: 0.1021, step time: 0.1049\n",
      "59/223, train_loss: 0.0976, step time: 0.1020\n",
      "60/223, train_loss: 0.1093, step time: 0.1403\n",
      "61/223, train_loss: 0.1214, step time: 0.1048\n",
      "62/223, train_loss: 0.1085, step time: 0.1334\n",
      "63/223, train_loss: 0.1132, step time: 0.1025\n",
      "64/223, train_loss: 0.1268, step time: 0.1127\n",
      "65/223, train_loss: 0.1109, step time: 0.1110\n",
      "66/223, train_loss: 0.1067, step time: 0.1193\n",
      "67/223, train_loss: 0.1203, step time: 0.1006\n",
      "68/223, train_loss: 0.1054, step time: 0.1017\n",
      "69/223, train_loss: 0.1117, step time: 0.1005\n",
      "70/223, train_loss: 0.1065, step time: 0.1092\n",
      "71/223, train_loss: 0.1195, step time: 0.1062\n",
      "72/223, train_loss: 0.1268, step time: 0.1008\n",
      "73/223, train_loss: 0.1096, step time: 0.1006\n",
      "74/223, train_loss: 0.1102, step time: 0.1371\n",
      "75/223, train_loss: 0.1040, step time: 0.1356\n",
      "76/223, train_loss: 0.1180, step time: 0.1426\n",
      "77/223, train_loss: 0.0957, step time: 0.1005\n",
      "78/223, train_loss: 0.0958, step time: 0.1159\n",
      "79/223, train_loss: 0.1106, step time: 0.1114\n",
      "80/223, train_loss: 0.1117, step time: 0.1033\n",
      "81/223, train_loss: 0.1106, step time: 0.1329\n",
      "82/223, train_loss: 0.1042, step time: 0.1268\n",
      "83/223, train_loss: 0.1152, step time: 0.1284\n",
      "84/223, train_loss: 0.1150, step time: 0.1007\n",
      "85/223, train_loss: 0.1081, step time: 0.1008\n",
      "86/223, train_loss: 0.1138, step time: 0.1116\n",
      "87/223, train_loss: 0.1187, step time: 0.1003\n",
      "88/223, train_loss: 0.1041, step time: 0.1183\n",
      "89/223, train_loss: 0.1047, step time: 0.1160\n",
      "90/223, train_loss: 0.1183, step time: 0.1242\n",
      "91/223, train_loss: 0.1042, step time: 0.1301\n",
      "92/223, train_loss: 0.1179, step time: 0.1001\n",
      "93/223, train_loss: 0.0956, step time: 0.1093\n",
      "94/223, train_loss: 0.1261, step time: 0.1238\n",
      "95/223, train_loss: 0.1091, step time: 0.0995\n",
      "96/223, train_loss: 0.1159, step time: 0.1000\n",
      "97/223, train_loss: 0.1118, step time: 0.1000\n",
      "98/223, train_loss: 0.1107, step time: 0.1102\n",
      "99/223, train_loss: 0.1308, step time: 0.1102\n",
      "100/223, train_loss: 0.1137, step time: 0.1110\n",
      "101/223, train_loss: 0.1022, step time: 0.1038\n",
      "102/223, train_loss: 0.0930, step time: 0.1260\n",
      "103/223, train_loss: 0.1061, step time: 0.1182\n",
      "104/223, train_loss: 0.1195, step time: 0.1144\n",
      "105/223, train_loss: 0.1162, step time: 0.1013\n",
      "106/223, train_loss: 0.1059, step time: 0.1211\n",
      "107/223, train_loss: 0.0995, step time: 0.1232\n",
      "108/223, train_loss: 0.1056, step time: 0.1012\n",
      "109/223, train_loss: 0.1024, step time: 0.1087\n",
      "110/223, train_loss: 0.1005, step time: 0.1047\n",
      "111/223, train_loss: 0.1063, step time: 0.1145\n",
      "112/223, train_loss: 0.1160, step time: 0.1080\n",
      "113/223, train_loss: 0.1135, step time: 0.1005\n",
      "114/223, train_loss: 0.1257, step time: 0.1124\n",
      "115/223, train_loss: 0.1106, step time: 0.1003\n",
      "116/223, train_loss: 0.1046, step time: 0.0996\n",
      "117/223, train_loss: 0.1096, step time: 0.1088\n",
      "118/223, train_loss: 0.1152, step time: 0.1186\n",
      "119/223, train_loss: 0.1034, step time: 0.1147\n",
      "120/223, train_loss: 0.1026, step time: 0.1060\n",
      "121/223, train_loss: 0.1123, step time: 0.1154\n",
      "122/223, train_loss: 0.1052, step time: 0.0997\n",
      "123/223, train_loss: 0.1112, step time: 0.0996\n",
      "124/223, train_loss: 0.1006, step time: 0.0995\n",
      "125/223, train_loss: 0.1237, step time: 0.1012\n",
      "126/223, train_loss: 0.0989, step time: 0.1287\n",
      "127/223, train_loss: 0.1089, step time: 0.1093\n",
      "128/223, train_loss: 0.1098, step time: 0.1189\n",
      "129/223, train_loss: 0.1087, step time: 0.1119\n",
      "130/223, train_loss: 0.0982, step time: 0.1200\n",
      "131/223, train_loss: 0.1104, step time: 0.1025\n",
      "132/223, train_loss: 0.0977, step time: 0.1202\n",
      "133/223, train_loss: 0.1048, step time: 0.1232\n",
      "134/223, train_loss: 0.0990, step time: 0.1164\n",
      "135/223, train_loss: 0.1033, step time: 0.1115\n",
      "136/223, train_loss: 0.1132, step time: 0.1218\n",
      "137/223, train_loss: 0.0993, step time: 0.1129\n",
      "138/223, train_loss: 0.1082, step time: 0.1014\n",
      "139/223, train_loss: 0.1045, step time: 0.1018\n",
      "140/223, train_loss: 0.1024, step time: 0.1273\n",
      "141/223, train_loss: 0.1050, step time: 0.0999\n",
      "142/223, train_loss: 0.1238, step time: 0.1041\n",
      "143/223, train_loss: 0.1059, step time: 0.1043\n",
      "144/223, train_loss: 0.1058, step time: 0.1056\n",
      "145/223, train_loss: 0.1085, step time: 0.1008\n",
      "146/223, train_loss: 0.0966, step time: 0.1260\n",
      "147/223, train_loss: 0.1185, step time: 0.1024\n",
      "148/223, train_loss: 0.1019, step time: 0.1349\n",
      "149/223, train_loss: 0.1057, step time: 0.1002\n",
      "150/223, train_loss: 0.1108, step time: 0.1210\n",
      "151/223, train_loss: 0.0985, step time: 0.1090\n",
      "152/223, train_loss: 0.1103, step time: 0.1185\n",
      "153/223, train_loss: 0.0997, step time: 0.1249\n",
      "154/223, train_loss: 0.1008, step time: 0.1086\n",
      "155/223, train_loss: 0.1008, step time: 0.1189\n",
      "156/223, train_loss: 0.0999, step time: 0.0995\n",
      "157/223, train_loss: 0.1137, step time: 0.1001\n",
      "158/223, train_loss: 0.1022, step time: 0.1073\n",
      "159/223, train_loss: 0.1101, step time: 0.1125\n",
      "160/223, train_loss: 0.1050, step time: 0.1091\n",
      "161/223, train_loss: 0.1094, step time: 0.0992\n",
      "162/223, train_loss: 0.1189, step time: 0.1006\n",
      "163/223, train_loss: 0.3016, step time: 0.1010\n",
      "164/223, train_loss: 0.1024, step time: 0.0992\n",
      "165/223, train_loss: 0.0993, step time: 0.1132\n",
      "166/223, train_loss: 0.1112, step time: 0.1187\n",
      "167/223, train_loss: 0.1116, step time: 0.1357\n",
      "168/223, train_loss: 0.1119, step time: 0.1034\n",
      "169/223, train_loss: 0.1113, step time: 0.0998\n",
      "170/223, train_loss: 0.1056, step time: 0.1092\n",
      "171/223, train_loss: 0.1308, step time: 0.1059\n",
      "172/223, train_loss: 0.1067, step time: 0.1066\n",
      "173/223, train_loss: 0.1066, step time: 0.1098\n",
      "174/223, train_loss: 0.1094, step time: 0.1143\n",
      "175/223, train_loss: 0.1052, step time: 0.1275\n",
      "176/223, train_loss: 0.1053, step time: 0.1169\n",
      "177/223, train_loss: 0.1141, step time: 0.1011\n",
      "178/223, train_loss: 0.1016, step time: 0.1004\n",
      "179/223, train_loss: 0.1065, step time: 0.1023\n",
      "180/223, train_loss: 0.1106, step time: 0.0994\n",
      "181/223, train_loss: 0.1061, step time: 0.1024\n",
      "182/223, train_loss: 0.1022, step time: 0.1005\n",
      "183/223, train_loss: 0.1028, step time: 0.1042\n",
      "184/223, train_loss: 0.1047, step time: 0.1260\n",
      "185/223, train_loss: 0.1008, step time: 0.1009\n",
      "186/223, train_loss: 0.1059, step time: 0.1013\n",
      "187/223, train_loss: 0.1031, step time: 0.0999\n",
      "188/223, train_loss: 0.1129, step time: 0.1030\n",
      "189/223, train_loss: 0.1027, step time: 0.1347\n",
      "190/223, train_loss: 0.1034, step time: 0.0998\n",
      "191/223, train_loss: 0.1054, step time: 0.1360\n",
      "192/223, train_loss: 0.1048, step time: 0.1139\n",
      "193/223, train_loss: 0.0961, step time: 0.1014\n",
      "194/223, train_loss: 0.1056, step time: 0.1008\n",
      "195/223, train_loss: 0.1259, step time: 0.1001\n",
      "196/223, train_loss: 0.1041, step time: 0.1267\n",
      "197/223, train_loss: 0.1108, step time: 0.1057\n",
      "198/223, train_loss: 0.1065, step time: 0.1036\n",
      "199/223, train_loss: 0.1089, step time: 0.1101\n",
      "200/223, train_loss: 0.0959, step time: 0.1190\n",
      "201/223, train_loss: 0.1048, step time: 0.1364\n",
      "202/223, train_loss: 0.1147, step time: 0.1113\n",
      "203/223, train_loss: 0.1127, step time: 0.1080\n",
      "204/223, train_loss: 0.1060, step time: 0.0997\n",
      "205/223, train_loss: 0.1120, step time: 0.1149\n",
      "206/223, train_loss: 0.1207, step time: 0.1149\n",
      "207/223, train_loss: 0.1145, step time: 0.1101\n",
      "208/223, train_loss: 0.1150, step time: 0.1177\n",
      "209/223, train_loss: 0.1130, step time: 0.1100\n",
      "210/223, train_loss: 0.1098, step time: 0.1146\n",
      "211/223, train_loss: 0.0994, step time: 0.1133\n",
      "212/223, train_loss: 0.1002, step time: 0.1157\n",
      "213/223, train_loss: 0.1098, step time: 0.1004\n",
      "214/223, train_loss: 0.1080, step time: 0.0996\n",
      "215/223, train_loss: 0.1136, step time: 0.1050\n",
      "216/223, train_loss: 0.1013, step time: 0.1058\n",
      "217/223, train_loss: 0.1060, step time: 0.1048\n",
      "218/223, train_loss: 0.1192, step time: 0.0994\n",
      "219/223, train_loss: 0.1042, step time: 0.1003\n",
      "220/223, train_loss: 0.1104, step time: 0.1004\n",
      "221/223, train_loss: 0.1004, step time: 0.0997\n",
      "222/223, train_loss: 0.1067, step time: 0.0995\n",
      "223/223, train_loss: 0.1082, step time: 0.0997\n",
      "epoch 135 average loss: 0.1091\n",
      "current epoch: 135 current mean dice: 0.8536 tc: 0.9178 wt: 0.8642 et: 0.7789\n",
      "best mean dice: 0.8551 at epoch: 125\n",
      "time consuming of epoch 135 is: 89.0935\n",
      "----------\n",
      "epoch 136/300\n",
      "1/223, train_loss: 0.1114, step time: 0.1258\n",
      "2/223, train_loss: 0.0994, step time: 0.1129\n",
      "3/223, train_loss: 0.0992, step time: 0.1164\n",
      "4/223, train_loss: 0.0998, step time: 0.1119\n",
      "5/223, train_loss: 0.1137, step time: 0.1208\n",
      "6/223, train_loss: 0.1137, step time: 0.1277\n",
      "7/223, train_loss: 0.1036, step time: 0.1177\n",
      "8/223, train_loss: 0.1064, step time: 0.1251\n",
      "9/223, train_loss: 0.1104, step time: 0.1121\n",
      "10/223, train_loss: 0.0957, step time: 0.1231\n",
      "11/223, train_loss: 0.1148, step time: 0.1163\n",
      "12/223, train_loss: 0.1268, step time: 0.0998\n",
      "13/223, train_loss: 0.1062, step time: 0.1128\n",
      "14/223, train_loss: 0.1060, step time: 0.1050\n",
      "15/223, train_loss: 0.1139, step time: 0.1137\n",
      "16/223, train_loss: 0.0967, step time: 0.1063\n",
      "17/223, train_loss: 0.1067, step time: 0.1045\n",
      "18/223, train_loss: 0.1013, step time: 0.1130\n",
      "19/223, train_loss: 0.1046, step time: 0.1205\n",
      "20/223, train_loss: 0.1031, step time: 0.1106\n",
      "21/223, train_loss: 0.0962, step time: 0.1197\n",
      "22/223, train_loss: 0.0998, step time: 0.1072\n",
      "23/223, train_loss: 0.1021, step time: 0.1149\n",
      "24/223, train_loss: 0.1094, step time: 0.1057\n",
      "25/223, train_loss: 0.1082, step time: 0.1141\n",
      "26/223, train_loss: 0.1000, step time: 0.1254\n",
      "27/223, train_loss: 0.0980, step time: 0.1000\n",
      "28/223, train_loss: 0.0987, step time: 0.1008\n",
      "29/223, train_loss: 0.1021, step time: 0.1195\n",
      "30/223, train_loss: 0.1139, step time: 0.1096\n",
      "31/223, train_loss: 0.0966, step time: 0.1115\n",
      "32/223, train_loss: 0.1077, step time: 0.1232\n",
      "33/223, train_loss: 0.1112, step time: 0.1138\n",
      "34/223, train_loss: 0.1148, step time: 0.1133\n",
      "35/223, train_loss: 0.1242, step time: 0.1000\n",
      "36/223, train_loss: 0.1129, step time: 0.1024\n",
      "37/223, train_loss: 0.0964, step time: 0.1074\n",
      "38/223, train_loss: 0.1051, step time: 0.1074\n",
      "39/223, train_loss: 0.1093, step time: 0.0999\n",
      "40/223, train_loss: 0.1084, step time: 0.1448\n",
      "41/223, train_loss: 0.1042, step time: 0.1164\n",
      "42/223, train_loss: 0.0922, step time: 0.1082\n",
      "43/223, train_loss: 0.3155, step time: 0.1002\n",
      "44/223, train_loss: 0.1210, step time: 0.1040\n",
      "45/223, train_loss: 0.1160, step time: 0.1064\n",
      "46/223, train_loss: 0.1183, step time: 0.1010\n",
      "47/223, train_loss: 0.1153, step time: 0.1232\n",
      "48/223, train_loss: 0.1125, step time: 0.1199\n",
      "49/223, train_loss: 0.1158, step time: 0.1145\n",
      "50/223, train_loss: 0.1034, step time: 0.0995\n",
      "51/223, train_loss: 0.1266, step time: 0.1065\n",
      "52/223, train_loss: 0.1041, step time: 0.1068\n",
      "53/223, train_loss: 0.1098, step time: 0.1194\n",
      "54/223, train_loss: 0.0986, step time: 0.1246\n",
      "55/223, train_loss: 0.1080, step time: 0.1056\n",
      "56/223, train_loss: 0.1072, step time: 0.1010\n",
      "57/223, train_loss: 0.1125, step time: 0.1039\n",
      "58/223, train_loss: 0.0996, step time: 0.1143\n",
      "59/223, train_loss: 0.1204, step time: 0.1057\n",
      "60/223, train_loss: 0.1022, step time: 0.1046\n",
      "61/223, train_loss: 0.1228, step time: 0.1151\n",
      "62/223, train_loss: 0.1180, step time: 0.1144\n",
      "63/223, train_loss: 0.1203, step time: 0.0999\n",
      "64/223, train_loss: 0.1109, step time: 0.1111\n",
      "65/223, train_loss: 0.1077, step time: 0.1001\n",
      "66/223, train_loss: 0.1096, step time: 0.1329\n",
      "67/223, train_loss: 0.1114, step time: 0.1041\n",
      "68/223, train_loss: 0.1021, step time: 0.1125\n",
      "69/223, train_loss: 0.1167, step time: 0.1238\n",
      "70/223, train_loss: 0.1053, step time: 0.1132\n",
      "71/223, train_loss: 0.1018, step time: 0.1004\n",
      "72/223, train_loss: 0.1031, step time: 0.1008\n",
      "73/223, train_loss: 0.1128, step time: 0.1005\n",
      "74/223, train_loss: 0.1070, step time: 0.1030\n",
      "75/223, train_loss: 0.1243, step time: 0.1155\n",
      "76/223, train_loss: 0.1175, step time: 0.1001\n",
      "77/223, train_loss: 0.1102, step time: 0.1203\n",
      "78/223, train_loss: 0.1024, step time: 0.1001\n",
      "79/223, train_loss: 0.1117, step time: 0.1155\n",
      "80/223, train_loss: 0.1011, step time: 0.1188\n",
      "81/223, train_loss: 0.1233, step time: 0.0996\n",
      "82/223, train_loss: 0.1039, step time: 0.1307\n",
      "83/223, train_loss: 0.1124, step time: 0.1021\n",
      "84/223, train_loss: 0.1026, step time: 0.1005\n",
      "85/223, train_loss: 0.1035, step time: 0.0995\n",
      "86/223, train_loss: 0.1034, step time: 0.1061\n",
      "87/223, train_loss: 0.1006, step time: 0.1079\n",
      "88/223, train_loss: 0.1009, step time: 0.1121\n",
      "89/223, train_loss: 0.1086, step time: 0.1015\n",
      "90/223, train_loss: 0.1054, step time: 0.1055\n",
      "91/223, train_loss: 0.1090, step time: 0.1127\n",
      "92/223, train_loss: 0.1080, step time: 0.1077\n",
      "93/223, train_loss: 0.0998, step time: 0.1269\n",
      "94/223, train_loss: 0.0984, step time: 0.1204\n",
      "95/223, train_loss: 0.1265, step time: 0.1100\n",
      "96/223, train_loss: 0.1167, step time: 0.1128\n",
      "97/223, train_loss: 0.1066, step time: 0.1376\n",
      "98/223, train_loss: 0.1051, step time: 0.1275\n",
      "99/223, train_loss: 0.1167, step time: 0.1106\n",
      "100/223, train_loss: 0.0993, step time: 0.1062\n",
      "101/223, train_loss: 0.1145, step time: 0.1005\n",
      "102/223, train_loss: 0.1064, step time: 0.1159\n",
      "103/223, train_loss: 0.1027, step time: 0.1004\n",
      "104/223, train_loss: 0.1156, step time: 0.1001\n",
      "105/223, train_loss: 0.1107, step time: 0.1121\n",
      "106/223, train_loss: 0.1027, step time: 0.1174\n",
      "107/223, train_loss: 0.1041, step time: 0.1118\n",
      "108/223, train_loss: 0.1088, step time: 0.1136\n",
      "109/223, train_loss: 0.1095, step time: 0.1006\n",
      "110/223, train_loss: 0.0988, step time: 0.1267\n",
      "111/223, train_loss: 0.1083, step time: 0.1048\n",
      "112/223, train_loss: 0.1053, step time: 0.1011\n",
      "113/223, train_loss: 0.1239, step time: 0.1015\n",
      "114/223, train_loss: 0.1018, step time: 0.1212\n",
      "115/223, train_loss: 0.1191, step time: 0.1007\n",
      "116/223, train_loss: 0.1008, step time: 0.1213\n",
      "117/223, train_loss: 0.1175, step time: 0.1059\n",
      "118/223, train_loss: 0.1034, step time: 0.1029\n",
      "119/223, train_loss: 0.1150, step time: 0.0996\n",
      "120/223, train_loss: 0.1177, step time: 0.1002\n",
      "121/223, train_loss: 0.1066, step time: 0.1156\n",
      "122/223, train_loss: 0.1167, step time: 0.1064\n",
      "123/223, train_loss: 0.1125, step time: 0.1280\n",
      "124/223, train_loss: 0.1022, step time: 0.1013\n",
      "125/223, train_loss: 0.1015, step time: 0.1008\n",
      "126/223, train_loss: 0.1043, step time: 0.1018\n",
      "127/223, train_loss: 0.1112, step time: 0.1133\n",
      "128/223, train_loss: 0.1153, step time: 0.1167\n",
      "129/223, train_loss: 0.1152, step time: 0.1239\n",
      "130/223, train_loss: 0.1156, step time: 0.1047\n",
      "131/223, train_loss: 0.1118, step time: 0.1059\n",
      "132/223, train_loss: 0.1106, step time: 0.1119\n",
      "133/223, train_loss: 0.1080, step time: 0.1021\n",
      "134/223, train_loss: 0.1038, step time: 0.1066\n",
      "135/223, train_loss: 0.1045, step time: 0.1032\n",
      "136/223, train_loss: 0.1027, step time: 0.1077\n",
      "137/223, train_loss: 0.0985, step time: 0.1201\n",
      "138/223, train_loss: 0.1149, step time: 0.1122\n",
      "139/223, train_loss: 0.1153, step time: 0.1226\n",
      "140/223, train_loss: 0.1037, step time: 0.1066\n",
      "141/223, train_loss: 0.1154, step time: 0.0992\n",
      "142/223, train_loss: 0.1028, step time: 0.1047\n",
      "143/223, train_loss: 0.1150, step time: 0.1014\n",
      "144/223, train_loss: 0.0985, step time: 0.1096\n",
      "145/223, train_loss: 0.1075, step time: 0.1245\n",
      "146/223, train_loss: 0.1186, step time: 0.1092\n",
      "147/223, train_loss: 0.1107, step time: 0.1057\n",
      "148/223, train_loss: 0.1179, step time: 0.1050\n",
      "149/223, train_loss: 0.1021, step time: 0.1397\n",
      "150/223, train_loss: 0.1061, step time: 0.1080\n",
      "151/223, train_loss: 0.1038, step time: 0.1193\n",
      "152/223, train_loss: 0.1069, step time: 0.1126\n",
      "153/223, train_loss: 0.1040, step time: 0.1081\n",
      "154/223, train_loss: 0.1099, step time: 0.1157\n",
      "155/223, train_loss: 0.1045, step time: 0.1188\n",
      "156/223, train_loss: 0.0948, step time: 0.1246\n",
      "157/223, train_loss: 0.1041, step time: 0.1163\n",
      "158/223, train_loss: 0.1117, step time: 0.1093\n",
      "159/223, train_loss: 0.1063, step time: 0.1007\n",
      "160/223, train_loss: 0.1064, step time: 0.1167\n",
      "161/223, train_loss: 0.0975, step time: 0.1029\n",
      "162/223, train_loss: 0.1055, step time: 0.1363\n",
      "163/223, train_loss: 0.1104, step time: 0.1060\n",
      "164/223, train_loss: 0.1048, step time: 0.1003\n",
      "165/223, train_loss: 0.1083, step time: 0.1011\n",
      "166/223, train_loss: 0.1073, step time: 0.1006\n",
      "167/223, train_loss: 0.1145, step time: 0.1012\n",
      "168/223, train_loss: 0.0963, step time: 0.1001\n",
      "169/223, train_loss: 0.1063, step time: 0.1105\n",
      "170/223, train_loss: 0.1120, step time: 0.1001\n",
      "171/223, train_loss: 0.0940, step time: 0.1003\n",
      "172/223, train_loss: 0.1208, step time: 0.1063\n",
      "173/223, train_loss: 0.1026, step time: 0.1005\n",
      "174/223, train_loss: 0.1120, step time: 0.1006\n",
      "175/223, train_loss: 0.1012, step time: 0.1006\n",
      "176/223, train_loss: 0.1113, step time: 0.1014\n",
      "177/223, train_loss: 0.1196, step time: 0.1251\n",
      "178/223, train_loss: 0.1175, step time: 0.1148\n",
      "179/223, train_loss: 0.1259, step time: 0.0999\n",
      "180/223, train_loss: 0.1032, step time: 0.0999\n",
      "181/223, train_loss: 0.0987, step time: 0.1002\n",
      "182/223, train_loss: 0.1045, step time: 0.1000\n",
      "183/223, train_loss: 0.0977, step time: 0.1010\n",
      "184/223, train_loss: 0.1118, step time: 0.0997\n",
      "185/223, train_loss: 0.0978, step time: 0.0999\n",
      "186/223, train_loss: 0.0956, step time: 0.1006\n",
      "187/223, train_loss: 0.1007, step time: 0.0996\n",
      "188/223, train_loss: 0.1117, step time: 0.0990\n",
      "189/223, train_loss: 0.1062, step time: 0.0993\n",
      "190/223, train_loss: 0.1053, step time: 0.1050\n",
      "191/223, train_loss: 0.1111, step time: 0.1179\n",
      "192/223, train_loss: 0.1081, step time: 0.0987\n",
      "193/223, train_loss: 0.1105, step time: 0.1001\n",
      "194/223, train_loss: 0.1232, step time: 0.0997\n",
      "195/223, train_loss: 0.1086, step time: 0.1004\n",
      "196/223, train_loss: 0.1170, step time: 0.1087\n",
      "197/223, train_loss: 0.0989, step time: 0.1167\n",
      "198/223, train_loss: 0.1119, step time: 0.1139\n",
      "199/223, train_loss: 0.1085, step time: 0.1247\n",
      "200/223, train_loss: 0.1114, step time: 0.1158\n",
      "201/223, train_loss: 0.1120, step time: 0.1013\n",
      "202/223, train_loss: 0.1075, step time: 0.1235\n",
      "203/223, train_loss: 0.1057, step time: 0.1025\n",
      "204/223, train_loss: 0.1026, step time: 0.1126\n",
      "205/223, train_loss: 0.1100, step time: 0.1259\n",
      "206/223, train_loss: 0.1070, step time: 0.1146\n",
      "207/223, train_loss: 0.1005, step time: 0.1180\n",
      "208/223, train_loss: 0.0944, step time: 0.1053\n",
      "209/223, train_loss: 0.1105, step time: 0.1063\n",
      "210/223, train_loss: 0.1056, step time: 0.1010\n",
      "211/223, train_loss: 0.0966, step time: 0.1028\n",
      "212/223, train_loss: 0.1157, step time: 0.1127\n",
      "213/223, train_loss: 0.1048, step time: 0.1288\n",
      "214/223, train_loss: 0.1019, step time: 0.1180\n",
      "215/223, train_loss: 0.1150, step time: 0.1093\n",
      "216/223, train_loss: 0.1048, step time: 0.1061\n",
      "217/223, train_loss: 0.1113, step time: 0.1008\n",
      "218/223, train_loss: 0.1125, step time: 0.0993\n",
      "219/223, train_loss: 0.1068, step time: 0.0998\n",
      "220/223, train_loss: 0.1073, step time: 0.0996\n",
      "221/223, train_loss: 0.1165, step time: 0.0993\n",
      "222/223, train_loss: 0.1038, step time: 0.0995\n",
      "223/223, train_loss: 0.0966, step time: 0.0996\n",
      "epoch 136 average loss: 0.1089\n",
      "time consuming of epoch 136 is: 91.6957\n",
      "----------\n",
      "epoch 137/300\n",
      "1/223, train_loss: 0.0979, step time: 0.1078\n",
      "2/223, train_loss: 0.1199, step time: 0.1017\n",
      "3/223, train_loss: 0.1040, step time: 0.1237\n",
      "4/223, train_loss: 0.1047, step time: 0.1137\n",
      "5/223, train_loss: 0.0948, step time: 0.1101\n",
      "6/223, train_loss: 0.1124, step time: 0.1068\n",
      "7/223, train_loss: 0.1176, step time: 0.1192\n",
      "8/223, train_loss: 0.1034, step time: 0.1325\n",
      "9/223, train_loss: 0.1034, step time: 0.1203\n",
      "10/223, train_loss: 0.1069, step time: 0.1166\n",
      "11/223, train_loss: 0.1108, step time: 0.1183\n",
      "12/223, train_loss: 0.1148, step time: 0.1002\n",
      "13/223, train_loss: 0.0967, step time: 0.1170\n",
      "14/223, train_loss: 0.1304, step time: 0.1075\n",
      "15/223, train_loss: 0.1122, step time: 0.1118\n",
      "16/223, train_loss: 0.1103, step time: 0.1175\n",
      "17/223, train_loss: 0.1072, step time: 0.1002\n",
      "18/223, train_loss: 0.1172, step time: 0.1176\n",
      "19/223, train_loss: 0.1030, step time: 0.1150\n",
      "20/223, train_loss: 0.1150, step time: 0.0988\n",
      "21/223, train_loss: 0.1073, step time: 0.1071\n",
      "22/223, train_loss: 0.1085, step time: 0.1104\n",
      "23/223, train_loss: 0.0900, step time: 0.1175\n",
      "24/223, train_loss: 0.1033, step time: 0.1031\n",
      "25/223, train_loss: 0.1093, step time: 0.1180\n",
      "26/223, train_loss: 0.1121, step time: 0.1112\n",
      "27/223, train_loss: 0.1205, step time: 0.1068\n",
      "28/223, train_loss: 0.0992, step time: 0.1154\n",
      "29/223, train_loss: 0.1143, step time: 0.1074\n",
      "30/223, train_loss: 0.1112, step time: 0.1209\n",
      "31/223, train_loss: 0.1073, step time: 0.1120\n",
      "32/223, train_loss: 0.1034, step time: 0.1048\n",
      "33/223, train_loss: 0.1143, step time: 0.1115\n",
      "34/223, train_loss: 0.1070, step time: 0.1150\n",
      "35/223, train_loss: 0.1112, step time: 0.1098\n",
      "36/223, train_loss: 0.1058, step time: 0.1182\n",
      "37/223, train_loss: 0.1084, step time: 0.1057\n",
      "38/223, train_loss: 0.1147, step time: 0.1094\n",
      "39/223, train_loss: 0.1038, step time: 0.1216\n",
      "40/223, train_loss: 0.1043, step time: 0.1168\n",
      "41/223, train_loss: 0.1125, step time: 0.1003\n",
      "42/223, train_loss: 0.1184, step time: 0.1084\n",
      "43/223, train_loss: 0.1114, step time: 0.1201\n",
      "44/223, train_loss: 0.1051, step time: 0.1002\n",
      "45/223, train_loss: 0.1084, step time: 0.1055\n",
      "46/223, train_loss: 0.1182, step time: 0.1138\n",
      "47/223, train_loss: 0.1190, step time: 0.1062\n",
      "48/223, train_loss: 0.1074, step time: 0.1106\n",
      "49/223, train_loss: 0.1069, step time: 0.1041\n",
      "50/223, train_loss: 0.1081, step time: 0.1128\n",
      "51/223, train_loss: 0.1129, step time: 0.1100\n",
      "52/223, train_loss: 0.1054, step time: 0.1073\n",
      "53/223, train_loss: 0.1033, step time: 0.1020\n",
      "54/223, train_loss: 0.1080, step time: 0.1145\n",
      "55/223, train_loss: 0.1094, step time: 0.1039\n",
      "56/223, train_loss: 0.1113, step time: 0.1196\n",
      "57/223, train_loss: 0.1089, step time: 0.1092\n",
      "58/223, train_loss: 0.1097, step time: 0.1103\n",
      "59/223, train_loss: 0.1032, step time: 0.1059\n",
      "60/223, train_loss: 0.1004, step time: 0.1326\n",
      "61/223, train_loss: 0.1135, step time: 0.1041\n",
      "62/223, train_loss: 0.1052, step time: 0.1069\n",
      "63/223, train_loss: 0.0994, step time: 0.1051\n",
      "64/223, train_loss: 0.1018, step time: 0.1188\n",
      "65/223, train_loss: 0.1112, step time: 0.1002\n",
      "66/223, train_loss: 0.1055, step time: 0.1510\n",
      "67/223, train_loss: 0.1067, step time: 0.1147\n",
      "68/223, train_loss: 0.1086, step time: 0.1074\n",
      "69/223, train_loss: 0.0999, step time: 0.1115\n",
      "70/223, train_loss: 0.1207, step time: 0.1068\n",
      "71/223, train_loss: 0.0968, step time: 0.1119\n",
      "72/223, train_loss: 0.1044, step time: 0.1004\n",
      "73/223, train_loss: 0.0978, step time: 0.0991\n",
      "74/223, train_loss: 0.1299, step time: 0.1039\n",
      "75/223, train_loss: 0.1135, step time: 0.1113\n",
      "76/223, train_loss: 0.1289, step time: 0.1097\n",
      "77/223, train_loss: 0.0978, step time: 0.1088\n",
      "78/223, train_loss: 0.3022, step time: 0.1060\n",
      "79/223, train_loss: 0.1077, step time: 0.1105\n",
      "80/223, train_loss: 0.0984, step time: 0.1101\n",
      "81/223, train_loss: 0.1218, step time: 0.1197\n",
      "82/223, train_loss: 0.1029, step time: 0.1003\n",
      "83/223, train_loss: 0.1033, step time: 0.1072\n",
      "84/223, train_loss: 0.1096, step time: 0.1027\n",
      "85/223, train_loss: 0.1136, step time: 0.1085\n",
      "86/223, train_loss: 0.1164, step time: 0.1099\n",
      "87/223, train_loss: 0.1119, step time: 0.1154\n",
      "88/223, train_loss: 0.1045, step time: 0.0995\n",
      "89/223, train_loss: 0.1073, step time: 0.1115\n",
      "90/223, train_loss: 0.1051, step time: 0.1149\n",
      "91/223, train_loss: 0.1064, step time: 0.1107\n",
      "92/223, train_loss: 0.1210, step time: 0.1132\n",
      "93/223, train_loss: 0.1177, step time: 0.1058\n",
      "94/223, train_loss: 0.1077, step time: 0.1058\n",
      "95/223, train_loss: 0.1128, step time: 0.1092\n",
      "96/223, train_loss: 0.1060, step time: 0.0997\n",
      "97/223, train_loss: 0.0995, step time: 0.1134\n",
      "98/223, train_loss: 0.1045, step time: 0.1003\n",
      "99/223, train_loss: 0.1129, step time: 0.1199\n",
      "100/223, train_loss: 0.1050, step time: 0.1108\n",
      "101/223, train_loss: 0.1043, step time: 0.1038\n",
      "102/223, train_loss: 0.1010, step time: 0.1120\n",
      "103/223, train_loss: 0.1050, step time: 0.1095\n",
      "104/223, train_loss: 0.1060, step time: 0.1299\n",
      "105/223, train_loss: 0.1087, step time: 0.1188\n",
      "106/223, train_loss: 0.1107, step time: 0.1110\n",
      "107/223, train_loss: 0.1056, step time: 0.1228\n",
      "108/223, train_loss: 0.0958, step time: 0.0997\n",
      "109/223, train_loss: 0.0987, step time: 0.1112\n",
      "110/223, train_loss: 0.1189, step time: 0.1142\n",
      "111/223, train_loss: 0.1039, step time: 0.1089\n",
      "112/223, train_loss: 0.0955, step time: 0.1049\n",
      "113/223, train_loss: 0.1068, step time: 0.1103\n",
      "114/223, train_loss: 0.1047, step time: 0.1216\n",
      "115/223, train_loss: 0.1050, step time: 0.1218\n",
      "116/223, train_loss: 0.1055, step time: 0.1177\n",
      "117/223, train_loss: 0.1134, step time: 0.1130\n",
      "118/223, train_loss: 0.1024, step time: 0.1164\n",
      "119/223, train_loss: 0.1064, step time: 0.1258\n",
      "120/223, train_loss: 0.1050, step time: 0.1052\n",
      "121/223, train_loss: 0.1143, step time: 0.1040\n",
      "122/223, train_loss: 0.1117, step time: 0.1101\n",
      "123/223, train_loss: 0.0961, step time: 0.1009\n",
      "124/223, train_loss: 0.1068, step time: 0.1133\n",
      "125/223, train_loss: 0.1146, step time: 0.1082\n",
      "126/223, train_loss: 0.1056, step time: 0.1131\n",
      "127/223, train_loss: 0.0977, step time: 0.1058\n",
      "128/223, train_loss: 0.1105, step time: 0.1203\n",
      "129/223, train_loss: 0.1154, step time: 0.1287\n",
      "130/223, train_loss: 0.1151, step time: 0.1118\n",
      "131/223, train_loss: 0.1100, step time: 0.1380\n",
      "132/223, train_loss: 0.1200, step time: 0.1047\n",
      "133/223, train_loss: 0.0991, step time: 0.1110\n",
      "134/223, train_loss: 0.1088, step time: 0.1117\n",
      "135/223, train_loss: 0.1038, step time: 0.1056\n",
      "136/223, train_loss: 0.0998, step time: 0.1183\n",
      "137/223, train_loss: 0.1009, step time: 0.1010\n",
      "138/223, train_loss: 0.1082, step time: 0.1217\n",
      "139/223, train_loss: 0.1013, step time: 0.1058\n",
      "140/223, train_loss: 0.1056, step time: 0.1186\n",
      "141/223, train_loss: 0.1082, step time: 0.1095\n",
      "142/223, train_loss: 0.0964, step time: 0.1098\n",
      "143/223, train_loss: 0.1063, step time: 0.1173\n",
      "144/223, train_loss: 0.1035, step time: 0.1140\n",
      "145/223, train_loss: 0.1056, step time: 0.1227\n",
      "146/223, train_loss: 0.0980, step time: 0.1047\n",
      "147/223, train_loss: 0.1031, step time: 0.1181\n",
      "148/223, train_loss: 0.1220, step time: 0.1004\n",
      "149/223, train_loss: 0.1112, step time: 0.1177\n",
      "150/223, train_loss: 0.0942, step time: 0.1174\n",
      "151/223, train_loss: 0.1144, step time: 0.1020\n",
      "152/223, train_loss: 0.1023, step time: 0.1147\n",
      "153/223, train_loss: 0.1109, step time: 0.1010\n",
      "154/223, train_loss: 0.0988, step time: 0.1045\n",
      "155/223, train_loss: 0.1032, step time: 0.1062\n",
      "156/223, train_loss: 0.1069, step time: 0.1016\n",
      "157/223, train_loss: 0.1022, step time: 0.1015\n",
      "158/223, train_loss: 0.1055, step time: 0.1116\n",
      "159/223, train_loss: 0.1124, step time: 0.1029\n",
      "160/223, train_loss: 0.1096, step time: 0.1262\n",
      "161/223, train_loss: 0.1043, step time: 0.1014\n",
      "162/223, train_loss: 0.1097, step time: 0.1587\n",
      "163/223, train_loss: 0.1124, step time: 0.1400\n",
      "164/223, train_loss: 0.1056, step time: 0.1366\n",
      "165/223, train_loss: 0.0957, step time: 0.1136\n",
      "166/223, train_loss: 0.1183, step time: 0.1066\n",
      "167/223, train_loss: 0.1071, step time: 0.1172\n",
      "168/223, train_loss: 0.1106, step time: 0.1247\n",
      "169/223, train_loss: 0.1210, step time: 0.1004\n",
      "170/223, train_loss: 0.1125, step time: 0.0999\n",
      "171/223, train_loss: 0.1023, step time: 0.1115\n",
      "172/223, train_loss: 0.1054, step time: 0.1047\n",
      "173/223, train_loss: 0.1074, step time: 0.1008\n",
      "174/223, train_loss: 0.1147, step time: 0.1254\n",
      "175/223, train_loss: 0.1024, step time: 0.1106\n",
      "176/223, train_loss: 0.1127, step time: 0.1006\n",
      "177/223, train_loss: 0.1150, step time: 0.1046\n",
      "178/223, train_loss: 0.1156, step time: 0.1157\n",
      "179/223, train_loss: 0.1025, step time: 0.1098\n",
      "180/223, train_loss: 0.1031, step time: 0.1075\n",
      "181/223, train_loss: 0.1149, step time: 0.1128\n",
      "182/223, train_loss: 0.1099, step time: 0.1116\n",
      "183/223, train_loss: 0.1081, step time: 0.1072\n",
      "184/223, train_loss: 0.1127, step time: 0.1005\n",
      "185/223, train_loss: 0.1033, step time: 0.1008\n",
      "186/223, train_loss: 0.1057, step time: 0.1220\n",
      "187/223, train_loss: 0.1120, step time: 0.1221\n",
      "188/223, train_loss: 0.1051, step time: 0.1155\n",
      "189/223, train_loss: 0.1041, step time: 0.0994\n",
      "190/223, train_loss: 0.1086, step time: 0.1248\n",
      "191/223, train_loss: 0.1093, step time: 0.1131\n",
      "192/223, train_loss: 0.1217, step time: 0.1322\n",
      "193/223, train_loss: 0.1004, step time: 0.1143\n",
      "194/223, train_loss: 0.1148, step time: 0.1116\n",
      "195/223, train_loss: 0.1216, step time: 0.1223\n",
      "196/223, train_loss: 0.1214, step time: 0.1333\n",
      "197/223, train_loss: 0.1081, step time: 0.1278\n",
      "198/223, train_loss: 0.1081, step time: 0.1078\n",
      "199/223, train_loss: 0.1080, step time: 0.1143\n",
      "200/223, train_loss: 0.0989, step time: 0.1135\n",
      "201/223, train_loss: 0.1138, step time: 0.1514\n",
      "202/223, train_loss: 0.1019, step time: 0.1140\n",
      "203/223, train_loss: 0.1176, step time: 0.1102\n",
      "204/223, train_loss: 0.0942, step time: 0.1201\n",
      "205/223, train_loss: 0.1121, step time: 0.1215\n",
      "206/223, train_loss: 0.0999, step time: 0.1115\n",
      "207/223, train_loss: 0.1100, step time: 0.1542\n",
      "208/223, train_loss: 0.1072, step time: 0.1388\n",
      "209/223, train_loss: 0.0916, step time: 0.1060\n",
      "210/223, train_loss: 0.1035, step time: 0.1083\n",
      "211/223, train_loss: 0.1043, step time: 0.1351\n",
      "212/223, train_loss: 0.0997, step time: 0.1090\n",
      "213/223, train_loss: 0.1128, step time: 0.1090\n",
      "214/223, train_loss: 0.1005, step time: 0.1157\n",
      "215/223, train_loss: 0.1084, step time: 0.1112\n",
      "216/223, train_loss: 0.1062, step time: 0.1021\n",
      "217/223, train_loss: 0.1143, step time: 0.1060\n",
      "218/223, train_loss: 0.1263, step time: 0.1260\n",
      "219/223, train_loss: 0.1262, step time: 0.1001\n",
      "220/223, train_loss: 0.1091, step time: 0.1001\n",
      "221/223, train_loss: 0.1179, step time: 0.1007\n",
      "222/223, train_loss: 0.0995, step time: 0.1002\n",
      "223/223, train_loss: 0.0977, step time: 0.1001\n",
      "epoch 137 average loss: 0.1089\n",
      "time consuming of epoch 137 is: 86.0123\n",
      "----------\n",
      "epoch 138/300\n",
      "1/223, train_loss: 0.1156, step time: 0.1059\n",
      "2/223, train_loss: 0.1096, step time: 0.0994\n",
      "3/223, train_loss: 0.1057, step time: 0.1016\n",
      "4/223, train_loss: 0.1122, step time: 0.0994\n",
      "5/223, train_loss: 0.1156, step time: 0.1080\n",
      "6/223, train_loss: 0.1089, step time: 0.1120\n",
      "7/223, train_loss: 0.1032, step time: 0.1396\n",
      "8/223, train_loss: 0.0901, step time: 0.1087\n",
      "9/223, train_loss: 0.0992, step time: 0.1204\n",
      "10/223, train_loss: 0.1007, step time: 0.0992\n",
      "11/223, train_loss: 0.1013, step time: 0.1055\n",
      "12/223, train_loss: 0.1137, step time: 0.1008\n",
      "13/223, train_loss: 0.1079, step time: 0.1164\n",
      "14/223, train_loss: 0.1072, step time: 0.1006\n",
      "15/223, train_loss: 0.1116, step time: 0.1005\n",
      "16/223, train_loss: 0.1113, step time: 0.0997\n",
      "17/223, train_loss: 0.1034, step time: 0.1148\n",
      "18/223, train_loss: 0.1081, step time: 0.1034\n",
      "19/223, train_loss: 0.1060, step time: 0.0993\n",
      "20/223, train_loss: 0.0976, step time: 0.1037\n",
      "21/223, train_loss: 0.1109, step time: 0.1139\n",
      "22/223, train_loss: 0.1063, step time: 0.1580\n",
      "23/223, train_loss: 0.1075, step time: 0.1052\n",
      "24/223, train_loss: 0.1257, step time: 0.1010\n",
      "25/223, train_loss: 0.0984, step time: 0.1204\n",
      "26/223, train_loss: 0.1237, step time: 0.1104\n",
      "27/223, train_loss: 0.0990, step time: 0.1495\n",
      "28/223, train_loss: 0.1018, step time: 0.1183\n",
      "29/223, train_loss: 0.1029, step time: 0.1094\n",
      "30/223, train_loss: 0.1163, step time: 0.1234\n",
      "31/223, train_loss: 0.1053, step time: 0.1026\n",
      "32/223, train_loss: 0.1057, step time: 0.1001\n",
      "33/223, train_loss: 0.1119, step time: 0.1107\n",
      "34/223, train_loss: 0.1169, step time: 0.1247\n",
      "35/223, train_loss: 0.1123, step time: 0.1216\n",
      "36/223, train_loss: 0.1037, step time: 0.1064\n",
      "37/223, train_loss: 0.1070, step time: 0.1111\n",
      "38/223, train_loss: 0.1213, step time: 0.1105\n",
      "39/223, train_loss: 0.1061, step time: 0.1234\n",
      "40/223, train_loss: 0.0982, step time: 0.1091\n",
      "41/223, train_loss: 0.1057, step time: 0.1089\n",
      "42/223, train_loss: 0.0985, step time: 0.1009\n",
      "43/223, train_loss: 0.1017, step time: 0.0999\n",
      "44/223, train_loss: 0.1028, step time: 0.1007\n",
      "45/223, train_loss: 0.1082, step time: 0.1006\n",
      "46/223, train_loss: 0.1122, step time: 0.1001\n",
      "47/223, train_loss: 0.1085, step time: 0.1007\n",
      "48/223, train_loss: 0.1056, step time: 0.1187\n",
      "49/223, train_loss: 0.1163, step time: 0.1093\n",
      "50/223, train_loss: 0.1281, step time: 0.1034\n",
      "51/223, train_loss: 0.1042, step time: 0.1196\n",
      "52/223, train_loss: 0.0998, step time: 0.1325\n",
      "53/223, train_loss: 0.1073, step time: 0.1071\n",
      "54/223, train_loss: 0.1013, step time: 0.1013\n",
      "55/223, train_loss: 0.1136, step time: 0.1114\n",
      "56/223, train_loss: 0.1015, step time: 0.1153\n",
      "57/223, train_loss: 0.1060, step time: 0.1213\n",
      "58/223, train_loss: 0.1115, step time: 0.1042\n",
      "59/223, train_loss: 0.0991, step time: 0.1264\n",
      "60/223, train_loss: 0.1092, step time: 0.1055\n",
      "61/223, train_loss: 0.0995, step time: 0.1219\n",
      "62/223, train_loss: 0.1024, step time: 0.1009\n",
      "63/223, train_loss: 0.1001, step time: 0.1098\n",
      "64/223, train_loss: 0.1000, step time: 0.1121\n",
      "65/223, train_loss: 0.1142, step time: 0.1076\n",
      "66/223, train_loss: 0.1046, step time: 0.1123\n",
      "67/223, train_loss: 0.1085, step time: 0.0991\n",
      "68/223, train_loss: 0.1066, step time: 0.1137\n",
      "69/223, train_loss: 0.1067, step time: 0.1160\n",
      "70/223, train_loss: 0.1116, step time: 0.1125\n",
      "71/223, train_loss: 0.1007, step time: 0.0997\n",
      "72/223, train_loss: 0.1106, step time: 0.1170\n",
      "73/223, train_loss: 0.0974, step time: 0.1113\n",
      "74/223, train_loss: 0.1045, step time: 0.1202\n",
      "75/223, train_loss: 0.1215, step time: 0.1078\n",
      "76/223, train_loss: 0.1078, step time: 0.1105\n",
      "77/223, train_loss: 0.1071, step time: 0.1146\n",
      "78/223, train_loss: 0.1054, step time: 0.1066\n",
      "79/223, train_loss: 0.1027, step time: 0.1128\n",
      "80/223, train_loss: 0.1055, step time: 0.1125\n",
      "81/223, train_loss: 0.1098, step time: 0.1158\n",
      "82/223, train_loss: 0.1029, step time: 0.1072\n",
      "83/223, train_loss: 0.0986, step time: 0.1101\n",
      "84/223, train_loss: 0.1117, step time: 0.1141\n",
      "85/223, train_loss: 0.1158, step time: 0.1090\n",
      "86/223, train_loss: 0.1100, step time: 0.1066\n",
      "87/223, train_loss: 0.1134, step time: 0.1099\n",
      "88/223, train_loss: 0.3002, step time: 0.0998\n",
      "89/223, train_loss: 0.1141, step time: 0.1204\n",
      "90/223, train_loss: 0.1172, step time: 0.0997\n",
      "91/223, train_loss: 0.1104, step time: 0.1126\n",
      "92/223, train_loss: 0.1045, step time: 0.1032\n",
      "93/223, train_loss: 0.1098, step time: 0.1142\n",
      "94/223, train_loss: 0.1197, step time: 0.0995\n",
      "95/223, train_loss: 0.1195, step time: 0.1138\n",
      "96/223, train_loss: 0.1009, step time: 0.1109\n",
      "97/223, train_loss: 0.1148, step time: 0.1172\n",
      "98/223, train_loss: 0.1201, step time: 0.1147\n",
      "99/223, train_loss: 0.1114, step time: 0.1107\n",
      "100/223, train_loss: 0.1241, step time: 0.1231\n",
      "101/223, train_loss: 0.1066, step time: 0.1148\n",
      "102/223, train_loss: 0.1044, step time: 0.1092\n",
      "103/223, train_loss: 0.1098, step time: 0.1337\n",
      "104/223, train_loss: 0.1143, step time: 0.1151\n",
      "105/223, train_loss: 0.1098, step time: 0.1136\n",
      "106/223, train_loss: 0.1049, step time: 0.1051\n",
      "107/223, train_loss: 0.1080, step time: 0.1060\n",
      "108/223, train_loss: 0.1231, step time: 0.1158\n",
      "109/223, train_loss: 0.1033, step time: 0.1131\n",
      "110/223, train_loss: 0.1146, step time: 0.1085\n",
      "111/223, train_loss: 0.1047, step time: 0.1138\n",
      "112/223, train_loss: 0.1016, step time: 0.1132\n",
      "113/223, train_loss: 0.1066, step time: 0.1175\n",
      "114/223, train_loss: 0.1067, step time: 0.1003\n",
      "115/223, train_loss: 0.1063, step time: 0.1213\n",
      "116/223, train_loss: 0.1044, step time: 0.0992\n",
      "117/223, train_loss: 0.1242, step time: 0.1079\n",
      "118/223, train_loss: 0.1185, step time: 0.1001\n",
      "119/223, train_loss: 0.0969, step time: 0.1109\n",
      "120/223, train_loss: 0.1072, step time: 0.1144\n",
      "121/223, train_loss: 0.0931, step time: 0.1145\n",
      "122/223, train_loss: 0.1159, step time: 0.1011\n",
      "123/223, train_loss: 0.1101, step time: 0.1137\n",
      "124/223, train_loss: 0.1101, step time: 0.0993\n",
      "125/223, train_loss: 0.0991, step time: 0.1110\n",
      "126/223, train_loss: 0.1140, step time: 0.1075\n",
      "127/223, train_loss: 0.1160, step time: 0.1299\n",
      "128/223, train_loss: 0.1277, step time: 0.1172\n",
      "129/223, train_loss: 0.1013, step time: 0.1012\n",
      "130/223, train_loss: 0.1032, step time: 0.1119\n",
      "131/223, train_loss: 0.1215, step time: 0.1074\n",
      "132/223, train_loss: 0.0995, step time: 0.1237\n",
      "133/223, train_loss: 0.1130, step time: 0.1063\n",
      "134/223, train_loss: 0.1122, step time: 0.1065\n",
      "135/223, train_loss: 0.1025, step time: 0.1084\n",
      "136/223, train_loss: 0.1102, step time: 0.1058\n",
      "137/223, train_loss: 0.1206, step time: 0.1055\n",
      "138/223, train_loss: 0.1064, step time: 0.1000\n",
      "139/223, train_loss: 0.0966, step time: 0.1047\n",
      "140/223, train_loss: 0.1134, step time: 0.1116\n",
      "141/223, train_loss: 0.1145, step time: 0.1203\n",
      "142/223, train_loss: 0.1077, step time: 0.1137\n",
      "143/223, train_loss: 0.1092, step time: 0.1277\n",
      "144/223, train_loss: 0.1030, step time: 0.1115\n",
      "145/223, train_loss: 0.1113, step time: 0.1003\n",
      "146/223, train_loss: 0.1154, step time: 0.1006\n",
      "147/223, train_loss: 0.1057, step time: 0.1017\n",
      "148/223, train_loss: 0.1172, step time: 0.1041\n",
      "149/223, train_loss: 0.1138, step time: 0.0995\n",
      "150/223, train_loss: 0.1033, step time: 0.1021\n",
      "151/223, train_loss: 0.1010, step time: 0.1000\n",
      "152/223, train_loss: 0.1061, step time: 0.1171\n",
      "153/223, train_loss: 0.0969, step time: 0.1373\n",
      "154/223, train_loss: 0.1012, step time: 0.1090\n",
      "155/223, train_loss: 0.1028, step time: 0.1011\n",
      "156/223, train_loss: 0.0981, step time: 0.1102\n",
      "157/223, train_loss: 0.1036, step time: 0.1163\n",
      "158/223, train_loss: 0.1139, step time: 0.1072\n",
      "159/223, train_loss: 0.1065, step time: 0.1092\n",
      "160/223, train_loss: 0.1129, step time: 0.1012\n",
      "161/223, train_loss: 0.1070, step time: 0.1102\n",
      "162/223, train_loss: 0.1023, step time: 0.1008\n",
      "163/223, train_loss: 0.1202, step time: 0.1020\n",
      "164/223, train_loss: 0.1037, step time: 0.1068\n",
      "165/223, train_loss: 0.1101, step time: 0.1209\n",
      "166/223, train_loss: 0.1079, step time: 0.1815\n",
      "167/223, train_loss: 0.0957, step time: 0.1022\n",
      "168/223, train_loss: 0.1107, step time: 0.1008\n",
      "169/223, train_loss: 0.0975, step time: 0.1158\n",
      "170/223, train_loss: 0.1150, step time: 0.1067\n",
      "171/223, train_loss: 0.1094, step time: 0.1491\n",
      "172/223, train_loss: 0.1010, step time: 0.0999\n",
      "173/223, train_loss: 0.1153, step time: 0.1148\n",
      "174/223, train_loss: 0.1049, step time: 0.1085\n",
      "175/223, train_loss: 0.1104, step time: 0.1027\n",
      "176/223, train_loss: 0.1216, step time: 0.1014\n",
      "177/223, train_loss: 0.1050, step time: 0.1082\n",
      "178/223, train_loss: 0.1076, step time: 0.1023\n",
      "179/223, train_loss: 0.0987, step time: 0.1024\n",
      "180/223, train_loss: 0.1092, step time: 0.1017\n",
      "181/223, train_loss: 0.1120, step time: 0.1204\n",
      "182/223, train_loss: 0.1063, step time: 0.1267\n",
      "183/223, train_loss: 0.1079, step time: 0.1006\n",
      "184/223, train_loss: 0.1060, step time: 0.1006\n",
      "185/223, train_loss: 0.1048, step time: 0.1136\n",
      "186/223, train_loss: 0.1195, step time: 0.1128\n",
      "187/223, train_loss: 0.1100, step time: 0.1191\n",
      "188/223, train_loss: 0.1047, step time: 0.1004\n",
      "189/223, train_loss: 0.1078, step time: 0.1081\n",
      "190/223, train_loss: 0.1119, step time: 0.1002\n",
      "191/223, train_loss: 0.1073, step time: 0.1003\n",
      "192/223, train_loss: 0.1068, step time: 0.1010\n",
      "193/223, train_loss: 0.1153, step time: 0.1046\n",
      "194/223, train_loss: 0.1156, step time: 0.0994\n",
      "195/223, train_loss: 0.1072, step time: 0.0994\n",
      "196/223, train_loss: 0.1063, step time: 0.0995\n",
      "197/223, train_loss: 0.1083, step time: 0.1002\n",
      "198/223, train_loss: 0.1028, step time: 0.1021\n",
      "199/223, train_loss: 0.1181, step time: 0.1168\n",
      "200/223, train_loss: 0.1046, step time: 0.1006\n",
      "201/223, train_loss: 0.1069, step time: 0.1013\n",
      "202/223, train_loss: 0.1196, step time: 0.1000\n",
      "203/223, train_loss: 0.1116, step time: 0.1232\n",
      "204/223, train_loss: 0.1051, step time: 0.1006\n",
      "205/223, train_loss: 0.0940, step time: 0.1120\n",
      "206/223, train_loss: 0.1084, step time: 0.1042\n",
      "207/223, train_loss: 0.0948, step time: 0.1192\n",
      "208/223, train_loss: 0.1119, step time: 0.1084\n",
      "209/223, train_loss: 0.1106, step time: 0.1073\n",
      "210/223, train_loss: 0.0964, step time: 0.1302\n",
      "211/223, train_loss: 0.1027, step time: 0.1329\n",
      "212/223, train_loss: 0.1074, step time: 0.1201\n",
      "213/223, train_loss: 0.1138, step time: 0.1156\n",
      "214/223, train_loss: 0.1021, step time: 0.1042\n",
      "215/223, train_loss: 0.1131, step time: 0.1227\n",
      "216/223, train_loss: 0.1086, step time: 0.1097\n",
      "217/223, train_loss: 0.1026, step time: 0.1007\n",
      "218/223, train_loss: 0.1110, step time: 0.1744\n",
      "219/223, train_loss: 0.0962, step time: 0.1140\n",
      "220/223, train_loss: 0.1050, step time: 0.1006\n",
      "221/223, train_loss: 0.1076, step time: 0.1000\n",
      "222/223, train_loss: 0.0975, step time: 0.0994\n",
      "223/223, train_loss: 0.1127, step time: 0.1001\n",
      "epoch 138 average loss: 0.1089\n",
      "time consuming of epoch 138 is: 86.0626\n",
      "----------\n",
      "epoch 139/300\n",
      "1/223, train_loss: 0.1175, step time: 0.1139\n",
      "2/223, train_loss: 0.0937, step time: 0.1117\n",
      "3/223, train_loss: 0.1158, step time: 0.0999\n",
      "4/223, train_loss: 0.0961, step time: 0.1073\n",
      "5/223, train_loss: 0.1242, step time: 0.1095\n",
      "6/223, train_loss: 0.1197, step time: 0.1084\n",
      "7/223, train_loss: 0.1119, step time: 0.1248\n",
      "8/223, train_loss: 0.0988, step time: 0.1204\n",
      "9/223, train_loss: 0.1020, step time: 0.1162\n",
      "10/223, train_loss: 0.1072, step time: 0.1025\n",
      "11/223, train_loss: 0.1122, step time: 0.1071\n",
      "12/223, train_loss: 0.1212, step time: 0.1190\n",
      "13/223, train_loss: 0.1014, step time: 0.1243\n",
      "14/223, train_loss: 0.0958, step time: 0.1204\n",
      "15/223, train_loss: 0.1102, step time: 0.1130\n",
      "16/223, train_loss: 0.1172, step time: 0.1000\n",
      "17/223, train_loss: 0.1047, step time: 0.1079\n",
      "18/223, train_loss: 0.1102, step time: 0.1294\n",
      "19/223, train_loss: 0.1072, step time: 0.1150\n",
      "20/223, train_loss: 0.1053, step time: 0.1161\n",
      "21/223, train_loss: 0.1066, step time: 0.1165\n",
      "22/223, train_loss: 0.1075, step time: 0.1135\n",
      "23/223, train_loss: 0.1213, step time: 0.1193\n",
      "24/223, train_loss: 0.1114, step time: 0.1035\n",
      "25/223, train_loss: 0.1015, step time: 0.1074\n",
      "26/223, train_loss: 0.1039, step time: 0.0987\n",
      "27/223, train_loss: 0.1105, step time: 0.0994\n",
      "28/223, train_loss: 0.1144, step time: 0.1100\n",
      "29/223, train_loss: 0.1034, step time: 0.1008\n",
      "30/223, train_loss: 0.1123, step time: 0.1089\n",
      "31/223, train_loss: 0.1166, step time: 0.1137\n",
      "32/223, train_loss: 0.0949, step time: 0.1020\n",
      "33/223, train_loss: 0.1024, step time: 0.1135\n",
      "34/223, train_loss: 0.1141, step time: 0.1060\n",
      "35/223, train_loss: 0.1132, step time: 0.1072\n",
      "36/223, train_loss: 0.1047, step time: 0.1205\n",
      "37/223, train_loss: 0.1019, step time: 0.1236\n",
      "38/223, train_loss: 0.1038, step time: 0.1027\n",
      "39/223, train_loss: 0.1109, step time: 0.1147\n",
      "40/223, train_loss: 0.1157, step time: 0.1167\n",
      "41/223, train_loss: 0.1111, step time: 0.1103\n",
      "42/223, train_loss: 0.1047, step time: 0.1098\n",
      "43/223, train_loss: 0.1145, step time: 0.0992\n",
      "44/223, train_loss: 0.1054, step time: 0.1242\n",
      "45/223, train_loss: 0.1114, step time: 0.1203\n",
      "46/223, train_loss: 0.1003, step time: 0.1175\n",
      "47/223, train_loss: 0.1059, step time: 0.1061\n",
      "48/223, train_loss: 0.0956, step time: 0.1093\n",
      "49/223, train_loss: 0.1240, step time: 0.1197\n",
      "50/223, train_loss: 0.1199, step time: 0.1284\n",
      "51/223, train_loss: 0.0982, step time: 0.1170\n",
      "52/223, train_loss: 0.1057, step time: 0.1005\n",
      "53/223, train_loss: 0.1069, step time: 0.0990\n",
      "54/223, train_loss: 0.1012, step time: 0.1043\n",
      "55/223, train_loss: 0.3031, step time: 0.1121\n",
      "56/223, train_loss: 0.1032, step time: 0.1133\n",
      "57/223, train_loss: 0.1090, step time: 0.1264\n",
      "58/223, train_loss: 0.1125, step time: 0.1049\n",
      "59/223, train_loss: 0.1162, step time: 0.1287\n",
      "60/223, train_loss: 0.1138, step time: 0.1105\n",
      "61/223, train_loss: 0.1056, step time: 0.1130\n",
      "62/223, train_loss: 0.1034, step time: 0.1214\n",
      "63/223, train_loss: 0.1030, step time: 0.1060\n",
      "64/223, train_loss: 0.0996, step time: 0.1079\n",
      "65/223, train_loss: 0.1105, step time: 0.1116\n",
      "66/223, train_loss: 0.1108, step time: 0.1320\n",
      "67/223, train_loss: 0.1142, step time: 0.1593\n",
      "68/223, train_loss: 0.1076, step time: 0.1009\n",
      "69/223, train_loss: 0.1078, step time: 0.1092\n",
      "70/223, train_loss: 0.0973, step time: 0.1162\n",
      "71/223, train_loss: 0.1059, step time: 0.1282\n",
      "72/223, train_loss: 0.1159, step time: 0.1001\n",
      "73/223, train_loss: 0.1087, step time: 0.1125\n",
      "74/223, train_loss: 0.1111, step time: 0.1154\n",
      "75/223, train_loss: 0.1037, step time: 0.1019\n",
      "76/223, train_loss: 0.1160, step time: 0.1004\n",
      "77/223, train_loss: 0.0973, step time: 0.1181\n",
      "78/223, train_loss: 0.0954, step time: 0.1157\n",
      "79/223, train_loss: 0.1093, step time: 0.1281\n",
      "80/223, train_loss: 0.0989, step time: 0.1202\n",
      "81/223, train_loss: 0.0980, step time: 0.1008\n",
      "82/223, train_loss: 0.1117, step time: 0.1009\n",
      "83/223, train_loss: 0.1040, step time: 0.1079\n",
      "84/223, train_loss: 0.1050, step time: 0.1296\n",
      "85/223, train_loss: 0.1101, step time: 0.1002\n",
      "86/223, train_loss: 0.0955, step time: 0.1007\n",
      "87/223, train_loss: 0.1018, step time: 0.0994\n",
      "88/223, train_loss: 0.1051, step time: 0.1008\n",
      "89/223, train_loss: 0.0963, step time: 0.0999\n",
      "90/223, train_loss: 0.1174, step time: 0.1006\n",
      "91/223, train_loss: 0.1031, step time: 0.1001\n",
      "92/223, train_loss: 0.1035, step time: 0.1207\n",
      "93/223, train_loss: 0.1023, step time: 0.1026\n",
      "94/223, train_loss: 0.1146, step time: 0.1072\n",
      "95/223, train_loss: 0.1063, step time: 0.1019\n",
      "96/223, train_loss: 0.1011, step time: 0.1008\n",
      "97/223, train_loss: 0.1132, step time: 0.1053\n",
      "98/223, train_loss: 0.1059, step time: 0.1123\n",
      "99/223, train_loss: 0.0993, step time: 0.1244\n",
      "100/223, train_loss: 0.1088, step time: 0.1142\n",
      "101/223, train_loss: 0.1095, step time: 0.1213\n",
      "102/223, train_loss: 0.1156, step time: 0.1056\n",
      "103/223, train_loss: 0.1089, step time: 0.1060\n",
      "104/223, train_loss: 0.0983, step time: 0.1306\n",
      "105/223, train_loss: 0.1062, step time: 0.1010\n",
      "106/223, train_loss: 0.1158, step time: 0.1193\n",
      "107/223, train_loss: 0.1101, step time: 0.1073\n",
      "108/223, train_loss: 0.1115, step time: 0.1106\n",
      "109/223, train_loss: 0.1157, step time: 0.1272\n",
      "110/223, train_loss: 0.1029, step time: 0.1127\n",
      "111/223, train_loss: 0.1100, step time: 0.1165\n",
      "112/223, train_loss: 0.1091, step time: 0.1175\n",
      "113/223, train_loss: 0.1082, step time: 0.0996\n",
      "114/223, train_loss: 0.1178, step time: 0.1216\n",
      "115/223, train_loss: 0.1034, step time: 0.1119\n",
      "116/223, train_loss: 0.1056, step time: 0.1195\n",
      "117/223, train_loss: 0.0971, step time: 0.1011\n",
      "118/223, train_loss: 0.1024, step time: 0.1178\n",
      "119/223, train_loss: 0.1159, step time: 0.1015\n",
      "120/223, train_loss: 0.0973, step time: 0.1209\n",
      "121/223, train_loss: 0.1105, step time: 0.1054\n",
      "122/223, train_loss: 0.1117, step time: 0.0994\n",
      "123/223, train_loss: 0.1068, step time: 0.0994\n",
      "124/223, train_loss: 0.1077, step time: 0.0986\n",
      "125/223, train_loss: 0.1141, step time: 0.1049\n",
      "126/223, train_loss: 0.1072, step time: 0.1129\n",
      "127/223, train_loss: 0.1009, step time: 0.1255\n",
      "128/223, train_loss: 0.0992, step time: 0.1257\n",
      "129/223, train_loss: 0.1346, step time: 0.1078\n",
      "130/223, train_loss: 0.1084, step time: 0.1083\n",
      "131/223, train_loss: 0.0997, step time: 0.1295\n",
      "132/223, train_loss: 0.1144, step time: 0.1098\n",
      "133/223, train_loss: 0.1084, step time: 0.1258\n",
      "134/223, train_loss: 0.0992, step time: 0.1066\n",
      "135/223, train_loss: 0.1062, step time: 0.1094\n",
      "136/223, train_loss: 0.1179, step time: 0.1228\n",
      "137/223, train_loss: 0.1106, step time: 0.1051\n",
      "138/223, train_loss: 0.0964, step time: 0.1083\n",
      "139/223, train_loss: 0.1076, step time: 0.1136\n",
      "140/223, train_loss: 0.1013, step time: 0.1088\n",
      "141/223, train_loss: 0.1018, step time: 0.1010\n",
      "142/223, train_loss: 0.1006, step time: 0.1010\n",
      "143/223, train_loss: 0.1253, step time: 0.0996\n",
      "144/223, train_loss: 0.1079, step time: 0.1009\n",
      "145/223, train_loss: 0.1076, step time: 0.1552\n",
      "146/223, train_loss: 0.1136, step time: 0.1547\n",
      "147/223, train_loss: 0.0961, step time: 0.1012\n",
      "148/223, train_loss: 0.1042, step time: 0.1004\n",
      "149/223, train_loss: 0.1100, step time: 0.1055\n",
      "150/223, train_loss: 0.1186, step time: 0.1263\n",
      "151/223, train_loss: 0.1020, step time: 0.1005\n",
      "152/223, train_loss: 0.1103, step time: 0.1002\n",
      "153/223, train_loss: 0.1042, step time: 0.1141\n",
      "154/223, train_loss: 0.1170, step time: 0.1028\n",
      "155/223, train_loss: 0.1099, step time: 0.1226\n",
      "156/223, train_loss: 0.1087, step time: 0.1091\n",
      "157/223, train_loss: 0.1015, step time: 0.1119\n",
      "158/223, train_loss: 0.1105, step time: 0.1114\n",
      "159/223, train_loss: 0.1069, step time: 0.1129\n",
      "160/223, train_loss: 0.1074, step time: 0.1047\n",
      "161/223, train_loss: 0.1051, step time: 0.1137\n",
      "162/223, train_loss: 0.1003, step time: 0.1068\n",
      "163/223, train_loss: 0.1114, step time: 0.1055\n",
      "164/223, train_loss: 0.1109, step time: 0.1106\n",
      "165/223, train_loss: 0.1147, step time: 0.1187\n",
      "166/223, train_loss: 0.1224, step time: 0.1009\n",
      "167/223, train_loss: 0.1078, step time: 0.1020\n",
      "168/223, train_loss: 0.1003, step time: 0.1140\n",
      "169/223, train_loss: 0.1092, step time: 0.1210\n",
      "170/223, train_loss: 0.1031, step time: 0.0996\n",
      "171/223, train_loss: 0.1001, step time: 0.1049\n",
      "172/223, train_loss: 0.1123, step time: 0.1001\n",
      "173/223, train_loss: 0.1052, step time: 0.1074\n",
      "174/223, train_loss: 0.1123, step time: 0.1101\n",
      "175/223, train_loss: 0.1051, step time: 0.1003\n",
      "176/223, train_loss: 0.1029, step time: 0.1011\n",
      "177/223, train_loss: 0.1069, step time: 0.1060\n",
      "178/223, train_loss: 0.1116, step time: 0.1001\n",
      "179/223, train_loss: 0.1021, step time: 0.1085\n",
      "180/223, train_loss: 0.1115, step time: 0.1112\n",
      "181/223, train_loss: 0.0941, step time: 0.1090\n",
      "182/223, train_loss: 0.1050, step time: 0.1176\n",
      "183/223, train_loss: 0.1083, step time: 0.1026\n",
      "184/223, train_loss: 0.1049, step time: 0.1126\n",
      "185/223, train_loss: 0.1027, step time: 0.1116\n",
      "186/223, train_loss: 0.1086, step time: 0.1005\n",
      "187/223, train_loss: 0.1158, step time: 0.1344\n",
      "188/223, train_loss: 0.1003, step time: 0.1811\n",
      "189/223, train_loss: 0.1150, step time: 0.1090\n",
      "190/223, train_loss: 0.0948, step time: 0.1001\n",
      "191/223, train_loss: 0.1147, step time: 0.1087\n",
      "192/223, train_loss: 0.1033, step time: 0.1186\n",
      "193/223, train_loss: 0.0955, step time: 0.1219\n",
      "194/223, train_loss: 0.1024, step time: 0.1007\n",
      "195/223, train_loss: 0.1061, step time: 0.1110\n",
      "196/223, train_loss: 0.1081, step time: 0.1272\n",
      "197/223, train_loss: 0.1014, step time: 0.1043\n",
      "198/223, train_loss: 0.1097, step time: 0.1001\n",
      "199/223, train_loss: 0.0999, step time: 0.1069\n",
      "200/223, train_loss: 0.1041, step time: 0.1232\n",
      "201/223, train_loss: 0.0916, step time: 0.1120\n",
      "202/223, train_loss: 0.0968, step time: 0.1043\n",
      "203/223, train_loss: 0.1089, step time: 0.1179\n",
      "204/223, train_loss: 0.1029, step time: 0.1070\n",
      "205/223, train_loss: 0.1215, step time: 0.1074\n",
      "206/223, train_loss: 0.1091, step time: 0.1222\n",
      "207/223, train_loss: 0.1022, step time: 0.1231\n",
      "208/223, train_loss: 0.1154, step time: 0.1048\n",
      "209/223, train_loss: 0.1136, step time: 0.1006\n",
      "210/223, train_loss: 0.0963, step time: 0.1068\n",
      "211/223, train_loss: 0.1015, step time: 0.1110\n",
      "212/223, train_loss: 0.1034, step time: 0.1048\n",
      "213/223, train_loss: 0.1208, step time: 0.1108\n",
      "214/223, train_loss: 0.1030, step time: 0.1176\n",
      "215/223, train_loss: 0.1050, step time: 0.1205\n",
      "216/223, train_loss: 0.1025, step time: 0.1116\n",
      "217/223, train_loss: 0.1035, step time: 0.1056\n",
      "218/223, train_loss: 0.1141, step time: 0.1113\n",
      "219/223, train_loss: 0.1080, step time: 0.1002\n",
      "220/223, train_loss: 0.1196, step time: 0.0995\n",
      "221/223, train_loss: 0.1097, step time: 0.0997\n",
      "222/223, train_loss: 0.1116, step time: 0.1002\n",
      "223/223, train_loss: 0.1221, step time: 0.1000\n",
      "epoch 139 average loss: 0.1083\n",
      "time consuming of epoch 139 is: 87.8553\n",
      "----------\n",
      "epoch 140/300\n",
      "1/223, train_loss: 0.0959, step time: 0.1075\n",
      "2/223, train_loss: 0.1035, step time: 0.1063\n",
      "3/223, train_loss: 0.1137, step time: 0.1124\n",
      "4/223, train_loss: 0.1100, step time: 0.1203\n",
      "5/223, train_loss: 0.1054, step time: 0.1144\n",
      "6/223, train_loss: 0.1021, step time: 0.1452\n",
      "7/223, train_loss: 0.1062, step time: 0.1151\n",
      "8/223, train_loss: 0.0952, step time: 0.1300\n",
      "9/223, train_loss: 0.1028, step time: 0.1068\n",
      "10/223, train_loss: 0.0997, step time: 0.1214\n",
      "11/223, train_loss: 0.0946, step time: 0.1189\n",
      "12/223, train_loss: 0.1231, step time: 0.1271\n",
      "13/223, train_loss: 0.1137, step time: 0.1094\n",
      "14/223, train_loss: 0.1031, step time: 0.1201\n",
      "15/223, train_loss: 0.1189, step time: 0.1104\n",
      "16/223, train_loss: 0.1028, step time: 0.1258\n",
      "17/223, train_loss: 0.1144, step time: 0.1075\n",
      "18/223, train_loss: 0.1058, step time: 0.1159\n",
      "19/223, train_loss: 0.1111, step time: 0.1110\n",
      "20/223, train_loss: 0.1042, step time: 0.1267\n",
      "21/223, train_loss: 0.1113, step time: 0.1003\n",
      "22/223, train_loss: 0.1177, step time: 0.1091\n",
      "23/223, train_loss: 0.1137, step time: 0.1047\n",
      "24/223, train_loss: 0.1034, step time: 0.1007\n",
      "25/223, train_loss: 0.1028, step time: 0.1075\n",
      "26/223, train_loss: 0.1070, step time: 0.1182\n",
      "27/223, train_loss: 0.1068, step time: 0.1066\n",
      "28/223, train_loss: 0.1021, step time: 0.1114\n",
      "29/223, train_loss: 0.1197, step time: 0.1087\n",
      "30/223, train_loss: 0.1151, step time: 0.1128\n",
      "31/223, train_loss: 0.1023, step time: 0.1166\n",
      "32/223, train_loss: 0.1140, step time: 0.1072\n",
      "33/223, train_loss: 0.1123, step time: 0.1147\n",
      "34/223, train_loss: 0.1099, step time: 0.0999\n",
      "35/223, train_loss: 0.1097, step time: 0.1214\n",
      "36/223, train_loss: 0.1137, step time: 0.1038\n",
      "37/223, train_loss: 0.1091, step time: 0.1393\n",
      "38/223, train_loss: 0.1227, step time: 0.1059\n",
      "39/223, train_loss: 0.1131, step time: 0.1001\n",
      "40/223, train_loss: 0.1053, step time: 0.1007\n",
      "41/223, train_loss: 0.1090, step time: 0.1138\n",
      "42/223, train_loss: 0.1124, step time: 0.1189\n",
      "43/223, train_loss: 0.1037, step time: 0.1364\n",
      "44/223, train_loss: 0.0993, step time: 0.1117\n",
      "45/223, train_loss: 0.1023, step time: 0.1110\n",
      "46/223, train_loss: 0.1110, step time: 0.1126\n",
      "47/223, train_loss: 0.1034, step time: 0.1046\n",
      "48/223, train_loss: 0.0948, step time: 0.1078\n",
      "49/223, train_loss: 0.1034, step time: 0.1076\n",
      "50/223, train_loss: 0.1127, step time: 0.1174\n",
      "51/223, train_loss: 0.1016, step time: 0.1034\n",
      "52/223, train_loss: 0.0990, step time: 0.1211\n",
      "53/223, train_loss: 0.0922, step time: 0.1183\n",
      "54/223, train_loss: 0.1204, step time: 0.1094\n",
      "55/223, train_loss: 0.1117, step time: 0.1098\n",
      "56/223, train_loss: 0.1182, step time: 0.1200\n",
      "57/223, train_loss: 0.0917, step time: 0.1200\n",
      "58/223, train_loss: 0.1101, step time: 0.1139\n",
      "59/223, train_loss: 0.1049, step time: 0.1130\n",
      "60/223, train_loss: 0.1086, step time: 0.1070\n",
      "61/223, train_loss: 0.1062, step time: 0.1517\n",
      "62/223, train_loss: 0.0957, step time: 0.1424\n",
      "63/223, train_loss: 0.1057, step time: 0.1041\n",
      "64/223, train_loss: 0.1140, step time: 0.1095\n",
      "65/223, train_loss: 0.1086, step time: 0.1141\n",
      "66/223, train_loss: 0.1059, step time: 0.1026\n",
      "67/223, train_loss: 0.1078, step time: 0.1249\n",
      "68/223, train_loss: 0.1179, step time: 0.1128\n",
      "69/223, train_loss: 0.1140, step time: 0.1115\n",
      "70/223, train_loss: 0.1083, step time: 0.1189\n",
      "71/223, train_loss: 0.0984, step time: 0.1201\n",
      "72/223, train_loss: 0.0992, step time: 0.1125\n",
      "73/223, train_loss: 0.1123, step time: 0.1185\n",
      "74/223, train_loss: 0.1202, step time: 0.1142\n",
      "75/223, train_loss: 0.1028, step time: 0.0997\n",
      "76/223, train_loss: 0.0974, step time: 0.1018\n",
      "77/223, train_loss: 0.1014, step time: 0.1078\n",
      "78/223, train_loss: 0.0979, step time: 0.1067\n",
      "79/223, train_loss: 0.1154, step time: 0.1192\n",
      "80/223, train_loss: 0.1135, step time: 0.1102\n",
      "81/223, train_loss: 0.1008, step time: 0.1136\n",
      "82/223, train_loss: 0.1097, step time: 0.1142\n",
      "83/223, train_loss: 0.1139, step time: 0.1160\n",
      "84/223, train_loss: 0.1053, step time: 0.1032\n",
      "85/223, train_loss: 0.1091, step time: 0.1162\n",
      "86/223, train_loss: 0.1063, step time: 0.1004\n",
      "87/223, train_loss: 0.1105, step time: 0.1112\n",
      "88/223, train_loss: 0.1021, step time: 0.1144\n",
      "89/223, train_loss: 0.1068, step time: 0.1169\n",
      "90/223, train_loss: 0.1146, step time: 0.1004\n",
      "91/223, train_loss: 0.1029, step time: 0.1009\n",
      "92/223, train_loss: 0.1125, step time: 0.1177\n",
      "93/223, train_loss: 0.1028, step time: 0.1179\n",
      "94/223, train_loss: 0.1055, step time: 0.0992\n",
      "95/223, train_loss: 0.1064, step time: 0.1013\n",
      "96/223, train_loss: 0.1137, step time: 0.1105\n",
      "97/223, train_loss: 0.0959, step time: 0.1011\n",
      "98/223, train_loss: 0.0960, step time: 0.0997\n",
      "99/223, train_loss: 0.0952, step time: 0.1082\n",
      "100/223, train_loss: 0.1091, step time: 0.1030\n",
      "101/223, train_loss: 0.1059, step time: 0.1142\n",
      "102/223, train_loss: 0.1018, step time: 0.1215\n",
      "103/223, train_loss: 0.1025, step time: 0.1143\n",
      "104/223, train_loss: 0.1105, step time: 0.1060\n",
      "105/223, train_loss: 0.1119, step time: 0.1132\n",
      "106/223, train_loss: 0.1108, step time: 0.1278\n",
      "107/223, train_loss: 0.1142, step time: 0.1158\n",
      "108/223, train_loss: 0.1044, step time: 0.0999\n",
      "109/223, train_loss: 0.1044, step time: 0.1279\n",
      "110/223, train_loss: 0.1116, step time: 0.1460\n",
      "111/223, train_loss: 0.1122, step time: 0.1009\n",
      "112/223, train_loss: 0.1015, step time: 0.1011\n",
      "113/223, train_loss: 0.1029, step time: 0.1198\n",
      "114/223, train_loss: 0.0980, step time: 0.1273\n",
      "115/223, train_loss: 0.1088, step time: 0.1037\n",
      "116/223, train_loss: 0.1032, step time: 0.1002\n",
      "117/223, train_loss: 0.1222, step time: 0.1154\n",
      "118/223, train_loss: 0.1028, step time: 0.1128\n",
      "119/223, train_loss: 0.1184, step time: 0.1005\n",
      "120/223, train_loss: 0.0994, step time: 0.1015\n",
      "121/223, train_loss: 0.1161, step time: 0.1222\n",
      "122/223, train_loss: 0.0980, step time: 0.1034\n",
      "123/223, train_loss: 0.1234, step time: 0.1111\n",
      "124/223, train_loss: 0.1151, step time: 0.1004\n",
      "125/223, train_loss: 0.0999, step time: 0.1057\n",
      "126/223, train_loss: 0.1095, step time: 0.1092\n",
      "127/223, train_loss: 0.1148, step time: 0.0999\n",
      "128/223, train_loss: 0.1031, step time: 0.1002\n",
      "129/223, train_loss: 0.1026, step time: 0.1001\n",
      "130/223, train_loss: 0.1023, step time: 0.1033\n",
      "131/223, train_loss: 0.1053, step time: 0.1034\n",
      "132/223, train_loss: 0.3108, step time: 0.1003\n",
      "133/223, train_loss: 0.1218, step time: 0.1131\n",
      "134/223, train_loss: 0.1020, step time: 0.1041\n",
      "135/223, train_loss: 0.1117, step time: 0.1138\n",
      "136/223, train_loss: 0.1057, step time: 0.1366\n",
      "137/223, train_loss: 0.0973, step time: 0.1103\n",
      "138/223, train_loss: 0.1008, step time: 0.1223\n",
      "139/223, train_loss: 0.0928, step time: 0.1036\n",
      "140/223, train_loss: 0.1146, step time: 0.1109\n",
      "141/223, train_loss: 0.1019, step time: 0.1103\n",
      "142/223, train_loss: 0.1141, step time: 0.1298\n",
      "143/223, train_loss: 0.1112, step time: 0.1213\n",
      "144/223, train_loss: 0.1026, step time: 0.1140\n",
      "145/223, train_loss: 0.1037, step time: 0.1006\n",
      "146/223, train_loss: 0.1189, step time: 0.0996\n",
      "147/223, train_loss: 0.0936, step time: 0.1002\n",
      "148/223, train_loss: 0.1026, step time: 0.1094\n",
      "149/223, train_loss: 0.1167, step time: 0.1093\n",
      "150/223, train_loss: 0.1042, step time: 0.1041\n",
      "151/223, train_loss: 0.1134, step time: 0.1003\n",
      "152/223, train_loss: 0.1063, step time: 0.1013\n",
      "153/223, train_loss: 0.0978, step time: 0.1164\n",
      "154/223, train_loss: 0.1025, step time: 0.1080\n",
      "155/223, train_loss: 0.1162, step time: 0.1033\n",
      "156/223, train_loss: 0.1023, step time: 0.0997\n",
      "157/223, train_loss: 0.1042, step time: 0.1140\n",
      "158/223, train_loss: 0.1114, step time: 0.1074\n",
      "159/223, train_loss: 0.1136, step time: 0.1239\n",
      "160/223, train_loss: 0.1071, step time: 0.1097\n",
      "161/223, train_loss: 0.1203, step time: 0.1078\n",
      "162/223, train_loss: 0.1173, step time: 0.1224\n",
      "163/223, train_loss: 0.0956, step time: 0.0997\n",
      "164/223, train_loss: 0.1211, step time: 0.1043\n",
      "165/223, train_loss: 0.1079, step time: 0.1188\n",
      "166/223, train_loss: 0.1120, step time: 0.1008\n",
      "167/223, train_loss: 0.1039, step time: 0.1004\n",
      "168/223, train_loss: 0.1254, step time: 0.1213\n",
      "169/223, train_loss: 0.1176, step time: 0.1173\n",
      "170/223, train_loss: 0.1110, step time: 0.1131\n",
      "171/223, train_loss: 0.1130, step time: 0.1068\n",
      "172/223, train_loss: 0.1011, step time: 0.1042\n",
      "173/223, train_loss: 0.1080, step time: 0.1159\n",
      "174/223, train_loss: 0.1056, step time: 0.1202\n",
      "175/223, train_loss: 0.1229, step time: 0.1333\n",
      "176/223, train_loss: 0.1184, step time: 0.1100\n",
      "177/223, train_loss: 0.1136, step time: 0.1048\n",
      "178/223, train_loss: 0.1058, step time: 0.1004\n",
      "179/223, train_loss: 0.1080, step time: 0.1184\n",
      "180/223, train_loss: 0.1105, step time: 0.1122\n",
      "181/223, train_loss: 0.1304, step time: 0.1226\n",
      "182/223, train_loss: 0.1003, step time: 0.1224\n",
      "183/223, train_loss: 0.1116, step time: 0.1065\n",
      "184/223, train_loss: 0.1033, step time: 0.1003\n",
      "185/223, train_loss: 0.1182, step time: 0.1259\n",
      "186/223, train_loss: 0.1024, step time: 0.1002\n",
      "187/223, train_loss: 0.1098, step time: 0.1028\n",
      "188/223, train_loss: 0.0985, step time: 0.1175\n",
      "189/223, train_loss: 0.1201, step time: 0.1421\n",
      "190/223, train_loss: 0.1089, step time: 0.1433\n",
      "191/223, train_loss: 0.1067, step time: 0.1000\n",
      "192/223, train_loss: 0.1236, step time: 0.1025\n",
      "193/223, train_loss: 0.0994, step time: 0.1133\n",
      "194/223, train_loss: 0.1068, step time: 0.1099\n",
      "195/223, train_loss: 0.0995, step time: 0.1011\n",
      "196/223, train_loss: 0.1078, step time: 0.1004\n",
      "197/223, train_loss: 0.1072, step time: 0.1165\n",
      "198/223, train_loss: 0.0951, step time: 0.1202\n",
      "199/223, train_loss: 0.1033, step time: 0.0998\n",
      "200/223, train_loss: 0.1011, step time: 0.1102\n",
      "201/223, train_loss: 0.1067, step time: 0.1031\n",
      "202/223, train_loss: 0.0979, step time: 0.1007\n",
      "203/223, train_loss: 0.1028, step time: 0.0999\n",
      "204/223, train_loss: 0.0987, step time: 0.1105\n",
      "205/223, train_loss: 0.1009, step time: 0.1229\n",
      "206/223, train_loss: 0.1030, step time: 0.1049\n",
      "207/223, train_loss: 0.1053, step time: 0.1183\n",
      "208/223, train_loss: 0.1147, step time: 0.1107\n",
      "209/223, train_loss: 0.1038, step time: 0.1163\n",
      "210/223, train_loss: 0.1097, step time: 0.1000\n",
      "211/223, train_loss: 0.1021, step time: 0.1005\n",
      "212/223, train_loss: 0.0987, step time: 0.1163\n",
      "213/223, train_loss: 0.1053, step time: 0.0996\n",
      "214/223, train_loss: 0.1100, step time: 0.0998\n",
      "215/223, train_loss: 0.0961, step time: 0.1017\n",
      "216/223, train_loss: 0.1021, step time: 0.1290\n",
      "217/223, train_loss: 0.1070, step time: 0.1001\n",
      "218/223, train_loss: 0.1023, step time: 0.1002\n",
      "219/223, train_loss: 0.1127, step time: 0.1005\n",
      "220/223, train_loss: 0.1080, step time: 0.1000\n",
      "221/223, train_loss: 0.1242, step time: 0.0996\n",
      "222/223, train_loss: 0.1040, step time: 0.0996\n",
      "223/223, train_loss: 0.0942, step time: 0.1001\n",
      "epoch 140 average loss: 0.1083\n",
      "current epoch: 140 current mean dice: 0.8538 tc: 0.9192 wt: 0.8658 et: 0.7765\n",
      "best mean dice: 0.8551 at epoch: 125\n",
      "time consuming of epoch 140 is: 90.0091\n",
      "----------\n",
      "epoch 141/300\n",
      "1/223, train_loss: 0.0974, step time: 0.1018\n",
      "2/223, train_loss: 0.0973, step time: 0.0999\n",
      "3/223, train_loss: 0.1030, step time: 0.1004\n",
      "4/223, train_loss: 0.1057, step time: 0.1008\n",
      "5/223, train_loss: 0.1116, step time: 0.1011\n",
      "6/223, train_loss: 0.0971, step time: 0.0999\n",
      "7/223, train_loss: 0.1132, step time: 0.1006\n",
      "8/223, train_loss: 0.1136, step time: 0.1060\n",
      "9/223, train_loss: 0.1186, step time: 0.1156\n",
      "10/223, train_loss: 0.1017, step time: 0.1165\n",
      "11/223, train_loss: 0.1182, step time: 0.1110\n",
      "12/223, train_loss: 0.0983, step time: 0.1149\n",
      "13/223, train_loss: 0.1210, step time: 0.1168\n",
      "14/223, train_loss: 0.1124, step time: 0.1089\n",
      "15/223, train_loss: 0.0974, step time: 0.1012\n",
      "16/223, train_loss: 0.1174, step time: 0.1007\n",
      "17/223, train_loss: 0.1291, step time: 0.1132\n",
      "18/223, train_loss: 0.1039, step time: 0.1200\n",
      "19/223, train_loss: 0.1136, step time: 0.0995\n",
      "20/223, train_loss: 0.0976, step time: 0.1007\n",
      "21/223, train_loss: 0.1231, step time: 0.1047\n",
      "22/223, train_loss: 0.0955, step time: 0.1007\n",
      "23/223, train_loss: 0.1098, step time: 0.1005\n",
      "24/223, train_loss: 0.1176, step time: 0.1003\n",
      "25/223, train_loss: 0.1024, step time: 0.1073\n",
      "26/223, train_loss: 0.1075, step time: 0.1223\n",
      "27/223, train_loss: 0.1008, step time: 0.1190\n",
      "28/223, train_loss: 0.0996, step time: 0.1012\n",
      "29/223, train_loss: 0.1009, step time: 0.1111\n",
      "30/223, train_loss: 0.1129, step time: 0.1070\n",
      "31/223, train_loss: 0.1106, step time: 0.1136\n",
      "32/223, train_loss: 0.1163, step time: 0.1008\n",
      "33/223, train_loss: 0.1076, step time: 0.1108\n",
      "34/223, train_loss: 0.1090, step time: 0.1044\n",
      "35/223, train_loss: 0.1078, step time: 0.1005\n",
      "36/223, train_loss: 0.1056, step time: 0.1007\n",
      "37/223, train_loss: 0.1099, step time: 0.1125\n",
      "38/223, train_loss: 0.1061, step time: 0.1004\n",
      "39/223, train_loss: 0.3070, step time: 0.1006\n",
      "40/223, train_loss: 0.1136, step time: 0.1036\n",
      "41/223, train_loss: 0.1013, step time: 0.1087\n",
      "42/223, train_loss: 0.1012, step time: 0.0999\n",
      "43/223, train_loss: 0.1017, step time: 0.1004\n",
      "44/223, train_loss: 0.0977, step time: 0.1184\n",
      "45/223, train_loss: 0.0978, step time: 0.1062\n",
      "46/223, train_loss: 0.1052, step time: 0.1173\n",
      "47/223, train_loss: 0.1141, step time: 0.1236\n",
      "48/223, train_loss: 0.1008, step time: 0.1009\n",
      "49/223, train_loss: 0.1101, step time: 0.1475\n",
      "50/223, train_loss: 0.1074, step time: 0.1114\n",
      "51/223, train_loss: 0.1150, step time: 0.1141\n",
      "52/223, train_loss: 0.1190, step time: 0.1124\n",
      "53/223, train_loss: 0.1087, step time: 0.1078\n",
      "54/223, train_loss: 0.1127, step time: 0.1418\n",
      "55/223, train_loss: 0.1055, step time: 0.1004\n",
      "56/223, train_loss: 0.1191, step time: 0.0997\n",
      "57/223, train_loss: 0.1156, step time: 0.1376\n",
      "58/223, train_loss: 0.1030, step time: 0.1285\n",
      "59/223, train_loss: 0.1067, step time: 0.1179\n",
      "60/223, train_loss: 0.1125, step time: 0.1324\n",
      "61/223, train_loss: 0.1096, step time: 0.1063\n",
      "62/223, train_loss: 0.1044, step time: 0.1129\n",
      "63/223, train_loss: 0.1158, step time: 0.1262\n",
      "64/223, train_loss: 0.0953, step time: 0.1007\n",
      "65/223, train_loss: 0.1090, step time: 0.1002\n",
      "66/223, train_loss: 0.1159, step time: 0.1024\n",
      "67/223, train_loss: 0.1146, step time: 0.1094\n",
      "68/223, train_loss: 0.0991, step time: 0.1008\n",
      "69/223, train_loss: 0.1222, step time: 0.1117\n",
      "70/223, train_loss: 0.1098, step time: 0.1106\n",
      "71/223, train_loss: 0.1121, step time: 0.1007\n",
      "72/223, train_loss: 0.0998, step time: 0.1005\n",
      "73/223, train_loss: 0.1008, step time: 0.1158\n",
      "74/223, train_loss: 0.1156, step time: 0.1033\n",
      "75/223, train_loss: 0.1034, step time: 0.1004\n",
      "76/223, train_loss: 0.1054, step time: 0.1180\n",
      "77/223, train_loss: 0.1076, step time: 0.1012\n",
      "78/223, train_loss: 0.1108, step time: 0.1007\n",
      "79/223, train_loss: 0.0962, step time: 0.1005\n",
      "80/223, train_loss: 0.1056, step time: 0.1041\n",
      "81/223, train_loss: 0.1072, step time: 0.1125\n",
      "82/223, train_loss: 0.1000, step time: 0.1291\n",
      "83/223, train_loss: 0.1105, step time: 0.1016\n",
      "84/223, train_loss: 0.1034, step time: 0.1383\n",
      "85/223, train_loss: 0.1026, step time: 0.1034\n",
      "86/223, train_loss: 0.1105, step time: 0.1059\n",
      "87/223, train_loss: 0.1060, step time: 0.1218\n",
      "88/223, train_loss: 0.1199, step time: 0.1091\n",
      "89/223, train_loss: 0.1071, step time: 0.1076\n",
      "90/223, train_loss: 0.1163, step time: 0.1189\n",
      "91/223, train_loss: 0.1151, step time: 0.1108\n",
      "92/223, train_loss: 0.1041, step time: 0.1009\n",
      "93/223, train_loss: 0.1123, step time: 0.1136\n",
      "94/223, train_loss: 0.1250, step time: 0.1226\n",
      "95/223, train_loss: 0.1024, step time: 0.1011\n",
      "96/223, train_loss: 0.0950, step time: 0.1003\n",
      "97/223, train_loss: 0.0987, step time: 0.1024\n",
      "98/223, train_loss: 0.0988, step time: 0.1004\n",
      "99/223, train_loss: 0.0990, step time: 0.1014\n",
      "100/223, train_loss: 0.1176, step time: 0.1268\n",
      "101/223, train_loss: 0.1051, step time: 0.1076\n",
      "102/223, train_loss: 0.1031, step time: 0.1111\n",
      "103/223, train_loss: 0.1017, step time: 0.1350\n",
      "104/223, train_loss: 0.1001, step time: 0.1249\n",
      "105/223, train_loss: 0.1113, step time: 0.1236\n",
      "106/223, train_loss: 0.1070, step time: 0.1129\n",
      "107/223, train_loss: 0.0951, step time: 0.1053\n",
      "108/223, train_loss: 0.1011, step time: 0.0999\n",
      "109/223, train_loss: 0.1072, step time: 0.1078\n",
      "110/223, train_loss: 0.1017, step time: 0.1094\n",
      "111/223, train_loss: 0.1072, step time: 0.1147\n",
      "112/223, train_loss: 0.1116, step time: 0.1006\n",
      "113/223, train_loss: 0.1034, step time: 0.1173\n",
      "114/223, train_loss: 0.1064, step time: 0.1185\n",
      "115/223, train_loss: 0.0999, step time: 0.1190\n",
      "116/223, train_loss: 0.1119, step time: 0.1124\n",
      "117/223, train_loss: 0.1035, step time: 0.1111\n",
      "118/223, train_loss: 0.1113, step time: 0.1329\n",
      "119/223, train_loss: 0.0995, step time: 0.1365\n",
      "120/223, train_loss: 0.0973, step time: 0.1148\n",
      "121/223, train_loss: 0.1028, step time: 0.1175\n",
      "122/223, train_loss: 0.1071, step time: 0.1108\n",
      "123/223, train_loss: 0.1013, step time: 0.1272\n",
      "124/223, train_loss: 0.1149, step time: 0.1291\n",
      "125/223, train_loss: 0.1135, step time: 0.1014\n",
      "126/223, train_loss: 0.1158, step time: 0.1205\n",
      "127/223, train_loss: 0.1179, step time: 0.1142\n",
      "128/223, train_loss: 0.1018, step time: 0.1032\n",
      "129/223, train_loss: 0.1133, step time: 0.1106\n",
      "130/223, train_loss: 0.1169, step time: 0.1010\n",
      "131/223, train_loss: 0.1222, step time: 0.1061\n",
      "132/223, train_loss: 0.1048, step time: 0.1118\n",
      "133/223, train_loss: 0.1106, step time: 0.1148\n",
      "134/223, train_loss: 0.1034, step time: 0.1192\n",
      "135/223, train_loss: 0.1085, step time: 0.1179\n",
      "136/223, train_loss: 0.1064, step time: 0.1057\n",
      "137/223, train_loss: 0.1000, step time: 0.1392\n",
      "138/223, train_loss: 0.0973, step time: 0.1304\n",
      "139/223, train_loss: 0.1159, step time: 0.1297\n",
      "140/223, train_loss: 0.1031, step time: 0.1218\n",
      "141/223, train_loss: 0.1100, step time: 0.0994\n",
      "142/223, train_loss: 0.1064, step time: 0.1005\n",
      "143/223, train_loss: 0.1160, step time: 0.1007\n",
      "144/223, train_loss: 0.0999, step time: 0.1023\n",
      "145/223, train_loss: 0.1224, step time: 0.0996\n",
      "146/223, train_loss: 0.1213, step time: 0.0999\n",
      "147/223, train_loss: 0.1106, step time: 0.1013\n",
      "148/223, train_loss: 0.1089, step time: 0.1005\n",
      "149/223, train_loss: 0.1121, step time: 0.0994\n",
      "150/223, train_loss: 0.1189, step time: 0.1000\n",
      "151/223, train_loss: 0.0985, step time: 0.1002\n",
      "152/223, train_loss: 0.1109, step time: 0.1019\n",
      "153/223, train_loss: 0.1148, step time: 0.1000\n",
      "154/223, train_loss: 0.1096, step time: 0.0993\n",
      "155/223, train_loss: 0.1034, step time: 0.1001\n",
      "156/223, train_loss: 0.1132, step time: 0.1006\n",
      "157/223, train_loss: 0.1082, step time: 0.1047\n",
      "158/223, train_loss: 0.1025, step time: 0.1046\n",
      "159/223, train_loss: 0.1048, step time: 0.1080\n",
      "160/223, train_loss: 0.1037, step time: 0.1277\n",
      "161/223, train_loss: 0.0973, step time: 0.1101\n",
      "162/223, train_loss: 0.0992, step time: 0.1094\n",
      "163/223, train_loss: 0.1113, step time: 0.1019\n",
      "164/223, train_loss: 0.1071, step time: 0.1014\n",
      "165/223, train_loss: 0.1036, step time: 0.1315\n",
      "166/223, train_loss: 0.1003, step time: 0.1047\n",
      "167/223, train_loss: 0.1144, step time: 0.0995\n",
      "168/223, train_loss: 0.1143, step time: 0.0989\n",
      "169/223, train_loss: 0.1095, step time: 0.1007\n",
      "170/223, train_loss: 0.1152, step time: 0.1007\n",
      "171/223, train_loss: 0.1076, step time: 0.1008\n",
      "172/223, train_loss: 0.1042, step time: 0.1271\n",
      "173/223, train_loss: 0.1100, step time: 0.1222\n",
      "174/223, train_loss: 0.1112, step time: 0.1202\n",
      "175/223, train_loss: 0.1083, step time: 0.1002\n",
      "176/223, train_loss: 0.1157, step time: 0.1014\n",
      "177/223, train_loss: 0.1103, step time: 0.1079\n",
      "178/223, train_loss: 0.1095, step time: 0.1125\n",
      "179/223, train_loss: 0.1053, step time: 0.1116\n",
      "180/223, train_loss: 0.0968, step time: 0.1123\n",
      "181/223, train_loss: 0.1036, step time: 0.1050\n",
      "182/223, train_loss: 0.1163, step time: 0.1000\n",
      "183/223, train_loss: 0.1084, step time: 0.1232\n",
      "184/223, train_loss: 0.1063, step time: 0.1006\n",
      "185/223, train_loss: 0.1035, step time: 0.1483\n",
      "186/223, train_loss: 0.1041, step time: 0.1126\n",
      "187/223, train_loss: 0.0959, step time: 0.1242\n",
      "188/223, train_loss: 0.1007, step time: 0.1272\n",
      "189/223, train_loss: 0.1049, step time: 0.1106\n",
      "190/223, train_loss: 0.1155, step time: 0.1042\n",
      "191/223, train_loss: 0.1056, step time: 0.1219\n",
      "192/223, train_loss: 0.1048, step time: 0.1109\n",
      "193/223, train_loss: 0.1112, step time: 0.1191\n",
      "194/223, train_loss: 0.1138, step time: 0.1208\n",
      "195/223, train_loss: 0.1292, step time: 0.1133\n",
      "196/223, train_loss: 0.0990, step time: 0.1218\n",
      "197/223, train_loss: 0.0961, step time: 0.1002\n",
      "198/223, train_loss: 0.1090, step time: 0.0999\n",
      "199/223, train_loss: 0.1029, step time: 0.1007\n",
      "200/223, train_loss: 0.0954, step time: 0.1044\n",
      "201/223, train_loss: 0.1096, step time: 0.1000\n",
      "202/223, train_loss: 0.1092, step time: 0.1005\n",
      "203/223, train_loss: 0.1095, step time: 0.1004\n",
      "204/223, train_loss: 0.1090, step time: 0.1010\n",
      "205/223, train_loss: 0.1082, step time: 0.1000\n",
      "206/223, train_loss: 0.0954, step time: 0.0994\n",
      "207/223, train_loss: 0.1094, step time: 0.1004\n",
      "208/223, train_loss: 0.1119, step time: 0.1142\n",
      "209/223, train_loss: 0.1059, step time: 0.0997\n",
      "210/223, train_loss: 0.1126, step time: 0.0994\n",
      "211/223, train_loss: 0.1093, step time: 0.1005\n",
      "212/223, train_loss: 0.1085, step time: 0.0997\n",
      "213/223, train_loss: 0.0983, step time: 0.1003\n",
      "214/223, train_loss: 0.0981, step time: 0.1003\n",
      "215/223, train_loss: 0.1121, step time: 0.1071\n",
      "216/223, train_loss: 0.1157, step time: 0.1007\n",
      "217/223, train_loss: 0.1205, step time: 0.1025\n",
      "218/223, train_loss: 0.0993, step time: 0.0998\n",
      "219/223, train_loss: 0.1167, step time: 0.0993\n",
      "220/223, train_loss: 0.0958, step time: 0.1002\n",
      "221/223, train_loss: 0.1043, step time: 0.1000\n",
      "222/223, train_loss: 0.1172, step time: 0.0996\n",
      "223/223, train_loss: 0.1029, step time: 0.0995\n",
      "epoch 141 average loss: 0.1086\n",
      "time consuming of epoch 141 is: 96.0856\n",
      "----------\n",
      "epoch 142/300\n",
      "1/223, train_loss: 0.1120, step time: 0.1012\n",
      "2/223, train_loss: 0.1044, step time: 0.1010\n",
      "3/223, train_loss: 0.0989, step time: 0.1042\n",
      "4/223, train_loss: 0.1090, step time: 0.1220\n",
      "5/223, train_loss: 0.0985, step time: 0.1080\n",
      "6/223, train_loss: 0.1137, step time: 0.1247\n",
      "7/223, train_loss: 0.0932, step time: 0.1070\n",
      "8/223, train_loss: 0.1048, step time: 0.1200\n",
      "9/223, train_loss: 0.1096, step time: 0.1155\n",
      "10/223, train_loss: 0.1124, step time: 0.1072\n",
      "11/223, train_loss: 0.1048, step time: 0.1293\n",
      "12/223, train_loss: 0.0913, step time: 0.1036\n",
      "13/223, train_loss: 0.1110, step time: 0.1205\n",
      "14/223, train_loss: 0.0975, step time: 0.1007\n",
      "15/223, train_loss: 0.1251, step time: 0.1022\n",
      "16/223, train_loss: 0.1011, step time: 0.1170\n",
      "17/223, train_loss: 0.1136, step time: 0.1238\n",
      "18/223, train_loss: 0.1115, step time: 0.1117\n",
      "19/223, train_loss: 0.1000, step time: 0.1036\n",
      "20/223, train_loss: 0.1187, step time: 0.0999\n",
      "21/223, train_loss: 0.1006, step time: 0.1073\n",
      "22/223, train_loss: 0.1006, step time: 0.1074\n",
      "23/223, train_loss: 0.1105, step time: 0.1347\n",
      "24/223, train_loss: 0.1080, step time: 0.1072\n",
      "25/223, train_loss: 0.1162, step time: 0.1018\n",
      "26/223, train_loss: 0.0974, step time: 0.1382\n",
      "27/223, train_loss: 0.1097, step time: 0.1012\n",
      "28/223, train_loss: 0.1002, step time: 0.1007\n",
      "29/223, train_loss: 0.0984, step time: 0.1099\n",
      "30/223, train_loss: 0.1036, step time: 0.1218\n",
      "31/223, train_loss: 0.1116, step time: 0.1007\n",
      "32/223, train_loss: 0.1057, step time: 0.1005\n",
      "33/223, train_loss: 0.1073, step time: 0.1335\n",
      "34/223, train_loss: 0.1154, step time: 0.1048\n",
      "35/223, train_loss: 0.1061, step time: 0.1016\n",
      "36/223, train_loss: 0.1011, step time: 0.1009\n",
      "37/223, train_loss: 0.1071, step time: 0.1416\n",
      "38/223, train_loss: 0.1076, step time: 0.1245\n",
      "39/223, train_loss: 0.0993, step time: 0.1053\n",
      "40/223, train_loss: 0.1271, step time: 0.1008\n",
      "41/223, train_loss: 0.1145, step time: 0.1042\n",
      "42/223, train_loss: 0.1111, step time: 0.1039\n",
      "43/223, train_loss: 0.1073, step time: 0.0995\n",
      "44/223, train_loss: 0.1089, step time: 0.1014\n",
      "45/223, train_loss: 0.0977, step time: 0.1141\n",
      "46/223, train_loss: 0.0980, step time: 0.1146\n",
      "47/223, train_loss: 0.1306, step time: 0.1062\n",
      "48/223, train_loss: 0.1000, step time: 0.1233\n",
      "49/223, train_loss: 0.1072, step time: 0.1332\n",
      "50/223, train_loss: 0.1086, step time: 0.1181\n",
      "51/223, train_loss: 0.0974, step time: 0.1060\n",
      "52/223, train_loss: 0.1093, step time: 0.1073\n",
      "53/223, train_loss: 0.1028, step time: 0.1061\n",
      "54/223, train_loss: 0.1083, step time: 0.1023\n",
      "55/223, train_loss: 0.0971, step time: 0.1004\n",
      "56/223, train_loss: 0.1159, step time: 0.1110\n",
      "57/223, train_loss: 0.1036, step time: 0.1040\n",
      "58/223, train_loss: 0.1082, step time: 0.1005\n",
      "59/223, train_loss: 0.1051, step time: 0.1003\n",
      "60/223, train_loss: 0.1057, step time: 0.1001\n",
      "61/223, train_loss: 0.1292, step time: 0.1212\n",
      "62/223, train_loss: 0.1156, step time: 0.1169\n",
      "63/223, train_loss: 0.1064, step time: 0.1177\n",
      "64/223, train_loss: 0.1030, step time: 0.1007\n",
      "65/223, train_loss: 0.1131, step time: 0.1166\n",
      "66/223, train_loss: 0.1127, step time: 0.1151\n",
      "67/223, train_loss: 0.0998, step time: 0.1011\n",
      "68/223, train_loss: 0.1050, step time: 0.0996\n",
      "69/223, train_loss: 0.1209, step time: 0.1145\n",
      "70/223, train_loss: 0.1034, step time: 0.1007\n",
      "71/223, train_loss: 0.1120, step time: 0.1079\n",
      "72/223, train_loss: 0.1112, step time: 0.0997\n",
      "73/223, train_loss: 0.1115, step time: 0.0995\n",
      "74/223, train_loss: 0.1027, step time: 0.1008\n",
      "75/223, train_loss: 0.0929, step time: 0.1058\n",
      "76/223, train_loss: 0.0993, step time: 0.1566\n",
      "77/223, train_loss: 0.1057, step time: 0.1085\n",
      "78/223, train_loss: 0.1024, step time: 0.1000\n",
      "79/223, train_loss: 0.1022, step time: 0.1001\n",
      "80/223, train_loss: 0.1227, step time: 0.1140\n",
      "81/223, train_loss: 0.1053, step time: 0.1094\n",
      "82/223, train_loss: 0.1029, step time: 0.1097\n",
      "83/223, train_loss: 0.1120, step time: 0.1008\n",
      "84/223, train_loss: 0.1151, step time: 0.1008\n",
      "85/223, train_loss: 0.1123, step time: 0.1073\n",
      "86/223, train_loss: 0.1067, step time: 0.1102\n",
      "87/223, train_loss: 0.1017, step time: 0.1076\n",
      "88/223, train_loss: 0.1198, step time: 0.1202\n",
      "89/223, train_loss: 0.0980, step time: 0.1041\n",
      "90/223, train_loss: 0.1054, step time: 0.1104\n",
      "91/223, train_loss: 0.0999, step time: 0.1052\n",
      "92/223, train_loss: 0.1018, step time: 0.1039\n",
      "93/223, train_loss: 0.1088, step time: 0.1296\n",
      "94/223, train_loss: 0.1116, step time: 0.1262\n",
      "95/223, train_loss: 0.1111, step time: 0.1103\n",
      "96/223, train_loss: 0.1080, step time: 0.1006\n",
      "97/223, train_loss: 0.1033, step time: 0.0997\n",
      "98/223, train_loss: 0.1053, step time: 0.0993\n",
      "99/223, train_loss: 0.1079, step time: 0.1012\n",
      "100/223, train_loss: 0.1053, step time: 0.1088\n",
      "101/223, train_loss: 0.1042, step time: 0.1048\n",
      "102/223, train_loss: 0.1046, step time: 0.1110\n",
      "103/223, train_loss: 0.1108, step time: 0.1237\n",
      "104/223, train_loss: 0.1177, step time: 0.1409\n",
      "105/223, train_loss: 0.1135, step time: 0.1157\n",
      "106/223, train_loss: 0.1163, step time: 0.1109\n",
      "107/223, train_loss: 0.1084, step time: 0.1132\n",
      "108/223, train_loss: 0.1083, step time: 0.1016\n",
      "109/223, train_loss: 0.1030, step time: 0.1097\n",
      "110/223, train_loss: 0.1063, step time: 0.0994\n",
      "111/223, train_loss: 0.1156, step time: 0.1011\n",
      "112/223, train_loss: 0.1031, step time: 0.1010\n",
      "113/223, train_loss: 0.0981, step time: 0.1253\n",
      "114/223, train_loss: 0.1099, step time: 0.1020\n",
      "115/223, train_loss: 0.1080, step time: 0.1069\n",
      "116/223, train_loss: 0.0987, step time: 0.1164\n",
      "117/223, train_loss: 0.1003, step time: 0.1006\n",
      "118/223, train_loss: 0.1076, step time: 0.1099\n",
      "119/223, train_loss: 0.1171, step time: 0.1006\n",
      "120/223, train_loss: 0.1086, step time: 0.1132\n",
      "121/223, train_loss: 0.1117, step time: 0.1187\n",
      "122/223, train_loss: 0.1035, step time: 0.0998\n",
      "123/223, train_loss: 0.0947, step time: 0.1001\n",
      "124/223, train_loss: 0.1086, step time: 0.1091\n",
      "125/223, train_loss: 0.1217, step time: 0.1367\n",
      "126/223, train_loss: 0.1154, step time: 0.1415\n",
      "127/223, train_loss: 0.1030, step time: 0.1072\n",
      "128/223, train_loss: 0.1157, step time: 0.1007\n",
      "129/223, train_loss: 0.1064, step time: 0.0997\n",
      "130/223, train_loss: 0.1026, step time: 0.1278\n",
      "131/223, train_loss: 0.1076, step time: 0.1105\n",
      "132/223, train_loss: 0.1004, step time: 0.0994\n",
      "133/223, train_loss: 0.1072, step time: 0.1094\n",
      "134/223, train_loss: 0.0937, step time: 0.1268\n",
      "135/223, train_loss: 0.1046, step time: 0.1617\n",
      "136/223, train_loss: 0.0946, step time: 0.1008\n",
      "137/223, train_loss: 0.1019, step time: 0.1077\n",
      "138/223, train_loss: 0.1045, step time: 0.1085\n",
      "139/223, train_loss: 0.1071, step time: 0.1042\n",
      "140/223, train_loss: 0.1138, step time: 0.0988\n",
      "141/223, train_loss: 0.1105, step time: 0.1040\n",
      "142/223, train_loss: 0.1092, step time: 0.1110\n",
      "143/223, train_loss: 0.1010, step time: 0.1079\n",
      "144/223, train_loss: 0.1164, step time: 0.0994\n",
      "145/223, train_loss: 0.1111, step time: 0.1003\n",
      "146/223, train_loss: 0.1139, step time: 0.1199\n",
      "147/223, train_loss: 0.1156, step time: 0.1058\n",
      "148/223, train_loss: 0.1080, step time: 0.1055\n",
      "149/223, train_loss: 0.1171, step time: 0.1214\n",
      "150/223, train_loss: 0.1190, step time: 0.1140\n",
      "151/223, train_loss: 0.1055, step time: 0.1259\n",
      "152/223, train_loss: 0.1187, step time: 0.0997\n",
      "153/223, train_loss: 0.1079, step time: 0.1048\n",
      "154/223, train_loss: 0.1088, step time: 0.1002\n",
      "155/223, train_loss: 0.1140, step time: 0.1154\n",
      "156/223, train_loss: 0.1208, step time: 0.1029\n",
      "157/223, train_loss: 0.1048, step time: 0.1127\n",
      "158/223, train_loss: 0.1031, step time: 0.1065\n",
      "159/223, train_loss: 0.1128, step time: 0.1068\n",
      "160/223, train_loss: 0.1103, step time: 0.1011\n",
      "161/223, train_loss: 0.1199, step time: 0.1079\n",
      "162/223, train_loss: 0.1006, step time: 0.1142\n",
      "163/223, train_loss: 0.1146, step time: 0.1218\n",
      "164/223, train_loss: 0.1102, step time: 0.1256\n",
      "165/223, train_loss: 0.1124, step time: 0.1041\n",
      "166/223, train_loss: 0.0978, step time: 0.1016\n",
      "167/223, train_loss: 0.1040, step time: 0.1298\n",
      "168/223, train_loss: 0.1074, step time: 0.0999\n",
      "169/223, train_loss: 0.1040, step time: 0.1004\n",
      "170/223, train_loss: 0.1098, step time: 0.1035\n",
      "171/223, train_loss: 0.1064, step time: 0.1181\n",
      "172/223, train_loss: 0.0979, step time: 0.1148\n",
      "173/223, train_loss: 0.1163, step time: 0.1149\n",
      "174/223, train_loss: 0.1182, step time: 0.1016\n",
      "175/223, train_loss: 0.0999, step time: 0.1139\n",
      "176/223, train_loss: 0.1060, step time: 0.1005\n",
      "177/223, train_loss: 0.0912, step time: 0.1027\n",
      "178/223, train_loss: 0.1193, step time: 0.1167\n",
      "179/223, train_loss: 0.1020, step time: 0.1515\n",
      "180/223, train_loss: 0.1107, step time: 0.1168\n",
      "181/223, train_loss: 0.0995, step time: 0.1073\n",
      "182/223, train_loss: 0.1053, step time: 0.1163\n",
      "183/223, train_loss: 0.1020, step time: 0.1108\n",
      "184/223, train_loss: 0.0953, step time: 0.1077\n",
      "185/223, train_loss: 0.0971, step time: 0.1079\n",
      "186/223, train_loss: 0.1007, step time: 0.1115\n",
      "187/223, train_loss: 0.1025, step time: 0.1006\n",
      "188/223, train_loss: 0.0937, step time: 0.1096\n",
      "189/223, train_loss: 0.1044, step time: 0.1162\n",
      "190/223, train_loss: 0.1195, step time: 0.1254\n",
      "191/223, train_loss: 0.1029, step time: 0.1024\n",
      "192/223, train_loss: 0.1033, step time: 0.1007\n",
      "193/223, train_loss: 0.1069, step time: 0.1113\n",
      "194/223, train_loss: 0.0992, step time: 0.1399\n",
      "195/223, train_loss: 0.1120, step time: 0.1244\n",
      "196/223, train_loss: 0.0947, step time: 0.0998\n",
      "197/223, train_loss: 0.1120, step time: 0.1094\n",
      "198/223, train_loss: 0.1088, step time: 0.1024\n",
      "199/223, train_loss: 0.1143, step time: 0.1013\n",
      "200/223, train_loss: 0.1107, step time: 0.1001\n",
      "201/223, train_loss: 0.1132, step time: 0.1082\n",
      "202/223, train_loss: 0.1125, step time: 0.1077\n",
      "203/223, train_loss: 0.0999, step time: 0.1345\n",
      "204/223, train_loss: 0.1092, step time: 0.1008\n",
      "205/223, train_loss: 0.1004, step time: 0.1226\n",
      "206/223, train_loss: 0.3089, step time: 0.1131\n",
      "207/223, train_loss: 0.1031, step time: 0.1276\n",
      "208/223, train_loss: 0.1102, step time: 0.1015\n",
      "209/223, train_loss: 0.1071, step time: 0.1016\n",
      "210/223, train_loss: 0.1145, step time: 0.1004\n",
      "211/223, train_loss: 0.1042, step time: 0.1013\n",
      "212/223, train_loss: 0.1071, step time: 0.1021\n",
      "213/223, train_loss: 0.1065, step time: 0.1134\n",
      "214/223, train_loss: 0.1101, step time: 0.1230\n",
      "215/223, train_loss: 0.1124, step time: 0.1281\n",
      "216/223, train_loss: 0.1077, step time: 0.1027\n",
      "217/223, train_loss: 0.1055, step time: 0.1018\n",
      "218/223, train_loss: 0.1048, step time: 0.1000\n",
      "219/223, train_loss: 0.1149, step time: 0.1004\n",
      "220/223, train_loss: 0.0946, step time: 0.1085\n",
      "221/223, train_loss: 0.1201, step time: 0.0989\n",
      "222/223, train_loss: 0.1031, step time: 0.1000\n",
      "223/223, train_loss: 0.0982, step time: 0.1005\n",
      "epoch 142 average loss: 0.1082\n",
      "time consuming of epoch 142 is: 88.2257\n",
      "----------\n",
      "epoch 143/300\n",
      "1/223, train_loss: 0.1194, step time: 0.1015\n",
      "2/223, train_loss: 0.1094, step time: 0.0996\n",
      "3/223, train_loss: 0.1175, step time: 0.1041\n",
      "4/223, train_loss: 0.1029, step time: 0.1003\n",
      "5/223, train_loss: 0.1073, step time: 0.1105\n",
      "6/223, train_loss: 0.1137, step time: 0.1230\n",
      "7/223, train_loss: 0.0981, step time: 0.1042\n",
      "8/223, train_loss: 0.1279, step time: 0.1196\n",
      "9/223, train_loss: 0.1029, step time: 0.1114\n",
      "10/223, train_loss: 0.1104, step time: 0.1094\n",
      "11/223, train_loss: 0.1103, step time: 0.1177\n",
      "12/223, train_loss: 0.1136, step time: 0.1217\n",
      "13/223, train_loss: 0.1027, step time: 0.1060\n",
      "14/223, train_loss: 0.1124, step time: 0.1046\n",
      "15/223, train_loss: 0.1011, step time: 0.1221\n",
      "16/223, train_loss: 0.1106, step time: 0.1009\n",
      "17/223, train_loss: 0.1091, step time: 0.1005\n",
      "18/223, train_loss: 0.1114, step time: 0.1052\n",
      "19/223, train_loss: 0.1014, step time: 0.0999\n",
      "20/223, train_loss: 0.1146, step time: 0.1173\n",
      "21/223, train_loss: 0.3005, step time: 0.1049\n",
      "22/223, train_loss: 0.1058, step time: 0.1069\n",
      "23/223, train_loss: 0.0971, step time: 0.1040\n",
      "24/223, train_loss: 0.1167, step time: 0.1204\n",
      "25/223, train_loss: 0.1043, step time: 0.1002\n",
      "26/223, train_loss: 0.1050, step time: 0.1003\n",
      "27/223, train_loss: 0.1134, step time: 0.1003\n",
      "28/223, train_loss: 0.1151, step time: 0.1002\n",
      "29/223, train_loss: 0.1111, step time: 0.1127\n",
      "30/223, train_loss: 0.1084, step time: 0.1074\n",
      "31/223, train_loss: 0.1062, step time: 0.1114\n",
      "32/223, train_loss: 0.1089, step time: 0.1009\n",
      "33/223, train_loss: 0.1016, step time: 0.1144\n",
      "34/223, train_loss: 0.1181, step time: 0.1059\n",
      "35/223, train_loss: 0.0966, step time: 0.1002\n",
      "36/223, train_loss: 0.1111, step time: 0.1006\n",
      "37/223, train_loss: 0.1029, step time: 0.1002\n",
      "38/223, train_loss: 0.1069, step time: 0.1045\n",
      "39/223, train_loss: 0.1173, step time: 0.1413\n",
      "40/223, train_loss: 0.1088, step time: 0.1098\n",
      "41/223, train_loss: 0.1075, step time: 0.1131\n",
      "42/223, train_loss: 0.1202, step time: 0.1277\n",
      "43/223, train_loss: 0.1028, step time: 0.1177\n",
      "44/223, train_loss: 0.1079, step time: 0.1106\n",
      "45/223, train_loss: 0.1068, step time: 0.1097\n",
      "46/223, train_loss: 0.1167, step time: 0.1419\n",
      "47/223, train_loss: 0.1093, step time: 0.1073\n",
      "48/223, train_loss: 0.1069, step time: 0.1156\n",
      "49/223, train_loss: 0.1049, step time: 0.1125\n",
      "50/223, train_loss: 0.1197, step time: 0.1303\n",
      "51/223, train_loss: 0.1191, step time: 0.1133\n",
      "52/223, train_loss: 0.1068, step time: 0.0999\n",
      "53/223, train_loss: 0.1096, step time: 0.1148\n",
      "54/223, train_loss: 0.1189, step time: 0.1071\n",
      "55/223, train_loss: 0.1096, step time: 0.1113\n",
      "56/223, train_loss: 0.1024, step time: 0.1004\n",
      "57/223, train_loss: 0.1030, step time: 0.1269\n",
      "58/223, train_loss: 0.1234, step time: 0.1052\n",
      "59/223, train_loss: 0.1241, step time: 0.1000\n",
      "60/223, train_loss: 0.1065, step time: 0.1006\n",
      "61/223, train_loss: 0.0974, step time: 0.1120\n",
      "62/223, train_loss: 0.1076, step time: 0.1088\n",
      "63/223, train_loss: 0.1227, step time: 0.1178\n",
      "64/223, train_loss: 0.0973, step time: 0.1003\n",
      "65/223, train_loss: 0.0991, step time: 0.1041\n",
      "66/223, train_loss: 0.1063, step time: 0.1138\n",
      "67/223, train_loss: 0.1003, step time: 0.1177\n",
      "68/223, train_loss: 0.0988, step time: 0.1119\n",
      "69/223, train_loss: 0.1023, step time: 0.1174\n",
      "70/223, train_loss: 0.1175, step time: 0.1003\n",
      "71/223, train_loss: 0.0998, step time: 0.1003\n",
      "72/223, train_loss: 0.1000, step time: 0.1000\n",
      "73/223, train_loss: 0.1011, step time: 0.1213\n",
      "74/223, train_loss: 0.1119, step time: 0.1128\n",
      "75/223, train_loss: 0.1126, step time: 0.1071\n",
      "76/223, train_loss: 0.1110, step time: 0.1184\n",
      "77/223, train_loss: 0.1072, step time: 0.1132\n",
      "78/223, train_loss: 0.1102, step time: 0.1060\n",
      "79/223, train_loss: 0.1156, step time: 0.1251\n",
      "80/223, train_loss: 0.1026, step time: 0.0993\n",
      "81/223, train_loss: 0.1042, step time: 0.1082\n",
      "82/223, train_loss: 0.1163, step time: 0.1016\n",
      "83/223, train_loss: 0.1044, step time: 0.1034\n",
      "84/223, train_loss: 0.1178, step time: 0.1017\n",
      "85/223, train_loss: 0.1168, step time: 0.1134\n",
      "86/223, train_loss: 0.1056, step time: 0.1108\n",
      "87/223, train_loss: 0.1028, step time: 0.1341\n",
      "88/223, train_loss: 0.1061, step time: 0.1001\n",
      "89/223, train_loss: 0.0962, step time: 0.1232\n",
      "90/223, train_loss: 0.1055, step time: 0.1434\n",
      "91/223, train_loss: 0.1040, step time: 0.1133\n",
      "92/223, train_loss: 0.1051, step time: 0.0999\n",
      "93/223, train_loss: 0.1076, step time: 0.1192\n",
      "94/223, train_loss: 0.1060, step time: 0.1075\n",
      "95/223, train_loss: 0.1081, step time: 0.1204\n",
      "96/223, train_loss: 0.1085, step time: 0.1002\n",
      "97/223, train_loss: 0.0992, step time: 0.1007\n",
      "98/223, train_loss: 0.1039, step time: 0.1015\n",
      "99/223, train_loss: 0.0975, step time: 0.1198\n",
      "100/223, train_loss: 0.0949, step time: 0.1181\n",
      "101/223, train_loss: 0.1189, step time: 0.1093\n",
      "102/223, train_loss: 0.1048, step time: 0.1003\n",
      "103/223, train_loss: 0.1119, step time: 0.1126\n",
      "104/223, train_loss: 0.1117, step time: 0.1075\n",
      "105/223, train_loss: 0.1019, step time: 0.1744\n",
      "106/223, train_loss: 0.0950, step time: 0.1460\n",
      "107/223, train_loss: 0.1025, step time: 0.0995\n",
      "108/223, train_loss: 0.0985, step time: 0.0991\n",
      "109/223, train_loss: 0.1073, step time: 0.1186\n",
      "110/223, train_loss: 0.1104, step time: 0.1105\n",
      "111/223, train_loss: 0.1043, step time: 0.1172\n",
      "112/223, train_loss: 0.1018, step time: 0.1099\n",
      "113/223, train_loss: 0.1072, step time: 0.1083\n",
      "114/223, train_loss: 0.1082, step time: 0.1080\n",
      "115/223, train_loss: 0.1118, step time: 0.1464\n",
      "116/223, train_loss: 0.1051, step time: 0.1144\n",
      "117/223, train_loss: 0.1104, step time: 0.1181\n",
      "118/223, train_loss: 0.0943, step time: 0.1108\n",
      "119/223, train_loss: 0.0993, step time: 0.1051\n",
      "120/223, train_loss: 0.1081, step time: 0.1060\n",
      "121/223, train_loss: 0.1080, step time: 0.1089\n",
      "122/223, train_loss: 0.1067, step time: 0.1171\n",
      "123/223, train_loss: 0.1142, step time: 0.1154\n",
      "124/223, train_loss: 0.1088, step time: 0.1167\n",
      "125/223, train_loss: 0.1078, step time: 0.1138\n",
      "126/223, train_loss: 0.0999, step time: 0.1163\n",
      "127/223, train_loss: 0.0972, step time: 0.1184\n",
      "128/223, train_loss: 0.1192, step time: 0.1076\n",
      "129/223, train_loss: 0.1000, step time: 0.1095\n",
      "130/223, train_loss: 0.1006, step time: 0.1093\n",
      "131/223, train_loss: 0.0985, step time: 0.1227\n",
      "132/223, train_loss: 0.1114, step time: 0.1002\n",
      "133/223, train_loss: 0.1199, step time: 0.1206\n",
      "134/223, train_loss: 0.0958, step time: 0.1007\n",
      "135/223, train_loss: 0.1129, step time: 0.1007\n",
      "136/223, train_loss: 0.1165, step time: 0.1009\n",
      "137/223, train_loss: 0.1004, step time: 0.1030\n",
      "138/223, train_loss: 0.0925, step time: 0.1294\n",
      "139/223, train_loss: 0.1167, step time: 0.0999\n",
      "140/223, train_loss: 0.1166, step time: 0.1087\n",
      "141/223, train_loss: 0.1015, step time: 0.1538\n",
      "142/223, train_loss: 0.1096, step time: 0.1285\n",
      "143/223, train_loss: 0.1168, step time: 0.1003\n",
      "144/223, train_loss: 0.1135, step time: 0.0998\n",
      "145/223, train_loss: 0.0996, step time: 0.1089\n",
      "146/223, train_loss: 0.1000, step time: 0.1382\n",
      "147/223, train_loss: 0.1071, step time: 0.1017\n",
      "148/223, train_loss: 0.1016, step time: 0.1010\n",
      "149/223, train_loss: 0.1007, step time: 0.1062\n",
      "150/223, train_loss: 0.1003, step time: 0.1002\n",
      "151/223, train_loss: 0.0992, step time: 0.1009\n",
      "152/223, train_loss: 0.1002, step time: 0.1023\n",
      "153/223, train_loss: 0.1025, step time: 0.1121\n",
      "154/223, train_loss: 0.1095, step time: 0.0998\n",
      "155/223, train_loss: 0.1045, step time: 0.1004\n",
      "156/223, train_loss: 0.1142, step time: 0.1008\n",
      "157/223, train_loss: 0.0979, step time: 0.1102\n",
      "158/223, train_loss: 0.0985, step time: 0.1099\n",
      "159/223, train_loss: 0.1155, step time: 0.1040\n",
      "160/223, train_loss: 0.0983, step time: 0.1028\n",
      "161/223, train_loss: 0.1106, step time: 0.1128\n",
      "162/223, train_loss: 0.1063, step time: 0.1183\n",
      "163/223, train_loss: 0.1104, step time: 0.1055\n",
      "164/223, train_loss: 0.1054, step time: 0.1000\n",
      "165/223, train_loss: 0.1169, step time: 0.0996\n",
      "166/223, train_loss: 0.1193, step time: 0.1134\n",
      "167/223, train_loss: 0.1047, step time: 0.1015\n",
      "168/223, train_loss: 0.1010, step time: 0.0993\n",
      "169/223, train_loss: 0.1051, step time: 0.1391\n",
      "170/223, train_loss: 0.1070, step time: 0.1014\n",
      "171/223, train_loss: 0.0990, step time: 0.1087\n",
      "172/223, train_loss: 0.1032, step time: 0.1138\n",
      "173/223, train_loss: 0.1172, step time: 0.1013\n",
      "174/223, train_loss: 0.1152, step time: 0.1212\n",
      "175/223, train_loss: 0.0943, step time: 0.1172\n",
      "176/223, train_loss: 0.1076, step time: 0.1143\n",
      "177/223, train_loss: 0.0982, step time: 0.0987\n",
      "178/223, train_loss: 0.1117, step time: 0.1029\n",
      "179/223, train_loss: 0.1048, step time: 0.1207\n",
      "180/223, train_loss: 0.1160, step time: 0.1181\n",
      "181/223, train_loss: 0.1098, step time: 0.1183\n",
      "182/223, train_loss: 0.1032, step time: 0.0996\n",
      "183/223, train_loss: 0.0955, step time: 0.1015\n",
      "184/223, train_loss: 0.1015, step time: 0.1224\n",
      "185/223, train_loss: 0.0975, step time: 0.1093\n",
      "186/223, train_loss: 0.1086, step time: 0.1216\n",
      "187/223, train_loss: 0.1040, step time: 0.1183\n",
      "188/223, train_loss: 0.1041, step time: 0.1011\n",
      "189/223, train_loss: 0.1174, step time: 0.1094\n",
      "190/223, train_loss: 0.1142, step time: 0.1191\n",
      "191/223, train_loss: 0.1086, step time: 0.1150\n",
      "192/223, train_loss: 0.0966, step time: 0.1305\n",
      "193/223, train_loss: 0.0966, step time: 0.1080\n",
      "194/223, train_loss: 0.1120, step time: 0.1052\n",
      "195/223, train_loss: 0.0958, step time: 0.1005\n",
      "196/223, train_loss: 0.1182, step time: 0.1164\n",
      "197/223, train_loss: 0.1150, step time: 0.1083\n",
      "198/223, train_loss: 0.1087, step time: 0.1005\n",
      "199/223, train_loss: 0.1005, step time: 0.1337\n",
      "200/223, train_loss: 0.1099, step time: 0.1304\n",
      "201/223, train_loss: 0.1022, step time: 0.1132\n",
      "202/223, train_loss: 0.1115, step time: 0.1087\n",
      "203/223, train_loss: 0.1200, step time: 0.1053\n",
      "204/223, train_loss: 0.1217, step time: 0.1000\n",
      "205/223, train_loss: 0.1038, step time: 0.1059\n",
      "206/223, train_loss: 0.1032, step time: 0.1031\n",
      "207/223, train_loss: 0.1043, step time: 0.1424\n",
      "208/223, train_loss: 0.1123, step time: 0.1004\n",
      "209/223, train_loss: 0.1100, step time: 0.1161\n",
      "210/223, train_loss: 0.0994, step time: 0.1590\n",
      "211/223, train_loss: 0.0983, step time: 0.1294\n",
      "212/223, train_loss: 0.1047, step time: 0.1103\n",
      "213/223, train_loss: 0.0941, step time: 0.1101\n",
      "214/223, train_loss: 0.1085, step time: 0.1208\n",
      "215/223, train_loss: 0.1146, step time: 0.1149\n",
      "216/223, train_loss: 0.1170, step time: 0.1052\n",
      "217/223, train_loss: 0.1035, step time: 0.1087\n",
      "218/223, train_loss: 0.1159, step time: 0.1012\n",
      "219/223, train_loss: 0.1054, step time: 0.1011\n",
      "220/223, train_loss: 0.1059, step time: 0.1042\n",
      "221/223, train_loss: 0.1100, step time: 0.1007\n",
      "222/223, train_loss: 0.1156, step time: 0.1002\n",
      "223/223, train_loss: 0.1031, step time: 0.1005\n",
      "epoch 143 average loss: 0.1081\n",
      "time consuming of epoch 143 is: 87.2459\n",
      "----------\n",
      "epoch 144/300\n",
      "1/223, train_loss: 0.1109, step time: 0.1148\n",
      "2/223, train_loss: 0.1167, step time: 0.1171\n",
      "3/223, train_loss: 0.1141, step time: 0.1003\n",
      "4/223, train_loss: 0.1336, step time: 0.1191\n",
      "5/223, train_loss: 0.1169, step time: 0.1001\n",
      "6/223, train_loss: 0.1026, step time: 0.1051\n",
      "7/223, train_loss: 0.1033, step time: 0.1086\n",
      "8/223, train_loss: 0.1083, step time: 0.1001\n",
      "9/223, train_loss: 0.1131, step time: 0.1148\n",
      "10/223, train_loss: 0.1013, step time: 0.1228\n",
      "11/223, train_loss: 0.1042, step time: 0.1168\n",
      "12/223, train_loss: 0.1082, step time: 0.1007\n",
      "13/223, train_loss: 0.0997, step time: 0.1052\n",
      "14/223, train_loss: 0.0920, step time: 0.1116\n",
      "15/223, train_loss: 0.1028, step time: 0.0993\n",
      "16/223, train_loss: 0.1037, step time: 0.1004\n",
      "17/223, train_loss: 0.1050, step time: 0.1132\n",
      "18/223, train_loss: 0.1145, step time: 0.1003\n",
      "19/223, train_loss: 0.1126, step time: 0.1193\n",
      "20/223, train_loss: 0.1216, step time: 0.1072\n",
      "21/223, train_loss: 0.1006, step time: 0.1060\n",
      "22/223, train_loss: 0.0979, step time: 0.1229\n",
      "23/223, train_loss: 0.1132, step time: 0.1096\n",
      "24/223, train_loss: 0.1117, step time: 0.1009\n",
      "25/223, train_loss: 0.1026, step time: 0.1010\n",
      "26/223, train_loss: 0.1174, step time: 0.1148\n",
      "27/223, train_loss: 0.1115, step time: 0.0999\n",
      "28/223, train_loss: 0.1085, step time: 0.0998\n",
      "29/223, train_loss: 0.1062, step time: 0.1006\n",
      "30/223, train_loss: 0.1145, step time: 0.1013\n",
      "31/223, train_loss: 0.1153, step time: 0.0996\n",
      "32/223, train_loss: 0.0959, step time: 0.1003\n",
      "33/223, train_loss: 0.1041, step time: 0.1001\n",
      "34/223, train_loss: 0.1004, step time: 0.1079\n",
      "35/223, train_loss: 0.1054, step time: 0.1003\n",
      "36/223, train_loss: 0.1212, step time: 0.1174\n",
      "37/223, train_loss: 0.1151, step time: 0.1005\n",
      "38/223, train_loss: 0.1065, step time: 0.1127\n",
      "39/223, train_loss: 0.1155, step time: 0.1001\n",
      "40/223, train_loss: 0.1063, step time: 0.1009\n",
      "41/223, train_loss: 0.1127, step time: 0.1071\n",
      "42/223, train_loss: 0.1085, step time: 0.1592\n",
      "43/223, train_loss: 0.1065, step time: 0.1053\n",
      "44/223, train_loss: 0.1057, step time: 0.1274\n",
      "45/223, train_loss: 0.1096, step time: 0.1291\n",
      "46/223, train_loss: 0.1026, step time: 0.1114\n",
      "47/223, train_loss: 0.0978, step time: 0.1002\n",
      "48/223, train_loss: 0.1116, step time: 0.1161\n",
      "49/223, train_loss: 0.1161, step time: 0.1125\n",
      "50/223, train_loss: 0.0970, step time: 0.0994\n",
      "51/223, train_loss: 0.0963, step time: 0.1354\n",
      "52/223, train_loss: 0.1052, step time: 0.1244\n",
      "53/223, train_loss: 0.1062, step time: 0.1151\n",
      "54/223, train_loss: 0.1073, step time: 0.1056\n",
      "55/223, train_loss: 0.1248, step time: 0.1242\n",
      "56/223, train_loss: 0.1081, step time: 0.1089\n",
      "57/223, train_loss: 0.1223, step time: 0.1116\n",
      "58/223, train_loss: 0.1094, step time: 0.1181\n",
      "59/223, train_loss: 0.0926, step time: 0.1070\n",
      "60/223, train_loss: 0.1090, step time: 0.1058\n",
      "61/223, train_loss: 0.1087, step time: 0.1008\n",
      "62/223, train_loss: 0.1157, step time: 0.1012\n",
      "63/223, train_loss: 0.1020, step time: 0.1030\n",
      "64/223, train_loss: 0.1030, step time: 0.1000\n",
      "65/223, train_loss: 0.1137, step time: 0.1000\n",
      "66/223, train_loss: 0.1027, step time: 0.0998\n",
      "67/223, train_loss: 0.1028, step time: 0.1169\n",
      "68/223, train_loss: 0.0942, step time: 0.1005\n",
      "69/223, train_loss: 0.1010, step time: 0.1001\n",
      "70/223, train_loss: 0.1099, step time: 0.1143\n",
      "71/223, train_loss: 0.1050, step time: 0.1128\n",
      "72/223, train_loss: 0.1040, step time: 0.1111\n",
      "73/223, train_loss: 0.1003, step time: 0.1108\n",
      "74/223, train_loss: 0.1039, step time: 0.1001\n",
      "75/223, train_loss: 0.1089, step time: 0.1092\n",
      "76/223, train_loss: 0.1129, step time: 0.1039\n",
      "77/223, train_loss: 0.1045, step time: 0.1182\n",
      "78/223, train_loss: 0.1166, step time: 0.1141\n",
      "79/223, train_loss: 0.1181, step time: 0.1035\n",
      "80/223, train_loss: 0.1018, step time: 0.1018\n",
      "81/223, train_loss: 0.0964, step time: 0.1202\n",
      "82/223, train_loss: 0.1110, step time: 0.1012\n",
      "83/223, train_loss: 0.1102, step time: 0.1378\n",
      "84/223, train_loss: 0.1022, step time: 0.1110\n",
      "85/223, train_loss: 0.1037, step time: 0.1205\n",
      "86/223, train_loss: 0.0925, step time: 0.1057\n",
      "87/223, train_loss: 0.0962, step time: 0.1000\n",
      "88/223, train_loss: 0.1098, step time: 0.1082\n",
      "89/223, train_loss: 0.1094, step time: 0.1223\n",
      "90/223, train_loss: 0.0965, step time: 0.1008\n",
      "91/223, train_loss: 0.1010, step time: 0.0996\n",
      "92/223, train_loss: 0.1011, step time: 0.1000\n",
      "93/223, train_loss: 0.1048, step time: 0.0998\n",
      "94/223, train_loss: 0.1060, step time: 0.1015\n",
      "95/223, train_loss: 0.0979, step time: 0.1150\n",
      "96/223, train_loss: 0.3188, step time: 0.1179\n",
      "97/223, train_loss: 0.0996, step time: 0.1001\n",
      "98/223, train_loss: 0.1169, step time: 0.1006\n",
      "99/223, train_loss: 0.1097, step time: 0.1090\n",
      "100/223, train_loss: 0.1033, step time: 0.1254\n",
      "101/223, train_loss: 0.1108, step time: 0.1205\n",
      "102/223, train_loss: 0.1011, step time: 0.1007\n",
      "103/223, train_loss: 0.0992, step time: 0.1003\n",
      "104/223, train_loss: 0.1224, step time: 0.1002\n",
      "105/223, train_loss: 0.1049, step time: 0.1008\n",
      "106/223, train_loss: 0.1247, step time: 0.1005\n",
      "107/223, train_loss: 0.1014, step time: 0.1052\n",
      "108/223, train_loss: 0.1021, step time: 0.1011\n",
      "109/223, train_loss: 0.1081, step time: 0.1020\n",
      "110/223, train_loss: 0.1042, step time: 0.1009\n",
      "111/223, train_loss: 0.1003, step time: 0.1223\n",
      "112/223, train_loss: 0.1190, step time: 0.1076\n",
      "113/223, train_loss: 0.1111, step time: 0.1175\n",
      "114/223, train_loss: 0.1228, step time: 0.1204\n",
      "115/223, train_loss: 0.1164, step time: 0.1250\n",
      "116/223, train_loss: 0.1173, step time: 0.1349\n",
      "117/223, train_loss: 0.1046, step time: 0.1005\n",
      "118/223, train_loss: 0.1022, step time: 0.1002\n",
      "119/223, train_loss: 0.1073, step time: 0.1292\n",
      "120/223, train_loss: 0.1121, step time: 0.1006\n",
      "121/223, train_loss: 0.1215, step time: 0.1137\n",
      "122/223, train_loss: 0.1106, step time: 0.1060\n",
      "123/223, train_loss: 0.1041, step time: 0.1141\n",
      "124/223, train_loss: 0.1203, step time: 0.1057\n",
      "125/223, train_loss: 0.1142, step time: 0.1237\n",
      "126/223, train_loss: 0.1102, step time: 0.1129\n",
      "127/223, train_loss: 0.1101, step time: 0.1145\n",
      "128/223, train_loss: 0.1122, step time: 0.1123\n",
      "129/223, train_loss: 0.1095, step time: 0.1202\n",
      "130/223, train_loss: 0.1028, step time: 0.1002\n",
      "131/223, train_loss: 0.1055, step time: 0.0998\n",
      "132/223, train_loss: 0.1119, step time: 0.1253\n",
      "133/223, train_loss: 0.1189, step time: 0.0999\n",
      "134/223, train_loss: 0.1104, step time: 0.1011\n",
      "135/223, train_loss: 0.1118, step time: 0.1075\n",
      "136/223, train_loss: 0.1072, step time: 0.1000\n",
      "137/223, train_loss: 0.1048, step time: 0.1072\n",
      "138/223, train_loss: 0.1107, step time: 0.1075\n",
      "139/223, train_loss: 0.1049, step time: 0.1203\n",
      "140/223, train_loss: 0.1047, step time: 0.1287\n",
      "141/223, train_loss: 0.0933, step time: 0.1809\n",
      "142/223, train_loss: 0.1000, step time: 0.1097\n",
      "143/223, train_loss: 0.1056, step time: 0.1147\n",
      "144/223, train_loss: 0.1100, step time: 0.1152\n",
      "145/223, train_loss: 0.1230, step time: 0.1163\n",
      "146/223, train_loss: 0.1013, step time: 0.1506\n",
      "147/223, train_loss: 0.1087, step time: 0.1059\n",
      "148/223, train_loss: 0.1111, step time: 0.1008\n",
      "149/223, train_loss: 0.1004, step time: 0.1137\n",
      "150/223, train_loss: 0.1176, step time: 0.1742\n",
      "151/223, train_loss: 0.1092, step time: 0.0999\n",
      "152/223, train_loss: 0.1067, step time: 0.0993\n",
      "153/223, train_loss: 0.1063, step time: 0.0989\n",
      "154/223, train_loss: 0.1140, step time: 0.0998\n",
      "155/223, train_loss: 0.1096, step time: 0.1004\n",
      "156/223, train_loss: 0.1022, step time: 0.0988\n",
      "157/223, train_loss: 0.1089, step time: 0.0992\n",
      "158/223, train_loss: 0.1055, step time: 0.1015\n",
      "159/223, train_loss: 0.1101, step time: 0.0997\n",
      "160/223, train_loss: 0.1182, step time: 0.0988\n",
      "161/223, train_loss: 0.1056, step time: 0.1003\n",
      "162/223, train_loss: 0.1072, step time: 0.1016\n",
      "163/223, train_loss: 0.1084, step time: 0.0993\n",
      "164/223, train_loss: 0.1070, step time: 0.0993\n",
      "165/223, train_loss: 0.1064, step time: 0.0999\n",
      "166/223, train_loss: 0.1059, step time: 0.1031\n",
      "167/223, train_loss: 0.1071, step time: 0.0998\n",
      "168/223, train_loss: 0.1146, step time: 0.0998\n",
      "169/223, train_loss: 0.1172, step time: 0.0993\n",
      "170/223, train_loss: 0.1065, step time: 0.1017\n",
      "171/223, train_loss: 0.1197, step time: 0.1061\n",
      "172/223, train_loss: 0.1127, step time: 0.1199\n",
      "173/223, train_loss: 0.1070, step time: 0.0998\n",
      "174/223, train_loss: 0.1174, step time: 0.1090\n",
      "175/223, train_loss: 0.1121, step time: 0.1008\n",
      "176/223, train_loss: 0.1088, step time: 0.1096\n",
      "177/223, train_loss: 0.1102, step time: 0.1321\n",
      "178/223, train_loss: 0.1213, step time: 0.1219\n",
      "179/223, train_loss: 0.0983, step time: 0.1292\n",
      "180/223, train_loss: 0.1094, step time: 0.0998\n",
      "181/223, train_loss: 0.1127, step time: 0.1016\n",
      "182/223, train_loss: 0.0963, step time: 0.1023\n",
      "183/223, train_loss: 0.1021, step time: 0.1057\n",
      "184/223, train_loss: 0.1059, step time: 0.1151\n",
      "185/223, train_loss: 0.0971, step time: 0.1484\n",
      "186/223, train_loss: 0.1023, step time: 0.1148\n",
      "187/223, train_loss: 0.0976, step time: 0.1114\n",
      "188/223, train_loss: 0.1009, step time: 0.1233\n",
      "189/223, train_loss: 0.1080, step time: 0.1156\n",
      "190/223, train_loss: 0.1024, step time: 0.0994\n",
      "191/223, train_loss: 0.1078, step time: 0.1106\n",
      "192/223, train_loss: 0.1174, step time: 0.1001\n",
      "193/223, train_loss: 0.1008, step time: 0.1188\n",
      "194/223, train_loss: 0.1014, step time: 0.1098\n",
      "195/223, train_loss: 0.1147, step time: 0.1171\n",
      "196/223, train_loss: 0.0959, step time: 0.1003\n",
      "197/223, train_loss: 0.0958, step time: 0.1006\n",
      "198/223, train_loss: 0.1061, step time: 0.1027\n",
      "199/223, train_loss: 0.0927, step time: 0.1294\n",
      "200/223, train_loss: 0.0990, step time: 0.1314\n",
      "201/223, train_loss: 0.1172, step time: 0.1138\n",
      "202/223, train_loss: 0.1023, step time: 0.1012\n",
      "203/223, train_loss: 0.1038, step time: 0.1102\n",
      "204/223, train_loss: 0.1018, step time: 0.1009\n",
      "205/223, train_loss: 0.1000, step time: 0.1010\n",
      "206/223, train_loss: 0.1020, step time: 0.1016\n",
      "207/223, train_loss: 0.1070, step time: 0.1139\n",
      "208/223, train_loss: 0.1195, step time: 0.1069\n",
      "209/223, train_loss: 0.1009, step time: 0.1001\n",
      "210/223, train_loss: 0.1113, step time: 0.1010\n",
      "211/223, train_loss: 0.1017, step time: 0.1044\n",
      "212/223, train_loss: 0.1064, step time: 0.1281\n",
      "213/223, train_loss: 0.1071, step time: 0.1112\n",
      "214/223, train_loss: 0.1068, step time: 0.0997\n",
      "215/223, train_loss: 0.0972, step time: 0.1033\n",
      "216/223, train_loss: 0.1024, step time: 0.1000\n",
      "217/223, train_loss: 0.1055, step time: 0.1011\n",
      "218/223, train_loss: 0.1008, step time: 0.1004\n",
      "219/223, train_loss: 0.0984, step time: 0.0999\n",
      "220/223, train_loss: 0.1172, step time: 0.0999\n",
      "221/223, train_loss: 0.1111, step time: 0.1006\n",
      "222/223, train_loss: 0.0980, step time: 0.1040\n",
      "223/223, train_loss: 0.1128, step time: 0.0995\n",
      "epoch 144 average loss: 0.1084\n",
      "time consuming of epoch 144 is: 93.2847\n",
      "----------\n",
      "epoch 145/300\n",
      "1/223, train_loss: 0.1026, step time: 0.1216\n",
      "2/223, train_loss: 0.1025, step time: 0.1087\n",
      "3/223, train_loss: 0.1088, step time: 0.1242\n",
      "4/223, train_loss: 0.1020, step time: 0.1045\n",
      "5/223, train_loss: 0.1084, step time: 0.1101\n",
      "6/223, train_loss: 0.1109, step time: 0.1043\n",
      "7/223, train_loss: 0.0975, step time: 0.1092\n",
      "8/223, train_loss: 0.1241, step time: 0.1167\n",
      "9/223, train_loss: 0.1097, step time: 0.1002\n",
      "10/223, train_loss: 0.1176, step time: 0.1009\n",
      "11/223, train_loss: 0.1021, step time: 0.1130\n",
      "12/223, train_loss: 0.3036, step time: 0.1179\n",
      "13/223, train_loss: 0.0985, step time: 0.1031\n",
      "14/223, train_loss: 0.1009, step time: 0.1010\n",
      "15/223, train_loss: 0.1137, step time: 0.1088\n",
      "16/223, train_loss: 0.1120, step time: 0.1104\n",
      "17/223, train_loss: 0.1020, step time: 0.1095\n",
      "18/223, train_loss: 0.1193, step time: 0.1006\n",
      "19/223, train_loss: 0.1011, step time: 0.1125\n",
      "20/223, train_loss: 0.1174, step time: 0.1086\n",
      "21/223, train_loss: 0.1028, step time: 0.1016\n",
      "22/223, train_loss: 0.0952, step time: 0.1009\n",
      "23/223, train_loss: 0.1056, step time: 0.1219\n",
      "24/223, train_loss: 0.1045, step time: 0.1224\n",
      "25/223, train_loss: 0.1124, step time: 0.1004\n",
      "26/223, train_loss: 0.1086, step time: 0.1020\n",
      "27/223, train_loss: 0.1128, step time: 0.1608\n",
      "28/223, train_loss: 0.0980, step time: 0.1178\n",
      "29/223, train_loss: 0.1088, step time: 0.1095\n",
      "30/223, train_loss: 0.1137, step time: 0.1238\n",
      "31/223, train_loss: 0.0976, step time: 0.1245\n",
      "32/223, train_loss: 0.1084, step time: 0.1089\n",
      "33/223, train_loss: 0.0993, step time: 0.1367\n",
      "34/223, train_loss: 0.0881, step time: 0.1273\n",
      "35/223, train_loss: 0.1084, step time: 0.1010\n",
      "36/223, train_loss: 0.1074, step time: 0.1006\n",
      "37/223, train_loss: 0.1073, step time: 0.1166\n",
      "38/223, train_loss: 0.1014, step time: 0.1087\n",
      "39/223, train_loss: 0.1039, step time: 0.1080\n",
      "40/223, train_loss: 0.1062, step time: 0.1107\n",
      "41/223, train_loss: 0.0998, step time: 0.1068\n",
      "42/223, train_loss: 0.1142, step time: 0.1002\n",
      "43/223, train_loss: 0.1130, step time: 0.1006\n",
      "44/223, train_loss: 0.0987, step time: 0.1041\n",
      "45/223, train_loss: 0.1098, step time: 0.1134\n",
      "46/223, train_loss: 0.1126, step time: 0.1360\n",
      "47/223, train_loss: 0.1113, step time: 0.1233\n",
      "48/223, train_loss: 0.1204, step time: 0.1003\n",
      "49/223, train_loss: 0.1115, step time: 0.1176\n",
      "50/223, train_loss: 0.1028, step time: 0.1064\n",
      "51/223, train_loss: 0.1091, step time: 0.1026\n",
      "52/223, train_loss: 0.1116, step time: 0.1078\n",
      "53/223, train_loss: 0.1162, step time: 0.1237\n",
      "54/223, train_loss: 0.1148, step time: 0.1031\n",
      "55/223, train_loss: 0.1152, step time: 0.1007\n",
      "56/223, train_loss: 0.1077, step time: 0.1003\n",
      "57/223, train_loss: 0.1009, step time: 0.1036\n",
      "58/223, train_loss: 0.1167, step time: 0.1004\n",
      "59/223, train_loss: 0.1202, step time: 0.1007\n",
      "60/223, train_loss: 0.1017, step time: 0.1011\n",
      "61/223, train_loss: 0.1143, step time: 0.1069\n",
      "62/223, train_loss: 0.0996, step time: 0.1143\n",
      "63/223, train_loss: 0.1035, step time: 0.1228\n",
      "64/223, train_loss: 0.1080, step time: 0.1203\n",
      "65/223, train_loss: 0.1095, step time: 0.1142\n",
      "66/223, train_loss: 0.1103, step time: 0.1154\n",
      "67/223, train_loss: 0.1146, step time: 0.1000\n",
      "68/223, train_loss: 0.0979, step time: 0.1067\n",
      "69/223, train_loss: 0.1195, step time: 0.1144\n",
      "70/223, train_loss: 0.1049, step time: 0.1121\n",
      "71/223, train_loss: 0.1128, step time: 0.1398\n",
      "72/223, train_loss: 0.1098, step time: 0.1076\n",
      "73/223, train_loss: 0.1063, step time: 0.1219\n",
      "74/223, train_loss: 0.0935, step time: 0.1331\n",
      "75/223, train_loss: 0.0996, step time: 0.1401\n",
      "76/223, train_loss: 0.1102, step time: 0.1020\n",
      "77/223, train_loss: 0.1114, step time: 0.1086\n",
      "78/223, train_loss: 0.1080, step time: 0.1335\n",
      "79/223, train_loss: 0.1120, step time: 0.1347\n",
      "80/223, train_loss: 0.0953, step time: 0.1159\n",
      "81/223, train_loss: 0.1021, step time: 0.1140\n",
      "82/223, train_loss: 0.1201, step time: 0.1370\n",
      "83/223, train_loss: 0.1012, step time: 0.1209\n",
      "84/223, train_loss: 0.1122, step time: 0.1363\n",
      "85/223, train_loss: 0.1114, step time: 0.1179\n",
      "86/223, train_loss: 0.1114, step time: 0.1062\n",
      "87/223, train_loss: 0.1091, step time: 0.1140\n",
      "88/223, train_loss: 0.1070, step time: 0.1217\n",
      "89/223, train_loss: 0.1063, step time: 0.1021\n",
      "90/223, train_loss: 0.1104, step time: 0.1072\n",
      "91/223, train_loss: 0.1082, step time: 0.1094\n",
      "92/223, train_loss: 0.1274, step time: 0.1256\n",
      "93/223, train_loss: 0.1157, step time: 0.1165\n",
      "94/223, train_loss: 0.1193, step time: 0.1004\n",
      "95/223, train_loss: 0.1151, step time: 0.1156\n",
      "96/223, train_loss: 0.0998, step time: 0.1495\n",
      "97/223, train_loss: 0.0982, step time: 0.1178\n",
      "98/223, train_loss: 0.0994, step time: 0.1193\n",
      "99/223, train_loss: 0.0984, step time: 0.1120\n",
      "100/223, train_loss: 0.1141, step time: 0.1258\n",
      "101/223, train_loss: 0.1039, step time: 0.1002\n",
      "102/223, train_loss: 0.1028, step time: 0.1023\n",
      "103/223, train_loss: 0.1110, step time: 0.1010\n",
      "104/223, train_loss: 0.1036, step time: 0.1008\n",
      "105/223, train_loss: 0.1065, step time: 0.1463\n",
      "106/223, train_loss: 0.1038, step time: 0.1188\n",
      "107/223, train_loss: 0.1139, step time: 0.1096\n",
      "108/223, train_loss: 0.1078, step time: 0.1282\n",
      "109/223, train_loss: 0.1084, step time: 0.1108\n",
      "110/223, train_loss: 0.1067, step time: 0.1073\n",
      "111/223, train_loss: 0.1080, step time: 0.1281\n",
      "112/223, train_loss: 0.1072, step time: 0.1235\n",
      "113/223, train_loss: 0.1133, step time: 0.1054\n",
      "114/223, train_loss: 0.1112, step time: 0.1086\n",
      "115/223, train_loss: 0.1169, step time: 0.1065\n",
      "116/223, train_loss: 0.1007, step time: 0.1021\n",
      "117/223, train_loss: 0.0995, step time: 0.1362\n",
      "118/223, train_loss: 0.0991, step time: 0.1125\n",
      "119/223, train_loss: 0.1109, step time: 0.1193\n",
      "120/223, train_loss: 0.1084, step time: 0.1310\n",
      "121/223, train_loss: 0.1077, step time: 0.1060\n",
      "122/223, train_loss: 0.0956, step time: 0.1024\n",
      "123/223, train_loss: 0.0999, step time: 0.1011\n",
      "124/223, train_loss: 0.0970, step time: 0.1003\n",
      "125/223, train_loss: 0.1221, step time: 0.1074\n",
      "126/223, train_loss: 0.1001, step time: 0.1000\n",
      "127/223, train_loss: 0.1172, step time: 0.1003\n",
      "128/223, train_loss: 0.1129, step time: 0.1281\n",
      "129/223, train_loss: 0.1105, step time: 0.1156\n",
      "130/223, train_loss: 0.1087, step time: 0.0996\n",
      "131/223, train_loss: 0.1080, step time: 0.0997\n",
      "132/223, train_loss: 0.1071, step time: 0.0999\n",
      "133/223, train_loss: 0.1122, step time: 0.1264\n",
      "134/223, train_loss: 0.1148, step time: 0.1030\n",
      "135/223, train_loss: 0.1010, step time: 0.1051\n",
      "136/223, train_loss: 0.1243, step time: 0.1005\n",
      "137/223, train_loss: 0.1040, step time: 0.1044\n",
      "138/223, train_loss: 0.1029, step time: 0.1154\n",
      "139/223, train_loss: 0.1074, step time: 0.1051\n",
      "140/223, train_loss: 0.1060, step time: 0.0997\n",
      "141/223, train_loss: 0.1121, step time: 0.1437\n",
      "142/223, train_loss: 0.1214, step time: 0.0997\n",
      "143/223, train_loss: 0.1057, step time: 0.0996\n",
      "144/223, train_loss: 0.0985, step time: 0.1105\n",
      "145/223, train_loss: 0.0969, step time: 0.1090\n",
      "146/223, train_loss: 0.1182, step time: 0.1058\n",
      "147/223, train_loss: 0.1154, step time: 0.1095\n",
      "148/223, train_loss: 0.1118, step time: 0.1112\n",
      "149/223, train_loss: 0.1136, step time: 0.1104\n",
      "150/223, train_loss: 0.1004, step time: 0.1042\n",
      "151/223, train_loss: 0.0976, step time: 0.1147\n",
      "152/223, train_loss: 0.1090, step time: 0.1157\n",
      "153/223, train_loss: 0.1206, step time: 0.1114\n",
      "154/223, train_loss: 0.1205, step time: 0.1028\n",
      "155/223, train_loss: 0.1025, step time: 0.1282\n",
      "156/223, train_loss: 0.1056, step time: 0.0998\n",
      "157/223, train_loss: 0.0963, step time: 0.1099\n",
      "158/223, train_loss: 0.1079, step time: 0.1250\n",
      "159/223, train_loss: 0.1074, step time: 0.1029\n",
      "160/223, train_loss: 0.1104, step time: 0.0994\n",
      "161/223, train_loss: 0.1059, step time: 0.1047\n",
      "162/223, train_loss: 0.0950, step time: 0.1144\n",
      "163/223, train_loss: 0.1093, step time: 0.1072\n",
      "164/223, train_loss: 0.0952, step time: 0.1003\n",
      "165/223, train_loss: 0.1060, step time: 0.1099\n",
      "166/223, train_loss: 0.0989, step time: 0.1079\n",
      "167/223, train_loss: 0.1034, step time: 0.1004\n",
      "168/223, train_loss: 0.1240, step time: 0.1092\n",
      "169/223, train_loss: 0.1033, step time: 0.0996\n",
      "170/223, train_loss: 0.1160, step time: 0.1004\n",
      "171/223, train_loss: 0.1001, step time: 0.0995\n",
      "172/223, train_loss: 0.1059, step time: 0.1018\n",
      "173/223, train_loss: 0.1067, step time: 0.1139\n",
      "174/223, train_loss: 0.1066, step time: 0.1118\n",
      "175/223, train_loss: 0.1252, step time: 0.1009\n",
      "176/223, train_loss: 0.1027, step time: 0.1010\n",
      "177/223, train_loss: 0.0957, step time: 0.1091\n",
      "178/223, train_loss: 0.0995, step time: 0.1039\n",
      "179/223, train_loss: 0.1092, step time: 0.1065\n",
      "180/223, train_loss: 0.0959, step time: 0.1123\n",
      "181/223, train_loss: 0.1156, step time: 0.1080\n",
      "182/223, train_loss: 0.1038, step time: 0.1125\n",
      "183/223, train_loss: 0.1088, step time: 0.1222\n",
      "184/223, train_loss: 0.0985, step time: 0.1217\n",
      "185/223, train_loss: 0.1075, step time: 0.0995\n",
      "186/223, train_loss: 0.1004, step time: 0.1126\n",
      "187/223, train_loss: 0.1150, step time: 0.1049\n",
      "188/223, train_loss: 0.1010, step time: 0.1089\n",
      "189/223, train_loss: 0.1050, step time: 0.1150\n",
      "190/223, train_loss: 0.1046, step time: 0.1047\n",
      "191/223, train_loss: 0.1066, step time: 0.1129\n",
      "192/223, train_loss: 0.0923, step time: 0.1118\n",
      "193/223, train_loss: 0.1123, step time: 0.1003\n",
      "194/223, train_loss: 0.1005, step time: 0.1058\n",
      "195/223, train_loss: 0.1124, step time: 0.0997\n",
      "196/223, train_loss: 0.1036, step time: 0.1002\n",
      "197/223, train_loss: 0.0988, step time: 0.1181\n",
      "198/223, train_loss: 0.0928, step time: 0.1183\n",
      "199/223, train_loss: 0.1030, step time: 0.1089\n",
      "200/223, train_loss: 0.1004, step time: 0.0999\n",
      "201/223, train_loss: 0.1073, step time: 0.1056\n",
      "202/223, train_loss: 0.1055, step time: 0.1068\n",
      "203/223, train_loss: 0.0966, step time: 0.1079\n",
      "204/223, train_loss: 0.1163, step time: 0.1008\n",
      "205/223, train_loss: 0.1000, step time: 0.1124\n",
      "206/223, train_loss: 0.1143, step time: 0.1071\n",
      "207/223, train_loss: 0.1218, step time: 0.1136\n",
      "208/223, train_loss: 0.1036, step time: 0.1183\n",
      "209/223, train_loss: 0.1099, step time: 0.1129\n",
      "210/223, train_loss: 0.0980, step time: 0.1140\n",
      "211/223, train_loss: 0.1057, step time: 0.1110\n",
      "212/223, train_loss: 0.1043, step time: 0.1136\n",
      "213/223, train_loss: 0.1023, step time: 0.1073\n",
      "214/223, train_loss: 0.1121, step time: 0.1116\n",
      "215/223, train_loss: 0.1089, step time: 0.1149\n",
      "216/223, train_loss: 0.1019, step time: 0.1002\n",
      "217/223, train_loss: 0.0995, step time: 0.1005\n",
      "218/223, train_loss: 0.1071, step time: 0.1003\n",
      "219/223, train_loss: 0.0970, step time: 0.0997\n",
      "220/223, train_loss: 0.1021, step time: 0.1005\n",
      "221/223, train_loss: 0.0994, step time: 0.0998\n",
      "222/223, train_loss: 0.1038, step time: 0.0989\n",
      "223/223, train_loss: 0.1051, step time: 0.0988\n",
      "epoch 145 average loss: 0.1079\n",
      "current epoch: 145 current mean dice: 0.8544 tc: 0.9201 wt: 0.8644 et: 0.7786\n",
      "best mean dice: 0.8551 at epoch: 125\n",
      "time consuming of epoch 145 is: 89.3635\n",
      "----------\n",
      "epoch 146/300\n",
      "1/223, train_loss: 0.1079, step time: 0.1056\n",
      "2/223, train_loss: 0.0939, step time: 0.1035\n",
      "3/223, train_loss: 0.0992, step time: 0.1112\n",
      "4/223, train_loss: 0.1104, step time: 0.1229\n",
      "5/223, train_loss: 0.1120, step time: 0.1044\n",
      "6/223, train_loss: 0.1124, step time: 0.1184\n",
      "7/223, train_loss: 0.1073, step time: 0.1271\n",
      "8/223, train_loss: 0.1044, step time: 0.0997\n",
      "9/223, train_loss: 0.1034, step time: 0.1020\n",
      "10/223, train_loss: 0.1186, step time: 0.1002\n",
      "11/223, train_loss: 0.1114, step time: 0.1133\n",
      "12/223, train_loss: 0.1109, step time: 0.1001\n",
      "13/223, train_loss: 0.1233, step time: 0.1110\n",
      "14/223, train_loss: 0.0979, step time: 0.1181\n",
      "15/223, train_loss: 0.1116, step time: 0.1085\n",
      "16/223, train_loss: 0.1089, step time: 0.1186\n",
      "17/223, train_loss: 0.1124, step time: 0.1070\n",
      "18/223, train_loss: 0.0980, step time: 0.1026\n",
      "19/223, train_loss: 0.0983, step time: 0.1128\n",
      "20/223, train_loss: 0.1144, step time: 0.1009\n",
      "21/223, train_loss: 0.1044, step time: 0.1045\n",
      "22/223, train_loss: 0.1006, step time: 0.1036\n",
      "23/223, train_loss: 0.1110, step time: 0.1010\n",
      "24/223, train_loss: 0.1223, step time: 0.1009\n",
      "25/223, train_loss: 0.1080, step time: 0.1190\n",
      "26/223, train_loss: 0.1019, step time: 0.1090\n",
      "27/223, train_loss: 0.1160, step time: 0.1034\n",
      "28/223, train_loss: 0.1095, step time: 0.1034\n",
      "29/223, train_loss: 0.1123, step time: 0.0993\n",
      "30/223, train_loss: 0.1094, step time: 0.1111\n",
      "31/223, train_loss: 0.1134, step time: 0.1005\n",
      "32/223, train_loss: 0.1179, step time: 0.1007\n",
      "33/223, train_loss: 0.1089, step time: 0.1167\n",
      "34/223, train_loss: 0.0942, step time: 0.1004\n",
      "35/223, train_loss: 0.1059, step time: 0.1041\n",
      "36/223, train_loss: 0.1143, step time: 0.1169\n",
      "37/223, train_loss: 0.1034, step time: 0.1005\n",
      "38/223, train_loss: 0.1139, step time: 0.1024\n",
      "39/223, train_loss: 0.0988, step time: 0.1094\n",
      "40/223, train_loss: 0.1129, step time: 0.1142\n",
      "41/223, train_loss: 0.1057, step time: 0.1162\n",
      "42/223, train_loss: 0.1067, step time: 0.1155\n",
      "43/223, train_loss: 0.1031, step time: 0.1169\n",
      "44/223, train_loss: 0.1202, step time: 0.1001\n",
      "45/223, train_loss: 0.1120, step time: 0.1229\n",
      "46/223, train_loss: 0.0960, step time: 0.1139\n",
      "47/223, train_loss: 0.1011, step time: 0.0995\n",
      "48/223, train_loss: 0.1112, step time: 0.1214\n",
      "49/223, train_loss: 0.1001, step time: 0.1067\n",
      "50/223, train_loss: 0.1042, step time: 0.1779\n",
      "51/223, train_loss: 0.0995, step time: 0.1069\n",
      "52/223, train_loss: 0.0970, step time: 0.1046\n",
      "53/223, train_loss: 0.1044, step time: 0.1108\n",
      "54/223, train_loss: 0.1132, step time: 0.1161\n",
      "55/223, train_loss: 0.1149, step time: 0.1023\n",
      "56/223, train_loss: 0.1077, step time: 0.1016\n",
      "57/223, train_loss: 0.0994, step time: 0.1254\n",
      "58/223, train_loss: 0.1055, step time: 0.1005\n",
      "59/223, train_loss: 0.1005, step time: 0.1009\n",
      "60/223, train_loss: 0.1040, step time: 0.1234\n",
      "61/223, train_loss: 0.1192, step time: 0.1094\n",
      "62/223, train_loss: 0.1030, step time: 0.1089\n",
      "63/223, train_loss: 0.1046, step time: 0.1198\n",
      "64/223, train_loss: 0.0939, step time: 0.1313\n",
      "65/223, train_loss: 0.1061, step time: 0.1128\n",
      "66/223, train_loss: 0.1012, step time: 0.1065\n",
      "67/223, train_loss: 0.1081, step time: 0.1013\n",
      "68/223, train_loss: 0.1053, step time: 0.1157\n",
      "69/223, train_loss: 0.1027, step time: 0.1274\n",
      "70/223, train_loss: 0.1019, step time: 0.1154\n",
      "71/223, train_loss: 0.1041, step time: 0.1005\n",
      "72/223, train_loss: 0.3107, step time: 0.1253\n",
      "73/223, train_loss: 0.1050, step time: 0.1097\n",
      "74/223, train_loss: 0.1009, step time: 0.1162\n",
      "75/223, train_loss: 0.0951, step time: 0.1210\n",
      "76/223, train_loss: 0.1110, step time: 0.1195\n",
      "77/223, train_loss: 0.0967, step time: 0.1029\n",
      "78/223, train_loss: 0.1126, step time: 0.1135\n",
      "79/223, train_loss: 0.1009, step time: 0.1066\n",
      "80/223, train_loss: 0.1019, step time: 0.1100\n",
      "81/223, train_loss: 0.1109, step time: 0.1225\n",
      "82/223, train_loss: 0.1048, step time: 0.1035\n",
      "83/223, train_loss: 0.1103, step time: 0.1164\n",
      "84/223, train_loss: 0.1124, step time: 0.1149\n",
      "85/223, train_loss: 0.1109, step time: 0.1112\n",
      "86/223, train_loss: 0.1021, step time: 0.1005\n",
      "87/223, train_loss: 0.1122, step time: 0.1095\n",
      "88/223, train_loss: 0.1069, step time: 0.1119\n",
      "89/223, train_loss: 0.1194, step time: 0.1003\n",
      "90/223, train_loss: 0.1162, step time: 0.1081\n",
      "91/223, train_loss: 0.0982, step time: 0.1005\n",
      "92/223, train_loss: 0.1042, step time: 0.1155\n",
      "93/223, train_loss: 0.1070, step time: 0.1142\n",
      "94/223, train_loss: 0.1023, step time: 0.1005\n",
      "95/223, train_loss: 0.1015, step time: 0.1002\n",
      "96/223, train_loss: 0.1173, step time: 0.1147\n",
      "97/223, train_loss: 0.0993, step time: 0.1026\n",
      "98/223, train_loss: 0.1198, step time: 0.1132\n",
      "99/223, train_loss: 0.0992, step time: 0.1107\n",
      "100/223, train_loss: 0.1227, step time: 0.0998\n",
      "101/223, train_loss: 0.1130, step time: 0.1037\n",
      "102/223, train_loss: 0.1122, step time: 0.1188\n",
      "103/223, train_loss: 0.1147, step time: 0.1261\n",
      "104/223, train_loss: 0.1194, step time: 0.1066\n",
      "105/223, train_loss: 0.1039, step time: 0.1599\n",
      "106/223, train_loss: 0.1006, step time: 0.1218\n",
      "107/223, train_loss: 0.1088, step time: 0.1170\n",
      "108/223, train_loss: 0.1301, step time: 0.1114\n",
      "109/223, train_loss: 0.0976, step time: 0.1085\n",
      "110/223, train_loss: 0.1059, step time: 0.1004\n",
      "111/223, train_loss: 0.1162, step time: 0.1136\n",
      "112/223, train_loss: 0.0996, step time: 0.1097\n",
      "113/223, train_loss: 0.1058, step time: 0.1003\n",
      "114/223, train_loss: 0.0966, step time: 0.1000\n",
      "115/223, train_loss: 0.1165, step time: 0.1010\n",
      "116/223, train_loss: 0.0998, step time: 0.1235\n",
      "117/223, train_loss: 0.1037, step time: 0.1061\n",
      "118/223, train_loss: 0.0994, step time: 0.1049\n",
      "119/223, train_loss: 0.1011, step time: 0.1009\n",
      "120/223, train_loss: 0.1074, step time: 0.1106\n",
      "121/223, train_loss: 0.1035, step time: 0.1107\n",
      "122/223, train_loss: 0.1186, step time: 0.1025\n",
      "123/223, train_loss: 0.1035, step time: 0.1037\n",
      "124/223, train_loss: 0.1018, step time: 0.1251\n",
      "125/223, train_loss: 0.1012, step time: 0.1019\n",
      "126/223, train_loss: 0.0923, step time: 0.1022\n",
      "127/223, train_loss: 0.1024, step time: 0.1061\n",
      "128/223, train_loss: 0.1108, step time: 0.1116\n",
      "129/223, train_loss: 0.1010, step time: 0.1283\n",
      "130/223, train_loss: 0.1218, step time: 0.1273\n",
      "131/223, train_loss: 0.1113, step time: 0.1181\n",
      "132/223, train_loss: 0.0997, step time: 0.1004\n",
      "133/223, train_loss: 0.1165, step time: 0.1005\n",
      "134/223, train_loss: 0.1127, step time: 0.1187\n",
      "135/223, train_loss: 0.1075, step time: 0.1012\n",
      "136/223, train_loss: 0.1161, step time: 0.1143\n",
      "137/223, train_loss: 0.1043, step time: 0.1001\n",
      "138/223, train_loss: 0.0909, step time: 0.1000\n",
      "139/223, train_loss: 0.0998, step time: 0.1011\n",
      "140/223, train_loss: 0.1013, step time: 0.1083\n",
      "141/223, train_loss: 0.1010, step time: 0.1065\n",
      "142/223, train_loss: 0.0952, step time: 0.1104\n",
      "143/223, train_loss: 0.1147, step time: 0.1049\n",
      "144/223, train_loss: 0.1101, step time: 0.1072\n",
      "145/223, train_loss: 0.0973, step time: 0.0997\n",
      "146/223, train_loss: 0.1102, step time: 0.1082\n",
      "147/223, train_loss: 0.1061, step time: 0.1161\n",
      "148/223, train_loss: 0.0997, step time: 0.1042\n",
      "149/223, train_loss: 0.1040, step time: 0.0999\n",
      "150/223, train_loss: 0.1032, step time: 0.1108\n",
      "151/223, train_loss: 0.1031, step time: 0.1399\n",
      "152/223, train_loss: 0.0968, step time: 0.1305\n",
      "153/223, train_loss: 0.1189, step time: 0.0998\n",
      "154/223, train_loss: 0.0980, step time: 0.1000\n",
      "155/223, train_loss: 0.1136, step time: 0.1041\n",
      "156/223, train_loss: 0.1009, step time: 0.1164\n",
      "157/223, train_loss: 0.1125, step time: 0.1063\n",
      "158/223, train_loss: 0.0989, step time: 0.1078\n",
      "159/223, train_loss: 0.1051, step time: 0.1008\n",
      "160/223, train_loss: 0.1065, step time: 0.1000\n",
      "161/223, train_loss: 0.1163, step time: 0.1008\n",
      "162/223, train_loss: 0.1027, step time: 0.1012\n",
      "163/223, train_loss: 0.1148, step time: 0.1149\n",
      "164/223, train_loss: 0.1182, step time: 0.1005\n",
      "165/223, train_loss: 0.1005, step time: 0.1239\n",
      "166/223, train_loss: 0.1157, step time: 0.1016\n",
      "167/223, train_loss: 0.0995, step time: 0.1008\n",
      "168/223, train_loss: 0.1222, step time: 0.1053\n",
      "169/223, train_loss: 0.0974, step time: 0.1078\n",
      "170/223, train_loss: 0.1101, step time: 0.1107\n",
      "171/223, train_loss: 0.1083, step time: 0.0997\n",
      "172/223, train_loss: 0.1111, step time: 0.1144\n",
      "173/223, train_loss: 0.0981, step time: 0.1112\n",
      "174/223, train_loss: 0.1037, step time: 0.1291\n",
      "175/223, train_loss: 0.1087, step time: 0.0996\n",
      "176/223, train_loss: 0.1118, step time: 0.1120\n",
      "177/223, train_loss: 0.1153, step time: 0.1606\n",
      "178/223, train_loss: 0.1020, step time: 0.1218\n",
      "179/223, train_loss: 0.1166, step time: 0.1146\n",
      "180/223, train_loss: 0.1111, step time: 0.1101\n",
      "181/223, train_loss: 0.1156, step time: 0.1117\n",
      "182/223, train_loss: 0.1113, step time: 0.1314\n",
      "183/223, train_loss: 0.1213, step time: 0.1024\n",
      "184/223, train_loss: 0.1109, step time: 0.1214\n",
      "185/223, train_loss: 0.1242, step time: 0.1711\n",
      "186/223, train_loss: 0.1186, step time: 0.1002\n",
      "187/223, train_loss: 0.1029, step time: 0.1002\n",
      "188/223, train_loss: 0.1012, step time: 0.0995\n",
      "189/223, train_loss: 0.0970, step time: 0.1239\n",
      "190/223, train_loss: 0.1144, step time: 0.1316\n",
      "191/223, train_loss: 0.1154, step time: 0.1008\n",
      "192/223, train_loss: 0.1087, step time: 0.1038\n",
      "193/223, train_loss: 0.0993, step time: 0.1013\n",
      "194/223, train_loss: 0.0973, step time: 0.1010\n",
      "195/223, train_loss: 0.0992, step time: 0.1050\n",
      "196/223, train_loss: 0.1182, step time: 0.1007\n",
      "197/223, train_loss: 0.0916, step time: 0.1151\n",
      "198/223, train_loss: 0.1035, step time: 0.1040\n",
      "199/223, train_loss: 0.1038, step time: 0.1109\n",
      "200/223, train_loss: 0.1130, step time: 0.1093\n",
      "201/223, train_loss: 0.1098, step time: 0.1089\n",
      "202/223, train_loss: 0.1020, step time: 0.1074\n",
      "203/223, train_loss: 0.1004, step time: 0.1517\n",
      "204/223, train_loss: 0.1046, step time: 0.1224\n",
      "205/223, train_loss: 0.1052, step time: 0.1087\n",
      "206/223, train_loss: 0.1167, step time: 0.1134\n",
      "207/223, train_loss: 0.0975, step time: 0.1234\n",
      "208/223, train_loss: 0.1105, step time: 0.1245\n",
      "209/223, train_loss: 0.1066, step time: 0.0994\n",
      "210/223, train_loss: 0.1061, step time: 0.1002\n",
      "211/223, train_loss: 0.0965, step time: 0.0998\n",
      "212/223, train_loss: 0.1016, step time: 0.1034\n",
      "213/223, train_loss: 0.1110, step time: 0.0996\n",
      "214/223, train_loss: 0.0985, step time: 0.0987\n",
      "215/223, train_loss: 0.1064, step time: 0.0994\n",
      "216/223, train_loss: 0.1052, step time: 0.1002\n",
      "217/223, train_loss: 0.1033, step time: 0.0999\n",
      "218/223, train_loss: 0.1051, step time: 0.1004\n",
      "219/223, train_loss: 0.1099, step time: 0.1059\n",
      "220/223, train_loss: 0.1110, step time: 0.1086\n",
      "221/223, train_loss: 0.1055, step time: 0.1000\n",
      "222/223, train_loss: 0.0977, step time: 0.0990\n",
      "223/223, train_loss: 0.0994, step time: 0.1003\n",
      "epoch 146 average loss: 0.1078\n",
      "time consuming of epoch 146 is: 90.7154\n",
      "----------\n",
      "epoch 147/300\n",
      "1/223, train_loss: 0.1036, step time: 0.1014\n",
      "2/223, train_loss: 0.1008, step time: 0.0998\n",
      "3/223, train_loss: 0.1185, step time: 0.1009\n",
      "4/223, train_loss: 0.1022, step time: 0.1252\n",
      "5/223, train_loss: 0.1018, step time: 0.1119\n",
      "6/223, train_loss: 0.1101, step time: 0.1004\n",
      "7/223, train_loss: 0.1107, step time: 0.1079\n",
      "8/223, train_loss: 0.1174, step time: 0.1017\n",
      "9/223, train_loss: 0.1052, step time: 0.1282\n",
      "10/223, train_loss: 0.1077, step time: 0.1037\n",
      "11/223, train_loss: 0.1066, step time: 0.1009\n",
      "12/223, train_loss: 0.1134, step time: 0.1001\n",
      "13/223, train_loss: 0.1029, step time: 0.1133\n",
      "14/223, train_loss: 0.0970, step time: 0.1172\n",
      "15/223, train_loss: 0.1132, step time: 0.1626\n",
      "16/223, train_loss: 0.0935, step time: 0.1000\n",
      "17/223, train_loss: 0.1091, step time: 0.1023\n",
      "18/223, train_loss: 0.0929, step time: 0.1208\n",
      "19/223, train_loss: 0.1094, step time: 0.1085\n",
      "20/223, train_loss: 0.1049, step time: 0.1056\n",
      "21/223, train_loss: 0.1133, step time: 0.1148\n",
      "22/223, train_loss: 0.0975, step time: 0.1167\n",
      "23/223, train_loss: 0.1036, step time: 0.1004\n",
      "24/223, train_loss: 0.1097, step time: 0.0993\n",
      "25/223, train_loss: 0.1114, step time: 0.1173\n",
      "26/223, train_loss: 0.1030, step time: 0.1131\n",
      "27/223, train_loss: 0.1054, step time: 0.1077\n",
      "28/223, train_loss: 0.1041, step time: 0.1104\n",
      "29/223, train_loss: 0.1043, step time: 0.1074\n",
      "30/223, train_loss: 0.1199, step time: 0.0999\n",
      "31/223, train_loss: 0.0922, step time: 0.1257\n",
      "32/223, train_loss: 0.0973, step time: 0.1095\n",
      "33/223, train_loss: 0.1150, step time: 0.1063\n",
      "34/223, train_loss: 0.1036, step time: 0.1219\n",
      "35/223, train_loss: 0.1192, step time: 0.1267\n",
      "36/223, train_loss: 0.1034, step time: 0.1088\n",
      "37/223, train_loss: 0.1145, step time: 0.1099\n",
      "38/223, train_loss: 0.1028, step time: 0.0993\n",
      "39/223, train_loss: 0.0962, step time: 0.1240\n",
      "40/223, train_loss: 0.0964, step time: 0.1005\n",
      "41/223, train_loss: 0.0941, step time: 0.1320\n",
      "42/223, train_loss: 0.1046, step time: 0.1207\n",
      "43/223, train_loss: 0.1095, step time: 0.1189\n",
      "44/223, train_loss: 0.1054, step time: 0.0995\n",
      "45/223, train_loss: 0.1037, step time: 0.1139\n",
      "46/223, train_loss: 0.0972, step time: 0.1216\n",
      "47/223, train_loss: 0.1024, step time: 0.1012\n",
      "48/223, train_loss: 0.1007, step time: 0.0997\n",
      "49/223, train_loss: 0.1097, step time: 0.1124\n",
      "50/223, train_loss: 0.1101, step time: 0.1065\n",
      "51/223, train_loss: 0.0985, step time: 0.1054\n",
      "52/223, train_loss: 0.0890, step time: 0.1123\n",
      "53/223, train_loss: 0.1171, step time: 0.1082\n",
      "54/223, train_loss: 0.0954, step time: 0.1099\n",
      "55/223, train_loss: 0.1131, step time: 0.1262\n",
      "56/223, train_loss: 0.1121, step time: 0.0995\n",
      "57/223, train_loss: 0.1061, step time: 0.1151\n",
      "58/223, train_loss: 0.1062, step time: 0.1016\n",
      "59/223, train_loss: 0.1006, step time: 0.1107\n",
      "60/223, train_loss: 0.1012, step time: 0.1049\n",
      "61/223, train_loss: 0.1069, step time: 0.1148\n",
      "62/223, train_loss: 0.1197, step time: 0.1007\n",
      "63/223, train_loss: 0.1012, step time: 0.1246\n",
      "64/223, train_loss: 0.1114, step time: 0.1295\n",
      "65/223, train_loss: 0.0981, step time: 0.1147\n",
      "66/223, train_loss: 0.1038, step time: 0.1145\n",
      "67/223, train_loss: 0.1086, step time: 0.1304\n",
      "68/223, train_loss: 0.1054, step time: 0.1032\n",
      "69/223, train_loss: 0.1000, step time: 0.1259\n",
      "70/223, train_loss: 0.1213, step time: 0.1211\n",
      "71/223, train_loss: 0.1033, step time: 0.1003\n",
      "72/223, train_loss: 0.1157, step time: 0.1001\n",
      "73/223, train_loss: 0.1181, step time: 0.1001\n",
      "74/223, train_loss: 0.1225, step time: 0.1339\n",
      "75/223, train_loss: 0.1104, step time: 0.1035\n",
      "76/223, train_loss: 0.1098, step time: 0.1199\n",
      "77/223, train_loss: 0.1148, step time: 0.1156\n",
      "78/223, train_loss: 0.0977, step time: 0.1048\n",
      "79/223, train_loss: 0.1035, step time: 0.1135\n",
      "80/223, train_loss: 0.1081, step time: 0.1009\n",
      "81/223, train_loss: 0.1144, step time: 0.1004\n",
      "82/223, train_loss: 0.1118, step time: 0.1096\n",
      "83/223, train_loss: 0.1124, step time: 0.0998\n",
      "84/223, train_loss: 0.1104, step time: 0.1002\n",
      "85/223, train_loss: 0.1035, step time: 0.1335\n",
      "86/223, train_loss: 0.1046, step time: 0.1381\n",
      "87/223, train_loss: 0.1081, step time: 0.1012\n",
      "88/223, train_loss: 0.1135, step time: 0.1052\n",
      "89/223, train_loss: 0.1017, step time: 0.1018\n",
      "90/223, train_loss: 0.1017, step time: 0.1007\n",
      "91/223, train_loss: 0.1212, step time: 0.1003\n",
      "92/223, train_loss: 0.0994, step time: 0.1000\n",
      "93/223, train_loss: 0.1056, step time: 0.1136\n",
      "94/223, train_loss: 0.1176, step time: 0.1103\n",
      "95/223, train_loss: 0.1162, step time: 0.1209\n",
      "96/223, train_loss: 0.1120, step time: 0.1014\n",
      "97/223, train_loss: 0.1088, step time: 0.1109\n",
      "98/223, train_loss: 0.1038, step time: 0.1088\n",
      "99/223, train_loss: 0.1020, step time: 0.1158\n",
      "100/223, train_loss: 0.0955, step time: 0.1003\n",
      "101/223, train_loss: 0.1129, step time: 0.1004\n",
      "102/223, train_loss: 0.1191, step time: 0.1001\n",
      "103/223, train_loss: 0.0947, step time: 0.1097\n",
      "104/223, train_loss: 0.1031, step time: 0.1213\n",
      "105/223, train_loss: 0.0978, step time: 0.1254\n",
      "106/223, train_loss: 0.1092, step time: 0.1095\n",
      "107/223, train_loss: 0.0990, step time: 0.1443\n",
      "108/223, train_loss: 0.1090, step time: 0.1274\n",
      "109/223, train_loss: 0.1035, step time: 0.1104\n",
      "110/223, train_loss: 0.1139, step time: 0.1064\n",
      "111/223, train_loss: 0.1131, step time: 0.1256\n",
      "112/223, train_loss: 0.1105, step time: 0.1058\n",
      "113/223, train_loss: 0.0978, step time: 0.1061\n",
      "114/223, train_loss: 0.1232, step time: 0.1349\n",
      "115/223, train_loss: 0.1000, step time: 0.1358\n",
      "116/223, train_loss: 0.0969, step time: 0.1098\n",
      "117/223, train_loss: 0.1070, step time: 0.1067\n",
      "118/223, train_loss: 0.1122, step time: 0.1140\n",
      "119/223, train_loss: 0.0983, step time: 0.1016\n",
      "120/223, train_loss: 0.1053, step time: 0.1001\n",
      "121/223, train_loss: 0.1147, step time: 0.1193\n",
      "122/223, train_loss: 0.0942, step time: 0.1106\n",
      "123/223, train_loss: 0.1067, step time: 0.1268\n",
      "124/223, train_loss: 0.1079, step time: 0.1001\n",
      "125/223, train_loss: 0.1118, step time: 0.1282\n",
      "126/223, train_loss: 0.1054, step time: 0.1260\n",
      "127/223, train_loss: 0.1119, step time: 0.1104\n",
      "128/223, train_loss: 0.1104, step time: 0.1166\n",
      "129/223, train_loss: 0.0974, step time: 0.1049\n",
      "130/223, train_loss: 0.0974, step time: 0.1418\n",
      "131/223, train_loss: 0.1182, step time: 0.1061\n",
      "132/223, train_loss: 0.1091, step time: 0.1000\n",
      "133/223, train_loss: 0.1038, step time: 0.1007\n",
      "134/223, train_loss: 0.1102, step time: 0.1150\n",
      "135/223, train_loss: 0.1084, step time: 0.1011\n",
      "136/223, train_loss: 0.0973, step time: 0.1011\n",
      "137/223, train_loss: 0.1115, step time: 0.1168\n",
      "138/223, train_loss: 0.0939, step time: 0.1161\n",
      "139/223, train_loss: 0.1008, step time: 0.1056\n",
      "140/223, train_loss: 0.1262, step time: 0.1042\n",
      "141/223, train_loss: 0.1082, step time: 0.0999\n",
      "142/223, train_loss: 0.1017, step time: 0.0999\n",
      "143/223, train_loss: 0.1009, step time: 0.1120\n",
      "144/223, train_loss: 0.1037, step time: 0.1015\n",
      "145/223, train_loss: 0.1043, step time: 0.1034\n",
      "146/223, train_loss: 0.1112, step time: 0.1021\n",
      "147/223, train_loss: 0.1058, step time: 0.1039\n",
      "148/223, train_loss: 0.1062, step time: 0.1005\n",
      "149/223, train_loss: 0.1016, step time: 0.1190\n",
      "150/223, train_loss: 0.1169, step time: 0.0999\n",
      "151/223, train_loss: 0.1107, step time: 0.1075\n",
      "152/223, train_loss: 0.1016, step time: 0.1068\n",
      "153/223, train_loss: 0.0969, step time: 0.1073\n",
      "154/223, train_loss: 0.1069, step time: 0.1081\n",
      "155/223, train_loss: 0.1113, step time: 0.1022\n",
      "156/223, train_loss: 0.1024, step time: 0.1003\n",
      "157/223, train_loss: 0.1126, step time: 0.1058\n",
      "158/223, train_loss: 0.1109, step time: 0.1066\n",
      "159/223, train_loss: 0.1049, step time: 0.1169\n",
      "160/223, train_loss: 0.1061, step time: 0.1010\n",
      "161/223, train_loss: 0.0952, step time: 0.1092\n",
      "162/223, train_loss: 0.1146, step time: 0.1082\n",
      "163/223, train_loss: 0.0981, step time: 0.1007\n",
      "164/223, train_loss: 0.1099, step time: 0.1048\n",
      "165/223, train_loss: 0.0995, step time: 0.1004\n",
      "166/223, train_loss: 0.1048, step time: 0.0993\n",
      "167/223, train_loss: 0.1031, step time: 0.1005\n",
      "168/223, train_loss: 0.1150, step time: 0.1003\n",
      "169/223, train_loss: 0.1042, step time: 0.1119\n",
      "170/223, train_loss: 0.1242, step time: 0.1005\n",
      "171/223, train_loss: 0.1068, step time: 0.0997\n",
      "172/223, train_loss: 0.1050, step time: 0.1201\n",
      "173/223, train_loss: 0.1084, step time: 0.1186\n",
      "174/223, train_loss: 0.1153, step time: 0.1311\n",
      "175/223, train_loss: 0.0985, step time: 0.1091\n",
      "176/223, train_loss: 0.1020, step time: 0.1004\n",
      "177/223, train_loss: 0.1046, step time: 0.1122\n",
      "178/223, train_loss: 0.1005, step time: 0.1151\n",
      "179/223, train_loss: 0.1050, step time: 0.1051\n",
      "180/223, train_loss: 0.0989, step time: 0.1129\n",
      "181/223, train_loss: 0.1117, step time: 0.1015\n",
      "182/223, train_loss: 0.1148, step time: 0.1312\n",
      "183/223, train_loss: 0.1157, step time: 0.1140\n",
      "184/223, train_loss: 0.1042, step time: 0.1081\n",
      "185/223, train_loss: 0.1040, step time: 0.1038\n",
      "186/223, train_loss: 0.1107, step time: 0.1487\n",
      "187/223, train_loss: 0.1076, step time: 0.1229\n",
      "188/223, train_loss: 0.1042, step time: 0.1118\n",
      "189/223, train_loss: 0.0949, step time: 0.1114\n",
      "190/223, train_loss: 0.0974, step time: 0.1164\n",
      "191/223, train_loss: 0.1121, step time: 0.1228\n",
      "192/223, train_loss: 0.0998, step time: 0.1209\n",
      "193/223, train_loss: 0.1092, step time: 0.1264\n",
      "194/223, train_loss: 0.0913, step time: 0.1226\n",
      "195/223, train_loss: 0.0945, step time: 0.1142\n",
      "196/223, train_loss: 0.1221, step time: 0.1005\n",
      "197/223, train_loss: 0.1123, step time: 0.1154\n",
      "198/223, train_loss: 0.0968, step time: 0.1118\n",
      "199/223, train_loss: 0.1105, step time: 0.1137\n",
      "200/223, train_loss: 0.0985, step time: 0.1002\n",
      "201/223, train_loss: 0.1050, step time: 0.1012\n",
      "202/223, train_loss: 0.1179, step time: 0.1056\n",
      "203/223, train_loss: 0.1005, step time: 0.1192\n",
      "204/223, train_loss: 0.1060, step time: 0.1115\n",
      "205/223, train_loss: 0.3047, step time: 0.1532\n",
      "206/223, train_loss: 0.1132, step time: 0.1074\n",
      "207/223, train_loss: 0.1120, step time: 0.1353\n",
      "208/223, train_loss: 0.1163, step time: 0.1400\n",
      "209/223, train_loss: 0.1047, step time: 0.1111\n",
      "210/223, train_loss: 0.0958, step time: 0.1000\n",
      "211/223, train_loss: 0.1114, step time: 0.1006\n",
      "212/223, train_loss: 0.1046, step time: 0.0998\n",
      "213/223, train_loss: 0.1086, step time: 0.1094\n",
      "214/223, train_loss: 0.1116, step time: 0.1000\n",
      "215/223, train_loss: 0.1030, step time: 0.1283\n",
      "216/223, train_loss: 0.1162, step time: 0.1142\n",
      "217/223, train_loss: 0.1047, step time: 0.1009\n",
      "218/223, train_loss: 0.1156, step time: 0.1474\n",
      "219/223, train_loss: 0.1047, step time: 0.1258\n",
      "220/223, train_loss: 0.1030, step time: 0.1006\n",
      "221/223, train_loss: 0.1153, step time: 0.0998\n",
      "222/223, train_loss: 0.1138, step time: 0.0992\n",
      "223/223, train_loss: 0.1139, step time: 0.0987\n",
      "epoch 147 average loss: 0.1075\n",
      "time consuming of epoch 147 is: 87.9986\n",
      "----------\n",
      "epoch 148/300\n",
      "1/223, train_loss: 0.1044, step time: 0.1015\n",
      "2/223, train_loss: 0.1096, step time: 0.1006\n",
      "3/223, train_loss: 0.1055, step time: 0.1010\n",
      "4/223, train_loss: 0.0963, step time: 0.1031\n",
      "5/223, train_loss: 0.1052, step time: 0.1070\n",
      "6/223, train_loss: 0.1065, step time: 0.1194\n",
      "7/223, train_loss: 0.1098, step time: 0.1306\n",
      "8/223, train_loss: 0.1147, step time: 0.1074\n",
      "9/223, train_loss: 0.0932, step time: 0.1148\n",
      "10/223, train_loss: 0.1073, step time: 0.1411\n",
      "11/223, train_loss: 0.1012, step time: 0.1554\n",
      "12/223, train_loss: 0.1148, step time: 0.1262\n",
      "13/223, train_loss: 0.1051, step time: 0.1138\n",
      "14/223, train_loss: 0.1171, step time: 0.1138\n",
      "15/223, train_loss: 0.1110, step time: 0.1264\n",
      "16/223, train_loss: 0.1087, step time: 0.1052\n",
      "17/223, train_loss: 0.1030, step time: 0.1082\n",
      "18/223, train_loss: 0.1100, step time: 0.0995\n",
      "19/223, train_loss: 0.1141, step time: 0.1133\n",
      "20/223, train_loss: 0.1132, step time: 0.1013\n",
      "21/223, train_loss: 0.1001, step time: 0.1034\n",
      "22/223, train_loss: 0.0995, step time: 0.1097\n",
      "23/223, train_loss: 0.1055, step time: 0.0997\n",
      "24/223, train_loss: 0.1086, step time: 0.1009\n",
      "25/223, train_loss: 0.1077, step time: 0.1465\n",
      "26/223, train_loss: 0.1086, step time: 0.1000\n",
      "27/223, train_loss: 0.0978, step time: 0.1004\n",
      "28/223, train_loss: 0.1050, step time: 0.1009\n",
      "29/223, train_loss: 0.1031, step time: 0.1061\n",
      "30/223, train_loss: 0.1030, step time: 0.1158\n",
      "31/223, train_loss: 0.1036, step time: 0.1146\n",
      "32/223, train_loss: 0.1095, step time: 0.1057\n",
      "33/223, train_loss: 0.1015, step time: 0.1060\n",
      "34/223, train_loss: 0.0950, step time: 0.0998\n",
      "35/223, train_loss: 0.1183, step time: 0.1003\n",
      "36/223, train_loss: 0.1049, step time: 0.1262\n",
      "37/223, train_loss: 0.1134, step time: 0.1051\n",
      "38/223, train_loss: 0.0995, step time: 0.1130\n",
      "39/223, train_loss: 0.1266, step time: 0.1056\n",
      "40/223, train_loss: 0.1156, step time: 0.1269\n",
      "41/223, train_loss: 0.1033, step time: 0.1189\n",
      "42/223, train_loss: 0.1019, step time: 0.1078\n",
      "43/223, train_loss: 0.0901, step time: 0.1078\n",
      "44/223, train_loss: 0.0973, step time: 0.1004\n",
      "45/223, train_loss: 0.1036, step time: 0.1451\n",
      "46/223, train_loss: 0.0932, step time: 0.1281\n",
      "47/223, train_loss: 0.1001, step time: 0.1339\n",
      "48/223, train_loss: 0.1205, step time: 0.1004\n",
      "49/223, train_loss: 0.1032, step time: 0.1468\n",
      "50/223, train_loss: 0.1111, step time: 0.1057\n",
      "51/223, train_loss: 0.1008, step time: 0.1153\n",
      "52/223, train_loss: 0.0976, step time: 0.1068\n",
      "53/223, train_loss: 0.1091, step time: 0.1121\n",
      "54/223, train_loss: 0.1065, step time: 0.1012\n",
      "55/223, train_loss: 0.1104, step time: 0.1006\n",
      "56/223, train_loss: 0.1209, step time: 0.0996\n",
      "57/223, train_loss: 0.1029, step time: 0.1225\n",
      "58/223, train_loss: 0.1017, step time: 0.1597\n",
      "59/223, train_loss: 0.1104, step time: 0.1601\n",
      "60/223, train_loss: 0.0996, step time: 0.1056\n",
      "61/223, train_loss: 0.3016, step time: 0.1108\n",
      "62/223, train_loss: 0.1012, step time: 0.1124\n",
      "63/223, train_loss: 0.1070, step time: 0.1349\n",
      "64/223, train_loss: 0.0994, step time: 0.1008\n",
      "65/223, train_loss: 0.1136, step time: 0.1551\n",
      "66/223, train_loss: 0.1193, step time: 0.1100\n",
      "67/223, train_loss: 0.1053, step time: 0.1024\n",
      "68/223, train_loss: 0.1032, step time: 0.1010\n",
      "69/223, train_loss: 0.1095, step time: 0.1341\n",
      "70/223, train_loss: 0.1150, step time: 0.1011\n",
      "71/223, train_loss: 0.1143, step time: 0.1056\n",
      "72/223, train_loss: 0.1058, step time: 0.1010\n",
      "73/223, train_loss: 0.1101, step time: 0.1008\n",
      "74/223, train_loss: 0.1099, step time: 0.1107\n",
      "75/223, train_loss: 0.0994, step time: 0.1318\n",
      "76/223, train_loss: 0.1194, step time: 0.1001\n",
      "77/223, train_loss: 0.1119, step time: 0.1058\n",
      "78/223, train_loss: 0.1182, step time: 0.1012\n",
      "79/223, train_loss: 0.1090, step time: 0.1071\n",
      "80/223, train_loss: 0.1042, step time: 0.1096\n",
      "81/223, train_loss: 0.0975, step time: 0.1010\n",
      "82/223, train_loss: 0.0983, step time: 0.1076\n",
      "83/223, train_loss: 0.1139, step time: 0.1294\n",
      "84/223, train_loss: 0.0989, step time: 0.1015\n",
      "85/223, train_loss: 0.1092, step time: 0.1006\n",
      "86/223, train_loss: 0.1134, step time: 0.0999\n",
      "87/223, train_loss: 0.1175, step time: 0.1207\n",
      "88/223, train_loss: 0.1171, step time: 0.1089\n",
      "89/223, train_loss: 0.1067, step time: 0.1256\n",
      "90/223, train_loss: 0.0978, step time: 0.1236\n",
      "91/223, train_loss: 0.1123, step time: 0.1066\n",
      "92/223, train_loss: 0.1044, step time: 0.1007\n",
      "93/223, train_loss: 0.1025, step time: 0.0998\n",
      "94/223, train_loss: 0.1137, step time: 0.1284\n",
      "95/223, train_loss: 0.1136, step time: 0.1004\n",
      "96/223, train_loss: 0.1097, step time: 0.1011\n",
      "97/223, train_loss: 0.1039, step time: 0.1211\n",
      "98/223, train_loss: 0.1073, step time: 0.1124\n",
      "99/223, train_loss: 0.1028, step time: 0.1082\n",
      "100/223, train_loss: 0.1100, step time: 0.0994\n",
      "101/223, train_loss: 0.0941, step time: 0.1114\n",
      "102/223, train_loss: 0.1069, step time: 0.1002\n",
      "103/223, train_loss: 0.1115, step time: 0.1130\n",
      "104/223, train_loss: 0.1290, step time: 0.1128\n",
      "105/223, train_loss: 0.1084, step time: 0.1247\n",
      "106/223, train_loss: 0.1202, step time: 0.1220\n",
      "107/223, train_loss: 0.1230, step time: 0.1253\n",
      "108/223, train_loss: 0.1203, step time: 0.1067\n",
      "109/223, train_loss: 0.1039, step time: 0.1252\n",
      "110/223, train_loss: 0.1033, step time: 0.1118\n",
      "111/223, train_loss: 0.1090, step time: 0.1397\n",
      "112/223, train_loss: 0.1091, step time: 0.1219\n",
      "113/223, train_loss: 0.1020, step time: 0.1164\n",
      "114/223, train_loss: 0.1202, step time: 0.1194\n",
      "115/223, train_loss: 0.1134, step time: 0.1047\n",
      "116/223, train_loss: 0.1165, step time: 0.1111\n",
      "117/223, train_loss: 0.0991, step time: 0.1015\n",
      "118/223, train_loss: 0.1133, step time: 0.1077\n",
      "119/223, train_loss: 0.1091, step time: 0.1300\n",
      "120/223, train_loss: 0.1096, step time: 0.1316\n",
      "121/223, train_loss: 0.1163, step time: 0.1045\n",
      "122/223, train_loss: 0.1134, step time: 0.1055\n",
      "123/223, train_loss: 0.1071, step time: 0.1124\n",
      "124/223, train_loss: 0.1039, step time: 0.1398\n",
      "125/223, train_loss: 0.1017, step time: 0.1176\n",
      "126/223, train_loss: 0.1091, step time: 0.1238\n",
      "127/223, train_loss: 0.1064, step time: 0.1206\n",
      "128/223, train_loss: 0.1032, step time: 0.1288\n",
      "129/223, train_loss: 0.0988, step time: 0.1137\n",
      "130/223, train_loss: 0.1160, step time: 0.1128\n",
      "131/223, train_loss: 0.1137, step time: 0.1366\n",
      "132/223, train_loss: 0.1006, step time: 0.1317\n",
      "133/223, train_loss: 0.1035, step time: 0.1315\n",
      "134/223, train_loss: 0.1091, step time: 0.1475\n",
      "135/223, train_loss: 0.1087, step time: 0.1327\n",
      "136/223, train_loss: 0.1117, step time: 0.1217\n",
      "137/223, train_loss: 0.0965, step time: 0.1132\n",
      "138/223, train_loss: 0.1120, step time: 0.1043\n",
      "139/223, train_loss: 0.1169, step time: 0.1055\n",
      "140/223, train_loss: 0.1022, step time: 0.1070\n",
      "141/223, train_loss: 0.1134, step time: 0.1077\n",
      "142/223, train_loss: 0.0952, step time: 0.1286\n",
      "143/223, train_loss: 0.0964, step time: 0.1131\n",
      "144/223, train_loss: 0.1083, step time: 0.1004\n",
      "145/223, train_loss: 0.0974, step time: 0.1308\n",
      "146/223, train_loss: 0.1111, step time: 0.1001\n",
      "147/223, train_loss: 0.1088, step time: 0.1005\n",
      "148/223, train_loss: 0.1033, step time: 0.1119\n",
      "149/223, train_loss: 0.1092, step time: 0.1011\n",
      "150/223, train_loss: 0.0987, step time: 0.1121\n",
      "151/223, train_loss: 0.1099, step time: 0.1005\n",
      "152/223, train_loss: 0.0976, step time: 0.1007\n",
      "153/223, train_loss: 0.1055, step time: 0.1042\n",
      "154/223, train_loss: 0.0969, step time: 0.1103\n",
      "155/223, train_loss: 0.1125, step time: 0.1253\n",
      "156/223, train_loss: 0.1077, step time: 0.1091\n",
      "157/223, train_loss: 0.1132, step time: 0.1187\n",
      "158/223, train_loss: 0.1130, step time: 0.1003\n",
      "159/223, train_loss: 0.1006, step time: 0.1001\n",
      "160/223, train_loss: 0.0971, step time: 0.1008\n",
      "161/223, train_loss: 0.1098, step time: 0.1118\n",
      "162/223, train_loss: 0.1181, step time: 0.1316\n",
      "163/223, train_loss: 0.0939, step time: 0.1005\n",
      "164/223, train_loss: 0.1003, step time: 0.1071\n",
      "165/223, train_loss: 0.1059, step time: 0.1207\n",
      "166/223, train_loss: 0.1182, step time: 0.1037\n",
      "167/223, train_loss: 0.0932, step time: 0.1003\n",
      "168/223, train_loss: 0.1186, step time: 0.1191\n",
      "169/223, train_loss: 0.1024, step time: 0.1464\n",
      "170/223, train_loss: 0.1006, step time: 0.1127\n",
      "171/223, train_loss: 0.1026, step time: 0.0999\n",
      "172/223, train_loss: 0.1146, step time: 0.0998\n",
      "173/223, train_loss: 0.1056, step time: 0.1351\n",
      "174/223, train_loss: 0.1072, step time: 0.1374\n",
      "175/223, train_loss: 0.1027, step time: 0.1005\n",
      "176/223, train_loss: 0.0956, step time: 0.1000\n",
      "177/223, train_loss: 0.1059, step time: 0.1213\n",
      "178/223, train_loss: 0.1050, step time: 0.1220\n",
      "179/223, train_loss: 0.1034, step time: 0.1094\n",
      "180/223, train_loss: 0.1101, step time: 0.1008\n",
      "181/223, train_loss: 0.1037, step time: 0.1010\n",
      "182/223, train_loss: 0.1022, step time: 0.1507\n",
      "183/223, train_loss: 0.1088, step time: 0.1172\n",
      "184/223, train_loss: 0.1116, step time: 0.1013\n",
      "185/223, train_loss: 0.1105, step time: 0.1071\n",
      "186/223, train_loss: 0.1172, step time: 0.1081\n",
      "187/223, train_loss: 0.1044, step time: 0.1057\n",
      "188/223, train_loss: 0.1000, step time: 0.1183\n",
      "189/223, train_loss: 0.1011, step time: 0.1144\n",
      "190/223, train_loss: 0.1067, step time: 0.1005\n",
      "191/223, train_loss: 0.1063, step time: 0.1013\n",
      "192/223, train_loss: 0.1114, step time: 0.1098\n",
      "193/223, train_loss: 0.1039, step time: 0.1006\n",
      "194/223, train_loss: 0.0958, step time: 0.0998\n",
      "195/223, train_loss: 0.1114, step time: 0.1073\n",
      "196/223, train_loss: 0.1197, step time: 0.1070\n",
      "197/223, train_loss: 0.1061, step time: 0.0997\n",
      "198/223, train_loss: 0.1085, step time: 0.1005\n",
      "199/223, train_loss: 0.1055, step time: 0.1005\n",
      "200/223, train_loss: 0.1053, step time: 0.1007\n",
      "201/223, train_loss: 0.1179, step time: 0.1146\n",
      "202/223, train_loss: 0.1143, step time: 0.1073\n",
      "203/223, train_loss: 0.1021, step time: 0.1070\n",
      "204/223, train_loss: 0.1139, step time: 0.1141\n",
      "205/223, train_loss: 0.1129, step time: 0.1169\n",
      "206/223, train_loss: 0.1044, step time: 0.1081\n",
      "207/223, train_loss: 0.1136, step time: 0.1044\n",
      "208/223, train_loss: 0.0943, step time: 0.1008\n",
      "209/223, train_loss: 0.1111, step time: 0.1154\n",
      "210/223, train_loss: 0.1057, step time: 0.1004\n",
      "211/223, train_loss: 0.1188, step time: 0.1005\n",
      "212/223, train_loss: 0.1011, step time: 0.1016\n",
      "213/223, train_loss: 0.1001, step time: 0.1124\n",
      "214/223, train_loss: 0.0954, step time: 0.1108\n",
      "215/223, train_loss: 0.0906, step time: 0.1061\n",
      "216/223, train_loss: 0.1124, step time: 0.1002\n",
      "217/223, train_loss: 0.1060, step time: 0.1001\n",
      "218/223, train_loss: 0.0990, step time: 0.1026\n",
      "219/223, train_loss: 0.1012, step time: 0.1009\n",
      "220/223, train_loss: 0.1046, step time: 0.1080\n",
      "221/223, train_loss: 0.0991, step time: 0.0991\n",
      "222/223, train_loss: 0.0978, step time: 0.1002\n",
      "223/223, train_loss: 0.1068, step time: 0.0999\n",
      "epoch 148 average loss: 0.1078\n",
      "time consuming of epoch 148 is: 89.2933\n",
      "----------\n",
      "epoch 149/300\n",
      "1/223, train_loss: 0.1092, step time: 0.1010\n",
      "2/223, train_loss: 0.1005, step time: 0.0998\n",
      "3/223, train_loss: 0.0997, step time: 0.1304\n",
      "4/223, train_loss: 0.1171, step time: 0.1194\n",
      "5/223, train_loss: 0.1059, step time: 0.1307\n",
      "6/223, train_loss: 0.1159, step time: 0.1001\n",
      "7/223, train_loss: 0.1061, step time: 0.1223\n",
      "8/223, train_loss: 0.1085, step time: 0.1104\n",
      "9/223, train_loss: 0.1006, step time: 0.1052\n",
      "10/223, train_loss: 0.1141, step time: 0.1004\n",
      "11/223, train_loss: 0.1107, step time: 0.1060\n",
      "12/223, train_loss: 0.1121, step time: 0.1003\n",
      "13/223, train_loss: 0.0935, step time: 0.1146\n",
      "14/223, train_loss: 0.1065, step time: 0.1056\n",
      "15/223, train_loss: 0.1171, step time: 0.1113\n",
      "16/223, train_loss: 0.1192, step time: 0.1032\n",
      "17/223, train_loss: 0.1023, step time: 0.1011\n",
      "18/223, train_loss: 0.1070, step time: 0.1045\n",
      "19/223, train_loss: 0.0955, step time: 0.1131\n",
      "20/223, train_loss: 0.1075, step time: 0.1004\n",
      "21/223, train_loss: 0.1009, step time: 0.1054\n",
      "22/223, train_loss: 0.1211, step time: 0.1110\n",
      "23/223, train_loss: 0.1016, step time: 0.1070\n",
      "24/223, train_loss: 0.0966, step time: 0.1116\n",
      "25/223, train_loss: 0.1096, step time: 0.1070\n",
      "26/223, train_loss: 0.1026, step time: 0.1334\n",
      "27/223, train_loss: 0.1096, step time: 0.1121\n",
      "28/223, train_loss: 0.1184, step time: 0.1129\n",
      "29/223, train_loss: 0.1057, step time: 0.1006\n",
      "30/223, train_loss: 0.1244, step time: 0.1059\n",
      "31/223, train_loss: 0.1184, step time: 0.1465\n",
      "32/223, train_loss: 0.1076, step time: 0.1129\n",
      "33/223, train_loss: 0.1072, step time: 0.0995\n",
      "34/223, train_loss: 0.1211, step time: 0.1000\n",
      "35/223, train_loss: 0.1031, step time: 0.1098\n",
      "36/223, train_loss: 0.1166, step time: 0.1061\n",
      "37/223, train_loss: 0.1089, step time: 0.1326\n",
      "38/223, train_loss: 0.1004, step time: 0.1052\n",
      "39/223, train_loss: 0.1009, step time: 0.1042\n",
      "40/223, train_loss: 0.1087, step time: 0.1007\n",
      "41/223, train_loss: 0.0969, step time: 0.1030\n",
      "42/223, train_loss: 0.1034, step time: 0.1073\n",
      "43/223, train_loss: 0.1109, step time: 0.1077\n",
      "44/223, train_loss: 0.1165, step time: 0.1040\n",
      "45/223, train_loss: 0.1084, step time: 0.1016\n",
      "46/223, train_loss: 0.1088, step time: 0.1078\n",
      "47/223, train_loss: 0.1117, step time: 0.1032\n",
      "48/223, train_loss: 0.1101, step time: 0.1023\n",
      "49/223, train_loss: 0.1043, step time: 0.1009\n",
      "50/223, train_loss: 0.1169, step time: 0.1014\n",
      "51/223, train_loss: 0.0992, step time: 0.1000\n",
      "52/223, train_loss: 0.0936, step time: 0.1068\n",
      "53/223, train_loss: 0.0976, step time: 0.1005\n",
      "54/223, train_loss: 0.1065, step time: 0.1002\n",
      "55/223, train_loss: 0.1074, step time: 0.0999\n",
      "56/223, train_loss: 0.1015, step time: 0.1011\n",
      "57/223, train_loss: 0.0982, step time: 0.1041\n",
      "58/223, train_loss: 0.0984, step time: 0.1143\n",
      "59/223, train_loss: 0.1050, step time: 0.1123\n",
      "60/223, train_loss: 0.1016, step time: 0.1669\n",
      "61/223, train_loss: 0.1162, step time: 0.0998\n",
      "62/223, train_loss: 0.1156, step time: 0.1007\n",
      "63/223, train_loss: 0.0903, step time: 0.1005\n",
      "64/223, train_loss: 0.1014, step time: 0.1157\n",
      "65/223, train_loss: 0.1113, step time: 0.1246\n",
      "66/223, train_loss: 0.0976, step time: 0.1164\n",
      "67/223, train_loss: 0.1055, step time: 0.1156\n",
      "68/223, train_loss: 0.1125, step time: 0.1038\n",
      "69/223, train_loss: 0.1054, step time: 0.0999\n",
      "70/223, train_loss: 0.1007, step time: 0.1011\n",
      "71/223, train_loss: 0.0983, step time: 0.1135\n",
      "72/223, train_loss: 0.1013, step time: 0.1194\n",
      "73/223, train_loss: 0.1072, step time: 0.1205\n",
      "74/223, train_loss: 0.1004, step time: 0.0993\n",
      "75/223, train_loss: 0.1095, step time: 0.1006\n",
      "76/223, train_loss: 0.1242, step time: 0.1075\n",
      "77/223, train_loss: 0.1020, step time: 0.1109\n",
      "78/223, train_loss: 0.1112, step time: 0.1025\n",
      "79/223, train_loss: 0.1059, step time: 0.1002\n",
      "80/223, train_loss: 0.1085, step time: 0.1109\n",
      "81/223, train_loss: 0.1056, step time: 0.1122\n",
      "82/223, train_loss: 0.1196, step time: 0.1043\n",
      "83/223, train_loss: 0.1010, step time: 0.1281\n",
      "84/223, train_loss: 0.1087, step time: 0.1142\n",
      "85/223, train_loss: 0.1005, step time: 0.1001\n",
      "86/223, train_loss: 0.0968, step time: 0.1052\n",
      "87/223, train_loss: 0.1001, step time: 0.1002\n",
      "88/223, train_loss: 0.1053, step time: 0.1023\n",
      "89/223, train_loss: 0.1141, step time: 0.1213\n",
      "90/223, train_loss: 0.1173, step time: 0.1084\n",
      "91/223, train_loss: 0.1057, step time: 0.1394\n",
      "92/223, train_loss: 0.1004, step time: 0.1086\n",
      "93/223, train_loss: 0.1076, step time: 0.1025\n",
      "94/223, train_loss: 0.1053, step time: 0.1063\n",
      "95/223, train_loss: 0.1256, step time: 0.1054\n",
      "96/223, train_loss: 0.1034, step time: 0.1354\n",
      "97/223, train_loss: 0.0999, step time: 0.1197\n",
      "98/223, train_loss: 0.1202, step time: 0.1061\n",
      "99/223, train_loss: 0.1184, step time: 0.1140\n",
      "100/223, train_loss: 0.1136, step time: 0.1036\n",
      "101/223, train_loss: 0.1090, step time: 0.1007\n",
      "102/223, train_loss: 0.1023, step time: 0.1034\n",
      "103/223, train_loss: 0.0940, step time: 0.1057\n",
      "104/223, train_loss: 0.1043, step time: 0.0999\n",
      "105/223, train_loss: 0.0989, step time: 0.1037\n",
      "106/223, train_loss: 0.0997, step time: 0.1117\n",
      "107/223, train_loss: 0.0977, step time: 0.1156\n",
      "108/223, train_loss: 0.1084, step time: 0.1129\n",
      "109/223, train_loss: 0.1044, step time: 0.1029\n",
      "110/223, train_loss: 0.1115, step time: 0.1011\n",
      "111/223, train_loss: 0.1044, step time: 0.1008\n",
      "112/223, train_loss: 0.1064, step time: 0.1138\n",
      "113/223, train_loss: 0.1044, step time: 0.1218\n",
      "114/223, train_loss: 0.1007, step time: 0.1154\n",
      "115/223, train_loss: 0.1008, step time: 0.1087\n",
      "116/223, train_loss: 0.1118, step time: 0.1178\n",
      "117/223, train_loss: 0.1165, step time: 0.0994\n",
      "118/223, train_loss: 0.0972, step time: 0.1004\n",
      "119/223, train_loss: 0.1022, step time: 0.1001\n",
      "120/223, train_loss: 0.1098, step time: 0.1016\n",
      "121/223, train_loss: 0.1026, step time: 0.1000\n",
      "122/223, train_loss: 0.0967, step time: 0.1050\n",
      "123/223, train_loss: 0.1112, step time: 0.1202\n",
      "124/223, train_loss: 0.0989, step time: 0.1094\n",
      "125/223, train_loss: 0.1049, step time: 0.1107\n",
      "126/223, train_loss: 0.0961, step time: 0.1191\n",
      "127/223, train_loss: 0.1043, step time: 0.1145\n",
      "128/223, train_loss: 0.1011, step time: 0.1004\n",
      "129/223, train_loss: 0.1028, step time: 0.1047\n",
      "130/223, train_loss: 0.1122, step time: 0.1166\n",
      "131/223, train_loss: 0.1032, step time: 0.1121\n",
      "132/223, train_loss: 0.1005, step time: 0.1099\n",
      "133/223, train_loss: 0.1035, step time: 0.0992\n",
      "134/223, train_loss: 0.1049, step time: 0.1168\n",
      "135/223, train_loss: 0.1017, step time: 0.1223\n",
      "136/223, train_loss: 0.0947, step time: 0.1115\n",
      "137/223, train_loss: 0.0970, step time: 0.1198\n",
      "138/223, train_loss: 0.1108, step time: 0.1144\n",
      "139/223, train_loss: 0.1016, step time: 0.1101\n",
      "140/223, train_loss: 0.0950, step time: 0.1596\n",
      "141/223, train_loss: 0.1074, step time: 0.1084\n",
      "142/223, train_loss: 0.1041, step time: 0.1150\n",
      "143/223, train_loss: 0.1070, step time: 0.1118\n",
      "144/223, train_loss: 0.1011, step time: 0.1004\n",
      "145/223, train_loss: 0.0979, step time: 0.1001\n",
      "146/223, train_loss: 0.1014, step time: 0.1013\n",
      "147/223, train_loss: 0.1043, step time: 0.1138\n",
      "148/223, train_loss: 0.1040, step time: 0.1222\n",
      "149/223, train_loss: 0.1032, step time: 0.1178\n",
      "150/223, train_loss: 0.1111, step time: 0.1083\n",
      "151/223, train_loss: 0.1137, step time: 0.1003\n",
      "152/223, train_loss: 0.1047, step time: 0.1085\n",
      "153/223, train_loss: 0.1009, step time: 0.1094\n",
      "154/223, train_loss: 0.1180, step time: 0.1067\n",
      "155/223, train_loss: 0.1163, step time: 0.1002\n",
      "156/223, train_loss: 0.1116, step time: 0.1152\n",
      "157/223, train_loss: 0.0933, step time: 0.1094\n",
      "158/223, train_loss: 0.1120, step time: 0.1185\n",
      "159/223, train_loss: 0.1093, step time: 0.1013\n",
      "160/223, train_loss: 0.1137, step time: 0.1083\n",
      "161/223, train_loss: 0.1074, step time: 0.1223\n",
      "162/223, train_loss: 0.0981, step time: 0.1107\n",
      "163/223, train_loss: 0.1044, step time: 0.1004\n",
      "164/223, train_loss: 0.1194, step time: 0.1097\n",
      "165/223, train_loss: 0.1030, step time: 0.1152\n",
      "166/223, train_loss: 0.0959, step time: 0.1001\n",
      "167/223, train_loss: 0.1085, step time: 0.1145\n",
      "168/223, train_loss: 0.1146, step time: 0.1251\n",
      "169/223, train_loss: 0.1024, step time: 0.1141\n",
      "170/223, train_loss: 0.1064, step time: 0.1108\n",
      "171/223, train_loss: 0.1041, step time: 0.1272\n",
      "172/223, train_loss: 0.1060, step time: 0.1373\n",
      "173/223, train_loss: 0.1059, step time: 0.1159\n",
      "174/223, train_loss: 0.1012, step time: 0.1161\n",
      "175/223, train_loss: 0.1046, step time: 0.1313\n",
      "176/223, train_loss: 0.1038, step time: 0.1141\n",
      "177/223, train_loss: 0.1026, step time: 0.1122\n",
      "178/223, train_loss: 0.1120, step time: 0.1088\n",
      "179/223, train_loss: 0.1006, step time: 0.1061\n",
      "180/223, train_loss: 0.1199, step time: 0.1091\n",
      "181/223, train_loss: 0.1035, step time: 0.1100\n",
      "182/223, train_loss: 0.1098, step time: 0.1083\n",
      "183/223, train_loss: 0.1057, step time: 0.1268\n",
      "184/223, train_loss: 0.1194, step time: 0.1013\n",
      "185/223, train_loss: 0.1030, step time: 0.1194\n",
      "186/223, train_loss: 0.1097, step time: 0.1079\n",
      "187/223, train_loss: 0.1062, step time: 0.1136\n",
      "188/223, train_loss: 0.0945, step time: 0.1130\n",
      "189/223, train_loss: 0.2995, step time: 0.1088\n",
      "190/223, train_loss: 0.1011, step time: 0.1041\n",
      "191/223, train_loss: 0.0980, step time: 0.1004\n",
      "192/223, train_loss: 0.1041, step time: 0.1198\n",
      "193/223, train_loss: 0.1032, step time: 0.1134\n",
      "194/223, train_loss: 0.0991, step time: 0.1150\n",
      "195/223, train_loss: 0.1054, step time: 0.1018\n",
      "196/223, train_loss: 0.0963, step time: 0.1146\n",
      "197/223, train_loss: 0.1068, step time: 0.1192\n",
      "198/223, train_loss: 0.0942, step time: 0.0996\n",
      "199/223, train_loss: 0.1039, step time: 0.1007\n",
      "200/223, train_loss: 0.0985, step time: 0.1131\n",
      "201/223, train_loss: 0.1130, step time: 0.1114\n",
      "202/223, train_loss: 0.0996, step time: 0.1117\n",
      "203/223, train_loss: 0.1023, step time: 0.1007\n",
      "204/223, train_loss: 0.1065, step time: 0.1180\n",
      "205/223, train_loss: 0.1276, step time: 0.0989\n",
      "206/223, train_loss: 0.1085, step time: 0.1099\n",
      "207/223, train_loss: 0.1061, step time: 0.1198\n",
      "208/223, train_loss: 0.1064, step time: 0.1142\n",
      "209/223, train_loss: 0.1082, step time: 0.1061\n",
      "210/223, train_loss: 0.0971, step time: 0.1028\n",
      "211/223, train_loss: 0.1143, step time: 0.1000\n",
      "212/223, train_loss: 0.1146, step time: 0.1072\n",
      "213/223, train_loss: 0.1093, step time: 0.1041\n",
      "214/223, train_loss: 0.1071, step time: 0.1005\n",
      "215/223, train_loss: 0.1122, step time: 0.1008\n",
      "216/223, train_loss: 0.1179, step time: 0.1013\n",
      "217/223, train_loss: 0.1037, step time: 0.1175\n",
      "218/223, train_loss: 0.0942, step time: 0.1218\n",
      "219/223, train_loss: 0.1090, step time: 0.1171\n",
      "220/223, train_loss: 0.1136, step time: 0.0987\n",
      "221/223, train_loss: 0.0980, step time: 0.0989\n",
      "222/223, train_loss: 0.1123, step time: 0.0995\n",
      "223/223, train_loss: 0.1104, step time: 0.1014\n",
      "epoch 149 average loss: 0.1070\n",
      "time consuming of epoch 149 is: 86.8323\n",
      "----------\n",
      "epoch 150/300\n",
      "1/223, train_loss: 0.1055, step time: 0.1075\n",
      "2/223, train_loss: 0.1052, step time: 0.0992\n",
      "3/223, train_loss: 0.1045, step time: 0.1003\n",
      "4/223, train_loss: 0.1034, step time: 0.1092\n",
      "5/223, train_loss: 0.1086, step time: 0.1027\n",
      "6/223, train_loss: 0.1045, step time: 0.1005\n",
      "7/223, train_loss: 0.1126, step time: 0.1021\n",
      "8/223, train_loss: 0.1053, step time: 0.1142\n",
      "9/223, train_loss: 0.1131, step time: 0.1296\n",
      "10/223, train_loss: 0.1146, step time: 0.1097\n",
      "11/223, train_loss: 0.1175, step time: 0.1086\n",
      "12/223, train_loss: 0.1025, step time: 0.0997\n",
      "13/223, train_loss: 0.1070, step time: 0.1000\n",
      "14/223, train_loss: 0.1164, step time: 0.1000\n",
      "15/223, train_loss: 0.1019, step time: 0.1115\n",
      "16/223, train_loss: 0.1078, step time: 0.1026\n",
      "17/223, train_loss: 0.1046, step time: 0.1211\n",
      "18/223, train_loss: 0.1123, step time: 0.1359\n",
      "19/223, train_loss: 0.0991, step time: 0.1021\n",
      "20/223, train_loss: 0.0992, step time: 0.1003\n",
      "21/223, train_loss: 0.0989, step time: 0.1254\n",
      "22/223, train_loss: 0.1178, step time: 0.1132\n",
      "23/223, train_loss: 0.0974, step time: 0.1148\n",
      "24/223, train_loss: 0.1031, step time: 0.1010\n",
      "25/223, train_loss: 0.1054, step time: 0.1096\n",
      "26/223, train_loss: 0.1259, step time: 0.1017\n",
      "27/223, train_loss: 0.1058, step time: 0.1006\n",
      "28/223, train_loss: 0.1026, step time: 0.1014\n",
      "29/223, train_loss: 0.1048, step time: 0.1326\n",
      "30/223, train_loss: 0.1059, step time: 0.1172\n",
      "31/223, train_loss: 0.1160, step time: 0.1274\n",
      "32/223, train_loss: 0.1170, step time: 0.1091\n",
      "33/223, train_loss: 0.0961, step time: 0.1012\n",
      "34/223, train_loss: 0.1043, step time: 0.1207\n",
      "35/223, train_loss: 0.1094, step time: 0.1498\n",
      "36/223, train_loss: 0.1045, step time: 0.1105\n",
      "37/223, train_loss: 0.1064, step time: 0.0996\n",
      "38/223, train_loss: 0.1074, step time: 0.1010\n",
      "39/223, train_loss: 0.1152, step time: 0.1125\n",
      "40/223, train_loss: 0.0939, step time: 0.1074\n",
      "41/223, train_loss: 0.1062, step time: 0.1059\n",
      "42/223, train_loss: 0.1105, step time: 0.1178\n",
      "43/223, train_loss: 0.1095, step time: 0.1025\n",
      "44/223, train_loss: 0.1037, step time: 0.1102\n",
      "45/223, train_loss: 0.1096, step time: 0.1052\n",
      "46/223, train_loss: 0.1067, step time: 0.1051\n",
      "47/223, train_loss: 0.1147, step time: 0.1012\n",
      "48/223, train_loss: 0.1156, step time: 0.0998\n",
      "49/223, train_loss: 0.1156, step time: 0.0995\n",
      "50/223, train_loss: 0.1033, step time: 0.1045\n",
      "51/223, train_loss: 0.1066, step time: 0.1004\n",
      "52/223, train_loss: 0.1085, step time: 0.1005\n",
      "53/223, train_loss: 0.1067, step time: 0.0996\n",
      "54/223, train_loss: 0.1084, step time: 0.1111\n",
      "55/223, train_loss: 0.0947, step time: 0.1139\n",
      "56/223, train_loss: 0.1051, step time: 0.1176\n",
      "57/223, train_loss: 0.0900, step time: 0.1001\n",
      "58/223, train_loss: 0.1104, step time: 0.0999\n",
      "59/223, train_loss: 0.1063, step time: 0.1082\n",
      "60/223, train_loss: 0.0972, step time: 0.1020\n",
      "61/223, train_loss: 0.1166, step time: 0.1087\n",
      "62/223, train_loss: 0.1028, step time: 0.1244\n",
      "63/223, train_loss: 0.1014, step time: 0.1199\n",
      "64/223, train_loss: 0.1198, step time: 0.1104\n",
      "65/223, train_loss: 0.1117, step time: 0.1066\n",
      "66/223, train_loss: 0.0959, step time: 0.1175\n",
      "67/223, train_loss: 0.1249, step time: 0.1156\n",
      "68/223, train_loss: 0.1013, step time: 0.1102\n",
      "69/223, train_loss: 0.0929, step time: 0.1185\n",
      "70/223, train_loss: 0.1068, step time: 0.0999\n",
      "71/223, train_loss: 0.1131, step time: 0.0996\n",
      "72/223, train_loss: 0.1011, step time: 0.1008\n",
      "73/223, train_loss: 0.0956, step time: 0.1344\n",
      "74/223, train_loss: 0.1229, step time: 0.1150\n",
      "75/223, train_loss: 0.1090, step time: 0.1194\n",
      "76/223, train_loss: 0.1123, step time: 0.1169\n",
      "77/223, train_loss: 0.1008, step time: 0.1172\n",
      "78/223, train_loss: 0.0983, step time: 0.1173\n",
      "79/223, train_loss: 0.0968, step time: 0.1001\n",
      "80/223, train_loss: 0.1020, step time: 0.0996\n",
      "81/223, train_loss: 0.1006, step time: 0.1095\n",
      "82/223, train_loss: 0.1068, step time: 0.1072\n",
      "83/223, train_loss: 0.1169, step time: 0.1509\n",
      "84/223, train_loss: 0.1000, step time: 0.1249\n",
      "85/223, train_loss: 0.1173, step time: 0.1058\n",
      "86/223, train_loss: 0.1053, step time: 0.1183\n",
      "87/223, train_loss: 0.1013, step time: 0.1011\n",
      "88/223, train_loss: 0.1271, step time: 0.1004\n",
      "89/223, train_loss: 0.1134, step time: 0.1065\n",
      "90/223, train_loss: 0.1078, step time: 0.1098\n",
      "91/223, train_loss: 0.1108, step time: 0.1036\n",
      "92/223, train_loss: 0.0964, step time: 0.1006\n",
      "93/223, train_loss: 0.1111, step time: 0.1103\n",
      "94/223, train_loss: 0.1169, step time: 0.1063\n",
      "95/223, train_loss: 0.0965, step time: 0.1041\n",
      "96/223, train_loss: 0.0978, step time: 0.1018\n",
      "97/223, train_loss: 0.1055, step time: 0.1003\n",
      "98/223, train_loss: 0.1021, step time: 0.0996\n",
      "99/223, train_loss: 0.1036, step time: 0.1122\n",
      "100/223, train_loss: 0.0986, step time: 0.1067\n",
      "101/223, train_loss: 0.0981, step time: 0.1201\n",
      "102/223, train_loss: 0.0997, step time: 0.1119\n",
      "103/223, train_loss: 0.1006, step time: 0.1304\n",
      "104/223, train_loss: 0.1138, step time: 0.1297\n",
      "105/223, train_loss: 0.0996, step time: 0.1006\n",
      "106/223, train_loss: 0.0953, step time: 0.0984\n",
      "107/223, train_loss: 0.1037, step time: 0.1004\n",
      "108/223, train_loss: 0.1065, step time: 0.1005\n",
      "109/223, train_loss: 0.1038, step time: 0.1140\n",
      "110/223, train_loss: 0.1113, step time: 0.1073\n",
      "111/223, train_loss: 0.1077, step time: 0.1168\n",
      "112/223, train_loss: 0.0996, step time: 0.1115\n",
      "113/223, train_loss: 0.1053, step time: 0.1060\n",
      "114/223, train_loss: 0.1068, step time: 0.1520\n",
      "115/223, train_loss: 0.1080, step time: 0.1060\n",
      "116/223, train_loss: 0.0965, step time: 0.0990\n",
      "117/223, train_loss: 0.0995, step time: 0.1137\n",
      "118/223, train_loss: 0.1076, step time: 0.1341\n",
      "119/223, train_loss: 0.0956, step time: 0.1227\n",
      "120/223, train_loss: 0.1144, step time: 0.1088\n",
      "121/223, train_loss: 0.1076, step time: 0.1159\n",
      "122/223, train_loss: 0.1082, step time: 0.1068\n",
      "123/223, train_loss: 0.1181, step time: 0.1017\n",
      "124/223, train_loss: 0.1168, step time: 0.1004\n",
      "125/223, train_loss: 0.1043, step time: 0.1097\n",
      "126/223, train_loss: 0.1174, step time: 0.1004\n",
      "127/223, train_loss: 0.1128, step time: 0.1087\n",
      "128/223, train_loss: 0.1037, step time: 0.1169\n",
      "129/223, train_loss: 0.1167, step time: 0.1213\n",
      "130/223, train_loss: 0.1149, step time: 0.1083\n",
      "131/223, train_loss: 0.1148, step time: 0.1035\n",
      "132/223, train_loss: 0.1221, step time: 0.1099\n",
      "133/223, train_loss: 0.1093, step time: 0.1326\n",
      "134/223, train_loss: 0.1030, step time: 0.1095\n",
      "135/223, train_loss: 0.1141, step time: 0.1000\n",
      "136/223, train_loss: 0.1166, step time: 0.1001\n",
      "137/223, train_loss: 0.1062, step time: 0.0992\n",
      "138/223, train_loss: 0.1101, step time: 0.1111\n",
      "139/223, train_loss: 0.1074, step time: 0.1509\n",
      "140/223, train_loss: 0.0979, step time: 0.1008\n",
      "141/223, train_loss: 0.1128, step time: 0.1125\n",
      "142/223, train_loss: 0.0999, step time: 0.1318\n",
      "143/223, train_loss: 0.1129, step time: 0.1091\n",
      "144/223, train_loss: 0.1029, step time: 0.1011\n",
      "145/223, train_loss: 0.1098, step time: 0.1691\n",
      "146/223, train_loss: 0.1154, step time: 0.1136\n",
      "147/223, train_loss: 0.1012, step time: 0.1072\n",
      "148/223, train_loss: 0.1166, step time: 0.1005\n",
      "149/223, train_loss: 0.1079, step time: 0.1188\n",
      "150/223, train_loss: 0.0972, step time: 0.1115\n",
      "151/223, train_loss: 0.1092, step time: 0.1086\n",
      "152/223, train_loss: 0.1042, step time: 0.1008\n",
      "153/223, train_loss: 0.1063, step time: 0.1075\n",
      "154/223, train_loss: 0.1197, step time: 0.1039\n",
      "155/223, train_loss: 0.1162, step time: 0.1011\n",
      "156/223, train_loss: 0.1012, step time: 0.1013\n",
      "157/223, train_loss: 0.1047, step time: 0.1059\n",
      "158/223, train_loss: 0.1058, step time: 0.1117\n",
      "159/223, train_loss: 0.1058, step time: 0.1103\n",
      "160/223, train_loss: 0.1109, step time: 0.1050\n",
      "161/223, train_loss: 0.1014, step time: 0.1145\n",
      "162/223, train_loss: 0.1140, step time: 0.0998\n",
      "163/223, train_loss: 0.1111, step time: 0.1001\n",
      "164/223, train_loss: 0.1091, step time: 0.1125\n",
      "165/223, train_loss: 0.1059, step time: 0.1036\n",
      "166/223, train_loss: 0.1066, step time: 0.1210\n",
      "167/223, train_loss: 0.1078, step time: 0.1010\n",
      "168/223, train_loss: 0.1071, step time: 0.1003\n",
      "169/223, train_loss: 0.1043, step time: 0.1341\n",
      "170/223, train_loss: 0.1214, step time: 0.1089\n",
      "171/223, train_loss: 0.1137, step time: 0.1002\n",
      "172/223, train_loss: 0.1144, step time: 0.1025\n",
      "173/223, train_loss: 0.1108, step time: 0.1143\n",
      "174/223, train_loss: 0.0968, step time: 0.1198\n",
      "175/223, train_loss: 0.1098, step time: 0.1188\n",
      "176/223, train_loss: 0.1032, step time: 0.1173\n",
      "177/223, train_loss: 0.1134, step time: 0.1047\n",
      "178/223, train_loss: 0.1104, step time: 0.1102\n",
      "179/223, train_loss: 0.1011, step time: 0.1106\n",
      "180/223, train_loss: 0.1079, step time: 0.1000\n",
      "181/223, train_loss: 0.0974, step time: 0.1002\n",
      "182/223, train_loss: 0.1023, step time: 0.1598\n",
      "183/223, train_loss: 0.1032, step time: 0.1260\n",
      "184/223, train_loss: 0.0967, step time: 0.1011\n",
      "185/223, train_loss: 0.1121, step time: 0.1069\n",
      "186/223, train_loss: 0.1034, step time: 0.1128\n",
      "187/223, train_loss: 0.1077, step time: 0.1172\n",
      "188/223, train_loss: 0.1195, step time: 0.1084\n",
      "189/223, train_loss: 0.1038, step time: 0.1096\n",
      "190/223, train_loss: 0.1047, step time: 0.1006\n",
      "191/223, train_loss: 0.1006, step time: 0.1022\n",
      "192/223, train_loss: 0.1072, step time: 0.1047\n",
      "193/223, train_loss: 0.0941, step time: 0.1027\n",
      "194/223, train_loss: 0.1002, step time: 0.1003\n",
      "195/223, train_loss: 0.1103, step time: 0.1007\n",
      "196/223, train_loss: 0.1031, step time: 0.1267\n",
      "197/223, train_loss: 0.1041, step time: 0.1095\n",
      "198/223, train_loss: 0.1026, step time: 0.1323\n",
      "199/223, train_loss: 0.0996, step time: 0.1559\n",
      "200/223, train_loss: 0.1077, step time: 0.1187\n",
      "201/223, train_loss: 0.1039, step time: 0.1043\n",
      "202/223, train_loss: 0.1127, step time: 0.0996\n",
      "203/223, train_loss: 0.0995, step time: 0.1183\n",
      "204/223, train_loss: 0.0994, step time: 0.0993\n",
      "205/223, train_loss: 0.1108, step time: 0.1102\n",
      "206/223, train_loss: 0.1023, step time: 0.1291\n",
      "207/223, train_loss: 0.1023, step time: 0.1033\n",
      "208/223, train_loss: 0.1064, step time: 0.1009\n",
      "209/223, train_loss: 0.1043, step time: 0.1215\n",
      "210/223, train_loss: 0.1008, step time: 0.1322\n",
      "211/223, train_loss: 0.1154, step time: 0.1054\n",
      "212/223, train_loss: 0.1218, step time: 0.1053\n",
      "213/223, train_loss: 0.0943, step time: 0.1386\n",
      "214/223, train_loss: 0.0977, step time: 0.1359\n",
      "215/223, train_loss: 0.0966, step time: 0.1335\n",
      "216/223, train_loss: 0.1023, step time: 0.1005\n",
      "217/223, train_loss: 0.0936, step time: 0.1072\n",
      "218/223, train_loss: 0.1019, step time: 0.1009\n",
      "219/223, train_loss: 0.1056, step time: 0.1006\n",
      "220/223, train_loss: 0.3008, step time: 0.1004\n",
      "221/223, train_loss: 0.0976, step time: 0.0993\n",
      "222/223, train_loss: 0.1079, step time: 0.1002\n",
      "223/223, train_loss: 0.1142, step time: 0.0999\n",
      "epoch 150 average loss: 0.1075\n",
      "saved new best metric model\n",
      "current epoch: 150 current mean dice: 0.8551 tc: 0.9183 wt: 0.8646 et: 0.7825\n",
      "best mean dice: 0.8551 at epoch: 150\n",
      "time consuming of epoch 150 is: 90.0611\n",
      "----------\n",
      "epoch 151/300\n",
      "1/223, train_loss: 0.1125, step time: 0.1094\n",
      "2/223, train_loss: 0.1039, step time: 0.1035\n",
      "3/223, train_loss: 0.1067, step time: 0.1257\n",
      "4/223, train_loss: 0.1030, step time: 0.1089\n",
      "5/223, train_loss: 0.1031, step time: 0.1076\n",
      "6/223, train_loss: 0.1062, step time: 0.1005\n",
      "7/223, train_loss: 0.1044, step time: 0.1338\n",
      "8/223, train_loss: 0.1002, step time: 0.1373\n",
      "9/223, train_loss: 0.1029, step time: 0.1321\n",
      "10/223, train_loss: 0.1145, step time: 0.1252\n",
      "11/223, train_loss: 0.1151, step time: 0.0992\n",
      "12/223, train_loss: 0.0993, step time: 0.1003\n",
      "13/223, train_loss: 0.1075, step time: 0.1130\n",
      "14/223, train_loss: 0.1011, step time: 0.1105\n",
      "15/223, train_loss: 0.1006, step time: 0.0994\n",
      "16/223, train_loss: 0.1131, step time: 0.1043\n",
      "17/223, train_loss: 0.1077, step time: 0.1247\n",
      "18/223, train_loss: 0.1130, step time: 0.1237\n",
      "19/223, train_loss: 0.1040, step time: 0.1178\n",
      "20/223, train_loss: 0.0990, step time: 0.1012\n",
      "21/223, train_loss: 0.1131, step time: 0.1135\n",
      "22/223, train_loss: 0.1104, step time: 0.1088\n",
      "23/223, train_loss: 0.1146, step time: 0.1275\n",
      "24/223, train_loss: 0.1249, step time: 0.1026\n",
      "25/223, train_loss: 0.1103, step time: 0.1120\n",
      "26/223, train_loss: 0.1182, step time: 0.1105\n",
      "27/223, train_loss: 0.1247, step time: 0.1196\n",
      "28/223, train_loss: 0.1021, step time: 0.1016\n",
      "29/223, train_loss: 0.1004, step time: 0.1115\n",
      "30/223, train_loss: 0.0951, step time: 0.1105\n",
      "31/223, train_loss: 0.1076, step time: 0.1005\n",
      "32/223, train_loss: 0.1079, step time: 0.1526\n",
      "33/223, train_loss: 0.1055, step time: 0.1147\n",
      "34/223, train_loss: 0.1028, step time: 0.1057\n",
      "35/223, train_loss: 0.1038, step time: 0.1145\n",
      "36/223, train_loss: 0.1077, step time: 0.1004\n",
      "37/223, train_loss: 0.1034, step time: 0.1130\n",
      "38/223, train_loss: 0.1100, step time: 0.1006\n",
      "39/223, train_loss: 0.1035, step time: 0.1131\n",
      "40/223, train_loss: 0.1183, step time: 0.1106\n",
      "41/223, train_loss: 0.1001, step time: 0.1135\n",
      "42/223, train_loss: 0.0993, step time: 0.1062\n",
      "43/223, train_loss: 0.1007, step time: 0.1209\n",
      "44/223, train_loss: 0.1013, step time: 0.1004\n",
      "45/223, train_loss: 0.1097, step time: 0.1084\n",
      "46/223, train_loss: 0.0928, step time: 0.1011\n",
      "47/223, train_loss: 0.1083, step time: 0.1076\n",
      "48/223, train_loss: 0.1191, step time: 0.1006\n",
      "49/223, train_loss: 0.1037, step time: 0.1153\n",
      "50/223, train_loss: 0.1083, step time: 0.1006\n",
      "51/223, train_loss: 0.1000, step time: 0.1108\n",
      "52/223, train_loss: 0.1068, step time: 0.1006\n",
      "53/223, train_loss: 0.1170, step time: 0.1148\n",
      "54/223, train_loss: 0.1000, step time: 0.0995\n",
      "55/223, train_loss: 0.0921, step time: 0.1079\n",
      "56/223, train_loss: 0.0997, step time: 0.1221\n",
      "57/223, train_loss: 0.1098, step time: 0.1194\n",
      "58/223, train_loss: 0.1105, step time: 0.1080\n",
      "59/223, train_loss: 0.1030, step time: 0.1002\n",
      "60/223, train_loss: 0.1074, step time: 0.1052\n",
      "61/223, train_loss: 0.1119, step time: 0.1127\n",
      "62/223, train_loss: 0.1209, step time: 0.1103\n",
      "63/223, train_loss: 0.1156, step time: 0.0999\n",
      "64/223, train_loss: 0.1196, step time: 0.1000\n",
      "65/223, train_loss: 0.1097, step time: 0.1024\n",
      "66/223, train_loss: 0.1117, step time: 0.1005\n",
      "67/223, train_loss: 0.1030, step time: 0.1313\n",
      "68/223, train_loss: 0.1016, step time: 0.1055\n",
      "69/223, train_loss: 0.1023, step time: 0.1000\n",
      "70/223, train_loss: 0.1019, step time: 0.0995\n",
      "71/223, train_loss: 0.1003, step time: 0.1130\n",
      "72/223, train_loss: 0.1002, step time: 0.1058\n",
      "73/223, train_loss: 0.1097, step time: 0.1196\n",
      "74/223, train_loss: 0.1081, step time: 0.1058\n",
      "75/223, train_loss: 0.1110, step time: 0.1184\n",
      "76/223, train_loss: 0.1055, step time: 0.1310\n",
      "77/223, train_loss: 0.1055, step time: 0.1046\n",
      "78/223, train_loss: 0.1075, step time: 0.1015\n",
      "79/223, train_loss: 0.3134, step time: 0.1127\n",
      "80/223, train_loss: 0.1033, step time: 0.1261\n",
      "81/223, train_loss: 0.1071, step time: 0.1141\n",
      "82/223, train_loss: 0.1090, step time: 0.1075\n",
      "83/223, train_loss: 0.1116, step time: 0.1472\n",
      "84/223, train_loss: 0.1098, step time: 0.1007\n",
      "85/223, train_loss: 0.0973, step time: 0.1010\n",
      "86/223, train_loss: 0.1074, step time: 0.0997\n",
      "87/223, train_loss: 0.1045, step time: 0.1271\n",
      "88/223, train_loss: 0.0999, step time: 0.1074\n",
      "89/223, train_loss: 0.0954, step time: 0.1011\n",
      "90/223, train_loss: 0.0999, step time: 0.1006\n",
      "91/223, train_loss: 0.0922, step time: 0.1093\n",
      "92/223, train_loss: 0.1030, step time: 0.1056\n",
      "93/223, train_loss: 0.1237, step time: 0.1093\n",
      "94/223, train_loss: 0.1127, step time: 0.1004\n",
      "95/223, train_loss: 0.1024, step time: 0.0997\n",
      "96/223, train_loss: 0.0983, step time: 0.1014\n",
      "97/223, train_loss: 0.1023, step time: 0.1095\n",
      "98/223, train_loss: 0.1105, step time: 0.1030\n",
      "99/223, train_loss: 0.1084, step time: 0.1246\n",
      "100/223, train_loss: 0.1022, step time: 0.1566\n",
      "101/223, train_loss: 0.1012, step time: 0.1143\n",
      "102/223, train_loss: 0.0988, step time: 0.1479\n",
      "103/223, train_loss: 0.1109, step time: 0.1379\n",
      "104/223, train_loss: 0.1080, step time: 0.1301\n",
      "105/223, train_loss: 0.0991, step time: 0.1179\n",
      "106/223, train_loss: 0.1091, step time: 0.1095\n",
      "107/223, train_loss: 0.1127, step time: 0.1130\n",
      "108/223, train_loss: 0.1060, step time: 0.1008\n",
      "109/223, train_loss: 0.1074, step time: 0.1001\n",
      "110/223, train_loss: 0.1051, step time: 0.1000\n",
      "111/223, train_loss: 0.0958, step time: 0.1099\n",
      "112/223, train_loss: 0.1086, step time: 0.1035\n",
      "113/223, train_loss: 0.1081, step time: 0.1750\n",
      "114/223, train_loss: 0.1025, step time: 0.1308\n",
      "115/223, train_loss: 0.1121, step time: 0.1054\n",
      "116/223, train_loss: 0.1075, step time: 0.0999\n",
      "117/223, train_loss: 0.1052, step time: 0.1065\n",
      "118/223, train_loss: 0.1029, step time: 0.0998\n",
      "119/223, train_loss: 0.1057, step time: 0.1011\n",
      "120/223, train_loss: 0.0983, step time: 0.1268\n",
      "121/223, train_loss: 0.1028, step time: 0.1579\n",
      "122/223, train_loss: 0.1087, step time: 0.1206\n",
      "123/223, train_loss: 0.0985, step time: 0.1003\n",
      "124/223, train_loss: 0.1175, step time: 0.1053\n",
      "125/223, train_loss: 0.1052, step time: 0.1111\n",
      "126/223, train_loss: 0.1132, step time: 0.1121\n",
      "127/223, train_loss: 0.1032, step time: 0.1007\n",
      "128/223, train_loss: 0.1030, step time: 0.1005\n",
      "129/223, train_loss: 0.1046, step time: 0.1100\n",
      "130/223, train_loss: 0.1203, step time: 0.1008\n",
      "131/223, train_loss: 0.1124, step time: 0.1142\n",
      "132/223, train_loss: 0.1176, step time: 0.1012\n",
      "133/223, train_loss: 0.1073, step time: 0.1018\n",
      "134/223, train_loss: 0.1045, step time: 0.0994\n",
      "135/223, train_loss: 0.1105, step time: 0.1152\n",
      "136/223, train_loss: 0.1035, step time: 0.1079\n",
      "137/223, train_loss: 0.1102, step time: 0.1031\n",
      "138/223, train_loss: 0.1126, step time: 0.1069\n",
      "139/223, train_loss: 0.1085, step time: 0.1234\n",
      "140/223, train_loss: 0.1039, step time: 0.1145\n",
      "141/223, train_loss: 0.0993, step time: 0.1098\n",
      "142/223, train_loss: 0.1016, step time: 0.1006\n",
      "143/223, train_loss: 0.1202, step time: 0.1005\n",
      "144/223, train_loss: 0.1063, step time: 0.1032\n",
      "145/223, train_loss: 0.1036, step time: 0.1000\n",
      "146/223, train_loss: 0.1061, step time: 0.0995\n",
      "147/223, train_loss: 0.1189, step time: 0.1006\n",
      "148/223, train_loss: 0.1085, step time: 0.1014\n",
      "149/223, train_loss: 0.1018, step time: 0.1012\n",
      "150/223, train_loss: 0.0976, step time: 0.0998\n",
      "151/223, train_loss: 0.1060, step time: 0.1004\n",
      "152/223, train_loss: 0.1121, step time: 0.1484\n",
      "153/223, train_loss: 0.1092, step time: 0.1026\n",
      "154/223, train_loss: 0.1106, step time: 0.1004\n",
      "155/223, train_loss: 0.1088, step time: 0.2119\n",
      "156/223, train_loss: 0.1111, step time: 0.1011\n",
      "157/223, train_loss: 0.1017, step time: 0.1195\n",
      "158/223, train_loss: 0.1085, step time: 0.1246\n",
      "159/223, train_loss: 0.0990, step time: 0.1064\n",
      "160/223, train_loss: 0.1081, step time: 0.1004\n",
      "161/223, train_loss: 0.1105, step time: 0.1038\n",
      "162/223, train_loss: 0.1109, step time: 0.1095\n",
      "163/223, train_loss: 0.0955, step time: 0.1029\n",
      "164/223, train_loss: 0.1204, step time: 0.0996\n",
      "165/223, train_loss: 0.0943, step time: 0.1074\n",
      "166/223, train_loss: 0.0972, step time: 0.1000\n",
      "167/223, train_loss: 0.1045, step time: 0.1049\n",
      "168/223, train_loss: 0.1168, step time: 0.1017\n",
      "169/223, train_loss: 0.0986, step time: 0.1166\n",
      "170/223, train_loss: 0.1082, step time: 0.1157\n",
      "171/223, train_loss: 0.0999, step time: 0.1017\n",
      "172/223, train_loss: 0.1040, step time: 0.1039\n",
      "173/223, train_loss: 0.0955, step time: 0.1174\n",
      "174/223, train_loss: 0.1122, step time: 0.1008\n",
      "175/223, train_loss: 0.1022, step time: 0.1012\n",
      "176/223, train_loss: 0.1096, step time: 0.1003\n",
      "177/223, train_loss: 0.0982, step time: 0.1154\n",
      "178/223, train_loss: 0.1097, step time: 0.1128\n",
      "179/223, train_loss: 0.1053, step time: 0.1655\n",
      "180/223, train_loss: 0.1043, step time: 0.1095\n",
      "181/223, train_loss: 0.0969, step time: 0.1092\n",
      "182/223, train_loss: 0.0972, step time: 0.1006\n",
      "183/223, train_loss: 0.1151, step time: 0.1001\n",
      "184/223, train_loss: 0.1010, step time: 0.1086\n",
      "185/223, train_loss: 0.1024, step time: 0.1003\n",
      "186/223, train_loss: 0.1126, step time: 0.1194\n",
      "187/223, train_loss: 0.1033, step time: 0.1169\n",
      "188/223, train_loss: 0.1043, step time: 0.1262\n",
      "189/223, train_loss: 0.0999, step time: 0.1060\n",
      "190/223, train_loss: 0.1048, step time: 0.1308\n",
      "191/223, train_loss: 0.1055, step time: 0.1145\n",
      "192/223, train_loss: 0.0989, step time: 0.1010\n",
      "193/223, train_loss: 0.1076, step time: 0.1200\n",
      "194/223, train_loss: 0.1011, step time: 0.1094\n",
      "195/223, train_loss: 0.0963, step time: 0.1016\n",
      "196/223, train_loss: 0.1127, step time: 0.1040\n",
      "197/223, train_loss: 0.1017, step time: 0.1030\n",
      "198/223, train_loss: 0.1142, step time: 0.1206\n",
      "199/223, train_loss: 0.1064, step time: 0.1054\n",
      "200/223, train_loss: 0.1157, step time: 0.1133\n",
      "201/223, train_loss: 0.1087, step time: 0.1134\n",
      "202/223, train_loss: 0.1027, step time: 0.1137\n",
      "203/223, train_loss: 0.1211, step time: 0.1185\n",
      "204/223, train_loss: 0.1099, step time: 0.1002\n",
      "205/223, train_loss: 0.1081, step time: 0.1202\n",
      "206/223, train_loss: 0.1098, step time: 0.1323\n",
      "207/223, train_loss: 0.1013, step time: 0.1004\n",
      "208/223, train_loss: 0.1115, step time: 0.1004\n",
      "209/223, train_loss: 0.1046, step time: 0.0998\n",
      "210/223, train_loss: 0.1200, step time: 0.1006\n",
      "211/223, train_loss: 0.1026, step time: 0.1041\n",
      "212/223, train_loss: 0.1157, step time: 0.1100\n",
      "213/223, train_loss: 0.1078, step time: 0.1134\n",
      "214/223, train_loss: 0.1021, step time: 0.1079\n",
      "215/223, train_loss: 0.1015, step time: 0.1022\n",
      "216/223, train_loss: 0.1041, step time: 0.1004\n",
      "217/223, train_loss: 0.1073, step time: 0.1006\n",
      "218/223, train_loss: 0.1129, step time: 0.1003\n",
      "219/223, train_loss: 0.1086, step time: 0.1027\n",
      "220/223, train_loss: 0.1073, step time: 0.1343\n",
      "221/223, train_loss: 0.1074, step time: 0.0994\n",
      "222/223, train_loss: 0.1137, step time: 0.0987\n",
      "223/223, train_loss: 0.1069, step time: 0.1006\n",
      "epoch 151 average loss: 0.1074\n",
      "time consuming of epoch 151 is: 90.5402\n",
      "----------\n",
      "epoch 152/300\n",
      "1/223, train_loss: 0.1040, step time: 0.1071\n",
      "2/223, train_loss: 0.1136, step time: 0.1077\n",
      "3/223, train_loss: 0.1053, step time: 0.1052\n",
      "4/223, train_loss: 0.1113, step time: 0.1103\n",
      "5/223, train_loss: 0.1050, step time: 0.1043\n",
      "6/223, train_loss: 0.1049, step time: 0.1130\n",
      "7/223, train_loss: 0.1069, step time: 0.1496\n",
      "8/223, train_loss: 0.1122, step time: 0.1190\n",
      "9/223, train_loss: 0.1112, step time: 0.1138\n",
      "10/223, train_loss: 0.1129, step time: 0.1433\n",
      "11/223, train_loss: 0.1037, step time: 0.1200\n",
      "12/223, train_loss: 0.1118, step time: 0.1289\n",
      "13/223, train_loss: 0.1033, step time: 0.1154\n",
      "14/223, train_loss: 0.0979, step time: 0.1008\n",
      "15/223, train_loss: 0.1060, step time: 0.1003\n",
      "16/223, train_loss: 0.1242, step time: 0.1004\n",
      "17/223, train_loss: 0.1026, step time: 0.1005\n",
      "18/223, train_loss: 0.1076, step time: 0.1150\n",
      "19/223, train_loss: 0.1037, step time: 0.1008\n",
      "20/223, train_loss: 0.1100, step time: 0.1016\n",
      "21/223, train_loss: 0.1029, step time: 0.1061\n",
      "22/223, train_loss: 0.1170, step time: 0.1068\n",
      "23/223, train_loss: 0.0971, step time: 0.1212\n",
      "24/223, train_loss: 0.1150, step time: 0.1184\n",
      "25/223, train_loss: 0.0924, step time: 0.1213\n",
      "26/223, train_loss: 0.1017, step time: 0.1156\n",
      "27/223, train_loss: 0.1071, step time: 0.1309\n",
      "28/223, train_loss: 0.1122, step time: 0.1045\n",
      "29/223, train_loss: 0.1073, step time: 0.1192\n",
      "30/223, train_loss: 0.1013, step time: 0.1471\n",
      "31/223, train_loss: 0.1202, step time: 0.1160\n",
      "32/223, train_loss: 0.1094, step time: 0.0996\n",
      "33/223, train_loss: 0.1282, step time: 0.1305\n",
      "34/223, train_loss: 0.1069, step time: 0.1087\n",
      "35/223, train_loss: 0.1168, step time: 0.1001\n",
      "36/223, train_loss: 0.1041, step time: 0.0998\n",
      "37/223, train_loss: 0.1079, step time: 0.1057\n",
      "38/223, train_loss: 0.0963, step time: 0.1109\n",
      "39/223, train_loss: 0.0952, step time: 0.1217\n",
      "40/223, train_loss: 0.1089, step time: 0.1016\n",
      "41/223, train_loss: 0.0943, step time: 0.1069\n",
      "42/223, train_loss: 0.1171, step time: 0.1225\n",
      "43/223, train_loss: 0.1022, step time: 0.1337\n",
      "44/223, train_loss: 0.1176, step time: 0.1000\n",
      "45/223, train_loss: 0.1260, step time: 0.1131\n",
      "46/223, train_loss: 0.1064, step time: 0.1405\n",
      "47/223, train_loss: 0.1064, step time: 0.1095\n",
      "48/223, train_loss: 0.1184, step time: 0.1148\n",
      "49/223, train_loss: 0.1003, step time: 0.1166\n",
      "50/223, train_loss: 0.1037, step time: 0.1146\n",
      "51/223, train_loss: 0.1040, step time: 0.1207\n",
      "52/223, train_loss: 0.1098, step time: 0.1063\n",
      "53/223, train_loss: 0.1029, step time: 0.1014\n",
      "54/223, train_loss: 0.1120, step time: 0.1042\n",
      "55/223, train_loss: 0.1142, step time: 0.1000\n",
      "56/223, train_loss: 0.1154, step time: 0.1208\n",
      "57/223, train_loss: 0.1153, step time: 0.1053\n",
      "58/223, train_loss: 0.1091, step time: 0.1190\n",
      "59/223, train_loss: 0.1132, step time: 0.1072\n",
      "60/223, train_loss: 0.0936, step time: 0.0999\n",
      "61/223, train_loss: 0.1049, step time: 0.1045\n",
      "62/223, train_loss: 0.1084, step time: 0.0998\n",
      "63/223, train_loss: 0.1057, step time: 0.1016\n",
      "64/223, train_loss: 0.1038, step time: 0.1026\n",
      "65/223, train_loss: 0.0967, step time: 0.1280\n",
      "66/223, train_loss: 0.1076, step time: 0.1058\n",
      "67/223, train_loss: 0.1124, step time: 0.1025\n",
      "68/223, train_loss: 0.1087, step time: 0.1144\n",
      "69/223, train_loss: 0.0985, step time: 0.0999\n",
      "70/223, train_loss: 0.1030, step time: 0.1061\n",
      "71/223, train_loss: 0.1045, step time: 0.1280\n",
      "72/223, train_loss: 0.1113, step time: 0.1045\n",
      "73/223, train_loss: 0.1104, step time: 0.1153\n",
      "74/223, train_loss: 0.1011, step time: 0.1128\n",
      "75/223, train_loss: 0.0971, step time: 0.0995\n",
      "76/223, train_loss: 0.1035, step time: 0.1251\n",
      "77/223, train_loss: 0.1102, step time: 0.1138\n",
      "78/223, train_loss: 0.1004, step time: 0.1002\n",
      "79/223, train_loss: 0.0961, step time: 0.0998\n",
      "80/223, train_loss: 0.0972, step time: 0.1097\n",
      "81/223, train_loss: 0.1055, step time: 0.1210\n",
      "82/223, train_loss: 0.1043, step time: 0.1186\n",
      "83/223, train_loss: 0.1002, step time: 0.1135\n",
      "84/223, train_loss: 0.1016, step time: 0.1049\n",
      "85/223, train_loss: 0.1118, step time: 0.1341\n",
      "86/223, train_loss: 0.1005, step time: 0.1126\n",
      "87/223, train_loss: 0.1086, step time: 0.1224\n",
      "88/223, train_loss: 0.1115, step time: 0.1042\n",
      "89/223, train_loss: 0.1062, step time: 0.1073\n",
      "90/223, train_loss: 0.1127, step time: 0.1239\n",
      "91/223, train_loss: 0.0922, step time: 0.1067\n",
      "92/223, train_loss: 0.0988, step time: 0.1001\n",
      "93/223, train_loss: 0.1204, step time: 0.1130\n",
      "94/223, train_loss: 0.0980, step time: 0.1035\n",
      "95/223, train_loss: 0.1003, step time: 0.1026\n",
      "96/223, train_loss: 0.0976, step time: 0.1016\n",
      "97/223, train_loss: 0.1109, step time: 0.0999\n",
      "98/223, train_loss: 0.1095, step time: 0.1011\n",
      "99/223, train_loss: 0.1055, step time: 0.1129\n",
      "100/223, train_loss: 0.1100, step time: 0.1115\n",
      "101/223, train_loss: 0.1116, step time: 0.1364\n",
      "102/223, train_loss: 0.1086, step time: 0.1103\n",
      "103/223, train_loss: 0.1034, step time: 0.1011\n",
      "104/223, train_loss: 0.1073, step time: 0.1030\n",
      "105/223, train_loss: 0.0985, step time: 0.1002\n",
      "106/223, train_loss: 0.0998, step time: 0.1212\n",
      "107/223, train_loss: 0.1145, step time: 0.1486\n",
      "108/223, train_loss: 0.1005, step time: 0.1105\n",
      "109/223, train_loss: 0.1068, step time: 0.1154\n",
      "110/223, train_loss: 0.1091, step time: 0.1461\n",
      "111/223, train_loss: 0.1072, step time: 0.1050\n",
      "112/223, train_loss: 0.1157, step time: 0.0997\n",
      "113/223, train_loss: 0.0993, step time: 0.1008\n",
      "114/223, train_loss: 0.1108, step time: 0.1192\n",
      "115/223, train_loss: 0.0996, step time: 0.1413\n",
      "116/223, train_loss: 0.0951, step time: 0.1336\n",
      "117/223, train_loss: 0.1209, step time: 0.1150\n",
      "118/223, train_loss: 0.1055, step time: 0.1013\n",
      "119/223, train_loss: 0.1090, step time: 0.1024\n",
      "120/223, train_loss: 0.1009, step time: 0.1118\n",
      "121/223, train_loss: 0.1022, step time: 0.1289\n",
      "122/223, train_loss: 0.1092, step time: 0.1437\n",
      "123/223, train_loss: 0.1172, step time: 0.1111\n",
      "124/223, train_loss: 0.1014, step time: 0.1000\n",
      "125/223, train_loss: 0.1048, step time: 0.1006\n",
      "126/223, train_loss: 0.0957, step time: 0.1072\n",
      "127/223, train_loss: 0.1026, step time: 0.1006\n",
      "128/223, train_loss: 0.1059, step time: 0.0996\n",
      "129/223, train_loss: 0.1079, step time: 0.1752\n",
      "130/223, train_loss: 0.1105, step time: 0.1428\n",
      "131/223, train_loss: 0.1032, step time: 0.1142\n",
      "132/223, train_loss: 0.0965, step time: 0.1008\n",
      "133/223, train_loss: 0.1161, step time: 0.1067\n",
      "134/223, train_loss: 0.1073, step time: 0.1005\n",
      "135/223, train_loss: 0.1035, step time: 0.1158\n",
      "136/223, train_loss: 0.0976, step time: 0.1356\n",
      "137/223, train_loss: 0.1029, step time: 0.1260\n",
      "138/223, train_loss: 0.1048, step time: 0.1110\n",
      "139/223, train_loss: 0.1065, step time: 0.1330\n",
      "140/223, train_loss: 0.1064, step time: 0.1093\n",
      "141/223, train_loss: 0.0983, step time: 0.1006\n",
      "142/223, train_loss: 0.1001, step time: 0.1520\n",
      "143/223, train_loss: 0.1032, step time: 0.1003\n",
      "144/223, train_loss: 0.1100, step time: 0.0993\n",
      "145/223, train_loss: 0.1113, step time: 0.1222\n",
      "146/223, train_loss: 0.1084, step time: 0.1228\n",
      "147/223, train_loss: 0.1018, step time: 0.1006\n",
      "148/223, train_loss: 0.1117, step time: 0.1006\n",
      "149/223, train_loss: 0.1075, step time: 0.1100\n",
      "150/223, train_loss: 0.1003, step time: 0.1115\n",
      "151/223, train_loss: 0.1001, step time: 0.1011\n",
      "152/223, train_loss: 0.1206, step time: 0.0999\n",
      "153/223, train_loss: 0.1116, step time: 0.1184\n",
      "154/223, train_loss: 0.1036, step time: 0.1054\n",
      "155/223, train_loss: 0.1004, step time: 0.1176\n",
      "156/223, train_loss: 0.1000, step time: 0.1125\n",
      "157/223, train_loss: 0.3162, step time: 0.1140\n",
      "158/223, train_loss: 0.1076, step time: 0.1494\n",
      "159/223, train_loss: 0.0980, step time: 0.1069\n",
      "160/223, train_loss: 0.1156, step time: 0.1038\n",
      "161/223, train_loss: 0.1151, step time: 0.1136\n",
      "162/223, train_loss: 0.1065, step time: 0.1174\n",
      "163/223, train_loss: 0.1080, step time: 0.1457\n",
      "164/223, train_loss: 0.1002, step time: 0.1292\n",
      "165/223, train_loss: 0.1141, step time: 0.1164\n",
      "166/223, train_loss: 0.1126, step time: 0.1408\n",
      "167/223, train_loss: 0.1027, step time: 0.1111\n",
      "168/223, train_loss: 0.1057, step time: 0.1185\n",
      "169/223, train_loss: 0.0888, step time: 0.1324\n",
      "170/223, train_loss: 0.1074, step time: 0.1055\n",
      "171/223, train_loss: 0.1103, step time: 0.1083\n",
      "172/223, train_loss: 0.1176, step time: 0.1167\n",
      "173/223, train_loss: 0.1133, step time: 0.1263\n",
      "174/223, train_loss: 0.1040, step time: 0.1244\n",
      "175/223, train_loss: 0.1085, step time: 0.1183\n",
      "176/223, train_loss: 0.1015, step time: 0.1142\n",
      "177/223, train_loss: 0.1002, step time: 0.1015\n",
      "178/223, train_loss: 0.1097, step time: 0.0998\n",
      "179/223, train_loss: 0.1081, step time: 0.1006\n",
      "180/223, train_loss: 0.1101, step time: 0.1210\n",
      "181/223, train_loss: 0.0950, step time: 0.1102\n",
      "182/223, train_loss: 0.1154, step time: 0.1281\n",
      "183/223, train_loss: 0.1104, step time: 0.1290\n",
      "184/223, train_loss: 0.1123, step time: 0.1309\n",
      "185/223, train_loss: 0.1058, step time: 0.1000\n",
      "186/223, train_loss: 0.1071, step time: 0.1001\n",
      "187/223, train_loss: 0.1175, step time: 0.1009\n",
      "188/223, train_loss: 0.1102, step time: 0.1183\n",
      "189/223, train_loss: 0.1056, step time: 0.0998\n",
      "190/223, train_loss: 0.1051, step time: 0.1092\n",
      "191/223, train_loss: 0.1066, step time: 0.1128\n",
      "192/223, train_loss: 0.1008, step time: 0.1254\n",
      "193/223, train_loss: 0.0954, step time: 0.1055\n",
      "194/223, train_loss: 0.0978, step time: 0.1514\n",
      "195/223, train_loss: 0.1008, step time: 0.1235\n",
      "196/223, train_loss: 0.1108, step time: 0.1010\n",
      "197/223, train_loss: 0.1060, step time: 0.1258\n",
      "198/223, train_loss: 0.1010, step time: 0.1360\n",
      "199/223, train_loss: 0.1028, step time: 0.1112\n",
      "200/223, train_loss: 0.1043, step time: 0.1008\n",
      "201/223, train_loss: 0.1119, step time: 0.1048\n",
      "202/223, train_loss: 0.1086, step time: 0.0993\n",
      "203/223, train_loss: 0.1049, step time: 0.0999\n",
      "204/223, train_loss: 0.1114, step time: 0.1003\n",
      "205/223, train_loss: 0.1158, step time: 0.1146\n",
      "206/223, train_loss: 0.1170, step time: 0.1054\n",
      "207/223, train_loss: 0.0963, step time: 0.1156\n",
      "208/223, train_loss: 0.0960, step time: 0.1427\n",
      "209/223, train_loss: 0.1094, step time: 0.1075\n",
      "210/223, train_loss: 0.1052, step time: 0.0999\n",
      "211/223, train_loss: 0.1021, step time: 0.1194\n",
      "212/223, train_loss: 0.1139, step time: 0.1049\n",
      "213/223, train_loss: 0.1066, step time: 0.1083\n",
      "214/223, train_loss: 0.1032, step time: 0.0994\n",
      "215/223, train_loss: 0.1064, step time: 0.1686\n",
      "216/223, train_loss: 0.1005, step time: 0.1004\n",
      "217/223, train_loss: 0.1013, step time: 0.1062\n",
      "218/223, train_loss: 0.0975, step time: 0.0985\n",
      "219/223, train_loss: 0.1114, step time: 0.0988\n",
      "220/223, train_loss: 0.1155, step time: 0.0990\n",
      "221/223, train_loss: 0.1076, step time: 0.0997\n",
      "222/223, train_loss: 0.1039, step time: 0.0985\n",
      "223/223, train_loss: 0.1021, step time: 0.0993\n",
      "epoch 152 average loss: 0.1073\n",
      "time consuming of epoch 152 is: 90.8209\n",
      "----------\n",
      "epoch 153/300\n",
      "1/223, train_loss: 0.1040, step time: 0.1014\n",
      "2/223, train_loss: 0.1026, step time: 0.1020\n",
      "3/223, train_loss: 0.1013, step time: 0.1004\n",
      "4/223, train_loss: 0.1122, step time: 0.1052\n",
      "5/223, train_loss: 0.0982, step time: 0.1090\n",
      "6/223, train_loss: 0.1081, step time: 0.1123\n",
      "7/223, train_loss: 0.1128, step time: 0.1227\n",
      "8/223, train_loss: 0.1046, step time: 0.1070\n",
      "9/223, train_loss: 0.1111, step time: 0.1114\n",
      "10/223, train_loss: 0.1036, step time: 0.1176\n",
      "11/223, train_loss: 0.1054, step time: 0.1184\n",
      "12/223, train_loss: 0.1003, step time: 0.1307\n",
      "13/223, train_loss: 0.1181, step time: 0.1095\n",
      "14/223, train_loss: 0.1039, step time: 0.1152\n",
      "15/223, train_loss: 0.0980, step time: 0.1042\n",
      "16/223, train_loss: 0.0945, step time: 0.1151\n",
      "17/223, train_loss: 0.1039, step time: 0.1002\n",
      "18/223, train_loss: 0.1153, step time: 0.1046\n",
      "19/223, train_loss: 0.1098, step time: 0.1006\n",
      "20/223, train_loss: 0.1040, step time: 0.1013\n",
      "21/223, train_loss: 0.0956, step time: 0.1175\n",
      "22/223, train_loss: 0.0986, step time: 0.1005\n",
      "23/223, train_loss: 0.0929, step time: 0.1132\n",
      "24/223, train_loss: 0.1088, step time: 0.1009\n",
      "25/223, train_loss: 0.1022, step time: 0.0996\n",
      "26/223, train_loss: 0.0996, step time: 0.1009\n",
      "27/223, train_loss: 0.1106, step time: 0.1182\n",
      "28/223, train_loss: 0.0999, step time: 0.1010\n",
      "29/223, train_loss: 0.1008, step time: 0.1048\n",
      "30/223, train_loss: 0.1077, step time: 0.1104\n",
      "31/223, train_loss: 0.1008, step time: 0.1081\n",
      "32/223, train_loss: 0.1030, step time: 0.1044\n",
      "33/223, train_loss: 0.1003, step time: 0.1103\n",
      "34/223, train_loss: 0.1141, step time: 0.1005\n",
      "35/223, train_loss: 0.0988, step time: 0.1003\n",
      "36/223, train_loss: 0.1080, step time: 0.1008\n",
      "37/223, train_loss: 0.1072, step time: 0.1131\n",
      "38/223, train_loss: 0.1124, step time: 0.0993\n",
      "39/223, train_loss: 0.1104, step time: 0.1155\n",
      "40/223, train_loss: 0.1085, step time: 0.1116\n",
      "41/223, train_loss: 0.1190, step time: 0.1058\n",
      "42/223, train_loss: 0.1010, step time: 0.1000\n",
      "43/223, train_loss: 0.0970, step time: 0.1115\n",
      "44/223, train_loss: 0.0985, step time: 0.1065\n",
      "45/223, train_loss: 0.1053, step time: 0.0997\n",
      "46/223, train_loss: 0.1178, step time: 0.0999\n",
      "47/223, train_loss: 0.1058, step time: 0.1057\n",
      "48/223, train_loss: 0.0949, step time: 0.1167\n",
      "49/223, train_loss: 0.1076, step time: 0.1083\n",
      "50/223, train_loss: 0.0969, step time: 0.0995\n",
      "51/223, train_loss: 0.1047, step time: 0.1013\n",
      "52/223, train_loss: 0.0950, step time: 0.1003\n",
      "53/223, train_loss: 0.1054, step time: 0.1121\n",
      "54/223, train_loss: 0.1231, step time: 0.0999\n",
      "55/223, train_loss: 0.1072, step time: 0.1003\n",
      "56/223, train_loss: 0.1066, step time: 0.1087\n",
      "57/223, train_loss: 0.0983, step time: 0.1044\n",
      "58/223, train_loss: 0.1143, step time: 0.1138\n",
      "59/223, train_loss: 0.0992, step time: 0.1098\n",
      "60/223, train_loss: 0.1182, step time: 0.1049\n",
      "61/223, train_loss: 0.1000, step time: 0.1005\n",
      "62/223, train_loss: 0.0935, step time: 0.0995\n",
      "63/223, train_loss: 0.1099, step time: 0.1014\n",
      "64/223, train_loss: 0.1154, step time: 0.0998\n",
      "65/223, train_loss: 0.1094, step time: 0.1034\n",
      "66/223, train_loss: 0.1019, step time: 0.1046\n",
      "67/223, train_loss: 0.0983, step time: 0.1164\n",
      "68/223, train_loss: 0.1074, step time: 0.1065\n",
      "69/223, train_loss: 0.1133, step time: 0.1069\n",
      "70/223, train_loss: 0.1107, step time: 0.1227\n",
      "71/223, train_loss: 0.1034, step time: 0.1047\n",
      "72/223, train_loss: 0.1057, step time: 0.1258\n",
      "73/223, train_loss: 0.1050, step time: 0.1000\n",
      "74/223, train_loss: 0.1076, step time: 0.1100\n",
      "75/223, train_loss: 0.1058, step time: 0.1081\n",
      "76/223, train_loss: 0.1069, step time: 0.1005\n",
      "77/223, train_loss: 0.1086, step time: 0.1041\n",
      "78/223, train_loss: 0.1124, step time: 0.1041\n",
      "79/223, train_loss: 0.1099, step time: 0.1075\n",
      "80/223, train_loss: 0.1167, step time: 0.1139\n",
      "81/223, train_loss: 0.1000, step time: 0.1051\n",
      "82/223, train_loss: 0.0970, step time: 0.0997\n",
      "83/223, train_loss: 0.1028, step time: 0.0999\n",
      "84/223, train_loss: 0.1019, step time: 0.1006\n",
      "85/223, train_loss: 0.1160, step time: 0.0995\n",
      "86/223, train_loss: 0.0998, step time: 0.1211\n",
      "87/223, train_loss: 0.1078, step time: 0.1071\n",
      "88/223, train_loss: 0.1135, step time: 0.1040\n",
      "89/223, train_loss: 0.1118, step time: 0.1207\n",
      "90/223, train_loss: 0.1016, step time: 0.1107\n",
      "91/223, train_loss: 0.1067, step time: 0.1062\n",
      "92/223, train_loss: 0.1016, step time: 0.1163\n",
      "93/223, train_loss: 0.1045, step time: 0.1147\n",
      "94/223, train_loss: 0.1002, step time: 0.1053\n",
      "95/223, train_loss: 0.0976, step time: 0.1147\n",
      "96/223, train_loss: 0.1110, step time: 0.1089\n",
      "97/223, train_loss: 0.1220, step time: 0.1083\n",
      "98/223, train_loss: 0.1153, step time: 0.1030\n",
      "99/223, train_loss: 0.0941, step time: 0.1140\n",
      "100/223, train_loss: 0.1145, step time: 0.1223\n",
      "101/223, train_loss: 0.1148, step time: 0.1070\n",
      "102/223, train_loss: 0.1091, step time: 0.1003\n",
      "103/223, train_loss: 0.1032, step time: 0.1014\n",
      "104/223, train_loss: 0.1044, step time: 0.1014\n",
      "105/223, train_loss: 0.1066, step time: 0.1052\n",
      "106/223, train_loss: 0.1155, step time: 0.1003\n",
      "107/223, train_loss: 0.0965, step time: 0.1052\n",
      "108/223, train_loss: 0.1109, step time: 0.1005\n",
      "109/223, train_loss: 0.1073, step time: 0.1080\n",
      "110/223, train_loss: 0.1085, step time: 0.0999\n",
      "111/223, train_loss: 0.1031, step time: 0.1074\n",
      "112/223, train_loss: 0.1163, step time: 0.1225\n",
      "113/223, train_loss: 0.1066, step time: 0.1180\n",
      "114/223, train_loss: 0.1107, step time: 0.1094\n",
      "115/223, train_loss: 0.0932, step time: 0.1182\n",
      "116/223, train_loss: 0.1070, step time: 0.1135\n",
      "117/223, train_loss: 0.1010, step time: 0.1040\n",
      "118/223, train_loss: 0.1070, step time: 0.1005\n",
      "119/223, train_loss: 0.0954, step time: 0.1012\n",
      "120/223, train_loss: 0.1138, step time: 0.1002\n",
      "121/223, train_loss: 0.1008, step time: 0.0997\n",
      "122/223, train_loss: 0.1072, step time: 0.0997\n",
      "123/223, train_loss: 0.1111, step time: 0.1072\n",
      "124/223, train_loss: 0.1075, step time: 0.1102\n",
      "125/223, train_loss: 0.1090, step time: 0.1153\n",
      "126/223, train_loss: 0.1104, step time: 0.1058\n",
      "127/223, train_loss: 0.1093, step time: 0.1246\n",
      "128/223, train_loss: 0.1037, step time: 0.1152\n",
      "129/223, train_loss: 0.0967, step time: 0.1012\n",
      "130/223, train_loss: 0.1150, step time: 0.1060\n",
      "131/223, train_loss: 0.1141, step time: 0.1107\n",
      "132/223, train_loss: 0.1037, step time: 0.1387\n",
      "133/223, train_loss: 0.1078, step time: 0.0999\n",
      "134/223, train_loss: 0.1037, step time: 0.1163\n",
      "135/223, train_loss: 0.1137, step time: 0.1097\n",
      "136/223, train_loss: 0.0984, step time: 0.1051\n",
      "137/223, train_loss: 0.1143, step time: 0.1206\n",
      "138/223, train_loss: 0.1045, step time: 0.1132\n",
      "139/223, train_loss: 0.1111, step time: 0.1042\n",
      "140/223, train_loss: 0.1035, step time: 0.1151\n",
      "141/223, train_loss: 0.1062, step time: 0.1118\n",
      "142/223, train_loss: 0.1145, step time: 0.1089\n",
      "143/223, train_loss: 0.1142, step time: 0.1103\n",
      "144/223, train_loss: 0.1285, step time: 0.1129\n",
      "145/223, train_loss: 0.1073, step time: 0.1066\n",
      "146/223, train_loss: 0.1121, step time: 0.1000\n",
      "147/223, train_loss: 0.0958, step time: 0.0995\n",
      "148/223, train_loss: 0.1027, step time: 0.1206\n",
      "149/223, train_loss: 0.1199, step time: 0.1085\n",
      "150/223, train_loss: 0.1240, step time: 0.1202\n",
      "151/223, train_loss: 0.0992, step time: 0.1010\n",
      "152/223, train_loss: 0.1032, step time: 0.1178\n",
      "153/223, train_loss: 0.1108, step time: 0.1040\n",
      "154/223, train_loss: 0.1091, step time: 0.1128\n",
      "155/223, train_loss: 0.1051, step time: 0.1073\n",
      "156/223, train_loss: 0.1019, step time: 0.1103\n",
      "157/223, train_loss: 0.1145, step time: 0.0993\n",
      "158/223, train_loss: 0.0985, step time: 0.1002\n",
      "159/223, train_loss: 0.1063, step time: 0.1006\n",
      "160/223, train_loss: 0.1104, step time: 0.1313\n",
      "161/223, train_loss: 0.1277, step time: 0.1089\n",
      "162/223, train_loss: 0.1232, step time: 0.1004\n",
      "163/223, train_loss: 0.0956, step time: 0.1002\n",
      "164/223, train_loss: 0.1283, step time: 0.1170\n",
      "165/223, train_loss: 0.1254, step time: 0.1071\n",
      "166/223, train_loss: 0.0999, step time: 0.1001\n",
      "167/223, train_loss: 0.1133, step time: 0.1053\n",
      "168/223, train_loss: 0.1295, step time: 0.1178\n",
      "169/223, train_loss: 0.1032, step time: 0.1125\n",
      "170/223, train_loss: 0.1020, step time: 0.1019\n",
      "171/223, train_loss: 0.1169, step time: 0.1000\n",
      "172/223, train_loss: 0.1109, step time: 0.1263\n",
      "173/223, train_loss: 0.1261, step time: 0.1229\n",
      "174/223, train_loss: 0.1054, step time: 0.1081\n",
      "175/223, train_loss: 0.1144, step time: 0.1000\n",
      "176/223, train_loss: 0.1188, step time: 0.1051\n",
      "177/223, train_loss: 0.1139, step time: 0.1193\n",
      "178/223, train_loss: 0.1109, step time: 0.1169\n",
      "179/223, train_loss: 0.1120, step time: 0.1013\n",
      "180/223, train_loss: 0.1217, step time: 0.1178\n",
      "181/223, train_loss: 0.1046, step time: 0.1052\n",
      "182/223, train_loss: 0.1303, step time: 0.1098\n",
      "183/223, train_loss: 0.0987, step time: 0.1010\n",
      "184/223, train_loss: 0.1115, step time: 0.1005\n",
      "185/223, train_loss: 0.1216, step time: 0.1074\n",
      "186/223, train_loss: 0.1079, step time: 0.1099\n",
      "187/223, train_loss: 0.1141, step time: 0.0998\n",
      "188/223, train_loss: 0.1111, step time: 0.1079\n",
      "189/223, train_loss: 0.1167, step time: 0.1002\n",
      "190/223, train_loss: 0.1013, step time: 0.1116\n",
      "191/223, train_loss: 0.1172, step time: 0.1003\n",
      "192/223, train_loss: 0.1023, step time: 0.1000\n",
      "193/223, train_loss: 0.1175, step time: 0.1054\n",
      "194/223, train_loss: 0.1142, step time: 0.1037\n",
      "195/223, train_loss: 0.1072, step time: 0.1113\n",
      "196/223, train_loss: 0.1107, step time: 0.1061\n",
      "197/223, train_loss: 0.1047, step time: 0.1118\n",
      "198/223, train_loss: 0.1076, step time: 0.1150\n",
      "199/223, train_loss: 0.1118, step time: 0.1004\n",
      "200/223, train_loss: 0.1108, step time: 0.1001\n",
      "201/223, train_loss: 0.1082, step time: 0.0997\n",
      "202/223, train_loss: 0.1037, step time: 0.0997\n",
      "203/223, train_loss: 0.1115, step time: 0.1011\n",
      "204/223, train_loss: 0.1053, step time: 0.0998\n",
      "205/223, train_loss: 0.1242, step time: 0.1092\n",
      "206/223, train_loss: 0.1098, step time: 0.1002\n",
      "207/223, train_loss: 0.1000, step time: 0.1001\n",
      "208/223, train_loss: 0.1061, step time: 0.1003\n",
      "209/223, train_loss: 0.1122, step time: 0.1030\n",
      "210/223, train_loss: 0.1047, step time: 0.1038\n",
      "211/223, train_loss: 0.1171, step time: 0.1003\n",
      "212/223, train_loss: 0.3149, step time: 0.1005\n",
      "213/223, train_loss: 0.0996, step time: 0.1099\n",
      "214/223, train_loss: 0.1133, step time: 0.1101\n",
      "215/223, train_loss: 0.0993, step time: 0.1048\n",
      "216/223, train_loss: 0.0949, step time: 0.1102\n",
      "217/223, train_loss: 0.1205, step time: 0.0998\n",
      "218/223, train_loss: 0.1053, step time: 0.0999\n",
      "219/223, train_loss: 0.1195, step time: 0.0995\n",
      "220/223, train_loss: 0.1150, step time: 0.0994\n",
      "221/223, train_loss: 0.1044, step time: 0.1002\n",
      "222/223, train_loss: 0.1092, step time: 0.1007\n",
      "223/223, train_loss: 0.1009, step time: 0.1005\n",
      "epoch 153 average loss: 0.1087\n",
      "time consuming of epoch 153 is: 83.8812\n",
      "----------\n",
      "epoch 154/300\n",
      "1/223, train_loss: 0.1037, step time: 0.1017\n",
      "2/223, train_loss: 0.1090, step time: 0.1002\n",
      "3/223, train_loss: 0.1010, step time: 0.1084\n",
      "4/223, train_loss: 0.1139, step time: 0.1011\n",
      "5/223, train_loss: 0.1034, step time: 0.0995\n",
      "6/223, train_loss: 0.0985, step time: 0.1121\n",
      "7/223, train_loss: 0.1131, step time: 0.1422\n",
      "8/223, train_loss: 0.1050, step time: 0.1038\n",
      "9/223, train_loss: 0.0947, step time: 0.1220\n",
      "10/223, train_loss: 0.1107, step time: 0.1089\n",
      "11/223, train_loss: 0.1132, step time: 0.0997\n",
      "12/223, train_loss: 0.1138, step time: 0.1071\n",
      "13/223, train_loss: 0.1053, step time: 0.1191\n",
      "14/223, train_loss: 0.1237, step time: 0.1140\n",
      "15/223, train_loss: 0.1138, step time: 0.0998\n",
      "16/223, train_loss: 0.1007, step time: 0.1080\n",
      "17/223, train_loss: 0.1090, step time: 0.1167\n",
      "18/223, train_loss: 0.1168, step time: 0.1175\n",
      "19/223, train_loss: 0.1048, step time: 0.1077\n",
      "20/223, train_loss: 0.1111, step time: 0.1165\n",
      "21/223, train_loss: 0.1134, step time: 0.1110\n",
      "22/223, train_loss: 0.1024, step time: 0.1055\n",
      "23/223, train_loss: 0.1011, step time: 0.1176\n",
      "24/223, train_loss: 0.1146, step time: 0.1111\n",
      "25/223, train_loss: 0.0928, step time: 0.1103\n",
      "26/223, train_loss: 0.1128, step time: 0.1041\n",
      "27/223, train_loss: 0.1175, step time: 0.1072\n",
      "28/223, train_loss: 0.1056, step time: 0.1022\n",
      "29/223, train_loss: 0.1138, step time: 0.1561\n",
      "30/223, train_loss: 0.1208, step time: 0.1409\n",
      "31/223, train_loss: 0.1026, step time: 0.1003\n",
      "32/223, train_loss: 0.1225, step time: 0.1376\n",
      "33/223, train_loss: 0.0958, step time: 0.1041\n",
      "34/223, train_loss: 0.1190, step time: 0.1012\n",
      "35/223, train_loss: 0.1051, step time: 0.0997\n",
      "36/223, train_loss: 0.1056, step time: 0.1046\n",
      "37/223, train_loss: 0.1081, step time: 0.1023\n",
      "38/223, train_loss: 0.1100, step time: 0.1004\n",
      "39/223, train_loss: 0.1018, step time: 0.1004\n",
      "40/223, train_loss: 0.0989, step time: 0.1232\n",
      "41/223, train_loss: 0.0997, step time: 0.1006\n",
      "42/223, train_loss: 0.0987, step time: 0.1324\n",
      "43/223, train_loss: 0.1081, step time: 0.1010\n",
      "44/223, train_loss: 0.1025, step time: 0.1110\n",
      "45/223, train_loss: 0.1113, step time: 0.1001\n",
      "46/223, train_loss: 0.1079, step time: 0.1159\n",
      "47/223, train_loss: 0.1224, step time: 0.1030\n",
      "48/223, train_loss: 0.1089, step time: 0.1178\n",
      "49/223, train_loss: 0.1095, step time: 0.1212\n",
      "50/223, train_loss: 0.0959, step time: 0.1393\n",
      "51/223, train_loss: 0.1020, step time: 0.1176\n",
      "52/223, train_loss: 0.1059, step time: 0.1197\n",
      "53/223, train_loss: 0.1140, step time: 0.0993\n",
      "54/223, train_loss: 0.0956, step time: 0.1097\n",
      "55/223, train_loss: 0.1024, step time: 0.1001\n",
      "56/223, train_loss: 0.1008, step time: 0.1211\n",
      "57/223, train_loss: 0.1083, step time: 0.1080\n",
      "58/223, train_loss: 0.1131, step time: 0.1019\n",
      "59/223, train_loss: 0.1186, step time: 0.1025\n",
      "60/223, train_loss: 0.1116, step time: 0.1015\n",
      "61/223, train_loss: 0.1025, step time: 0.1034\n",
      "62/223, train_loss: 0.1050, step time: 0.1140\n",
      "63/223, train_loss: 0.1122, step time: 0.1171\n",
      "64/223, train_loss: 0.0958, step time: 0.1091\n",
      "65/223, train_loss: 0.1073, step time: 0.0998\n",
      "66/223, train_loss: 0.1055, step time: 0.1149\n",
      "67/223, train_loss: 0.1032, step time: 0.1177\n",
      "68/223, train_loss: 0.1106, step time: 0.1001\n",
      "69/223, train_loss: 0.1007, step time: 0.0995\n",
      "70/223, train_loss: 0.1075, step time: 0.1156\n",
      "71/223, train_loss: 0.1147, step time: 0.1416\n",
      "72/223, train_loss: 0.1151, step time: 0.1039\n",
      "73/223, train_loss: 0.1107, step time: 0.1090\n",
      "74/223, train_loss: 0.1073, step time: 0.1090\n",
      "75/223, train_loss: 0.1152, step time: 0.1083\n",
      "76/223, train_loss: 0.1170, step time: 0.1248\n",
      "77/223, train_loss: 0.1073, step time: 0.1053\n",
      "78/223, train_loss: 0.1038, step time: 0.1262\n",
      "79/223, train_loss: 0.1159, step time: 0.1111\n",
      "80/223, train_loss: 0.1053, step time: 0.1011\n",
      "81/223, train_loss: 0.1126, step time: 0.1000\n",
      "82/223, train_loss: 0.1032, step time: 0.1367\n",
      "83/223, train_loss: 0.1105, step time: 0.1209\n",
      "84/223, train_loss: 0.1048, step time: 0.1002\n",
      "85/223, train_loss: 0.1081, step time: 0.0997\n",
      "86/223, train_loss: 0.1202, step time: 0.1878\n",
      "87/223, train_loss: 0.0880, step time: 0.1007\n",
      "88/223, train_loss: 0.1034, step time: 0.1008\n",
      "89/223, train_loss: 0.1092, step time: 0.1006\n",
      "90/223, train_loss: 0.1093, step time: 0.1114\n",
      "91/223, train_loss: 0.1041, step time: 0.1229\n",
      "92/223, train_loss: 0.0976, step time: 0.1105\n",
      "93/223, train_loss: 0.1034, step time: 0.1136\n",
      "94/223, train_loss: 0.1046, step time: 0.0995\n",
      "95/223, train_loss: 0.1076, step time: 0.1005\n",
      "96/223, train_loss: 0.0960, step time: 0.1013\n",
      "97/223, train_loss: 0.1138, step time: 0.1069\n",
      "98/223, train_loss: 0.1185, step time: 0.1096\n",
      "99/223, train_loss: 0.1015, step time: 0.1161\n",
      "100/223, train_loss: 0.1090, step time: 0.1160\n",
      "101/223, train_loss: 0.1042, step time: 0.1043\n",
      "102/223, train_loss: 0.1045, step time: 0.1325\n",
      "103/223, train_loss: 0.1042, step time: 0.1526\n",
      "104/223, train_loss: 0.0953, step time: 0.1157\n",
      "105/223, train_loss: 0.1217, step time: 0.1001\n",
      "106/223, train_loss: 0.1015, step time: 0.1092\n",
      "107/223, train_loss: 0.1091, step time: 0.1222\n",
      "108/223, train_loss: 0.1088, step time: 0.1061\n",
      "109/223, train_loss: 0.1070, step time: 0.1001\n",
      "110/223, train_loss: 0.1230, step time: 0.1378\n",
      "111/223, train_loss: 0.0977, step time: 0.1078\n",
      "112/223, train_loss: 0.0977, step time: 0.1067\n",
      "113/223, train_loss: 0.1001, step time: 0.1011\n",
      "114/223, train_loss: 0.1100, step time: 0.1310\n",
      "115/223, train_loss: 0.1059, step time: 0.1242\n",
      "116/223, train_loss: 0.1213, step time: 0.1050\n",
      "117/223, train_loss: 0.1135, step time: 0.1129\n",
      "118/223, train_loss: 0.1008, step time: 0.1022\n",
      "119/223, train_loss: 0.1140, step time: 0.1003\n",
      "120/223, train_loss: 0.1066, step time: 0.1090\n",
      "121/223, train_loss: 0.1150, step time: 0.1000\n",
      "122/223, train_loss: 0.1116, step time: 0.1102\n",
      "123/223, train_loss: 0.1060, step time: 0.1049\n",
      "124/223, train_loss: 0.0934, step time: 0.1013\n",
      "125/223, train_loss: 0.1170, step time: 0.1009\n",
      "126/223, train_loss: 0.1075, step time: 0.1183\n",
      "127/223, train_loss: 0.1085, step time: 0.1012\n",
      "128/223, train_loss: 0.1003, step time: 0.0993\n",
      "129/223, train_loss: 0.1167, step time: 0.1015\n",
      "130/223, train_loss: 0.1009, step time: 0.1328\n",
      "131/223, train_loss: 0.0954, step time: 0.1116\n",
      "132/223, train_loss: 0.1044, step time: 0.1082\n",
      "133/223, train_loss: 0.1064, step time: 0.1090\n",
      "134/223, train_loss: 0.1042, step time: 0.1060\n",
      "135/223, train_loss: 0.1011, step time: 0.1105\n",
      "136/223, train_loss: 0.1100, step time: 0.1003\n",
      "137/223, train_loss: 0.1084, step time: 0.1014\n",
      "138/223, train_loss: 0.1043, step time: 0.1209\n",
      "139/223, train_loss: 0.1042, step time: 0.1353\n",
      "140/223, train_loss: 0.1106, step time: 0.1189\n",
      "141/223, train_loss: 0.1043, step time: 0.1013\n",
      "142/223, train_loss: 0.0983, step time: 0.1228\n",
      "143/223, train_loss: 0.1035, step time: 0.1155\n",
      "144/223, train_loss: 0.1131, step time: 0.1041\n",
      "145/223, train_loss: 0.1044, step time: 0.1032\n",
      "146/223, train_loss: 0.0988, step time: 0.1124\n",
      "147/223, train_loss: 0.1040, step time: 0.1222\n",
      "148/223, train_loss: 0.1000, step time: 0.1005\n",
      "149/223, train_loss: 0.3029, step time: 0.1410\n",
      "150/223, train_loss: 0.1077, step time: 0.1073\n",
      "151/223, train_loss: 0.1007, step time: 0.1133\n",
      "152/223, train_loss: 0.0967, step time: 0.1340\n",
      "153/223, train_loss: 0.0998, step time: 0.1085\n",
      "154/223, train_loss: 0.1076, step time: 0.1185\n",
      "155/223, train_loss: 0.1109, step time: 0.1150\n",
      "156/223, train_loss: 0.1158, step time: 0.1003\n",
      "157/223, train_loss: 0.0930, step time: 0.1000\n",
      "158/223, train_loss: 0.1023, step time: 0.1387\n",
      "159/223, train_loss: 0.1011, step time: 0.1400\n",
      "160/223, train_loss: 0.1042, step time: 0.1123\n",
      "161/223, train_loss: 0.1015, step time: 0.1080\n",
      "162/223, train_loss: 0.1078, step time: 0.1181\n",
      "163/223, train_loss: 0.1025, step time: 0.1208\n",
      "164/223, train_loss: 0.0984, step time: 0.1252\n",
      "165/223, train_loss: 0.1009, step time: 0.1001\n",
      "166/223, train_loss: 0.0984, step time: 0.0987\n",
      "167/223, train_loss: 0.0957, step time: 0.0985\n",
      "168/223, train_loss: 0.0957, step time: 0.0997\n",
      "169/223, train_loss: 0.0948, step time: 0.0992\n",
      "170/223, train_loss: 0.1013, step time: 0.0995\n",
      "171/223, train_loss: 0.1047, step time: 0.0999\n",
      "172/223, train_loss: 0.1290, step time: 0.0998\n",
      "173/223, train_loss: 0.1149, step time: 0.1003\n",
      "174/223, train_loss: 0.1117, step time: 0.0999\n",
      "175/223, train_loss: 0.0958, step time: 0.0998\n",
      "176/223, train_loss: 0.1001, step time: 0.0998\n",
      "177/223, train_loss: 0.1083, step time: 0.1109\n",
      "178/223, train_loss: 0.0971, step time: 0.0990\n",
      "179/223, train_loss: 0.1069, step time: 0.1002\n",
      "180/223, train_loss: 0.1173, step time: 0.1010\n",
      "181/223, train_loss: 0.1006, step time: 0.1111\n",
      "182/223, train_loss: 0.1097, step time: 0.1117\n",
      "183/223, train_loss: 0.1019, step time: 0.1153\n",
      "184/223, train_loss: 0.1129, step time: 0.1262\n",
      "185/223, train_loss: 0.1043, step time: 0.1118\n",
      "186/223, train_loss: 0.0986, step time: 0.1067\n",
      "187/223, train_loss: 0.1158, step time: 0.1094\n",
      "188/223, train_loss: 0.1114, step time: 0.1203\n",
      "189/223, train_loss: 0.1061, step time: 0.1145\n",
      "190/223, train_loss: 0.1042, step time: 0.1211\n",
      "191/223, train_loss: 0.1103, step time: 0.1429\n",
      "192/223, train_loss: 0.1105, step time: 0.0999\n",
      "193/223, train_loss: 0.1059, step time: 0.1065\n",
      "194/223, train_loss: 0.1011, step time: 0.1334\n",
      "195/223, train_loss: 0.1045, step time: 0.1023\n",
      "196/223, train_loss: 0.1101, step time: 0.1144\n",
      "197/223, train_loss: 0.1056, step time: 0.0998\n",
      "198/223, train_loss: 0.0954, step time: 0.1144\n",
      "199/223, train_loss: 0.1042, step time: 0.1075\n",
      "200/223, train_loss: 0.1132, step time: 0.1178\n",
      "201/223, train_loss: 0.1103, step time: 0.1006\n",
      "202/223, train_loss: 0.1037, step time: 0.1065\n",
      "203/223, train_loss: 0.0952, step time: 0.1088\n",
      "204/223, train_loss: 0.1041, step time: 0.1101\n",
      "205/223, train_loss: 0.1092, step time: 0.1195\n",
      "206/223, train_loss: 0.0922, step time: 0.1075\n",
      "207/223, train_loss: 0.0985, step time: 0.1161\n",
      "208/223, train_loss: 0.0929, step time: 0.1000\n",
      "209/223, train_loss: 0.1029, step time: 0.1004\n",
      "210/223, train_loss: 0.1236, step time: 0.1108\n",
      "211/223, train_loss: 0.1076, step time: 0.1139\n",
      "212/223, train_loss: 0.1167, step time: 0.0994\n",
      "213/223, train_loss: 0.0979, step time: 0.1056\n",
      "214/223, train_loss: 0.0997, step time: 0.1347\n",
      "215/223, train_loss: 0.0991, step time: 0.1120\n",
      "216/223, train_loss: 0.1000, step time: 0.1199\n",
      "217/223, train_loss: 0.1013, step time: 0.1004\n",
      "218/223, train_loss: 0.1066, step time: 0.1001\n",
      "219/223, train_loss: 0.1039, step time: 0.1011\n",
      "220/223, train_loss: 0.1011, step time: 0.0988\n",
      "221/223, train_loss: 0.1118, step time: 0.1000\n",
      "222/223, train_loss: 0.1092, step time: 0.0994\n",
      "223/223, train_loss: 0.1024, step time: 0.0997\n",
      "epoch 154 average loss: 0.1072\n",
      "time consuming of epoch 154 is: 91.3301\n",
      "----------\n",
      "epoch 155/300\n",
      "1/223, train_loss: 0.0940, step time: 0.1014\n",
      "2/223, train_loss: 0.1033, step time: 0.1012\n",
      "3/223, train_loss: 0.0955, step time: 0.1002\n",
      "4/223, train_loss: 0.0957, step time: 0.1072\n",
      "5/223, train_loss: 0.1074, step time: 0.1097\n",
      "6/223, train_loss: 0.0904, step time: 0.1087\n",
      "7/223, train_loss: 0.1123, step time: 0.1007\n",
      "8/223, train_loss: 0.0954, step time: 0.1030\n",
      "9/223, train_loss: 0.1023, step time: 0.1009\n",
      "10/223, train_loss: 0.1046, step time: 0.1007\n",
      "11/223, train_loss: 0.1026, step time: 0.1003\n",
      "12/223, train_loss: 0.1103, step time: 0.1000\n",
      "13/223, train_loss: 0.0916, step time: 0.1309\n",
      "14/223, train_loss: 0.1047, step time: 0.1350\n",
      "15/223, train_loss: 0.1040, step time: 0.1007\n",
      "16/223, train_loss: 0.1104, step time: 0.0993\n",
      "17/223, train_loss: 0.1127, step time: 0.1000\n",
      "18/223, train_loss: 0.1229, step time: 0.1005\n",
      "19/223, train_loss: 0.0936, step time: 0.1050\n",
      "20/223, train_loss: 0.0944, step time: 0.1004\n",
      "21/223, train_loss: 0.1073, step time: 0.1039\n",
      "22/223, train_loss: 0.1101, step time: 0.0996\n",
      "23/223, train_loss: 0.1004, step time: 0.1004\n",
      "24/223, train_loss: 0.1031, step time: 0.1096\n",
      "25/223, train_loss: 0.1025, step time: 0.1000\n",
      "26/223, train_loss: 0.0992, step time: 0.1004\n",
      "27/223, train_loss: 0.1127, step time: 0.1006\n",
      "28/223, train_loss: 0.1089, step time: 0.1009\n",
      "29/223, train_loss: 0.1085, step time: 0.1011\n",
      "30/223, train_loss: 0.1028, step time: 0.1062\n",
      "31/223, train_loss: 0.1124, step time: 0.1006\n",
      "32/223, train_loss: 0.0956, step time: 0.1242\n",
      "33/223, train_loss: 0.1190, step time: 0.1100\n",
      "34/223, train_loss: 0.1096, step time: 0.1071\n",
      "35/223, train_loss: 0.1033, step time: 0.1287\n",
      "36/223, train_loss: 0.0982, step time: 0.1254\n",
      "37/223, train_loss: 0.0961, step time: 0.1089\n",
      "38/223, train_loss: 0.1137, step time: 0.1143\n",
      "39/223, train_loss: 0.0982, step time: 0.1083\n",
      "40/223, train_loss: 0.0985, step time: 0.1068\n",
      "41/223, train_loss: 0.0965, step time: 0.1108\n",
      "42/223, train_loss: 0.0982, step time: 0.1149\n",
      "43/223, train_loss: 0.1018, step time: 0.1164\n",
      "44/223, train_loss: 0.1185, step time: 0.1044\n",
      "45/223, train_loss: 0.1135, step time: 0.1026\n",
      "46/223, train_loss: 0.1017, step time: 0.1054\n",
      "47/223, train_loss: 0.1011, step time: 0.1027\n",
      "48/223, train_loss: 0.1218, step time: 0.1005\n",
      "49/223, train_loss: 0.1155, step time: 0.1131\n",
      "50/223, train_loss: 0.1084, step time: 0.1052\n",
      "51/223, train_loss: 0.1145, step time: 0.1150\n",
      "52/223, train_loss: 0.1005, step time: 0.1169\n",
      "53/223, train_loss: 0.1125, step time: 0.1058\n",
      "54/223, train_loss: 0.1067, step time: 0.1125\n",
      "55/223, train_loss: 0.1188, step time: 0.1077\n",
      "56/223, train_loss: 0.0985, step time: 0.1158\n",
      "57/223, train_loss: 0.0921, step time: 0.1104\n",
      "58/223, train_loss: 0.0976, step time: 0.0995\n",
      "59/223, train_loss: 0.1083, step time: 0.0995\n",
      "60/223, train_loss: 0.1139, step time: 0.1235\n",
      "61/223, train_loss: 0.1151, step time: 0.1023\n",
      "62/223, train_loss: 0.1158, step time: 0.1139\n",
      "63/223, train_loss: 0.1042, step time: 0.1260\n",
      "64/223, train_loss: 0.1161, step time: 0.1153\n",
      "65/223, train_loss: 0.0938, step time: 0.1049\n",
      "66/223, train_loss: 0.1256, step time: 0.1109\n",
      "67/223, train_loss: 0.1247, step time: 0.1604\n",
      "68/223, train_loss: 0.1069, step time: 0.1117\n",
      "69/223, train_loss: 0.1140, step time: 0.1117\n",
      "70/223, train_loss: 0.1018, step time: 0.1183\n",
      "71/223, train_loss: 0.1114, step time: 0.0996\n",
      "72/223, train_loss: 0.1046, step time: 0.1030\n",
      "73/223, train_loss: 0.1055, step time: 0.1309\n",
      "74/223, train_loss: 0.0954, step time: 0.1234\n",
      "75/223, train_loss: 0.0944, step time: 0.1270\n",
      "76/223, train_loss: 0.1011, step time: 0.1019\n",
      "77/223, train_loss: 0.1094, step time: 0.1072\n",
      "78/223, train_loss: 0.1052, step time: 0.1011\n",
      "79/223, train_loss: 0.1098, step time: 0.1593\n",
      "80/223, train_loss: 0.1147, step time: 0.1238\n",
      "81/223, train_loss: 0.1062, step time: 0.1000\n",
      "82/223, train_loss: 0.0984, step time: 0.1230\n",
      "83/223, train_loss: 0.1208, step time: 0.1173\n",
      "84/223, train_loss: 0.1084, step time: 0.1115\n",
      "85/223, train_loss: 0.1091, step time: 0.1023\n",
      "86/223, train_loss: 0.1048, step time: 0.1177\n",
      "87/223, train_loss: 0.0973, step time: 0.1192\n",
      "88/223, train_loss: 0.1139, step time: 0.1198\n",
      "89/223, train_loss: 0.1084, step time: 0.1041\n",
      "90/223, train_loss: 0.1013, step time: 0.1102\n",
      "91/223, train_loss: 0.1073, step time: 0.1005\n",
      "92/223, train_loss: 0.1089, step time: 0.1173\n",
      "93/223, train_loss: 0.1079, step time: 0.1166\n",
      "94/223, train_loss: 0.1145, step time: 0.1157\n",
      "95/223, train_loss: 0.1020, step time: 0.1006\n",
      "96/223, train_loss: 0.1075, step time: 0.1072\n",
      "97/223, train_loss: 0.0968, step time: 0.1182\n",
      "98/223, train_loss: 0.0996, step time: 0.1057\n",
      "99/223, train_loss: 0.1098, step time: 0.1520\n",
      "100/223, train_loss: 0.1106, step time: 0.1436\n",
      "101/223, train_loss: 0.1008, step time: 0.0999\n",
      "102/223, train_loss: 0.1044, step time: 0.1126\n",
      "103/223, train_loss: 0.1034, step time: 0.1130\n",
      "104/223, train_loss: 0.0955, step time: 0.1030\n",
      "105/223, train_loss: 0.1027, step time: 0.1109\n",
      "106/223, train_loss: 0.1065, step time: 0.1164\n",
      "107/223, train_loss: 0.1006, step time: 0.1290\n",
      "108/223, train_loss: 0.1077, step time: 0.1012\n",
      "109/223, train_loss: 0.0989, step time: 0.1198\n",
      "110/223, train_loss: 0.0977, step time: 0.1000\n",
      "111/223, train_loss: 0.1063, step time: 0.1010\n",
      "112/223, train_loss: 0.0977, step time: 0.1171\n",
      "113/223, train_loss: 0.1018, step time: 0.1036\n",
      "114/223, train_loss: 0.0955, step time: 0.1118\n",
      "115/223, train_loss: 0.0971, step time: 0.1188\n",
      "116/223, train_loss: 0.1034, step time: 0.1091\n",
      "117/223, train_loss: 0.1032, step time: 0.1089\n",
      "118/223, train_loss: 0.0984, step time: 0.1291\n",
      "119/223, train_loss: 0.1111, step time: 0.1260\n",
      "120/223, train_loss: 0.1072, step time: 0.1285\n",
      "121/223, train_loss: 0.0944, step time: 0.1159\n",
      "122/223, train_loss: 0.1003, step time: 0.1285\n",
      "123/223, train_loss: 0.0954, step time: 0.1398\n",
      "124/223, train_loss: 0.0979, step time: 0.1006\n",
      "125/223, train_loss: 0.1108, step time: 0.1006\n",
      "126/223, train_loss: 0.1085, step time: 0.1002\n",
      "127/223, train_loss: 0.1082, step time: 0.1195\n",
      "128/223, train_loss: 0.1253, step time: 0.1044\n",
      "129/223, train_loss: 0.1064, step time: 0.1007\n",
      "130/223, train_loss: 0.2986, step time: 0.1004\n",
      "131/223, train_loss: 0.0987, step time: 0.0997\n",
      "132/223, train_loss: 0.0913, step time: 0.1008\n",
      "133/223, train_loss: 0.1062, step time: 0.1100\n",
      "134/223, train_loss: 0.0982, step time: 0.1029\n",
      "135/223, train_loss: 0.1035, step time: 0.1006\n",
      "136/223, train_loss: 0.0999, step time: 0.0998\n",
      "137/223, train_loss: 0.0968, step time: 0.1064\n",
      "138/223, train_loss: 0.1023, step time: 0.1102\n",
      "139/223, train_loss: 0.1014, step time: 0.1073\n",
      "140/223, train_loss: 0.1000, step time: 0.1002\n",
      "141/223, train_loss: 0.1121, step time: 0.1007\n",
      "142/223, train_loss: 0.1074, step time: 0.1010\n",
      "143/223, train_loss: 0.1118, step time: 0.1084\n",
      "144/223, train_loss: 0.1030, step time: 0.1015\n",
      "145/223, train_loss: 0.0963, step time: 0.1088\n",
      "146/223, train_loss: 0.1018, step time: 0.1095\n",
      "147/223, train_loss: 0.1133, step time: 0.1084\n",
      "148/223, train_loss: 0.1021, step time: 0.1137\n",
      "149/223, train_loss: 0.1001, step time: 0.1078\n",
      "150/223, train_loss: 0.1090, step time: 0.1117\n",
      "151/223, train_loss: 0.1118, step time: 0.1174\n",
      "152/223, train_loss: 0.0993, step time: 0.1010\n",
      "153/223, train_loss: 0.1079, step time: 0.1112\n",
      "154/223, train_loss: 0.1021, step time: 0.1073\n",
      "155/223, train_loss: 0.1038, step time: 0.1111\n",
      "156/223, train_loss: 0.1096, step time: 0.1003\n",
      "157/223, train_loss: 0.1148, step time: 0.1357\n",
      "158/223, train_loss: 0.1044, step time: 0.1266\n",
      "159/223, train_loss: 0.1068, step time: 0.1111\n",
      "160/223, train_loss: 0.1086, step time: 0.0999\n",
      "161/223, train_loss: 0.0977, step time: 0.1007\n",
      "162/223, train_loss: 0.1008, step time: 0.1142\n",
      "163/223, train_loss: 0.1251, step time: 0.1012\n",
      "164/223, train_loss: 0.1012, step time: 0.1255\n",
      "165/223, train_loss: 0.0941, step time: 0.1070\n",
      "166/223, train_loss: 0.1002, step time: 0.1004\n",
      "167/223, train_loss: 0.0944, step time: 0.1157\n",
      "168/223, train_loss: 0.1070, step time: 0.1173\n",
      "169/223, train_loss: 0.1064, step time: 0.1209\n",
      "170/223, train_loss: 0.1270, step time: 0.1084\n",
      "171/223, train_loss: 0.1128, step time: 0.1102\n",
      "172/223, train_loss: 0.1116, step time: 0.1126\n",
      "173/223, train_loss: 0.1122, step time: 0.1076\n",
      "174/223, train_loss: 0.1109, step time: 0.1006\n",
      "175/223, train_loss: 0.1009, step time: 0.1234\n",
      "176/223, train_loss: 0.1042, step time: 0.1012\n",
      "177/223, train_loss: 0.0983, step time: 0.0995\n",
      "178/223, train_loss: 0.1080, step time: 0.1081\n",
      "179/223, train_loss: 0.1043, step time: 0.1229\n",
      "180/223, train_loss: 0.0964, step time: 0.1046\n",
      "181/223, train_loss: 0.1103, step time: 0.1289\n",
      "182/223, train_loss: 0.1157, step time: 0.0996\n",
      "183/223, train_loss: 0.1002, step time: 0.1007\n",
      "184/223, train_loss: 0.1010, step time: 0.1064\n",
      "185/223, train_loss: 0.1090, step time: 0.1285\n",
      "186/223, train_loss: 0.1002, step time: 0.1113\n",
      "187/223, train_loss: 0.1026, step time: 0.1308\n",
      "188/223, train_loss: 0.1056, step time: 0.1083\n",
      "189/223, train_loss: 0.1090, step time: 0.1067\n",
      "190/223, train_loss: 0.1067, step time: 0.1110\n",
      "191/223, train_loss: 0.1036, step time: 0.1273\n",
      "192/223, train_loss: 0.1031, step time: 0.1203\n",
      "193/223, train_loss: 0.1127, step time: 0.1449\n",
      "194/223, train_loss: 0.1154, step time: 0.1358\n",
      "195/223, train_loss: 0.0974, step time: 0.1170\n",
      "196/223, train_loss: 0.1039, step time: 0.1000\n",
      "197/223, train_loss: 0.1224, step time: 0.1007\n",
      "198/223, train_loss: 0.1096, step time: 0.1057\n",
      "199/223, train_loss: 0.0996, step time: 0.1374\n",
      "200/223, train_loss: 0.1118, step time: 0.1004\n",
      "201/223, train_loss: 0.1292, step time: 0.1099\n",
      "202/223, train_loss: 0.1016, step time: 0.1005\n",
      "203/223, train_loss: 0.1199, step time: 0.1008\n",
      "204/223, train_loss: 0.1041, step time: 0.1021\n",
      "205/223, train_loss: 0.1335, step time: 0.0992\n",
      "206/223, train_loss: 0.0967, step time: 0.1003\n",
      "207/223, train_loss: 0.1093, step time: 0.1005\n",
      "208/223, train_loss: 0.1063, step time: 0.1110\n",
      "209/223, train_loss: 0.1128, step time: 0.1018\n",
      "210/223, train_loss: 0.1026, step time: 0.1214\n",
      "211/223, train_loss: 0.1181, step time: 0.1004\n",
      "212/223, train_loss: 0.1061, step time: 0.1019\n",
      "213/223, train_loss: 0.1054, step time: 0.1043\n",
      "214/223, train_loss: 0.1196, step time: 0.1069\n",
      "215/223, train_loss: 0.1005, step time: 0.1176\n",
      "216/223, train_loss: 0.1046, step time: 0.0995\n",
      "217/223, train_loss: 0.1060, step time: 0.0995\n",
      "218/223, train_loss: 0.1077, step time: 0.1005\n",
      "219/223, train_loss: 0.0948, step time: 0.1024\n",
      "220/223, train_loss: 0.0982, step time: 0.1000\n",
      "221/223, train_loss: 0.1043, step time: 0.0995\n",
      "222/223, train_loss: 0.1096, step time: 0.0995\n",
      "223/223, train_loss: 0.1123, step time: 0.0999\n",
      "epoch 155 average loss: 0.1065\n",
      "saved new best metric model\n",
      "current epoch: 155 current mean dice: 0.8578 tc: 0.9206 wt: 0.8676 et: 0.7851\n",
      "best mean dice: 0.8578 at epoch: 155\n",
      "time consuming of epoch 155 is: 94.3312\n",
      "----------\n",
      "epoch 156/300\n",
      "1/223, train_loss: 0.0990, step time: 0.1017\n",
      "2/223, train_loss: 0.1003, step time: 0.1002\n",
      "3/223, train_loss: 0.1118, step time: 0.1050\n",
      "4/223, train_loss: 0.1048, step time: 0.1003\n",
      "5/223, train_loss: 0.0918, step time: 0.1293\n",
      "6/223, train_loss: 0.1040, step time: 0.1324\n",
      "7/223, train_loss: 0.1055, step time: 0.1227\n",
      "8/223, train_loss: 0.0967, step time: 0.1005\n",
      "9/223, train_loss: 0.1159, step time: 0.1166\n",
      "10/223, train_loss: 0.1029, step time: 0.1096\n",
      "11/223, train_loss: 0.1263, step time: 0.1137\n",
      "12/223, train_loss: 0.0989, step time: 0.1277\n",
      "13/223, train_loss: 0.1100, step time: 0.1047\n",
      "14/223, train_loss: 0.1083, step time: 0.1127\n",
      "15/223, train_loss: 0.1171, step time: 0.1077\n",
      "16/223, train_loss: 0.1193, step time: 0.1087\n",
      "17/223, train_loss: 0.1114, step time: 0.1191\n",
      "18/223, train_loss: 0.1112, step time: 0.1057\n",
      "19/223, train_loss: 0.1035, step time: 0.1100\n",
      "20/223, train_loss: 0.0998, step time: 0.1098\n",
      "21/223, train_loss: 0.1067, step time: 0.1217\n",
      "22/223, train_loss: 0.1141, step time: 0.1286\n",
      "23/223, train_loss: 0.1089, step time: 0.1007\n",
      "24/223, train_loss: 0.1052, step time: 0.1058\n",
      "25/223, train_loss: 0.1018, step time: 0.1032\n",
      "26/223, train_loss: 0.1184, step time: 0.1133\n",
      "27/223, train_loss: 0.0975, step time: 0.1104\n",
      "28/223, train_loss: 0.1035, step time: 0.1009\n",
      "29/223, train_loss: 0.0991, step time: 0.1163\n",
      "30/223, train_loss: 0.1083, step time: 0.1000\n",
      "31/223, train_loss: 0.1002, step time: 0.1009\n",
      "32/223, train_loss: 0.1062, step time: 0.1083\n",
      "33/223, train_loss: 0.1077, step time: 0.1316\n",
      "34/223, train_loss: 0.1039, step time: 0.1092\n",
      "35/223, train_loss: 0.1077, step time: 0.1032\n",
      "36/223, train_loss: 0.1055, step time: 0.1085\n",
      "37/223, train_loss: 0.1120, step time: 0.1087\n",
      "38/223, train_loss: 0.1076, step time: 0.1043\n",
      "39/223, train_loss: 0.1011, step time: 0.1204\n",
      "40/223, train_loss: 0.1025, step time: 0.1100\n",
      "41/223, train_loss: 0.1033, step time: 0.1008\n",
      "42/223, train_loss: 0.1132, step time: 0.1145\n",
      "43/223, train_loss: 0.0987, step time: 0.0995\n",
      "44/223, train_loss: 0.1113, step time: 0.1133\n",
      "45/223, train_loss: 0.1026, step time: 0.1166\n",
      "46/223, train_loss: 0.1132, step time: 0.1279\n",
      "47/223, train_loss: 0.1088, step time: 0.1001\n",
      "48/223, train_loss: 0.1135, step time: 0.1040\n",
      "49/223, train_loss: 0.1186, step time: 0.1152\n",
      "50/223, train_loss: 0.1088, step time: 0.1005\n",
      "51/223, train_loss: 0.1118, step time: 0.1004\n",
      "52/223, train_loss: 0.1090, step time: 0.1017\n",
      "53/223, train_loss: 0.1152, step time: 0.1149\n",
      "54/223, train_loss: 0.0994, step time: 0.1011\n",
      "55/223, train_loss: 0.1231, step time: 0.1044\n",
      "56/223, train_loss: 0.1002, step time: 0.1212\n",
      "57/223, train_loss: 0.1013, step time: 0.1031\n",
      "58/223, train_loss: 0.1044, step time: 0.1180\n",
      "59/223, train_loss: 0.0958, step time: 0.1027\n",
      "60/223, train_loss: 0.1080, step time: 0.1006\n",
      "61/223, train_loss: 0.1033, step time: 0.1113\n",
      "62/223, train_loss: 0.1051, step time: 0.1133\n",
      "63/223, train_loss: 0.1106, step time: 0.1008\n",
      "64/223, train_loss: 0.0954, step time: 0.1010\n",
      "65/223, train_loss: 0.1120, step time: 0.1126\n",
      "66/223, train_loss: 0.0959, step time: 0.1111\n",
      "67/223, train_loss: 0.1000, step time: 0.1250\n",
      "68/223, train_loss: 0.1031, step time: 0.1153\n",
      "69/223, train_loss: 0.1101, step time: 0.1200\n",
      "70/223, train_loss: 0.1241, step time: 0.1154\n",
      "71/223, train_loss: 0.0976, step time: 0.1164\n",
      "72/223, train_loss: 0.1143, step time: 0.1110\n",
      "73/223, train_loss: 0.0995, step time: 0.0990\n",
      "74/223, train_loss: 0.1121, step time: 0.1110\n",
      "75/223, train_loss: 0.1064, step time: 0.1132\n",
      "76/223, train_loss: 0.1034, step time: 0.1073\n",
      "77/223, train_loss: 0.1139, step time: 0.1116\n",
      "78/223, train_loss: 0.1183, step time: 0.1098\n",
      "79/223, train_loss: 0.1087, step time: 0.1378\n",
      "80/223, train_loss: 0.1103, step time: 0.1006\n",
      "81/223, train_loss: 0.0959, step time: 0.1194\n",
      "82/223, train_loss: 0.1050, step time: 0.1052\n",
      "83/223, train_loss: 0.1117, step time: 0.1187\n",
      "84/223, train_loss: 0.0935, step time: 0.1217\n",
      "85/223, train_loss: 0.1226, step time: 0.1178\n",
      "86/223, train_loss: 0.1046, step time: 0.1054\n",
      "87/223, train_loss: 0.1205, step time: 0.1228\n",
      "88/223, train_loss: 0.1136, step time: 0.1212\n",
      "89/223, train_loss: 0.1043, step time: 0.1182\n",
      "90/223, train_loss: 0.1046, step time: 0.1061\n",
      "91/223, train_loss: 0.1069, step time: 0.1032\n",
      "92/223, train_loss: 0.1068, step time: 0.1034\n",
      "93/223, train_loss: 0.1014, step time: 0.1012\n",
      "94/223, train_loss: 0.0941, step time: 0.1005\n",
      "95/223, train_loss: 0.1139, step time: 0.1087\n",
      "96/223, train_loss: 0.1080, step time: 0.1001\n",
      "97/223, train_loss: 0.3034, step time: 0.1139\n",
      "98/223, train_loss: 0.0971, step time: 0.1070\n",
      "99/223, train_loss: 0.0974, step time: 0.1169\n",
      "100/223, train_loss: 0.1093, step time: 0.1095\n",
      "101/223, train_loss: 0.1046, step time: 0.0995\n",
      "102/223, train_loss: 0.1036, step time: 0.1204\n",
      "103/223, train_loss: 0.1121, step time: 0.0993\n",
      "104/223, train_loss: 0.0990, step time: 0.1404\n",
      "105/223, train_loss: 0.1124, step time: 0.1002\n",
      "106/223, train_loss: 0.1022, step time: 0.1006\n",
      "107/223, train_loss: 0.1194, step time: 0.1005\n",
      "108/223, train_loss: 0.1061, step time: 0.1203\n",
      "109/223, train_loss: 0.1030, step time: 0.1169\n",
      "110/223, train_loss: 0.1010, step time: 0.1005\n",
      "111/223, train_loss: 0.1065, step time: 0.1015\n",
      "112/223, train_loss: 0.0948, step time: 0.1007\n",
      "113/223, train_loss: 0.0907, step time: 0.1130\n",
      "114/223, train_loss: 0.1028, step time: 0.1001\n",
      "115/223, train_loss: 0.0970, step time: 0.1032\n",
      "116/223, train_loss: 0.1119, step time: 0.1004\n",
      "117/223, train_loss: 0.1068, step time: 0.1203\n",
      "118/223, train_loss: 0.0923, step time: 0.0994\n",
      "119/223, train_loss: 0.1090, step time: 0.1179\n",
      "120/223, train_loss: 0.1047, step time: 0.1057\n",
      "121/223, train_loss: 0.1192, step time: 0.1002\n",
      "122/223, train_loss: 0.1001, step time: 0.1035\n",
      "123/223, train_loss: 0.1007, step time: 0.1134\n",
      "124/223, train_loss: 0.1080, step time: 0.1172\n",
      "125/223, train_loss: 0.1058, step time: 0.1148\n",
      "126/223, train_loss: 0.1045, step time: 0.0988\n",
      "127/223, train_loss: 0.1041, step time: 0.0993\n",
      "128/223, train_loss: 0.1034, step time: 0.1116\n",
      "129/223, train_loss: 0.1074, step time: 0.1122\n",
      "130/223, train_loss: 0.1025, step time: 0.1005\n",
      "131/223, train_loss: 0.1019, step time: 0.1304\n",
      "132/223, train_loss: 0.1011, step time: 0.1079\n",
      "133/223, train_loss: 0.1056, step time: 0.0999\n",
      "134/223, train_loss: 0.1055, step time: 0.1002\n",
      "135/223, train_loss: 0.1057, step time: 0.1007\n",
      "136/223, train_loss: 0.0998, step time: 0.1002\n",
      "137/223, train_loss: 0.1013, step time: 0.1168\n",
      "138/223, train_loss: 0.0982, step time: 0.1172\n",
      "139/223, train_loss: 0.1149, step time: 0.1177\n",
      "140/223, train_loss: 0.1034, step time: 0.1190\n",
      "141/223, train_loss: 0.1054, step time: 0.1042\n",
      "142/223, train_loss: 0.1077, step time: 0.1046\n",
      "143/223, train_loss: 0.1123, step time: 0.1132\n",
      "144/223, train_loss: 0.1115, step time: 0.1094\n",
      "145/223, train_loss: 0.0961, step time: 0.1064\n",
      "146/223, train_loss: 0.1031, step time: 0.1001\n",
      "147/223, train_loss: 0.0940, step time: 0.1105\n",
      "148/223, train_loss: 0.1015, step time: 0.1271\n",
      "149/223, train_loss: 0.1071, step time: 0.1093\n",
      "150/223, train_loss: 0.1105, step time: 0.1129\n",
      "151/223, train_loss: 0.1051, step time: 0.1221\n",
      "152/223, train_loss: 0.1057, step time: 0.1214\n",
      "153/223, train_loss: 0.0995, step time: 0.1317\n",
      "154/223, train_loss: 0.1166, step time: 0.0996\n",
      "155/223, train_loss: 0.1081, step time: 0.1269\n",
      "156/223, train_loss: 0.0988, step time: 0.1076\n",
      "157/223, train_loss: 0.1130, step time: 0.1133\n",
      "158/223, train_loss: 0.1036, step time: 0.1063\n",
      "159/223, train_loss: 0.1016, step time: 0.1165\n",
      "160/223, train_loss: 0.0993, step time: 0.0993\n",
      "161/223, train_loss: 0.1085, step time: 0.1131\n",
      "162/223, train_loss: 0.0980, step time: 0.1099\n",
      "163/223, train_loss: 0.0984, step time: 0.1044\n",
      "164/223, train_loss: 0.0982, step time: 0.1173\n",
      "165/223, train_loss: 0.1118, step time: 0.1032\n",
      "166/223, train_loss: 0.1178, step time: 0.1026\n",
      "167/223, train_loss: 0.0909, step time: 0.1158\n",
      "168/223, train_loss: 0.1041, step time: 0.1201\n",
      "169/223, train_loss: 0.0993, step time: 0.1030\n",
      "170/223, train_loss: 0.1083, step time: 0.1054\n",
      "171/223, train_loss: 0.1137, step time: 0.1003\n",
      "172/223, train_loss: 0.1137, step time: 0.1072\n",
      "173/223, train_loss: 0.1025, step time: 0.1047\n",
      "174/223, train_loss: 0.1044, step time: 0.1008\n",
      "175/223, train_loss: 0.1023, step time: 0.1007\n",
      "176/223, train_loss: 0.1072, step time: 0.1105\n",
      "177/223, train_loss: 0.1050, step time: 0.1087\n",
      "178/223, train_loss: 0.1184, step time: 0.1007\n",
      "179/223, train_loss: 0.0969, step time: 0.1069\n",
      "180/223, train_loss: 0.1109, step time: 0.1116\n",
      "181/223, train_loss: 0.1107, step time: 0.1041\n",
      "182/223, train_loss: 0.0983, step time: 0.1083\n",
      "183/223, train_loss: 0.1048, step time: 0.1184\n",
      "184/223, train_loss: 0.1183, step time: 0.1207\n",
      "185/223, train_loss: 0.0995, step time: 0.1165\n",
      "186/223, train_loss: 0.0973, step time: 0.1184\n",
      "187/223, train_loss: 0.1186, step time: 0.1159\n",
      "188/223, train_loss: 0.0936, step time: 0.1124\n",
      "189/223, train_loss: 0.1099, step time: 0.1068\n",
      "190/223, train_loss: 0.1161, step time: 0.0998\n",
      "191/223, train_loss: 0.1060, step time: 0.1038\n",
      "192/223, train_loss: 0.1132, step time: 0.1001\n",
      "193/223, train_loss: 0.1015, step time: 0.1103\n",
      "194/223, train_loss: 0.0995, step time: 0.1123\n",
      "195/223, train_loss: 0.1000, step time: 0.1004\n",
      "196/223, train_loss: 0.1101, step time: 0.1002\n",
      "197/223, train_loss: 0.0879, step time: 0.1185\n",
      "198/223, train_loss: 0.1097, step time: 0.1282\n",
      "199/223, train_loss: 0.0987, step time: 0.1067\n",
      "200/223, train_loss: 0.1276, step time: 0.1013\n",
      "201/223, train_loss: 0.0933, step time: 0.1053\n",
      "202/223, train_loss: 0.1008, step time: 0.1031\n",
      "203/223, train_loss: 0.0953, step time: 0.1040\n",
      "204/223, train_loss: 0.1075, step time: 0.1040\n",
      "205/223, train_loss: 0.1175, step time: 0.1016\n",
      "206/223, train_loss: 0.1074, step time: 0.1039\n",
      "207/223, train_loss: 0.1019, step time: 0.1004\n",
      "208/223, train_loss: 0.1109, step time: 0.1209\n",
      "209/223, train_loss: 0.1084, step time: 0.1130\n",
      "210/223, train_loss: 0.1032, step time: 0.1001\n",
      "211/223, train_loss: 0.1051, step time: 0.1252\n",
      "212/223, train_loss: 0.1065, step time: 0.1599\n",
      "213/223, train_loss: 0.1027, step time: 0.1139\n",
      "214/223, train_loss: 0.0956, step time: 0.1086\n",
      "215/223, train_loss: 0.1022, step time: 0.0996\n",
      "216/223, train_loss: 0.0976, step time: 0.1008\n",
      "217/223, train_loss: 0.1288, step time: 0.1145\n",
      "218/223, train_loss: 0.1101, step time: 0.1172\n",
      "219/223, train_loss: 0.1083, step time: 0.1028\n",
      "220/223, train_loss: 0.1019, step time: 0.0991\n",
      "221/223, train_loss: 0.0963, step time: 0.0996\n",
      "222/223, train_loss: 0.0966, step time: 0.0992\n",
      "223/223, train_loss: 0.1070, step time: 0.1003\n",
      "epoch 156 average loss: 0.1067\n",
      "time consuming of epoch 156 is: 87.1804\n",
      "----------\n",
      "epoch 157/300\n",
      "1/223, train_loss: 0.0985, step time: 0.1022\n",
      "2/223, train_loss: 0.1108, step time: 0.1097\n",
      "3/223, train_loss: 0.1048, step time: 0.1005\n",
      "4/223, train_loss: 0.1066, step time: 0.1324\n",
      "5/223, train_loss: 0.1057, step time: 0.1100\n",
      "6/223, train_loss: 0.1138, step time: 0.1038\n",
      "7/223, train_loss: 0.1100, step time: 0.1000\n",
      "8/223, train_loss: 0.1122, step time: 0.1141\n",
      "9/223, train_loss: 0.1000, step time: 0.1050\n",
      "10/223, train_loss: 0.0933, step time: 0.1122\n",
      "11/223, train_loss: 0.0946, step time: 0.1048\n",
      "12/223, train_loss: 0.0996, step time: 0.1010\n",
      "13/223, train_loss: 0.1166, step time: 0.1012\n",
      "14/223, train_loss: 0.0993, step time: 0.1090\n",
      "15/223, train_loss: 0.1160, step time: 0.1099\n",
      "16/223, train_loss: 0.0929, step time: 0.1046\n",
      "17/223, train_loss: 0.1131, step time: 0.1104\n",
      "18/223, train_loss: 0.1088, step time: 0.1057\n",
      "19/223, train_loss: 0.1040, step time: 0.1112\n",
      "20/223, train_loss: 0.0941, step time: 0.1000\n",
      "21/223, train_loss: 0.1069, step time: 0.0998\n",
      "22/223, train_loss: 0.1003, step time: 0.1066\n",
      "23/223, train_loss: 0.1036, step time: 0.1097\n",
      "24/223, train_loss: 0.1066, step time: 0.1039\n",
      "25/223, train_loss: 0.0978, step time: 0.1005\n",
      "26/223, train_loss: 0.1168, step time: 0.1222\n",
      "27/223, train_loss: 0.0927, step time: 0.0998\n",
      "28/223, train_loss: 0.1160, step time: 0.1112\n",
      "29/223, train_loss: 0.0977, step time: 0.1157\n",
      "30/223, train_loss: 0.1042, step time: 0.1007\n",
      "31/223, train_loss: 0.1092, step time: 0.1006\n",
      "32/223, train_loss: 0.0958, step time: 0.1164\n",
      "33/223, train_loss: 0.1046, step time: 0.1005\n",
      "34/223, train_loss: 0.1029, step time: 0.1100\n",
      "35/223, train_loss: 0.1044, step time: 0.1003\n",
      "36/223, train_loss: 0.1067, step time: 0.1000\n",
      "37/223, train_loss: 0.0963, step time: 0.0997\n",
      "38/223, train_loss: 0.1016, step time: 0.1077\n",
      "39/223, train_loss: 0.0916, step time: 0.1039\n",
      "40/223, train_loss: 0.0958, step time: 0.1012\n",
      "41/223, train_loss: 0.1028, step time: 0.1082\n",
      "42/223, train_loss: 0.1129, step time: 0.1123\n",
      "43/223, train_loss: 0.1197, step time: 0.1147\n",
      "44/223, train_loss: 0.0974, step time: 0.1150\n",
      "45/223, train_loss: 0.1182, step time: 0.1002\n",
      "46/223, train_loss: 0.1079, step time: 0.1117\n",
      "47/223, train_loss: 0.1126, step time: 0.1193\n",
      "48/223, train_loss: 0.1059, step time: 0.1102\n",
      "49/223, train_loss: 0.1006, step time: 0.1129\n",
      "50/223, train_loss: 0.1010, step time: 0.1208\n",
      "51/223, train_loss: 0.1069, step time: 0.1171\n",
      "52/223, train_loss: 0.0997, step time: 0.1000\n",
      "53/223, train_loss: 0.1081, step time: 0.1132\n",
      "54/223, train_loss: 0.0969, step time: 0.1061\n",
      "55/223, train_loss: 0.0968, step time: 0.1182\n",
      "56/223, train_loss: 0.1016, step time: 0.1196\n",
      "57/223, train_loss: 0.0983, step time: 0.1145\n",
      "58/223, train_loss: 0.1095, step time: 0.1151\n",
      "59/223, train_loss: 0.1031, step time: 0.1010\n",
      "60/223, train_loss: 0.1019, step time: 0.1120\n",
      "61/223, train_loss: 0.0952, step time: 0.1121\n",
      "62/223, train_loss: 0.1031, step time: 0.1061\n",
      "63/223, train_loss: 0.1046, step time: 0.1050\n",
      "64/223, train_loss: 0.0957, step time: 0.1213\n",
      "65/223, train_loss: 0.1144, step time: 0.1089\n",
      "66/223, train_loss: 0.1016, step time: 0.1054\n",
      "67/223, train_loss: 0.0973, step time: 0.1121\n",
      "68/223, train_loss: 0.0967, step time: 0.1002\n",
      "69/223, train_loss: 0.1121, step time: 0.0999\n",
      "70/223, train_loss: 0.1033, step time: 0.1035\n",
      "71/223, train_loss: 0.1185, step time: 0.1000\n",
      "72/223, train_loss: 0.0996, step time: 0.1006\n",
      "73/223, train_loss: 0.1013, step time: 0.1107\n",
      "74/223, train_loss: 0.1093, step time: 0.1180\n",
      "75/223, train_loss: 0.1071, step time: 0.1038\n",
      "76/223, train_loss: 0.1030, step time: 0.1067\n",
      "77/223, train_loss: 0.1148, step time: 0.1182\n",
      "78/223, train_loss: 0.1190, step time: 0.1107\n",
      "79/223, train_loss: 0.1020, step time: 0.1216\n",
      "80/223, train_loss: 0.0945, step time: 0.1199\n",
      "81/223, train_loss: 0.1060, step time: 0.1113\n",
      "82/223, train_loss: 0.1094, step time: 0.1086\n",
      "83/223, train_loss: 0.1182, step time: 0.1100\n",
      "84/223, train_loss: 0.1201, step time: 0.1197\n",
      "85/223, train_loss: 0.1011, step time: 0.1126\n",
      "86/223, train_loss: 0.1040, step time: 0.1258\n",
      "87/223, train_loss: 0.1154, step time: 0.1118\n",
      "88/223, train_loss: 0.1015, step time: 0.1226\n",
      "89/223, train_loss: 0.0998, step time: 0.1133\n",
      "90/223, train_loss: 0.1047, step time: 0.1043\n",
      "91/223, train_loss: 0.1032, step time: 0.1057\n",
      "92/223, train_loss: 0.0994, step time: 0.1134\n",
      "93/223, train_loss: 0.1040, step time: 0.1152\n",
      "94/223, train_loss: 0.1181, step time: 0.1036\n",
      "95/223, train_loss: 0.1036, step time: 0.1095\n",
      "96/223, train_loss: 0.1151, step time: 0.1112\n",
      "97/223, train_loss: 0.0946, step time: 0.1091\n",
      "98/223, train_loss: 0.1038, step time: 0.1265\n",
      "99/223, train_loss: 0.1176, step time: 0.1416\n",
      "100/223, train_loss: 0.1132, step time: 0.1209\n",
      "101/223, train_loss: 0.1029, step time: 0.1069\n",
      "102/223, train_loss: 0.0931, step time: 0.1016\n",
      "103/223, train_loss: 0.1138, step time: 0.0997\n",
      "104/223, train_loss: 0.0934, step time: 0.0999\n",
      "105/223, train_loss: 0.1088, step time: 0.1429\n",
      "106/223, train_loss: 0.1104, step time: 0.1159\n",
      "107/223, train_loss: 0.1075, step time: 0.1003\n",
      "108/223, train_loss: 0.1081, step time: 0.1254\n",
      "109/223, train_loss: 0.1003, step time: 0.1590\n",
      "110/223, train_loss: 0.1085, step time: 0.1159\n",
      "111/223, train_loss: 0.1053, step time: 0.1005\n",
      "112/223, train_loss: 0.1005, step time: 0.0997\n",
      "113/223, train_loss: 0.1014, step time: 0.1083\n",
      "114/223, train_loss: 0.1123, step time: 0.1161\n",
      "115/223, train_loss: 0.1012, step time: 0.1237\n",
      "116/223, train_loss: 0.0992, step time: 0.1155\n",
      "117/223, train_loss: 0.0975, step time: 0.0997\n",
      "118/223, train_loss: 0.1060, step time: 0.1012\n",
      "119/223, train_loss: 0.0997, step time: 0.1175\n",
      "120/223, train_loss: 0.0953, step time: 0.1008\n",
      "121/223, train_loss: 0.1061, step time: 0.1017\n",
      "122/223, train_loss: 0.1105, step time: 0.1006\n",
      "123/223, train_loss: 0.1113, step time: 0.1542\n",
      "124/223, train_loss: 0.1110, step time: 0.1006\n",
      "125/223, train_loss: 0.0999, step time: 0.1021\n",
      "126/223, train_loss: 0.1110, step time: 0.1003\n",
      "127/223, train_loss: 0.1080, step time: 0.1294\n",
      "128/223, train_loss: 0.0993, step time: 0.1196\n",
      "129/223, train_loss: 0.1050, step time: 0.1004\n",
      "130/223, train_loss: 0.1046, step time: 0.1005\n",
      "131/223, train_loss: 0.1058, step time: 0.1004\n",
      "132/223, train_loss: 0.1042, step time: 0.0999\n",
      "133/223, train_loss: 0.1106, step time: 0.1010\n",
      "134/223, train_loss: 0.1113, step time: 0.1130\n",
      "135/223, train_loss: 0.1052, step time: 0.1097\n",
      "136/223, train_loss: 0.1003, step time: 0.1105\n",
      "137/223, train_loss: 0.0998, step time: 0.1155\n",
      "138/223, train_loss: 0.1102, step time: 0.1196\n",
      "139/223, train_loss: 0.1000, step time: 0.1086\n",
      "140/223, train_loss: 0.2940, step time: 0.1234\n",
      "141/223, train_loss: 0.0950, step time: 0.1005\n",
      "142/223, train_loss: 0.0960, step time: 0.1066\n",
      "143/223, train_loss: 0.1033, step time: 0.1089\n",
      "144/223, train_loss: 0.0928, step time: 0.1009\n",
      "145/223, train_loss: 0.1114, step time: 0.1001\n",
      "146/223, train_loss: 0.1071, step time: 0.1167\n",
      "147/223, train_loss: 0.1065, step time: 0.1293\n",
      "148/223, train_loss: 0.1027, step time: 0.1220\n",
      "149/223, train_loss: 0.1052, step time: 0.1017\n",
      "150/223, train_loss: 0.0989, step time: 0.1155\n",
      "151/223, train_loss: 0.1119, step time: 0.1273\n",
      "152/223, train_loss: 0.1062, step time: 0.1644\n",
      "153/223, train_loss: 0.1058, step time: 0.1128\n",
      "154/223, train_loss: 0.1105, step time: 0.0995\n",
      "155/223, train_loss: 0.1144, step time: 0.1095\n",
      "156/223, train_loss: 0.0974, step time: 0.1401\n",
      "157/223, train_loss: 0.1063, step time: 0.1053\n",
      "158/223, train_loss: 0.1117, step time: 0.1129\n",
      "159/223, train_loss: 0.1032, step time: 0.1526\n",
      "160/223, train_loss: 0.1178, step time: 0.1244\n",
      "161/223, train_loss: 0.1030, step time: 0.1097\n",
      "162/223, train_loss: 0.1197, step time: 0.1215\n",
      "163/223, train_loss: 0.1060, step time: 0.1481\n",
      "164/223, train_loss: 0.1068, step time: 0.1216\n",
      "165/223, train_loss: 0.1059, step time: 0.1116\n",
      "166/223, train_loss: 0.0972, step time: 0.1014\n",
      "167/223, train_loss: 0.1117, step time: 0.1057\n",
      "168/223, train_loss: 0.1081, step time: 0.1537\n",
      "169/223, train_loss: 0.0980, step time: 0.1243\n",
      "170/223, train_loss: 0.1076, step time: 0.1244\n",
      "171/223, train_loss: 0.1049, step time: 0.1002\n",
      "172/223, train_loss: 0.1174, step time: 0.1080\n",
      "173/223, train_loss: 0.1005, step time: 0.1028\n",
      "174/223, train_loss: 0.0994, step time: 0.1129\n",
      "175/223, train_loss: 0.0948, step time: 0.1262\n",
      "176/223, train_loss: 0.0988, step time: 0.1061\n",
      "177/223, train_loss: 0.1191, step time: 0.1053\n",
      "178/223, train_loss: 0.0894, step time: 0.1221\n",
      "179/223, train_loss: 0.1000, step time: 0.1192\n",
      "180/223, train_loss: 0.1141, step time: 0.1118\n",
      "181/223, train_loss: 0.1138, step time: 0.1029\n",
      "182/223, train_loss: 0.0964, step time: 0.1002\n",
      "183/223, train_loss: 0.1077, step time: 0.0996\n",
      "184/223, train_loss: 0.1074, step time: 0.0996\n",
      "185/223, train_loss: 0.1006, step time: 0.1067\n",
      "186/223, train_loss: 0.1006, step time: 0.1317\n",
      "187/223, train_loss: 0.1204, step time: 0.0999\n",
      "188/223, train_loss: 0.1115, step time: 0.1019\n",
      "189/223, train_loss: 0.1123, step time: 0.1116\n",
      "190/223, train_loss: 0.1021, step time: 0.1085\n",
      "191/223, train_loss: 0.1129, step time: 0.1352\n",
      "192/223, train_loss: 0.0993, step time: 0.1486\n",
      "193/223, train_loss: 0.1094, step time: 0.1150\n",
      "194/223, train_loss: 0.1197, step time: 0.1203\n",
      "195/223, train_loss: 0.1024, step time: 0.1167\n",
      "196/223, train_loss: 0.1090, step time: 0.1004\n",
      "197/223, train_loss: 0.1156, step time: 0.1282\n",
      "198/223, train_loss: 0.1113, step time: 0.1127\n",
      "199/223, train_loss: 0.0974, step time: 0.1010\n",
      "200/223, train_loss: 0.1065, step time: 0.1008\n",
      "201/223, train_loss: 0.1119, step time: 0.1271\n",
      "202/223, train_loss: 0.1055, step time: 0.1292\n",
      "203/223, train_loss: 0.1121, step time: 0.1173\n",
      "204/223, train_loss: 0.1097, step time: 0.1415\n",
      "205/223, train_loss: 0.1141, step time: 0.0993\n",
      "206/223, train_loss: 0.0948, step time: 0.1016\n",
      "207/223, train_loss: 0.1086, step time: 0.1037\n",
      "208/223, train_loss: 0.1152, step time: 0.1155\n",
      "209/223, train_loss: 0.0968, step time: 0.1005\n",
      "210/223, train_loss: 0.0945, step time: 0.1075\n",
      "211/223, train_loss: 0.1065, step time: 0.0990\n",
      "212/223, train_loss: 0.1026, step time: 0.0991\n",
      "213/223, train_loss: 0.1040, step time: 0.0989\n",
      "214/223, train_loss: 0.0984, step time: 0.1307\n",
      "215/223, train_loss: 0.0941, step time: 0.1055\n",
      "216/223, train_loss: 0.1141, step time: 0.1063\n",
      "217/223, train_loss: 0.1006, step time: 0.1015\n",
      "218/223, train_loss: 0.1202, step time: 0.1001\n",
      "219/223, train_loss: 0.1170, step time: 0.1001\n",
      "220/223, train_loss: 0.1041, step time: 0.1166\n",
      "221/223, train_loss: 0.1130, step time: 0.1019\n",
      "222/223, train_loss: 0.1127, step time: 0.0986\n",
      "223/223, train_loss: 0.1047, step time: 0.0994\n",
      "epoch 157 average loss: 0.1062\n",
      "time consuming of epoch 157 is: 88.9235\n",
      "----------\n",
      "epoch 158/300\n",
      "1/223, train_loss: 0.1072, step time: 0.1010\n",
      "2/223, train_loss: 0.1260, step time: 0.1066\n",
      "3/223, train_loss: 0.1067, step time: 0.1065\n",
      "4/223, train_loss: 0.0939, step time: 0.1011\n",
      "5/223, train_loss: 0.1022, step time: 0.1105\n",
      "6/223, train_loss: 0.1045, step time: 0.1202\n",
      "7/223, train_loss: 0.1043, step time: 0.1100\n",
      "8/223, train_loss: 0.0974, step time: 0.1214\n",
      "9/223, train_loss: 0.0999, step time: 0.1146\n",
      "10/223, train_loss: 0.1110, step time: 0.1203\n",
      "11/223, train_loss: 0.1020, step time: 0.1312\n",
      "12/223, train_loss: 0.1006, step time: 0.1066\n",
      "13/223, train_loss: 0.1150, step time: 0.1174\n",
      "14/223, train_loss: 0.1077, step time: 0.1233\n",
      "15/223, train_loss: 0.1250, step time: 0.1261\n",
      "16/223, train_loss: 0.0995, step time: 0.1150\n",
      "17/223, train_loss: 0.1104, step time: 0.1001\n",
      "18/223, train_loss: 0.1085, step time: 0.1135\n",
      "19/223, train_loss: 0.0984, step time: 0.1125\n",
      "20/223, train_loss: 0.1034, step time: 0.1247\n",
      "21/223, train_loss: 0.1058, step time: 0.1140\n",
      "22/223, train_loss: 0.1008, step time: 0.1176\n",
      "23/223, train_loss: 0.0985, step time: 0.1135\n",
      "24/223, train_loss: 0.1070, step time: 0.1200\n",
      "25/223, train_loss: 0.0866, step time: 0.1138\n",
      "26/223, train_loss: 0.1092, step time: 0.1152\n",
      "27/223, train_loss: 0.1245, step time: 0.1103\n",
      "28/223, train_loss: 0.1070, step time: 0.1010\n",
      "29/223, train_loss: 0.1029, step time: 0.1149\n",
      "30/223, train_loss: 0.1010, step time: 0.1092\n",
      "31/223, train_loss: 0.1216, step time: 0.1101\n",
      "32/223, train_loss: 0.1074, step time: 0.1200\n",
      "33/223, train_loss: 0.1078, step time: 0.0998\n",
      "34/223, train_loss: 0.0973, step time: 0.1001\n",
      "35/223, train_loss: 0.1124, step time: 0.1025\n",
      "36/223, train_loss: 0.0975, step time: 0.1013\n",
      "37/223, train_loss: 0.1067, step time: 0.1177\n",
      "38/223, train_loss: 0.1072, step time: 0.1167\n",
      "39/223, train_loss: 0.0999, step time: 0.1096\n",
      "40/223, train_loss: 0.1014, step time: 0.1006\n",
      "41/223, train_loss: 0.1122, step time: 0.1001\n",
      "42/223, train_loss: 0.0975, step time: 0.1044\n",
      "43/223, train_loss: 0.1061, step time: 0.1083\n",
      "44/223, train_loss: 0.1082, step time: 0.1006\n",
      "45/223, train_loss: 0.1121, step time: 0.1148\n",
      "46/223, train_loss: 0.1115, step time: 0.1032\n",
      "47/223, train_loss: 0.1131, step time: 0.1001\n",
      "48/223, train_loss: 0.1083, step time: 0.0998\n",
      "49/223, train_loss: 0.1073, step time: 0.1071\n",
      "50/223, train_loss: 0.1050, step time: 0.0996\n",
      "51/223, train_loss: 0.1043, step time: 0.0985\n",
      "52/223, train_loss: 0.1162, step time: 0.0999\n",
      "53/223, train_loss: 0.1111, step time: 0.1007\n",
      "54/223, train_loss: 0.1100, step time: 0.1018\n",
      "55/223, train_loss: 0.1068, step time: 0.0994\n",
      "56/223, train_loss: 0.0964, step time: 0.0991\n",
      "57/223, train_loss: 0.1098, step time: 0.1007\n",
      "58/223, train_loss: 0.1015, step time: 0.1071\n",
      "59/223, train_loss: 0.1131, step time: 0.1053\n",
      "60/223, train_loss: 0.1019, step time: 0.1061\n",
      "61/223, train_loss: 0.1049, step time: 0.1054\n",
      "62/223, train_loss: 0.0984, step time: 0.1106\n",
      "63/223, train_loss: 0.1072, step time: 0.1080\n",
      "64/223, train_loss: 0.0963, step time: 0.1191\n",
      "65/223, train_loss: 0.0971, step time: 0.1099\n",
      "66/223, train_loss: 0.1078, step time: 0.1002\n",
      "67/223, train_loss: 0.0989, step time: 0.1071\n",
      "68/223, train_loss: 0.1086, step time: 0.1185\n",
      "69/223, train_loss: 0.0936, step time: 0.1064\n",
      "70/223, train_loss: 0.1184, step time: 0.1070\n",
      "71/223, train_loss: 0.0973, step time: 0.1025\n",
      "72/223, train_loss: 0.1093, step time: 0.1074\n",
      "73/223, train_loss: 0.0969, step time: 0.1081\n",
      "74/223, train_loss: 0.1119, step time: 0.1254\n",
      "75/223, train_loss: 0.0988, step time: 0.1138\n",
      "76/223, train_loss: 0.1029, step time: 0.1030\n",
      "77/223, train_loss: 0.1000, step time: 0.1092\n",
      "78/223, train_loss: 0.0996, step time: 0.1103\n",
      "79/223, train_loss: 0.0908, step time: 0.1065\n",
      "80/223, train_loss: 0.1056, step time: 0.1139\n",
      "81/223, train_loss: 0.1067, step time: 0.1091\n",
      "82/223, train_loss: 0.0959, step time: 0.1128\n",
      "83/223, train_loss: 0.1037, step time: 0.1078\n",
      "84/223, train_loss: 0.0958, step time: 0.1026\n",
      "85/223, train_loss: 0.1088, step time: 0.1097\n",
      "86/223, train_loss: 0.0973, step time: 0.1129\n",
      "87/223, train_loss: 0.0984, step time: 0.1217\n",
      "88/223, train_loss: 0.1016, step time: 0.1102\n",
      "89/223, train_loss: 0.1166, step time: 0.1055\n",
      "90/223, train_loss: 0.1027, step time: 0.1003\n",
      "91/223, train_loss: 0.0927, step time: 0.1170\n",
      "92/223, train_loss: 0.0947, step time: 0.1067\n",
      "93/223, train_loss: 0.0966, step time: 0.1150\n",
      "94/223, train_loss: 0.0976, step time: 0.1040\n",
      "95/223, train_loss: 0.1133, step time: 0.1091\n",
      "96/223, train_loss: 0.1168, step time: 0.1115\n",
      "97/223, train_loss: 0.1046, step time: 0.1083\n",
      "98/223, train_loss: 0.1012, step time: 0.1149\n",
      "99/223, train_loss: 0.1031, step time: 0.1053\n",
      "100/223, train_loss: 0.1088, step time: 0.1008\n",
      "101/223, train_loss: 0.1030, step time: 0.1150\n",
      "102/223, train_loss: 0.1023, step time: 0.1011\n",
      "103/223, train_loss: 0.1161, step time: 0.1151\n",
      "104/223, train_loss: 0.0945, step time: 0.1004\n",
      "105/223, train_loss: 0.1096, step time: 0.1031\n",
      "106/223, train_loss: 0.1160, step time: 0.1164\n",
      "107/223, train_loss: 0.1049, step time: 0.1103\n",
      "108/223, train_loss: 0.1124, step time: 0.1013\n",
      "109/223, train_loss: 0.1016, step time: 0.1094\n",
      "110/223, train_loss: 0.1252, step time: 0.1125\n",
      "111/223, train_loss: 0.1023, step time: 0.1015\n",
      "112/223, train_loss: 0.0967, step time: 0.1156\n",
      "113/223, train_loss: 0.0958, step time: 0.1090\n",
      "114/223, train_loss: 0.1133, step time: 0.1174\n",
      "115/223, train_loss: 0.0995, step time: 0.1003\n",
      "116/223, train_loss: 0.0966, step time: 0.1010\n",
      "117/223, train_loss: 0.1039, step time: 0.1005\n",
      "118/223, train_loss: 0.1001, step time: 0.1134\n",
      "119/223, train_loss: 0.0940, step time: 0.1173\n",
      "120/223, train_loss: 0.0961, step time: 0.1332\n",
      "121/223, train_loss: 0.1269, step time: 0.1086\n",
      "122/223, train_loss: 0.1016, step time: 0.1150\n",
      "123/223, train_loss: 0.1072, step time: 0.0996\n",
      "124/223, train_loss: 0.1093, step time: 0.1002\n",
      "125/223, train_loss: 0.1110, step time: 0.1005\n",
      "126/223, train_loss: 0.1076, step time: 0.1109\n",
      "127/223, train_loss: 0.1024, step time: 0.1054\n",
      "128/223, train_loss: 0.0974, step time: 0.1033\n",
      "129/223, train_loss: 0.1070, step time: 0.1022\n",
      "130/223, train_loss: 0.1115, step time: 0.1001\n",
      "131/223, train_loss: 0.1011, step time: 0.1021\n",
      "132/223, train_loss: 0.1020, step time: 0.1186\n",
      "133/223, train_loss: 0.1025, step time: 0.1003\n",
      "134/223, train_loss: 0.0981, step time: 0.1139\n",
      "135/223, train_loss: 0.0961, step time: 0.1006\n",
      "136/223, train_loss: 0.1124, step time: 0.0999\n",
      "137/223, train_loss: 0.1030, step time: 0.1135\n",
      "138/223, train_loss: 0.1040, step time: 0.1103\n",
      "139/223, train_loss: 0.1097, step time: 0.1061\n",
      "140/223, train_loss: 0.1131, step time: 0.0999\n",
      "141/223, train_loss: 0.1056, step time: 0.0998\n",
      "142/223, train_loss: 0.1151, step time: 0.1096\n",
      "143/223, train_loss: 0.0980, step time: 0.1103\n",
      "144/223, train_loss: 0.0980, step time: 0.1001\n",
      "145/223, train_loss: 0.1012, step time: 0.1117\n",
      "146/223, train_loss: 0.1072, step time: 0.1053\n",
      "147/223, train_loss: 0.1056, step time: 0.1183\n",
      "148/223, train_loss: 0.1126, step time: 0.1044\n",
      "149/223, train_loss: 0.1145, step time: 0.1060\n",
      "150/223, train_loss: 0.1068, step time: 0.1109\n",
      "151/223, train_loss: 0.1022, step time: 0.1295\n",
      "152/223, train_loss: 0.1147, step time: 0.1173\n",
      "153/223, train_loss: 0.1114, step time: 0.1003\n",
      "154/223, train_loss: 0.1085, step time: 0.1128\n",
      "155/223, train_loss: 0.1072, step time: 0.1115\n",
      "156/223, train_loss: 0.3014, step time: 0.0997\n",
      "157/223, train_loss: 0.0998, step time: 0.1003\n",
      "158/223, train_loss: 0.1101, step time: 0.1004\n",
      "159/223, train_loss: 0.1155, step time: 0.0997\n",
      "160/223, train_loss: 0.0976, step time: 0.1006\n",
      "161/223, train_loss: 0.1045, step time: 0.0999\n",
      "162/223, train_loss: 0.1090, step time: 0.1001\n",
      "163/223, train_loss: 0.1049, step time: 0.1023\n",
      "164/223, train_loss: 0.1036, step time: 0.0998\n",
      "165/223, train_loss: 0.1115, step time: 0.1056\n",
      "166/223, train_loss: 0.0943, step time: 0.1005\n",
      "167/223, train_loss: 0.1127, step time: 0.1152\n",
      "168/223, train_loss: 0.0937, step time: 0.1249\n",
      "169/223, train_loss: 0.1027, step time: 0.1512\n",
      "170/223, train_loss: 0.1214, step time: 0.0998\n",
      "171/223, train_loss: 0.0968, step time: 0.1003\n",
      "172/223, train_loss: 0.1048, step time: 0.1105\n",
      "173/223, train_loss: 0.0966, step time: 0.0999\n",
      "174/223, train_loss: 0.1067, step time: 0.1051\n",
      "175/223, train_loss: 0.0989, step time: 0.1061\n",
      "176/223, train_loss: 0.1235, step time: 0.1154\n",
      "177/223, train_loss: 0.1030, step time: 0.1024\n",
      "178/223, train_loss: 0.1052, step time: 0.1057\n",
      "179/223, train_loss: 0.1008, step time: 0.1007\n",
      "180/223, train_loss: 0.1023, step time: 0.1150\n",
      "181/223, train_loss: 0.1151, step time: 0.1373\n",
      "182/223, train_loss: 0.1109, step time: 0.1001\n",
      "183/223, train_loss: 0.1019, step time: 0.1444\n",
      "184/223, train_loss: 0.1044, step time: 0.1155\n",
      "185/223, train_loss: 0.1149, step time: 0.1166\n",
      "186/223, train_loss: 0.1043, step time: 0.1084\n",
      "187/223, train_loss: 0.0950, step time: 0.1097\n",
      "188/223, train_loss: 0.1021, step time: 0.1141\n",
      "189/223, train_loss: 0.1166, step time: 0.1236\n",
      "190/223, train_loss: 0.0972, step time: 0.1003\n",
      "191/223, train_loss: 0.1048, step time: 0.1009\n",
      "192/223, train_loss: 0.0978, step time: 0.1042\n",
      "193/223, train_loss: 0.1035, step time: 0.1250\n",
      "194/223, train_loss: 0.1118, step time: 0.1011\n",
      "195/223, train_loss: 0.1128, step time: 0.0992\n",
      "196/223, train_loss: 0.1063, step time: 0.1061\n",
      "197/223, train_loss: 0.0999, step time: 0.1484\n",
      "198/223, train_loss: 0.1006, step time: 0.1003\n",
      "199/223, train_loss: 0.0931, step time: 0.1003\n",
      "200/223, train_loss: 0.0978, step time: 0.1004\n",
      "201/223, train_loss: 0.1079, step time: 0.1002\n",
      "202/223, train_loss: 0.1116, step time: 0.1003\n",
      "203/223, train_loss: 0.1137, step time: 0.1038\n",
      "204/223, train_loss: 0.1098, step time: 0.1377\n",
      "205/223, train_loss: 0.1184, step time: 0.1100\n",
      "206/223, train_loss: 0.1026, step time: 0.1004\n",
      "207/223, train_loss: 0.1141, step time: 0.1008\n",
      "208/223, train_loss: 0.1270, step time: 0.1042\n",
      "209/223, train_loss: 0.1069, step time: 0.1069\n",
      "210/223, train_loss: 0.1045, step time: 0.1059\n",
      "211/223, train_loss: 0.1103, step time: 0.1279\n",
      "212/223, train_loss: 0.0911, step time: 0.1263\n",
      "213/223, train_loss: 0.1032, step time: 0.1131\n",
      "214/223, train_loss: 0.1002, step time: 0.1006\n",
      "215/223, train_loss: 0.0956, step time: 0.1102\n",
      "216/223, train_loss: 0.1107, step time: 0.1030\n",
      "217/223, train_loss: 0.1142, step time: 0.1019\n",
      "218/223, train_loss: 0.1074, step time: 0.1002\n",
      "219/223, train_loss: 0.1102, step time: 0.1001\n",
      "220/223, train_loss: 0.0972, step time: 0.0998\n",
      "221/223, train_loss: 0.1082, step time: 0.1002\n",
      "222/223, train_loss: 0.1087, step time: 0.1005\n",
      "223/223, train_loss: 0.1096, step time: 0.0995\n",
      "epoch 158 average loss: 0.1062\n",
      "time consuming of epoch 158 is: 85.7827\n",
      "----------\n",
      "epoch 159/300\n",
      "1/223, train_loss: 0.1099, step time: 0.1062\n",
      "2/223, train_loss: 0.1025, step time: 0.0997\n",
      "3/223, train_loss: 0.1117, step time: 0.1265\n",
      "4/223, train_loss: 0.1014, step time: 0.1156\n",
      "5/223, train_loss: 0.1006, step time: 0.1013\n",
      "6/223, train_loss: 0.1132, step time: 0.1211\n",
      "7/223, train_loss: 0.1027, step time: 0.1366\n",
      "8/223, train_loss: 0.0988, step time: 0.1155\n",
      "9/223, train_loss: 0.0944, step time: 0.1007\n",
      "10/223, train_loss: 0.1057, step time: 0.1568\n",
      "11/223, train_loss: 0.1057, step time: 0.1010\n",
      "12/223, train_loss: 0.0957, step time: 0.1004\n",
      "13/223, train_loss: 0.0988, step time: 0.1080\n",
      "14/223, train_loss: 0.1034, step time: 0.1282\n",
      "15/223, train_loss: 0.0947, step time: 0.1161\n",
      "16/223, train_loss: 0.1156, step time: 0.1068\n",
      "17/223, train_loss: 0.0941, step time: 0.1178\n",
      "18/223, train_loss: 0.1249, step time: 0.1213\n",
      "19/223, train_loss: 0.1005, step time: 0.1424\n",
      "20/223, train_loss: 0.1010, step time: 0.1010\n",
      "21/223, train_loss: 0.1020, step time: 0.1122\n",
      "22/223, train_loss: 0.1046, step time: 0.1235\n",
      "23/223, train_loss: 0.0921, step time: 0.1115\n",
      "24/223, train_loss: 0.1007, step time: 0.1332\n",
      "25/223, train_loss: 0.1111, step time: 0.1048\n",
      "26/223, train_loss: 0.1186, step time: 0.1016\n",
      "27/223, train_loss: 0.1095, step time: 0.1350\n",
      "28/223, train_loss: 0.1108, step time: 0.1295\n",
      "29/223, train_loss: 0.0969, step time: 0.1100\n",
      "30/223, train_loss: 0.1085, step time: 0.1528\n",
      "31/223, train_loss: 0.1139, step time: 0.1051\n",
      "32/223, train_loss: 0.1084, step time: 0.1039\n",
      "33/223, train_loss: 0.1058, step time: 0.1263\n",
      "34/223, train_loss: 0.1040, step time: 0.1163\n",
      "35/223, train_loss: 0.1000, step time: 0.1148\n",
      "36/223, train_loss: 0.1120, step time: 0.1008\n",
      "37/223, train_loss: 0.0952, step time: 0.0996\n",
      "38/223, train_loss: 0.1135, step time: 0.1063\n",
      "39/223, train_loss: 0.1042, step time: 0.1238\n",
      "40/223, train_loss: 0.0952, step time: 0.1069\n",
      "41/223, train_loss: 0.1132, step time: 0.1095\n",
      "42/223, train_loss: 0.1017, step time: 0.1056\n",
      "43/223, train_loss: 0.1161, step time: 0.1073\n",
      "44/223, train_loss: 0.1054, step time: 0.1247\n",
      "45/223, train_loss: 0.0979, step time: 0.1119\n",
      "46/223, train_loss: 0.1105, step time: 0.1153\n",
      "47/223, train_loss: 0.1093, step time: 0.1223\n",
      "48/223, train_loss: 0.1216, step time: 0.1048\n",
      "49/223, train_loss: 0.1001, step time: 0.1471\n",
      "50/223, train_loss: 0.1090, step time: 0.1064\n",
      "51/223, train_loss: 0.1100, step time: 0.1335\n",
      "52/223, train_loss: 0.1070, step time: 0.1155\n",
      "53/223, train_loss: 0.1188, step time: 0.1119\n",
      "54/223, train_loss: 0.1184, step time: 0.1278\n",
      "55/223, train_loss: 0.1069, step time: 0.1248\n",
      "56/223, train_loss: 0.1080, step time: 0.1092\n",
      "57/223, train_loss: 0.1107, step time: 0.1140\n",
      "58/223, train_loss: 0.1042, step time: 0.1524\n",
      "59/223, train_loss: 0.1146, step time: 0.1275\n",
      "60/223, train_loss: 0.1110, step time: 0.1089\n",
      "61/223, train_loss: 0.1229, step time: 0.1211\n",
      "62/223, train_loss: 0.0977, step time: 0.1135\n",
      "63/223, train_loss: 0.0999, step time: 0.1054\n",
      "64/223, train_loss: 0.1018, step time: 0.1161\n",
      "65/223, train_loss: 0.1088, step time: 0.1126\n",
      "66/223, train_loss: 0.1029, step time: 0.1266\n",
      "67/223, train_loss: 0.1079, step time: 0.1209\n",
      "68/223, train_loss: 0.1057, step time: 0.1086\n",
      "69/223, train_loss: 0.1155, step time: 0.1184\n",
      "70/223, train_loss: 0.1053, step time: 0.1203\n",
      "71/223, train_loss: 0.1218, step time: 0.1172\n",
      "72/223, train_loss: 0.0964, step time: 0.1064\n",
      "73/223, train_loss: 0.1125, step time: 0.1202\n",
      "74/223, train_loss: 0.1064, step time: 0.1183\n",
      "75/223, train_loss: 0.1200, step time: 0.1147\n",
      "76/223, train_loss: 0.1194, step time: 0.1029\n",
      "77/223, train_loss: 0.0997, step time: 0.1147\n",
      "78/223, train_loss: 0.1087, step time: 0.1273\n",
      "79/223, train_loss: 0.1018, step time: 0.1318\n",
      "80/223, train_loss: 0.1063, step time: 0.1004\n",
      "81/223, train_loss: 0.0924, step time: 0.1163\n",
      "82/223, train_loss: 0.1078, step time: 0.1026\n",
      "83/223, train_loss: 0.1012, step time: 0.1009\n",
      "84/223, train_loss: 0.1060, step time: 0.1088\n",
      "85/223, train_loss: 0.0963, step time: 0.1004\n",
      "86/223, train_loss: 0.0965, step time: 0.1222\n",
      "87/223, train_loss: 0.1050, step time: 0.1104\n",
      "88/223, train_loss: 0.1031, step time: 0.1167\n",
      "89/223, train_loss: 0.1052, step time: 0.0996\n",
      "90/223, train_loss: 0.1007, step time: 0.1043\n",
      "91/223, train_loss: 0.1093, step time: 0.1162\n",
      "92/223, train_loss: 0.1148, step time: 0.1080\n",
      "93/223, train_loss: 0.1108, step time: 0.1225\n",
      "94/223, train_loss: 0.1222, step time: 0.1060\n",
      "95/223, train_loss: 0.1001, step time: 0.1005\n",
      "96/223, train_loss: 0.1101, step time: 0.1323\n",
      "97/223, train_loss: 0.1212, step time: 0.1012\n",
      "98/223, train_loss: 0.1016, step time: 0.1008\n",
      "99/223, train_loss: 0.1203, step time: 0.1062\n",
      "100/223, train_loss: 0.1043, step time: 0.1003\n",
      "101/223, train_loss: 0.1213, step time: 0.1013\n",
      "102/223, train_loss: 0.0938, step time: 0.1399\n",
      "103/223, train_loss: 0.1001, step time: 0.1339\n",
      "104/223, train_loss: 0.0889, step time: 0.1224\n",
      "105/223, train_loss: 0.1092, step time: 0.1008\n",
      "106/223, train_loss: 0.0991, step time: 0.1006\n",
      "107/223, train_loss: 0.1130, step time: 0.1002\n",
      "108/223, train_loss: 0.1076, step time: 0.1000\n",
      "109/223, train_loss: 0.0982, step time: 0.1243\n",
      "110/223, train_loss: 0.1091, step time: 0.1002\n",
      "111/223, train_loss: 0.1062, step time: 0.1034\n",
      "112/223, train_loss: 0.1034, step time: 0.1394\n",
      "113/223, train_loss: 0.1032, step time: 0.1352\n",
      "114/223, train_loss: 0.1052, step time: 0.1014\n",
      "115/223, train_loss: 0.1027, step time: 0.0997\n",
      "116/223, train_loss: 0.1055, step time: 0.0996\n",
      "117/223, train_loss: 0.1030, step time: 0.1101\n",
      "118/223, train_loss: 0.0967, step time: 0.1016\n",
      "119/223, train_loss: 0.1096, step time: 0.1002\n",
      "120/223, train_loss: 0.1014, step time: 0.1001\n",
      "121/223, train_loss: 0.1074, step time: 0.1002\n",
      "122/223, train_loss: 0.1045, step time: 0.1138\n",
      "123/223, train_loss: 0.1131, step time: 0.1008\n",
      "124/223, train_loss: 0.1111, step time: 0.1122\n",
      "125/223, train_loss: 0.1046, step time: 0.1073\n",
      "126/223, train_loss: 0.1088, step time: 0.1074\n",
      "127/223, train_loss: 0.1192, step time: 0.1265\n",
      "128/223, train_loss: 0.1117, step time: 0.1137\n",
      "129/223, train_loss: 0.1012, step time: 0.1152\n",
      "130/223, train_loss: 0.0992, step time: 0.1089\n",
      "131/223, train_loss: 0.0924, step time: 0.1000\n",
      "132/223, train_loss: 0.0946, step time: 0.1010\n",
      "133/223, train_loss: 0.1178, step time: 0.1002\n",
      "134/223, train_loss: 0.1079, step time: 0.1057\n",
      "135/223, train_loss: 0.1059, step time: 0.1048\n",
      "136/223, train_loss: 0.0932, step time: 0.1155\n",
      "137/223, train_loss: 0.0969, step time: 0.1024\n",
      "138/223, train_loss: 0.0951, step time: 0.1185\n",
      "139/223, train_loss: 0.1088, step time: 0.1250\n",
      "140/223, train_loss: 0.1067, step time: 0.1040\n",
      "141/223, train_loss: 0.1000, step time: 0.1216\n",
      "142/223, train_loss: 0.1091, step time: 0.1094\n",
      "143/223, train_loss: 0.1023, step time: 0.1428\n",
      "144/223, train_loss: 0.1193, step time: 0.1209\n",
      "145/223, train_loss: 0.1155, step time: 0.1047\n",
      "146/223, train_loss: 0.0942, step time: 0.1120\n",
      "147/223, train_loss: 0.2973, step time: 0.1002\n",
      "148/223, train_loss: 0.0958, step time: 0.1125\n",
      "149/223, train_loss: 0.0947, step time: 0.1223\n",
      "150/223, train_loss: 0.1075, step time: 0.1002\n",
      "151/223, train_loss: 0.1026, step time: 0.1007\n",
      "152/223, train_loss: 0.1008, step time: 0.0993\n",
      "153/223, train_loss: 0.1164, step time: 0.1008\n",
      "154/223, train_loss: 0.0999, step time: 0.1003\n",
      "155/223, train_loss: 0.0957, step time: 0.1088\n",
      "156/223, train_loss: 0.0999, step time: 0.1001\n",
      "157/223, train_loss: 0.1044, step time: 0.0998\n",
      "158/223, train_loss: 0.1069, step time: 0.1002\n",
      "159/223, train_loss: 0.1012, step time: 0.1163\n",
      "160/223, train_loss: 0.0970, step time: 0.1143\n",
      "161/223, train_loss: 0.1151, step time: 0.1005\n",
      "162/223, train_loss: 0.0978, step time: 0.1055\n",
      "163/223, train_loss: 0.1043, step time: 0.1092\n",
      "164/223, train_loss: 0.1107, step time: 0.1027\n",
      "165/223, train_loss: 0.0956, step time: 0.1005\n",
      "166/223, train_loss: 0.0932, step time: 0.1147\n",
      "167/223, train_loss: 0.1073, step time: 0.1004\n",
      "168/223, train_loss: 0.1121, step time: 0.1002\n",
      "169/223, train_loss: 0.1010, step time: 0.1007\n",
      "170/223, train_loss: 0.1069, step time: 0.1000\n",
      "171/223, train_loss: 0.1015, step time: 0.1059\n",
      "172/223, train_loss: 0.1156, step time: 0.1242\n",
      "173/223, train_loss: 0.1060, step time: 0.1146\n",
      "174/223, train_loss: 0.1054, step time: 0.1000\n",
      "175/223, train_loss: 0.1153, step time: 0.1122\n",
      "176/223, train_loss: 0.0982, step time: 0.1095\n",
      "177/223, train_loss: 0.1026, step time: 0.1005\n",
      "178/223, train_loss: 0.1044, step time: 0.1131\n",
      "179/223, train_loss: 0.0987, step time: 0.1114\n",
      "180/223, train_loss: 0.1056, step time: 0.1005\n",
      "181/223, train_loss: 0.0990, step time: 0.1003\n",
      "182/223, train_loss: 0.1049, step time: 0.1009\n",
      "183/223, train_loss: 0.1031, step time: 0.1231\n",
      "184/223, train_loss: 0.1070, step time: 0.1071\n",
      "185/223, train_loss: 0.0980, step time: 0.1006\n",
      "186/223, train_loss: 0.1008, step time: 0.1001\n",
      "187/223, train_loss: 0.1036, step time: 0.1095\n",
      "188/223, train_loss: 0.1063, step time: 0.1219\n",
      "189/223, train_loss: 0.1092, step time: 0.1005\n",
      "190/223, train_loss: 0.1155, step time: 0.1001\n",
      "191/223, train_loss: 0.1162, step time: 0.1105\n",
      "192/223, train_loss: 0.1023, step time: 0.1072\n",
      "193/223, train_loss: 0.0969, step time: 0.1200\n",
      "194/223, train_loss: 0.1053, step time: 0.1000\n",
      "195/223, train_loss: 0.1027, step time: 0.1001\n",
      "196/223, train_loss: 0.0939, step time: 0.1150\n",
      "197/223, train_loss: 0.1004, step time: 0.1200\n",
      "198/223, train_loss: 0.1022, step time: 0.1160\n",
      "199/223, train_loss: 0.1114, step time: 0.1134\n",
      "200/223, train_loss: 0.1150, step time: 0.1104\n",
      "201/223, train_loss: 0.1094, step time: 0.0995\n",
      "202/223, train_loss: 0.1096, step time: 0.1078\n",
      "203/223, train_loss: 0.1153, step time: 0.1002\n",
      "204/223, train_loss: 0.1000, step time: 0.1054\n",
      "205/223, train_loss: 0.0953, step time: 0.1173\n",
      "206/223, train_loss: 0.1015, step time: 0.1008\n",
      "207/223, train_loss: 0.1086, step time: 0.1013\n",
      "208/223, train_loss: 0.1046, step time: 0.1005\n",
      "209/223, train_loss: 0.0986, step time: 0.1004\n",
      "210/223, train_loss: 0.1030, step time: 0.1135\n",
      "211/223, train_loss: 0.1146, step time: 0.1113\n",
      "212/223, train_loss: 0.1027, step time: 0.1003\n",
      "213/223, train_loss: 0.0969, step time: 0.1005\n",
      "214/223, train_loss: 0.1128, step time: 0.1060\n",
      "215/223, train_loss: 0.1093, step time: 0.1621\n",
      "216/223, train_loss: 0.1075, step time: 0.1269\n",
      "217/223, train_loss: 0.1052, step time: 0.1724\n",
      "218/223, train_loss: 0.0974, step time: 0.1010\n",
      "219/223, train_loss: 0.0992, step time: 0.1010\n",
      "220/223, train_loss: 0.0985, step time: 0.1005\n",
      "221/223, train_loss: 0.1154, step time: 0.1043\n",
      "222/223, train_loss: 0.1061, step time: 0.1010\n",
      "223/223, train_loss: 0.1018, step time: 0.1022\n",
      "epoch 159 average loss: 0.1063\n",
      "time consuming of epoch 159 is: 90.4629\n",
      "----------\n",
      "epoch 160/300\n",
      "1/223, train_loss: 0.1078, step time: 0.1076\n",
      "2/223, train_loss: 0.1161, step time: 0.1003\n",
      "3/223, train_loss: 0.1180, step time: 0.1001\n",
      "4/223, train_loss: 0.1007, step time: 0.1300\n",
      "5/223, train_loss: 0.1006, step time: 0.1071\n",
      "6/223, train_loss: 0.1098, step time: 0.1310\n",
      "7/223, train_loss: 0.1133, step time: 0.1304\n",
      "8/223, train_loss: 0.0975, step time: 0.1115\n",
      "9/223, train_loss: 0.1073, step time: 0.1226\n",
      "10/223, train_loss: 0.1051, step time: 0.1433\n",
      "11/223, train_loss: 0.1146, step time: 0.1052\n",
      "12/223, train_loss: 0.1000, step time: 0.1006\n",
      "13/223, train_loss: 0.1025, step time: 0.1042\n",
      "14/223, train_loss: 0.0927, step time: 0.1127\n",
      "15/223, train_loss: 0.1197, step time: 0.1084\n",
      "16/223, train_loss: 0.0960, step time: 0.1008\n",
      "17/223, train_loss: 0.1033, step time: 0.1171\n",
      "18/223, train_loss: 0.1163, step time: 0.1132\n",
      "19/223, train_loss: 0.1028, step time: 0.1408\n",
      "20/223, train_loss: 0.0982, step time: 0.1041\n",
      "21/223, train_loss: 0.1030, step time: 0.1080\n",
      "22/223, train_loss: 0.1013, step time: 0.1075\n",
      "23/223, train_loss: 0.0988, step time: 0.1004\n",
      "24/223, train_loss: 0.1038, step time: 0.1013\n",
      "25/223, train_loss: 0.0902, step time: 0.1144\n",
      "26/223, train_loss: 0.1027, step time: 0.1079\n",
      "27/223, train_loss: 0.0989, step time: 0.0996\n",
      "28/223, train_loss: 0.1163, step time: 0.1123\n",
      "29/223, train_loss: 0.1091, step time: 0.1120\n",
      "30/223, train_loss: 0.1071, step time: 0.1120\n",
      "31/223, train_loss: 0.1048, step time: 0.1284\n",
      "32/223, train_loss: 0.1072, step time: 0.1210\n",
      "33/223, train_loss: 0.1075, step time: 0.1270\n",
      "34/223, train_loss: 0.1033, step time: 0.1363\n",
      "35/223, train_loss: 0.1019, step time: 0.1009\n",
      "36/223, train_loss: 0.1006, step time: 0.1152\n",
      "37/223, train_loss: 0.1051, step time: 0.1144\n",
      "38/223, train_loss: 0.1056, step time: 0.1357\n",
      "39/223, train_loss: 0.1161, step time: 0.1204\n",
      "40/223, train_loss: 0.1005, step time: 0.1314\n",
      "41/223, train_loss: 0.0977, step time: 0.1244\n",
      "42/223, train_loss: 0.0949, step time: 0.1161\n",
      "43/223, train_loss: 0.1068, step time: 0.1254\n",
      "44/223, train_loss: 0.1109, step time: 0.1226\n",
      "45/223, train_loss: 0.1012, step time: 0.1176\n",
      "46/223, train_loss: 0.0996, step time: 0.1158\n",
      "47/223, train_loss: 0.1054, step time: 0.1207\n",
      "48/223, train_loss: 0.1054, step time: 0.1198\n",
      "49/223, train_loss: 0.1023, step time: 0.1187\n",
      "50/223, train_loss: 0.0970, step time: 0.1101\n",
      "51/223, train_loss: 0.1065, step time: 0.1153\n",
      "52/223, train_loss: 0.1080, step time: 0.1124\n",
      "53/223, train_loss: 0.1093, step time: 0.1001\n",
      "54/223, train_loss: 0.0952, step time: 0.1021\n",
      "55/223, train_loss: 0.1031, step time: 0.1160\n",
      "56/223, train_loss: 0.1109, step time: 0.1006\n",
      "57/223, train_loss: 0.1027, step time: 0.1232\n",
      "58/223, train_loss: 0.1012, step time: 0.1283\n",
      "59/223, train_loss: 0.1135, step time: 0.1041\n",
      "60/223, train_loss: 0.1182, step time: 0.1049\n",
      "61/223, train_loss: 0.1133, step time: 0.1044\n",
      "62/223, train_loss: 0.0940, step time: 0.1091\n",
      "63/223, train_loss: 0.1042, step time: 0.1129\n",
      "64/223, train_loss: 0.0939, step time: 0.1141\n",
      "65/223, train_loss: 0.1211, step time: 0.1043\n",
      "66/223, train_loss: 0.1132, step time: 0.1144\n",
      "67/223, train_loss: 0.0963, step time: 0.1046\n",
      "68/223, train_loss: 0.1129, step time: 0.1276\n",
      "69/223, train_loss: 0.1025, step time: 0.1085\n",
      "70/223, train_loss: 0.1057, step time: 0.1054\n",
      "71/223, train_loss: 0.0975, step time: 0.1146\n",
      "72/223, train_loss: 0.1119, step time: 0.1116\n",
      "73/223, train_loss: 0.1124, step time: 0.1188\n",
      "74/223, train_loss: 0.1021, step time: 0.0996\n",
      "75/223, train_loss: 0.1083, step time: 0.1147\n",
      "76/223, train_loss: 0.1047, step time: 0.1021\n",
      "77/223, train_loss: 0.1016, step time: 0.1193\n",
      "78/223, train_loss: 0.1044, step time: 0.1092\n",
      "79/223, train_loss: 0.1065, step time: 0.1209\n",
      "80/223, train_loss: 0.1002, step time: 0.1193\n",
      "81/223, train_loss: 0.1029, step time: 0.1459\n",
      "82/223, train_loss: 0.0972, step time: 0.1167\n",
      "83/223, train_loss: 0.1071, step time: 0.1208\n",
      "84/223, train_loss: 0.1031, step time: 0.1409\n",
      "85/223, train_loss: 0.1044, step time: 0.1002\n",
      "86/223, train_loss: 0.1085, step time: 0.0992\n",
      "87/223, train_loss: 0.1042, step time: 0.1001\n",
      "88/223, train_loss: 0.0963, step time: 0.1165\n",
      "89/223, train_loss: 0.1182, step time: 0.1003\n",
      "90/223, train_loss: 0.1061, step time: 0.1000\n",
      "91/223, train_loss: 0.1060, step time: 0.1003\n",
      "92/223, train_loss: 0.1048, step time: 0.1021\n",
      "93/223, train_loss: 0.1033, step time: 0.1010\n",
      "94/223, train_loss: 0.1071, step time: 0.1217\n",
      "95/223, train_loss: 0.0987, step time: 0.1002\n",
      "96/223, train_loss: 0.0995, step time: 0.1097\n",
      "97/223, train_loss: 0.1047, step time: 0.1055\n",
      "98/223, train_loss: 0.0960, step time: 0.1121\n",
      "99/223, train_loss: 0.1047, step time: 0.1304\n",
      "100/223, train_loss: 0.1031, step time: 0.1299\n",
      "101/223, train_loss: 0.1027, step time: 0.1154\n",
      "102/223, train_loss: 0.1045, step time: 0.1011\n",
      "103/223, train_loss: 0.1002, step time: 0.1242\n",
      "104/223, train_loss: 0.1091, step time: 0.1732\n",
      "105/223, train_loss: 0.0928, step time: 0.1001\n",
      "106/223, train_loss: 0.1127, step time: 0.1006\n",
      "107/223, train_loss: 0.1019, step time: 0.0999\n",
      "108/223, train_loss: 0.1009, step time: 0.1034\n",
      "109/223, train_loss: 0.1150, step time: 0.1161\n",
      "110/223, train_loss: 0.1009, step time: 0.1009\n",
      "111/223, train_loss: 0.0948, step time: 0.1001\n",
      "112/223, train_loss: 0.1160, step time: 0.1007\n",
      "113/223, train_loss: 0.1200, step time: 0.1125\n",
      "114/223, train_loss: 0.1012, step time: 0.1169\n",
      "115/223, train_loss: 0.1141, step time: 0.1049\n",
      "116/223, train_loss: 0.1011, step time: 0.1207\n",
      "117/223, train_loss: 0.1018, step time: 0.1126\n",
      "118/223, train_loss: 0.1003, step time: 0.1128\n",
      "119/223, train_loss: 0.1041, step time: 0.1181\n",
      "120/223, train_loss: 0.1128, step time: 0.1038\n",
      "121/223, train_loss: 0.1008, step time: 0.1074\n",
      "122/223, train_loss: 0.1118, step time: 0.1090\n",
      "123/223, train_loss: 0.0978, step time: 0.1173\n",
      "124/223, train_loss: 0.1002, step time: 0.1190\n",
      "125/223, train_loss: 0.1017, step time: 0.0992\n",
      "126/223, train_loss: 0.1099, step time: 0.0984\n",
      "127/223, train_loss: 0.0968, step time: 0.0987\n",
      "128/223, train_loss: 0.0975, step time: 0.0993\n",
      "129/223, train_loss: 0.1055, step time: 0.0986\n",
      "130/223, train_loss: 0.1179, step time: 0.1169\n",
      "131/223, train_loss: 0.1002, step time: 0.1005\n",
      "132/223, train_loss: 0.0946, step time: 0.1037\n",
      "133/223, train_loss: 0.1068, step time: 0.1148\n",
      "134/223, train_loss: 0.1048, step time: 0.1231\n",
      "135/223, train_loss: 0.1076, step time: 0.1219\n",
      "136/223, train_loss: 0.1062, step time: 0.1161\n",
      "137/223, train_loss: 0.1153, step time: 0.1135\n",
      "138/223, train_loss: 0.1033, step time: 0.1193\n",
      "139/223, train_loss: 0.1023, step time: 0.1002\n",
      "140/223, train_loss: 0.1008, step time: 0.1009\n",
      "141/223, train_loss: 0.1140, step time: 0.1188\n",
      "142/223, train_loss: 0.1002, step time: 0.1050\n",
      "143/223, train_loss: 0.1116, step time: 0.1312\n",
      "144/223, train_loss: 0.1076, step time: 0.1070\n",
      "145/223, train_loss: 0.1149, step time: 0.1133\n",
      "146/223, train_loss: 0.0993, step time: 0.1164\n",
      "147/223, train_loss: 0.0982, step time: 0.1000\n",
      "148/223, train_loss: 0.1055, step time: 0.1016\n",
      "149/223, train_loss: 0.1068, step time: 0.1014\n",
      "150/223, train_loss: 0.1108, step time: 0.1061\n",
      "151/223, train_loss: 0.1033, step time: 0.1241\n",
      "152/223, train_loss: 0.0999, step time: 0.1502\n",
      "153/223, train_loss: 0.1068, step time: 0.1132\n",
      "154/223, train_loss: 0.1036, step time: 0.1125\n",
      "155/223, train_loss: 0.1023, step time: 0.1093\n",
      "156/223, train_loss: 0.1144, step time: 0.1003\n",
      "157/223, train_loss: 0.1011, step time: 0.1019\n",
      "158/223, train_loss: 0.0985, step time: 0.1005\n",
      "159/223, train_loss: 0.1065, step time: 0.1012\n",
      "160/223, train_loss: 0.1122, step time: 0.1472\n",
      "161/223, train_loss: 0.1129, step time: 0.1042\n",
      "162/223, train_loss: 0.1118, step time: 0.1107\n",
      "163/223, train_loss: 0.0951, step time: 0.1057\n",
      "164/223, train_loss: 0.1174, step time: 0.1063\n",
      "165/223, train_loss: 0.1017, step time: 0.1011\n",
      "166/223, train_loss: 0.1044, step time: 0.1199\n",
      "167/223, train_loss: 0.1000, step time: 0.1078\n",
      "168/223, train_loss: 0.1032, step time: 0.1008\n",
      "169/223, train_loss: 0.1071, step time: 0.1009\n",
      "170/223, train_loss: 0.0968, step time: 0.1224\n",
      "171/223, train_loss: 0.1049, step time: 0.1515\n",
      "172/223, train_loss: 0.0969, step time: 0.1472\n",
      "173/223, train_loss: 0.0992, step time: 0.1175\n",
      "174/223, train_loss: 0.1112, step time: 0.1043\n",
      "175/223, train_loss: 0.1251, step time: 0.1477\n",
      "176/223, train_loss: 0.0985, step time: 0.1094\n",
      "177/223, train_loss: 0.1108, step time: 0.1252\n",
      "178/223, train_loss: 0.1140, step time: 0.1128\n",
      "179/223, train_loss: 0.1010, step time: 0.1099\n",
      "180/223, train_loss: 0.1018, step time: 0.1106\n",
      "181/223, train_loss: 0.1164, step time: 0.1004\n",
      "182/223, train_loss: 0.1070, step time: 0.1000\n",
      "183/223, train_loss: 0.0911, step time: 0.0999\n",
      "184/223, train_loss: 0.1117, step time: 0.1010\n",
      "185/223, train_loss: 0.0941, step time: 0.1048\n",
      "186/223, train_loss: 0.1014, step time: 0.1264\n",
      "187/223, train_loss: 0.1027, step time: 0.1046\n",
      "188/223, train_loss: 0.1015, step time: 0.1326\n",
      "189/223, train_loss: 0.1130, step time: 0.1040\n",
      "190/223, train_loss: 0.1042, step time: 0.1116\n",
      "191/223, train_loss: 0.1014, step time: 0.1184\n",
      "192/223, train_loss: 0.1237, step time: 0.1216\n",
      "193/223, train_loss: 0.1112, step time: 0.1005\n",
      "194/223, train_loss: 0.0996, step time: 0.1196\n",
      "195/223, train_loss: 0.0943, step time: 0.1179\n",
      "196/223, train_loss: 0.1034, step time: 0.1134\n",
      "197/223, train_loss: 0.1087, step time: 0.1122\n",
      "198/223, train_loss: 0.0976, step time: 0.1066\n",
      "199/223, train_loss: 0.0979, step time: 0.1168\n",
      "200/223, train_loss: 0.2964, step time: 0.1168\n",
      "201/223, train_loss: 0.1139, step time: 0.1221\n",
      "202/223, train_loss: 0.1113, step time: 0.1028\n",
      "203/223, train_loss: 0.1079, step time: 0.1054\n",
      "204/223, train_loss: 0.1010, step time: 0.1007\n",
      "205/223, train_loss: 0.0980, step time: 0.1017\n",
      "206/223, train_loss: 0.1088, step time: 0.1024\n",
      "207/223, train_loss: 0.0963, step time: 0.1157\n",
      "208/223, train_loss: 0.1086, step time: 0.1019\n",
      "209/223, train_loss: 0.1165, step time: 0.0996\n",
      "210/223, train_loss: 0.1147, step time: 0.1159\n",
      "211/223, train_loss: 0.1006, step time: 0.1720\n",
      "212/223, train_loss: 0.0934, step time: 0.1021\n",
      "213/223, train_loss: 0.1104, step time: 0.1036\n",
      "214/223, train_loss: 0.0984, step time: 0.0999\n",
      "215/223, train_loss: 0.1105, step time: 0.1051\n",
      "216/223, train_loss: 0.1202, step time: 0.1003\n",
      "217/223, train_loss: 0.1163, step time: 0.1007\n",
      "218/223, train_loss: 0.1025, step time: 0.1050\n",
      "219/223, train_loss: 0.1125, step time: 0.1004\n",
      "220/223, train_loss: 0.1025, step time: 0.1023\n",
      "221/223, train_loss: 0.1103, step time: 0.1025\n",
      "222/223, train_loss: 0.0991, step time: 0.0995\n",
      "223/223, train_loss: 0.1132, step time: 0.0997\n",
      "epoch 160 average loss: 0.1060\n",
      "current epoch: 160 current mean dice: 0.8573 tc: 0.9198 wt: 0.8667 et: 0.7854\n",
      "best mean dice: 0.8578 at epoch: 155\n",
      "time consuming of epoch 160 is: 94.7039\n",
      "----------\n",
      "epoch 161/300\n",
      "1/223, train_loss: 0.1010, step time: 0.1012\n",
      "2/223, train_loss: 0.1045, step time: 0.1137\n",
      "3/223, train_loss: 0.1033, step time: 0.1005\n",
      "4/223, train_loss: 0.1221, step time: 0.1011\n",
      "5/223, train_loss: 0.1022, step time: 0.1182\n",
      "6/223, train_loss: 0.0943, step time: 0.1230\n",
      "7/223, train_loss: 0.0991, step time: 0.1017\n",
      "8/223, train_loss: 0.1003, step time: 0.1003\n",
      "9/223, train_loss: 0.0962, step time: 0.1207\n",
      "10/223, train_loss: 0.0969, step time: 0.1213\n",
      "11/223, train_loss: 0.1050, step time: 0.1091\n",
      "12/223, train_loss: 0.1027, step time: 0.1006\n",
      "13/223, train_loss: 0.1018, step time: 0.1048\n",
      "14/223, train_loss: 0.0895, step time: 0.1122\n",
      "15/223, train_loss: 0.0975, step time: 0.1114\n",
      "16/223, train_loss: 0.0966, step time: 0.1159\n",
      "17/223, train_loss: 0.1024, step time: 0.0996\n",
      "18/223, train_loss: 0.1016, step time: 0.1142\n",
      "19/223, train_loss: 0.1053, step time: 0.1000\n",
      "20/223, train_loss: 0.1147, step time: 0.1017\n",
      "21/223, train_loss: 0.1237, step time: 0.1341\n",
      "22/223, train_loss: 0.1238, step time: 0.1240\n",
      "23/223, train_loss: 0.0909, step time: 0.1318\n",
      "24/223, train_loss: 0.1082, step time: 0.1004\n",
      "25/223, train_loss: 0.0927, step time: 0.1145\n",
      "26/223, train_loss: 0.1149, step time: 0.0996\n",
      "27/223, train_loss: 0.1053, step time: 0.0999\n",
      "28/223, train_loss: 0.1075, step time: 0.1003\n",
      "29/223, train_loss: 0.0975, step time: 0.1017\n",
      "30/223, train_loss: 0.0946, step time: 0.1068\n",
      "31/223, train_loss: 0.1010, step time: 0.1086\n",
      "32/223, train_loss: 0.1009, step time: 0.0995\n",
      "33/223, train_loss: 0.0993, step time: 0.1106\n",
      "34/223, train_loss: 0.1200, step time: 0.1216\n",
      "35/223, train_loss: 0.0992, step time: 0.1001\n",
      "36/223, train_loss: 0.0969, step time: 0.1008\n",
      "37/223, train_loss: 0.1132, step time: 0.1108\n",
      "38/223, train_loss: 0.0952, step time: 0.0998\n",
      "39/223, train_loss: 0.1145, step time: 0.1000\n",
      "40/223, train_loss: 0.1092, step time: 0.1155\n",
      "41/223, train_loss: 0.1023, step time: 0.1175\n",
      "42/223, train_loss: 0.1044, step time: 0.1166\n",
      "43/223, train_loss: 0.1054, step time: 0.0995\n",
      "44/223, train_loss: 0.1112, step time: 0.1049\n",
      "45/223, train_loss: 0.1151, step time: 0.1031\n",
      "46/223, train_loss: 0.1126, step time: 0.0998\n",
      "47/223, train_loss: 0.0959, step time: 0.1103\n",
      "48/223, train_loss: 0.1173, step time: 0.1006\n",
      "49/223, train_loss: 0.0946, step time: 0.1001\n",
      "50/223, train_loss: 0.1093, step time: 0.0995\n",
      "51/223, train_loss: 0.1065, step time: 0.1017\n",
      "52/223, train_loss: 0.1115, step time: 0.1020\n",
      "53/223, train_loss: 0.1081, step time: 0.1110\n",
      "54/223, train_loss: 0.1005, step time: 0.1119\n",
      "55/223, train_loss: 0.0922, step time: 0.1059\n",
      "56/223, train_loss: 0.1030, step time: 0.1006\n",
      "57/223, train_loss: 0.1075, step time: 0.1130\n",
      "58/223, train_loss: 0.1006, step time: 0.1183\n",
      "59/223, train_loss: 0.1119, step time: 0.1008\n",
      "60/223, train_loss: 0.1048, step time: 0.0993\n",
      "61/223, train_loss: 0.1141, step time: 0.1206\n",
      "62/223, train_loss: 0.1096, step time: 0.1112\n",
      "63/223, train_loss: 0.0925, step time: 0.1215\n",
      "64/223, train_loss: 0.0989, step time: 0.1002\n",
      "65/223, train_loss: 0.1146, step time: 0.1229\n",
      "66/223, train_loss: 0.1034, step time: 0.1556\n",
      "67/223, train_loss: 0.1056, step time: 0.1254\n",
      "68/223, train_loss: 0.1176, step time: 0.1001\n",
      "69/223, train_loss: 0.1141, step time: 0.1004\n",
      "70/223, train_loss: 0.1044, step time: 0.1013\n",
      "71/223, train_loss: 0.1071, step time: 0.1008\n",
      "72/223, train_loss: 0.0998, step time: 0.1010\n",
      "73/223, train_loss: 0.1081, step time: 0.1226\n",
      "74/223, train_loss: 0.1155, step time: 0.1014\n",
      "75/223, train_loss: 0.0987, step time: 0.1089\n",
      "76/223, train_loss: 0.1095, step time: 0.1002\n",
      "77/223, train_loss: 0.1111, step time: 0.1205\n",
      "78/223, train_loss: 0.1068, step time: 0.1007\n",
      "79/223, train_loss: 0.1031, step time: 0.1010\n",
      "80/223, train_loss: 0.0976, step time: 0.1088\n",
      "81/223, train_loss: 0.0919, step time: 0.1047\n",
      "82/223, train_loss: 0.1119, step time: 0.1129\n",
      "83/223, train_loss: 0.1001, step time: 0.1137\n",
      "84/223, train_loss: 0.1122, step time: 0.1370\n",
      "85/223, train_loss: 0.1175, step time: 0.1105\n",
      "86/223, train_loss: 0.1173, step time: 0.1031\n",
      "87/223, train_loss: 0.1160, step time: 0.1278\n",
      "88/223, train_loss: 0.1217, step time: 0.1080\n",
      "89/223, train_loss: 0.1118, step time: 0.1212\n",
      "90/223, train_loss: 0.1070, step time: 0.1163\n",
      "91/223, train_loss: 0.1087, step time: 0.1197\n",
      "92/223, train_loss: 0.1090, step time: 0.0997\n",
      "93/223, train_loss: 0.0969, step time: 0.1036\n",
      "94/223, train_loss: 0.1018, step time: 0.1149\n",
      "95/223, train_loss: 0.1154, step time: 0.1117\n",
      "96/223, train_loss: 0.1031, step time: 0.1120\n",
      "97/223, train_loss: 0.1052, step time: 0.1004\n",
      "98/223, train_loss: 0.1068, step time: 0.1006\n",
      "99/223, train_loss: 0.1115, step time: 0.1001\n",
      "100/223, train_loss: 0.1116, step time: 0.1021\n",
      "101/223, train_loss: 0.1036, step time: 0.1007\n",
      "102/223, train_loss: 0.1096, step time: 0.1209\n",
      "103/223, train_loss: 0.0924, step time: 0.1125\n",
      "104/223, train_loss: 0.0989, step time: 0.1213\n",
      "105/223, train_loss: 0.0957, step time: 0.1000\n",
      "106/223, train_loss: 0.1079, step time: 0.1028\n",
      "107/223, train_loss: 0.0990, step time: 0.1076\n",
      "108/223, train_loss: 0.1012, step time: 0.1055\n",
      "109/223, train_loss: 0.0992, step time: 0.1309\n",
      "110/223, train_loss: 0.1018, step time: 0.1079\n",
      "111/223, train_loss: 0.0982, step time: 0.1116\n",
      "112/223, train_loss: 0.0982, step time: 0.1076\n",
      "113/223, train_loss: 0.1135, step time: 0.1129\n",
      "114/223, train_loss: 0.1014, step time: 0.1164\n",
      "115/223, train_loss: 0.1000, step time: 0.1086\n",
      "116/223, train_loss: 0.1015, step time: 0.1030\n",
      "117/223, train_loss: 0.0950, step time: 0.1070\n",
      "118/223, train_loss: 0.1016, step time: 0.1006\n",
      "119/223, train_loss: 0.1140, step time: 0.1017\n",
      "120/223, train_loss: 0.1142, step time: 0.1050\n",
      "121/223, train_loss: 0.1020, step time: 0.1070\n",
      "122/223, train_loss: 0.0986, step time: 0.1075\n",
      "123/223, train_loss: 0.1126, step time: 0.1156\n",
      "124/223, train_loss: 0.1049, step time: 0.1094\n",
      "125/223, train_loss: 0.1119, step time: 0.1152\n",
      "126/223, train_loss: 0.1097, step time: 0.1180\n",
      "127/223, train_loss: 0.1093, step time: 0.1081\n",
      "128/223, train_loss: 0.0925, step time: 0.1091\n",
      "129/223, train_loss: 0.1149, step time: 0.1155\n",
      "130/223, train_loss: 0.1033, step time: 0.1154\n",
      "131/223, train_loss: 0.1074, step time: 0.1104\n",
      "132/223, train_loss: 0.1066, step time: 0.1012\n",
      "133/223, train_loss: 0.3036, step time: 0.1147\n",
      "134/223, train_loss: 0.1017, step time: 0.1222\n",
      "135/223, train_loss: 0.1157, step time: 0.1022\n",
      "136/223, train_loss: 0.1037, step time: 0.1006\n",
      "137/223, train_loss: 0.0986, step time: 0.1268\n",
      "138/223, train_loss: 0.1099, step time: 0.1037\n",
      "139/223, train_loss: 0.1011, step time: 0.1072\n",
      "140/223, train_loss: 0.1056, step time: 0.1017\n",
      "141/223, train_loss: 0.1123, step time: 0.1135\n",
      "142/223, train_loss: 0.0980, step time: 0.1069\n",
      "143/223, train_loss: 0.0987, step time: 0.1034\n",
      "144/223, train_loss: 0.1038, step time: 0.0991\n",
      "145/223, train_loss: 0.1080, step time: 0.1146\n",
      "146/223, train_loss: 0.1055, step time: 0.1264\n",
      "147/223, train_loss: 0.0987, step time: 0.1002\n",
      "148/223, train_loss: 0.1016, step time: 0.0997\n",
      "149/223, train_loss: 0.1046, step time: 0.1086\n",
      "150/223, train_loss: 0.1080, step time: 0.1097\n",
      "151/223, train_loss: 0.1203, step time: 0.1002\n",
      "152/223, train_loss: 0.1055, step time: 0.1101\n",
      "153/223, train_loss: 0.1035, step time: 0.1071\n",
      "154/223, train_loss: 0.0955, step time: 0.0997\n",
      "155/223, train_loss: 0.1099, step time: 0.1006\n",
      "156/223, train_loss: 0.1039, step time: 0.1027\n",
      "157/223, train_loss: 0.1027, step time: 0.1108\n",
      "158/223, train_loss: 0.1127, step time: 0.1112\n",
      "159/223, train_loss: 0.0960, step time: 0.1104\n",
      "160/223, train_loss: 0.1000, step time: 0.1254\n",
      "161/223, train_loss: 0.1125, step time: 0.1082\n",
      "162/223, train_loss: 0.1073, step time: 0.1178\n",
      "163/223, train_loss: 0.1039, step time: 0.1028\n",
      "164/223, train_loss: 0.1056, step time: 0.1003\n",
      "165/223, train_loss: 0.1004, step time: 0.1145\n",
      "166/223, train_loss: 0.1003, step time: 0.1038\n",
      "167/223, train_loss: 0.1067, step time: 0.1218\n",
      "168/223, train_loss: 0.1180, step time: 0.1021\n",
      "169/223, train_loss: 0.0982, step time: 0.1109\n",
      "170/223, train_loss: 0.1138, step time: 0.1165\n",
      "171/223, train_loss: 0.1102, step time: 0.1158\n",
      "172/223, train_loss: 0.1035, step time: 0.1054\n",
      "173/223, train_loss: 0.1162, step time: 0.1269\n",
      "174/223, train_loss: 0.1114, step time: 0.1483\n",
      "175/223, train_loss: 0.1042, step time: 0.1244\n",
      "176/223, train_loss: 0.0967, step time: 0.1027\n",
      "177/223, train_loss: 0.0958, step time: 0.0995\n",
      "178/223, train_loss: 0.1001, step time: 0.0995\n",
      "179/223, train_loss: 0.1036, step time: 0.0995\n",
      "180/223, train_loss: 0.0986, step time: 0.1001\n",
      "181/223, train_loss: 0.1115, step time: 0.1000\n",
      "182/223, train_loss: 0.1040, step time: 0.1004\n",
      "183/223, train_loss: 0.1028, step time: 0.1001\n",
      "184/223, train_loss: 0.1052, step time: 0.1327\n",
      "185/223, train_loss: 0.1081, step time: 0.0990\n",
      "186/223, train_loss: 0.0980, step time: 0.1110\n",
      "187/223, train_loss: 0.1028, step time: 0.1272\n",
      "188/223, train_loss: 0.1208, step time: 0.1012\n",
      "189/223, train_loss: 0.1036, step time: 0.1048\n",
      "190/223, train_loss: 0.0968, step time: 0.1074\n",
      "191/223, train_loss: 0.1090, step time: 0.1007\n",
      "192/223, train_loss: 0.1094, step time: 0.1161\n",
      "193/223, train_loss: 0.0939, step time: 0.1092\n",
      "194/223, train_loss: 0.1006, step time: 0.1070\n",
      "195/223, train_loss: 0.1040, step time: 0.1143\n",
      "196/223, train_loss: 0.1205, step time: 0.1039\n",
      "197/223, train_loss: 0.1086, step time: 0.1176\n",
      "198/223, train_loss: 0.1007, step time: 0.1180\n",
      "199/223, train_loss: 0.0994, step time: 0.0990\n",
      "200/223, train_loss: 0.0956, step time: 0.0986\n",
      "201/223, train_loss: 0.1052, step time: 0.1066\n",
      "202/223, train_loss: 0.1140, step time: 0.1283\n",
      "203/223, train_loss: 0.0996, step time: 0.0999\n",
      "204/223, train_loss: 0.1013, step time: 0.1010\n",
      "205/223, train_loss: 0.1121, step time: 0.1158\n",
      "206/223, train_loss: 0.0973, step time: 0.0998\n",
      "207/223, train_loss: 0.1089, step time: 0.1356\n",
      "208/223, train_loss: 0.1037, step time: 0.0991\n",
      "209/223, train_loss: 0.0963, step time: 0.1319\n",
      "210/223, train_loss: 0.1075, step time: 0.1312\n",
      "211/223, train_loss: 0.1027, step time: 0.1293\n",
      "212/223, train_loss: 0.1069, step time: 0.1174\n",
      "213/223, train_loss: 0.1071, step time: 0.1018\n",
      "214/223, train_loss: 0.1130, step time: 0.1067\n",
      "215/223, train_loss: 0.1102, step time: 0.1034\n",
      "216/223, train_loss: 0.1117, step time: 0.1106\n",
      "217/223, train_loss: 0.1030, step time: 0.1010\n",
      "218/223, train_loss: 0.1012, step time: 0.1092\n",
      "219/223, train_loss: 0.1002, step time: 0.1007\n",
      "220/223, train_loss: 0.1078, step time: 0.1052\n",
      "221/223, train_loss: 0.1208, step time: 0.0991\n",
      "222/223, train_loss: 0.0999, step time: 0.0988\n",
      "223/223, train_loss: 0.1044, step time: 0.1003\n",
      "epoch 161 average loss: 0.1061\n",
      "time consuming of epoch 161 is: 89.5399\n",
      "----------\n",
      "epoch 162/300\n",
      "1/223, train_loss: 0.1003, step time: 0.1019\n",
      "2/223, train_loss: 0.0987, step time: 0.1069\n",
      "3/223, train_loss: 0.0937, step time: 0.1034\n",
      "4/223, train_loss: 0.1111, step time: 0.1009\n",
      "5/223, train_loss: 0.0983, step time: 0.1056\n",
      "6/223, train_loss: 0.1023, step time: 0.1656\n",
      "7/223, train_loss: 0.0943, step time: 0.1010\n",
      "8/223, train_loss: 0.1145, step time: 0.1014\n",
      "9/223, train_loss: 0.1128, step time: 0.1263\n",
      "10/223, train_loss: 0.1055, step time: 0.1119\n",
      "11/223, train_loss: 0.1249, step time: 0.1178\n",
      "12/223, train_loss: 0.0947, step time: 0.1167\n",
      "13/223, train_loss: 0.3048, step time: 0.1185\n",
      "14/223, train_loss: 0.0964, step time: 0.1135\n",
      "15/223, train_loss: 0.1153, step time: 0.0998\n",
      "16/223, train_loss: 0.0998, step time: 0.1175\n",
      "17/223, train_loss: 0.1009, step time: 0.1099\n",
      "18/223, train_loss: 0.1013, step time: 0.1189\n",
      "19/223, train_loss: 0.1014, step time: 0.1154\n",
      "20/223, train_loss: 0.0985, step time: 0.1004\n",
      "21/223, train_loss: 0.1139, step time: 0.1128\n",
      "22/223, train_loss: 0.1102, step time: 0.1112\n",
      "23/223, train_loss: 0.1123, step time: 0.1053\n",
      "24/223, train_loss: 0.1125, step time: 0.1249\n",
      "25/223, train_loss: 0.1163, step time: 0.1146\n",
      "26/223, train_loss: 0.1233, step time: 0.1064\n",
      "27/223, train_loss: 0.1086, step time: 0.1160\n",
      "28/223, train_loss: 0.1145, step time: 0.1377\n",
      "29/223, train_loss: 0.1059, step time: 0.1135\n",
      "30/223, train_loss: 0.1039, step time: 0.1093\n",
      "31/223, train_loss: 0.1178, step time: 0.1313\n",
      "32/223, train_loss: 0.0977, step time: 0.1131\n",
      "33/223, train_loss: 0.1065, step time: 0.1126\n",
      "34/223, train_loss: 0.1008, step time: 0.1149\n",
      "35/223, train_loss: 0.1196, step time: 0.1161\n",
      "36/223, train_loss: 0.1061, step time: 0.1292\n",
      "37/223, train_loss: 0.1070, step time: 0.1172\n",
      "38/223, train_loss: 0.1108, step time: 0.1177\n",
      "39/223, train_loss: 0.1122, step time: 0.1010\n",
      "40/223, train_loss: 0.1032, step time: 0.1053\n",
      "41/223, train_loss: 0.0945, step time: 0.1099\n",
      "42/223, train_loss: 0.1080, step time: 0.1081\n",
      "43/223, train_loss: 0.1076, step time: 0.1175\n",
      "44/223, train_loss: 0.1002, step time: 0.1121\n",
      "45/223, train_loss: 0.1067, step time: 0.1078\n",
      "46/223, train_loss: 0.1158, step time: 0.1171\n",
      "47/223, train_loss: 0.1007, step time: 0.1162\n",
      "48/223, train_loss: 0.1061, step time: 0.1173\n",
      "49/223, train_loss: 0.1047, step time: 0.1152\n",
      "50/223, train_loss: 0.1076, step time: 0.1019\n",
      "51/223, train_loss: 0.1142, step time: 0.1279\n",
      "52/223, train_loss: 0.1023, step time: 0.1002\n",
      "53/223, train_loss: 0.1063, step time: 0.1173\n",
      "54/223, train_loss: 0.1006, step time: 0.1488\n",
      "55/223, train_loss: 0.0940, step time: 0.0997\n",
      "56/223, train_loss: 0.0935, step time: 0.0996\n",
      "57/223, train_loss: 0.1174, step time: 0.1011\n",
      "58/223, train_loss: 0.0980, step time: 0.1002\n",
      "59/223, train_loss: 0.1052, step time: 0.1007\n",
      "60/223, train_loss: 0.1028, step time: 0.1173\n",
      "61/223, train_loss: 0.0990, step time: 0.0993\n",
      "62/223, train_loss: 0.1005, step time: 0.1427\n",
      "63/223, train_loss: 0.1148, step time: 0.1047\n",
      "64/223, train_loss: 0.1023, step time: 0.1035\n",
      "65/223, train_loss: 0.1025, step time: 0.1097\n",
      "66/223, train_loss: 0.0915, step time: 0.1356\n",
      "67/223, train_loss: 0.1054, step time: 0.1056\n",
      "68/223, train_loss: 0.1055, step time: 0.1216\n",
      "69/223, train_loss: 0.1160, step time: 0.1092\n",
      "70/223, train_loss: 0.0948, step time: 0.1003\n",
      "71/223, train_loss: 0.1000, step time: 0.1003\n",
      "72/223, train_loss: 0.1248, step time: 0.1078\n",
      "73/223, train_loss: 0.1083, step time: 0.1006\n",
      "74/223, train_loss: 0.0924, step time: 0.1082\n",
      "75/223, train_loss: 0.1092, step time: 0.0990\n",
      "76/223, train_loss: 0.1027, step time: 0.0986\n",
      "77/223, train_loss: 0.1074, step time: 0.1110\n",
      "78/223, train_loss: 0.1086, step time: 0.1029\n",
      "79/223, train_loss: 0.1028, step time: 0.1322\n",
      "80/223, train_loss: 0.0988, step time: 0.1091\n",
      "81/223, train_loss: 0.1063, step time: 0.1101\n",
      "82/223, train_loss: 0.0959, step time: 0.1233\n",
      "83/223, train_loss: 0.1008, step time: 0.1100\n",
      "84/223, train_loss: 0.1018, step time: 0.1016\n",
      "85/223, train_loss: 0.1016, step time: 0.0998\n",
      "86/223, train_loss: 0.1103, step time: 0.1013\n",
      "87/223, train_loss: 0.1189, step time: 0.1080\n",
      "88/223, train_loss: 0.1002, step time: 0.1007\n",
      "89/223, train_loss: 0.0989, step time: 0.1075\n",
      "90/223, train_loss: 0.0981, step time: 0.1005\n",
      "91/223, train_loss: 0.1113, step time: 0.1031\n",
      "92/223, train_loss: 0.1101, step time: 0.0996\n",
      "93/223, train_loss: 0.1157, step time: 0.1084\n",
      "94/223, train_loss: 0.1094, step time: 0.1110\n",
      "95/223, train_loss: 0.1040, step time: 0.1043\n",
      "96/223, train_loss: 0.1094, step time: 0.1026\n",
      "97/223, train_loss: 0.1021, step time: 0.1185\n",
      "98/223, train_loss: 0.1024, step time: 0.1152\n",
      "99/223, train_loss: 0.0959, step time: 0.1141\n",
      "100/223, train_loss: 0.1004, step time: 0.1076\n",
      "101/223, train_loss: 0.1001, step time: 0.1170\n",
      "102/223, train_loss: 0.1015, step time: 0.1099\n",
      "103/223, train_loss: 0.0930, step time: 0.1094\n",
      "104/223, train_loss: 0.1105, step time: 0.1118\n",
      "105/223, train_loss: 0.1100, step time: 0.1083\n",
      "106/223, train_loss: 0.1042, step time: 0.1502\n",
      "107/223, train_loss: 0.1013, step time: 0.1007\n",
      "108/223, train_loss: 0.1155, step time: 0.1009\n",
      "109/223, train_loss: 0.1053, step time: 0.1050\n",
      "110/223, train_loss: 0.1063, step time: 0.1004\n",
      "111/223, train_loss: 0.1013, step time: 0.1014\n",
      "112/223, train_loss: 0.1074, step time: 0.1201\n",
      "113/223, train_loss: 0.0971, step time: 0.1110\n",
      "114/223, train_loss: 0.1013, step time: 0.1081\n",
      "115/223, train_loss: 0.1004, step time: 0.1155\n",
      "116/223, train_loss: 0.1079, step time: 0.1011\n",
      "117/223, train_loss: 0.1057, step time: 0.1169\n",
      "118/223, train_loss: 0.0971, step time: 0.1424\n",
      "119/223, train_loss: 0.0985, step time: 0.1231\n",
      "120/223, train_loss: 0.1062, step time: 0.1056\n",
      "121/223, train_loss: 0.1169, step time: 0.1119\n",
      "122/223, train_loss: 0.0962, step time: 0.0997\n",
      "123/223, train_loss: 0.1063, step time: 0.1002\n",
      "124/223, train_loss: 0.1055, step time: 0.1004\n",
      "125/223, train_loss: 0.1067, step time: 0.1095\n",
      "126/223, train_loss: 0.1134, step time: 0.1191\n",
      "127/223, train_loss: 0.1057, step time: 0.1087\n",
      "128/223, train_loss: 0.1006, step time: 0.1153\n",
      "129/223, train_loss: 0.1058, step time: 0.1146\n",
      "130/223, train_loss: 0.1103, step time: 0.0997\n",
      "131/223, train_loss: 0.1105, step time: 0.1003\n",
      "132/223, train_loss: 0.1061, step time: 0.1250\n",
      "133/223, train_loss: 0.1104, step time: 0.1154\n",
      "134/223, train_loss: 0.0959, step time: 0.1179\n",
      "135/223, train_loss: 0.1101, step time: 0.1199\n",
      "136/223, train_loss: 0.1066, step time: 0.1010\n",
      "137/223, train_loss: 0.1163, step time: 0.1111\n",
      "138/223, train_loss: 0.1055, step time: 0.1023\n",
      "139/223, train_loss: 0.1083, step time: 0.1003\n",
      "140/223, train_loss: 0.1035, step time: 0.1108\n",
      "141/223, train_loss: 0.0957, step time: 0.1283\n",
      "142/223, train_loss: 0.1079, step time: 0.1041\n",
      "143/223, train_loss: 0.1036, step time: 0.1070\n",
      "144/223, train_loss: 0.1060, step time: 0.1025\n",
      "145/223, train_loss: 0.0999, step time: 0.1072\n",
      "146/223, train_loss: 0.1215, step time: 0.1117\n",
      "147/223, train_loss: 0.0964, step time: 0.1369\n",
      "148/223, train_loss: 0.1141, step time: 0.1114\n",
      "149/223, train_loss: 0.1155, step time: 0.1095\n",
      "150/223, train_loss: 0.0966, step time: 0.1003\n",
      "151/223, train_loss: 0.0956, step time: 0.0998\n",
      "152/223, train_loss: 0.1067, step time: 0.1066\n",
      "153/223, train_loss: 0.1095, step time: 0.1115\n",
      "154/223, train_loss: 0.1037, step time: 0.1320\n",
      "155/223, train_loss: 0.0940, step time: 0.1045\n",
      "156/223, train_loss: 0.0980, step time: 0.1000\n",
      "157/223, train_loss: 0.1115, step time: 0.1014\n",
      "158/223, train_loss: 0.0986, step time: 0.1508\n",
      "159/223, train_loss: 0.1133, step time: 0.1329\n",
      "160/223, train_loss: 0.0974, step time: 0.1144\n",
      "161/223, train_loss: 0.1040, step time: 0.1191\n",
      "162/223, train_loss: 0.1084, step time: 0.1082\n",
      "163/223, train_loss: 0.1132, step time: 0.1048\n",
      "164/223, train_loss: 0.1042, step time: 0.1000\n",
      "165/223, train_loss: 0.1076, step time: 0.1179\n",
      "166/223, train_loss: 0.1097, step time: 0.1010\n",
      "167/223, train_loss: 0.0983, step time: 0.1000\n",
      "168/223, train_loss: 0.1011, step time: 0.1050\n",
      "169/223, train_loss: 0.1008, step time: 0.1129\n",
      "170/223, train_loss: 0.1161, step time: 0.1126\n",
      "171/223, train_loss: 0.1009, step time: 0.1152\n",
      "172/223, train_loss: 0.1030, step time: 0.1019\n",
      "173/223, train_loss: 0.0888, step time: 0.1041\n",
      "174/223, train_loss: 0.0953, step time: 0.1167\n",
      "175/223, train_loss: 0.1037, step time: 0.1174\n",
      "176/223, train_loss: 0.1034, step time: 0.1308\n",
      "177/223, train_loss: 0.1147, step time: 0.1258\n",
      "178/223, train_loss: 0.1092, step time: 0.1009\n",
      "179/223, train_loss: 0.1131, step time: 0.1015\n",
      "180/223, train_loss: 0.1041, step time: 0.1105\n",
      "181/223, train_loss: 0.0966, step time: 0.1145\n",
      "182/223, train_loss: 0.1010, step time: 0.1174\n",
      "183/223, train_loss: 0.1074, step time: 0.1005\n",
      "184/223, train_loss: 0.1033, step time: 0.1110\n",
      "185/223, train_loss: 0.1114, step time: 0.1127\n",
      "186/223, train_loss: 0.1063, step time: 0.1004\n",
      "187/223, train_loss: 0.0950, step time: 0.1358\n",
      "188/223, train_loss: 0.1030, step time: 0.1158\n",
      "189/223, train_loss: 0.0986, step time: 0.1114\n",
      "190/223, train_loss: 0.1043, step time: 0.1201\n",
      "191/223, train_loss: 0.1008, step time: 0.1098\n",
      "192/223, train_loss: 0.1092, step time: 0.1064\n",
      "193/223, train_loss: 0.0961, step time: 0.1004\n",
      "194/223, train_loss: 0.1059, step time: 0.1274\n",
      "195/223, train_loss: 0.0976, step time: 0.1567\n",
      "196/223, train_loss: 0.1002, step time: 0.1191\n",
      "197/223, train_loss: 0.1023, step time: 0.1042\n",
      "198/223, train_loss: 0.1073, step time: 0.1363\n",
      "199/223, train_loss: 0.0948, step time: 0.1121\n",
      "200/223, train_loss: 0.1153, step time: 0.1178\n",
      "201/223, train_loss: 0.0916, step time: 0.1088\n",
      "202/223, train_loss: 0.1029, step time: 0.1530\n",
      "203/223, train_loss: 0.1119, step time: 0.1255\n",
      "204/223, train_loss: 0.1008, step time: 0.1141\n",
      "205/223, train_loss: 0.0996, step time: 0.1053\n",
      "206/223, train_loss: 0.1038, step time: 0.1338\n",
      "207/223, train_loss: 0.1073, step time: 0.1653\n",
      "208/223, train_loss: 0.1145, step time: 0.1027\n",
      "209/223, train_loss: 0.0975, step time: 0.1222\n",
      "210/223, train_loss: 0.0957, step time: 0.0995\n",
      "211/223, train_loss: 0.1039, step time: 0.1092\n",
      "212/223, train_loss: 0.1047, step time: 0.1017\n",
      "213/223, train_loss: 0.1052, step time: 0.1248\n",
      "214/223, train_loss: 0.1076, step time: 0.1089\n",
      "215/223, train_loss: 0.1189, step time: 0.1102\n",
      "216/223, train_loss: 0.1038, step time: 0.1055\n",
      "217/223, train_loss: 0.0945, step time: 0.1094\n",
      "218/223, train_loss: 0.0936, step time: 0.1000\n",
      "219/223, train_loss: 0.0987, step time: 0.0991\n",
      "220/223, train_loss: 0.1064, step time: 0.1012\n",
      "221/223, train_loss: 0.1156, step time: 0.1003\n",
      "222/223, train_loss: 0.1117, step time: 0.0997\n",
      "223/223, train_loss: 0.1171, step time: 0.1012\n",
      "epoch 162 average loss: 0.1058\n",
      "time consuming of epoch 162 is: 89.2035\n",
      "----------\n",
      "epoch 163/300\n",
      "1/223, train_loss: 0.1038, step time: 0.1019\n",
      "2/223, train_loss: 0.1004, step time: 0.0999\n",
      "3/223, train_loss: 0.1148, step time: 0.1008\n",
      "4/223, train_loss: 0.0962, step time: 0.1126\n",
      "5/223, train_loss: 0.1049, step time: 0.1014\n",
      "6/223, train_loss: 0.0976, step time: 0.1024\n",
      "7/223, train_loss: 0.1067, step time: 0.1157\n",
      "8/223, train_loss: 0.1106, step time: 0.1168\n",
      "9/223, train_loss: 0.0958, step time: 0.1253\n",
      "10/223, train_loss: 0.0979, step time: 0.1005\n",
      "11/223, train_loss: 0.1148, step time: 0.1003\n",
      "12/223, train_loss: 0.1166, step time: 0.1001\n",
      "13/223, train_loss: 0.1061, step time: 0.0997\n",
      "14/223, train_loss: 0.1055, step time: 0.1163\n",
      "15/223, train_loss: 0.0994, step time: 0.1130\n",
      "16/223, train_loss: 0.1155, step time: 0.1142\n",
      "17/223, train_loss: 0.1051, step time: 0.1170\n",
      "18/223, train_loss: 0.1067, step time: 0.1004\n",
      "19/223, train_loss: 0.1080, step time: 0.1004\n",
      "20/223, train_loss: 0.0973, step time: 0.1002\n",
      "21/223, train_loss: 0.1022, step time: 0.1047\n",
      "22/223, train_loss: 0.1040, step time: 0.1040\n",
      "23/223, train_loss: 0.0998, step time: 0.1004\n",
      "24/223, train_loss: 0.1093, step time: 0.1006\n",
      "25/223, train_loss: 0.1178, step time: 0.1214\n",
      "26/223, train_loss: 0.0993, step time: 0.1112\n",
      "27/223, train_loss: 0.1120, step time: 0.0998\n",
      "28/223, train_loss: 0.0969, step time: 0.1014\n",
      "29/223, train_loss: 0.1149, step time: 0.1044\n",
      "30/223, train_loss: 0.1145, step time: 0.1149\n",
      "31/223, train_loss: 0.1066, step time: 0.1082\n",
      "32/223, train_loss: 0.1182, step time: 0.1057\n",
      "33/223, train_loss: 0.1134, step time: 0.1043\n",
      "34/223, train_loss: 0.1031, step time: 0.1057\n",
      "35/223, train_loss: 0.1062, step time: 0.1137\n",
      "36/223, train_loss: 0.0972, step time: 0.1277\n",
      "37/223, train_loss: 0.1077, step time: 0.1167\n",
      "38/223, train_loss: 0.1103, step time: 0.1078\n",
      "39/223, train_loss: 0.0965, step time: 0.1249\n",
      "40/223, train_loss: 0.1040, step time: 0.1138\n",
      "41/223, train_loss: 0.0949, step time: 0.1007\n",
      "42/223, train_loss: 0.1128, step time: 0.1246\n",
      "43/223, train_loss: 0.1038, step time: 0.1100\n",
      "44/223, train_loss: 0.0945, step time: 0.1058\n",
      "45/223, train_loss: 0.1055, step time: 0.1073\n",
      "46/223, train_loss: 0.0969, step time: 0.1167\n",
      "47/223, train_loss: 0.1011, step time: 0.1138\n",
      "48/223, train_loss: 0.1163, step time: 0.1710\n",
      "49/223, train_loss: 0.0935, step time: 0.1280\n",
      "50/223, train_loss: 0.1083, step time: 0.1103\n",
      "51/223, train_loss: 0.1070, step time: 0.1211\n",
      "52/223, train_loss: 0.1013, step time: 0.1144\n",
      "53/223, train_loss: 0.1045, step time: 0.1039\n",
      "54/223, train_loss: 0.1067, step time: 0.1002\n",
      "55/223, train_loss: 0.1044, step time: 0.1060\n",
      "56/223, train_loss: 0.1013, step time: 0.1010\n",
      "57/223, train_loss: 0.1032, step time: 0.0998\n",
      "58/223, train_loss: 0.1057, step time: 0.1003\n",
      "59/223, train_loss: 0.1061, step time: 0.1078\n",
      "60/223, train_loss: 0.0958, step time: 0.1219\n",
      "61/223, train_loss: 0.0976, step time: 0.1016\n",
      "62/223, train_loss: 0.0974, step time: 0.1005\n",
      "63/223, train_loss: 0.1040, step time: 0.1050\n",
      "64/223, train_loss: 0.1064, step time: 0.1006\n",
      "65/223, train_loss: 0.1003, step time: 0.0997\n",
      "66/223, train_loss: 0.1163, step time: 0.1000\n",
      "67/223, train_loss: 0.1003, step time: 0.1007\n",
      "68/223, train_loss: 0.1046, step time: 0.1133\n",
      "69/223, train_loss: 0.0983, step time: 0.1154\n",
      "70/223, train_loss: 0.1027, step time: 0.1001\n",
      "71/223, train_loss: 0.1076, step time: 0.1046\n",
      "72/223, train_loss: 0.0963, step time: 0.1003\n",
      "73/223, train_loss: 0.0942, step time: 0.1004\n",
      "74/223, train_loss: 0.1089, step time: 0.0999\n",
      "75/223, train_loss: 0.1117, step time: 0.1157\n",
      "76/223, train_loss: 0.1069, step time: 0.1084\n",
      "77/223, train_loss: 0.1043, step time: 0.1100\n",
      "78/223, train_loss: 0.1160, step time: 0.1202\n",
      "79/223, train_loss: 0.1096, step time: 0.1199\n",
      "80/223, train_loss: 0.1026, step time: 0.1009\n",
      "81/223, train_loss: 0.1204, step time: 0.1039\n",
      "82/223, train_loss: 0.1084, step time: 0.1012\n",
      "83/223, train_loss: 0.1013, step time: 0.1005\n",
      "84/223, train_loss: 0.1027, step time: 0.0994\n",
      "85/223, train_loss: 0.1185, step time: 0.1004\n",
      "86/223, train_loss: 0.1002, step time: 0.1022\n",
      "87/223, train_loss: 0.1185, step time: 0.1142\n",
      "88/223, train_loss: 0.0929, step time: 0.0997\n",
      "89/223, train_loss: 0.0975, step time: 0.1112\n",
      "90/223, train_loss: 0.1036, step time: 0.0999\n",
      "91/223, train_loss: 0.1111, step time: 0.1010\n",
      "92/223, train_loss: 0.1069, step time: 0.0998\n",
      "93/223, train_loss: 0.1108, step time: 0.1103\n",
      "94/223, train_loss: 0.1121, step time: 0.0991\n",
      "95/223, train_loss: 0.0993, step time: 0.1052\n",
      "96/223, train_loss: 0.1098, step time: 0.1079\n",
      "97/223, train_loss: 0.0944, step time: 0.1162\n",
      "98/223, train_loss: 0.1049, step time: 0.0996\n",
      "99/223, train_loss: 0.0999, step time: 0.1060\n",
      "100/223, train_loss: 0.1001, step time: 0.1135\n",
      "101/223, train_loss: 0.1035, step time: 0.1125\n",
      "102/223, train_loss: 0.1152, step time: 0.1017\n",
      "103/223, train_loss: 0.1077, step time: 0.1109\n",
      "104/223, train_loss: 0.1018, step time: 0.1109\n",
      "105/223, train_loss: 0.1120, step time: 0.1002\n",
      "106/223, train_loss: 0.1096, step time: 0.1034\n",
      "107/223, train_loss: 0.0956, step time: 0.1188\n",
      "108/223, train_loss: 0.0966, step time: 0.1009\n",
      "109/223, train_loss: 0.0958, step time: 0.1004\n",
      "110/223, train_loss: 0.1064, step time: 0.1003\n",
      "111/223, train_loss: 0.1104, step time: 0.1003\n",
      "112/223, train_loss: 0.1276, step time: 0.1007\n",
      "113/223, train_loss: 0.1147, step time: 0.0995\n",
      "114/223, train_loss: 0.1046, step time: 0.0992\n",
      "115/223, train_loss: 0.1102, step time: 0.1011\n",
      "116/223, train_loss: 0.1114, step time: 0.1010\n",
      "117/223, train_loss: 0.1122, step time: 0.0999\n",
      "118/223, train_loss: 0.0966, step time: 0.0995\n",
      "119/223, train_loss: 0.1098, step time: 0.1007\n",
      "120/223, train_loss: 0.1076, step time: 0.1008\n",
      "121/223, train_loss: 0.1056, step time: 0.0993\n",
      "122/223, train_loss: 0.1118, step time: 0.0990\n",
      "123/223, train_loss: 0.1089, step time: 0.1000\n",
      "124/223, train_loss: 0.1009, step time: 0.1021\n",
      "125/223, train_loss: 0.1056, step time: 0.1152\n",
      "126/223, train_loss: 0.3012, step time: 0.1183\n",
      "127/223, train_loss: 0.0946, step time: 0.1270\n",
      "128/223, train_loss: 0.1162, step time: 0.1237\n",
      "129/223, train_loss: 0.1097, step time: 0.1059\n",
      "130/223, train_loss: 0.1092, step time: 0.1154\n",
      "131/223, train_loss: 0.1073, step time: 0.1054\n",
      "132/223, train_loss: 0.0962, step time: 0.1210\n",
      "133/223, train_loss: 0.1133, step time: 0.1130\n",
      "134/223, train_loss: 0.1098, step time: 0.1014\n",
      "135/223, train_loss: 0.1045, step time: 0.1266\n",
      "136/223, train_loss: 0.1019, step time: 0.0997\n",
      "137/223, train_loss: 0.1110, step time: 0.1018\n",
      "138/223, train_loss: 0.0990, step time: 0.1174\n",
      "139/223, train_loss: 0.1071, step time: 0.1044\n",
      "140/223, train_loss: 0.0930, step time: 0.1173\n",
      "141/223, train_loss: 0.1202, step time: 0.1131\n",
      "142/223, train_loss: 0.1037, step time: 0.1140\n",
      "143/223, train_loss: 0.0953, step time: 0.1255\n",
      "144/223, train_loss: 0.1086, step time: 0.1234\n",
      "145/223, train_loss: 0.0998, step time: 0.1109\n",
      "146/223, train_loss: 0.1124, step time: 0.1198\n",
      "147/223, train_loss: 0.1116, step time: 0.1170\n",
      "148/223, train_loss: 0.0964, step time: 0.1166\n",
      "149/223, train_loss: 0.1126, step time: 0.1131\n",
      "150/223, train_loss: 0.1085, step time: 0.1132\n",
      "151/223, train_loss: 0.1040, step time: 0.1002\n",
      "152/223, train_loss: 0.1038, step time: 0.1007\n",
      "153/223, train_loss: 0.0935, step time: 0.1118\n",
      "154/223, train_loss: 0.1009, step time: 0.1130\n",
      "155/223, train_loss: 0.1030, step time: 0.1253\n",
      "156/223, train_loss: 0.1076, step time: 0.1207\n",
      "157/223, train_loss: 0.1007, step time: 0.1143\n",
      "158/223, train_loss: 0.0914, step time: 0.1061\n",
      "159/223, train_loss: 0.0972, step time: 0.1165\n",
      "160/223, train_loss: 0.1154, step time: 0.1051\n",
      "161/223, train_loss: 0.1031, step time: 0.1076\n",
      "162/223, train_loss: 0.1132, step time: 0.1104\n",
      "163/223, train_loss: 0.1059, step time: 0.0990\n",
      "164/223, train_loss: 0.1128, step time: 0.1009\n",
      "165/223, train_loss: 0.1008, step time: 0.1071\n",
      "166/223, train_loss: 0.1046, step time: 0.1078\n",
      "167/223, train_loss: 0.1085, step time: 0.1043\n",
      "168/223, train_loss: 0.1026, step time: 0.0986\n",
      "169/223, train_loss: 0.1078, step time: 0.1102\n",
      "170/223, train_loss: 0.0965, step time: 0.1364\n",
      "171/223, train_loss: 0.1107, step time: 0.1215\n",
      "172/223, train_loss: 0.1085, step time: 0.0998\n",
      "173/223, train_loss: 0.1039, step time: 0.1180\n",
      "174/223, train_loss: 0.1034, step time: 0.1199\n",
      "175/223, train_loss: 0.0979, step time: 0.1092\n",
      "176/223, train_loss: 0.0997, step time: 0.1009\n",
      "177/223, train_loss: 0.1116, step time: 0.1190\n",
      "178/223, train_loss: 0.1017, step time: 0.1604\n",
      "179/223, train_loss: 0.0936, step time: 0.1395\n",
      "180/223, train_loss: 0.0970, step time: 0.1109\n",
      "181/223, train_loss: 0.0964, step time: 0.1143\n",
      "182/223, train_loss: 0.0986, step time: 0.1175\n",
      "183/223, train_loss: 0.1104, step time: 0.1181\n",
      "184/223, train_loss: 0.0975, step time: 0.1006\n",
      "185/223, train_loss: 0.1130, step time: 0.1166\n",
      "186/223, train_loss: 0.1023, step time: 0.1064\n",
      "187/223, train_loss: 0.1029, step time: 0.1055\n",
      "188/223, train_loss: 0.0919, step time: 0.1004\n",
      "189/223, train_loss: 0.1001, step time: 0.0998\n",
      "190/223, train_loss: 0.1036, step time: 0.1299\n",
      "191/223, train_loss: 0.1089, step time: 0.1139\n",
      "192/223, train_loss: 0.1082, step time: 0.0998\n",
      "193/223, train_loss: 0.1085, step time: 0.1179\n",
      "194/223, train_loss: 0.1098, step time: 0.1077\n",
      "195/223, train_loss: 0.1043, step time: 0.1296\n",
      "196/223, train_loss: 0.1033, step time: 0.1006\n",
      "197/223, train_loss: 0.0941, step time: 0.1047\n",
      "198/223, train_loss: 0.0999, step time: 0.0999\n",
      "199/223, train_loss: 0.1034, step time: 0.0997\n",
      "200/223, train_loss: 0.1100, step time: 0.0995\n",
      "201/223, train_loss: 0.1010, step time: 0.1059\n",
      "202/223, train_loss: 0.1020, step time: 0.1145\n",
      "203/223, train_loss: 0.1028, step time: 0.1210\n",
      "204/223, train_loss: 0.0960, step time: 0.1001\n",
      "205/223, train_loss: 0.1095, step time: 0.1068\n",
      "206/223, train_loss: 0.1030, step time: 0.1081\n",
      "207/223, train_loss: 0.0976, step time: 0.1006\n",
      "208/223, train_loss: 0.0968, step time: 0.1005\n",
      "209/223, train_loss: 0.1072, step time: 0.1091\n",
      "210/223, train_loss: 0.1223, step time: 0.1030\n",
      "211/223, train_loss: 0.1147, step time: 0.1053\n",
      "212/223, train_loss: 0.0990, step time: 0.1005\n",
      "213/223, train_loss: 0.1170, step time: 0.1102\n",
      "214/223, train_loss: 0.1163, step time: 0.1045\n",
      "215/223, train_loss: 0.1046, step time: 0.1190\n",
      "216/223, train_loss: 0.0958, step time: 0.1058\n",
      "217/223, train_loss: 0.1009, step time: 0.1028\n",
      "218/223, train_loss: 0.1108, step time: 0.1001\n",
      "219/223, train_loss: 0.0987, step time: 0.0994\n",
      "220/223, train_loss: 0.0989, step time: 0.1000\n",
      "221/223, train_loss: 0.1033, step time: 0.0990\n",
      "222/223, train_loss: 0.0911, step time: 0.0987\n",
      "223/223, train_loss: 0.1040, step time: 0.0999\n",
      "epoch 163 average loss: 0.1058\n",
      "time consuming of epoch 163 is: 90.7596\n",
      "----------\n",
      "epoch 164/300\n",
      "1/223, train_loss: 0.1079, step time: 0.1007\n",
      "2/223, train_loss: 0.1068, step time: 0.0998\n",
      "3/223, train_loss: 0.0899, step time: 0.1007\n",
      "4/223, train_loss: 0.1023, step time: 0.1006\n",
      "5/223, train_loss: 0.1026, step time: 0.1039\n",
      "6/223, train_loss: 0.1066, step time: 0.1546\n",
      "7/223, train_loss: 0.1120, step time: 0.1115\n",
      "8/223, train_loss: 0.1146, step time: 0.1153\n",
      "9/223, train_loss: 0.0951, step time: 0.1050\n",
      "10/223, train_loss: 0.1060, step time: 0.1088\n",
      "11/223, train_loss: 0.1002, step time: 0.1006\n",
      "12/223, train_loss: 0.1069, step time: 0.1001\n",
      "13/223, train_loss: 0.0991, step time: 0.1060\n",
      "14/223, train_loss: 0.1054, step time: 0.1095\n",
      "15/223, train_loss: 0.0968, step time: 0.1014\n",
      "16/223, train_loss: 0.1025, step time: 0.1006\n",
      "17/223, train_loss: 0.0923, step time: 0.1209\n",
      "18/223, train_loss: 0.1141, step time: 0.1295\n",
      "19/223, train_loss: 0.1008, step time: 0.1107\n",
      "20/223, train_loss: 0.1194, step time: 0.1000\n",
      "21/223, train_loss: 0.0973, step time: 0.1045\n",
      "22/223, train_loss: 0.1092, step time: 0.0997\n",
      "23/223, train_loss: 0.1061, step time: 0.1004\n",
      "24/223, train_loss: 0.0979, step time: 0.1311\n",
      "25/223, train_loss: 0.1011, step time: 0.1901\n",
      "26/223, train_loss: 0.1026, step time: 0.1000\n",
      "27/223, train_loss: 0.1102, step time: 0.1003\n",
      "28/223, train_loss: 0.1100, step time: 0.1105\n",
      "29/223, train_loss: 0.1002, step time: 0.1002\n",
      "30/223, train_loss: 0.1070, step time: 0.0998\n",
      "31/223, train_loss: 0.1105, step time: 0.1009\n",
      "32/223, train_loss: 0.1147, step time: 0.1248\n",
      "33/223, train_loss: 0.0987, step time: 0.1189\n",
      "34/223, train_loss: 0.1037, step time: 0.1336\n",
      "35/223, train_loss: 0.1017, step time: 0.1288\n",
      "36/223, train_loss: 0.1115, step time: 0.1005\n",
      "37/223, train_loss: 0.1029, step time: 0.1110\n",
      "38/223, train_loss: 0.1136, step time: 0.1063\n",
      "39/223, train_loss: 0.0972, step time: 0.1111\n",
      "40/223, train_loss: 0.1107, step time: 0.1023\n",
      "41/223, train_loss: 0.0996, step time: 0.1151\n",
      "42/223, train_loss: 0.1140, step time: 0.1030\n",
      "43/223, train_loss: 0.0965, step time: 0.1198\n",
      "44/223, train_loss: 0.0958, step time: 0.1007\n",
      "45/223, train_loss: 0.1056, step time: 0.1109\n",
      "46/223, train_loss: 0.1205, step time: 0.1020\n",
      "47/223, train_loss: 0.1082, step time: 0.1044\n",
      "48/223, train_loss: 0.1074, step time: 0.1127\n",
      "49/223, train_loss: 0.1161, step time: 0.1000\n",
      "50/223, train_loss: 0.1059, step time: 0.1040\n",
      "51/223, train_loss: 0.1075, step time: 0.1229\n",
      "52/223, train_loss: 0.1094, step time: 0.1138\n",
      "53/223, train_loss: 0.0968, step time: 0.1220\n",
      "54/223, train_loss: 0.1063, step time: 0.1130\n",
      "55/223, train_loss: 0.1034, step time: 0.1058\n",
      "56/223, train_loss: 0.1061, step time: 0.1307\n",
      "57/223, train_loss: 0.1085, step time: 0.1160\n",
      "58/223, train_loss: 0.0964, step time: 0.1063\n",
      "59/223, train_loss: 0.1077, step time: 0.1185\n",
      "60/223, train_loss: 0.1054, step time: 0.1055\n",
      "61/223, train_loss: 0.1078, step time: 0.1287\n",
      "62/223, train_loss: 0.0958, step time: 0.1051\n",
      "63/223, train_loss: 0.0969, step time: 0.1546\n",
      "64/223, train_loss: 0.1046, step time: 0.1071\n",
      "65/223, train_loss: 0.1040, step time: 0.1132\n",
      "66/223, train_loss: 0.1044, step time: 0.1028\n",
      "67/223, train_loss: 0.1119, step time: 0.1025\n",
      "68/223, train_loss: 0.1074, step time: 0.1110\n",
      "69/223, train_loss: 0.0945, step time: 0.1113\n",
      "70/223, train_loss: 0.0908, step time: 0.1082\n",
      "71/223, train_loss: 0.0995, step time: 0.1071\n",
      "72/223, train_loss: 0.1105, step time: 0.1010\n",
      "73/223, train_loss: 0.1208, step time: 0.1143\n",
      "74/223, train_loss: 0.0989, step time: 0.1034\n",
      "75/223, train_loss: 0.0981, step time: 0.1007\n",
      "76/223, train_loss: 0.1004, step time: 0.1099\n",
      "77/223, train_loss: 0.1003, step time: 0.1117\n",
      "78/223, train_loss: 0.1042, step time: 0.1107\n",
      "79/223, train_loss: 0.1075, step time: 0.1057\n",
      "80/223, train_loss: 0.1004, step time: 0.1174\n",
      "81/223, train_loss: 0.0990, step time: 0.1159\n",
      "82/223, train_loss: 0.1044, step time: 0.1082\n",
      "83/223, train_loss: 0.0939, step time: 0.1161\n",
      "84/223, train_loss: 0.1072, step time: 0.1002\n",
      "85/223, train_loss: 0.1045, step time: 0.1223\n",
      "86/223, train_loss: 0.3093, step time: 0.1236\n",
      "87/223, train_loss: 0.1056, step time: 0.1226\n",
      "88/223, train_loss: 0.1007, step time: 0.1146\n",
      "89/223, train_loss: 0.1079, step time: 0.1173\n",
      "90/223, train_loss: 0.1188, step time: 0.1096\n",
      "91/223, train_loss: 0.1100, step time: 0.1192\n",
      "92/223, train_loss: 0.1023, step time: 0.1042\n",
      "93/223, train_loss: 0.1051, step time: 0.1000\n",
      "94/223, train_loss: 0.1051, step time: 0.1109\n",
      "95/223, train_loss: 0.0981, step time: 0.1001\n",
      "96/223, train_loss: 0.1041, step time: 0.1004\n",
      "97/223, train_loss: 0.1203, step time: 0.1178\n",
      "98/223, train_loss: 0.1044, step time: 0.1303\n",
      "99/223, train_loss: 0.1043, step time: 0.1066\n",
      "100/223, train_loss: 0.1001, step time: 0.1008\n",
      "101/223, train_loss: 0.0976, step time: 0.1002\n",
      "102/223, train_loss: 0.1129, step time: 0.1344\n",
      "103/223, train_loss: 0.1025, step time: 0.1005\n",
      "104/223, train_loss: 0.1017, step time: 0.1004\n",
      "105/223, train_loss: 0.0901, step time: 0.1107\n",
      "106/223, train_loss: 0.1241, step time: 0.1068\n",
      "107/223, train_loss: 0.1059, step time: 0.1002\n",
      "108/223, train_loss: 0.0968, step time: 0.1001\n",
      "109/223, train_loss: 0.0980, step time: 0.1356\n",
      "110/223, train_loss: 0.1038, step time: 0.1267\n",
      "111/223, train_loss: 0.0995, step time: 0.1078\n",
      "112/223, train_loss: 0.1107, step time: 0.1169\n",
      "113/223, train_loss: 0.1029, step time: 0.1328\n",
      "114/223, train_loss: 0.1163, step time: 0.1058\n",
      "115/223, train_loss: 0.1079, step time: 0.1145\n",
      "116/223, train_loss: 0.1062, step time: 0.1011\n",
      "117/223, train_loss: 0.1038, step time: 0.1042\n",
      "118/223, train_loss: 0.1067, step time: 0.1007\n",
      "119/223, train_loss: 0.0969, step time: 0.1082\n",
      "120/223, train_loss: 0.1014, step time: 0.1001\n",
      "121/223, train_loss: 0.1156, step time: 0.1019\n",
      "122/223, train_loss: 0.1058, step time: 0.1002\n",
      "123/223, train_loss: 0.0996, step time: 0.1001\n",
      "124/223, train_loss: 0.0975, step time: 0.1083\n",
      "125/223, train_loss: 0.1057, step time: 0.1076\n",
      "126/223, train_loss: 0.1090, step time: 0.1000\n",
      "127/223, train_loss: 0.1034, step time: 0.1015\n",
      "128/223, train_loss: 0.1088, step time: 0.1082\n",
      "129/223, train_loss: 0.1048, step time: 0.1001\n",
      "130/223, train_loss: 0.0905, step time: 0.0992\n",
      "131/223, train_loss: 0.1024, step time: 0.1002\n",
      "132/223, train_loss: 0.1155, step time: 0.1004\n",
      "133/223, train_loss: 0.0949, step time: 0.1050\n",
      "134/223, train_loss: 0.1013, step time: 0.1010\n",
      "135/223, train_loss: 0.1118, step time: 0.1013\n",
      "136/223, train_loss: 0.1160, step time: 0.1132\n",
      "137/223, train_loss: 0.1036, step time: 0.0997\n",
      "138/223, train_loss: 0.0982, step time: 0.1004\n",
      "139/223, train_loss: 0.0975, step time: 0.1003\n",
      "140/223, train_loss: 0.1198, step time: 0.1046\n",
      "141/223, train_loss: 0.0879, step time: 0.1092\n",
      "142/223, train_loss: 0.0964, step time: 0.1000\n",
      "143/223, train_loss: 0.1051, step time: 0.1176\n",
      "144/223, train_loss: 0.1067, step time: 0.1031\n",
      "145/223, train_loss: 0.1023, step time: 0.1030\n",
      "146/223, train_loss: 0.1139, step time: 0.1312\n",
      "147/223, train_loss: 0.1198, step time: 0.1285\n",
      "148/223, train_loss: 0.1084, step time: 0.1020\n",
      "149/223, train_loss: 0.1148, step time: 0.0996\n",
      "150/223, train_loss: 0.1014, step time: 0.1001\n",
      "151/223, train_loss: 0.1049, step time: 0.1050\n",
      "152/223, train_loss: 0.1070, step time: 0.1001\n",
      "153/223, train_loss: 0.1059, step time: 0.0998\n",
      "154/223, train_loss: 0.1154, step time: 0.1000\n",
      "155/223, train_loss: 0.1095, step time: 0.1003\n",
      "156/223, train_loss: 0.1070, step time: 0.1002\n",
      "157/223, train_loss: 0.0989, step time: 0.1143\n",
      "158/223, train_loss: 0.1075, step time: 0.0999\n",
      "159/223, train_loss: 0.0954, step time: 0.1009\n",
      "160/223, train_loss: 0.1085, step time: 0.1031\n",
      "161/223, train_loss: 0.1100, step time: 0.1144\n",
      "162/223, train_loss: 0.1129, step time: 0.1303\n",
      "163/223, train_loss: 0.1071, step time: 0.1006\n",
      "164/223, train_loss: 0.1088, step time: 0.1003\n",
      "165/223, train_loss: 0.1171, step time: 0.1048\n",
      "166/223, train_loss: 0.1147, step time: 0.0995\n",
      "167/223, train_loss: 0.1156, step time: 0.1007\n",
      "168/223, train_loss: 0.1011, step time: 0.1007\n",
      "169/223, train_loss: 0.1104, step time: 0.1422\n",
      "170/223, train_loss: 0.1009, step time: 0.1079\n",
      "171/223, train_loss: 0.0985, step time: 0.1000\n",
      "172/223, train_loss: 0.1100, step time: 0.1060\n",
      "173/223, train_loss: 0.1026, step time: 0.1086\n",
      "174/223, train_loss: 0.0952, step time: 0.1010\n",
      "175/223, train_loss: 0.1106, step time: 0.1001\n",
      "176/223, train_loss: 0.1057, step time: 0.1000\n",
      "177/223, train_loss: 0.0997, step time: 0.1048\n",
      "178/223, train_loss: 0.1038, step time: 0.1010\n",
      "179/223, train_loss: 0.1167, step time: 0.1049\n",
      "180/223, train_loss: 0.1169, step time: 0.1003\n",
      "181/223, train_loss: 0.1099, step time: 0.1044\n",
      "182/223, train_loss: 0.0991, step time: 0.1005\n",
      "183/223, train_loss: 0.1005, step time: 0.1003\n",
      "184/223, train_loss: 0.1073, step time: 0.1003\n",
      "185/223, train_loss: 0.1025, step time: 0.1052\n",
      "186/223, train_loss: 0.1059, step time: 0.1002\n",
      "187/223, train_loss: 0.1000, step time: 0.1009\n",
      "188/223, train_loss: 0.1096, step time: 0.1151\n",
      "189/223, train_loss: 0.0929, step time: 0.1162\n",
      "190/223, train_loss: 0.1033, step time: 0.1159\n",
      "191/223, train_loss: 0.1031, step time: 0.1000\n",
      "192/223, train_loss: 0.1062, step time: 0.1008\n",
      "193/223, train_loss: 0.0932, step time: 0.1070\n",
      "194/223, train_loss: 0.1061, step time: 0.1006\n",
      "195/223, train_loss: 0.1140, step time: 0.1015\n",
      "196/223, train_loss: 0.1088, step time: 0.1094\n",
      "197/223, train_loss: 0.1123, step time: 0.1041\n",
      "198/223, train_loss: 0.0987, step time: 0.1062\n",
      "199/223, train_loss: 0.1116, step time: 0.1255\n",
      "200/223, train_loss: 0.0916, step time: 0.1160\n",
      "201/223, train_loss: 0.1040, step time: 0.1175\n",
      "202/223, train_loss: 0.1067, step time: 0.1088\n",
      "203/223, train_loss: 0.0981, step time: 0.1076\n",
      "204/223, train_loss: 0.1029, step time: 0.1007\n",
      "205/223, train_loss: 0.1074, step time: 0.1119\n",
      "206/223, train_loss: 0.0978, step time: 0.1006\n",
      "207/223, train_loss: 0.1002, step time: 0.0997\n",
      "208/223, train_loss: 0.1135, step time: 0.1044\n",
      "209/223, train_loss: 0.1060, step time: 0.1009\n",
      "210/223, train_loss: 0.1009, step time: 0.1080\n",
      "211/223, train_loss: 0.1113, step time: 0.1023\n",
      "212/223, train_loss: 0.0952, step time: 0.1002\n",
      "213/223, train_loss: 0.1069, step time: 0.1233\n",
      "214/223, train_loss: 0.0933, step time: 0.1061\n",
      "215/223, train_loss: 0.1067, step time: 0.1118\n",
      "216/223, train_loss: 0.0999, step time: 0.1008\n",
      "217/223, train_loss: 0.1103, step time: 0.1006\n",
      "218/223, train_loss: 0.1098, step time: 0.1037\n",
      "219/223, train_loss: 0.1051, step time: 0.0999\n",
      "220/223, train_loss: 0.0994, step time: 0.1009\n",
      "221/223, train_loss: 0.0942, step time: 0.0992\n",
      "222/223, train_loss: 0.1024, step time: 0.0997\n",
      "223/223, train_loss: 0.1024, step time: 0.1009\n",
      "epoch 164 average loss: 0.1058\n",
      "time consuming of epoch 164 is: 89.5153\n",
      "----------\n",
      "epoch 165/300\n",
      "1/223, train_loss: 0.1030, step time: 0.1137\n",
      "2/223, train_loss: 0.1126, step time: 0.1006\n",
      "3/223, train_loss: 0.0991, step time: 0.1246\n",
      "4/223, train_loss: 0.0982, step time: 0.1005\n",
      "5/223, train_loss: 0.1012, step time: 0.1070\n",
      "6/223, train_loss: 0.1059, step time: 0.1104\n",
      "7/223, train_loss: 0.1149, step time: 0.1138\n",
      "8/223, train_loss: 0.1170, step time: 0.1045\n",
      "9/223, train_loss: 0.0978, step time: 0.1171\n",
      "10/223, train_loss: 0.0925, step time: 0.1068\n",
      "11/223, train_loss: 0.1123, step time: 0.1241\n",
      "12/223, train_loss: 0.1023, step time: 0.1131\n",
      "13/223, train_loss: 0.0953, step time: 0.1113\n",
      "14/223, train_loss: 0.1134, step time: 0.1108\n",
      "15/223, train_loss: 0.1058, step time: 0.1083\n",
      "16/223, train_loss: 0.1056, step time: 0.1059\n",
      "17/223, train_loss: 0.0963, step time: 0.0992\n",
      "18/223, train_loss: 0.0976, step time: 0.1221\n",
      "19/223, train_loss: 0.0945, step time: 0.1150\n",
      "20/223, train_loss: 0.1015, step time: 0.1154\n",
      "21/223, train_loss: 0.0961, step time: 0.1072\n",
      "22/223, train_loss: 0.1013, step time: 0.1478\n",
      "23/223, train_loss: 0.0970, step time: 0.1870\n",
      "24/223, train_loss: 0.1026, step time: 0.1127\n",
      "25/223, train_loss: 0.0963, step time: 0.1009\n",
      "26/223, train_loss: 0.1005, step time: 0.1000\n",
      "27/223, train_loss: 0.1079, step time: 0.1062\n",
      "28/223, train_loss: 0.0977, step time: 0.1022\n",
      "29/223, train_loss: 0.1035, step time: 0.1144\n",
      "30/223, train_loss: 0.1048, step time: 0.1188\n",
      "31/223, train_loss: 0.1030, step time: 0.1046\n",
      "32/223, train_loss: 0.0990, step time: 0.1116\n",
      "33/223, train_loss: 0.1092, step time: 0.1055\n",
      "34/223, train_loss: 0.0979, step time: 0.1169\n",
      "35/223, train_loss: 0.1002, step time: 0.1079\n",
      "36/223, train_loss: 0.1210, step time: 0.1186\n",
      "37/223, train_loss: 0.0978, step time: 0.1145\n",
      "38/223, train_loss: 0.1039, step time: 0.1172\n",
      "39/223, train_loss: 0.1010, step time: 0.1040\n",
      "40/223, train_loss: 0.1028, step time: 0.1311\n",
      "41/223, train_loss: 0.1061, step time: 0.1059\n",
      "42/223, train_loss: 0.1066, step time: 0.1360\n",
      "43/223, train_loss: 0.0966, step time: 0.1387\n",
      "44/223, train_loss: 0.1071, step time: 0.1062\n",
      "45/223, train_loss: 0.1068, step time: 0.1121\n",
      "46/223, train_loss: 0.0954, step time: 0.1426\n",
      "47/223, train_loss: 0.1123, step time: 0.1023\n",
      "48/223, train_loss: 0.1024, step time: 0.1062\n",
      "49/223, train_loss: 0.1112, step time: 0.1216\n",
      "50/223, train_loss: 0.0919, step time: 0.1666\n",
      "51/223, train_loss: 0.0921, step time: 0.1924\n",
      "52/223, train_loss: 0.1081, step time: 0.1284\n",
      "53/223, train_loss: 0.1094, step time: 0.0997\n",
      "54/223, train_loss: 0.0970, step time: 0.1156\n",
      "55/223, train_loss: 0.1035, step time: 0.1145\n",
      "56/223, train_loss: 0.1103, step time: 0.1007\n",
      "57/223, train_loss: 0.1041, step time: 0.1322\n",
      "58/223, train_loss: 0.1135, step time: 0.1389\n",
      "59/223, train_loss: 0.1067, step time: 0.1403\n",
      "60/223, train_loss: 0.1020, step time: 0.1000\n",
      "61/223, train_loss: 0.1077, step time: 0.1043\n",
      "62/223, train_loss: 0.1133, step time: 0.1166\n",
      "63/223, train_loss: 0.1174, step time: 0.1173\n",
      "64/223, train_loss: 0.1141, step time: 0.1196\n",
      "65/223, train_loss: 0.0979, step time: 0.1217\n",
      "66/223, train_loss: 0.1019, step time: 0.1068\n",
      "67/223, train_loss: 0.0945, step time: 0.1022\n",
      "68/223, train_loss: 0.0929, step time: 0.1190\n",
      "69/223, train_loss: 0.0988, step time: 0.1195\n",
      "70/223, train_loss: 0.1282, step time: 0.1064\n",
      "71/223, train_loss: 0.1085, step time: 0.1126\n",
      "72/223, train_loss: 0.0977, step time: 0.1007\n",
      "73/223, train_loss: 0.1012, step time: 0.1084\n",
      "74/223, train_loss: 0.1079, step time: 0.1005\n",
      "75/223, train_loss: 0.1175, step time: 0.1418\n",
      "76/223, train_loss: 0.1095, step time: 0.1007\n",
      "77/223, train_loss: 0.1072, step time: 0.1011\n",
      "78/223, train_loss: 0.1085, step time: 0.0995\n",
      "79/223, train_loss: 0.1168, step time: 0.1010\n",
      "80/223, train_loss: 0.1077, step time: 0.1041\n",
      "81/223, train_loss: 0.1098, step time: 0.1062\n",
      "82/223, train_loss: 0.1068, step time: 0.1028\n",
      "83/223, train_loss: 0.1014, step time: 0.1090\n",
      "84/223, train_loss: 0.1034, step time: 0.1035\n",
      "85/223, train_loss: 0.1057, step time: 0.0991\n",
      "86/223, train_loss: 0.1074, step time: 0.0989\n",
      "87/223, train_loss: 0.1052, step time: 0.0998\n",
      "88/223, train_loss: 0.1022, step time: 0.0994\n",
      "89/223, train_loss: 0.0984, step time: 0.1136\n",
      "90/223, train_loss: 0.1030, step time: 0.1003\n",
      "91/223, train_loss: 0.0978, step time: 0.0999\n",
      "92/223, train_loss: 0.1015, step time: 0.1369\n",
      "93/223, train_loss: 0.1163, step time: 0.1003\n",
      "94/223, train_loss: 0.1164, step time: 0.1060\n",
      "95/223, train_loss: 0.1042, step time: 0.1004\n",
      "96/223, train_loss: 0.1100, step time: 0.1011\n",
      "97/223, train_loss: 0.1066, step time: 0.1102\n",
      "98/223, train_loss: 0.1013, step time: 0.1139\n",
      "99/223, train_loss: 0.1030, step time: 0.1103\n",
      "100/223, train_loss: 0.0989, step time: 0.1010\n",
      "101/223, train_loss: 0.1133, step time: 0.1126\n",
      "102/223, train_loss: 0.0983, step time: 0.1083\n",
      "103/223, train_loss: 0.1059, step time: 0.1058\n",
      "104/223, train_loss: 0.1063, step time: 0.1004\n",
      "105/223, train_loss: 0.0942, step time: 0.1035\n",
      "106/223, train_loss: 0.0944, step time: 0.1161\n",
      "107/223, train_loss: 0.1133, step time: 0.1079\n",
      "108/223, train_loss: 0.1062, step time: 0.1144\n",
      "109/223, train_loss: 0.1188, step time: 0.1202\n",
      "110/223, train_loss: 0.0881, step time: 0.1807\n",
      "111/223, train_loss: 0.1173, step time: 0.1168\n",
      "112/223, train_loss: 0.0937, step time: 0.1047\n",
      "113/223, train_loss: 0.1053, step time: 0.1033\n",
      "114/223, train_loss: 0.1081, step time: 0.1267\n",
      "115/223, train_loss: 0.0976, step time: 0.1054\n",
      "116/223, train_loss: 0.0974, step time: 0.1060\n",
      "117/223, train_loss: 0.1121, step time: 0.1216\n",
      "118/223, train_loss: 0.1060, step time: 0.1072\n",
      "119/223, train_loss: 0.1109, step time: 0.1155\n",
      "120/223, train_loss: 0.1181, step time: 0.1080\n",
      "121/223, train_loss: 0.1061, step time: 0.1091\n",
      "122/223, train_loss: 0.1011, step time: 0.0995\n",
      "123/223, train_loss: 0.1054, step time: 0.0999\n",
      "124/223, train_loss: 0.1128, step time: 0.1006\n",
      "125/223, train_loss: 0.0938, step time: 0.1111\n",
      "126/223, train_loss: 0.1087, step time: 0.1009\n",
      "127/223, train_loss: 0.0995, step time: 0.1010\n",
      "128/223, train_loss: 0.1031, step time: 0.1014\n",
      "129/223, train_loss: 0.0953, step time: 0.1007\n",
      "130/223, train_loss: 0.1092, step time: 0.1001\n",
      "131/223, train_loss: 0.0981, step time: 0.1011\n",
      "132/223, train_loss: 0.0972, step time: 0.1004\n",
      "133/223, train_loss: 0.1030, step time: 0.1103\n",
      "134/223, train_loss: 0.1188, step time: 0.1076\n",
      "135/223, train_loss: 0.1073, step time: 0.1076\n",
      "136/223, train_loss: 0.1114, step time: 0.1065\n",
      "137/223, train_loss: 0.1041, step time: 0.1134\n",
      "138/223, train_loss: 0.1220, step time: 0.1096\n",
      "139/223, train_loss: 0.1051, step time: 0.1008\n",
      "140/223, train_loss: 0.1056, step time: 0.1125\n",
      "141/223, train_loss: 0.0928, step time: 0.1035\n",
      "142/223, train_loss: 0.0966, step time: 0.1005\n",
      "143/223, train_loss: 0.1116, step time: 0.1013\n",
      "144/223, train_loss: 0.1112, step time: 0.1004\n",
      "145/223, train_loss: 0.1051, step time: 0.1005\n",
      "146/223, train_loss: 0.1146, step time: 0.1006\n",
      "147/223, train_loss: 0.0996, step time: 0.1004\n",
      "148/223, train_loss: 0.1048, step time: 0.1006\n",
      "149/223, train_loss: 0.1127, step time: 0.0995\n",
      "150/223, train_loss: 0.1063, step time: 0.1005\n",
      "151/223, train_loss: 0.1069, step time: 0.1006\n",
      "152/223, train_loss: 0.1184, step time: 0.1055\n",
      "153/223, train_loss: 0.1020, step time: 0.1086\n",
      "154/223, train_loss: 0.1216, step time: 0.1007\n",
      "155/223, train_loss: 0.1179, step time: 0.1002\n",
      "156/223, train_loss: 0.1000, step time: 0.1062\n",
      "157/223, train_loss: 0.0944, step time: 0.1004\n",
      "158/223, train_loss: 0.0985, step time: 0.1004\n",
      "159/223, train_loss: 0.1105, step time: 0.1005\n",
      "160/223, train_loss: 0.1158, step time: 0.1031\n",
      "161/223, train_loss: 0.1008, step time: 0.1110\n",
      "162/223, train_loss: 0.1081, step time: 0.1163\n",
      "163/223, train_loss: 0.0934, step time: 0.1004\n",
      "164/223, train_loss: 0.1080, step time: 0.1004\n",
      "165/223, train_loss: 0.1090, step time: 0.1084\n",
      "166/223, train_loss: 0.1110, step time: 0.1006\n",
      "167/223, train_loss: 0.1000, step time: 0.1000\n",
      "168/223, train_loss: 0.0961, step time: 0.1083\n",
      "169/223, train_loss: 0.0995, step time: 0.1135\n",
      "170/223, train_loss: 0.1034, step time: 0.1152\n",
      "171/223, train_loss: 0.1030, step time: 0.1111\n",
      "172/223, train_loss: 0.1097, step time: 0.1123\n",
      "173/223, train_loss: 0.0980, step time: 0.1099\n",
      "174/223, train_loss: 0.1132, step time: 0.1006\n",
      "175/223, train_loss: 0.1023, step time: 0.1007\n",
      "176/223, train_loss: 0.1040, step time: 0.1084\n",
      "177/223, train_loss: 0.1086, step time: 0.1112\n",
      "178/223, train_loss: 0.1167, step time: 0.1083\n",
      "179/223, train_loss: 0.1001, step time: 0.1073\n",
      "180/223, train_loss: 0.1100, step time: 0.1062\n",
      "181/223, train_loss: 0.0999, step time: 0.1021\n",
      "182/223, train_loss: 0.1129, step time: 0.1004\n",
      "183/223, train_loss: 0.1131, step time: 0.1002\n",
      "184/223, train_loss: 0.1044, step time: 0.1128\n",
      "185/223, train_loss: 0.1102, step time: 0.1004\n",
      "186/223, train_loss: 0.1061, step time: 0.1039\n",
      "187/223, train_loss: 0.1069, step time: 0.1103\n",
      "188/223, train_loss: 0.1026, step time: 0.1120\n",
      "189/223, train_loss: 0.1158, step time: 0.1027\n",
      "190/223, train_loss: 0.1026, step time: 0.1141\n",
      "191/223, train_loss: 0.0931, step time: 0.1164\n",
      "192/223, train_loss: 0.1124, step time: 0.1105\n",
      "193/223, train_loss: 0.1008, step time: 0.1073\n",
      "194/223, train_loss: 0.1037, step time: 0.1160\n",
      "195/223, train_loss: 0.1022, step time: 0.1111\n",
      "196/223, train_loss: 0.2971, step time: 0.1007\n",
      "197/223, train_loss: 0.1031, step time: 0.1057\n",
      "198/223, train_loss: 0.1044, step time: 0.1105\n",
      "199/223, train_loss: 0.1025, step time: 0.1136\n",
      "200/223, train_loss: 0.0907, step time: 0.1059\n",
      "201/223, train_loss: 0.1003, step time: 0.1009\n",
      "202/223, train_loss: 0.0972, step time: 0.1004\n",
      "203/223, train_loss: 0.1109, step time: 0.0998\n",
      "204/223, train_loss: 0.1016, step time: 0.1146\n",
      "205/223, train_loss: 0.1121, step time: 0.1166\n",
      "206/223, train_loss: 0.1045, step time: 0.1006\n",
      "207/223, train_loss: 0.1038, step time: 0.1181\n",
      "208/223, train_loss: 0.1089, step time: 0.1006\n",
      "209/223, train_loss: 0.1028, step time: 0.0997\n",
      "210/223, train_loss: 0.0974, step time: 0.1004\n",
      "211/223, train_loss: 0.1137, step time: 0.1002\n",
      "212/223, train_loss: 0.1064, step time: 0.1060\n",
      "213/223, train_loss: 0.1020, step time: 0.1137\n",
      "214/223, train_loss: 0.1070, step time: 0.1001\n",
      "215/223, train_loss: 0.1092, step time: 0.1207\n",
      "216/223, train_loss: 0.1095, step time: 0.1003\n",
      "217/223, train_loss: 0.1052, step time: 0.1133\n",
      "218/223, train_loss: 0.1013, step time: 0.1001\n",
      "219/223, train_loss: 0.1066, step time: 0.0998\n",
      "220/223, train_loss: 0.1064, step time: 0.0990\n",
      "221/223, train_loss: 0.0936, step time: 0.0996\n",
      "222/223, train_loss: 0.1058, step time: 0.0996\n",
      "223/223, train_loss: 0.1008, step time: 0.0991\n",
      "epoch 165 average loss: 0.1057\n",
      "current epoch: 165 current mean dice: 0.8568 tc: 0.9204 wt: 0.8671 et: 0.7830\n",
      "best mean dice: 0.8578 at epoch: 155\n",
      "time consuming of epoch 165 is: 89.7322\n",
      "----------\n",
      "epoch 166/300\n",
      "1/223, train_loss: 0.1052, step time: 0.1018\n",
      "2/223, train_loss: 0.1006, step time: 0.1159\n",
      "3/223, train_loss: 0.1043, step time: 0.1089\n",
      "4/223, train_loss: 0.0994, step time: 0.1097\n",
      "5/223, train_loss: 0.1039, step time: 0.1474\n",
      "6/223, train_loss: 0.1095, step time: 0.1151\n",
      "7/223, train_loss: 0.1014, step time: 0.1003\n",
      "8/223, train_loss: 0.0997, step time: 0.1007\n",
      "9/223, train_loss: 0.1171, step time: 0.1449\n",
      "10/223, train_loss: 0.0942, step time: 0.1133\n",
      "11/223, train_loss: 0.1152, step time: 0.1001\n",
      "12/223, train_loss: 0.1022, step time: 0.1000\n",
      "13/223, train_loss: 0.1141, step time: 0.1506\n",
      "14/223, train_loss: 0.1110, step time: 0.1000\n",
      "15/223, train_loss: 0.1045, step time: 0.1112\n",
      "16/223, train_loss: 0.1060, step time: 0.1229\n",
      "17/223, train_loss: 0.1102, step time: 0.1005\n",
      "18/223, train_loss: 0.0945, step time: 0.1001\n",
      "19/223, train_loss: 0.1063, step time: 0.1219\n",
      "20/223, train_loss: 0.1032, step time: 0.1030\n",
      "21/223, train_loss: 0.1071, step time: 0.1004\n",
      "22/223, train_loss: 0.1051, step time: 0.0998\n",
      "23/223, train_loss: 0.1074, step time: 0.1009\n",
      "24/223, train_loss: 0.1015, step time: 0.1110\n",
      "25/223, train_loss: 0.0959, step time: 0.0995\n",
      "26/223, train_loss: 0.1121, step time: 0.1016\n",
      "27/223, train_loss: 0.1102, step time: 0.1279\n",
      "28/223, train_loss: 0.1161, step time: 0.1007\n",
      "29/223, train_loss: 0.1116, step time: 0.1126\n",
      "30/223, train_loss: 0.0973, step time: 0.1218\n",
      "31/223, train_loss: 0.0982, step time: 0.1091\n",
      "32/223, train_loss: 0.1168, step time: 0.1061\n",
      "33/223, train_loss: 0.1017, step time: 0.1216\n",
      "34/223, train_loss: 0.1075, step time: 0.1027\n",
      "35/223, train_loss: 0.0927, step time: 0.1046\n",
      "36/223, train_loss: 0.1100, step time: 0.1005\n",
      "37/223, train_loss: 0.1082, step time: 0.1349\n",
      "38/223, train_loss: 0.1034, step time: 0.1003\n",
      "39/223, train_loss: 0.0982, step time: 0.1010\n",
      "40/223, train_loss: 0.1035, step time: 0.1280\n",
      "41/223, train_loss: 0.1070, step time: 0.1144\n",
      "42/223, train_loss: 0.0989, step time: 0.1002\n",
      "43/223, train_loss: 0.1015, step time: 0.1062\n",
      "44/223, train_loss: 0.0908, step time: 0.1133\n",
      "45/223, train_loss: 0.1033, step time: 0.0998\n",
      "46/223, train_loss: 0.1119, step time: 0.0996\n",
      "47/223, train_loss: 0.1105, step time: 0.1041\n",
      "48/223, train_loss: 0.1031, step time: 0.1279\n",
      "49/223, train_loss: 0.0944, step time: 0.1152\n",
      "50/223, train_loss: 0.1012, step time: 0.1298\n",
      "51/223, train_loss: 0.1032, step time: 0.1000\n",
      "52/223, train_loss: 0.1222, step time: 0.1134\n",
      "53/223, train_loss: 0.0989, step time: 0.1223\n",
      "54/223, train_loss: 0.0973, step time: 0.1610\n",
      "55/223, train_loss: 0.1022, step time: 0.1000\n",
      "56/223, train_loss: 0.1011, step time: 0.1014\n",
      "57/223, train_loss: 0.0875, step time: 0.1219\n",
      "58/223, train_loss: 0.1045, step time: 0.1245\n",
      "59/223, train_loss: 0.1115, step time: 0.1349\n",
      "60/223, train_loss: 0.1038, step time: 0.1011\n",
      "61/223, train_loss: 0.1015, step time: 0.1275\n",
      "62/223, train_loss: 0.1053, step time: 0.1143\n",
      "63/223, train_loss: 0.1063, step time: 0.1006\n",
      "64/223, train_loss: 0.0993, step time: 0.1002\n",
      "65/223, train_loss: 0.0940, step time: 0.1177\n",
      "66/223, train_loss: 0.1068, step time: 0.1118\n",
      "67/223, train_loss: 0.1032, step time: 0.0999\n",
      "68/223, train_loss: 0.1032, step time: 0.1029\n",
      "69/223, train_loss: 0.1090, step time: 0.1130\n",
      "70/223, train_loss: 0.0982, step time: 0.1106\n",
      "71/223, train_loss: 0.1104, step time: 0.1043\n",
      "72/223, train_loss: 0.1100, step time: 0.0996\n",
      "73/223, train_loss: 0.1063, step time: 0.1178\n",
      "74/223, train_loss: 0.1012, step time: 0.1165\n",
      "75/223, train_loss: 0.0976, step time: 0.1124\n",
      "76/223, train_loss: 0.0953, step time: 0.1112\n",
      "77/223, train_loss: 0.0946, step time: 0.1011\n",
      "78/223, train_loss: 0.1115, step time: 0.1051\n",
      "79/223, train_loss: 0.0886, step time: 0.1025\n",
      "80/223, train_loss: 0.0895, step time: 0.1323\n",
      "81/223, train_loss: 0.0905, step time: 0.1129\n",
      "82/223, train_loss: 0.1043, step time: 0.1004\n",
      "83/223, train_loss: 0.1043, step time: 0.1287\n",
      "84/223, train_loss: 0.1056, step time: 0.1141\n",
      "85/223, train_loss: 0.1121, step time: 0.1193\n",
      "86/223, train_loss: 0.1127, step time: 0.1380\n",
      "87/223, train_loss: 0.1059, step time: 0.1007\n",
      "88/223, train_loss: 0.0923, step time: 0.0999\n",
      "89/223, train_loss: 0.1114, step time: 0.1004\n",
      "90/223, train_loss: 0.0978, step time: 0.1003\n",
      "91/223, train_loss: 0.1027, step time: 0.1000\n",
      "92/223, train_loss: 0.1015, step time: 0.1011\n",
      "93/223, train_loss: 0.0957, step time: 0.1183\n",
      "94/223, train_loss: 0.1191, step time: 0.1049\n",
      "95/223, train_loss: 0.1043, step time: 0.1215\n",
      "96/223, train_loss: 0.1243, step time: 0.1488\n",
      "97/223, train_loss: 0.0943, step time: 0.1107\n",
      "98/223, train_loss: 0.1001, step time: 0.1001\n",
      "99/223, train_loss: 0.1096, step time: 0.1230\n",
      "100/223, train_loss: 0.1148, step time: 0.1008\n",
      "101/223, train_loss: 0.0995, step time: 0.1005\n",
      "102/223, train_loss: 0.1008, step time: 0.1001\n",
      "103/223, train_loss: 0.1076, step time: 0.1002\n",
      "104/223, train_loss: 0.1123, step time: 0.1289\n",
      "105/223, train_loss: 0.1079, step time: 0.1205\n",
      "106/223, train_loss: 0.0912, step time: 0.1222\n",
      "107/223, train_loss: 0.1118, step time: 0.1080\n",
      "108/223, train_loss: 0.1163, step time: 0.1073\n",
      "109/223, train_loss: 0.1114, step time: 0.1005\n",
      "110/223, train_loss: 0.1008, step time: 0.1008\n",
      "111/223, train_loss: 0.0963, step time: 0.1317\n",
      "112/223, train_loss: 0.1000, step time: 0.1120\n",
      "113/223, train_loss: 0.1001, step time: 0.1289\n",
      "114/223, train_loss: 0.1082, step time: 0.1269\n",
      "115/223, train_loss: 0.0964, step time: 0.1104\n",
      "116/223, train_loss: 0.1098, step time: 0.1149\n",
      "117/223, train_loss: 0.1185, step time: 0.1104\n",
      "118/223, train_loss: 0.1075, step time: 0.1022\n",
      "119/223, train_loss: 0.1020, step time: 0.1023\n",
      "120/223, train_loss: 0.1094, step time: 0.1001\n",
      "121/223, train_loss: 0.1153, step time: 0.1243\n",
      "122/223, train_loss: 0.1128, step time: 0.1291\n",
      "123/223, train_loss: 0.1019, step time: 0.1082\n",
      "124/223, train_loss: 0.1055, step time: 0.1005\n",
      "125/223, train_loss: 0.0998, step time: 0.1399\n",
      "126/223, train_loss: 0.1001, step time: 0.1003\n",
      "127/223, train_loss: 0.1022, step time: 0.0997\n",
      "128/223, train_loss: 0.1014, step time: 0.1001\n",
      "129/223, train_loss: 0.0939, step time: 0.1004\n",
      "130/223, train_loss: 0.1010, step time: 0.1003\n",
      "131/223, train_loss: 0.1146, step time: 0.1023\n",
      "132/223, train_loss: 0.1026, step time: 0.1223\n",
      "133/223, train_loss: 0.1089, step time: 0.1501\n",
      "134/223, train_loss: 0.0908, step time: 0.1176\n",
      "135/223, train_loss: 0.1171, step time: 0.1117\n",
      "136/223, train_loss: 0.0947, step time: 0.1024\n",
      "137/223, train_loss: 0.1055, step time: 0.1088\n",
      "138/223, train_loss: 0.1068, step time: 0.1124\n",
      "139/223, train_loss: 0.1026, step time: 0.1117\n",
      "140/223, train_loss: 0.0981, step time: 0.1006\n",
      "141/223, train_loss: 0.1068, step time: 0.1173\n",
      "142/223, train_loss: 0.1095, step time: 0.1181\n",
      "143/223, train_loss: 0.0928, step time: 0.1003\n",
      "144/223, train_loss: 0.0990, step time: 0.1010\n",
      "145/223, train_loss: 0.1036, step time: 0.1002\n",
      "146/223, train_loss: 0.1074, step time: 0.1041\n",
      "147/223, train_loss: 0.0979, step time: 0.1104\n",
      "148/223, train_loss: 0.1040, step time: 0.1189\n",
      "149/223, train_loss: 0.1045, step time: 0.1001\n",
      "150/223, train_loss: 0.1120, step time: 0.1564\n",
      "151/223, train_loss: 0.1067, step time: 0.0994\n",
      "152/223, train_loss: 0.1143, step time: 0.1083\n",
      "153/223, train_loss: 0.1080, step time: 0.1232\n",
      "154/223, train_loss: 0.0989, step time: 0.1018\n",
      "155/223, train_loss: 0.1113, step time: 0.1005\n",
      "156/223, train_loss: 0.0930, step time: 0.1006\n",
      "157/223, train_loss: 0.0998, step time: 0.1205\n",
      "158/223, train_loss: 0.1033, step time: 0.1147\n",
      "159/223, train_loss: 0.1040, step time: 0.1107\n",
      "160/223, train_loss: 0.0992, step time: 0.1236\n",
      "161/223, train_loss: 0.1010, step time: 0.1087\n",
      "162/223, train_loss: 0.1102, step time: 0.1557\n",
      "163/223, train_loss: 0.1026, step time: 0.1044\n",
      "164/223, train_loss: 0.1070, step time: 0.1003\n",
      "165/223, train_loss: 0.1060, step time: 0.1023\n",
      "166/223, train_loss: 0.1080, step time: 0.1090\n",
      "167/223, train_loss: 0.0964, step time: 0.0992\n",
      "168/223, train_loss: 0.1032, step time: 0.0997\n",
      "169/223, train_loss: 0.1106, step time: 0.0998\n",
      "170/223, train_loss: 0.0997, step time: 0.0999\n",
      "171/223, train_loss: 0.0980, step time: 0.0994\n",
      "172/223, train_loss: 0.0954, step time: 0.1231\n",
      "173/223, train_loss: 0.1218, step time: 0.1360\n",
      "174/223, train_loss: 0.1009, step time: 0.1063\n",
      "175/223, train_loss: 0.0972, step time: 0.1013\n",
      "176/223, train_loss: 0.1106, step time: 0.1137\n",
      "177/223, train_loss: 0.1031, step time: 0.1122\n",
      "178/223, train_loss: 0.1127, step time: 0.1135\n",
      "179/223, train_loss: 0.1130, step time: 0.1002\n",
      "180/223, train_loss: 0.1036, step time: 0.1006\n",
      "181/223, train_loss: 0.0974, step time: 0.1077\n",
      "182/223, train_loss: 0.1124, step time: 0.1280\n",
      "183/223, train_loss: 0.1158, step time: 0.1332\n",
      "184/223, train_loss: 0.1024, step time: 0.1128\n",
      "185/223, train_loss: 0.0973, step time: 0.1012\n",
      "186/223, train_loss: 0.0987, step time: 0.1003\n",
      "187/223, train_loss: 0.0980, step time: 0.1144\n",
      "188/223, train_loss: 0.0969, step time: 0.1305\n",
      "189/223, train_loss: 0.1160, step time: 0.1030\n",
      "190/223, train_loss: 0.1102, step time: 0.1154\n",
      "191/223, train_loss: 0.1136, step time: 0.1535\n",
      "192/223, train_loss: 0.1051, step time: 0.1042\n",
      "193/223, train_loss: 0.1074, step time: 0.1182\n",
      "194/223, train_loss: 0.0991, step time: 0.1111\n",
      "195/223, train_loss: 0.1100, step time: 0.1004\n",
      "196/223, train_loss: 0.1082, step time: 0.1297\n",
      "197/223, train_loss: 0.1049, step time: 0.1613\n",
      "198/223, train_loss: 0.0983, step time: 0.1003\n",
      "199/223, train_loss: 0.1025, step time: 0.1001\n",
      "200/223, train_loss: 0.1006, step time: 0.1011\n",
      "201/223, train_loss: 0.1076, step time: 0.1156\n",
      "202/223, train_loss: 0.1253, step time: 0.1271\n",
      "203/223, train_loss: 0.1024, step time: 0.1240\n",
      "204/223, train_loss: 0.3061, step time: 0.1347\n",
      "205/223, train_loss: 0.1056, step time: 0.1180\n",
      "206/223, train_loss: 0.1116, step time: 0.1229\n",
      "207/223, train_loss: 0.1133, step time: 0.1055\n",
      "208/223, train_loss: 0.1139, step time: 0.1006\n",
      "209/223, train_loss: 0.1008, step time: 0.1526\n",
      "210/223, train_loss: 0.1071, step time: 0.1246\n",
      "211/223, train_loss: 0.1189, step time: 0.1025\n",
      "212/223, train_loss: 0.1055, step time: 0.1002\n",
      "213/223, train_loss: 0.1007, step time: 0.1019\n",
      "214/223, train_loss: 0.1160, step time: 0.0997\n",
      "215/223, train_loss: 0.1005, step time: 0.1174\n",
      "216/223, train_loss: 0.0893, step time: 0.1039\n",
      "217/223, train_loss: 0.1002, step time: 0.1035\n",
      "218/223, train_loss: 0.1010, step time: 0.1035\n",
      "219/223, train_loss: 0.0967, step time: 0.1029\n",
      "220/223, train_loss: 0.1113, step time: 0.1008\n",
      "221/223, train_loss: 0.1005, step time: 0.0999\n",
      "222/223, train_loss: 0.1019, step time: 0.0997\n",
      "223/223, train_loss: 0.1020, step time: 0.1001\n",
      "epoch 166 average loss: 0.1053\n",
      "time consuming of epoch 166 is: 89.1254\n",
      "----------\n",
      "epoch 167/300\n",
      "1/223, train_loss: 0.0989, step time: 0.0999\n",
      "2/223, train_loss: 0.1121, step time: 0.0994\n",
      "3/223, train_loss: 0.1236, step time: 0.1003\n",
      "4/223, train_loss: 0.1098, step time: 0.1080\n",
      "5/223, train_loss: 0.1121, step time: 0.1002\n",
      "6/223, train_loss: 0.1062, step time: 0.1011\n",
      "7/223, train_loss: 0.1024, step time: 0.1557\n",
      "8/223, train_loss: 0.0948, step time: 0.1186\n",
      "9/223, train_loss: 0.0967, step time: 0.1007\n",
      "10/223, train_loss: 0.1026, step time: 0.0996\n",
      "11/223, train_loss: 0.0983, step time: 0.1074\n",
      "12/223, train_loss: 0.0950, step time: 0.1014\n",
      "13/223, train_loss: 0.1234, step time: 0.1303\n",
      "14/223, train_loss: 0.1006, step time: 0.0997\n",
      "15/223, train_loss: 0.0968, step time: 0.1140\n",
      "16/223, train_loss: 0.0954, step time: 0.1216\n",
      "17/223, train_loss: 0.1020, step time: 0.1000\n",
      "18/223, train_loss: 0.1013, step time: 0.1077\n",
      "19/223, train_loss: 0.0923, step time: 0.1101\n",
      "20/223, train_loss: 0.1032, step time: 0.1049\n",
      "21/223, train_loss: 0.1019, step time: 0.1158\n",
      "22/223, train_loss: 0.1019, step time: 0.1056\n",
      "23/223, train_loss: 0.0998, step time: 0.1054\n",
      "24/223, train_loss: 0.0952, step time: 0.1081\n",
      "25/223, train_loss: 0.0969, step time: 0.1208\n",
      "26/223, train_loss: 0.1135, step time: 0.1039\n",
      "27/223, train_loss: 0.1079, step time: 0.1044\n",
      "28/223, train_loss: 0.1065, step time: 0.1051\n",
      "29/223, train_loss: 0.1196, step time: 0.1093\n",
      "30/223, train_loss: 0.1076, step time: 0.1110\n",
      "31/223, train_loss: 0.1023, step time: 0.1172\n",
      "32/223, train_loss: 0.1058, step time: 0.1051\n",
      "33/223, train_loss: 0.0999, step time: 0.1218\n",
      "34/223, train_loss: 0.0995, step time: 0.1132\n",
      "35/223, train_loss: 0.1033, step time: 0.1045\n",
      "36/223, train_loss: 0.1054, step time: 0.1092\n",
      "37/223, train_loss: 0.0997, step time: 0.1089\n",
      "38/223, train_loss: 0.1143, step time: 0.1005\n",
      "39/223, train_loss: 0.1082, step time: 0.1107\n",
      "40/223, train_loss: 0.1040, step time: 0.1000\n",
      "41/223, train_loss: 0.1070, step time: 0.1169\n",
      "42/223, train_loss: 0.1056, step time: 0.1128\n",
      "43/223, train_loss: 0.1117, step time: 0.1071\n",
      "44/223, train_loss: 0.1000, step time: 0.1186\n",
      "45/223, train_loss: 0.1142, step time: 0.1102\n",
      "46/223, train_loss: 0.1121, step time: 0.1001\n",
      "47/223, train_loss: 0.0979, step time: 0.1003\n",
      "48/223, train_loss: 0.0928, step time: 0.1122\n",
      "49/223, train_loss: 0.1077, step time: 0.1106\n",
      "50/223, train_loss: 0.1086, step time: 0.1091\n",
      "51/223, train_loss: 0.0998, step time: 0.1003\n",
      "52/223, train_loss: 0.1064, step time: 0.1004\n",
      "53/223, train_loss: 0.1011, step time: 0.1177\n",
      "54/223, train_loss: 0.1229, step time: 0.1019\n",
      "55/223, train_loss: 0.1077, step time: 0.1144\n",
      "56/223, train_loss: 0.1103, step time: 0.1004\n",
      "57/223, train_loss: 0.1079, step time: 0.1151\n",
      "58/223, train_loss: 0.0996, step time: 0.1158\n",
      "59/223, train_loss: 0.0979, step time: 0.1069\n",
      "60/223, train_loss: 0.1030, step time: 0.1015\n",
      "61/223, train_loss: 0.0962, step time: 0.1162\n",
      "62/223, train_loss: 0.1067, step time: 0.1059\n",
      "63/223, train_loss: 0.1019, step time: 0.0979\n",
      "64/223, train_loss: 0.1008, step time: 0.1005\n",
      "65/223, train_loss: 0.0995, step time: 0.1575\n",
      "66/223, train_loss: 0.1004, step time: 0.1155\n",
      "67/223, train_loss: 0.1040, step time: 0.1003\n",
      "68/223, train_loss: 0.1167, step time: 0.1085\n",
      "69/223, train_loss: 0.1084, step time: 0.1210\n",
      "70/223, train_loss: 0.1085, step time: 0.0995\n",
      "71/223, train_loss: 0.1067, step time: 0.1075\n",
      "72/223, train_loss: 0.1008, step time: 0.1252\n",
      "73/223, train_loss: 0.1086, step time: 0.1060\n",
      "74/223, train_loss: 0.1032, step time: 0.1231\n",
      "75/223, train_loss: 0.1149, step time: 0.1129\n",
      "76/223, train_loss: 0.1080, step time: 0.0997\n",
      "77/223, train_loss: 0.1090, step time: 0.1111\n",
      "78/223, train_loss: 0.1048, step time: 0.1252\n",
      "79/223, train_loss: 0.1141, step time: 0.1219\n",
      "80/223, train_loss: 0.1038, step time: 0.1143\n",
      "81/223, train_loss: 0.0993, step time: 0.1015\n",
      "82/223, train_loss: 0.1064, step time: 0.1269\n",
      "83/223, train_loss: 0.1153, step time: 0.1025\n",
      "84/223, train_loss: 0.1008, step time: 0.1110\n",
      "85/223, train_loss: 0.0991, step time: 0.1009\n",
      "86/223, train_loss: 0.1084, step time: 0.1000\n",
      "87/223, train_loss: 0.1102, step time: 0.0994\n",
      "88/223, train_loss: 0.0991, step time: 0.1102\n",
      "89/223, train_loss: 0.1173, step time: 0.1009\n",
      "90/223, train_loss: 0.1024, step time: 0.1006\n",
      "91/223, train_loss: 0.1013, step time: 0.1093\n",
      "92/223, train_loss: 0.1034, step time: 0.1139\n",
      "93/223, train_loss: 0.1004, step time: 0.1346\n",
      "94/223, train_loss: 0.0919, step time: 0.0985\n",
      "95/223, train_loss: 0.1021, step time: 0.1250\n",
      "96/223, train_loss: 0.1003, step time: 0.1170\n",
      "97/223, train_loss: 0.1204, step time: 0.1067\n",
      "98/223, train_loss: 0.1089, step time: 0.1007\n",
      "99/223, train_loss: 0.1141, step time: 0.1017\n",
      "100/223, train_loss: 0.0969, step time: 0.1407\n",
      "101/223, train_loss: 0.0993, step time: 0.1076\n",
      "102/223, train_loss: 0.0966, step time: 0.0998\n",
      "103/223, train_loss: 0.1010, step time: 0.1077\n",
      "104/223, train_loss: 0.1110, step time: 0.1586\n",
      "105/223, train_loss: 0.1035, step time: 0.1003\n",
      "106/223, train_loss: 0.1001, step time: 0.1014\n",
      "107/223, train_loss: 0.1101, step time: 0.1008\n",
      "108/223, train_loss: 0.3015, step time: 0.1002\n",
      "109/223, train_loss: 0.1059, step time: 0.1170\n",
      "110/223, train_loss: 0.1122, step time: 0.1164\n",
      "111/223, train_loss: 0.1074, step time: 0.1015\n",
      "112/223, train_loss: 0.0999, step time: 0.0999\n",
      "113/223, train_loss: 0.1002, step time: 0.1297\n",
      "114/223, train_loss: 0.1058, step time: 0.1000\n",
      "115/223, train_loss: 0.0944, step time: 0.0999\n",
      "116/223, train_loss: 0.0944, step time: 0.1004\n",
      "117/223, train_loss: 0.1009, step time: 0.1005\n",
      "118/223, train_loss: 0.1078, step time: 0.1001\n",
      "119/223, train_loss: 0.1047, step time: 0.1002\n",
      "120/223, train_loss: 0.1042, step time: 0.1057\n",
      "121/223, train_loss: 0.0974, step time: 0.1293\n",
      "122/223, train_loss: 0.0962, step time: 0.1003\n",
      "123/223, train_loss: 0.1061, step time: 0.1007\n",
      "124/223, train_loss: 0.0942, step time: 0.1010\n",
      "125/223, train_loss: 0.1041, step time: 0.1213\n",
      "126/223, train_loss: 0.1158, step time: 0.1086\n",
      "127/223, train_loss: 0.1048, step time: 0.1285\n",
      "128/223, train_loss: 0.0980, step time: 0.1011\n",
      "129/223, train_loss: 0.1061, step time: 0.1124\n",
      "130/223, train_loss: 0.1016, step time: 0.1005\n",
      "131/223, train_loss: 0.1055, step time: 0.1103\n",
      "132/223, train_loss: 0.1014, step time: 0.1399\n",
      "133/223, train_loss: 0.1098, step time: 0.1066\n",
      "134/223, train_loss: 0.1014, step time: 0.1240\n",
      "135/223, train_loss: 0.1031, step time: 0.1149\n",
      "136/223, train_loss: 0.1046, step time: 0.1198\n",
      "137/223, train_loss: 0.1035, step time: 0.1192\n",
      "138/223, train_loss: 0.1043, step time: 0.1234\n",
      "139/223, train_loss: 0.1101, step time: 0.1137\n",
      "140/223, train_loss: 0.0984, step time: 0.1154\n",
      "141/223, train_loss: 0.1076, step time: 0.1456\n",
      "142/223, train_loss: 0.1013, step time: 0.1054\n",
      "143/223, train_loss: 0.1025, step time: 0.1332\n",
      "144/223, train_loss: 0.0982, step time: 0.1247\n",
      "145/223, train_loss: 0.0939, step time: 0.1006\n",
      "146/223, train_loss: 0.1047, step time: 0.1009\n",
      "147/223, train_loss: 0.1132, step time: 0.1233\n",
      "148/223, train_loss: 0.0912, step time: 0.1004\n",
      "149/223, train_loss: 0.1093, step time: 0.1013\n",
      "150/223, train_loss: 0.1027, step time: 0.1258\n",
      "151/223, train_loss: 0.1098, step time: 0.1043\n",
      "152/223, train_loss: 0.1125, step time: 0.1027\n",
      "153/223, train_loss: 0.0906, step time: 0.1307\n",
      "154/223, train_loss: 0.1190, step time: 0.1154\n",
      "155/223, train_loss: 0.1162, step time: 0.0995\n",
      "156/223, train_loss: 0.0965, step time: 0.1003\n",
      "157/223, train_loss: 0.1055, step time: 0.1130\n",
      "158/223, train_loss: 0.0977, step time: 0.1021\n",
      "159/223, train_loss: 0.1095, step time: 0.1014\n",
      "160/223, train_loss: 0.0990, step time: 0.1089\n",
      "161/223, train_loss: 0.1226, step time: 0.0997\n",
      "162/223, train_loss: 0.1145, step time: 0.0988\n",
      "163/223, train_loss: 0.1133, step time: 0.1006\n",
      "164/223, train_loss: 0.1121, step time: 0.1013\n",
      "165/223, train_loss: 0.1078, step time: 0.1334\n",
      "166/223, train_loss: 0.1156, step time: 0.1005\n",
      "167/223, train_loss: 0.1065, step time: 0.1004\n",
      "168/223, train_loss: 0.0999, step time: 0.1017\n",
      "169/223, train_loss: 0.0990, step time: 0.1063\n",
      "170/223, train_loss: 0.1036, step time: 0.1258\n",
      "171/223, train_loss: 0.1055, step time: 0.1011\n",
      "172/223, train_loss: 0.0959, step time: 0.1005\n",
      "173/223, train_loss: 0.0955, step time: 0.1009\n",
      "174/223, train_loss: 0.1036, step time: 0.1022\n",
      "175/223, train_loss: 0.1120, step time: 0.1127\n",
      "176/223, train_loss: 0.1138, step time: 0.1089\n",
      "177/223, train_loss: 0.1145, step time: 0.1326\n",
      "178/223, train_loss: 0.1075, step time: 0.1124\n",
      "179/223, train_loss: 0.0970, step time: 0.1023\n",
      "180/223, train_loss: 0.1185, step time: 0.1092\n",
      "181/223, train_loss: 0.1035, step time: 0.1251\n",
      "182/223, train_loss: 0.1051, step time: 0.1234\n",
      "183/223, train_loss: 0.1092, step time: 0.1162\n",
      "184/223, train_loss: 0.1026, step time: 0.1138\n",
      "185/223, train_loss: 0.1077, step time: 0.1202\n",
      "186/223, train_loss: 0.0973, step time: 0.1028\n",
      "187/223, train_loss: 0.1136, step time: 0.1231\n",
      "188/223, train_loss: 0.1013, step time: 0.1125\n",
      "189/223, train_loss: 0.0990, step time: 0.1099\n",
      "190/223, train_loss: 0.0988, step time: 0.1144\n",
      "191/223, train_loss: 0.1035, step time: 0.1146\n",
      "192/223, train_loss: 0.1026, step time: 0.1146\n",
      "193/223, train_loss: 0.0984, step time: 0.1117\n",
      "194/223, train_loss: 0.1028, step time: 0.1238\n",
      "195/223, train_loss: 0.0956, step time: 0.1150\n",
      "196/223, train_loss: 0.1165, step time: 0.1234\n",
      "197/223, train_loss: 0.1111, step time: 0.1228\n",
      "198/223, train_loss: 0.1047, step time: 0.1021\n",
      "199/223, train_loss: 0.1016, step time: 0.1142\n",
      "200/223, train_loss: 0.1070, step time: 0.1122\n",
      "201/223, train_loss: 0.1019, step time: 0.1159\n",
      "202/223, train_loss: 0.0933, step time: 0.1194\n",
      "203/223, train_loss: 0.1041, step time: 0.1043\n",
      "204/223, train_loss: 0.0951, step time: 0.1251\n",
      "205/223, train_loss: 0.1033, step time: 0.1314\n",
      "206/223, train_loss: 0.1039, step time: 0.1058\n",
      "207/223, train_loss: 0.1160, step time: 0.1130\n",
      "208/223, train_loss: 0.1102, step time: 0.1122\n",
      "209/223, train_loss: 0.0978, step time: 0.1119\n",
      "210/223, train_loss: 0.1023, step time: 0.0995\n",
      "211/223, train_loss: 0.1128, step time: 0.1113\n",
      "212/223, train_loss: 0.1014, step time: 0.1143\n",
      "213/223, train_loss: 0.1059, step time: 0.1103\n",
      "214/223, train_loss: 0.1147, step time: 0.1179\n",
      "215/223, train_loss: 0.1041, step time: 0.1127\n",
      "216/223, train_loss: 0.1014, step time: 0.1065\n",
      "217/223, train_loss: 0.0940, step time: 0.1133\n",
      "218/223, train_loss: 0.0965, step time: 0.1006\n",
      "219/223, train_loss: 0.1043, step time: 0.0999\n",
      "220/223, train_loss: 0.0920, step time: 0.1005\n",
      "221/223, train_loss: 0.1049, step time: 0.1008\n",
      "222/223, train_loss: 0.1012, step time: 0.1040\n",
      "223/223, train_loss: 0.0933, step time: 0.0995\n",
      "epoch 167 average loss: 0.1054\n",
      "time consuming of epoch 167 is: 88.1517\n",
      "----------\n",
      "epoch 168/300\n",
      "1/223, train_loss: 0.1053, step time: 0.1085\n",
      "2/223, train_loss: 0.1004, step time: 0.1100\n",
      "3/223, train_loss: 0.1050, step time: 0.1194\n",
      "4/223, train_loss: 0.0924, step time: 0.1009\n",
      "5/223, train_loss: 0.0984, step time: 0.1151\n",
      "6/223, train_loss: 0.0956, step time: 0.1219\n",
      "7/223, train_loss: 0.0963, step time: 0.1296\n",
      "8/223, train_loss: 0.1025, step time: 0.1232\n",
      "9/223, train_loss: 0.1145, step time: 0.1089\n",
      "10/223, train_loss: 0.0965, step time: 0.1228\n",
      "11/223, train_loss: 0.1181, step time: 0.1024\n",
      "12/223, train_loss: 0.1090, step time: 0.1391\n",
      "13/223, train_loss: 0.1031, step time: 0.1111\n",
      "14/223, train_loss: 0.1031, step time: 0.1645\n",
      "15/223, train_loss: 0.0999, step time: 0.1171\n",
      "16/223, train_loss: 0.1131, step time: 0.1162\n",
      "17/223, train_loss: 0.1069, step time: 0.1053\n",
      "18/223, train_loss: 0.1052, step time: 0.1080\n",
      "19/223, train_loss: 0.1150, step time: 0.1165\n",
      "20/223, train_loss: 0.1066, step time: 0.1031\n",
      "21/223, train_loss: 0.0960, step time: 0.1004\n",
      "22/223, train_loss: 0.0957, step time: 0.1007\n",
      "23/223, train_loss: 0.1002, step time: 0.0988\n",
      "24/223, train_loss: 0.1116, step time: 0.0987\n",
      "25/223, train_loss: 0.1150, step time: 0.1214\n",
      "26/223, train_loss: 0.1093, step time: 0.1181\n",
      "27/223, train_loss: 0.0985, step time: 0.1209\n",
      "28/223, train_loss: 0.1092, step time: 0.1136\n",
      "29/223, train_loss: 0.1023, step time: 0.1009\n",
      "30/223, train_loss: 0.1066, step time: 0.1257\n",
      "31/223, train_loss: 0.1012, step time: 0.1102\n",
      "32/223, train_loss: 0.1030, step time: 0.1106\n",
      "33/223, train_loss: 0.1070, step time: 0.0996\n",
      "34/223, train_loss: 0.1035, step time: 0.1254\n",
      "35/223, train_loss: 0.1131, step time: 0.1491\n",
      "36/223, train_loss: 0.0994, step time: 0.1018\n",
      "37/223, train_loss: 0.1132, step time: 0.1009\n",
      "38/223, train_loss: 0.1056, step time: 0.1131\n",
      "39/223, train_loss: 0.0944, step time: 0.1342\n",
      "40/223, train_loss: 0.1115, step time: 0.1159\n",
      "41/223, train_loss: 0.1070, step time: 0.1114\n",
      "42/223, train_loss: 0.1035, step time: 0.1217\n",
      "43/223, train_loss: 0.0933, step time: 0.1102\n",
      "44/223, train_loss: 0.0966, step time: 0.1040\n",
      "45/223, train_loss: 0.1050, step time: 0.1246\n",
      "46/223, train_loss: 0.0905, step time: 0.1086\n",
      "47/223, train_loss: 0.1156, step time: 0.1176\n",
      "48/223, train_loss: 0.1157, step time: 0.1163\n",
      "49/223, train_loss: 0.1112, step time: 0.1103\n",
      "50/223, train_loss: 0.1021, step time: 0.1165\n",
      "51/223, train_loss: 0.1033, step time: 0.1101\n",
      "52/223, train_loss: 0.0987, step time: 0.1294\n",
      "53/223, train_loss: 0.1079, step time: 0.1009\n",
      "54/223, train_loss: 0.0948, step time: 0.1047\n",
      "55/223, train_loss: 0.0891, step time: 0.1030\n",
      "56/223, train_loss: 0.0948, step time: 0.1156\n",
      "57/223, train_loss: 0.0977, step time: 0.1230\n",
      "58/223, train_loss: 0.0988, step time: 0.1102\n",
      "59/223, train_loss: 0.0966, step time: 0.1160\n",
      "60/223, train_loss: 0.1037, step time: 0.1193\n",
      "61/223, train_loss: 0.1073, step time: 0.1255\n",
      "62/223, train_loss: 0.1060, step time: 0.1003\n",
      "63/223, train_loss: 0.0975, step time: 0.1012\n",
      "64/223, train_loss: 0.0996, step time: 0.1328\n",
      "65/223, train_loss: 0.0992, step time: 0.1076\n",
      "66/223, train_loss: 0.0957, step time: 0.1137\n",
      "67/223, train_loss: 0.1037, step time: 0.1007\n",
      "68/223, train_loss: 0.1116, step time: 0.1011\n",
      "69/223, train_loss: 0.1086, step time: 0.1000\n",
      "70/223, train_loss: 0.1157, step time: 0.1119\n",
      "71/223, train_loss: 0.1079, step time: 0.1072\n",
      "72/223, train_loss: 0.1106, step time: 0.1055\n",
      "73/223, train_loss: 0.1037, step time: 0.0997\n",
      "74/223, train_loss: 0.0993, step time: 0.1211\n",
      "75/223, train_loss: 0.0976, step time: 0.0998\n",
      "76/223, train_loss: 0.1104, step time: 0.1012\n",
      "77/223, train_loss: 0.1065, step time: 0.1002\n",
      "78/223, train_loss: 0.1014, step time: 0.1114\n",
      "79/223, train_loss: 0.1194, step time: 0.1133\n",
      "80/223, train_loss: 0.1067, step time: 0.1046\n",
      "81/223, train_loss: 0.1038, step time: 0.1012\n",
      "82/223, train_loss: 0.1090, step time: 0.1001\n",
      "83/223, train_loss: 0.0997, step time: 0.1012\n",
      "84/223, train_loss: 0.0971, step time: 0.1020\n",
      "85/223, train_loss: 0.1019, step time: 0.1010\n",
      "86/223, train_loss: 0.0943, step time: 0.1039\n",
      "87/223, train_loss: 0.1119, step time: 0.1003\n",
      "88/223, train_loss: 0.0991, step time: 0.1100\n",
      "89/223, train_loss: 0.1057, step time: 0.1173\n",
      "90/223, train_loss: 0.0964, step time: 0.1100\n",
      "91/223, train_loss: 0.1244, step time: 0.1105\n",
      "92/223, train_loss: 0.1053, step time: 0.1020\n",
      "93/223, train_loss: 0.1123, step time: 0.1000\n",
      "94/223, train_loss: 0.0976, step time: 0.1014\n",
      "95/223, train_loss: 0.0941, step time: 0.1107\n",
      "96/223, train_loss: 0.1177, step time: 0.1004\n",
      "97/223, train_loss: 0.0953, step time: 0.1008\n",
      "98/223, train_loss: 0.0989, step time: 0.1149\n",
      "99/223, train_loss: 0.1020, step time: 0.0999\n",
      "100/223, train_loss: 0.1010, step time: 0.1194\n",
      "101/223, train_loss: 0.1060, step time: 0.1021\n",
      "102/223, train_loss: 0.1048, step time: 0.1135\n",
      "103/223, train_loss: 0.0969, step time: 0.1005\n",
      "104/223, train_loss: 0.1010, step time: 0.1013\n",
      "105/223, train_loss: 0.1103, step time: 0.1011\n",
      "106/223, train_loss: 0.1026, step time: 0.1119\n",
      "107/223, train_loss: 0.1050, step time: 0.1067\n",
      "108/223, train_loss: 0.1055, step time: 0.1225\n",
      "109/223, train_loss: 0.1044, step time: 0.1324\n",
      "110/223, train_loss: 0.1111, step time: 0.1100\n",
      "111/223, train_loss: 0.1122, step time: 0.1000\n",
      "112/223, train_loss: 0.1057, step time: 0.1126\n",
      "113/223, train_loss: 0.0960, step time: 0.1031\n",
      "114/223, train_loss: 0.0982, step time: 0.1235\n",
      "115/223, train_loss: 0.0988, step time: 0.0998\n",
      "116/223, train_loss: 0.1038, step time: 0.1012\n",
      "117/223, train_loss: 0.0989, step time: 0.1055\n",
      "118/223, train_loss: 0.1007, step time: 0.1268\n",
      "119/223, train_loss: 0.1071, step time: 0.1028\n",
      "120/223, train_loss: 0.1002, step time: 0.1351\n",
      "121/223, train_loss: 0.1080, step time: 0.1002\n",
      "122/223, train_loss: 0.1005, step time: 0.1125\n",
      "123/223, train_loss: 0.1022, step time: 0.1046\n",
      "124/223, train_loss: 0.1028, step time: 0.1290\n",
      "125/223, train_loss: 0.1039, step time: 0.1005\n",
      "126/223, train_loss: 0.1060, step time: 0.1154\n",
      "127/223, train_loss: 0.1065, step time: 0.1004\n",
      "128/223, train_loss: 0.1075, step time: 0.1088\n",
      "129/223, train_loss: 0.1095, step time: 0.1017\n",
      "130/223, train_loss: 0.1117, step time: 0.1174\n",
      "131/223, train_loss: 0.1154, step time: 0.0999\n",
      "132/223, train_loss: 0.1069, step time: 0.1203\n",
      "133/223, train_loss: 0.1030, step time: 0.1006\n",
      "134/223, train_loss: 0.2988, step time: 0.1097\n",
      "135/223, train_loss: 0.1176, step time: 0.1098\n",
      "136/223, train_loss: 0.1059, step time: 0.1167\n",
      "137/223, train_loss: 0.0979, step time: 0.1031\n",
      "138/223, train_loss: 0.0988, step time: 0.1166\n",
      "139/223, train_loss: 0.1194, step time: 0.1001\n",
      "140/223, train_loss: 0.0994, step time: 0.1073\n",
      "141/223, train_loss: 0.1206, step time: 0.1010\n",
      "142/223, train_loss: 0.1057, step time: 0.1069\n",
      "143/223, train_loss: 0.1014, step time: 0.1005\n",
      "144/223, train_loss: 0.0915, step time: 0.1093\n",
      "145/223, train_loss: 0.1006, step time: 0.1034\n",
      "146/223, train_loss: 0.1104, step time: 0.1052\n",
      "147/223, train_loss: 0.1018, step time: 0.1006\n",
      "148/223, train_loss: 0.1092, step time: 0.1000\n",
      "149/223, train_loss: 0.0943, step time: 0.1196\n",
      "150/223, train_loss: 0.0992, step time: 0.1012\n",
      "151/223, train_loss: 0.1048, step time: 0.1065\n",
      "152/223, train_loss: 0.1046, step time: 0.1141\n",
      "153/223, train_loss: 0.0984, step time: 0.1014\n",
      "154/223, train_loss: 0.0994, step time: 0.1139\n",
      "155/223, train_loss: 0.1035, step time: 0.1166\n",
      "156/223, train_loss: 0.0982, step time: 0.1106\n",
      "157/223, train_loss: 0.0939, step time: 0.1105\n",
      "158/223, train_loss: 0.1084, step time: 0.1096\n",
      "159/223, train_loss: 0.1176, step time: 0.1063\n",
      "160/223, train_loss: 0.0952, step time: 0.1028\n",
      "161/223, train_loss: 0.1037, step time: 0.1001\n",
      "162/223, train_loss: 0.1008, step time: 0.0998\n",
      "163/223, train_loss: 0.1111, step time: 0.1010\n",
      "164/223, train_loss: 0.1029, step time: 0.1005\n",
      "165/223, train_loss: 0.1063, step time: 0.1044\n",
      "166/223, train_loss: 0.1039, step time: 0.1343\n",
      "167/223, train_loss: 0.1117, step time: 0.1109\n",
      "168/223, train_loss: 0.1182, step time: 0.1392\n",
      "169/223, train_loss: 0.1162, step time: 0.0999\n",
      "170/223, train_loss: 0.1026, step time: 0.1052\n",
      "171/223, train_loss: 0.0900, step time: 0.1146\n",
      "172/223, train_loss: 0.1073, step time: 0.1007\n",
      "173/223, train_loss: 0.1008, step time: 0.1005\n",
      "174/223, train_loss: 0.1178, step time: 0.0996\n",
      "175/223, train_loss: 0.1083, step time: 0.1195\n",
      "176/223, train_loss: 0.1046, step time: 0.1010\n",
      "177/223, train_loss: 0.0988, step time: 0.1014\n",
      "178/223, train_loss: 0.1002, step time: 0.1144\n",
      "179/223, train_loss: 0.1212, step time: 0.1142\n",
      "180/223, train_loss: 0.1090, step time: 0.1095\n",
      "181/223, train_loss: 0.1010, step time: 0.1001\n",
      "182/223, train_loss: 0.0919, step time: 0.1154\n",
      "183/223, train_loss: 0.1023, step time: 0.0996\n",
      "184/223, train_loss: 0.0994, step time: 0.1031\n",
      "185/223, train_loss: 0.1072, step time: 0.1016\n",
      "186/223, train_loss: 0.0951, step time: 0.1038\n",
      "187/223, train_loss: 0.1066, step time: 0.1002\n",
      "188/223, train_loss: 0.1023, step time: 0.1204\n",
      "189/223, train_loss: 0.1115, step time: 0.1073\n",
      "190/223, train_loss: 0.0987, step time: 0.1002\n",
      "191/223, train_loss: 0.1159, step time: 0.1052\n",
      "192/223, train_loss: 0.1072, step time: 0.1041\n",
      "193/223, train_loss: 0.1183, step time: 0.1099\n",
      "194/223, train_loss: 0.1094, step time: 0.1042\n",
      "195/223, train_loss: 0.1004, step time: 0.1007\n",
      "196/223, train_loss: 0.0978, step time: 0.1017\n",
      "197/223, train_loss: 0.0950, step time: 0.1104\n",
      "198/223, train_loss: 0.1001, step time: 0.1102\n",
      "199/223, train_loss: 0.1109, step time: 0.1033\n",
      "200/223, train_loss: 0.1021, step time: 0.1003\n",
      "201/223, train_loss: 0.1030, step time: 0.1097\n",
      "202/223, train_loss: 0.1036, step time: 0.1378\n",
      "203/223, train_loss: 0.1097, step time: 0.1000\n",
      "204/223, train_loss: 0.1076, step time: 0.1005\n",
      "205/223, train_loss: 0.1107, step time: 0.1105\n",
      "206/223, train_loss: 0.0998, step time: 0.1166\n",
      "207/223, train_loss: 0.1105, step time: 0.1201\n",
      "208/223, train_loss: 0.1026, step time: 0.1007\n",
      "209/223, train_loss: 0.0917, step time: 0.1007\n",
      "210/223, train_loss: 0.1127, step time: 0.1008\n",
      "211/223, train_loss: 0.1121, step time: 0.0998\n",
      "212/223, train_loss: 0.1044, step time: 0.1440\n",
      "213/223, train_loss: 0.1098, step time: 0.1001\n",
      "214/223, train_loss: 0.1030, step time: 0.1171\n",
      "215/223, train_loss: 0.1138, step time: 0.1023\n",
      "216/223, train_loss: 0.0975, step time: 0.1008\n",
      "217/223, train_loss: 0.1105, step time: 0.1118\n",
      "218/223, train_loss: 0.0992, step time: 0.1006\n",
      "219/223, train_loss: 0.1098, step time: 0.1016\n",
      "220/223, train_loss: 0.1033, step time: 0.1069\n",
      "221/223, train_loss: 0.1102, step time: 0.1017\n",
      "222/223, train_loss: 0.0970, step time: 0.0992\n",
      "223/223, train_loss: 0.1032, step time: 0.0997\n",
      "epoch 168 average loss: 0.1052\n",
      "time consuming of epoch 168 is: 87.3453\n",
      "----------\n",
      "epoch 169/300\n",
      "1/223, train_loss: 0.1111, step time: 0.1137\n",
      "2/223, train_loss: 0.0969, step time: 0.1127\n",
      "3/223, train_loss: 0.1124, step time: 0.1020\n",
      "4/223, train_loss: 0.1001, step time: 0.1092\n",
      "5/223, train_loss: 0.1122, step time: 0.1002\n",
      "6/223, train_loss: 0.1039, step time: 0.1000\n",
      "7/223, train_loss: 0.0967, step time: 0.1097\n",
      "8/223, train_loss: 0.1010, step time: 0.1139\n",
      "9/223, train_loss: 0.1030, step time: 0.1184\n",
      "10/223, train_loss: 0.1182, step time: 0.1031\n",
      "11/223, train_loss: 0.1003, step time: 0.1000\n",
      "12/223, train_loss: 0.1036, step time: 0.1007\n",
      "13/223, train_loss: 0.0887, step time: 0.1082\n",
      "14/223, train_loss: 0.0992, step time: 0.1141\n",
      "15/223, train_loss: 0.1022, step time: 0.1161\n",
      "16/223, train_loss: 0.1005, step time: 0.1068\n",
      "17/223, train_loss: 0.1064, step time: 0.1115\n",
      "18/223, train_loss: 0.0979, step time: 0.1035\n",
      "19/223, train_loss: 0.1171, step time: 0.0994\n",
      "20/223, train_loss: 0.1002, step time: 0.1161\n",
      "21/223, train_loss: 0.1127, step time: 0.1000\n",
      "22/223, train_loss: 0.1032, step time: 0.1015\n",
      "23/223, train_loss: 0.1078, step time: 0.1061\n",
      "24/223, train_loss: 0.1151, step time: 0.1203\n",
      "25/223, train_loss: 0.0933, step time: 0.1054\n",
      "26/223, train_loss: 0.1030, step time: 0.1005\n",
      "27/223, train_loss: 0.1104, step time: 0.1251\n",
      "28/223, train_loss: 0.1041, step time: 0.1136\n",
      "29/223, train_loss: 0.0961, step time: 0.1047\n",
      "30/223, train_loss: 0.0999, step time: 0.1002\n",
      "31/223, train_loss: 0.0995, step time: 0.1109\n",
      "32/223, train_loss: 0.1043, step time: 0.1113\n",
      "33/223, train_loss: 0.0969, step time: 0.1018\n",
      "34/223, train_loss: 0.0997, step time: 0.1102\n",
      "35/223, train_loss: 0.1007, step time: 0.1030\n",
      "36/223, train_loss: 0.1004, step time: 0.0995\n",
      "37/223, train_loss: 0.1025, step time: 0.1052\n",
      "38/223, train_loss: 0.1191, step time: 0.1238\n",
      "39/223, train_loss: 0.1047, step time: 0.1001\n",
      "40/223, train_loss: 0.1016, step time: 0.1041\n",
      "41/223, train_loss: 0.3001, step time: 0.1290\n",
      "42/223, train_loss: 0.1071, step time: 0.1182\n",
      "43/223, train_loss: 0.1018, step time: 0.1007\n",
      "44/223, train_loss: 0.0996, step time: 0.1008\n",
      "45/223, train_loss: 0.1054, step time: 0.1022\n",
      "46/223, train_loss: 0.0994, step time: 0.1015\n",
      "47/223, train_loss: 0.1004, step time: 0.1139\n",
      "48/223, train_loss: 0.1041, step time: 0.1194\n",
      "49/223, train_loss: 0.1011, step time: 0.1145\n",
      "50/223, train_loss: 0.1154, step time: 0.1142\n",
      "51/223, train_loss: 0.1016, step time: 0.1149\n",
      "52/223, train_loss: 0.1045, step time: 0.1098\n",
      "53/223, train_loss: 0.1001, step time: 0.1049\n",
      "54/223, train_loss: 0.1025, step time: 0.1041\n",
      "55/223, train_loss: 0.1133, step time: 0.1299\n",
      "56/223, train_loss: 0.1025, step time: 0.1094\n",
      "57/223, train_loss: 0.1064, step time: 0.1017\n",
      "58/223, train_loss: 0.0987, step time: 0.1005\n",
      "59/223, train_loss: 0.1011, step time: 0.1050\n",
      "60/223, train_loss: 0.1132, step time: 0.1152\n",
      "61/223, train_loss: 0.1044, step time: 0.1060\n",
      "62/223, train_loss: 0.1044, step time: 0.1082\n",
      "63/223, train_loss: 0.1093, step time: 0.1158\n",
      "64/223, train_loss: 0.1001, step time: 0.1107\n",
      "65/223, train_loss: 0.1037, step time: 0.1064\n",
      "66/223, train_loss: 0.1028, step time: 0.1047\n",
      "67/223, train_loss: 0.1110, step time: 0.1112\n",
      "68/223, train_loss: 0.1022, step time: 0.1225\n",
      "69/223, train_loss: 0.1082, step time: 0.1078\n",
      "70/223, train_loss: 0.0993, step time: 0.0998\n",
      "71/223, train_loss: 0.1026, step time: 0.1014\n",
      "72/223, train_loss: 0.1130, step time: 0.1099\n",
      "73/223, train_loss: 0.0964, step time: 0.1396\n",
      "74/223, train_loss: 0.1013, step time: 0.1007\n",
      "75/223, train_loss: 0.1095, step time: 0.1070\n",
      "76/223, train_loss: 0.1028, step time: 0.1010\n",
      "77/223, train_loss: 0.1105, step time: 0.1046\n",
      "78/223, train_loss: 0.0965, step time: 0.1001\n",
      "79/223, train_loss: 0.1109, step time: 0.1098\n",
      "80/223, train_loss: 0.0999, step time: 0.1022\n",
      "81/223, train_loss: 0.1057, step time: 0.1104\n",
      "82/223, train_loss: 0.0965, step time: 0.1177\n",
      "83/223, train_loss: 0.1036, step time: 0.1237\n",
      "84/223, train_loss: 0.0999, step time: 0.1093\n",
      "85/223, train_loss: 0.0956, step time: 0.1072\n",
      "86/223, train_loss: 0.1056, step time: 0.1077\n",
      "87/223, train_loss: 0.0964, step time: 0.1313\n",
      "88/223, train_loss: 0.1019, step time: 0.1165\n",
      "89/223, train_loss: 0.1008, step time: 0.1265\n",
      "90/223, train_loss: 0.0927, step time: 0.1207\n",
      "91/223, train_loss: 0.1098, step time: 0.1072\n",
      "92/223, train_loss: 0.1013, step time: 0.1117\n",
      "93/223, train_loss: 0.1054, step time: 0.1084\n",
      "94/223, train_loss: 0.1064, step time: 0.1008\n",
      "95/223, train_loss: 0.1025, step time: 0.0990\n",
      "96/223, train_loss: 0.0958, step time: 0.1155\n",
      "97/223, train_loss: 0.1050, step time: 0.1192\n",
      "98/223, train_loss: 0.1074, step time: 0.1005\n",
      "99/223, train_loss: 0.1129, step time: 0.1330\n",
      "100/223, train_loss: 0.1078, step time: 0.1092\n",
      "101/223, train_loss: 0.1055, step time: 0.1011\n",
      "102/223, train_loss: 0.1127, step time: 0.1005\n",
      "103/223, train_loss: 0.1061, step time: 0.1135\n",
      "104/223, train_loss: 0.0999, step time: 0.1003\n",
      "105/223, train_loss: 0.0989, step time: 0.1011\n",
      "106/223, train_loss: 0.0917, step time: 0.1207\n",
      "107/223, train_loss: 0.1142, step time: 0.1144\n",
      "108/223, train_loss: 0.1126, step time: 0.1053\n",
      "109/223, train_loss: 0.1177, step time: 0.1059\n",
      "110/223, train_loss: 0.1220, step time: 0.1035\n",
      "111/223, train_loss: 0.1110, step time: 0.1073\n",
      "112/223, train_loss: 0.1013, step time: 0.1053\n",
      "113/223, train_loss: 0.1108, step time: 0.1161\n",
      "114/223, train_loss: 0.1129, step time: 0.1132\n",
      "115/223, train_loss: 0.1024, step time: 0.1073\n",
      "116/223, train_loss: 0.0947, step time: 0.1125\n",
      "117/223, train_loss: 0.0967, step time: 0.1013\n",
      "118/223, train_loss: 0.1061, step time: 0.0998\n",
      "119/223, train_loss: 0.1008, step time: 0.1137\n",
      "120/223, train_loss: 0.1088, step time: 0.1083\n",
      "121/223, train_loss: 0.0986, step time: 0.1115\n",
      "122/223, train_loss: 0.0969, step time: 0.1026\n",
      "123/223, train_loss: 0.1053, step time: 0.1031\n",
      "124/223, train_loss: 0.1005, step time: 0.1023\n",
      "125/223, train_loss: 0.0911, step time: 0.1154\n",
      "126/223, train_loss: 0.1091, step time: 0.0996\n",
      "127/223, train_loss: 0.1043, step time: 0.1119\n",
      "128/223, train_loss: 0.1026, step time: 0.1005\n",
      "129/223, train_loss: 0.0926, step time: 0.1008\n",
      "130/223, train_loss: 0.1273, step time: 0.1006\n",
      "131/223, train_loss: 0.1033, step time: 0.1066\n",
      "132/223, train_loss: 0.1032, step time: 0.1333\n",
      "133/223, train_loss: 0.0920, step time: 0.1038\n",
      "134/223, train_loss: 0.1057, step time: 0.1122\n",
      "135/223, train_loss: 0.1054, step time: 0.1183\n",
      "136/223, train_loss: 0.1194, step time: 0.1244\n",
      "137/223, train_loss: 0.1093, step time: 0.1196\n",
      "138/223, train_loss: 0.1175, step time: 0.1002\n",
      "139/223, train_loss: 0.1004, step time: 0.1045\n",
      "140/223, train_loss: 0.1039, step time: 0.1086\n",
      "141/223, train_loss: 0.1049, step time: 0.1063\n",
      "142/223, train_loss: 0.1035, step time: 0.1053\n",
      "143/223, train_loss: 0.1035, step time: 0.1018\n",
      "144/223, train_loss: 0.1157, step time: 0.1002\n",
      "145/223, train_loss: 0.1104, step time: 0.1013\n",
      "146/223, train_loss: 0.1120, step time: 0.1325\n",
      "147/223, train_loss: 0.1194, step time: 0.1116\n",
      "148/223, train_loss: 0.1089, step time: 0.1088\n",
      "149/223, train_loss: 0.1086, step time: 0.1087\n",
      "150/223, train_loss: 0.1182, step time: 0.1023\n",
      "151/223, train_loss: 0.0995, step time: 0.0997\n",
      "152/223, train_loss: 0.1095, step time: 0.1059\n",
      "153/223, train_loss: 0.0934, step time: 0.1134\n",
      "154/223, train_loss: 0.1032, step time: 0.1087\n",
      "155/223, train_loss: 0.1093, step time: 0.1088\n",
      "156/223, train_loss: 0.1182, step time: 0.1065\n",
      "157/223, train_loss: 0.1179, step time: 0.1064\n",
      "158/223, train_loss: 0.1205, step time: 0.1010\n",
      "159/223, train_loss: 0.0941, step time: 0.1042\n",
      "160/223, train_loss: 0.1110, step time: 0.1214\n",
      "161/223, train_loss: 0.1007, step time: 0.1167\n",
      "162/223, train_loss: 0.1094, step time: 0.1004\n",
      "163/223, train_loss: 0.1051, step time: 0.1167\n",
      "164/223, train_loss: 0.1001, step time: 0.1110\n",
      "165/223, train_loss: 0.1092, step time: 0.0999\n",
      "166/223, train_loss: 0.1025, step time: 0.1103\n",
      "167/223, train_loss: 0.1037, step time: 0.1101\n",
      "168/223, train_loss: 0.1081, step time: 0.0998\n",
      "169/223, train_loss: 0.1031, step time: 0.1006\n",
      "170/223, train_loss: 0.1021, step time: 0.1129\n",
      "171/223, train_loss: 0.1152, step time: 0.1171\n",
      "172/223, train_loss: 0.1045, step time: 0.0994\n",
      "173/223, train_loss: 0.1140, step time: 0.1012\n",
      "174/223, train_loss: 0.1082, step time: 0.1052\n",
      "175/223, train_loss: 0.1142, step time: 0.1193\n",
      "176/223, train_loss: 0.1034, step time: 0.1137\n",
      "177/223, train_loss: 0.1176, step time: 0.1009\n",
      "178/223, train_loss: 0.1085, step time: 0.1013\n",
      "179/223, train_loss: 0.1048, step time: 0.1001\n",
      "180/223, train_loss: 0.0971, step time: 0.1046\n",
      "181/223, train_loss: 0.1159, step time: 0.1104\n",
      "182/223, train_loss: 0.1172, step time: 0.1080\n",
      "183/223, train_loss: 0.0997, step time: 0.1007\n",
      "184/223, train_loss: 0.1106, step time: 0.1006\n",
      "185/223, train_loss: 0.1049, step time: 0.1010\n",
      "186/223, train_loss: 0.0978, step time: 0.1045\n",
      "187/223, train_loss: 0.0979, step time: 0.1091\n",
      "188/223, train_loss: 0.1140, step time: 0.1102\n",
      "189/223, train_loss: 0.1058, step time: 0.1221\n",
      "190/223, train_loss: 0.1088, step time: 0.1032\n",
      "191/223, train_loss: 0.1038, step time: 0.1159\n",
      "192/223, train_loss: 0.1065, step time: 0.0995\n",
      "193/223, train_loss: 0.1071, step time: 0.1004\n",
      "194/223, train_loss: 0.1152, step time: 0.1192\n",
      "195/223, train_loss: 0.0955, step time: 0.1129\n",
      "196/223, train_loss: 0.1012, step time: 0.1002\n",
      "197/223, train_loss: 0.1006, step time: 0.1124\n",
      "198/223, train_loss: 0.1095, step time: 0.1416\n",
      "199/223, train_loss: 0.0976, step time: 0.1123\n",
      "200/223, train_loss: 0.0963, step time: 0.0995\n",
      "201/223, train_loss: 0.0924, step time: 0.1002\n",
      "202/223, train_loss: 0.1046, step time: 0.1165\n",
      "203/223, train_loss: 0.1042, step time: 0.1065\n",
      "204/223, train_loss: 0.0922, step time: 0.1102\n",
      "205/223, train_loss: 0.1018, step time: 0.1054\n",
      "206/223, train_loss: 0.1108, step time: 0.1181\n",
      "207/223, train_loss: 0.1052, step time: 0.1176\n",
      "208/223, train_loss: 0.1057, step time: 0.1000\n",
      "209/223, train_loss: 0.1132, step time: 0.1065\n",
      "210/223, train_loss: 0.0938, step time: 0.1102\n",
      "211/223, train_loss: 0.1046, step time: 0.1112\n",
      "212/223, train_loss: 0.0927, step time: 0.0997\n",
      "213/223, train_loss: 0.1071, step time: 0.1041\n",
      "214/223, train_loss: 0.1009, step time: 0.1004\n",
      "215/223, train_loss: 0.1037, step time: 0.1398\n",
      "216/223, train_loss: 0.1019, step time: 0.1350\n",
      "217/223, train_loss: 0.0998, step time: 0.1073\n",
      "218/223, train_loss: 0.0982, step time: 0.1140\n",
      "219/223, train_loss: 0.1104, step time: 0.1001\n",
      "220/223, train_loss: 0.1082, step time: 0.1003\n",
      "221/223, train_loss: 0.1152, step time: 0.1010\n",
      "222/223, train_loss: 0.1044, step time: 0.1006\n",
      "223/223, train_loss: 0.1142, step time: 0.0988\n",
      "epoch 169 average loss: 0.1058\n",
      "time consuming of epoch 169 is: 87.1239\n",
      "----------\n",
      "epoch 170/300\n",
      "1/223, train_loss: 0.1145, step time: 0.1076\n",
      "2/223, train_loss: 0.1044, step time: 0.1156\n",
      "3/223, train_loss: 0.1088, step time: 0.1142\n",
      "4/223, train_loss: 0.1008, step time: 0.1000\n",
      "5/223, train_loss: 0.1074, step time: 0.1177\n",
      "6/223, train_loss: 0.1027, step time: 0.1122\n",
      "7/223, train_loss: 0.1071, step time: 0.1306\n",
      "8/223, train_loss: 0.0957, step time: 0.1216\n",
      "9/223, train_loss: 0.1093, step time: 0.1105\n",
      "10/223, train_loss: 0.0959, step time: 0.1135\n",
      "11/223, train_loss: 0.1055, step time: 0.1187\n",
      "12/223, train_loss: 0.1071, step time: 0.1109\n",
      "13/223, train_loss: 0.1131, step time: 0.1147\n",
      "14/223, train_loss: 0.1048, step time: 0.1113\n",
      "15/223, train_loss: 0.0963, step time: 0.1186\n",
      "16/223, train_loss: 0.1100, step time: 0.1000\n",
      "17/223, train_loss: 0.1024, step time: 0.1077\n",
      "18/223, train_loss: 0.1172, step time: 0.1211\n",
      "19/223, train_loss: 0.1026, step time: 0.1248\n",
      "20/223, train_loss: 0.1012, step time: 0.1080\n",
      "21/223, train_loss: 0.1126, step time: 0.1179\n",
      "22/223, train_loss: 0.0970, step time: 0.1116\n",
      "23/223, train_loss: 0.1054, step time: 0.1116\n",
      "24/223, train_loss: 0.1074, step time: 0.1044\n",
      "25/223, train_loss: 0.0907, step time: 0.1182\n",
      "26/223, train_loss: 0.1059, step time: 0.1018\n",
      "27/223, train_loss: 0.1029, step time: 0.1019\n",
      "28/223, train_loss: 0.0997, step time: 0.1184\n",
      "29/223, train_loss: 0.1067, step time: 0.1130\n",
      "30/223, train_loss: 0.0979, step time: 0.1011\n",
      "31/223, train_loss: 0.1020, step time: 0.1001\n",
      "32/223, train_loss: 0.0929, step time: 0.1098\n",
      "33/223, train_loss: 0.1121, step time: 0.0990\n",
      "34/223, train_loss: 0.1177, step time: 0.1163\n",
      "35/223, train_loss: 0.1097, step time: 0.1058\n",
      "36/223, train_loss: 0.1124, step time: 0.1234\n",
      "37/223, train_loss: 0.0972, step time: 0.0983\n",
      "38/223, train_loss: 0.0968, step time: 0.1144\n",
      "39/223, train_loss: 0.1062, step time: 0.1135\n",
      "40/223, train_loss: 0.0983, step time: 0.0991\n",
      "41/223, train_loss: 0.1035, step time: 0.0993\n",
      "42/223, train_loss: 0.0989, step time: 0.0999\n",
      "43/223, train_loss: 0.0991, step time: 0.1120\n",
      "44/223, train_loss: 0.1271, step time: 0.1353\n",
      "45/223, train_loss: 0.1046, step time: 0.1104\n",
      "46/223, train_loss: 0.1023, step time: 0.1006\n",
      "47/223, train_loss: 0.1052, step time: 0.1153\n",
      "48/223, train_loss: 0.1050, step time: 0.1042\n",
      "49/223, train_loss: 0.1066, step time: 0.1003\n",
      "50/223, train_loss: 0.1050, step time: 0.1452\n",
      "51/223, train_loss: 0.1089, step time: 0.1152\n",
      "52/223, train_loss: 0.1095, step time: 0.1066\n",
      "53/223, train_loss: 0.1069, step time: 0.1004\n",
      "54/223, train_loss: 0.1033, step time: 0.1184\n",
      "55/223, train_loss: 0.1054, step time: 0.1177\n",
      "56/223, train_loss: 0.1064, step time: 0.1051\n",
      "57/223, train_loss: 0.0954, step time: 0.1129\n",
      "58/223, train_loss: 0.1004, step time: 0.1097\n",
      "59/223, train_loss: 0.1182, step time: 0.1132\n",
      "60/223, train_loss: 0.1131, step time: 0.1132\n",
      "61/223, train_loss: 0.1025, step time: 0.1070\n",
      "62/223, train_loss: 0.0991, step time: 0.1086\n",
      "63/223, train_loss: 0.0965, step time: 0.1238\n",
      "64/223, train_loss: 0.0974, step time: 0.1066\n",
      "65/223, train_loss: 0.0956, step time: 0.1005\n",
      "66/223, train_loss: 0.1008, step time: 0.1029\n",
      "67/223, train_loss: 0.1006, step time: 0.0996\n",
      "68/223, train_loss: 0.1077, step time: 0.0996\n",
      "69/223, train_loss: 0.1089, step time: 0.1088\n",
      "70/223, train_loss: 0.1100, step time: 0.0998\n",
      "71/223, train_loss: 0.1019, step time: 0.1003\n",
      "72/223, train_loss: 0.0955, step time: 0.1067\n",
      "73/223, train_loss: 0.1063, step time: 0.1014\n",
      "74/223, train_loss: 0.1053, step time: 0.1138\n",
      "75/223, train_loss: 0.0998, step time: 0.1028\n",
      "76/223, train_loss: 0.3012, step time: 0.1252\n",
      "77/223, train_loss: 0.1072, step time: 0.1050\n",
      "78/223, train_loss: 0.1025, step time: 0.1189\n",
      "79/223, train_loss: 0.1124, step time: 0.1125\n",
      "80/223, train_loss: 0.1160, step time: 0.1110\n",
      "81/223, train_loss: 0.0998, step time: 0.1134\n",
      "82/223, train_loss: 0.1041, step time: 0.1179\n",
      "83/223, train_loss: 0.0925, step time: 0.1092\n",
      "84/223, train_loss: 0.1059, step time: 0.1126\n",
      "85/223, train_loss: 0.1170, step time: 0.1000\n",
      "86/223, train_loss: 0.1050, step time: 0.1092\n",
      "87/223, train_loss: 0.1152, step time: 0.1166\n",
      "88/223, train_loss: 0.1071, step time: 0.1114\n",
      "89/223, train_loss: 0.0879, step time: 0.1247\n",
      "90/223, train_loss: 0.1148, step time: 0.1099\n",
      "91/223, train_loss: 0.0959, step time: 0.1049\n",
      "92/223, train_loss: 0.0974, step time: 0.1022\n",
      "93/223, train_loss: 0.0930, step time: 0.1119\n",
      "94/223, train_loss: 0.1043, step time: 0.1028\n",
      "95/223, train_loss: 0.1057, step time: 0.1141\n",
      "96/223, train_loss: 0.1140, step time: 0.1138\n",
      "97/223, train_loss: 0.1075, step time: 0.1124\n",
      "98/223, train_loss: 0.1072, step time: 0.0996\n",
      "99/223, train_loss: 0.1070, step time: 0.1064\n",
      "100/223, train_loss: 0.0955, step time: 0.0999\n",
      "101/223, train_loss: 0.1039, step time: 0.1287\n",
      "102/223, train_loss: 0.1147, step time: 0.1010\n",
      "103/223, train_loss: 0.1017, step time: 0.1203\n",
      "104/223, train_loss: 0.0983, step time: 0.0996\n",
      "105/223, train_loss: 0.1009, step time: 0.1073\n",
      "106/223, train_loss: 0.0946, step time: 0.1069\n",
      "107/223, train_loss: 0.1069, step time: 0.1068\n",
      "108/223, train_loss: 0.1090, step time: 0.1005\n",
      "109/223, train_loss: 0.1142, step time: 0.1101\n",
      "110/223, train_loss: 0.0993, step time: 0.1006\n",
      "111/223, train_loss: 0.1195, step time: 0.1127\n",
      "112/223, train_loss: 0.1044, step time: 0.1597\n",
      "113/223, train_loss: 0.1041, step time: 0.1085\n",
      "114/223, train_loss: 0.0985, step time: 0.0990\n",
      "115/223, train_loss: 0.1004, step time: 0.0993\n",
      "116/223, train_loss: 0.1069, step time: 0.0994\n",
      "117/223, train_loss: 0.1003, step time: 0.1129\n",
      "118/223, train_loss: 0.0997, step time: 0.1068\n",
      "119/223, train_loss: 0.1082, step time: 0.1069\n",
      "120/223, train_loss: 0.1089, step time: 0.1197\n",
      "121/223, train_loss: 0.1040, step time: 0.1073\n",
      "122/223, train_loss: 0.0932, step time: 0.1003\n",
      "123/223, train_loss: 0.0970, step time: 0.1017\n",
      "124/223, train_loss: 0.0983, step time: 0.1012\n",
      "125/223, train_loss: 0.0981, step time: 0.1048\n",
      "126/223, train_loss: 0.1051, step time: 0.1169\n",
      "127/223, train_loss: 0.0968, step time: 0.1033\n",
      "128/223, train_loss: 0.1039, step time: 0.1020\n",
      "129/223, train_loss: 0.1150, step time: 0.1008\n",
      "130/223, train_loss: 0.0982, step time: 0.1135\n",
      "131/223, train_loss: 0.1127, step time: 0.0997\n",
      "132/223, train_loss: 0.1086, step time: 0.0999\n",
      "133/223, train_loss: 0.1009, step time: 0.1059\n",
      "134/223, train_loss: 0.1070, step time: 0.1256\n",
      "135/223, train_loss: 0.1006, step time: 0.1000\n",
      "136/223, train_loss: 0.1124, step time: 0.1055\n",
      "137/223, train_loss: 0.0992, step time: 0.0995\n",
      "138/223, train_loss: 0.1016, step time: 0.1012\n",
      "139/223, train_loss: 0.0974, step time: 0.1085\n",
      "140/223, train_loss: 0.1049, step time: 0.1057\n",
      "141/223, train_loss: 0.0994, step time: 0.1007\n",
      "142/223, train_loss: 0.1069, step time: 0.1154\n",
      "143/223, train_loss: 0.1037, step time: 0.1049\n",
      "144/223, train_loss: 0.1026, step time: 0.1007\n",
      "145/223, train_loss: 0.1043, step time: 0.1168\n",
      "146/223, train_loss: 0.0985, step time: 0.1010\n",
      "147/223, train_loss: 0.0969, step time: 0.1064\n",
      "148/223, train_loss: 0.0992, step time: 0.1007\n",
      "149/223, train_loss: 0.1163, step time: 0.1009\n",
      "150/223, train_loss: 0.0918, step time: 0.1008\n",
      "151/223, train_loss: 0.1027, step time: 0.1002\n",
      "152/223, train_loss: 0.1019, step time: 0.1064\n",
      "153/223, train_loss: 0.1146, step time: 0.1154\n",
      "154/223, train_loss: 0.1046, step time: 0.1005\n",
      "155/223, train_loss: 0.0996, step time: 0.1002\n",
      "156/223, train_loss: 0.0925, step time: 0.0990\n",
      "157/223, train_loss: 0.0993, step time: 0.1059\n",
      "158/223, train_loss: 0.0945, step time: 0.1177\n",
      "159/223, train_loss: 0.1171, step time: 0.1004\n",
      "160/223, train_loss: 0.0997, step time: 0.1006\n",
      "161/223, train_loss: 0.1022, step time: 0.1045\n",
      "162/223, train_loss: 0.0981, step time: 0.0995\n",
      "163/223, train_loss: 0.1031, step time: 0.1007\n",
      "164/223, train_loss: 0.1126, step time: 0.1003\n",
      "165/223, train_loss: 0.1076, step time: 0.1073\n",
      "166/223, train_loss: 0.1046, step time: 0.1006\n",
      "167/223, train_loss: 0.1086, step time: 0.1157\n",
      "168/223, train_loss: 0.1082, step time: 0.1005\n",
      "169/223, train_loss: 0.1129, step time: 0.1081\n",
      "170/223, train_loss: 0.0967, step time: 0.1161\n",
      "171/223, train_loss: 0.0970, step time: 0.1001\n",
      "172/223, train_loss: 0.1065, step time: 0.1002\n",
      "173/223, train_loss: 0.1049, step time: 0.1114\n",
      "174/223, train_loss: 0.0991, step time: 0.1096\n",
      "175/223, train_loss: 0.1054, step time: 0.1054\n",
      "176/223, train_loss: 0.1060, step time: 0.1038\n",
      "177/223, train_loss: 0.1130, step time: 0.1034\n",
      "178/223, train_loss: 0.0943, step time: 0.1136\n",
      "179/223, train_loss: 0.0921, step time: 0.1061\n",
      "180/223, train_loss: 0.1070, step time: 0.1183\n",
      "181/223, train_loss: 0.0990, step time: 0.1120\n",
      "182/223, train_loss: 0.1043, step time: 0.1074\n",
      "183/223, train_loss: 0.1115, step time: 0.1012\n",
      "184/223, train_loss: 0.1036, step time: 0.1210\n",
      "185/223, train_loss: 0.1018, step time: 0.1350\n",
      "186/223, train_loss: 0.0981, step time: 0.1007\n",
      "187/223, train_loss: 0.1123, step time: 0.1102\n",
      "188/223, train_loss: 0.1040, step time: 0.1036\n",
      "189/223, train_loss: 0.1183, step time: 0.1088\n",
      "190/223, train_loss: 0.1009, step time: 0.0994\n",
      "191/223, train_loss: 0.1115, step time: 0.1009\n",
      "192/223, train_loss: 0.1105, step time: 0.1006\n",
      "193/223, train_loss: 0.0978, step time: 0.1112\n",
      "194/223, train_loss: 0.1034, step time: 0.1009\n",
      "195/223, train_loss: 0.1027, step time: 0.1011\n",
      "196/223, train_loss: 0.1099, step time: 0.1071\n",
      "197/223, train_loss: 0.1014, step time: 0.1053\n",
      "198/223, train_loss: 0.1100, step time: 0.1086\n",
      "199/223, train_loss: 0.1072, step time: 0.1045\n",
      "200/223, train_loss: 0.1027, step time: 0.1120\n",
      "201/223, train_loss: 0.1171, step time: 0.1096\n",
      "202/223, train_loss: 0.1089, step time: 0.1086\n",
      "203/223, train_loss: 0.0913, step time: 0.1030\n",
      "204/223, train_loss: 0.0966, step time: 0.1062\n",
      "205/223, train_loss: 0.1009, step time: 0.1115\n",
      "206/223, train_loss: 0.1033, step time: 0.1163\n",
      "207/223, train_loss: 0.1074, step time: 0.1148\n",
      "208/223, train_loss: 0.1035, step time: 0.1093\n",
      "209/223, train_loss: 0.0986, step time: 0.1222\n",
      "210/223, train_loss: 0.1016, step time: 0.1349\n",
      "211/223, train_loss: 0.1091, step time: 0.1020\n",
      "212/223, train_loss: 0.0939, step time: 0.1429\n",
      "213/223, train_loss: 0.0999, step time: 0.1252\n",
      "214/223, train_loss: 0.1079, step time: 0.1070\n",
      "215/223, train_loss: 0.1026, step time: 0.1001\n",
      "216/223, train_loss: 0.1195, step time: 0.1005\n",
      "217/223, train_loss: 0.0907, step time: 0.1086\n",
      "218/223, train_loss: 0.1116, step time: 0.1009\n",
      "219/223, train_loss: 0.1105, step time: 0.1129\n",
      "220/223, train_loss: 0.1036, step time: 0.1093\n",
      "221/223, train_loss: 0.0996, step time: 0.1001\n",
      "222/223, train_loss: 0.1000, step time: 0.1009\n",
      "223/223, train_loss: 0.1070, step time: 0.0990\n",
      "epoch 170 average loss: 0.1050\n",
      "saved new best metric model\n",
      "current epoch: 170 current mean dice: 0.8600 tc: 0.9216 wt: 0.8697 et: 0.7889\n",
      "best mean dice: 0.8600 at epoch: 170\n",
      "time consuming of epoch 170 is: 90.0753\n",
      "----------\n",
      "epoch 171/300\n",
      "1/223, train_loss: 0.0997, step time: 0.1077\n",
      "2/223, train_loss: 0.0995, step time: 0.1049\n",
      "3/223, train_loss: 0.0918, step time: 0.1050\n",
      "4/223, train_loss: 0.1135, step time: 0.1113\n",
      "5/223, train_loss: 0.1147, step time: 0.1052\n",
      "6/223, train_loss: 0.0984, step time: 0.1007\n",
      "7/223, train_loss: 0.1037, step time: 0.1062\n",
      "8/223, train_loss: 0.0987, step time: 0.1234\n",
      "9/223, train_loss: 0.1060, step time: 0.1050\n",
      "10/223, train_loss: 0.1132, step time: 0.1107\n",
      "11/223, train_loss: 0.1018, step time: 0.1424\n",
      "12/223, train_loss: 0.1068, step time: 0.1056\n",
      "13/223, train_loss: 0.1076, step time: 0.1034\n",
      "14/223, train_loss: 0.1002, step time: 0.1270\n",
      "15/223, train_loss: 0.1128, step time: 0.1082\n",
      "16/223, train_loss: 0.1018, step time: 0.1034\n",
      "17/223, train_loss: 0.1131, step time: 0.1012\n",
      "18/223, train_loss: 0.1166, step time: 0.1202\n",
      "19/223, train_loss: 0.1086, step time: 0.1155\n",
      "20/223, train_loss: 0.1054, step time: 0.1083\n",
      "21/223, train_loss: 0.1083, step time: 0.1041\n",
      "22/223, train_loss: 0.1114, step time: 0.1193\n",
      "23/223, train_loss: 0.1077, step time: 0.1122\n",
      "24/223, train_loss: 0.1086, step time: 0.1024\n",
      "25/223, train_loss: 0.1017, step time: 0.1135\n",
      "26/223, train_loss: 0.1023, step time: 0.1112\n",
      "27/223, train_loss: 0.1002, step time: 0.1008\n",
      "28/223, train_loss: 0.0974, step time: 0.1014\n",
      "29/223, train_loss: 0.1037, step time: 0.1058\n",
      "30/223, train_loss: 0.1034, step time: 0.1055\n",
      "31/223, train_loss: 0.0931, step time: 0.1025\n",
      "32/223, train_loss: 0.1132, step time: 0.1087\n",
      "33/223, train_loss: 0.0899, step time: 0.1101\n",
      "34/223, train_loss: 0.0998, step time: 0.1253\n",
      "35/223, train_loss: 0.0973, step time: 0.1203\n",
      "36/223, train_loss: 0.1014, step time: 0.1164\n",
      "37/223, train_loss: 0.1043, step time: 0.1161\n",
      "38/223, train_loss: 0.0983, step time: 0.1189\n",
      "39/223, train_loss: 0.1029, step time: 0.1142\n",
      "40/223, train_loss: 0.1015, step time: 0.1186\n",
      "41/223, train_loss: 0.1070, step time: 0.1101\n",
      "42/223, train_loss: 0.1084, step time: 0.1210\n",
      "43/223, train_loss: 0.1071, step time: 0.1007\n",
      "44/223, train_loss: 0.0948, step time: 0.1014\n",
      "45/223, train_loss: 0.1088, step time: 0.1006\n",
      "46/223, train_loss: 0.1018, step time: 0.1113\n",
      "47/223, train_loss: 0.1044, step time: 0.1007\n",
      "48/223, train_loss: 0.1112, step time: 0.0999\n",
      "49/223, train_loss: 0.1096, step time: 0.1122\n",
      "50/223, train_loss: 0.1058, step time: 0.1016\n",
      "51/223, train_loss: 0.1038, step time: 0.1010\n",
      "52/223, train_loss: 0.1138, step time: 0.1012\n",
      "53/223, train_loss: 0.1096, step time: 0.1123\n",
      "54/223, train_loss: 0.0926, step time: 0.1012\n",
      "55/223, train_loss: 0.0926, step time: 0.1005\n",
      "56/223, train_loss: 0.1083, step time: 0.1089\n",
      "57/223, train_loss: 0.1099, step time: 0.1006\n",
      "58/223, train_loss: 0.0937, step time: 0.1007\n",
      "59/223, train_loss: 0.1003, step time: 0.1014\n",
      "60/223, train_loss: 0.0975, step time: 0.0994\n",
      "61/223, train_loss: 0.1101, step time: 0.1063\n",
      "62/223, train_loss: 0.1139, step time: 0.0998\n",
      "63/223, train_loss: 0.1060, step time: 0.1062\n",
      "64/223, train_loss: 0.0972, step time: 0.1148\n",
      "65/223, train_loss: 0.1116, step time: 0.1045\n",
      "66/223, train_loss: 0.0994, step time: 0.1009\n",
      "67/223, train_loss: 0.1202, step time: 0.1004\n",
      "68/223, train_loss: 0.0976, step time: 0.1007\n",
      "69/223, train_loss: 0.1201, step time: 0.1028\n",
      "70/223, train_loss: 0.1022, step time: 0.1006\n",
      "71/223, train_loss: 0.1090, step time: 0.1014\n",
      "72/223, train_loss: 0.1073, step time: 0.1003\n",
      "73/223, train_loss: 0.1026, step time: 0.1002\n",
      "74/223, train_loss: 0.1184, step time: 0.1007\n",
      "75/223, train_loss: 0.1019, step time: 0.1026\n",
      "76/223, train_loss: 0.1022, step time: 0.1038\n",
      "77/223, train_loss: 0.0995, step time: 0.1047\n",
      "78/223, train_loss: 0.1089, step time: 0.1121\n",
      "79/223, train_loss: 0.1058, step time: 0.1239\n",
      "80/223, train_loss: 0.0905, step time: 0.1028\n",
      "81/223, train_loss: 0.1055, step time: 0.1012\n",
      "82/223, train_loss: 0.1017, step time: 0.1039\n",
      "83/223, train_loss: 0.0979, step time: 0.1007\n",
      "84/223, train_loss: 0.1144, step time: 0.0997\n",
      "85/223, train_loss: 0.1008, step time: 0.1103\n",
      "86/223, train_loss: 0.0973, step time: 0.1018\n",
      "87/223, train_loss: 0.0976, step time: 0.1007\n",
      "88/223, train_loss: 0.1132, step time: 0.1004\n",
      "89/223, train_loss: 0.1035, step time: 0.1032\n",
      "90/223, train_loss: 0.1077, step time: 0.0994\n",
      "91/223, train_loss: 0.1052, step time: 0.1007\n",
      "92/223, train_loss: 0.0956, step time: 0.1004\n",
      "93/223, train_loss: 0.0998, step time: 0.1073\n",
      "94/223, train_loss: 0.1199, step time: 0.1062\n",
      "95/223, train_loss: 0.1172, step time: 0.1006\n",
      "96/223, train_loss: 0.0946, step time: 0.1075\n",
      "97/223, train_loss: 0.1123, step time: 0.1000\n",
      "98/223, train_loss: 0.1080, step time: 0.0993\n",
      "99/223, train_loss: 0.0985, step time: 0.1001\n",
      "100/223, train_loss: 0.0963, step time: 0.1082\n",
      "101/223, train_loss: 0.0993, step time: 0.1027\n",
      "102/223, train_loss: 0.0978, step time: 0.1143\n",
      "103/223, train_loss: 0.1056, step time: 0.1007\n",
      "104/223, train_loss: 0.1037, step time: 0.1001\n",
      "105/223, train_loss: 0.1012, step time: 0.1006\n",
      "106/223, train_loss: 0.1005, step time: 0.1195\n",
      "107/223, train_loss: 0.1011, step time: 0.0998\n",
      "108/223, train_loss: 0.0951, step time: 0.1014\n",
      "109/223, train_loss: 0.1017, step time: 0.1061\n",
      "110/223, train_loss: 0.1081, step time: 0.1109\n",
      "111/223, train_loss: 0.1097, step time: 0.1233\n",
      "112/223, train_loss: 0.1023, step time: 0.1294\n",
      "113/223, train_loss: 0.1077, step time: 0.1379\n",
      "114/223, train_loss: 0.1034, step time: 0.1004\n",
      "115/223, train_loss: 0.1019, step time: 0.1193\n",
      "116/223, train_loss: 0.0977, step time: 0.1152\n",
      "117/223, train_loss: 0.1177, step time: 0.1276\n",
      "118/223, train_loss: 0.1009, step time: 0.1091\n",
      "119/223, train_loss: 0.0916, step time: 0.1500\n",
      "120/223, train_loss: 0.1215, step time: 0.1181\n",
      "121/223, train_loss: 0.1016, step time: 0.1077\n",
      "122/223, train_loss: 0.1108, step time: 0.1187\n",
      "123/223, train_loss: 0.1101, step time: 0.1121\n",
      "124/223, train_loss: 0.1074, step time: 0.1290\n",
      "125/223, train_loss: 0.1064, step time: 0.1124\n",
      "126/223, train_loss: 0.1009, step time: 0.1008\n",
      "127/223, train_loss: 0.0968, step time: 0.0997\n",
      "128/223, train_loss: 0.0991, step time: 0.1003\n",
      "129/223, train_loss: 0.1052, step time: 0.1021\n",
      "130/223, train_loss: 0.0953, step time: 0.1140\n",
      "131/223, train_loss: 0.1134, step time: 0.1017\n",
      "132/223, train_loss: 0.0951, step time: 0.1194\n",
      "133/223, train_loss: 0.1099, step time: 0.1034\n",
      "134/223, train_loss: 0.1006, step time: 0.1029\n",
      "135/223, train_loss: 0.1169, step time: 0.1076\n",
      "136/223, train_loss: 0.1005, step time: 0.1001\n",
      "137/223, train_loss: 0.1060, step time: 0.1000\n",
      "138/223, train_loss: 0.0944, step time: 0.1104\n",
      "139/223, train_loss: 0.3022, step time: 0.1060\n",
      "140/223, train_loss: 0.1052, step time: 0.1015\n",
      "141/223, train_loss: 0.1046, step time: 0.1029\n",
      "142/223, train_loss: 0.0961, step time: 0.1225\n",
      "143/223, train_loss: 0.1096, step time: 0.1011\n",
      "144/223, train_loss: 0.1051, step time: 0.1125\n",
      "145/223, train_loss: 0.1126, step time: 0.1260\n",
      "146/223, train_loss: 0.1071, step time: 0.1110\n",
      "147/223, train_loss: 0.1041, step time: 0.1054\n",
      "148/223, train_loss: 0.1002, step time: 0.1009\n",
      "149/223, train_loss: 0.0977, step time: 0.1004\n",
      "150/223, train_loss: 0.1007, step time: 0.1127\n",
      "151/223, train_loss: 0.1113, step time: 0.1031\n",
      "152/223, train_loss: 0.1052, step time: 0.1457\n",
      "153/223, train_loss: 0.1017, step time: 0.1017\n",
      "154/223, train_loss: 0.1018, step time: 0.1033\n",
      "155/223, train_loss: 0.1080, step time: 0.1006\n",
      "156/223, train_loss: 0.1121, step time: 0.0997\n",
      "157/223, train_loss: 0.0995, step time: 0.1407\n",
      "158/223, train_loss: 0.0997, step time: 0.1057\n",
      "159/223, train_loss: 0.0981, step time: 0.1085\n",
      "160/223, train_loss: 0.1016, step time: 0.1050\n",
      "161/223, train_loss: 0.1112, step time: 0.1141\n",
      "162/223, train_loss: 0.1022, step time: 0.1097\n",
      "163/223, train_loss: 0.0946, step time: 0.1031\n",
      "164/223, train_loss: 0.1053, step time: 0.1079\n",
      "165/223, train_loss: 0.1030, step time: 0.1002\n",
      "166/223, train_loss: 0.1063, step time: 0.1003\n",
      "167/223, train_loss: 0.1067, step time: 0.1002\n",
      "168/223, train_loss: 0.1141, step time: 0.0999\n",
      "169/223, train_loss: 0.1088, step time: 0.0998\n",
      "170/223, train_loss: 0.0973, step time: 0.0993\n",
      "171/223, train_loss: 0.1077, step time: 0.0992\n",
      "172/223, train_loss: 0.1062, step time: 0.0998\n",
      "173/223, train_loss: 0.1068, step time: 0.1005\n",
      "174/223, train_loss: 0.0949, step time: 0.0996\n",
      "175/223, train_loss: 0.1065, step time: 0.1001\n",
      "176/223, train_loss: 0.0915, step time: 0.0995\n",
      "177/223, train_loss: 0.1057, step time: 0.1110\n",
      "178/223, train_loss: 0.1041, step time: 0.1000\n",
      "179/223, train_loss: 0.1046, step time: 0.0992\n",
      "180/223, train_loss: 0.1100, step time: 0.0997\n",
      "181/223, train_loss: 0.0989, step time: 0.1077\n",
      "182/223, train_loss: 0.1041, step time: 0.0993\n",
      "183/223, train_loss: 0.1054, step time: 0.0992\n",
      "184/223, train_loss: 0.1035, step time: 0.1000\n",
      "185/223, train_loss: 0.0978, step time: 0.1010\n",
      "186/223, train_loss: 0.1024, step time: 0.0996\n",
      "187/223, train_loss: 0.1097, step time: 0.0994\n",
      "188/223, train_loss: 0.0920, step time: 0.0988\n",
      "189/223, train_loss: 0.1129, step time: 0.1237\n",
      "190/223, train_loss: 0.1002, step time: 0.1053\n",
      "191/223, train_loss: 0.1114, step time: 0.1060\n",
      "192/223, train_loss: 0.0909, step time: 0.1220\n",
      "193/223, train_loss: 0.0996, step time: 0.1240\n",
      "194/223, train_loss: 0.0991, step time: 0.1106\n",
      "195/223, train_loss: 0.1099, step time: 0.1006\n",
      "196/223, train_loss: 0.1147, step time: 0.1089\n",
      "197/223, train_loss: 0.0949, step time: 0.1209\n",
      "198/223, train_loss: 0.1085, step time: 0.1216\n",
      "199/223, train_loss: 0.1033, step time: 0.1135\n",
      "200/223, train_loss: 0.1036, step time: 0.0999\n",
      "201/223, train_loss: 0.1025, step time: 0.1055\n",
      "202/223, train_loss: 0.1101, step time: 0.1017\n",
      "203/223, train_loss: 0.1140, step time: 0.1253\n",
      "204/223, train_loss: 0.1068, step time: 0.1006\n",
      "205/223, train_loss: 0.1060, step time: 0.1207\n",
      "206/223, train_loss: 0.1024, step time: 0.1006\n",
      "207/223, train_loss: 0.0897, step time: 0.1016\n",
      "208/223, train_loss: 0.0948, step time: 0.1051\n",
      "209/223, train_loss: 0.0996, step time: 0.1000\n",
      "210/223, train_loss: 0.1022, step time: 0.1240\n",
      "211/223, train_loss: 0.1092, step time: 0.1238\n",
      "212/223, train_loss: 0.0980, step time: 0.1079\n",
      "213/223, train_loss: 0.1017, step time: 0.1014\n",
      "214/223, train_loss: 0.1117, step time: 0.0996\n",
      "215/223, train_loss: 0.1001, step time: 0.1007\n",
      "216/223, train_loss: 0.1056, step time: 0.1176\n",
      "217/223, train_loss: 0.0971, step time: 0.1082\n",
      "218/223, train_loss: 0.1185, step time: 0.1002\n",
      "219/223, train_loss: 0.1128, step time: 0.0998\n",
      "220/223, train_loss: 0.0986, step time: 0.0999\n",
      "221/223, train_loss: 0.0953, step time: 0.1080\n",
      "222/223, train_loss: 0.1177, step time: 0.0997\n",
      "223/223, train_loss: 0.1065, step time: 0.0998\n",
      "epoch 171 average loss: 0.1051\n",
      "time consuming of epoch 171 is: 94.4484\n",
      "----------\n",
      "epoch 172/300\n",
      "1/223, train_loss: 0.0951, step time: 0.1046\n",
      "2/223, train_loss: 0.1275, step time: 0.1050\n",
      "3/223, train_loss: 0.1093, step time: 0.1266\n",
      "4/223, train_loss: 0.1080, step time: 0.1108\n",
      "5/223, train_loss: 0.0943, step time: 0.1072\n",
      "6/223, train_loss: 0.1024, step time: 0.1145\n",
      "7/223, train_loss: 0.1103, step time: 0.1106\n",
      "8/223, train_loss: 0.1033, step time: 0.1580\n",
      "9/223, train_loss: 0.1067, step time: 0.1032\n",
      "10/223, train_loss: 0.1072, step time: 0.1073\n",
      "11/223, train_loss: 0.1079, step time: 0.0997\n",
      "12/223, train_loss: 0.1103, step time: 0.1037\n",
      "13/223, train_loss: 0.1036, step time: 0.1007\n",
      "14/223, train_loss: 0.0958, step time: 0.1062\n",
      "15/223, train_loss: 0.1090, step time: 0.1123\n",
      "16/223, train_loss: 0.1049, step time: 0.1073\n",
      "17/223, train_loss: 0.0994, step time: 0.1133\n",
      "18/223, train_loss: 0.0996, step time: 0.1002\n",
      "19/223, train_loss: 0.1000, step time: 0.0994\n",
      "20/223, train_loss: 0.1100, step time: 0.1100\n",
      "21/223, train_loss: 0.1121, step time: 0.1008\n",
      "22/223, train_loss: 0.0959, step time: 0.1150\n",
      "23/223, train_loss: 0.1005, step time: 0.1013\n",
      "24/223, train_loss: 0.1086, step time: 0.1085\n",
      "25/223, train_loss: 0.1166, step time: 0.1079\n",
      "26/223, train_loss: 0.3061, step time: 0.1105\n",
      "27/223, train_loss: 0.1039, step time: 0.0998\n",
      "28/223, train_loss: 0.0997, step time: 0.1009\n",
      "29/223, train_loss: 0.1127, step time: 0.1003\n",
      "30/223, train_loss: 0.0945, step time: 0.1062\n",
      "31/223, train_loss: 0.0966, step time: 0.0998\n",
      "32/223, train_loss: 0.1037, step time: 0.1009\n",
      "33/223, train_loss: 0.1089, step time: 0.1003\n",
      "34/223, train_loss: 0.0963, step time: 0.1002\n",
      "35/223, train_loss: 0.1023, step time: 0.1014\n",
      "36/223, train_loss: 0.1070, step time: 0.0996\n",
      "37/223, train_loss: 0.0952, step time: 0.1143\n",
      "38/223, train_loss: 0.0950, step time: 0.1008\n",
      "39/223, train_loss: 0.0997, step time: 0.1002\n",
      "40/223, train_loss: 0.1079, step time: 0.1013\n",
      "41/223, train_loss: 0.1042, step time: 0.1153\n",
      "42/223, train_loss: 0.1089, step time: 0.1118\n",
      "43/223, train_loss: 0.1024, step time: 0.1219\n",
      "44/223, train_loss: 0.1116, step time: 0.1091\n",
      "45/223, train_loss: 0.0988, step time: 0.1256\n",
      "46/223, train_loss: 0.1124, step time: 0.1137\n",
      "47/223, train_loss: 0.0905, step time: 0.1002\n",
      "48/223, train_loss: 0.1001, step time: 0.1007\n",
      "49/223, train_loss: 0.1014, step time: 0.1167\n",
      "50/223, train_loss: 0.0991, step time: 0.1090\n",
      "51/223, train_loss: 0.1062, step time: 0.1091\n",
      "52/223, train_loss: 0.0978, step time: 0.1006\n",
      "53/223, train_loss: 0.1005, step time: 0.1122\n",
      "54/223, train_loss: 0.0985, step time: 0.1005\n",
      "55/223, train_loss: 0.1020, step time: 0.1005\n",
      "56/223, train_loss: 0.1039, step time: 0.1007\n",
      "57/223, train_loss: 0.1112, step time: 0.1041\n",
      "58/223, train_loss: 0.1081, step time: 0.1087\n",
      "59/223, train_loss: 0.1037, step time: 0.1111\n",
      "60/223, train_loss: 0.0962, step time: 0.1070\n",
      "61/223, train_loss: 0.1124, step time: 0.1066\n",
      "62/223, train_loss: 0.1035, step time: 0.1054\n",
      "63/223, train_loss: 0.0994, step time: 0.1003\n",
      "64/223, train_loss: 0.0989, step time: 0.1006\n",
      "65/223, train_loss: 0.1084, step time: 0.1104\n",
      "66/223, train_loss: 0.1136, step time: 0.1262\n",
      "67/223, train_loss: 0.1072, step time: 0.1004\n",
      "68/223, train_loss: 0.1171, step time: 0.1002\n",
      "69/223, train_loss: 0.1086, step time: 0.1044\n",
      "70/223, train_loss: 0.1160, step time: 0.1143\n",
      "71/223, train_loss: 0.0960, step time: 0.1093\n",
      "72/223, train_loss: 0.1023, step time: 0.1290\n",
      "73/223, train_loss: 0.1059, step time: 0.1003\n",
      "74/223, train_loss: 0.1007, step time: 0.1239\n",
      "75/223, train_loss: 0.1016, step time: 0.1176\n",
      "76/223, train_loss: 0.1000, step time: 0.1137\n",
      "77/223, train_loss: 0.1071, step time: 0.1000\n",
      "78/223, train_loss: 0.1032, step time: 0.1168\n",
      "79/223, train_loss: 0.0930, step time: 0.1585\n",
      "80/223, train_loss: 0.1056, step time: 0.1112\n",
      "81/223, train_loss: 0.1147, step time: 0.1127\n",
      "82/223, train_loss: 0.1129, step time: 0.0994\n",
      "83/223, train_loss: 0.1018, step time: 0.1014\n",
      "84/223, train_loss: 0.0948, step time: 0.1240\n",
      "85/223, train_loss: 0.0946, step time: 0.1053\n",
      "86/223, train_loss: 0.1094, step time: 0.1121\n",
      "87/223, train_loss: 0.1186, step time: 0.1171\n",
      "88/223, train_loss: 0.1150, step time: 0.1280\n",
      "89/223, train_loss: 0.0978, step time: 0.1113\n",
      "90/223, train_loss: 0.1037, step time: 0.1276\n",
      "91/223, train_loss: 0.0987, step time: 0.1094\n",
      "92/223, train_loss: 0.1048, step time: 0.1005\n",
      "93/223, train_loss: 0.1017, step time: 0.1046\n",
      "94/223, train_loss: 0.0994, step time: 0.1136\n",
      "95/223, train_loss: 0.1029, step time: 0.1216\n",
      "96/223, train_loss: 0.1085, step time: 0.1073\n",
      "97/223, train_loss: 0.1137, step time: 0.1259\n",
      "98/223, train_loss: 0.1020, step time: 0.1149\n",
      "99/223, train_loss: 0.1140, step time: 0.1141\n",
      "100/223, train_loss: 0.1000, step time: 0.1179\n",
      "101/223, train_loss: 0.0992, step time: 0.1068\n",
      "102/223, train_loss: 0.1000, step time: 0.1073\n",
      "103/223, train_loss: 0.0999, step time: 0.1096\n",
      "104/223, train_loss: 0.1045, step time: 0.1007\n",
      "105/223, train_loss: 0.0966, step time: 0.1163\n",
      "106/223, train_loss: 0.1083, step time: 0.1199\n",
      "107/223, train_loss: 0.1133, step time: 0.1310\n",
      "108/223, train_loss: 0.1073, step time: 0.1177\n",
      "109/223, train_loss: 0.1007, step time: 0.1163\n",
      "110/223, train_loss: 0.1019, step time: 0.1041\n",
      "111/223, train_loss: 0.0953, step time: 0.0993\n",
      "112/223, train_loss: 0.1063, step time: 0.1015\n",
      "113/223, train_loss: 0.1061, step time: 0.1136\n",
      "114/223, train_loss: 0.1099, step time: 0.1110\n",
      "115/223, train_loss: 0.0953, step time: 0.1060\n",
      "116/223, train_loss: 0.0969, step time: 0.1117\n",
      "117/223, train_loss: 0.0877, step time: 0.1106\n",
      "118/223, train_loss: 0.1136, step time: 0.1059\n",
      "119/223, train_loss: 0.1031, step time: 0.1160\n",
      "120/223, train_loss: 0.0965, step time: 0.1003\n",
      "121/223, train_loss: 0.0976, step time: 0.1015\n",
      "122/223, train_loss: 0.1077, step time: 0.1006\n",
      "123/223, train_loss: 0.0954, step time: 0.1010\n",
      "124/223, train_loss: 0.1058, step time: 0.1235\n",
      "125/223, train_loss: 0.1148, step time: 0.1049\n",
      "126/223, train_loss: 0.1148, step time: 0.1012\n",
      "127/223, train_loss: 0.1024, step time: 0.1006\n",
      "128/223, train_loss: 0.1055, step time: 0.1019\n",
      "129/223, train_loss: 0.1047, step time: 0.1152\n",
      "130/223, train_loss: 0.1035, step time: 0.1358\n",
      "131/223, train_loss: 0.1039, step time: 0.1273\n",
      "132/223, train_loss: 0.1093, step time: 0.1004\n",
      "133/223, train_loss: 0.1052, step time: 0.1056\n",
      "134/223, train_loss: 0.0959, step time: 0.1139\n",
      "135/223, train_loss: 0.0978, step time: 0.1138\n",
      "136/223, train_loss: 0.1153, step time: 0.1195\n",
      "137/223, train_loss: 0.1133, step time: 0.1223\n",
      "138/223, train_loss: 0.1128, step time: 0.1171\n",
      "139/223, train_loss: 0.0979, step time: 0.1185\n",
      "140/223, train_loss: 0.0997, step time: 0.1170\n",
      "141/223, train_loss: 0.0888, step time: 0.1061\n",
      "142/223, train_loss: 0.0982, step time: 0.1137\n",
      "143/223, train_loss: 0.1165, step time: 0.1124\n",
      "144/223, train_loss: 0.0971, step time: 0.1085\n",
      "145/223, train_loss: 0.1070, step time: 0.1239\n",
      "146/223, train_loss: 0.1029, step time: 0.1249\n",
      "147/223, train_loss: 0.1074, step time: 0.1159\n",
      "148/223, train_loss: 0.0952, step time: 0.1002\n",
      "149/223, train_loss: 0.1057, step time: 0.1081\n",
      "150/223, train_loss: 0.1044, step time: 0.1197\n",
      "151/223, train_loss: 0.1012, step time: 0.1001\n",
      "152/223, train_loss: 0.0920, step time: 0.0998\n",
      "153/223, train_loss: 0.0985, step time: 0.1187\n",
      "154/223, train_loss: 0.0957, step time: 0.1004\n",
      "155/223, train_loss: 0.1007, step time: 0.1057\n",
      "156/223, train_loss: 0.1171, step time: 0.1294\n",
      "157/223, train_loss: 0.1112, step time: 0.1216\n",
      "158/223, train_loss: 0.0963, step time: 0.0999\n",
      "159/223, train_loss: 0.1005, step time: 0.1038\n",
      "160/223, train_loss: 0.1033, step time: 0.1133\n",
      "161/223, train_loss: 0.1183, step time: 0.1135\n",
      "162/223, train_loss: 0.1058, step time: 0.1101\n",
      "163/223, train_loss: 0.1041, step time: 0.1142\n",
      "164/223, train_loss: 0.1101, step time: 0.1001\n",
      "165/223, train_loss: 0.1006, step time: 0.1073\n",
      "166/223, train_loss: 0.0982, step time: 0.1179\n",
      "167/223, train_loss: 0.0961, step time: 0.1058\n",
      "168/223, train_loss: 0.1189, step time: 0.1088\n",
      "169/223, train_loss: 0.0934, step time: 0.1184\n",
      "170/223, train_loss: 0.1120, step time: 0.1119\n",
      "171/223, train_loss: 0.0910, step time: 0.1099\n",
      "172/223, train_loss: 0.1052, step time: 0.1089\n",
      "173/223, train_loss: 0.1112, step time: 0.1193\n",
      "174/223, train_loss: 0.1037, step time: 0.1175\n",
      "175/223, train_loss: 0.1035, step time: 0.1037\n",
      "176/223, train_loss: 0.0982, step time: 0.0996\n",
      "177/223, train_loss: 0.1081, step time: 0.1171\n",
      "178/223, train_loss: 0.1021, step time: 0.1002\n",
      "179/223, train_loss: 0.1137, step time: 0.1002\n",
      "180/223, train_loss: 0.1037, step time: 0.1275\n",
      "181/223, train_loss: 0.1149, step time: 0.1127\n",
      "182/223, train_loss: 0.0973, step time: 0.1168\n",
      "183/223, train_loss: 0.0983, step time: 0.1148\n",
      "184/223, train_loss: 0.1108, step time: 0.1075\n",
      "185/223, train_loss: 0.1089, step time: 0.1185\n",
      "186/223, train_loss: 0.1016, step time: 0.1006\n",
      "187/223, train_loss: 0.1042, step time: 0.1008\n",
      "188/223, train_loss: 0.0893, step time: 0.1188\n",
      "189/223, train_loss: 0.1079, step time: 0.0997\n",
      "190/223, train_loss: 0.1186, step time: 0.1006\n",
      "191/223, train_loss: 0.1009, step time: 0.1003\n",
      "192/223, train_loss: 0.1157, step time: 0.1005\n",
      "193/223, train_loss: 0.1091, step time: 0.1132\n",
      "194/223, train_loss: 0.1147, step time: 0.1144\n",
      "195/223, train_loss: 0.0988, step time: 0.1037\n",
      "196/223, train_loss: 0.1130, step time: 0.1005\n",
      "197/223, train_loss: 0.1099, step time: 0.1005\n",
      "198/223, train_loss: 0.0942, step time: 0.1016\n",
      "199/223, train_loss: 0.0962, step time: 0.1000\n",
      "200/223, train_loss: 0.0942, step time: 0.1009\n",
      "201/223, train_loss: 0.1030, step time: 0.1187\n",
      "202/223, train_loss: 0.1095, step time: 0.1267\n",
      "203/223, train_loss: 0.0934, step time: 0.1171\n",
      "204/223, train_loss: 0.0922, step time: 0.1128\n",
      "205/223, train_loss: 0.1077, step time: 0.1002\n",
      "206/223, train_loss: 0.1075, step time: 0.1244\n",
      "207/223, train_loss: 0.0927, step time: 0.0997\n",
      "208/223, train_loss: 0.0954, step time: 0.1004\n",
      "209/223, train_loss: 0.0965, step time: 0.1003\n",
      "210/223, train_loss: 0.1185, step time: 0.1231\n",
      "211/223, train_loss: 0.0993, step time: 0.1252\n",
      "212/223, train_loss: 0.1152, step time: 0.1005\n",
      "213/223, train_loss: 0.0971, step time: 0.0993\n",
      "214/223, train_loss: 0.1071, step time: 0.0996\n",
      "215/223, train_loss: 0.1237, step time: 0.0993\n",
      "216/223, train_loss: 0.1110, step time: 0.0987\n",
      "217/223, train_loss: 0.1008, step time: 0.1030\n",
      "218/223, train_loss: 0.1081, step time: 0.1009\n",
      "219/223, train_loss: 0.0924, step time: 0.1007\n",
      "220/223, train_loss: 0.1199, step time: 0.1002\n",
      "221/223, train_loss: 0.1005, step time: 0.1165\n",
      "222/223, train_loss: 0.0999, step time: 0.0994\n",
      "223/223, train_loss: 0.1069, step time: 0.0994\n",
      "epoch 172 average loss: 0.1050\n",
      "time consuming of epoch 172 is: 87.8948\n",
      "----------\n",
      "epoch 173/300\n",
      "1/223, train_loss: 0.1080, step time: 0.1010\n",
      "2/223, train_loss: 0.1054, step time: 0.0997\n",
      "3/223, train_loss: 0.0951, step time: 0.1059\n",
      "4/223, train_loss: 0.1054, step time: 0.1154\n",
      "5/223, train_loss: 0.1097, step time: 0.1100\n",
      "6/223, train_loss: 0.0956, step time: 0.1288\n",
      "7/223, train_loss: 0.0927, step time: 0.1103\n",
      "8/223, train_loss: 0.0946, step time: 0.1318\n",
      "9/223, train_loss: 0.0929, step time: 0.1027\n",
      "10/223, train_loss: 0.0999, step time: 0.1133\n",
      "11/223, train_loss: 0.1081, step time: 0.1135\n",
      "12/223, train_loss: 0.1123, step time: 0.1167\n",
      "13/223, train_loss: 0.0997, step time: 0.1093\n",
      "14/223, train_loss: 0.0997, step time: 0.1180\n",
      "15/223, train_loss: 0.1120, step time: 0.1060\n",
      "16/223, train_loss: 0.1042, step time: 0.1003\n",
      "17/223, train_loss: 0.1167, step time: 0.1036\n",
      "18/223, train_loss: 0.1038, step time: 0.1089\n",
      "19/223, train_loss: 0.1034, step time: 0.1109\n",
      "20/223, train_loss: 0.1098, step time: 0.1258\n",
      "21/223, train_loss: 0.0996, step time: 0.1050\n",
      "22/223, train_loss: 0.1054, step time: 0.1161\n",
      "23/223, train_loss: 0.1035, step time: 0.1042\n",
      "24/223, train_loss: 0.0999, step time: 0.1010\n",
      "25/223, train_loss: 0.0968, step time: 0.1004\n",
      "26/223, train_loss: 0.1057, step time: 0.1097\n",
      "27/223, train_loss: 0.1013, step time: 0.0999\n",
      "28/223, train_loss: 0.0939, step time: 0.1008\n",
      "29/223, train_loss: 0.1185, step time: 0.1000\n",
      "30/223, train_loss: 0.1057, step time: 0.1116\n",
      "31/223, train_loss: 0.0925, step time: 0.0997\n",
      "32/223, train_loss: 0.1098, step time: 0.1004\n",
      "33/223, train_loss: 0.1087, step time: 0.0996\n",
      "34/223, train_loss: 0.0963, step time: 0.1129\n",
      "35/223, train_loss: 0.1036, step time: 0.1016\n",
      "36/223, train_loss: 0.1078, step time: 0.1009\n",
      "37/223, train_loss: 0.1088, step time: 0.1085\n",
      "38/223, train_loss: 0.0944, step time: 0.1001\n",
      "39/223, train_loss: 0.0995, step time: 0.1007\n",
      "40/223, train_loss: 0.1003, step time: 0.1009\n",
      "41/223, train_loss: 0.0988, step time: 0.1068\n",
      "42/223, train_loss: 0.0995, step time: 0.1006\n",
      "43/223, train_loss: 0.1079, step time: 0.1007\n",
      "44/223, train_loss: 0.0997, step time: 0.1008\n",
      "45/223, train_loss: 0.1020, step time: 0.1113\n",
      "46/223, train_loss: 0.0976, step time: 0.1046\n",
      "47/223, train_loss: 0.0919, step time: 0.1006\n",
      "48/223, train_loss: 0.0987, step time: 0.1008\n",
      "49/223, train_loss: 0.0974, step time: 0.1075\n",
      "50/223, train_loss: 0.1045, step time: 0.1053\n",
      "51/223, train_loss: 0.0933, step time: 0.1016\n",
      "52/223, train_loss: 0.1089, step time: 0.1131\n",
      "53/223, train_loss: 0.1107, step time: 0.0985\n",
      "54/223, train_loss: 0.1006, step time: 0.1110\n",
      "55/223, train_loss: 0.0985, step time: 0.0998\n",
      "56/223, train_loss: 0.1048, step time: 0.0998\n",
      "57/223, train_loss: 0.1061, step time: 0.1135\n",
      "58/223, train_loss: 0.1058, step time: 0.1136\n",
      "59/223, train_loss: 0.0980, step time: 0.1024\n",
      "60/223, train_loss: 0.1021, step time: 0.1274\n",
      "61/223, train_loss: 0.1055, step time: 0.1058\n",
      "62/223, train_loss: 0.1034, step time: 0.1062\n",
      "63/223, train_loss: 0.1136, step time: 0.1004\n",
      "64/223, train_loss: 0.1129, step time: 0.1008\n",
      "65/223, train_loss: 0.1182, step time: 0.1023\n",
      "66/223, train_loss: 0.1144, step time: 0.1093\n",
      "67/223, train_loss: 0.1161, step time: 0.0994\n",
      "68/223, train_loss: 0.1083, step time: 0.1010\n",
      "69/223, train_loss: 0.1038, step time: 0.1165\n",
      "70/223, train_loss: 0.0973, step time: 0.1080\n",
      "71/223, train_loss: 0.0902, step time: 0.1011\n",
      "72/223, train_loss: 0.1075, step time: 0.1232\n",
      "73/223, train_loss: 0.1012, step time: 0.1090\n",
      "74/223, train_loss: 0.1015, step time: 0.1061\n",
      "75/223, train_loss: 0.0994, step time: 0.1140\n",
      "76/223, train_loss: 0.0936, step time: 0.1069\n",
      "77/223, train_loss: 0.1113, step time: 0.1178\n",
      "78/223, train_loss: 0.1042, step time: 0.1087\n",
      "79/223, train_loss: 0.1010, step time: 0.1146\n",
      "80/223, train_loss: 0.1018, step time: 0.1156\n",
      "81/223, train_loss: 0.1012, step time: 0.1051\n",
      "82/223, train_loss: 0.1045, step time: 0.1232\n",
      "83/223, train_loss: 0.1063, step time: 0.1029\n",
      "84/223, train_loss: 0.1005, step time: 0.1248\n",
      "85/223, train_loss: 0.1113, step time: 0.1031\n",
      "86/223, train_loss: 0.0957, step time: 0.1142\n",
      "87/223, train_loss: 0.0990, step time: 0.1140\n",
      "88/223, train_loss: 0.1093, step time: 0.1004\n",
      "89/223, train_loss: 0.0954, step time: 0.1200\n",
      "90/223, train_loss: 0.0972, step time: 0.1319\n",
      "91/223, train_loss: 0.1076, step time: 0.1151\n",
      "92/223, train_loss: 0.0955, step time: 0.1155\n",
      "93/223, train_loss: 0.1127, step time: 0.1095\n",
      "94/223, train_loss: 0.0984, step time: 0.1217\n",
      "95/223, train_loss: 0.1251, step time: 0.1227\n",
      "96/223, train_loss: 0.0905, step time: 0.1067\n",
      "97/223, train_loss: 0.0945, step time: 0.1076\n",
      "98/223, train_loss: 0.0981, step time: 0.1158\n",
      "99/223, train_loss: 0.1131, step time: 0.1194\n",
      "100/223, train_loss: 0.1073, step time: 0.1521\n",
      "101/223, train_loss: 0.1084, step time: 0.1241\n",
      "102/223, train_loss: 0.1124, step time: 0.0992\n",
      "103/223, train_loss: 0.0972, step time: 0.1000\n",
      "104/223, train_loss: 0.1055, step time: 0.0993\n",
      "105/223, train_loss: 0.1161, step time: 0.1001\n",
      "106/223, train_loss: 0.1049, step time: 0.0988\n",
      "107/223, train_loss: 0.0922, step time: 0.0997\n",
      "108/223, train_loss: 0.1013, step time: 0.1006\n",
      "109/223, train_loss: 0.1007, step time: 0.1002\n",
      "110/223, train_loss: 0.1046, step time: 0.0995\n",
      "111/223, train_loss: 0.3015, step time: 0.0998\n",
      "112/223, train_loss: 0.0947, step time: 0.1003\n",
      "113/223, train_loss: 0.1012, step time: 0.1012\n",
      "114/223, train_loss: 0.1233, step time: 0.1263\n",
      "115/223, train_loss: 0.1015, step time: 0.1071\n",
      "116/223, train_loss: 0.1003, step time: 0.1150\n",
      "117/223, train_loss: 0.1058, step time: 0.1159\n",
      "118/223, train_loss: 0.1097, step time: 0.1289\n",
      "119/223, train_loss: 0.1052, step time: 0.1013\n",
      "120/223, train_loss: 0.1012, step time: 0.1031\n",
      "121/223, train_loss: 0.1069, step time: 0.1262\n",
      "122/223, train_loss: 0.1101, step time: 0.1057\n",
      "123/223, train_loss: 0.1131, step time: 0.1002\n",
      "124/223, train_loss: 0.1109, step time: 0.1009\n",
      "125/223, train_loss: 0.1107, step time: 0.1007\n",
      "126/223, train_loss: 0.1080, step time: 0.1076\n",
      "127/223, train_loss: 0.1095, step time: 0.1093\n",
      "128/223, train_loss: 0.1030, step time: 0.1299\n",
      "129/223, train_loss: 0.0978, step time: 0.1157\n",
      "130/223, train_loss: 0.1083, step time: 0.1269\n",
      "131/223, train_loss: 0.1065, step time: 0.1137\n",
      "132/223, train_loss: 0.0973, step time: 0.1060\n",
      "133/223, train_loss: 0.0988, step time: 0.1004\n",
      "134/223, train_loss: 0.1027, step time: 0.1313\n",
      "135/223, train_loss: 0.1067, step time: 0.0991\n",
      "136/223, train_loss: 0.1091, step time: 0.1020\n",
      "137/223, train_loss: 0.1040, step time: 0.1132\n",
      "138/223, train_loss: 0.0998, step time: 0.1164\n",
      "139/223, train_loss: 0.0933, step time: 0.1145\n",
      "140/223, train_loss: 0.1215, step time: 0.0995\n",
      "141/223, train_loss: 0.0934, step time: 0.0984\n",
      "142/223, train_loss: 0.1165, step time: 0.1147\n",
      "143/223, train_loss: 0.0988, step time: 0.1249\n",
      "144/223, train_loss: 0.1052, step time: 0.0997\n",
      "145/223, train_loss: 0.1052, step time: 0.1138\n",
      "146/223, train_loss: 0.1067, step time: 0.1003\n",
      "147/223, train_loss: 0.1074, step time: 0.0990\n",
      "148/223, train_loss: 0.0998, step time: 0.1144\n",
      "149/223, train_loss: 0.1138, step time: 0.1330\n",
      "150/223, train_loss: 0.1072, step time: 0.0994\n",
      "151/223, train_loss: 0.1017, step time: 0.1002\n",
      "152/223, train_loss: 0.1025, step time: 0.1009\n",
      "153/223, train_loss: 0.1017, step time: 0.1062\n",
      "154/223, train_loss: 0.0876, step time: 0.0995\n",
      "155/223, train_loss: 0.1019, step time: 0.0992\n",
      "156/223, train_loss: 0.0931, step time: 0.1002\n",
      "157/223, train_loss: 0.1018, step time: 0.1226\n",
      "158/223, train_loss: 0.1021, step time: 0.0996\n",
      "159/223, train_loss: 0.0917, step time: 0.0994\n",
      "160/223, train_loss: 0.1031, step time: 0.1004\n",
      "161/223, train_loss: 0.0957, step time: 0.1006\n",
      "162/223, train_loss: 0.0938, step time: 0.0986\n",
      "163/223, train_loss: 0.1011, step time: 0.0994\n",
      "164/223, train_loss: 0.1057, step time: 0.1005\n",
      "165/223, train_loss: 0.1051, step time: 0.1002\n",
      "166/223, train_loss: 0.0996, step time: 0.1076\n",
      "167/223, train_loss: 0.1118, step time: 0.1013\n",
      "168/223, train_loss: 0.1070, step time: 0.1014\n",
      "169/223, train_loss: 0.0962, step time: 0.1005\n",
      "170/223, train_loss: 0.1009, step time: 0.1264\n",
      "171/223, train_loss: 0.1032, step time: 0.1157\n",
      "172/223, train_loss: 0.1027, step time: 0.1222\n",
      "173/223, train_loss: 0.1203, step time: 0.1078\n",
      "174/223, train_loss: 0.1010, step time: 0.1241\n",
      "175/223, train_loss: 0.1070, step time: 0.1211\n",
      "176/223, train_loss: 0.1129, step time: 0.1039\n",
      "177/223, train_loss: 0.1012, step time: 0.0997\n",
      "178/223, train_loss: 0.1051, step time: 0.0997\n",
      "179/223, train_loss: 0.0944, step time: 0.0995\n",
      "180/223, train_loss: 0.0974, step time: 0.1017\n",
      "181/223, train_loss: 0.0964, step time: 0.1319\n",
      "182/223, train_loss: 0.1174, step time: 0.0995\n",
      "183/223, train_loss: 0.0953, step time: 0.0987\n",
      "184/223, train_loss: 0.1032, step time: 0.1017\n",
      "185/223, train_loss: 0.1080, step time: 0.1061\n",
      "186/223, train_loss: 0.1028, step time: 0.1021\n",
      "187/223, train_loss: 0.1059, step time: 0.1300\n",
      "188/223, train_loss: 0.1152, step time: 0.0991\n",
      "189/223, train_loss: 0.1068, step time: 0.1027\n",
      "190/223, train_loss: 0.1204, step time: 0.1113\n",
      "191/223, train_loss: 0.1045, step time: 0.1088\n",
      "192/223, train_loss: 0.1090, step time: 0.1019\n",
      "193/223, train_loss: 0.0930, step time: 0.0985\n",
      "194/223, train_loss: 0.1006, step time: 0.1001\n",
      "195/223, train_loss: 0.1039, step time: 0.1155\n",
      "196/223, train_loss: 0.0921, step time: 0.1126\n",
      "197/223, train_loss: 0.1083, step time: 0.0989\n",
      "198/223, train_loss: 0.1060, step time: 0.0993\n",
      "199/223, train_loss: 0.1056, step time: 0.1520\n",
      "200/223, train_loss: 0.1061, step time: 0.0990\n",
      "201/223, train_loss: 0.1115, step time: 0.0987\n",
      "202/223, train_loss: 0.1040, step time: 0.1371\n",
      "203/223, train_loss: 0.1039, step time: 0.1081\n",
      "204/223, train_loss: 0.0996, step time: 0.1003\n",
      "205/223, train_loss: 0.1011, step time: 0.1127\n",
      "206/223, train_loss: 0.1067, step time: 0.1098\n",
      "207/223, train_loss: 0.1087, step time: 0.1063\n",
      "208/223, train_loss: 0.1085, step time: 0.1011\n",
      "209/223, train_loss: 0.1037, step time: 0.1058\n",
      "210/223, train_loss: 0.1146, step time: 0.1135\n",
      "211/223, train_loss: 0.0988, step time: 0.1097\n",
      "212/223, train_loss: 0.0967, step time: 0.1099\n",
      "213/223, train_loss: 0.1109, step time: 0.1009\n",
      "214/223, train_loss: 0.1119, step time: 0.1146\n",
      "215/223, train_loss: 0.0956, step time: 0.1001\n",
      "216/223, train_loss: 0.1220, step time: 0.1006\n",
      "217/223, train_loss: 0.1046, step time: 0.0998\n",
      "218/223, train_loss: 0.0988, step time: 0.0994\n",
      "219/223, train_loss: 0.0974, step time: 0.1035\n",
      "220/223, train_loss: 0.1120, step time: 0.0997\n",
      "221/223, train_loss: 0.1120, step time: 0.0998\n",
      "222/223, train_loss: 0.1022, step time: 0.0998\n",
      "223/223, train_loss: 0.1073, step time: 0.0992\n",
      "epoch 173 average loss: 0.1047\n",
      "time consuming of epoch 173 is: 96.2870\n",
      "----------\n",
      "epoch 174/300\n",
      "1/223, train_loss: 0.1180, step time: 0.1245\n",
      "2/223, train_loss: 0.1017, step time: 0.1138\n",
      "3/223, train_loss: 0.1062, step time: 0.1195\n",
      "4/223, train_loss: 0.1165, step time: 0.1010\n",
      "5/223, train_loss: 0.1132, step time: 0.1053\n",
      "6/223, train_loss: 0.1023, step time: 0.1157\n",
      "7/223, train_loss: 0.1244, step time: 0.1004\n",
      "8/223, train_loss: 0.1065, step time: 0.1292\n",
      "9/223, train_loss: 0.1050, step time: 0.1056\n",
      "10/223, train_loss: 0.1069, step time: 0.0999\n",
      "11/223, train_loss: 0.1065, step time: 0.1234\n",
      "12/223, train_loss: 0.1068, step time: 0.1007\n",
      "13/223, train_loss: 0.1131, step time: 0.1024\n",
      "14/223, train_loss: 0.1000, step time: 0.1113\n",
      "15/223, train_loss: 0.0979, step time: 0.1005\n",
      "16/223, train_loss: 0.0984, step time: 0.1038\n",
      "17/223, train_loss: 0.1012, step time: 0.1093\n",
      "18/223, train_loss: 0.0945, step time: 0.1002\n",
      "19/223, train_loss: 0.1053, step time: 0.1150\n",
      "20/223, train_loss: 0.1028, step time: 0.1009\n",
      "21/223, train_loss: 0.1032, step time: 0.1589\n",
      "22/223, train_loss: 0.1026, step time: 0.1183\n",
      "23/223, train_loss: 0.0947, step time: 0.1016\n",
      "24/223, train_loss: 0.1103, step time: 0.1016\n",
      "25/223, train_loss: 0.1069, step time: 0.1142\n",
      "26/223, train_loss: 0.0986, step time: 0.1237\n",
      "27/223, train_loss: 0.0952, step time: 0.1158\n",
      "28/223, train_loss: 0.1120, step time: 0.1115\n",
      "29/223, train_loss: 0.1088, step time: 0.1034\n",
      "30/223, train_loss: 0.1132, step time: 0.1285\n",
      "31/223, train_loss: 0.0958, step time: 0.1122\n",
      "32/223, train_loss: 0.1117, step time: 0.1101\n",
      "33/223, train_loss: 0.1103, step time: 0.1012\n",
      "34/223, train_loss: 0.1096, step time: 0.1392\n",
      "35/223, train_loss: 0.0999, step time: 0.1157\n",
      "36/223, train_loss: 0.1032, step time: 0.1022\n",
      "37/223, train_loss: 0.1094, step time: 0.1001\n",
      "38/223, train_loss: 0.1121, step time: 0.1233\n",
      "39/223, train_loss: 0.0996, step time: 0.1209\n",
      "40/223, train_loss: 0.1029, step time: 0.1241\n",
      "41/223, train_loss: 0.0889, step time: 0.1143\n",
      "42/223, train_loss: 0.1061, step time: 0.0999\n",
      "43/223, train_loss: 0.1054, step time: 0.1005\n",
      "44/223, train_loss: 0.1042, step time: 0.1008\n",
      "45/223, train_loss: 0.0964, step time: 0.1170\n",
      "46/223, train_loss: 0.0889, step time: 0.1134\n",
      "47/223, train_loss: 0.1070, step time: 0.1359\n",
      "48/223, train_loss: 0.1124, step time: 0.1041\n",
      "49/223, train_loss: 0.1002, step time: 0.1088\n",
      "50/223, train_loss: 0.1093, step time: 0.1149\n",
      "51/223, train_loss: 0.1137, step time: 0.0995\n",
      "52/223, train_loss: 0.0964, step time: 0.1026\n",
      "53/223, train_loss: 0.0879, step time: 0.1120\n",
      "54/223, train_loss: 0.0948, step time: 0.0997\n",
      "55/223, train_loss: 0.1006, step time: 0.0999\n",
      "56/223, train_loss: 0.1009, step time: 0.1153\n",
      "57/223, train_loss: 0.0995, step time: 0.1068\n",
      "58/223, train_loss: 0.1192, step time: 0.1075\n",
      "59/223, train_loss: 0.1239, step time: 0.1059\n",
      "60/223, train_loss: 0.1062, step time: 0.1237\n",
      "61/223, train_loss: 0.1025, step time: 0.1130\n",
      "62/223, train_loss: 0.0939, step time: 0.1026\n",
      "63/223, train_loss: 0.0979, step time: 0.1025\n",
      "64/223, train_loss: 0.0973, step time: 0.1007\n",
      "65/223, train_loss: 0.1078, step time: 0.1626\n",
      "66/223, train_loss: 0.1146, step time: 0.1168\n",
      "67/223, train_loss: 0.1042, step time: 0.1032\n",
      "68/223, train_loss: 0.1172, step time: 0.1021\n",
      "69/223, train_loss: 0.1087, step time: 0.0993\n",
      "70/223, train_loss: 0.1011, step time: 0.1329\n",
      "71/223, train_loss: 0.1061, step time: 0.1593\n",
      "72/223, train_loss: 0.1090, step time: 0.1043\n",
      "73/223, train_loss: 0.1037, step time: 0.1198\n",
      "74/223, train_loss: 0.0890, step time: 0.1281\n",
      "75/223, train_loss: 0.1121, step time: 0.1308\n",
      "76/223, train_loss: 0.1030, step time: 0.1005\n",
      "77/223, train_loss: 0.1005, step time: 0.1203\n",
      "78/223, train_loss: 0.1020, step time: 0.1119\n",
      "79/223, train_loss: 0.1029, step time: 0.1013\n",
      "80/223, train_loss: 0.1013, step time: 0.1007\n",
      "81/223, train_loss: 0.1058, step time: 0.1165\n",
      "82/223, train_loss: 0.0928, step time: 0.1087\n",
      "83/223, train_loss: 0.1060, step time: 0.1004\n",
      "84/223, train_loss: 0.1096, step time: 0.0997\n",
      "85/223, train_loss: 0.0991, step time: 0.1124\n",
      "86/223, train_loss: 0.0964, step time: 0.1427\n",
      "87/223, train_loss: 0.1009, step time: 0.1164\n",
      "88/223, train_loss: 0.1008, step time: 0.0996\n",
      "89/223, train_loss: 0.0893, step time: 0.1097\n",
      "90/223, train_loss: 0.0947, step time: 0.1055\n",
      "91/223, train_loss: 0.0985, step time: 0.1084\n",
      "92/223, train_loss: 0.1032, step time: 0.0998\n",
      "93/223, train_loss: 0.1140, step time: 0.1146\n",
      "94/223, train_loss: 0.1156, step time: 0.1001\n",
      "95/223, train_loss: 0.0966, step time: 0.1003\n",
      "96/223, train_loss: 0.1037, step time: 0.1131\n",
      "97/223, train_loss: 0.1123, step time: 0.1081\n",
      "98/223, train_loss: 0.1080, step time: 0.1081\n",
      "99/223, train_loss: 0.1088, step time: 0.1282\n",
      "100/223, train_loss: 0.0987, step time: 0.1257\n",
      "101/223, train_loss: 0.1077, step time: 0.1139\n",
      "102/223, train_loss: 0.1166, step time: 0.1390\n",
      "103/223, train_loss: 0.1173, step time: 0.1150\n",
      "104/223, train_loss: 0.0974, step time: 0.1075\n",
      "105/223, train_loss: 0.0957, step time: 0.1072\n",
      "106/223, train_loss: 0.1008, step time: 0.1075\n",
      "107/223, train_loss: 0.0969, step time: 0.1114\n",
      "108/223, train_loss: 0.0978, step time: 0.1116\n",
      "109/223, train_loss: 0.0987, step time: 0.1127\n",
      "110/223, train_loss: 0.1125, step time: 0.1165\n",
      "111/223, train_loss: 0.1031, step time: 0.1223\n",
      "112/223, train_loss: 0.1021, step time: 0.1084\n",
      "113/223, train_loss: 0.1142, step time: 0.1105\n",
      "114/223, train_loss: 0.1116, step time: 0.1267\n",
      "115/223, train_loss: 0.0953, step time: 0.1192\n",
      "116/223, train_loss: 0.1110, step time: 0.1078\n",
      "117/223, train_loss: 0.0929, step time: 0.1484\n",
      "118/223, train_loss: 0.0906, step time: 0.1235\n",
      "119/223, train_loss: 0.0982, step time: 0.1091\n",
      "120/223, train_loss: 0.1217, step time: 0.1196\n",
      "121/223, train_loss: 0.1057, step time: 0.1102\n",
      "122/223, train_loss: 0.1092, step time: 0.1014\n",
      "123/223, train_loss: 0.0983, step time: 0.1214\n",
      "124/223, train_loss: 0.0971, step time: 0.1001\n",
      "125/223, train_loss: 0.0972, step time: 0.1073\n",
      "126/223, train_loss: 0.0878, step time: 0.1052\n",
      "127/223, train_loss: 0.1128, step time: 0.1008\n",
      "128/223, train_loss: 0.1052, step time: 0.1111\n",
      "129/223, train_loss: 0.1016, step time: 0.1084\n",
      "130/223, train_loss: 0.0933, step time: 0.1090\n",
      "131/223, train_loss: 0.1050, step time: 0.1311\n",
      "132/223, train_loss: 0.1139, step time: 0.1125\n",
      "133/223, train_loss: 0.1066, step time: 0.1012\n",
      "134/223, train_loss: 0.1182, step time: 0.1271\n",
      "135/223, train_loss: 0.0956, step time: 0.1298\n",
      "136/223, train_loss: 0.1080, step time: 0.1005\n",
      "137/223, train_loss: 0.1103, step time: 0.1005\n",
      "138/223, train_loss: 0.1098, step time: 0.1004\n",
      "139/223, train_loss: 0.1008, step time: 0.1007\n",
      "140/223, train_loss: 0.1093, step time: 0.1056\n",
      "141/223, train_loss: 0.1003, step time: 0.1113\n",
      "142/223, train_loss: 0.1081, step time: 0.1165\n",
      "143/223, train_loss: 0.1005, step time: 0.0993\n",
      "144/223, train_loss: 0.1043, step time: 0.1623\n",
      "145/223, train_loss: 0.1042, step time: 0.1067\n",
      "146/223, train_loss: 0.0981, step time: 0.1164\n",
      "147/223, train_loss: 0.1040, step time: 0.1116\n",
      "148/223, train_loss: 0.1019, step time: 0.0992\n",
      "149/223, train_loss: 0.1005, step time: 0.1346\n",
      "150/223, train_loss: 0.1044, step time: 0.1567\n",
      "151/223, train_loss: 0.1018, step time: 0.1368\n",
      "152/223, train_loss: 0.1029, step time: 0.1055\n",
      "153/223, train_loss: 0.0963, step time: 0.1004\n",
      "154/223, train_loss: 0.1050, step time: 0.1082\n",
      "155/223, train_loss: 0.1042, step time: 0.1008\n",
      "156/223, train_loss: 0.0920, step time: 0.1010\n",
      "157/223, train_loss: 0.0968, step time: 0.1002\n",
      "158/223, train_loss: 0.1047, step time: 0.1571\n",
      "159/223, train_loss: 0.0976, step time: 0.1002\n",
      "160/223, train_loss: 0.1035, step time: 0.1004\n",
      "161/223, train_loss: 0.1038, step time: 0.1159\n",
      "162/223, train_loss: 0.1077, step time: 0.1003\n",
      "163/223, train_loss: 0.1117, step time: 0.1001\n",
      "164/223, train_loss: 0.0989, step time: 0.1047\n",
      "165/223, train_loss: 0.0951, step time: 0.1164\n",
      "166/223, train_loss: 0.1070, step time: 0.1137\n",
      "167/223, train_loss: 0.0934, step time: 0.1134\n",
      "168/223, train_loss: 0.1076, step time: 0.1007\n",
      "169/223, train_loss: 0.1023, step time: 0.1007\n",
      "170/223, train_loss: 0.1019, step time: 0.1084\n",
      "171/223, train_loss: 0.1031, step time: 0.1058\n",
      "172/223, train_loss: 0.0983, step time: 0.1044\n",
      "173/223, train_loss: 0.0956, step time: 0.0997\n",
      "174/223, train_loss: 0.0983, step time: 0.0999\n",
      "175/223, train_loss: 0.1187, step time: 0.1009\n",
      "176/223, train_loss: 0.1118, step time: 0.1010\n",
      "177/223, train_loss: 0.1013, step time: 0.1009\n",
      "178/223, train_loss: 0.1098, step time: 0.1608\n",
      "179/223, train_loss: 0.0969, step time: 0.1239\n",
      "180/223, train_loss: 0.1079, step time: 0.1042\n",
      "181/223, train_loss: 0.0932, step time: 0.1010\n",
      "182/223, train_loss: 0.0935, step time: 0.1005\n",
      "183/223, train_loss: 0.1052, step time: 0.1006\n",
      "184/223, train_loss: 0.1071, step time: 0.1070\n",
      "185/223, train_loss: 0.0966, step time: 0.1100\n",
      "186/223, train_loss: 0.1090, step time: 0.1124\n",
      "187/223, train_loss: 0.1078, step time: 0.1032\n",
      "188/223, train_loss: 0.2980, step time: 0.1023\n",
      "189/223, train_loss: 0.1092, step time: 0.1172\n",
      "190/223, train_loss: 0.1044, step time: 0.1476\n",
      "191/223, train_loss: 0.0937, step time: 0.1269\n",
      "192/223, train_loss: 0.1123, step time: 0.1133\n",
      "193/223, train_loss: 0.0920, step time: 0.1111\n",
      "194/223, train_loss: 0.1085, step time: 0.1104\n",
      "195/223, train_loss: 0.0951, step time: 0.1191\n",
      "196/223, train_loss: 0.1183, step time: 0.1064\n",
      "197/223, train_loss: 0.1053, step time: 0.0999\n",
      "198/223, train_loss: 0.1092, step time: 0.1002\n",
      "199/223, train_loss: 0.1019, step time: 0.1005\n",
      "200/223, train_loss: 0.1001, step time: 0.1149\n",
      "201/223, train_loss: 0.0976, step time: 0.1005\n",
      "202/223, train_loss: 0.0968, step time: 0.1002\n",
      "203/223, train_loss: 0.1033, step time: 0.1006\n",
      "204/223, train_loss: 0.1018, step time: 0.0997\n",
      "205/223, train_loss: 0.0928, step time: 0.1004\n",
      "206/223, train_loss: 0.0892, step time: 0.1229\n",
      "207/223, train_loss: 0.0979, step time: 0.1399\n",
      "208/223, train_loss: 0.1152, step time: 0.1505\n",
      "209/223, train_loss: 0.1034, step time: 0.1084\n",
      "210/223, train_loss: 0.1152, step time: 0.1195\n",
      "211/223, train_loss: 0.0962, step time: 0.1138\n",
      "212/223, train_loss: 0.1026, step time: 0.1070\n",
      "213/223, train_loss: 0.1033, step time: 0.1003\n",
      "214/223, train_loss: 0.1187, step time: 0.1011\n",
      "215/223, train_loss: 0.0951, step time: 0.1083\n",
      "216/223, train_loss: 0.1117, step time: 0.1277\n",
      "217/223, train_loss: 0.1007, step time: 0.1005\n",
      "218/223, train_loss: 0.0949, step time: 0.1177\n",
      "219/223, train_loss: 0.1192, step time: 0.1064\n",
      "220/223, train_loss: 0.0989, step time: 0.1011\n",
      "221/223, train_loss: 0.0909, step time: 0.0992\n",
      "222/223, train_loss: 0.1091, step time: 0.0996\n",
      "223/223, train_loss: 0.0931, step time: 0.0998\n",
      "epoch 174 average loss: 0.1045\n",
      "time consuming of epoch 174 is: 92.4170\n",
      "----------\n",
      "epoch 175/300\n",
      "1/223, train_loss: 0.1094, step time: 0.1045\n",
      "2/223, train_loss: 0.1148, step time: 0.1054\n",
      "3/223, train_loss: 0.0895, step time: 0.1257\n",
      "4/223, train_loss: 0.1009, step time: 0.1254\n",
      "5/223, train_loss: 0.1122, step time: 0.1001\n",
      "6/223, train_loss: 0.1003, step time: 0.1132\n",
      "7/223, train_loss: 0.0990, step time: 0.1062\n",
      "8/223, train_loss: 0.1076, step time: 0.1344\n",
      "9/223, train_loss: 0.1102, step time: 0.1037\n",
      "10/223, train_loss: 0.1043, step time: 0.1038\n",
      "11/223, train_loss: 0.1037, step time: 0.1014\n",
      "12/223, train_loss: 0.1124, step time: 0.1451\n",
      "13/223, train_loss: 0.1079, step time: 0.1125\n",
      "14/223, train_loss: 0.0948, step time: 0.0995\n",
      "15/223, train_loss: 0.0967, step time: 0.0998\n",
      "16/223, train_loss: 0.1067, step time: 0.1159\n",
      "17/223, train_loss: 0.0948, step time: 0.1332\n",
      "18/223, train_loss: 0.0942, step time: 0.1245\n",
      "19/223, train_loss: 0.0990, step time: 0.1096\n",
      "20/223, train_loss: 0.1137, step time: 0.1058\n",
      "21/223, train_loss: 0.0935, step time: 0.1060\n",
      "22/223, train_loss: 0.0992, step time: 0.1127\n",
      "23/223, train_loss: 0.1040, step time: 0.1459\n",
      "24/223, train_loss: 0.0946, step time: 0.1108\n",
      "25/223, train_loss: 0.0999, step time: 0.1140\n",
      "26/223, train_loss: 0.0977, step time: 0.1012\n",
      "27/223, train_loss: 0.1131, step time: 0.1081\n",
      "28/223, train_loss: 0.1154, step time: 0.1006\n",
      "29/223, train_loss: 0.1115, step time: 0.1023\n",
      "30/223, train_loss: 0.1104, step time: 0.1035\n",
      "31/223, train_loss: 0.1041, step time: 0.1044\n",
      "32/223, train_loss: 0.1018, step time: 0.1003\n",
      "33/223, train_loss: 0.1087, step time: 0.1302\n",
      "34/223, train_loss: 0.0901, step time: 0.1173\n",
      "35/223, train_loss: 0.0924, step time: 0.1122\n",
      "36/223, train_loss: 0.1164, step time: 0.1010\n",
      "37/223, train_loss: 0.1159, step time: 0.1213\n",
      "38/223, train_loss: 0.0973, step time: 0.1107\n",
      "39/223, train_loss: 0.1040, step time: 0.1177\n",
      "40/223, train_loss: 0.1056, step time: 0.1041\n",
      "41/223, train_loss: 0.1027, step time: 0.1197\n",
      "42/223, train_loss: 0.1056, step time: 0.1308\n",
      "43/223, train_loss: 0.1129, step time: 0.1234\n",
      "44/223, train_loss: 0.1016, step time: 0.0998\n",
      "45/223, train_loss: 0.1183, step time: 0.1014\n",
      "46/223, train_loss: 0.1112, step time: 0.1055\n",
      "47/223, train_loss: 0.1051, step time: 0.1109\n",
      "48/223, train_loss: 0.1012, step time: 0.1052\n",
      "49/223, train_loss: 0.1056, step time: 0.1353\n",
      "50/223, train_loss: 0.0978, step time: 0.1110\n",
      "51/223, train_loss: 0.1142, step time: 0.1121\n",
      "52/223, train_loss: 0.1089, step time: 0.0999\n",
      "53/223, train_loss: 0.1026, step time: 0.1000\n",
      "54/223, train_loss: 0.0921, step time: 0.1014\n",
      "55/223, train_loss: 0.0997, step time: 0.1001\n",
      "56/223, train_loss: 0.1098, step time: 0.1170\n",
      "57/223, train_loss: 0.0996, step time: 0.1408\n",
      "58/223, train_loss: 0.0932, step time: 0.1000\n",
      "59/223, train_loss: 0.1133, step time: 0.1004\n",
      "60/223, train_loss: 0.1141, step time: 0.1005\n",
      "61/223, train_loss: 0.0998, step time: 0.1233\n",
      "62/223, train_loss: 0.1027, step time: 0.1112\n",
      "63/223, train_loss: 0.1024, step time: 0.1289\n",
      "64/223, train_loss: 0.1010, step time: 0.1126\n",
      "65/223, train_loss: 0.1017, step time: 0.1115\n",
      "66/223, train_loss: 0.0928, step time: 0.1093\n",
      "67/223, train_loss: 0.1059, step time: 0.1314\n",
      "68/223, train_loss: 0.1017, step time: 0.1098\n",
      "69/223, train_loss: 0.0944, step time: 0.1157\n",
      "70/223, train_loss: 0.1029, step time: 0.1183\n",
      "71/223, train_loss: 0.0985, step time: 0.1168\n",
      "72/223, train_loss: 0.1007, step time: 0.1003\n",
      "73/223, train_loss: 0.0981, step time: 0.1094\n",
      "74/223, train_loss: 0.0969, step time: 0.1002\n",
      "75/223, train_loss: 0.1012, step time: 0.1095\n",
      "76/223, train_loss: 0.1093, step time: 0.1113\n",
      "77/223, train_loss: 0.1001, step time: 0.1016\n",
      "78/223, train_loss: 0.1245, step time: 0.1190\n",
      "79/223, train_loss: 0.1039, step time: 0.1174\n",
      "80/223, train_loss: 0.0950, step time: 0.1173\n",
      "81/223, train_loss: 0.1081, step time: 0.1698\n",
      "82/223, train_loss: 0.0925, step time: 0.1232\n",
      "83/223, train_loss: 0.1081, step time: 0.1089\n",
      "84/223, train_loss: 0.0975, step time: 0.1182\n",
      "85/223, train_loss: 0.0995, step time: 0.1107\n",
      "86/223, train_loss: 0.1117, step time: 0.1270\n",
      "87/223, train_loss: 0.1107, step time: 0.1086\n",
      "88/223, train_loss: 0.0952, step time: 0.1019\n",
      "89/223, train_loss: 0.0998, step time: 0.1049\n",
      "90/223, train_loss: 0.1180, step time: 0.1016\n",
      "91/223, train_loss: 0.1047, step time: 0.1174\n",
      "92/223, train_loss: 0.1006, step time: 0.1103\n",
      "93/223, train_loss: 0.0919, step time: 0.1176\n",
      "94/223, train_loss: 0.0986, step time: 0.1391\n",
      "95/223, train_loss: 0.1058, step time: 0.1003\n",
      "96/223, train_loss: 0.0946, step time: 0.1001\n",
      "97/223, train_loss: 0.0999, step time: 0.0993\n",
      "98/223, train_loss: 0.1089, step time: 0.0995\n",
      "99/223, train_loss: 0.1045, step time: 0.0997\n",
      "100/223, train_loss: 0.0952, step time: 0.1046\n",
      "101/223, train_loss: 0.1026, step time: 0.0997\n",
      "102/223, train_loss: 0.1143, step time: 0.0996\n",
      "103/223, train_loss: 0.0975, step time: 0.0996\n",
      "104/223, train_loss: 0.1055, step time: 0.1009\n",
      "105/223, train_loss: 0.1132, step time: 0.1005\n",
      "106/223, train_loss: 0.1099, step time: 0.1322\n",
      "107/223, train_loss: 0.0958, step time: 0.1002\n",
      "108/223, train_loss: 0.1093, step time: 0.1002\n",
      "109/223, train_loss: 0.1028, step time: 0.1001\n",
      "110/223, train_loss: 0.0990, step time: 0.1005\n",
      "111/223, train_loss: 0.1121, step time: 0.1252\n",
      "112/223, train_loss: 0.1089, step time: 0.1001\n",
      "113/223, train_loss: 0.1083, step time: 0.1020\n",
      "114/223, train_loss: 0.1020, step time: 0.1319\n",
      "115/223, train_loss: 0.1002, step time: 0.1322\n",
      "116/223, train_loss: 0.1019, step time: 0.1160\n",
      "117/223, train_loss: 0.1041, step time: 0.1103\n",
      "118/223, train_loss: 0.1064, step time: 0.1072\n",
      "119/223, train_loss: 0.1091, step time: 0.1191\n",
      "120/223, train_loss: 0.1022, step time: 0.1204\n",
      "121/223, train_loss: 0.0979, step time: 0.1351\n",
      "122/223, train_loss: 0.1048, step time: 0.1012\n",
      "123/223, train_loss: 0.1012, step time: 0.1195\n",
      "124/223, train_loss: 0.1055, step time: 0.1058\n",
      "125/223, train_loss: 0.0941, step time: 0.1148\n",
      "126/223, train_loss: 0.1273, step time: 0.1061\n",
      "127/223, train_loss: 0.1029, step time: 0.1154\n",
      "128/223, train_loss: 0.0954, step time: 0.1036\n",
      "129/223, train_loss: 0.0987, step time: 0.0999\n",
      "130/223, train_loss: 0.1078, step time: 0.1006\n",
      "131/223, train_loss: 0.1119, step time: 0.1029\n",
      "132/223, train_loss: 0.1030, step time: 0.1040\n",
      "133/223, train_loss: 0.1120, step time: 0.1053\n",
      "134/223, train_loss: 0.0970, step time: 0.1015\n",
      "135/223, train_loss: 0.1074, step time: 0.0999\n",
      "136/223, train_loss: 0.1071, step time: 0.1109\n",
      "137/223, train_loss: 0.0965, step time: 0.1079\n",
      "138/223, train_loss: 0.0996, step time: 0.1232\n",
      "139/223, train_loss: 0.0982, step time: 0.1051\n",
      "140/223, train_loss: 0.1010, step time: 0.1293\n",
      "141/223, train_loss: 0.1096, step time: 0.1121\n",
      "142/223, train_loss: 0.0909, step time: 0.1091\n",
      "143/223, train_loss: 0.1053, step time: 0.1321\n",
      "144/223, train_loss: 0.0917, step time: 0.1190\n",
      "145/223, train_loss: 0.1060, step time: 0.0992\n",
      "146/223, train_loss: 0.0910, step time: 0.1175\n",
      "147/223, train_loss: 0.1022, step time: 0.1329\n",
      "148/223, train_loss: 0.1034, step time: 0.1129\n",
      "149/223, train_loss: 0.1058, step time: 0.1067\n",
      "150/223, train_loss: 0.1141, step time: 0.1378\n",
      "151/223, train_loss: 0.0966, step time: 0.1006\n",
      "152/223, train_loss: 0.1079, step time: 0.1198\n",
      "153/223, train_loss: 0.1114, step time: 0.0995\n",
      "154/223, train_loss: 0.0956, step time: 0.1139\n",
      "155/223, train_loss: 0.1122, step time: 0.1420\n",
      "156/223, train_loss: 0.1055, step time: 0.1068\n",
      "157/223, train_loss: 0.1017, step time: 0.1000\n",
      "158/223, train_loss: 0.0994, step time: 0.1160\n",
      "159/223, train_loss: 0.0894, step time: 0.1074\n",
      "160/223, train_loss: 0.0966, step time: 0.1057\n",
      "161/223, train_loss: 0.1109, step time: 0.0999\n",
      "162/223, train_loss: 0.1010, step time: 0.1125\n",
      "163/223, train_loss: 0.1208, step time: 0.1091\n",
      "164/223, train_loss: 0.1030, step time: 0.1053\n",
      "165/223, train_loss: 0.2947, step time: 0.1088\n",
      "166/223, train_loss: 0.1110, step time: 0.1160\n",
      "167/223, train_loss: 0.1041, step time: 0.1053\n",
      "168/223, train_loss: 0.1003, step time: 0.1245\n",
      "169/223, train_loss: 0.1081, step time: 0.1171\n",
      "170/223, train_loss: 0.1151, step time: 0.1041\n",
      "171/223, train_loss: 0.1063, step time: 0.1150\n",
      "172/223, train_loss: 0.0955, step time: 0.1103\n",
      "173/223, train_loss: 0.1011, step time: 0.1243\n",
      "174/223, train_loss: 0.1078, step time: 0.1161\n",
      "175/223, train_loss: 0.1201, step time: 0.1334\n",
      "176/223, train_loss: 0.0972, step time: 0.1223\n",
      "177/223, train_loss: 0.0974, step time: 0.1155\n",
      "178/223, train_loss: 0.1133, step time: 0.1136\n",
      "179/223, train_loss: 0.1237, step time: 0.1135\n",
      "180/223, train_loss: 0.1093, step time: 0.1061\n",
      "181/223, train_loss: 0.1029, step time: 0.1135\n",
      "182/223, train_loss: 0.1117, step time: 0.1096\n",
      "183/223, train_loss: 0.1047, step time: 0.1198\n",
      "184/223, train_loss: 0.1062, step time: 0.1172\n",
      "185/223, train_loss: 0.1037, step time: 0.1148\n",
      "186/223, train_loss: 0.1035, step time: 0.1104\n",
      "187/223, train_loss: 0.1050, step time: 0.1341\n",
      "188/223, train_loss: 0.0958, step time: 0.1245\n",
      "189/223, train_loss: 0.1026, step time: 0.1001\n",
      "190/223, train_loss: 0.1000, step time: 0.0998\n",
      "191/223, train_loss: 0.1054, step time: 0.1002\n",
      "192/223, train_loss: 0.1064, step time: 0.1026\n",
      "193/223, train_loss: 0.1077, step time: 0.1202\n",
      "194/223, train_loss: 0.0952, step time: 0.1113\n",
      "195/223, train_loss: 0.0950, step time: 0.1077\n",
      "196/223, train_loss: 0.1013, step time: 0.1052\n",
      "197/223, train_loss: 0.1019, step time: 0.1202\n",
      "198/223, train_loss: 0.0981, step time: 0.1092\n",
      "199/223, train_loss: 0.0940, step time: 0.1090\n",
      "200/223, train_loss: 0.1042, step time: 0.0997\n",
      "201/223, train_loss: 0.0996, step time: 0.1150\n",
      "202/223, train_loss: 0.1086, step time: 0.1026\n",
      "203/223, train_loss: 0.1032, step time: 0.1170\n",
      "204/223, train_loss: 0.1135, step time: 0.0995\n",
      "205/223, train_loss: 0.1085, step time: 0.0985\n",
      "206/223, train_loss: 0.1033, step time: 0.1001\n",
      "207/223, train_loss: 0.1142, step time: 0.1049\n",
      "208/223, train_loss: 0.1055, step time: 0.1053\n",
      "209/223, train_loss: 0.0999, step time: 0.1150\n",
      "210/223, train_loss: 0.1013, step time: 0.1089\n",
      "211/223, train_loss: 0.1129, step time: 0.1055\n",
      "212/223, train_loss: 0.0952, step time: 0.1025\n",
      "213/223, train_loss: 0.1124, step time: 0.1122\n",
      "214/223, train_loss: 0.1082, step time: 0.1200\n",
      "215/223, train_loss: 0.1091, step time: 0.1122\n",
      "216/223, train_loss: 0.1043, step time: 0.1073\n",
      "217/223, train_loss: 0.1135, step time: 0.1138\n",
      "218/223, train_loss: 0.1048, step time: 0.1195\n",
      "219/223, train_loss: 0.0922, step time: 0.1005\n",
      "220/223, train_loss: 0.1057, step time: 0.1000\n",
      "221/223, train_loss: 0.1051, step time: 0.0988\n",
      "222/223, train_loss: 0.1057, step time: 0.0997\n",
      "223/223, train_loss: 0.1117, step time: 0.1059\n",
      "epoch 175 average loss: 0.1048\n",
      "current epoch: 175 current mean dice: 0.8585 tc: 0.9210 wt: 0.8688 et: 0.7857\n",
      "best mean dice: 0.8600 at epoch: 170\n",
      "time consuming of epoch 175 is: 95.8833\n",
      "----------\n",
      "epoch 176/300\n",
      "1/223, train_loss: 0.0930, step time: 0.1101\n",
      "2/223, train_loss: 0.0977, step time: 0.1010\n",
      "3/223, train_loss: 0.1095, step time: 0.1211\n",
      "4/223, train_loss: 0.0951, step time: 0.1221\n",
      "5/223, train_loss: 0.0972, step time: 0.0999\n",
      "6/223, train_loss: 0.0988, step time: 0.1002\n",
      "7/223, train_loss: 0.1157, step time: 0.1156\n",
      "8/223, train_loss: 0.0964, step time: 0.1164\n",
      "9/223, train_loss: 0.0968, step time: 0.1037\n",
      "10/223, train_loss: 0.1019, step time: 0.1173\n",
      "11/223, train_loss: 0.1101, step time: 0.1102\n",
      "12/223, train_loss: 0.1097, step time: 0.1060\n",
      "13/223, train_loss: 0.0970, step time: 0.1216\n",
      "14/223, train_loss: 0.1037, step time: 0.0996\n",
      "15/223, train_loss: 0.1127, step time: 0.1016\n",
      "16/223, train_loss: 0.1146, step time: 0.1120\n",
      "17/223, train_loss: 0.0954, step time: 0.0999\n",
      "18/223, train_loss: 0.1093, step time: 0.0998\n",
      "19/223, train_loss: 0.1012, step time: 0.0996\n",
      "20/223, train_loss: 0.1084, step time: 0.1000\n",
      "21/223, train_loss: 0.0972, step time: 0.1146\n",
      "22/223, train_loss: 0.0927, step time: 0.1044\n",
      "23/223, train_loss: 0.1093, step time: 0.1125\n",
      "24/223, train_loss: 0.3093, step time: 0.1228\n",
      "25/223, train_loss: 0.1018, step time: 0.1138\n",
      "26/223, train_loss: 0.1037, step time: 0.1209\n",
      "27/223, train_loss: 0.1037, step time: 0.1169\n",
      "28/223, train_loss: 0.1068, step time: 0.1031\n",
      "29/223, train_loss: 0.0999, step time: 0.1043\n",
      "30/223, train_loss: 0.1103, step time: 0.1004\n",
      "31/223, train_loss: 0.1049, step time: 0.1016\n",
      "32/223, train_loss: 0.1073, step time: 0.1002\n",
      "33/223, train_loss: 0.1064, step time: 0.1004\n",
      "34/223, train_loss: 0.1036, step time: 0.1019\n",
      "35/223, train_loss: 0.0989, step time: 0.1007\n",
      "36/223, train_loss: 0.1020, step time: 0.1006\n",
      "37/223, train_loss: 0.1055, step time: 0.1032\n",
      "38/223, train_loss: 0.1034, step time: 0.1159\n",
      "39/223, train_loss: 0.1043, step time: 0.0999\n",
      "40/223, train_loss: 0.1186, step time: 0.1146\n",
      "41/223, train_loss: 0.1050, step time: 0.1190\n",
      "42/223, train_loss: 0.1051, step time: 0.1030\n",
      "43/223, train_loss: 0.0903, step time: 0.1007\n",
      "44/223, train_loss: 0.0937, step time: 0.1024\n",
      "45/223, train_loss: 0.1042, step time: 0.1056\n",
      "46/223, train_loss: 0.1034, step time: 0.0994\n",
      "47/223, train_loss: 0.1194, step time: 0.1002\n",
      "48/223, train_loss: 0.1091, step time: 0.1006\n",
      "49/223, train_loss: 0.0940, step time: 0.1108\n",
      "50/223, train_loss: 0.0999, step time: 0.1036\n",
      "51/223, train_loss: 0.0952, step time: 0.1071\n",
      "52/223, train_loss: 0.1018, step time: 0.1016\n",
      "53/223, train_loss: 0.1106, step time: 0.1108\n",
      "54/223, train_loss: 0.0977, step time: 0.1010\n",
      "55/223, train_loss: 0.1049, step time: 0.1010\n",
      "56/223, train_loss: 0.1069, step time: 0.1064\n",
      "57/223, train_loss: 0.0943, step time: 0.0999\n",
      "58/223, train_loss: 0.1086, step time: 0.1006\n",
      "59/223, train_loss: 0.1146, step time: 0.1004\n",
      "60/223, train_loss: 0.1016, step time: 0.1104\n",
      "61/223, train_loss: 0.1175, step time: 0.1001\n",
      "62/223, train_loss: 0.1018, step time: 0.1146\n",
      "63/223, train_loss: 0.1051, step time: 0.1074\n",
      "64/223, train_loss: 0.0991, step time: 0.1052\n",
      "65/223, train_loss: 0.0922, step time: 0.1253\n",
      "66/223, train_loss: 0.1022, step time: 0.1191\n",
      "67/223, train_loss: 0.1083, step time: 0.0982\n",
      "68/223, train_loss: 0.1030, step time: 0.0992\n",
      "69/223, train_loss: 0.1005, step time: 0.0985\n",
      "70/223, train_loss: 0.1087, step time: 0.1006\n",
      "71/223, train_loss: 0.0949, step time: 0.0996\n",
      "72/223, train_loss: 0.1018, step time: 0.1032\n",
      "73/223, train_loss: 0.1072, step time: 0.0991\n",
      "74/223, train_loss: 0.1213, step time: 0.1025\n",
      "75/223, train_loss: 0.0980, step time: 0.1384\n",
      "76/223, train_loss: 0.1043, step time: 0.1263\n",
      "77/223, train_loss: 0.1051, step time: 0.1308\n",
      "78/223, train_loss: 0.0965, step time: 0.1349\n",
      "79/223, train_loss: 0.0977, step time: 0.1091\n",
      "80/223, train_loss: 0.1049, step time: 0.0995\n",
      "81/223, train_loss: 0.1087, step time: 0.1201\n",
      "82/223, train_loss: 0.1126, step time: 0.1138\n",
      "83/223, train_loss: 0.0983, step time: 0.1078\n",
      "84/223, train_loss: 0.1019, step time: 0.1089\n",
      "85/223, train_loss: 0.1112, step time: 0.1105\n",
      "86/223, train_loss: 0.1002, step time: 0.1487\n",
      "87/223, train_loss: 0.0979, step time: 0.1256\n",
      "88/223, train_loss: 0.1104, step time: 0.1138\n",
      "89/223, train_loss: 0.1159, step time: 0.1003\n",
      "90/223, train_loss: 0.1144, step time: 0.1047\n",
      "91/223, train_loss: 0.0939, step time: 0.1011\n",
      "92/223, train_loss: 0.1047, step time: 0.1078\n",
      "93/223, train_loss: 0.0999, step time: 0.1071\n",
      "94/223, train_loss: 0.0984, step time: 0.1297\n",
      "95/223, train_loss: 0.1154, step time: 0.1101\n",
      "96/223, train_loss: 0.1183, step time: 0.1175\n",
      "97/223, train_loss: 0.0946, step time: 0.1182\n",
      "98/223, train_loss: 0.1038, step time: 0.0999\n",
      "99/223, train_loss: 0.0985, step time: 0.1015\n",
      "100/223, train_loss: 0.1173, step time: 0.1053\n",
      "101/223, train_loss: 0.1098, step time: 0.1007\n",
      "102/223, train_loss: 0.1066, step time: 0.1462\n",
      "103/223, train_loss: 0.0940, step time: 0.1095\n",
      "104/223, train_loss: 0.1033, step time: 0.1090\n",
      "105/223, train_loss: 0.0963, step time: 0.1065\n",
      "106/223, train_loss: 0.1027, step time: 0.1573\n",
      "107/223, train_loss: 0.0927, step time: 0.1175\n",
      "108/223, train_loss: 0.1075, step time: 0.1001\n",
      "109/223, train_loss: 0.1094, step time: 0.1003\n",
      "110/223, train_loss: 0.1034, step time: 0.1096\n",
      "111/223, train_loss: 0.1074, step time: 0.1329\n",
      "112/223, train_loss: 0.1069, step time: 0.1005\n",
      "113/223, train_loss: 0.0993, step time: 0.1002\n",
      "114/223, train_loss: 0.1048, step time: 0.1303\n",
      "115/223, train_loss: 0.1046, step time: 0.1053\n",
      "116/223, train_loss: 0.0960, step time: 0.1172\n",
      "117/223, train_loss: 0.0998, step time: 0.1198\n",
      "118/223, train_loss: 0.0973, step time: 0.1132\n",
      "119/223, train_loss: 0.1010, step time: 0.1114\n",
      "120/223, train_loss: 0.0973, step time: 0.1153\n",
      "121/223, train_loss: 0.1130, step time: 0.0996\n",
      "122/223, train_loss: 0.0980, step time: 0.1016\n",
      "123/223, train_loss: 0.1193, step time: 0.1242\n",
      "124/223, train_loss: 0.0953, step time: 0.1178\n",
      "125/223, train_loss: 0.1093, step time: 0.1007\n",
      "126/223, train_loss: 0.0999, step time: 0.1201\n",
      "127/223, train_loss: 0.1078, step time: 0.1008\n",
      "128/223, train_loss: 0.0884, step time: 0.1290\n",
      "129/223, train_loss: 0.0962, step time: 0.1000\n",
      "130/223, train_loss: 0.1030, step time: 0.1034\n",
      "131/223, train_loss: 0.1150, step time: 0.1257\n",
      "132/223, train_loss: 0.0964, step time: 0.1094\n",
      "133/223, train_loss: 0.1125, step time: 0.1257\n",
      "134/223, train_loss: 0.1095, step time: 0.1193\n",
      "135/223, train_loss: 0.1075, step time: 0.1130\n",
      "136/223, train_loss: 0.0988, step time: 0.1184\n",
      "137/223, train_loss: 0.0996, step time: 0.1447\n",
      "138/223, train_loss: 0.0944, step time: 0.1080\n",
      "139/223, train_loss: 0.1214, step time: 0.1326\n",
      "140/223, train_loss: 0.1119, step time: 0.1008\n",
      "141/223, train_loss: 0.0995, step time: 0.1018\n",
      "142/223, train_loss: 0.0982, step time: 0.1108\n",
      "143/223, train_loss: 0.1084, step time: 0.1092\n",
      "144/223, train_loss: 0.0959, step time: 0.1202\n",
      "145/223, train_loss: 0.0967, step time: 0.1114\n",
      "146/223, train_loss: 0.1032, step time: 0.1162\n",
      "147/223, train_loss: 0.1032, step time: 0.1303\n",
      "148/223, train_loss: 0.1071, step time: 0.1230\n",
      "149/223, train_loss: 0.0999, step time: 0.1003\n",
      "150/223, train_loss: 0.1010, step time: 0.1306\n",
      "151/223, train_loss: 0.0948, step time: 0.1001\n",
      "152/223, train_loss: 0.0965, step time: 0.1138\n",
      "153/223, train_loss: 0.0941, step time: 0.1171\n",
      "154/223, train_loss: 0.0944, step time: 0.1095\n",
      "155/223, train_loss: 0.1048, step time: 0.1242\n",
      "156/223, train_loss: 0.1173, step time: 0.1220\n",
      "157/223, train_loss: 0.1182, step time: 0.0989\n",
      "158/223, train_loss: 0.1102, step time: 0.0995\n",
      "159/223, train_loss: 0.1010, step time: 0.1017\n",
      "160/223, train_loss: 0.1056, step time: 0.1178\n",
      "161/223, train_loss: 0.0913, step time: 0.1005\n",
      "162/223, train_loss: 0.1016, step time: 0.1005\n",
      "163/223, train_loss: 0.0991, step time: 0.1011\n",
      "164/223, train_loss: 0.0956, step time: 0.1127\n",
      "165/223, train_loss: 0.0998, step time: 0.1116\n",
      "166/223, train_loss: 0.1099, step time: 0.1002\n",
      "167/223, train_loss: 0.0940, step time: 0.0994\n",
      "168/223, train_loss: 0.1056, step time: 0.0992\n",
      "169/223, train_loss: 0.0925, step time: 0.1103\n",
      "170/223, train_loss: 0.1034, step time: 0.1014\n",
      "171/223, train_loss: 0.0953, step time: 0.0994\n",
      "172/223, train_loss: 0.1072, step time: 0.1011\n",
      "173/223, train_loss: 0.1003, step time: 0.1112\n",
      "174/223, train_loss: 0.1057, step time: 0.1005\n",
      "175/223, train_loss: 0.0972, step time: 0.1001\n",
      "176/223, train_loss: 0.1082, step time: 0.1106\n",
      "177/223, train_loss: 0.1084, step time: 0.1202\n",
      "178/223, train_loss: 0.0930, step time: 0.1203\n",
      "179/223, train_loss: 0.0975, step time: 0.1317\n",
      "180/223, train_loss: 0.0989, step time: 0.1009\n",
      "181/223, train_loss: 0.1027, step time: 0.1100\n",
      "182/223, train_loss: 0.1078, step time: 0.1020\n",
      "183/223, train_loss: 0.1058, step time: 0.1009\n",
      "184/223, train_loss: 0.0983, step time: 0.1346\n",
      "185/223, train_loss: 0.0916, step time: 0.0999\n",
      "186/223, train_loss: 0.1053, step time: 0.1334\n",
      "187/223, train_loss: 0.0950, step time: 0.1493\n",
      "188/223, train_loss: 0.1102, step time: 0.1005\n",
      "189/223, train_loss: 0.0963, step time: 0.1015\n",
      "190/223, train_loss: 0.1000, step time: 0.1055\n",
      "191/223, train_loss: 0.1038, step time: 0.1123\n",
      "192/223, train_loss: 0.1002, step time: 0.1162\n",
      "193/223, train_loss: 0.1061, step time: 0.0998\n",
      "194/223, train_loss: 0.0905, step time: 0.1011\n",
      "195/223, train_loss: 0.0907, step time: 0.1005\n",
      "196/223, train_loss: 0.1140, step time: 0.1001\n",
      "197/223, train_loss: 0.1012, step time: 0.0996\n",
      "198/223, train_loss: 0.0979, step time: 0.1005\n",
      "199/223, train_loss: 0.0930, step time: 0.1006\n",
      "200/223, train_loss: 0.1017, step time: 0.1009\n",
      "201/223, train_loss: 0.1138, step time: 0.0998\n",
      "202/223, train_loss: 0.1061, step time: 0.1143\n",
      "203/223, train_loss: 0.1007, step time: 0.1000\n",
      "204/223, train_loss: 0.1184, step time: 0.0991\n",
      "205/223, train_loss: 0.1074, step time: 0.0993\n",
      "206/223, train_loss: 0.0981, step time: 0.1003\n",
      "207/223, train_loss: 0.1106, step time: 0.1007\n",
      "208/223, train_loss: 0.0963, step time: 0.1057\n",
      "209/223, train_loss: 0.1031, step time: 0.0995\n",
      "210/223, train_loss: 0.1019, step time: 0.1025\n",
      "211/223, train_loss: 0.0984, step time: 0.1004\n",
      "212/223, train_loss: 0.0990, step time: 0.1152\n",
      "213/223, train_loss: 0.1165, step time: 0.1354\n",
      "214/223, train_loss: 0.1035, step time: 0.1553\n",
      "215/223, train_loss: 0.1100, step time: 0.1058\n",
      "216/223, train_loss: 0.0997, step time: 0.1093\n",
      "217/223, train_loss: 0.1058, step time: 0.1004\n",
      "218/223, train_loss: 0.1211, step time: 0.1001\n",
      "219/223, train_loss: 0.1257, step time: 0.1002\n",
      "220/223, train_loss: 0.1077, step time: 0.1001\n",
      "221/223, train_loss: 0.1115, step time: 0.0997\n",
      "222/223, train_loss: 0.0950, step time: 0.0997\n",
      "223/223, train_loss: 0.1008, step time: 0.0998\n",
      "epoch 176 average loss: 0.1043\n",
      "time consuming of epoch 176 is: 94.2874\n",
      "----------\n",
      "epoch 177/300\n",
      "1/223, train_loss: 0.0998, step time: 0.1060\n",
      "2/223, train_loss: 0.1014, step time: 0.1005\n",
      "3/223, train_loss: 0.1038, step time: 0.1005\n",
      "4/223, train_loss: 0.1016, step time: 0.1091\n",
      "5/223, train_loss: 0.1046, step time: 0.1135\n",
      "6/223, train_loss: 0.0940, step time: 0.1007\n",
      "7/223, train_loss: 0.0969, step time: 0.1009\n",
      "8/223, train_loss: 0.1018, step time: 0.1007\n",
      "9/223, train_loss: 0.0981, step time: 0.1245\n",
      "10/223, train_loss: 0.1054, step time: 0.1484\n",
      "11/223, train_loss: 0.1082, step time: 0.1577\n",
      "12/223, train_loss: 0.0915, step time: 0.1161\n",
      "13/223, train_loss: 0.1063, step time: 0.1001\n",
      "14/223, train_loss: 0.0953, step time: 0.1174\n",
      "15/223, train_loss: 0.1115, step time: 0.1135\n",
      "16/223, train_loss: 0.1044, step time: 0.1081\n",
      "17/223, train_loss: 0.0970, step time: 0.1012\n",
      "18/223, train_loss: 0.1010, step time: 0.1030\n",
      "19/223, train_loss: 0.1123, step time: 0.1107\n",
      "20/223, train_loss: 0.0917, step time: 0.1020\n",
      "21/223, train_loss: 0.1034, step time: 0.1024\n",
      "22/223, train_loss: 0.0984, step time: 0.1050\n",
      "23/223, train_loss: 0.1081, step time: 0.0998\n",
      "24/223, train_loss: 0.0990, step time: 0.1078\n",
      "25/223, train_loss: 0.1136, step time: 0.1060\n",
      "26/223, train_loss: 0.0889, step time: 0.1126\n",
      "27/223, train_loss: 0.0990, step time: 0.1020\n",
      "28/223, train_loss: 0.0964, step time: 0.1029\n",
      "29/223, train_loss: 0.1152, step time: 0.1015\n",
      "30/223, train_loss: 0.0994, step time: 0.1003\n",
      "31/223, train_loss: 0.1130, step time: 0.1026\n",
      "32/223, train_loss: 0.1065, step time: 0.1344\n",
      "33/223, train_loss: 0.0974, step time: 0.1056\n",
      "34/223, train_loss: 0.0984, step time: 0.1157\n",
      "35/223, train_loss: 0.0914, step time: 0.1001\n",
      "36/223, train_loss: 0.1000, step time: 0.1284\n",
      "37/223, train_loss: 0.1029, step time: 0.1060\n",
      "38/223, train_loss: 0.1088, step time: 0.1050\n",
      "39/223, train_loss: 0.1220, step time: 0.1000\n",
      "40/223, train_loss: 0.1060, step time: 0.1014\n",
      "41/223, train_loss: 0.1212, step time: 0.1005\n",
      "42/223, train_loss: 0.1018, step time: 0.1135\n",
      "43/223, train_loss: 0.1022, step time: 0.1006\n",
      "44/223, train_loss: 0.1059, step time: 0.1258\n",
      "45/223, train_loss: 0.1104, step time: 0.1003\n",
      "46/223, train_loss: 0.1137, step time: 0.1032\n",
      "47/223, train_loss: 0.1176, step time: 0.1001\n",
      "48/223, train_loss: 0.1073, step time: 0.1004\n",
      "49/223, train_loss: 0.1047, step time: 0.1155\n",
      "50/223, train_loss: 0.0998, step time: 0.1067\n",
      "51/223, train_loss: 0.0967, step time: 0.0994\n",
      "52/223, train_loss: 0.1043, step time: 0.1001\n",
      "53/223, train_loss: 0.1017, step time: 0.1078\n",
      "54/223, train_loss: 0.0948, step time: 0.1171\n",
      "55/223, train_loss: 0.1092, step time: 0.1052\n",
      "56/223, train_loss: 0.1176, step time: 0.1008\n",
      "57/223, train_loss: 0.1129, step time: 0.1019\n",
      "58/223, train_loss: 0.0977, step time: 0.1060\n",
      "59/223, train_loss: 0.1056, step time: 0.1000\n",
      "60/223, train_loss: 0.1029, step time: 0.0993\n",
      "61/223, train_loss: 0.0973, step time: 0.1095\n",
      "62/223, train_loss: 0.1074, step time: 0.1108\n",
      "63/223, train_loss: 0.1051, step time: 0.1244\n",
      "64/223, train_loss: 0.1087, step time: 0.1179\n",
      "65/223, train_loss: 0.1157, step time: 0.1101\n",
      "66/223, train_loss: 0.1004, step time: 0.1035\n",
      "67/223, train_loss: 0.1004, step time: 0.0998\n",
      "68/223, train_loss: 0.0970, step time: 0.1522\n",
      "69/223, train_loss: 0.0919, step time: 0.1293\n",
      "70/223, train_loss: 0.0850, step time: 0.1083\n",
      "71/223, train_loss: 0.1037, step time: 0.1070\n",
      "72/223, train_loss: 0.1005, step time: 0.1017\n",
      "73/223, train_loss: 0.0965, step time: 0.1067\n",
      "74/223, train_loss: 0.1020, step time: 0.1015\n",
      "75/223, train_loss: 0.1139, step time: 0.1002\n",
      "76/223, train_loss: 0.1007, step time: 0.1005\n",
      "77/223, train_loss: 0.0916, step time: 0.1132\n",
      "78/223, train_loss: 0.1020, step time: 0.1008\n",
      "79/223, train_loss: 0.1119, step time: 0.1014\n",
      "80/223, train_loss: 0.0923, step time: 0.1001\n",
      "81/223, train_loss: 0.0916, step time: 0.1197\n",
      "82/223, train_loss: 0.0943, step time: 0.0999\n",
      "83/223, train_loss: 0.1005, step time: 0.1003\n",
      "84/223, train_loss: 0.1124, step time: 0.1009\n",
      "85/223, train_loss: 0.1024, step time: 0.1076\n",
      "86/223, train_loss: 0.0906, step time: 0.1005\n",
      "87/223, train_loss: 0.1085, step time: 0.1002\n",
      "88/223, train_loss: 0.1018, step time: 0.1239\n",
      "89/223, train_loss: 0.0923, step time: 0.1083\n",
      "90/223, train_loss: 0.0932, step time: 0.1182\n",
      "91/223, train_loss: 0.1113, step time: 0.1121\n",
      "92/223, train_loss: 0.0987, step time: 0.1130\n",
      "93/223, train_loss: 0.0988, step time: 0.1219\n",
      "94/223, train_loss: 0.0946, step time: 0.1096\n",
      "95/223, train_loss: 0.1013, step time: 0.1023\n",
      "96/223, train_loss: 0.3043, step time: 0.1011\n",
      "97/223, train_loss: 0.0989, step time: 0.1054\n",
      "98/223, train_loss: 0.0995, step time: 0.1022\n",
      "99/223, train_loss: 0.1060, step time: 0.0996\n",
      "100/223, train_loss: 0.0965, step time: 0.1053\n",
      "101/223, train_loss: 0.1137, step time: 0.1125\n",
      "102/223, train_loss: 0.1126, step time: 0.1044\n",
      "103/223, train_loss: 0.1055, step time: 0.1140\n",
      "104/223, train_loss: 0.1100, step time: 0.1012\n",
      "105/223, train_loss: 0.1175, step time: 0.1049\n",
      "106/223, train_loss: 0.1105, step time: 0.1081\n",
      "107/223, train_loss: 0.0933, step time: 0.1000\n",
      "108/223, train_loss: 0.1092, step time: 0.1040\n",
      "109/223, train_loss: 0.1079, step time: 0.1089\n",
      "110/223, train_loss: 0.1183, step time: 0.1225\n",
      "111/223, train_loss: 0.1031, step time: 0.1124\n",
      "112/223, train_loss: 0.1015, step time: 0.1088\n",
      "113/223, train_loss: 0.0966, step time: 0.1321\n",
      "114/223, train_loss: 0.1032, step time: 0.0997\n",
      "115/223, train_loss: 0.1117, step time: 0.1070\n",
      "116/223, train_loss: 0.1110, step time: 0.1095\n",
      "117/223, train_loss: 0.1074, step time: 0.1010\n",
      "118/223, train_loss: 0.1008, step time: 0.0999\n",
      "119/223, train_loss: 0.1005, step time: 0.1110\n",
      "120/223, train_loss: 0.1034, step time: 0.1011\n",
      "121/223, train_loss: 0.0957, step time: 0.1000\n",
      "122/223, train_loss: 0.1018, step time: 0.0996\n",
      "123/223, train_loss: 0.1016, step time: 0.1011\n",
      "124/223, train_loss: 0.1124, step time: 0.1008\n",
      "125/223, train_loss: 0.1043, step time: 0.0997\n",
      "126/223, train_loss: 0.1108, step time: 0.1001\n",
      "127/223, train_loss: 0.1105, step time: 0.0995\n",
      "128/223, train_loss: 0.0986, step time: 0.1011\n",
      "129/223, train_loss: 0.1171, step time: 0.1002\n",
      "130/223, train_loss: 0.1033, step time: 0.1010\n",
      "131/223, train_loss: 0.1047, step time: 0.1132\n",
      "132/223, train_loss: 0.0991, step time: 0.1228\n",
      "133/223, train_loss: 0.0962, step time: 0.0995\n",
      "134/223, train_loss: 0.0961, step time: 0.1209\n",
      "135/223, train_loss: 0.1036, step time: 0.1169\n",
      "136/223, train_loss: 0.1024, step time: 0.1145\n",
      "137/223, train_loss: 0.0999, step time: 0.0998\n",
      "138/223, train_loss: 0.0997, step time: 0.1264\n",
      "139/223, train_loss: 0.1077, step time: 0.1004\n",
      "140/223, train_loss: 0.0922, step time: 0.1008\n",
      "141/223, train_loss: 0.0980, step time: 0.1167\n",
      "142/223, train_loss: 0.1048, step time: 0.1037\n",
      "143/223, train_loss: 0.0972, step time: 0.1061\n",
      "144/223, train_loss: 0.1013, step time: 0.1137\n",
      "145/223, train_loss: 0.0917, step time: 0.1184\n",
      "146/223, train_loss: 0.1097, step time: 0.1242\n",
      "147/223, train_loss: 0.0920, step time: 0.1496\n",
      "148/223, train_loss: 0.1115, step time: 0.1045\n",
      "149/223, train_loss: 0.1121, step time: 0.1145\n",
      "150/223, train_loss: 0.1016, step time: 0.1076\n",
      "151/223, train_loss: 0.1015, step time: 0.1041\n",
      "152/223, train_loss: 0.1027, step time: 0.1295\n",
      "153/223, train_loss: 0.1200, step time: 0.1146\n",
      "154/223, train_loss: 0.1080, step time: 0.1007\n",
      "155/223, train_loss: 0.1042, step time: 0.1145\n",
      "156/223, train_loss: 0.1065, step time: 0.1002\n",
      "157/223, train_loss: 0.1050, step time: 0.1036\n",
      "158/223, train_loss: 0.1099, step time: 0.1175\n",
      "159/223, train_loss: 0.1028, step time: 0.1219\n",
      "160/223, train_loss: 0.1141, step time: 0.1005\n",
      "161/223, train_loss: 0.1083, step time: 0.1348\n",
      "162/223, train_loss: 0.1059, step time: 0.1121\n",
      "163/223, train_loss: 0.1074, step time: 0.1166\n",
      "164/223, train_loss: 0.0962, step time: 0.1167\n",
      "165/223, train_loss: 0.1099, step time: 0.1638\n",
      "166/223, train_loss: 0.1010, step time: 0.1002\n",
      "167/223, train_loss: 0.1094, step time: 0.1236\n",
      "168/223, train_loss: 0.1012, step time: 0.1041\n",
      "169/223, train_loss: 0.1022, step time: 0.1125\n",
      "170/223, train_loss: 0.0969, step time: 0.1063\n",
      "171/223, train_loss: 0.0974, step time: 0.1181\n",
      "172/223, train_loss: 0.1176, step time: 0.1004\n",
      "173/223, train_loss: 0.0977, step time: 0.1154\n",
      "174/223, train_loss: 0.1110, step time: 0.1104\n",
      "175/223, train_loss: 0.0992, step time: 0.1307\n",
      "176/223, train_loss: 0.1053, step time: 0.1086\n",
      "177/223, train_loss: 0.1018, step time: 0.1154\n",
      "178/223, train_loss: 0.0985, step time: 0.1107\n",
      "179/223, train_loss: 0.0988, step time: 0.1134\n",
      "180/223, train_loss: 0.1045, step time: 0.1098\n",
      "181/223, train_loss: 0.1077, step time: 0.1205\n",
      "182/223, train_loss: 0.1130, step time: 0.1113\n",
      "183/223, train_loss: 0.1117, step time: 0.1080\n",
      "184/223, train_loss: 0.1081, step time: 0.1191\n",
      "185/223, train_loss: 0.1118, step time: 0.1196\n",
      "186/223, train_loss: 0.1005, step time: 0.1091\n",
      "187/223, train_loss: 0.0899, step time: 0.1195\n",
      "188/223, train_loss: 0.1285, step time: 0.1186\n",
      "189/223, train_loss: 0.1019, step time: 0.1017\n",
      "190/223, train_loss: 0.0989, step time: 0.1103\n",
      "191/223, train_loss: 0.1058, step time: 0.1233\n",
      "192/223, train_loss: 0.1077, step time: 0.1020\n",
      "193/223, train_loss: 0.1045, step time: 0.1154\n",
      "194/223, train_loss: 0.1003, step time: 0.1059\n",
      "195/223, train_loss: 0.0979, step time: 0.1236\n",
      "196/223, train_loss: 0.1106, step time: 0.1134\n",
      "197/223, train_loss: 0.1134, step time: 0.1064\n",
      "198/223, train_loss: 0.1055, step time: 0.1010\n",
      "199/223, train_loss: 0.1056, step time: 0.1097\n",
      "200/223, train_loss: 0.1112, step time: 0.1194\n",
      "201/223, train_loss: 0.0929, step time: 0.1137\n",
      "202/223, train_loss: 0.1131, step time: 0.0992\n",
      "203/223, train_loss: 0.0933, step time: 0.0999\n",
      "204/223, train_loss: 0.1137, step time: 0.1058\n",
      "205/223, train_loss: 0.0931, step time: 0.1115\n",
      "206/223, train_loss: 0.0914, step time: 0.1258\n",
      "207/223, train_loss: 0.0993, step time: 0.1003\n",
      "208/223, train_loss: 0.1055, step time: 0.1010\n",
      "209/223, train_loss: 0.0977, step time: 0.1182\n",
      "210/223, train_loss: 0.1015, step time: 0.1053\n",
      "211/223, train_loss: 0.1138, step time: 0.1009\n",
      "212/223, train_loss: 0.1009, step time: 0.1018\n",
      "213/223, train_loss: 0.1149, step time: 0.1000\n",
      "214/223, train_loss: 0.1123, step time: 0.1026\n",
      "215/223, train_loss: 0.1021, step time: 0.1081\n",
      "216/223, train_loss: 0.0989, step time: 0.1206\n",
      "217/223, train_loss: 0.0952, step time: 0.0994\n",
      "218/223, train_loss: 0.1086, step time: 0.1012\n",
      "219/223, train_loss: 0.1137, step time: 0.0999\n",
      "220/223, train_loss: 0.1179, step time: 0.1005\n",
      "221/223, train_loss: 0.1030, step time: 0.1002\n",
      "222/223, train_loss: 0.1098, step time: 0.1032\n",
      "223/223, train_loss: 0.0999, step time: 0.1004\n",
      "epoch 177 average loss: 0.1046\n",
      "time consuming of epoch 177 is: 91.3922\n",
      "----------\n",
      "epoch 178/300\n",
      "1/223, train_loss: 0.0918, step time: 0.1048\n",
      "2/223, train_loss: 0.1042, step time: 0.1000\n",
      "3/223, train_loss: 0.1130, step time: 0.1009\n",
      "4/223, train_loss: 0.1027, step time: 0.1004\n",
      "5/223, train_loss: 0.1057, step time: 0.1151\n",
      "6/223, train_loss: 0.1114, step time: 0.1060\n",
      "7/223, train_loss: 0.1155, step time: 0.1087\n",
      "8/223, train_loss: 0.1097, step time: 0.1205\n",
      "9/223, train_loss: 0.1048, step time: 0.1213\n",
      "10/223, train_loss: 0.1030, step time: 0.1396\n",
      "11/223, train_loss: 0.1065, step time: 0.1005\n",
      "12/223, train_loss: 0.0957, step time: 0.1155\n",
      "13/223, train_loss: 0.1038, step time: 0.1002\n",
      "14/223, train_loss: 0.0927, step time: 0.1024\n",
      "15/223, train_loss: 0.0972, step time: 0.1342\n",
      "16/223, train_loss: 0.1037, step time: 0.1139\n",
      "17/223, train_loss: 0.1020, step time: 0.1003\n",
      "18/223, train_loss: 0.0976, step time: 0.1006\n",
      "19/223, train_loss: 0.1155, step time: 0.1122\n",
      "20/223, train_loss: 0.1015, step time: 0.1225\n",
      "21/223, train_loss: 0.1192, step time: 0.1119\n",
      "22/223, train_loss: 0.1215, step time: 0.1279\n",
      "23/223, train_loss: 0.0978, step time: 0.1135\n",
      "24/223, train_loss: 0.1063, step time: 0.1050\n",
      "25/223, train_loss: 0.1066, step time: 0.1188\n",
      "26/223, train_loss: 0.1029, step time: 0.1220\n",
      "27/223, train_loss: 0.1065, step time: 0.1132\n",
      "28/223, train_loss: 0.1050, step time: 0.1007\n",
      "29/223, train_loss: 0.1178, step time: 0.1253\n",
      "30/223, train_loss: 0.1033, step time: 0.1053\n",
      "31/223, train_loss: 0.0949, step time: 0.1193\n",
      "32/223, train_loss: 0.1195, step time: 0.1001\n",
      "33/223, train_loss: 0.1097, step time: 0.1071\n",
      "34/223, train_loss: 0.1283, step time: 0.1025\n",
      "35/223, train_loss: 0.1154, step time: 0.1046\n",
      "36/223, train_loss: 0.1087, step time: 0.1008\n",
      "37/223, train_loss: 0.0944, step time: 0.1131\n",
      "38/223, train_loss: 0.1045, step time: 0.1060\n",
      "39/223, train_loss: 0.1076, step time: 0.1050\n",
      "40/223, train_loss: 0.1003, step time: 0.1166\n",
      "41/223, train_loss: 0.1048, step time: 0.1162\n",
      "42/223, train_loss: 0.1015, step time: 0.1133\n",
      "43/223, train_loss: 0.1105, step time: 0.1126\n",
      "44/223, train_loss: 0.0957, step time: 0.1005\n",
      "45/223, train_loss: 0.1051, step time: 0.1001\n",
      "46/223, train_loss: 0.1090, step time: 0.1008\n",
      "47/223, train_loss: 0.1020, step time: 0.1037\n",
      "48/223, train_loss: 0.1092, step time: 0.1011\n",
      "49/223, train_loss: 0.0985, step time: 0.1105\n",
      "50/223, train_loss: 0.0975, step time: 0.1078\n",
      "51/223, train_loss: 0.0945, step time: 0.1007\n",
      "52/223, train_loss: 0.1036, step time: 0.1008\n",
      "53/223, train_loss: 0.1100, step time: 0.1008\n",
      "54/223, train_loss: 0.1047, step time: 0.1007\n",
      "55/223, train_loss: 0.1158, step time: 0.1135\n",
      "56/223, train_loss: 0.1055, step time: 0.1169\n",
      "57/223, train_loss: 0.1002, step time: 0.1138\n",
      "58/223, train_loss: 0.1046, step time: 0.0993\n",
      "59/223, train_loss: 0.1180, step time: 0.1085\n",
      "60/223, train_loss: 0.1081, step time: 0.1004\n",
      "61/223, train_loss: 0.0961, step time: 0.0999\n",
      "62/223, train_loss: 0.0978, step time: 0.0998\n",
      "63/223, train_loss: 0.1008, step time: 0.1046\n",
      "64/223, train_loss: 0.0936, step time: 0.1006\n",
      "65/223, train_loss: 0.1015, step time: 0.1042\n",
      "66/223, train_loss: 0.1053, step time: 0.1018\n",
      "67/223, train_loss: 0.1129, step time: 0.1044\n",
      "68/223, train_loss: 0.1005, step time: 0.1008\n",
      "69/223, train_loss: 0.1118, step time: 0.1004\n",
      "70/223, train_loss: 0.1034, step time: 0.1008\n",
      "71/223, train_loss: 0.1097, step time: 0.1008\n",
      "72/223, train_loss: 0.0961, step time: 0.1111\n",
      "73/223, train_loss: 0.1000, step time: 0.1066\n",
      "74/223, train_loss: 0.0940, step time: 0.1016\n",
      "75/223, train_loss: 0.1139, step time: 0.1061\n",
      "76/223, train_loss: 0.1038, step time: 0.1012\n",
      "77/223, train_loss: 0.0969, step time: 0.0998\n",
      "78/223, train_loss: 0.0939, step time: 0.1010\n",
      "79/223, train_loss: 0.1016, step time: 0.1127\n",
      "80/223, train_loss: 0.1012, step time: 0.1075\n",
      "81/223, train_loss: 0.0912, step time: 0.1002\n",
      "82/223, train_loss: 0.1030, step time: 0.0998\n",
      "83/223, train_loss: 0.1079, step time: 0.1010\n",
      "84/223, train_loss: 0.0931, step time: 0.1170\n",
      "85/223, train_loss: 0.1055, step time: 0.1025\n",
      "86/223, train_loss: 0.0975, step time: 0.1011\n",
      "87/223, train_loss: 0.0985, step time: 0.1096\n",
      "88/223, train_loss: 0.1053, step time: 0.1068\n",
      "89/223, train_loss: 0.0987, step time: 0.1143\n",
      "90/223, train_loss: 0.1175, step time: 0.1016\n",
      "91/223, train_loss: 0.1014, step time: 0.1007\n",
      "92/223, train_loss: 0.0964, step time: 0.1284\n",
      "93/223, train_loss: 0.1023, step time: 0.1176\n",
      "94/223, train_loss: 0.0989, step time: 0.1108\n",
      "95/223, train_loss: 0.1105, step time: 0.1059\n",
      "96/223, train_loss: 0.0936, step time: 0.1013\n",
      "97/223, train_loss: 0.0981, step time: 0.1000\n",
      "98/223, train_loss: 0.1039, step time: 0.1008\n",
      "99/223, train_loss: 0.1061, step time: 0.1289\n",
      "100/223, train_loss: 0.0992, step time: 0.1160\n",
      "101/223, train_loss: 0.1094, step time: 0.1006\n",
      "102/223, train_loss: 0.1026, step time: 0.1041\n",
      "103/223, train_loss: 0.1048, step time: 0.1064\n",
      "104/223, train_loss: 0.1066, step time: 0.1089\n",
      "105/223, train_loss: 0.0984, step time: 0.1175\n",
      "106/223, train_loss: 0.0949, step time: 0.1117\n",
      "107/223, train_loss: 0.0965, step time: 0.1236\n",
      "108/223, train_loss: 0.1015, step time: 0.1414\n",
      "109/223, train_loss: 0.1102, step time: 0.1118\n",
      "110/223, train_loss: 0.0967, step time: 0.1118\n",
      "111/223, train_loss: 0.0970, step time: 0.1150\n",
      "112/223, train_loss: 0.0921, step time: 0.1178\n",
      "113/223, train_loss: 0.0991, step time: 0.1005\n",
      "114/223, train_loss: 0.0964, step time: 0.1008\n",
      "115/223, train_loss: 0.0936, step time: 0.1195\n",
      "116/223, train_loss: 0.1105, step time: 0.1099\n",
      "117/223, train_loss: 0.1037, step time: 0.1089\n",
      "118/223, train_loss: 0.1163, step time: 0.1146\n",
      "119/223, train_loss: 0.1203, step time: 0.1046\n",
      "120/223, train_loss: 0.0975, step time: 0.1292\n",
      "121/223, train_loss: 0.1062, step time: 0.1147\n",
      "122/223, train_loss: 0.1072, step time: 0.1048\n",
      "123/223, train_loss: 0.1069, step time: 0.0993\n",
      "124/223, train_loss: 0.1160, step time: 0.1003\n",
      "125/223, train_loss: 0.0931, step time: 0.1057\n",
      "126/223, train_loss: 0.0956, step time: 0.1002\n",
      "127/223, train_loss: 0.0993, step time: 0.1010\n",
      "128/223, train_loss: 0.1109, step time: 0.1038\n",
      "129/223, train_loss: 0.1016, step time: 0.1126\n",
      "130/223, train_loss: 0.1061, step time: 0.1008\n",
      "131/223, train_loss: 0.1031, step time: 0.1176\n",
      "132/223, train_loss: 0.0963, step time: 0.1070\n",
      "133/223, train_loss: 0.1083, step time: 0.1134\n",
      "134/223, train_loss: 0.0986, step time: 0.1006\n",
      "135/223, train_loss: 0.1034, step time: 0.1112\n",
      "136/223, train_loss: 0.0981, step time: 0.1116\n",
      "137/223, train_loss: 0.0965, step time: 0.1041\n",
      "138/223, train_loss: 0.1034, step time: 0.1059\n",
      "139/223, train_loss: 0.1068, step time: 0.1063\n",
      "140/223, train_loss: 0.0976, step time: 0.1043\n",
      "141/223, train_loss: 0.1112, step time: 0.1028\n",
      "142/223, train_loss: 0.1185, step time: 0.1011\n",
      "143/223, train_loss: 0.0992, step time: 0.1055\n",
      "144/223, train_loss: 0.1163, step time: 0.1000\n",
      "145/223, train_loss: 0.1071, step time: 0.1159\n",
      "146/223, train_loss: 0.1010, step time: 0.1004\n",
      "147/223, train_loss: 0.0975, step time: 0.1054\n",
      "148/223, train_loss: 0.1014, step time: 0.1006\n",
      "149/223, train_loss: 0.1028, step time: 0.1100\n",
      "150/223, train_loss: 0.1051, step time: 0.1004\n",
      "151/223, train_loss: 0.1068, step time: 0.1165\n",
      "152/223, train_loss: 0.1015, step time: 0.1211\n",
      "153/223, train_loss: 0.0948, step time: 0.1014\n",
      "154/223, train_loss: 0.1032, step time: 0.1160\n",
      "155/223, train_loss: 0.0932, step time: 0.1055\n",
      "156/223, train_loss: 0.0995, step time: 0.1182\n",
      "157/223, train_loss: 0.1091, step time: 0.1001\n",
      "158/223, train_loss: 0.1060, step time: 0.1061\n",
      "159/223, train_loss: 0.0970, step time: 0.1100\n",
      "160/223, train_loss: 0.1095, step time: 0.0987\n",
      "161/223, train_loss: 0.0969, step time: 0.1003\n",
      "162/223, train_loss: 0.0952, step time: 0.1119\n",
      "163/223, train_loss: 0.1082, step time: 0.1117\n",
      "164/223, train_loss: 0.1010, step time: 0.1100\n",
      "165/223, train_loss: 0.1064, step time: 0.1105\n",
      "166/223, train_loss: 0.0931, step time: 0.1009\n",
      "167/223, train_loss: 0.1188, step time: 0.1186\n",
      "168/223, train_loss: 0.0975, step time: 0.1123\n",
      "169/223, train_loss: 0.0883, step time: 0.1013\n",
      "170/223, train_loss: 0.1017, step time: 0.1002\n",
      "171/223, train_loss: 0.0996, step time: 0.1190\n",
      "172/223, train_loss: 0.0944, step time: 0.1390\n",
      "173/223, train_loss: 0.1024, step time: 0.1097\n",
      "174/223, train_loss: 0.1047, step time: 0.1123\n",
      "175/223, train_loss: 0.1074, step time: 0.1197\n",
      "176/223, train_loss: 0.0926, step time: 0.1005\n",
      "177/223, train_loss: 0.0910, step time: 0.1074\n",
      "178/223, train_loss: 0.1043, step time: 0.1000\n",
      "179/223, train_loss: 0.1024, step time: 0.1112\n",
      "180/223, train_loss: 0.1100, step time: 0.1026\n",
      "181/223, train_loss: 0.1178, step time: 0.1102\n",
      "182/223, train_loss: 0.0991, step time: 0.1043\n",
      "183/223, train_loss: 0.1023, step time: 0.1128\n",
      "184/223, train_loss: 0.1083, step time: 0.1129\n",
      "185/223, train_loss: 0.1057, step time: 0.1045\n",
      "186/223, train_loss: 0.1071, step time: 0.1191\n",
      "187/223, train_loss: 0.0951, step time: 0.1312\n",
      "188/223, train_loss: 0.1004, step time: 0.1090\n",
      "189/223, train_loss: 0.1038, step time: 0.1055\n",
      "190/223, train_loss: 0.0957, step time: 0.1220\n",
      "191/223, train_loss: 0.1002, step time: 0.1095\n",
      "192/223, train_loss: 0.0938, step time: 0.1128\n",
      "193/223, train_loss: 0.0965, step time: 0.1183\n",
      "194/223, train_loss: 0.1059, step time: 0.1178\n",
      "195/223, train_loss: 0.1095, step time: 0.1172\n",
      "196/223, train_loss: 0.1204, step time: 0.1179\n",
      "197/223, train_loss: 0.1037, step time: 0.1008\n",
      "198/223, train_loss: 0.1147, step time: 0.1003\n",
      "199/223, train_loss: 0.1047, step time: 0.1063\n",
      "200/223, train_loss: 0.1043, step time: 0.1147\n",
      "201/223, train_loss: 0.1021, step time: 0.1220\n",
      "202/223, train_loss: 0.1036, step time: 0.0998\n",
      "203/223, train_loss: 0.0987, step time: 0.1181\n",
      "204/223, train_loss: 0.2975, step time: 0.1191\n",
      "205/223, train_loss: 0.1031, step time: 0.1118\n",
      "206/223, train_loss: 0.1126, step time: 0.1329\n",
      "207/223, train_loss: 0.0960, step time: 0.1096\n",
      "208/223, train_loss: 0.1037, step time: 0.1156\n",
      "209/223, train_loss: 0.0949, step time: 0.1295\n",
      "210/223, train_loss: 0.0929, step time: 0.1361\n",
      "211/223, train_loss: 0.1128, step time: 0.1057\n",
      "212/223, train_loss: 0.1092, step time: 0.1012\n",
      "213/223, train_loss: 0.1127, step time: 0.1094\n",
      "214/223, train_loss: 0.1119, step time: 0.1037\n",
      "215/223, train_loss: 0.0974, step time: 0.1131\n",
      "216/223, train_loss: 0.1203, step time: 0.1065\n",
      "217/223, train_loss: 0.0961, step time: 0.1012\n",
      "218/223, train_loss: 0.1094, step time: 0.1184\n",
      "219/223, train_loss: 0.1051, step time: 0.1156\n",
      "220/223, train_loss: 0.1030, step time: 0.1006\n",
      "221/223, train_loss: 0.1000, step time: 0.1002\n",
      "222/223, train_loss: 0.0959, step time: 0.0996\n",
      "223/223, train_loss: 0.1037, step time: 0.0992\n",
      "epoch 178 average loss: 0.1044\n",
      "time consuming of epoch 178 is: 85.6605\n",
      "----------\n",
      "epoch 179/300\n",
      "1/223, train_loss: 0.0993, step time: 0.1099\n",
      "2/223, train_loss: 0.0967, step time: 0.1150\n",
      "3/223, train_loss: 0.1009, step time: 0.1008\n",
      "4/223, train_loss: 0.0993, step time: 0.1014\n",
      "5/223, train_loss: 0.1049, step time: 0.1083\n",
      "6/223, train_loss: 0.1083, step time: 0.1049\n",
      "7/223, train_loss: 0.1097, step time: 0.1273\n",
      "8/223, train_loss: 0.1046, step time: 0.1143\n",
      "9/223, train_loss: 0.1153, step time: 0.1079\n",
      "10/223, train_loss: 0.1017, step time: 0.1014\n",
      "11/223, train_loss: 0.0968, step time: 0.1219\n",
      "12/223, train_loss: 0.0981, step time: 0.1165\n",
      "13/223, train_loss: 0.1014, step time: 0.1000\n",
      "14/223, train_loss: 0.1040, step time: 0.1004\n",
      "15/223, train_loss: 0.0976, step time: 0.1190\n",
      "16/223, train_loss: 0.0995, step time: 0.1329\n",
      "17/223, train_loss: 0.1062, step time: 0.1066\n",
      "18/223, train_loss: 0.1123, step time: 0.1104\n",
      "19/223, train_loss: 0.1035, step time: 0.1099\n",
      "20/223, train_loss: 0.1110, step time: 0.1258\n",
      "21/223, train_loss: 0.0984, step time: 0.1178\n",
      "22/223, train_loss: 0.1151, step time: 0.1196\n",
      "23/223, train_loss: 0.0953, step time: 0.1273\n",
      "24/223, train_loss: 0.1021, step time: 0.1153\n",
      "25/223, train_loss: 0.1054, step time: 0.1297\n",
      "26/223, train_loss: 0.1011, step time: 0.1106\n",
      "27/223, train_loss: 0.1049, step time: 0.1195\n",
      "28/223, train_loss: 0.1022, step time: 0.1132\n",
      "29/223, train_loss: 0.1140, step time: 0.1372\n",
      "30/223, train_loss: 0.0980, step time: 0.1389\n",
      "31/223, train_loss: 0.1124, step time: 0.1425\n",
      "32/223, train_loss: 0.1077, step time: 0.1003\n",
      "33/223, train_loss: 0.1158, step time: 0.1073\n",
      "34/223, train_loss: 0.1138, step time: 0.1525\n",
      "35/223, train_loss: 0.1054, step time: 0.1002\n",
      "36/223, train_loss: 0.1056, step time: 0.1002\n",
      "37/223, train_loss: 0.1084, step time: 0.1217\n",
      "38/223, train_loss: 0.1043, step time: 0.1214\n",
      "39/223, train_loss: 0.1017, step time: 0.1429\n",
      "40/223, train_loss: 0.1051, step time: 0.1216\n",
      "41/223, train_loss: 0.1160, step time: 0.1265\n",
      "42/223, train_loss: 0.1086, step time: 0.1152\n",
      "43/223, train_loss: 0.1013, step time: 0.1123\n",
      "44/223, train_loss: 0.0929, step time: 0.1122\n",
      "45/223, train_loss: 0.0979, step time: 0.1060\n",
      "46/223, train_loss: 0.0915, step time: 0.1105\n",
      "47/223, train_loss: 0.0962, step time: 0.1362\n",
      "48/223, train_loss: 0.1068, step time: 0.1045\n",
      "49/223, train_loss: 0.1080, step time: 0.1247\n",
      "50/223, train_loss: 0.1119, step time: 0.1266\n",
      "51/223, train_loss: 0.1143, step time: 0.1192\n",
      "52/223, train_loss: 0.1016, step time: 0.1077\n",
      "53/223, train_loss: 0.1123, step time: 0.1126\n",
      "54/223, train_loss: 0.1048, step time: 0.1054\n",
      "55/223, train_loss: 0.0995, step time: 0.1354\n",
      "56/223, train_loss: 0.0973, step time: 0.1004\n",
      "57/223, train_loss: 0.1124, step time: 0.1443\n",
      "58/223, train_loss: 0.1044, step time: 0.1005\n",
      "59/223, train_loss: 0.1080, step time: 0.1000\n",
      "60/223, train_loss: 0.1013, step time: 0.1151\n",
      "61/223, train_loss: 0.1062, step time: 0.1448\n",
      "62/223, train_loss: 0.0992, step time: 0.1337\n",
      "63/223, train_loss: 0.1043, step time: 0.1227\n",
      "64/223, train_loss: 0.1139, step time: 0.1149\n",
      "65/223, train_loss: 0.1013, step time: 0.1275\n",
      "66/223, train_loss: 0.1089, step time: 0.1035\n",
      "67/223, train_loss: 0.1007, step time: 0.1238\n",
      "68/223, train_loss: 0.1035, step time: 0.1181\n",
      "69/223, train_loss: 0.2996, step time: 0.1243\n",
      "70/223, train_loss: 0.0981, step time: 0.1139\n",
      "71/223, train_loss: 0.1064, step time: 0.1081\n",
      "72/223, train_loss: 0.0943, step time: 0.1071\n",
      "73/223, train_loss: 0.0998, step time: 0.1007\n",
      "74/223, train_loss: 0.1147, step time: 0.1062\n",
      "75/223, train_loss: 0.1093, step time: 0.1183\n",
      "76/223, train_loss: 0.1008, step time: 0.1318\n",
      "77/223, train_loss: 0.1068, step time: 0.1617\n",
      "78/223, train_loss: 0.1023, step time: 0.1031\n",
      "79/223, train_loss: 0.1015, step time: 0.1118\n",
      "80/223, train_loss: 0.1131, step time: 0.1276\n",
      "81/223, train_loss: 0.1012, step time: 0.1263\n",
      "82/223, train_loss: 0.0918, step time: 0.1132\n",
      "83/223, train_loss: 0.1031, step time: 0.1282\n",
      "84/223, train_loss: 0.1107, step time: 0.1007\n",
      "85/223, train_loss: 0.1050, step time: 0.1001\n",
      "86/223, train_loss: 0.0961, step time: 0.1282\n",
      "87/223, train_loss: 0.1139, step time: 0.1053\n",
      "88/223, train_loss: 0.1009, step time: 0.1009\n",
      "89/223, train_loss: 0.0943, step time: 0.1127\n",
      "90/223, train_loss: 0.1072, step time: 0.1067\n",
      "91/223, train_loss: 0.0968, step time: 0.1007\n",
      "92/223, train_loss: 0.0928, step time: 0.0997\n",
      "93/223, train_loss: 0.1019, step time: 0.1244\n",
      "94/223, train_loss: 0.0966, step time: 0.1100\n",
      "95/223, train_loss: 0.1034, step time: 0.1429\n",
      "96/223, train_loss: 0.0954, step time: 0.1134\n",
      "97/223, train_loss: 0.0956, step time: 0.1203\n",
      "98/223, train_loss: 0.1066, step time: 0.1200\n",
      "99/223, train_loss: 0.1100, step time: 0.0995\n",
      "100/223, train_loss: 0.0945, step time: 0.0995\n",
      "101/223, train_loss: 0.1066, step time: 0.0994\n",
      "102/223, train_loss: 0.1106, step time: 0.1002\n",
      "103/223, train_loss: 0.0988, step time: 0.0992\n",
      "104/223, train_loss: 0.1003, step time: 0.1060\n",
      "105/223, train_loss: 0.1097, step time: 0.1008\n",
      "106/223, train_loss: 0.1060, step time: 0.1010\n",
      "107/223, train_loss: 0.1083, step time: 0.1008\n",
      "108/223, train_loss: 0.1081, step time: 0.1000\n",
      "109/223, train_loss: 0.0941, step time: 0.1068\n",
      "110/223, train_loss: 0.0939, step time: 0.1256\n",
      "111/223, train_loss: 0.0999, step time: 0.1332\n",
      "112/223, train_loss: 0.1118, step time: 0.1105\n",
      "113/223, train_loss: 0.0987, step time: 0.1076\n",
      "114/223, train_loss: 0.1082, step time: 0.1172\n",
      "115/223, train_loss: 0.1002, step time: 0.1201\n",
      "116/223, train_loss: 0.1153, step time: 0.1164\n",
      "117/223, train_loss: 0.0973, step time: 0.1368\n",
      "118/223, train_loss: 0.0913, step time: 0.1214\n",
      "119/223, train_loss: 0.1048, step time: 0.1316\n",
      "120/223, train_loss: 0.1113, step time: 0.1008\n",
      "121/223, train_loss: 0.0980, step time: 0.1000\n",
      "122/223, train_loss: 0.0974, step time: 0.0988\n",
      "123/223, train_loss: 0.0942, step time: 0.0995\n",
      "124/223, train_loss: 0.0985, step time: 0.1084\n",
      "125/223, train_loss: 0.0948, step time: 0.1004\n",
      "126/223, train_loss: 0.0944, step time: 0.0992\n",
      "127/223, train_loss: 0.1154, step time: 0.1004\n",
      "128/223, train_loss: 0.1031, step time: 0.1005\n",
      "129/223, train_loss: 0.0962, step time: 0.0999\n",
      "130/223, train_loss: 0.1125, step time: 0.0989\n",
      "131/223, train_loss: 0.0984, step time: 0.0995\n",
      "132/223, train_loss: 0.1010, step time: 0.0998\n",
      "133/223, train_loss: 0.1080, step time: 0.1005\n",
      "134/223, train_loss: 0.1150, step time: 0.0993\n",
      "135/223, train_loss: 0.1175, step time: 0.0994\n",
      "136/223, train_loss: 0.1007, step time: 0.1174\n",
      "137/223, train_loss: 0.1076, step time: 0.1068\n",
      "138/223, train_loss: 0.1052, step time: 0.1012\n",
      "139/223, train_loss: 0.0988, step time: 0.1053\n",
      "140/223, train_loss: 0.1113, step time: 0.1079\n",
      "141/223, train_loss: 0.1031, step time: 0.1073\n",
      "142/223, train_loss: 0.1170, step time: 0.1040\n",
      "143/223, train_loss: 0.0976, step time: 0.1655\n",
      "144/223, train_loss: 0.1041, step time: 0.1406\n",
      "145/223, train_loss: 0.0936, step time: 0.1052\n",
      "146/223, train_loss: 0.1039, step time: 0.1002\n",
      "147/223, train_loss: 0.0967, step time: 0.1003\n",
      "148/223, train_loss: 0.1078, step time: 0.1237\n",
      "149/223, train_loss: 0.1143, step time: 0.1207\n",
      "150/223, train_loss: 0.1056, step time: 0.1524\n",
      "151/223, train_loss: 0.0900, step time: 0.1004\n",
      "152/223, train_loss: 0.0917, step time: 0.1117\n",
      "153/223, train_loss: 0.0916, step time: 0.1202\n",
      "154/223, train_loss: 0.0979, step time: 0.1111\n",
      "155/223, train_loss: 0.0997, step time: 0.1460\n",
      "156/223, train_loss: 0.1013, step time: 0.1164\n",
      "157/223, train_loss: 0.1138, step time: 0.1130\n",
      "158/223, train_loss: 0.0995, step time: 0.0999\n",
      "159/223, train_loss: 0.1051, step time: 0.0995\n",
      "160/223, train_loss: 0.1032, step time: 0.1043\n",
      "161/223, train_loss: 0.1026, step time: 0.1161\n",
      "162/223, train_loss: 0.1036, step time: 0.1139\n",
      "163/223, train_loss: 0.0960, step time: 0.1010\n",
      "164/223, train_loss: 0.0953, step time: 0.1013\n",
      "165/223, train_loss: 0.1051, step time: 0.1117\n",
      "166/223, train_loss: 0.1118, step time: 0.1184\n",
      "167/223, train_loss: 0.0930, step time: 0.1300\n",
      "168/223, train_loss: 0.0974, step time: 0.1008\n",
      "169/223, train_loss: 0.0945, step time: 0.1010\n",
      "170/223, train_loss: 0.1117, step time: 0.1624\n",
      "171/223, train_loss: 0.1010, step time: 0.1052\n",
      "172/223, train_loss: 0.1104, step time: 0.0998\n",
      "173/223, train_loss: 0.0951, step time: 0.1078\n",
      "174/223, train_loss: 0.1014, step time: 0.1193\n",
      "175/223, train_loss: 0.1000, step time: 0.1188\n",
      "176/223, train_loss: 0.1041, step time: 0.1240\n",
      "177/223, train_loss: 0.1043, step time: 0.1004\n",
      "178/223, train_loss: 0.1098, step time: 0.1083\n",
      "179/223, train_loss: 0.0972, step time: 0.1224\n",
      "180/223, train_loss: 0.1014, step time: 0.1172\n",
      "181/223, train_loss: 0.1013, step time: 0.1105\n",
      "182/223, train_loss: 0.0939, step time: 0.1093\n",
      "183/223, train_loss: 0.1038, step time: 0.1004\n",
      "184/223, train_loss: 0.1007, step time: 0.0999\n",
      "185/223, train_loss: 0.0969, step time: 0.1059\n",
      "186/223, train_loss: 0.1157, step time: 0.1002\n",
      "187/223, train_loss: 0.1105, step time: 0.1125\n",
      "188/223, train_loss: 0.1126, step time: 0.1191\n",
      "189/223, train_loss: 0.0987, step time: 0.1077\n",
      "190/223, train_loss: 0.0938, step time: 0.1143\n",
      "191/223, train_loss: 0.1013, step time: 0.1035\n",
      "192/223, train_loss: 0.0953, step time: 0.1166\n",
      "193/223, train_loss: 0.1098, step time: 0.1134\n",
      "194/223, train_loss: 0.1053, step time: 0.1001\n",
      "195/223, train_loss: 0.1153, step time: 0.1015\n",
      "196/223, train_loss: 0.1049, step time: 0.1009\n",
      "197/223, train_loss: 0.1066, step time: 0.1070\n",
      "198/223, train_loss: 0.0962, step time: 0.1149\n",
      "199/223, train_loss: 0.0986, step time: 0.1013\n",
      "200/223, train_loss: 0.1015, step time: 0.1245\n",
      "201/223, train_loss: 0.1068, step time: 0.1005\n",
      "202/223, train_loss: 0.1019, step time: 0.1061\n",
      "203/223, train_loss: 0.1008, step time: 0.1012\n",
      "204/223, train_loss: 0.0994, step time: 0.1009\n",
      "205/223, train_loss: 0.0996, step time: 0.1114\n",
      "206/223, train_loss: 0.0997, step time: 0.1036\n",
      "207/223, train_loss: 0.0912, step time: 0.1007\n",
      "208/223, train_loss: 0.0948, step time: 0.1008\n",
      "209/223, train_loss: 0.1048, step time: 0.1121\n",
      "210/223, train_loss: 0.1072, step time: 0.1009\n",
      "211/223, train_loss: 0.1068, step time: 0.1009\n",
      "212/223, train_loss: 0.0937, step time: 0.1014\n",
      "213/223, train_loss: 0.1108, step time: 0.1205\n",
      "214/223, train_loss: 0.1049, step time: 0.1061\n",
      "215/223, train_loss: 0.1160, step time: 0.1033\n",
      "216/223, train_loss: 0.1123, step time: 0.1005\n",
      "217/223, train_loss: 0.0990, step time: 0.1290\n",
      "218/223, train_loss: 0.0987, step time: 0.1015\n",
      "219/223, train_loss: 0.1040, step time: 0.1326\n",
      "220/223, train_loss: 0.1028, step time: 0.1005\n",
      "221/223, train_loss: 0.0996, step time: 0.1005\n",
      "222/223, train_loss: 0.0949, step time: 0.0986\n",
      "223/223, train_loss: 0.0976, step time: 0.0995\n",
      "epoch 179 average loss: 0.1041\n",
      "time consuming of epoch 179 is: 91.6341\n",
      "----------\n",
      "epoch 180/300\n",
      "1/223, train_loss: 0.1182, step time: 0.1010\n",
      "2/223, train_loss: 0.1011, step time: 0.1003\n",
      "3/223, train_loss: 0.1002, step time: 0.1000\n",
      "4/223, train_loss: 0.0914, step time: 0.1118\n",
      "5/223, train_loss: 0.1096, step time: 0.1281\n",
      "6/223, train_loss: 0.1142, step time: 0.1398\n",
      "7/223, train_loss: 0.1029, step time: 0.1150\n",
      "8/223, train_loss: 0.0961, step time: 0.1166\n",
      "9/223, train_loss: 0.1078, step time: 0.1336\n",
      "10/223, train_loss: 0.0921, step time: 0.1009\n",
      "11/223, train_loss: 0.0931, step time: 0.0996\n",
      "12/223, train_loss: 0.1006, step time: 0.1000\n",
      "13/223, train_loss: 0.1061, step time: 0.1266\n",
      "14/223, train_loss: 0.0970, step time: 0.1266\n",
      "15/223, train_loss: 0.0997, step time: 0.1296\n",
      "16/223, train_loss: 0.1055, step time: 0.1316\n",
      "17/223, train_loss: 0.1074, step time: 0.1253\n",
      "18/223, train_loss: 0.0934, step time: 0.1037\n",
      "19/223, train_loss: 0.1009, step time: 0.1006\n",
      "20/223, train_loss: 0.1036, step time: 0.1062\n",
      "21/223, train_loss: 0.0957, step time: 0.1005\n",
      "22/223, train_loss: 0.1037, step time: 0.1003\n",
      "23/223, train_loss: 0.1004, step time: 0.1006\n",
      "24/223, train_loss: 0.1066, step time: 0.1004\n",
      "25/223, train_loss: 0.1023, step time: 0.0998\n",
      "26/223, train_loss: 0.1073, step time: 0.1228\n",
      "27/223, train_loss: 0.0980, step time: 0.1086\n",
      "28/223, train_loss: 0.1092, step time: 0.0999\n",
      "29/223, train_loss: 0.1047, step time: 0.1134\n",
      "30/223, train_loss: 0.0906, step time: 0.1018\n",
      "31/223, train_loss: 0.1053, step time: 0.1054\n",
      "32/223, train_loss: 0.0991, step time: 0.1000\n",
      "33/223, train_loss: 0.0975, step time: 0.1147\n",
      "34/223, train_loss: 0.0937, step time: 0.1074\n",
      "35/223, train_loss: 0.1103, step time: 0.1031\n",
      "36/223, train_loss: 0.0979, step time: 0.1095\n",
      "37/223, train_loss: 0.1114, step time: 0.1140\n",
      "38/223, train_loss: 0.0981, step time: 0.1004\n",
      "39/223, train_loss: 0.1052, step time: 0.1007\n",
      "40/223, train_loss: 0.1004, step time: 0.1027\n",
      "41/223, train_loss: 0.1014, step time: 0.1033\n",
      "42/223, train_loss: 0.0887, step time: 0.1013\n",
      "43/223, train_loss: 0.1017, step time: 0.0997\n",
      "44/223, train_loss: 0.1075, step time: 0.1007\n",
      "45/223, train_loss: 0.1084, step time: 0.1194\n",
      "46/223, train_loss: 0.0953, step time: 0.1004\n",
      "47/223, train_loss: 0.1074, step time: 0.0998\n",
      "48/223, train_loss: 0.1120, step time: 0.1004\n",
      "49/223, train_loss: 0.1116, step time: 0.1023\n",
      "50/223, train_loss: 0.1048, step time: 0.1084\n",
      "51/223, train_loss: 0.0964, step time: 0.1143\n",
      "52/223, train_loss: 0.1007, step time: 0.1008\n",
      "53/223, train_loss: 0.0920, step time: 0.1237\n",
      "54/223, train_loss: 0.1093, step time: 0.1042\n",
      "55/223, train_loss: 0.0955, step time: 0.1006\n",
      "56/223, train_loss: 0.1101, step time: 0.1014\n",
      "57/223, train_loss: 0.0995, step time: 0.1352\n",
      "58/223, train_loss: 0.1066, step time: 0.1048\n",
      "59/223, train_loss: 0.1027, step time: 0.1018\n",
      "60/223, train_loss: 0.0928, step time: 0.1062\n",
      "61/223, train_loss: 0.1055, step time: 0.1183\n",
      "62/223, train_loss: 0.1076, step time: 0.1172\n",
      "63/223, train_loss: 0.1064, step time: 0.1077\n",
      "64/223, train_loss: 0.0951, step time: 0.1019\n",
      "65/223, train_loss: 0.0933, step time: 0.1001\n",
      "66/223, train_loss: 0.1124, step time: 0.1221\n",
      "67/223, train_loss: 0.1067, step time: 0.1149\n",
      "68/223, train_loss: 0.1078, step time: 0.0998\n",
      "69/223, train_loss: 0.1166, step time: 0.1229\n",
      "70/223, train_loss: 0.0936, step time: 0.1088\n",
      "71/223, train_loss: 0.1060, step time: 0.1012\n",
      "72/223, train_loss: 0.0992, step time: 0.1092\n",
      "73/223, train_loss: 0.0974, step time: 0.1135\n",
      "74/223, train_loss: 0.1095, step time: 0.1001\n",
      "75/223, train_loss: 0.0924, step time: 0.1242\n",
      "76/223, train_loss: 0.1190, step time: 0.1191\n",
      "77/223, train_loss: 0.1018, step time: 0.1045\n",
      "78/223, train_loss: 0.0960, step time: 0.0993\n",
      "79/223, train_loss: 0.1088, step time: 0.1199\n",
      "80/223, train_loss: 0.1110, step time: 0.1108\n",
      "81/223, train_loss: 0.3018, step time: 0.1030\n",
      "82/223, train_loss: 0.1090, step time: 0.1015\n",
      "83/223, train_loss: 0.0946, step time: 0.1153\n",
      "84/223, train_loss: 0.1020, step time: 0.1008\n",
      "85/223, train_loss: 0.1097, step time: 0.1079\n",
      "86/223, train_loss: 0.0994, step time: 0.1007\n",
      "87/223, train_loss: 0.0944, step time: 0.1008\n",
      "88/223, train_loss: 0.1072, step time: 0.1028\n",
      "89/223, train_loss: 0.0920, step time: 0.1606\n",
      "90/223, train_loss: 0.0953, step time: 0.1244\n",
      "91/223, train_loss: 0.0995, step time: 0.1233\n",
      "92/223, train_loss: 0.1128, step time: 0.1077\n",
      "93/223, train_loss: 0.1121, step time: 0.0995\n",
      "94/223, train_loss: 0.1082, step time: 0.1102\n",
      "95/223, train_loss: 0.1021, step time: 0.1238\n",
      "96/223, train_loss: 0.0945, step time: 0.1033\n",
      "97/223, train_loss: 0.0997, step time: 0.1138\n",
      "98/223, train_loss: 0.0994, step time: 0.1101\n",
      "99/223, train_loss: 0.1039, step time: 0.1157\n",
      "100/223, train_loss: 0.1092, step time: 0.1015\n",
      "101/223, train_loss: 0.0998, step time: 0.1072\n",
      "102/223, train_loss: 0.1073, step time: 0.1047\n",
      "103/223, train_loss: 0.1000, step time: 0.1252\n",
      "104/223, train_loss: 0.0991, step time: 0.0991\n",
      "105/223, train_loss: 0.1200, step time: 0.1268\n",
      "106/223, train_loss: 0.0998, step time: 0.1700\n",
      "107/223, train_loss: 0.1048, step time: 0.1017\n",
      "108/223, train_loss: 0.0949, step time: 0.1014\n",
      "109/223, train_loss: 0.1002, step time: 0.1014\n",
      "110/223, train_loss: 0.1165, step time: 0.1011\n",
      "111/223, train_loss: 0.0974, step time: 0.1012\n",
      "112/223, train_loss: 0.0942, step time: 0.1006\n",
      "113/223, train_loss: 0.1148, step time: 0.1000\n",
      "114/223, train_loss: 0.0967, step time: 0.1060\n",
      "115/223, train_loss: 0.1029, step time: 0.1473\n",
      "116/223, train_loss: 0.1073, step time: 0.1285\n",
      "117/223, train_loss: 0.0918, step time: 0.1006\n",
      "118/223, train_loss: 0.1012, step time: 0.1009\n",
      "119/223, train_loss: 0.0935, step time: 0.0998\n",
      "120/223, train_loss: 0.1032, step time: 0.1067\n",
      "121/223, train_loss: 0.1136, step time: 0.1002\n",
      "122/223, train_loss: 0.1009, step time: 0.1016\n",
      "123/223, train_loss: 0.1155, step time: 0.1007\n",
      "124/223, train_loss: 0.0985, step time: 0.1059\n",
      "125/223, train_loss: 0.1048, step time: 0.1245\n",
      "126/223, train_loss: 0.1094, step time: 0.1007\n",
      "127/223, train_loss: 0.1004, step time: 0.1009\n",
      "128/223, train_loss: 0.1187, step time: 0.1009\n",
      "129/223, train_loss: 0.1024, step time: 0.1205\n",
      "130/223, train_loss: 0.1086, step time: 0.1386\n",
      "131/223, train_loss: 0.0995, step time: 0.1111\n",
      "132/223, train_loss: 0.0980, step time: 0.1003\n",
      "133/223, train_loss: 0.1057, step time: 0.1057\n",
      "134/223, train_loss: 0.1047, step time: 0.1228\n",
      "135/223, train_loss: 0.1163, step time: 0.1019\n",
      "136/223, train_loss: 0.1133, step time: 0.1057\n",
      "137/223, train_loss: 0.1136, step time: 0.1032\n",
      "138/223, train_loss: 0.1115, step time: 0.1154\n",
      "139/223, train_loss: 0.0992, step time: 0.1121\n",
      "140/223, train_loss: 0.0956, step time: 0.1171\n",
      "141/223, train_loss: 0.1066, step time: 0.1134\n",
      "142/223, train_loss: 0.1115, step time: 0.1095\n",
      "143/223, train_loss: 0.1082, step time: 0.1009\n",
      "144/223, train_loss: 0.0974, step time: 0.1065\n",
      "145/223, train_loss: 0.1101, step time: 0.1003\n",
      "146/223, train_loss: 0.0959, step time: 0.1002\n",
      "147/223, train_loss: 0.0941, step time: 0.1168\n",
      "148/223, train_loss: 0.0980, step time: 0.1290\n",
      "149/223, train_loss: 0.1142, step time: 0.1014\n",
      "150/223, train_loss: 0.1115, step time: 0.0999\n",
      "151/223, train_loss: 0.1047, step time: 0.1288\n",
      "152/223, train_loss: 0.0995, step time: 0.1170\n",
      "153/223, train_loss: 0.1060, step time: 0.1003\n",
      "154/223, train_loss: 0.1066, step time: 0.1284\n",
      "155/223, train_loss: 0.1087, step time: 0.1264\n",
      "156/223, train_loss: 0.1030, step time: 0.1356\n",
      "157/223, train_loss: 0.1064, step time: 0.1037\n",
      "158/223, train_loss: 0.1019, step time: 0.1049\n",
      "159/223, train_loss: 0.1000, step time: 0.1094\n",
      "160/223, train_loss: 0.1027, step time: 0.1533\n",
      "161/223, train_loss: 0.1109, step time: 0.1153\n",
      "162/223, train_loss: 0.1006, step time: 0.1287\n",
      "163/223, train_loss: 0.1036, step time: 0.1166\n",
      "164/223, train_loss: 0.1112, step time: 0.1082\n",
      "165/223, train_loss: 0.1006, step time: 0.1062\n",
      "166/223, train_loss: 0.1015, step time: 0.1042\n",
      "167/223, train_loss: 0.1066, step time: 0.1087\n",
      "168/223, train_loss: 0.0963, step time: 0.1451\n",
      "169/223, train_loss: 0.0989, step time: 0.1095\n",
      "170/223, train_loss: 0.0976, step time: 0.1016\n",
      "171/223, train_loss: 0.1029, step time: 0.1020\n",
      "172/223, train_loss: 0.0947, step time: 0.1047\n",
      "173/223, train_loss: 0.0992, step time: 0.1046\n",
      "174/223, train_loss: 0.1252, step time: 0.1026\n",
      "175/223, train_loss: 0.1012, step time: 0.1186\n",
      "176/223, train_loss: 0.1095, step time: 0.1264\n",
      "177/223, train_loss: 0.1066, step time: 0.1058\n",
      "178/223, train_loss: 0.0968, step time: 0.1127\n",
      "179/223, train_loss: 0.1091, step time: 0.1135\n",
      "180/223, train_loss: 0.0989, step time: 0.1114\n",
      "181/223, train_loss: 0.1072, step time: 0.1043\n",
      "182/223, train_loss: 0.0946, step time: 0.0994\n",
      "183/223, train_loss: 0.0996, step time: 0.1300\n",
      "184/223, train_loss: 0.1104, step time: 0.1003\n",
      "185/223, train_loss: 0.1018, step time: 0.1007\n",
      "186/223, train_loss: 0.1049, step time: 0.1126\n",
      "187/223, train_loss: 0.1105, step time: 0.1001\n",
      "188/223, train_loss: 0.1070, step time: 0.1047\n",
      "189/223, train_loss: 0.1005, step time: 0.1235\n",
      "190/223, train_loss: 0.1085, step time: 0.1114\n",
      "191/223, train_loss: 0.1127, step time: 0.1146\n",
      "192/223, train_loss: 0.1026, step time: 0.1075\n",
      "193/223, train_loss: 0.0928, step time: 0.1191\n",
      "194/223, train_loss: 0.0943, step time: 0.1115\n",
      "195/223, train_loss: 0.0997, step time: 0.1152\n",
      "196/223, train_loss: 0.0995, step time: 0.1228\n",
      "197/223, train_loss: 0.0978, step time: 0.1013\n",
      "198/223, train_loss: 0.1054, step time: 0.1086\n",
      "199/223, train_loss: 0.1136, step time: 0.1252\n",
      "200/223, train_loss: 0.1129, step time: 0.1271\n",
      "201/223, train_loss: 0.0956, step time: 0.1084\n",
      "202/223, train_loss: 0.1067, step time: 0.1075\n",
      "203/223, train_loss: 0.1044, step time: 0.1077\n",
      "204/223, train_loss: 0.0956, step time: 0.1075\n",
      "205/223, train_loss: 0.1110, step time: 0.1002\n",
      "206/223, train_loss: 0.0992, step time: 0.1109\n",
      "207/223, train_loss: 0.0921, step time: 0.1064\n",
      "208/223, train_loss: 0.1029, step time: 0.1119\n",
      "209/223, train_loss: 0.0998, step time: 0.1309\n",
      "210/223, train_loss: 0.1094, step time: 0.1080\n",
      "211/223, train_loss: 0.1063, step time: 0.1013\n",
      "212/223, train_loss: 0.1038, step time: 0.1090\n",
      "213/223, train_loss: 0.0992, step time: 0.1251\n",
      "214/223, train_loss: 0.0967, step time: 0.1165\n",
      "215/223, train_loss: 0.1051, step time: 0.1010\n",
      "216/223, train_loss: 0.0892, step time: 0.1065\n",
      "217/223, train_loss: 0.0926, step time: 0.1247\n",
      "218/223, train_loss: 0.1148, step time: 0.1039\n",
      "219/223, train_loss: 0.1012, step time: 0.1009\n",
      "220/223, train_loss: 0.0957, step time: 0.1004\n",
      "221/223, train_loss: 0.1018, step time: 0.1275\n",
      "222/223, train_loss: 0.1109, step time: 0.0995\n",
      "223/223, train_loss: 0.1097, step time: 0.1001\n",
      "epoch 180 average loss: 0.1041\n",
      "current epoch: 180 current mean dice: 0.8598 tc: 0.9214 wt: 0.8697 et: 0.7884\n",
      "best mean dice: 0.8600 at epoch: 170\n",
      "time consuming of epoch 180 is: 92.4811\n",
      "----------\n",
      "epoch 181/300\n",
      "1/223, train_loss: 0.0938, step time: 0.1103\n",
      "2/223, train_loss: 0.1018, step time: 0.0995\n",
      "3/223, train_loss: 0.0955, step time: 0.1097\n",
      "4/223, train_loss: 0.1014, step time: 0.1245\n",
      "5/223, train_loss: 0.0963, step time: 0.1056\n",
      "6/223, train_loss: 0.1092, step time: 0.1200\n",
      "7/223, train_loss: 0.1001, step time: 0.1310\n",
      "8/223, train_loss: 0.1056, step time: 0.1555\n",
      "9/223, train_loss: 0.1119, step time: 0.1275\n",
      "10/223, train_loss: 0.1252, step time: 0.1160\n",
      "11/223, train_loss: 0.1015, step time: 0.1009\n",
      "12/223, train_loss: 0.0961, step time: 0.1213\n",
      "13/223, train_loss: 0.1126, step time: 0.0997\n",
      "14/223, train_loss: 0.0902, step time: 0.1003\n",
      "15/223, train_loss: 0.0974, step time: 0.1007\n",
      "16/223, train_loss: 0.1172, step time: 0.1088\n",
      "17/223, train_loss: 0.0988, step time: 0.1128\n",
      "18/223, train_loss: 0.1175, step time: 0.1382\n",
      "19/223, train_loss: 0.1110, step time: 0.1284\n",
      "20/223, train_loss: 0.1055, step time: 0.1003\n",
      "21/223, train_loss: 0.1049, step time: 0.1072\n",
      "22/223, train_loss: 0.1039, step time: 0.1007\n",
      "23/223, train_loss: 0.1094, step time: 0.1149\n",
      "24/223, train_loss: 0.0962, step time: 0.1006\n",
      "25/223, train_loss: 0.1003, step time: 0.1062\n",
      "26/223, train_loss: 0.0932, step time: 0.1121\n",
      "27/223, train_loss: 0.0997, step time: 0.1004\n",
      "28/223, train_loss: 0.1190, step time: 0.1060\n",
      "29/223, train_loss: 0.1050, step time: 0.1133\n",
      "30/223, train_loss: 0.0985, step time: 0.1022\n",
      "31/223, train_loss: 0.1061, step time: 0.1000\n",
      "32/223, train_loss: 0.1058, step time: 0.1012\n",
      "33/223, train_loss: 0.1029, step time: 0.1252\n",
      "34/223, train_loss: 0.0888, step time: 0.1221\n",
      "35/223, train_loss: 0.0950, step time: 0.1009\n",
      "36/223, train_loss: 0.1061, step time: 0.1000\n",
      "37/223, train_loss: 0.1040, step time: 0.1435\n",
      "38/223, train_loss: 0.1111, step time: 0.1085\n",
      "39/223, train_loss: 0.1065, step time: 0.1007\n",
      "40/223, train_loss: 0.0979, step time: 0.1009\n",
      "41/223, train_loss: 0.2962, step time: 0.1146\n",
      "42/223, train_loss: 0.1043, step time: 0.1032\n",
      "43/223, train_loss: 0.0963, step time: 0.1301\n",
      "44/223, train_loss: 0.0913, step time: 0.1238\n",
      "45/223, train_loss: 0.1069, step time: 0.1124\n",
      "46/223, train_loss: 0.1010, step time: 0.1318\n",
      "47/223, train_loss: 0.0924, step time: 0.1173\n",
      "48/223, train_loss: 0.1168, step time: 0.1128\n",
      "49/223, train_loss: 0.0983, step time: 0.1423\n",
      "50/223, train_loss: 0.0894, step time: 0.1302\n",
      "51/223, train_loss: 0.0923, step time: 0.1026\n",
      "52/223, train_loss: 0.1072, step time: 0.0997\n",
      "53/223, train_loss: 0.0993, step time: 0.1115\n",
      "54/223, train_loss: 0.1182, step time: 0.1177\n",
      "55/223, train_loss: 0.1007, step time: 0.1199\n",
      "56/223, train_loss: 0.1103, step time: 0.1088\n",
      "57/223, train_loss: 0.1112, step time: 0.1042\n",
      "58/223, train_loss: 0.0971, step time: 0.1450\n",
      "59/223, train_loss: 0.1094, step time: 0.0997\n",
      "60/223, train_loss: 0.1105, step time: 0.1046\n",
      "61/223, train_loss: 0.1012, step time: 0.1023\n",
      "62/223, train_loss: 0.1042, step time: 0.1006\n",
      "63/223, train_loss: 0.1122, step time: 0.1011\n",
      "64/223, train_loss: 0.1030, step time: 0.1151\n",
      "65/223, train_loss: 0.1013, step time: 0.1203\n",
      "66/223, train_loss: 0.1089, step time: 0.1307\n",
      "67/223, train_loss: 0.0964, step time: 0.1245\n",
      "68/223, train_loss: 0.1081, step time: 0.1045\n",
      "69/223, train_loss: 0.1071, step time: 0.1129\n",
      "70/223, train_loss: 0.1271, step time: 0.1004\n",
      "71/223, train_loss: 0.0933, step time: 0.1013\n",
      "72/223, train_loss: 0.1032, step time: 0.1060\n",
      "73/223, train_loss: 0.0933, step time: 0.1054\n",
      "74/223, train_loss: 0.1100, step time: 0.1077\n",
      "75/223, train_loss: 0.1042, step time: 0.1005\n",
      "76/223, train_loss: 0.1079, step time: 0.0999\n",
      "77/223, train_loss: 0.0972, step time: 0.1229\n",
      "78/223, train_loss: 0.0946, step time: 0.1001\n",
      "79/223, train_loss: 0.1081, step time: 0.1041\n",
      "80/223, train_loss: 0.1128, step time: 0.1010\n",
      "81/223, train_loss: 0.1133, step time: 0.1521\n",
      "82/223, train_loss: 0.1178, step time: 0.1063\n",
      "83/223, train_loss: 0.1109, step time: 0.1013\n",
      "84/223, train_loss: 0.0968, step time: 0.1009\n",
      "85/223, train_loss: 0.0959, step time: 0.0997\n",
      "86/223, train_loss: 0.1095, step time: 0.1005\n",
      "87/223, train_loss: 0.0942, step time: 0.1005\n",
      "88/223, train_loss: 0.1069, step time: 0.1010\n",
      "89/223, train_loss: 0.0980, step time: 0.1040\n",
      "90/223, train_loss: 0.1020, step time: 0.1005\n",
      "91/223, train_loss: 0.1085, step time: 0.1207\n",
      "92/223, train_loss: 0.1145, step time: 0.1092\n",
      "93/223, train_loss: 0.0943, step time: 0.1125\n",
      "94/223, train_loss: 0.0961, step time: 0.1637\n",
      "95/223, train_loss: 0.0859, step time: 0.1544\n",
      "96/223, train_loss: 0.1016, step time: 0.1248\n",
      "97/223, train_loss: 0.1119, step time: 0.1103\n",
      "98/223, train_loss: 0.0952, step time: 0.1493\n",
      "99/223, train_loss: 0.1053, step time: 0.1244\n",
      "100/223, train_loss: 0.1020, step time: 0.1214\n",
      "101/223, train_loss: 0.1079, step time: 0.1097\n",
      "102/223, train_loss: 0.0962, step time: 0.1237\n",
      "103/223, train_loss: 0.1089, step time: 0.1234\n",
      "104/223, train_loss: 0.0988, step time: 0.1076\n",
      "105/223, train_loss: 0.1021, step time: 0.1030\n",
      "106/223, train_loss: 0.0998, step time: 0.1031\n",
      "107/223, train_loss: 0.1035, step time: 0.1058\n",
      "108/223, train_loss: 0.1018, step time: 0.1046\n",
      "109/223, train_loss: 0.1118, step time: 0.1081\n",
      "110/223, train_loss: 0.1081, step time: 0.1182\n",
      "111/223, train_loss: 0.1093, step time: 0.1312\n",
      "112/223, train_loss: 0.1024, step time: 0.1126\n",
      "113/223, train_loss: 0.1124, step time: 0.1183\n",
      "114/223, train_loss: 0.0964, step time: 0.1004\n",
      "115/223, train_loss: 0.0901, step time: 0.1005\n",
      "116/223, train_loss: 0.1089, step time: 0.1053\n",
      "117/223, train_loss: 0.0944, step time: 0.1274\n",
      "118/223, train_loss: 0.0944, step time: 0.1320\n",
      "119/223, train_loss: 0.0963, step time: 0.1081\n",
      "120/223, train_loss: 0.1025, step time: 0.1030\n",
      "121/223, train_loss: 0.0977, step time: 0.1180\n",
      "122/223, train_loss: 0.0983, step time: 0.1076\n",
      "123/223, train_loss: 0.0955, step time: 0.1060\n",
      "124/223, train_loss: 0.1088, step time: 0.1011\n",
      "125/223, train_loss: 0.1117, step time: 0.1009\n",
      "126/223, train_loss: 0.0960, step time: 0.1014\n",
      "127/223, train_loss: 0.1098, step time: 0.1007\n",
      "128/223, train_loss: 0.0994, step time: 0.1137\n",
      "129/223, train_loss: 0.1123, step time: 0.1159\n",
      "130/223, train_loss: 0.1031, step time: 0.1232\n",
      "131/223, train_loss: 0.1064, step time: 0.1294\n",
      "132/223, train_loss: 0.0978, step time: 0.1023\n",
      "133/223, train_loss: 0.0957, step time: 0.1266\n",
      "134/223, train_loss: 0.1182, step time: 0.1087\n",
      "135/223, train_loss: 0.1097, step time: 0.1003\n",
      "136/223, train_loss: 0.1108, step time: 0.1003\n",
      "137/223, train_loss: 0.1132, step time: 0.1097\n",
      "138/223, train_loss: 0.1054, step time: 0.1097\n",
      "139/223, train_loss: 0.1124, step time: 0.1001\n",
      "140/223, train_loss: 0.0954, step time: 0.1203\n",
      "141/223, train_loss: 0.1004, step time: 0.1057\n",
      "142/223, train_loss: 0.1008, step time: 0.1018\n",
      "143/223, train_loss: 0.1080, step time: 0.1000\n",
      "144/223, train_loss: 0.1085, step time: 0.1236\n",
      "145/223, train_loss: 0.1012, step time: 0.1006\n",
      "146/223, train_loss: 0.1113, step time: 0.0988\n",
      "147/223, train_loss: 0.1000, step time: 0.1078\n",
      "148/223, train_loss: 0.1051, step time: 0.1279\n",
      "149/223, train_loss: 0.1008, step time: 0.1001\n",
      "150/223, train_loss: 0.1063, step time: 0.1106\n",
      "151/223, train_loss: 0.1023, step time: 0.1038\n",
      "152/223, train_loss: 0.1045, step time: 0.1070\n",
      "153/223, train_loss: 0.1051, step time: 0.1135\n",
      "154/223, train_loss: 0.1156, step time: 0.1112\n",
      "155/223, train_loss: 0.1138, step time: 0.1148\n",
      "156/223, train_loss: 0.1147, step time: 0.0991\n",
      "157/223, train_loss: 0.0939, step time: 0.1084\n",
      "158/223, train_loss: 0.1115, step time: 0.1093\n",
      "159/223, train_loss: 0.0906, step time: 0.0992\n",
      "160/223, train_loss: 0.1056, step time: 0.0997\n",
      "161/223, train_loss: 0.0977, step time: 0.1157\n",
      "162/223, train_loss: 0.0893, step time: 0.1165\n",
      "163/223, train_loss: 0.1050, step time: 0.0993\n",
      "164/223, train_loss: 0.1103, step time: 0.1007\n",
      "165/223, train_loss: 0.1025, step time: 0.1005\n",
      "166/223, train_loss: 0.1054, step time: 0.1052\n",
      "167/223, train_loss: 0.1139, step time: 0.1099\n",
      "168/223, train_loss: 0.1016, step time: 0.0999\n",
      "169/223, train_loss: 0.1041, step time: 0.1204\n",
      "170/223, train_loss: 0.1042, step time: 0.1157\n",
      "171/223, train_loss: 0.0932, step time: 0.1042\n",
      "172/223, train_loss: 0.0967, step time: 0.1091\n",
      "173/223, train_loss: 0.0974, step time: 0.1087\n",
      "174/223, train_loss: 0.0958, step time: 0.1028\n",
      "175/223, train_loss: 0.1163, step time: 0.1255\n",
      "176/223, train_loss: 0.0966, step time: 0.1015\n",
      "177/223, train_loss: 0.1030, step time: 0.1214\n",
      "178/223, train_loss: 0.1061, step time: 0.1113\n",
      "179/223, train_loss: 0.0979, step time: 0.1212\n",
      "180/223, train_loss: 0.0975, step time: 0.1033\n",
      "181/223, train_loss: 0.0971, step time: 0.1183\n",
      "182/223, train_loss: 0.1139, step time: 0.1045\n",
      "183/223, train_loss: 0.1005, step time: 0.1165\n",
      "184/223, train_loss: 0.1041, step time: 0.1003\n",
      "185/223, train_loss: 0.0924, step time: 0.1071\n",
      "186/223, train_loss: 0.1010, step time: 0.1271\n",
      "187/223, train_loss: 0.0968, step time: 0.1021\n",
      "188/223, train_loss: 0.1025, step time: 0.1092\n",
      "189/223, train_loss: 0.1079, step time: 0.1110\n",
      "190/223, train_loss: 0.1030, step time: 0.1264\n",
      "191/223, train_loss: 0.0999, step time: 0.1149\n",
      "192/223, train_loss: 0.1088, step time: 0.1014\n",
      "193/223, train_loss: 0.0914, step time: 0.1113\n",
      "194/223, train_loss: 0.0979, step time: 0.1005\n",
      "195/223, train_loss: 0.1048, step time: 0.1005\n",
      "196/223, train_loss: 0.1007, step time: 0.1288\n",
      "197/223, train_loss: 0.0937, step time: 0.1174\n",
      "198/223, train_loss: 0.1072, step time: 0.1030\n",
      "199/223, train_loss: 0.1111, step time: 0.1005\n",
      "200/223, train_loss: 0.0986, step time: 0.1006\n",
      "201/223, train_loss: 0.1058, step time: 0.0990\n",
      "202/223, train_loss: 0.1016, step time: 0.1150\n",
      "203/223, train_loss: 0.0928, step time: 0.1191\n",
      "204/223, train_loss: 0.0998, step time: 0.1093\n",
      "205/223, train_loss: 0.1066, step time: 0.1155\n",
      "206/223, train_loss: 0.1093, step time: 0.1200\n",
      "207/223, train_loss: 0.1001, step time: 0.1465\n",
      "208/223, train_loss: 0.1144, step time: 0.1009\n",
      "209/223, train_loss: 0.1083, step time: 0.1125\n",
      "210/223, train_loss: 0.1028, step time: 0.1067\n",
      "211/223, train_loss: 0.1003, step time: 0.1411\n",
      "212/223, train_loss: 0.1034, step time: 0.1344\n",
      "213/223, train_loss: 0.1012, step time: 0.1198\n",
      "214/223, train_loss: 0.0957, step time: 0.1122\n",
      "215/223, train_loss: 0.1055, step time: 0.1061\n",
      "216/223, train_loss: 0.1115, step time: 0.1001\n",
      "217/223, train_loss: 0.1070, step time: 0.0990\n",
      "218/223, train_loss: 0.1062, step time: 0.0992\n",
      "219/223, train_loss: 0.0988, step time: 0.1000\n",
      "220/223, train_loss: 0.0972, step time: 0.1002\n",
      "221/223, train_loss: 0.1137, step time: 0.0996\n",
      "222/223, train_loss: 0.0944, step time: 0.0988\n",
      "223/223, train_loss: 0.1072, step time: 0.1000\n",
      "epoch 181 average loss: 0.1042\n",
      "time consuming of epoch 181 is: 87.5615\n",
      "----------\n",
      "epoch 182/300\n",
      "1/223, train_loss: 0.1074, step time: 0.1011\n",
      "2/223, train_loss: 0.1019, step time: 0.1003\n",
      "3/223, train_loss: 0.0977, step time: 0.1178\n",
      "4/223, train_loss: 0.1118, step time: 0.1199\n",
      "5/223, train_loss: 0.1053, step time: 0.0993\n",
      "6/223, train_loss: 0.0959, step time: 0.1001\n",
      "7/223, train_loss: 0.0930, step time: 0.1066\n",
      "8/223, train_loss: 0.1084, step time: 0.1024\n",
      "9/223, train_loss: 0.2963, step time: 0.1124\n",
      "10/223, train_loss: 0.1033, step time: 0.1009\n",
      "11/223, train_loss: 0.1062, step time: 0.1423\n",
      "12/223, train_loss: 0.0995, step time: 0.1000\n",
      "13/223, train_loss: 0.1148, step time: 0.1152\n",
      "14/223, train_loss: 0.1064, step time: 0.1003\n",
      "15/223, train_loss: 0.0978, step time: 0.1013\n",
      "16/223, train_loss: 0.1070, step time: 0.1037\n",
      "17/223, train_loss: 0.1113, step time: 0.1188\n",
      "18/223, train_loss: 0.1042, step time: 0.1007\n",
      "19/223, train_loss: 0.1000, step time: 0.1403\n",
      "20/223, train_loss: 0.1239, step time: 0.1084\n",
      "21/223, train_loss: 0.0991, step time: 0.1136\n",
      "22/223, train_loss: 0.0955, step time: 0.1389\n",
      "23/223, train_loss: 0.1042, step time: 0.1007\n",
      "24/223, train_loss: 0.0924, step time: 0.0999\n",
      "25/223, train_loss: 0.0963, step time: 0.1075\n",
      "26/223, train_loss: 0.0994, step time: 0.0997\n",
      "27/223, train_loss: 0.1149, step time: 0.1005\n",
      "28/223, train_loss: 0.1059, step time: 0.1182\n",
      "29/223, train_loss: 0.1071, step time: 0.1239\n",
      "30/223, train_loss: 0.1080, step time: 0.1230\n",
      "31/223, train_loss: 0.1086, step time: 0.1001\n",
      "32/223, train_loss: 0.1064, step time: 0.1003\n",
      "33/223, train_loss: 0.1016, step time: 0.1183\n",
      "34/223, train_loss: 0.0968, step time: 0.1172\n",
      "35/223, train_loss: 0.1031, step time: 0.1008\n",
      "36/223, train_loss: 0.0982, step time: 0.1005\n",
      "37/223, train_loss: 0.0996, step time: 0.1031\n",
      "38/223, train_loss: 0.1017, step time: 0.1131\n",
      "39/223, train_loss: 0.1160, step time: 0.1461\n",
      "40/223, train_loss: 0.1053, step time: 0.1107\n",
      "41/223, train_loss: 0.0999, step time: 0.1273\n",
      "42/223, train_loss: 0.1031, step time: 0.1157\n",
      "43/223, train_loss: 0.1065, step time: 0.1250\n",
      "44/223, train_loss: 0.0968, step time: 0.1024\n",
      "45/223, train_loss: 0.0952, step time: 0.1039\n",
      "46/223, train_loss: 0.1141, step time: 0.1088\n",
      "47/223, train_loss: 0.1052, step time: 0.1089\n",
      "48/223, train_loss: 0.1065, step time: 0.1185\n",
      "49/223, train_loss: 0.1066, step time: 0.1229\n",
      "50/223, train_loss: 0.1125, step time: 0.1071\n",
      "51/223, train_loss: 0.1001, step time: 0.1173\n",
      "52/223, train_loss: 0.1148, step time: 0.1143\n",
      "53/223, train_loss: 0.1070, step time: 0.1172\n",
      "54/223, train_loss: 0.1019, step time: 0.1122\n",
      "55/223, train_loss: 0.1145, step time: 0.1399\n",
      "56/223, train_loss: 0.0965, step time: 0.1086\n",
      "57/223, train_loss: 0.1106, step time: 0.1140\n",
      "58/223, train_loss: 0.0961, step time: 0.1195\n",
      "59/223, train_loss: 0.1176, step time: 0.1557\n",
      "60/223, train_loss: 0.1007, step time: 0.1243\n",
      "61/223, train_loss: 0.1084, step time: 0.1030\n",
      "62/223, train_loss: 0.1059, step time: 0.1003\n",
      "63/223, train_loss: 0.0919, step time: 0.1319\n",
      "64/223, train_loss: 0.0994, step time: 0.1007\n",
      "65/223, train_loss: 0.1027, step time: 0.1069\n",
      "66/223, train_loss: 0.0966, step time: 0.1015\n",
      "67/223, train_loss: 0.1080, step time: 0.1169\n",
      "68/223, train_loss: 0.1147, step time: 0.1083\n",
      "69/223, train_loss: 0.1075, step time: 0.1129\n",
      "70/223, train_loss: 0.1038, step time: 0.1095\n",
      "71/223, train_loss: 0.0939, step time: 0.1246\n",
      "72/223, train_loss: 0.1023, step time: 0.1139\n",
      "73/223, train_loss: 0.1026, step time: 0.1096\n",
      "74/223, train_loss: 0.1088, step time: 0.1098\n",
      "75/223, train_loss: 0.1016, step time: 0.1022\n",
      "76/223, train_loss: 0.0983, step time: 0.1018\n",
      "77/223, train_loss: 0.1025, step time: 0.1077\n",
      "78/223, train_loss: 0.0975, step time: 0.1105\n",
      "79/223, train_loss: 0.1029, step time: 0.1006\n",
      "80/223, train_loss: 0.0975, step time: 0.1000\n",
      "81/223, train_loss: 0.1059, step time: 0.1085\n",
      "82/223, train_loss: 0.0937, step time: 0.0995\n",
      "83/223, train_loss: 0.1222, step time: 0.0997\n",
      "84/223, train_loss: 0.1021, step time: 0.1001\n",
      "85/223, train_loss: 0.1059, step time: 0.1053\n",
      "86/223, train_loss: 0.0940, step time: 0.1059\n",
      "87/223, train_loss: 0.0983, step time: 0.1039\n",
      "88/223, train_loss: 0.1148, step time: 0.1100\n",
      "89/223, train_loss: 0.1066, step time: 0.1291\n",
      "90/223, train_loss: 0.1070, step time: 0.1077\n",
      "91/223, train_loss: 0.0954, step time: 0.0998\n",
      "92/223, train_loss: 0.1001, step time: 0.1020\n",
      "93/223, train_loss: 0.1023, step time: 0.1158\n",
      "94/223, train_loss: 0.0964, step time: 0.1000\n",
      "95/223, train_loss: 0.1083, step time: 0.1348\n",
      "96/223, train_loss: 0.0983, step time: 0.1311\n",
      "97/223, train_loss: 0.1078, step time: 0.1686\n",
      "98/223, train_loss: 0.1130, step time: 0.1009\n",
      "99/223, train_loss: 0.1032, step time: 0.1012\n",
      "100/223, train_loss: 0.1021, step time: 0.1004\n",
      "101/223, train_loss: 0.0965, step time: 0.1010\n",
      "102/223, train_loss: 0.1000, step time: 0.1005\n",
      "103/223, train_loss: 0.0945, step time: 0.1112\n",
      "104/223, train_loss: 0.1053, step time: 0.1324\n",
      "105/223, train_loss: 0.0970, step time: 0.1102\n",
      "106/223, train_loss: 0.1033, step time: 0.1010\n",
      "107/223, train_loss: 0.1082, step time: 0.1005\n",
      "108/223, train_loss: 0.1152, step time: 0.1088\n",
      "109/223, train_loss: 0.1208, step time: 0.1147\n",
      "110/223, train_loss: 0.1012, step time: 0.1000\n",
      "111/223, train_loss: 0.1084, step time: 0.1087\n",
      "112/223, train_loss: 0.1073, step time: 0.1048\n",
      "113/223, train_loss: 0.1052, step time: 0.1837\n",
      "114/223, train_loss: 0.1046, step time: 0.1003\n",
      "115/223, train_loss: 0.1030, step time: 0.1117\n",
      "116/223, train_loss: 0.0976, step time: 0.1027\n",
      "117/223, train_loss: 0.1027, step time: 0.1141\n",
      "118/223, train_loss: 0.0998, step time: 0.1002\n",
      "119/223, train_loss: 0.1070, step time: 0.1190\n",
      "120/223, train_loss: 0.0988, step time: 0.1211\n",
      "121/223, train_loss: 0.1072, step time: 0.1034\n",
      "122/223, train_loss: 0.1154, step time: 0.1023\n",
      "123/223, train_loss: 0.1027, step time: 0.1061\n",
      "124/223, train_loss: 0.0977, step time: 0.1123\n",
      "125/223, train_loss: 0.1087, step time: 0.1171\n",
      "126/223, train_loss: 0.0953, step time: 0.1000\n",
      "127/223, train_loss: 0.1031, step time: 0.1016\n",
      "128/223, train_loss: 0.1024, step time: 0.0997\n",
      "129/223, train_loss: 0.1119, step time: 0.1278\n",
      "130/223, train_loss: 0.1049, step time: 0.1288\n",
      "131/223, train_loss: 0.1069, step time: 0.1166\n",
      "132/223, train_loss: 0.1011, step time: 0.0997\n",
      "133/223, train_loss: 0.0995, step time: 0.0999\n",
      "134/223, train_loss: 0.1182, step time: 0.1212\n",
      "135/223, train_loss: 0.0924, step time: 0.1095\n",
      "136/223, train_loss: 0.1158, step time: 0.1092\n",
      "137/223, train_loss: 0.1042, step time: 0.1102\n",
      "138/223, train_loss: 0.1027, step time: 0.1003\n",
      "139/223, train_loss: 0.1007, step time: 0.0997\n",
      "140/223, train_loss: 0.1165, step time: 0.1151\n",
      "141/223, train_loss: 0.1050, step time: 0.1154\n",
      "142/223, train_loss: 0.1068, step time: 0.1111\n",
      "143/223, train_loss: 0.1002, step time: 0.1083\n",
      "144/223, train_loss: 0.1027, step time: 0.1086\n",
      "145/223, train_loss: 0.0928, step time: 0.1172\n",
      "146/223, train_loss: 0.1044, step time: 0.1116\n",
      "147/223, train_loss: 0.1022, step time: 0.1220\n",
      "148/223, train_loss: 0.0934, step time: 0.1203\n",
      "149/223, train_loss: 0.0910, step time: 0.1139\n",
      "150/223, train_loss: 0.1031, step time: 0.1201\n",
      "151/223, train_loss: 0.0963, step time: 0.1152\n",
      "152/223, train_loss: 0.1040, step time: 0.1117\n",
      "153/223, train_loss: 0.0999, step time: 0.1118\n",
      "154/223, train_loss: 0.1057, step time: 0.0997\n",
      "155/223, train_loss: 0.1071, step time: 0.1059\n",
      "156/223, train_loss: 0.1028, step time: 0.1320\n",
      "157/223, train_loss: 0.1111, step time: 0.1149\n",
      "158/223, train_loss: 0.1069, step time: 0.1172\n",
      "159/223, train_loss: 0.1031, step time: 0.1156\n",
      "160/223, train_loss: 0.0952, step time: 0.1261\n",
      "161/223, train_loss: 0.1008, step time: 0.1380\n",
      "162/223, train_loss: 0.1044, step time: 0.1320\n",
      "163/223, train_loss: 0.0889, step time: 0.0998\n",
      "164/223, train_loss: 0.0950, step time: 0.1099\n",
      "165/223, train_loss: 0.1091, step time: 0.1101\n",
      "166/223, train_loss: 0.0984, step time: 0.1224\n",
      "167/223, train_loss: 0.0969, step time: 0.1082\n",
      "168/223, train_loss: 0.1000, step time: 0.1201\n",
      "169/223, train_loss: 0.0957, step time: 0.1329\n",
      "170/223, train_loss: 0.0985, step time: 0.1118\n",
      "171/223, train_loss: 0.0947, step time: 0.1123\n",
      "172/223, train_loss: 0.1058, step time: 0.1255\n",
      "173/223, train_loss: 0.1045, step time: 0.1065\n",
      "174/223, train_loss: 0.0969, step time: 0.1207\n",
      "175/223, train_loss: 0.1230, step time: 0.1279\n",
      "176/223, train_loss: 0.1062, step time: 0.1101\n",
      "177/223, train_loss: 0.0948, step time: 0.1074\n",
      "178/223, train_loss: 0.1011, step time: 0.1017\n",
      "179/223, train_loss: 0.0913, step time: 0.1008\n",
      "180/223, train_loss: 0.1116, step time: 0.0997\n",
      "181/223, train_loss: 0.0947, step time: 0.1129\n",
      "182/223, train_loss: 0.0925, step time: 0.1044\n",
      "183/223, train_loss: 0.1016, step time: 0.1000\n",
      "184/223, train_loss: 0.1155, step time: 0.1150\n",
      "185/223, train_loss: 0.1002, step time: 0.1019\n",
      "186/223, train_loss: 0.1138, step time: 0.1059\n",
      "187/223, train_loss: 0.1133, step time: 0.1132\n",
      "188/223, train_loss: 0.1089, step time: 0.1177\n",
      "189/223, train_loss: 0.0936, step time: 0.1232\n",
      "190/223, train_loss: 0.1056, step time: 0.1229\n",
      "191/223, train_loss: 0.1086, step time: 0.0996\n",
      "192/223, train_loss: 0.0983, step time: 0.1003\n",
      "193/223, train_loss: 0.1008, step time: 0.1009\n",
      "194/223, train_loss: 0.0984, step time: 0.1284\n",
      "195/223, train_loss: 0.1016, step time: 0.1106\n",
      "196/223, train_loss: 0.1008, step time: 0.1137\n",
      "197/223, train_loss: 0.1109, step time: 0.1460\n",
      "198/223, train_loss: 0.1034, step time: 0.1151\n",
      "199/223, train_loss: 0.1065, step time: 0.1314\n",
      "200/223, train_loss: 0.1022, step time: 0.1102\n",
      "201/223, train_loss: 0.1100, step time: 0.1007\n",
      "202/223, train_loss: 0.0891, step time: 0.1001\n",
      "203/223, train_loss: 0.0994, step time: 0.1173\n",
      "204/223, train_loss: 0.0995, step time: 0.1143\n",
      "205/223, train_loss: 0.1004, step time: 0.1358\n",
      "206/223, train_loss: 0.0874, step time: 0.1146\n",
      "207/223, train_loss: 0.1097, step time: 0.1009\n",
      "208/223, train_loss: 0.1162, step time: 0.1385\n",
      "209/223, train_loss: 0.0988, step time: 0.1505\n",
      "210/223, train_loss: 0.1035, step time: 0.1050\n",
      "211/223, train_loss: 0.1024, step time: 0.1152\n",
      "212/223, train_loss: 0.1103, step time: 0.1138\n",
      "213/223, train_loss: 0.0986, step time: 0.1120\n",
      "214/223, train_loss: 0.1067, step time: 0.1002\n",
      "215/223, train_loss: 0.0943, step time: 0.1275\n",
      "216/223, train_loss: 0.1053, step time: 0.1217\n",
      "217/223, train_loss: 0.1114, step time: 0.1172\n",
      "218/223, train_loss: 0.1023, step time: 0.1001\n",
      "219/223, train_loss: 0.1044, step time: 0.1026\n",
      "220/223, train_loss: 0.1046, step time: 0.0996\n",
      "221/223, train_loss: 0.1171, step time: 0.0999\n",
      "222/223, train_loss: 0.0966, step time: 0.1000\n",
      "223/223, train_loss: 0.1059, step time: 0.0995\n",
      "epoch 182 average loss: 0.1043\n",
      "time consuming of epoch 182 is: 87.3460\n",
      "----------\n",
      "epoch 183/300\n",
      "1/223, train_loss: 0.1069, step time: 0.1094\n",
      "2/223, train_loss: 0.0948, step time: 0.1000\n",
      "3/223, train_loss: 0.1022, step time: 0.1118\n",
      "4/223, train_loss: 0.0969, step time: 0.1150\n",
      "5/223, train_loss: 0.0944, step time: 0.1170\n",
      "6/223, train_loss: 0.0975, step time: 0.1078\n",
      "7/223, train_loss: 0.1037, step time: 0.1126\n",
      "8/223, train_loss: 0.1044, step time: 0.1115\n",
      "9/223, train_loss: 0.0996, step time: 0.1048\n",
      "10/223, train_loss: 0.1068, step time: 0.1113\n",
      "11/223, train_loss: 0.1105, step time: 0.1132\n",
      "12/223, train_loss: 0.1099, step time: 0.1117\n",
      "13/223, train_loss: 0.0974, step time: 0.1131\n",
      "14/223, train_loss: 0.1038, step time: 0.1152\n",
      "15/223, train_loss: 0.0913, step time: 0.1077\n",
      "16/223, train_loss: 0.1040, step time: 0.1014\n",
      "17/223, train_loss: 0.1034, step time: 0.1121\n",
      "18/223, train_loss: 0.1104, step time: 0.1126\n",
      "19/223, train_loss: 0.0993, step time: 0.1232\n",
      "20/223, train_loss: 0.0903, step time: 0.1250\n",
      "21/223, train_loss: 0.1153, step time: 0.1022\n",
      "22/223, train_loss: 0.1102, step time: 0.1112\n",
      "23/223, train_loss: 0.1068, step time: 0.1188\n",
      "24/223, train_loss: 0.1023, step time: 0.1152\n",
      "25/223, train_loss: 0.0974, step time: 0.1040\n",
      "26/223, train_loss: 0.1059, step time: 0.1579\n",
      "27/223, train_loss: 0.1065, step time: 0.1156\n",
      "28/223, train_loss: 0.0923, step time: 0.1268\n",
      "29/223, train_loss: 0.1046, step time: 0.1047\n",
      "30/223, train_loss: 0.1076, step time: 0.1002\n",
      "31/223, train_loss: 0.1025, step time: 0.1626\n",
      "32/223, train_loss: 0.1007, step time: 0.1053\n",
      "33/223, train_loss: 0.1029, step time: 0.1045\n",
      "34/223, train_loss: 0.0947, step time: 0.1678\n",
      "35/223, train_loss: 0.0980, step time: 0.1193\n",
      "36/223, train_loss: 0.0996, step time: 0.1109\n",
      "37/223, train_loss: 0.1034, step time: 0.1268\n",
      "38/223, train_loss: 0.1020, step time: 0.0995\n",
      "39/223, train_loss: 0.1053, step time: 0.1367\n",
      "40/223, train_loss: 0.0954, step time: 0.1000\n",
      "41/223, train_loss: 0.1089, step time: 0.1217\n",
      "42/223, train_loss: 0.1035, step time: 0.1578\n",
      "43/223, train_loss: 0.0911, step time: 0.1361\n",
      "44/223, train_loss: 0.1090, step time: 0.1146\n",
      "45/223, train_loss: 0.1024, step time: 0.1126\n",
      "46/223, train_loss: 0.0969, step time: 0.1116\n",
      "47/223, train_loss: 0.1121, step time: 0.1304\n",
      "48/223, train_loss: 0.1022, step time: 0.1172\n",
      "49/223, train_loss: 0.1015, step time: 0.1729\n",
      "50/223, train_loss: 0.0923, step time: 0.1251\n",
      "51/223, train_loss: 0.1010, step time: 0.1185\n",
      "52/223, train_loss: 0.0998, step time: 0.1173\n",
      "53/223, train_loss: 0.0984, step time: 0.1014\n",
      "54/223, train_loss: 0.1100, step time: 0.1439\n",
      "55/223, train_loss: 0.0983, step time: 0.1145\n",
      "56/223, train_loss: 0.1112, step time: 0.1155\n",
      "57/223, train_loss: 0.0956, step time: 0.1082\n",
      "58/223, train_loss: 0.1013, step time: 0.1513\n",
      "59/223, train_loss: 0.1224, step time: 0.1289\n",
      "60/223, train_loss: 0.0978, step time: 0.1001\n",
      "61/223, train_loss: 0.1064, step time: 0.1030\n",
      "62/223, train_loss: 0.1056, step time: 0.0998\n",
      "63/223, train_loss: 0.1101, step time: 0.1008\n",
      "64/223, train_loss: 0.1100, step time: 0.1006\n",
      "65/223, train_loss: 0.1008, step time: 0.1061\n",
      "66/223, train_loss: 0.1065, step time: 0.1272\n",
      "67/223, train_loss: 0.1046, step time: 0.1087\n",
      "68/223, train_loss: 0.1033, step time: 0.1076\n",
      "69/223, train_loss: 0.1038, step time: 0.1219\n",
      "70/223, train_loss: 0.0994, step time: 0.1073\n",
      "71/223, train_loss: 0.1033, step time: 0.1019\n",
      "72/223, train_loss: 0.1270, step time: 0.1046\n",
      "73/223, train_loss: 0.0924, step time: 0.1053\n",
      "74/223, train_loss: 0.0972, step time: 0.1062\n",
      "75/223, train_loss: 0.1001, step time: 0.1102\n",
      "76/223, train_loss: 0.1217, step time: 0.1207\n",
      "77/223, train_loss: 0.1043, step time: 0.1056\n",
      "78/223, train_loss: 0.1043, step time: 0.1001\n",
      "79/223, train_loss: 0.1020, step time: 0.1265\n",
      "80/223, train_loss: 0.1010, step time: 0.1005\n",
      "81/223, train_loss: 0.0987, step time: 0.1175\n",
      "82/223, train_loss: 0.1171, step time: 0.1287\n",
      "83/223, train_loss: 0.1052, step time: 0.1010\n",
      "84/223, train_loss: 0.1114, step time: 0.1023\n",
      "85/223, train_loss: 0.1027, step time: 0.1107\n",
      "86/223, train_loss: 0.0976, step time: 0.1155\n",
      "87/223, train_loss: 0.1015, step time: 0.1048\n",
      "88/223, train_loss: 0.1008, step time: 0.1002\n",
      "89/223, train_loss: 0.1014, step time: 0.1246\n",
      "90/223, train_loss: 0.0991, step time: 0.1120\n",
      "91/223, train_loss: 0.0972, step time: 0.1209\n",
      "92/223, train_loss: 0.1019, step time: 0.1088\n",
      "93/223, train_loss: 0.0918, step time: 0.1182\n",
      "94/223, train_loss: 0.0979, step time: 0.1076\n",
      "95/223, train_loss: 0.1035, step time: 0.1012\n",
      "96/223, train_loss: 0.3074, step time: 0.1503\n",
      "97/223, train_loss: 0.1014, step time: 0.1266\n",
      "98/223, train_loss: 0.0910, step time: 0.1252\n",
      "99/223, train_loss: 0.1112, step time: 0.1001\n",
      "100/223, train_loss: 0.0923, step time: 0.1020\n",
      "101/223, train_loss: 0.0922, step time: 0.1137\n",
      "102/223, train_loss: 0.0977, step time: 0.1188\n",
      "103/223, train_loss: 0.0904, step time: 0.1309\n",
      "104/223, train_loss: 0.1076, step time: 0.1130\n",
      "105/223, train_loss: 0.1126, step time: 0.1222\n",
      "106/223, train_loss: 0.0962, step time: 0.1122\n",
      "107/223, train_loss: 0.1009, step time: 0.1149\n",
      "108/223, train_loss: 0.0923, step time: 0.1327\n",
      "109/223, train_loss: 0.0908, step time: 0.1087\n",
      "110/223, train_loss: 0.1023, step time: 0.1112\n",
      "111/223, train_loss: 0.1156, step time: 0.1136\n",
      "112/223, train_loss: 0.0938, step time: 0.1165\n",
      "113/223, train_loss: 0.1132, step time: 0.1089\n",
      "114/223, train_loss: 0.1008, step time: 0.1069\n",
      "115/223, train_loss: 0.1127, step time: 0.1003\n",
      "116/223, train_loss: 0.0966, step time: 0.1015\n",
      "117/223, train_loss: 0.1091, step time: 0.1102\n",
      "118/223, train_loss: 0.0952, step time: 0.1077\n",
      "119/223, train_loss: 0.0904, step time: 0.1010\n",
      "120/223, train_loss: 0.1082, step time: 0.1061\n",
      "121/223, train_loss: 0.1061, step time: 0.1141\n",
      "122/223, train_loss: 0.1094, step time: 0.1555\n",
      "123/223, train_loss: 0.0972, step time: 0.1416\n",
      "124/223, train_loss: 0.0988, step time: 0.1101\n",
      "125/223, train_loss: 0.1042, step time: 0.1092\n",
      "126/223, train_loss: 0.1056, step time: 0.1009\n",
      "127/223, train_loss: 0.1150, step time: 0.1011\n",
      "128/223, train_loss: 0.1071, step time: 0.1044\n",
      "129/223, train_loss: 0.1019, step time: 0.1141\n",
      "130/223, train_loss: 0.1034, step time: 0.0996\n",
      "131/223, train_loss: 0.1061, step time: 0.1006\n",
      "132/223, train_loss: 0.1103, step time: 0.1022\n",
      "133/223, train_loss: 0.1138, step time: 0.1058\n",
      "134/223, train_loss: 0.1190, step time: 0.1420\n",
      "135/223, train_loss: 0.1025, step time: 0.1270\n",
      "136/223, train_loss: 0.1066, step time: 0.1180\n",
      "137/223, train_loss: 0.0947, step time: 0.1193\n",
      "138/223, train_loss: 0.1060, step time: 0.1169\n",
      "139/223, train_loss: 0.1018, step time: 0.1071\n",
      "140/223, train_loss: 0.1000, step time: 0.1031\n",
      "141/223, train_loss: 0.1096, step time: 0.0995\n",
      "142/223, train_loss: 0.1119, step time: 0.0999\n",
      "143/223, train_loss: 0.0918, step time: 0.1006\n",
      "144/223, train_loss: 0.0944, step time: 0.1009\n",
      "145/223, train_loss: 0.1061, step time: 0.0992\n",
      "146/223, train_loss: 0.1153, step time: 0.0990\n",
      "147/223, train_loss: 0.1067, step time: 0.1001\n",
      "148/223, train_loss: 0.1054, step time: 0.1065\n",
      "149/223, train_loss: 0.1081, step time: 0.0998\n",
      "150/223, train_loss: 0.1002, step time: 0.1001\n",
      "151/223, train_loss: 0.1115, step time: 0.1002\n",
      "152/223, train_loss: 0.1163, step time: 0.1006\n",
      "153/223, train_loss: 0.1130, step time: 0.0999\n",
      "154/223, train_loss: 0.1143, step time: 0.1010\n",
      "155/223, train_loss: 0.0930, step time: 0.1164\n",
      "156/223, train_loss: 0.1051, step time: 0.1027\n",
      "157/223, train_loss: 0.0929, step time: 0.1100\n",
      "158/223, train_loss: 0.0969, step time: 0.1159\n",
      "159/223, train_loss: 0.1012, step time: 0.1240\n",
      "160/223, train_loss: 0.1049, step time: 0.1097\n",
      "161/223, train_loss: 0.1017, step time: 0.1133\n",
      "162/223, train_loss: 0.1087, step time: 0.0990\n",
      "163/223, train_loss: 0.0993, step time: 0.0994\n",
      "164/223, train_loss: 0.0985, step time: 0.1018\n",
      "165/223, train_loss: 0.1000, step time: 0.1166\n",
      "166/223, train_loss: 0.1185, step time: 0.1082\n",
      "167/223, train_loss: 0.0959, step time: 0.1041\n",
      "168/223, train_loss: 0.0934, step time: 0.1004\n",
      "169/223, train_loss: 0.1071, step time: 0.1113\n",
      "170/223, train_loss: 0.0913, step time: 0.1108\n",
      "171/223, train_loss: 0.1111, step time: 0.1023\n",
      "172/223, train_loss: 0.0967, step time: 0.1051\n",
      "173/223, train_loss: 0.1033, step time: 0.1172\n",
      "174/223, train_loss: 0.1153, step time: 0.1193\n",
      "175/223, train_loss: 0.1122, step time: 0.1142\n",
      "176/223, train_loss: 0.1000, step time: 0.1005\n",
      "177/223, train_loss: 0.1156, step time: 0.1228\n",
      "178/223, train_loss: 0.1033, step time: 0.0996\n",
      "179/223, train_loss: 0.1003, step time: 0.1086\n",
      "180/223, train_loss: 0.0967, step time: 0.0996\n",
      "181/223, train_loss: 0.1017, step time: 0.1035\n",
      "182/223, train_loss: 0.0957, step time: 0.1197\n",
      "183/223, train_loss: 0.1087, step time: 0.1189\n",
      "184/223, train_loss: 0.1042, step time: 0.1199\n",
      "185/223, train_loss: 0.0950, step time: 0.1074\n",
      "186/223, train_loss: 0.0951, step time: 0.1234\n",
      "187/223, train_loss: 0.0981, step time: 0.1171\n",
      "188/223, train_loss: 0.1056, step time: 0.1137\n",
      "189/223, train_loss: 0.1022, step time: 0.1197\n",
      "190/223, train_loss: 0.0990, step time: 0.1126\n",
      "191/223, train_loss: 0.1114, step time: 0.1017\n",
      "192/223, train_loss: 0.1021, step time: 0.1013\n",
      "193/223, train_loss: 0.0933, step time: 0.1159\n",
      "194/223, train_loss: 0.1007, step time: 0.1139\n",
      "195/223, train_loss: 0.1154, step time: 0.1148\n",
      "196/223, train_loss: 0.0992, step time: 0.1104\n",
      "197/223, train_loss: 0.0955, step time: 0.1507\n",
      "198/223, train_loss: 0.1100, step time: 0.1189\n",
      "199/223, train_loss: 0.0953, step time: 0.1120\n",
      "200/223, train_loss: 0.1037, step time: 0.1087\n",
      "201/223, train_loss: 0.1063, step time: 0.1001\n",
      "202/223, train_loss: 0.0978, step time: 0.1000\n",
      "203/223, train_loss: 0.1055, step time: 0.1143\n",
      "204/223, train_loss: 0.1053, step time: 0.1093\n",
      "205/223, train_loss: 0.1180, step time: 0.1038\n",
      "206/223, train_loss: 0.1004, step time: 0.1130\n",
      "207/223, train_loss: 0.1069, step time: 0.1187\n",
      "208/223, train_loss: 0.1000, step time: 0.1637\n",
      "209/223, train_loss: 0.1010, step time: 0.1199\n",
      "210/223, train_loss: 0.1110, step time: 0.1053\n",
      "211/223, train_loss: 0.1064, step time: 0.1012\n",
      "212/223, train_loss: 0.1052, step time: 0.0999\n",
      "213/223, train_loss: 0.1115, step time: 0.1158\n",
      "214/223, train_loss: 0.1069, step time: 0.1145\n",
      "215/223, train_loss: 0.0990, step time: 0.1171\n",
      "216/223, train_loss: 0.1027, step time: 0.1036\n",
      "217/223, train_loss: 0.0946, step time: 0.1123\n",
      "218/223, train_loss: 0.1135, step time: 0.1080\n",
      "219/223, train_loss: 0.1047, step time: 0.1137\n",
      "220/223, train_loss: 0.0966, step time: 0.1114\n",
      "221/223, train_loss: 0.1032, step time: 0.1015\n",
      "222/223, train_loss: 0.1004, step time: 0.0994\n",
      "223/223, train_loss: 0.0965, step time: 0.0995\n",
      "epoch 183 average loss: 0.1040\n",
      "time consuming of epoch 183 is: 91.1526\n",
      "----------\n",
      "epoch 184/300\n",
      "1/223, train_loss: 0.0928, step time: 0.1072\n",
      "2/223, train_loss: 0.1072, step time: 0.1117\n",
      "3/223, train_loss: 0.0981, step time: 0.1114\n",
      "4/223, train_loss: 0.1059, step time: 0.1180\n",
      "5/223, train_loss: 0.1021, step time: 0.1115\n",
      "6/223, train_loss: 0.1076, step time: 0.1054\n",
      "7/223, train_loss: 0.0975, step time: 0.1173\n",
      "8/223, train_loss: 0.0953, step time: 0.1135\n",
      "9/223, train_loss: 0.0994, step time: 0.1134\n",
      "10/223, train_loss: 0.0965, step time: 0.1246\n",
      "11/223, train_loss: 0.1119, step time: 0.1167\n",
      "12/223, train_loss: 0.0933, step time: 0.1093\n",
      "13/223, train_loss: 0.1033, step time: 0.1126\n",
      "14/223, train_loss: 0.1024, step time: 0.1012\n",
      "15/223, train_loss: 0.1055, step time: 0.1163\n",
      "16/223, train_loss: 0.1192, step time: 0.1077\n",
      "17/223, train_loss: 0.0928, step time: 0.1134\n",
      "18/223, train_loss: 0.0960, step time: 0.1005\n",
      "19/223, train_loss: 0.0984, step time: 0.1003\n",
      "20/223, train_loss: 0.1041, step time: 0.1151\n",
      "21/223, train_loss: 0.1009, step time: 0.0985\n",
      "22/223, train_loss: 0.0977, step time: 0.1001\n",
      "23/223, train_loss: 0.1025, step time: 0.0994\n",
      "24/223, train_loss: 0.1167, step time: 0.0997\n",
      "25/223, train_loss: 0.1002, step time: 0.1155\n",
      "26/223, train_loss: 0.1036, step time: 0.1113\n",
      "27/223, train_loss: 0.0998, step time: 0.1010\n",
      "28/223, train_loss: 0.0995, step time: 0.1005\n",
      "29/223, train_loss: 0.0952, step time: 0.1063\n",
      "30/223, train_loss: 0.1057, step time: 0.1070\n",
      "31/223, train_loss: 0.1118, step time: 0.1006\n",
      "32/223, train_loss: 0.1056, step time: 0.1022\n",
      "33/223, train_loss: 0.1025, step time: 0.1001\n",
      "34/223, train_loss: 0.0950, step time: 0.1051\n",
      "35/223, train_loss: 0.1073, step time: 0.1001\n",
      "36/223, train_loss: 0.0953, step time: 0.1095\n",
      "37/223, train_loss: 0.1179, step time: 0.1175\n",
      "38/223, train_loss: 0.1057, step time: 0.1001\n",
      "39/223, train_loss: 0.1034, step time: 0.1004\n",
      "40/223, train_loss: 0.0957, step time: 0.1008\n",
      "41/223, train_loss: 0.0988, step time: 0.1049\n",
      "42/223, train_loss: 0.0939, step time: 0.1003\n",
      "43/223, train_loss: 0.1046, step time: 0.1152\n",
      "44/223, train_loss: 0.1077, step time: 0.1181\n",
      "45/223, train_loss: 0.1025, step time: 0.1127\n",
      "46/223, train_loss: 0.0959, step time: 0.1004\n",
      "47/223, train_loss: 0.1026, step time: 0.1007\n",
      "48/223, train_loss: 0.1090, step time: 0.1004\n",
      "49/223, train_loss: 0.0983, step time: 0.1068\n",
      "50/223, train_loss: 0.1036, step time: 0.1178\n",
      "51/223, train_loss: 0.1055, step time: 0.1124\n",
      "52/223, train_loss: 0.0974, step time: 0.1136\n",
      "53/223, train_loss: 0.0978, step time: 0.1099\n",
      "54/223, train_loss: 0.0926, step time: 0.1156\n",
      "55/223, train_loss: 0.1088, step time: 0.1237\n",
      "56/223, train_loss: 0.1103, step time: 0.1234\n",
      "57/223, train_loss: 0.0991, step time: 0.1058\n",
      "58/223, train_loss: 0.1017, step time: 0.1050\n",
      "59/223, train_loss: 0.0987, step time: 0.1006\n",
      "60/223, train_loss: 0.1017, step time: 0.0996\n",
      "61/223, train_loss: 0.1053, step time: 0.1103\n",
      "62/223, train_loss: 0.0927, step time: 0.1064\n",
      "63/223, train_loss: 0.0921, step time: 0.1109\n",
      "64/223, train_loss: 0.1052, step time: 0.1287\n",
      "65/223, train_loss: 0.1099, step time: 0.1091\n",
      "66/223, train_loss: 0.0903, step time: 0.1227\n",
      "67/223, train_loss: 0.0981, step time: 0.1091\n",
      "68/223, train_loss: 0.0967, step time: 0.1086\n",
      "69/223, train_loss: 0.0969, step time: 0.1113\n",
      "70/223, train_loss: 0.1017, step time: 0.1159\n",
      "71/223, train_loss: 0.1054, step time: 0.1112\n",
      "72/223, train_loss: 0.1050, step time: 0.1087\n",
      "73/223, train_loss: 0.1069, step time: 0.1065\n",
      "74/223, train_loss: 0.1149, step time: 0.1199\n",
      "75/223, train_loss: 0.0993, step time: 0.1079\n",
      "76/223, train_loss: 0.1060, step time: 0.1475\n",
      "77/223, train_loss: 0.1059, step time: 0.1178\n",
      "78/223, train_loss: 0.0949, step time: 0.1088\n",
      "79/223, train_loss: 0.1022, step time: 0.1370\n",
      "80/223, train_loss: 0.3058, step time: 0.1006\n",
      "81/223, train_loss: 0.0993, step time: 0.1009\n",
      "82/223, train_loss: 0.1109, step time: 0.1015\n",
      "83/223, train_loss: 0.1063, step time: 0.1103\n",
      "84/223, train_loss: 0.0935, step time: 0.1002\n",
      "85/223, train_loss: 0.0956, step time: 0.1004\n",
      "86/223, train_loss: 0.1085, step time: 0.1271\n",
      "87/223, train_loss: 0.1187, step time: 0.1098\n",
      "88/223, train_loss: 0.1164, step time: 0.1157\n",
      "89/223, train_loss: 0.0978, step time: 0.1235\n",
      "90/223, train_loss: 0.0947, step time: 0.1208\n",
      "91/223, train_loss: 0.1121, step time: 0.1248\n",
      "92/223, train_loss: 0.1012, step time: 0.0996\n",
      "93/223, train_loss: 0.1119, step time: 0.1028\n",
      "94/223, train_loss: 0.1016, step time: 0.1289\n",
      "95/223, train_loss: 0.1070, step time: 0.1191\n",
      "96/223, train_loss: 0.1067, step time: 0.1026\n",
      "97/223, train_loss: 0.1120, step time: 0.1009\n",
      "98/223, train_loss: 0.0966, step time: 0.1000\n",
      "99/223, train_loss: 0.1054, step time: 0.1269\n",
      "100/223, train_loss: 0.1046, step time: 0.1162\n",
      "101/223, train_loss: 0.1128, step time: 0.0998\n",
      "102/223, train_loss: 0.1123, step time: 0.1240\n",
      "103/223, train_loss: 0.1081, step time: 0.1105\n",
      "104/223, train_loss: 0.0976, step time: 0.1115\n",
      "105/223, train_loss: 0.0916, step time: 0.1099\n",
      "106/223, train_loss: 0.0952, step time: 0.1118\n",
      "107/223, train_loss: 0.1079, step time: 0.1162\n",
      "108/223, train_loss: 0.0918, step time: 0.1103\n",
      "109/223, train_loss: 0.1019, step time: 0.1332\n",
      "110/223, train_loss: 0.1087, step time: 0.1089\n",
      "111/223, train_loss: 0.1232, step time: 0.1483\n",
      "112/223, train_loss: 0.1109, step time: 0.1387\n",
      "113/223, train_loss: 0.1003, step time: 0.1135\n",
      "114/223, train_loss: 0.0963, step time: 0.1166\n",
      "115/223, train_loss: 0.1131, step time: 0.1137\n",
      "116/223, train_loss: 0.1006, step time: 0.1147\n",
      "117/223, train_loss: 0.0990, step time: 0.1241\n",
      "118/223, train_loss: 0.1019, step time: 0.1000\n",
      "119/223, train_loss: 0.0979, step time: 0.1078\n",
      "120/223, train_loss: 0.1047, step time: 0.1301\n",
      "121/223, train_loss: 0.1133, step time: 0.1080\n",
      "122/223, train_loss: 0.1063, step time: 0.1110\n",
      "123/223, train_loss: 0.0934, step time: 0.1224\n",
      "124/223, train_loss: 0.0952, step time: 0.1038\n",
      "125/223, train_loss: 0.0952, step time: 0.1256\n",
      "126/223, train_loss: 0.0985, step time: 0.1015\n",
      "127/223, train_loss: 0.1050, step time: 0.1248\n",
      "128/223, train_loss: 0.1007, step time: 0.1244\n",
      "129/223, train_loss: 0.0913, step time: 0.1182\n",
      "130/223, train_loss: 0.0995, step time: 0.1085\n",
      "131/223, train_loss: 0.0967, step time: 0.1171\n",
      "132/223, train_loss: 0.1007, step time: 0.1207\n",
      "133/223, train_loss: 0.0951, step time: 0.1079\n",
      "134/223, train_loss: 0.0920, step time: 0.1365\n",
      "135/223, train_loss: 0.1048, step time: 0.1082\n",
      "136/223, train_loss: 0.0934, step time: 0.1154\n",
      "137/223, train_loss: 0.0968, step time: 0.1183\n",
      "138/223, train_loss: 0.0908, step time: 0.1153\n",
      "139/223, train_loss: 0.1037, step time: 0.1048\n",
      "140/223, train_loss: 0.1070, step time: 0.1295\n",
      "141/223, train_loss: 0.1115, step time: 0.1009\n",
      "142/223, train_loss: 0.1006, step time: 0.1223\n",
      "143/223, train_loss: 0.1113, step time: 0.1050\n",
      "144/223, train_loss: 0.0992, step time: 0.1205\n",
      "145/223, train_loss: 0.1054, step time: 0.1157\n",
      "146/223, train_loss: 0.0980, step time: 0.1071\n",
      "147/223, train_loss: 0.1202, step time: 0.1072\n",
      "148/223, train_loss: 0.0935, step time: 0.1046\n",
      "149/223, train_loss: 0.1129, step time: 0.1005\n",
      "150/223, train_loss: 0.1222, step time: 0.1059\n",
      "151/223, train_loss: 0.1013, step time: 0.1156\n",
      "152/223, train_loss: 0.1024, step time: 0.1003\n",
      "153/223, train_loss: 0.1001, step time: 0.1094\n",
      "154/223, train_loss: 0.0946, step time: 0.1091\n",
      "155/223, train_loss: 0.1055, step time: 0.1010\n",
      "156/223, train_loss: 0.1021, step time: 0.0996\n",
      "157/223, train_loss: 0.1088, step time: 0.1043\n",
      "158/223, train_loss: 0.1009, step time: 0.1003\n",
      "159/223, train_loss: 0.1094, step time: 0.1084\n",
      "160/223, train_loss: 0.1094, step time: 0.1194\n",
      "161/223, train_loss: 0.0914, step time: 0.1026\n",
      "162/223, train_loss: 0.0933, step time: 0.1442\n",
      "163/223, train_loss: 0.1106, step time: 0.0991\n",
      "164/223, train_loss: 0.1150, step time: 0.0997\n",
      "165/223, train_loss: 0.1011, step time: 0.1119\n",
      "166/223, train_loss: 0.1047, step time: 0.1011\n",
      "167/223, train_loss: 0.1140, step time: 0.1060\n",
      "168/223, train_loss: 0.0973, step time: 0.1274\n",
      "169/223, train_loss: 0.1011, step time: 0.1117\n",
      "170/223, train_loss: 0.0908, step time: 0.1199\n",
      "171/223, train_loss: 0.1097, step time: 0.1006\n",
      "172/223, train_loss: 0.1005, step time: 0.0997\n",
      "173/223, train_loss: 0.1035, step time: 0.1000\n",
      "174/223, train_loss: 0.0997, step time: 0.1208\n",
      "175/223, train_loss: 0.1108, step time: 0.1002\n",
      "176/223, train_loss: 0.1061, step time: 0.0998\n",
      "177/223, train_loss: 0.1051, step time: 0.1000\n",
      "178/223, train_loss: 0.1001, step time: 0.1236\n",
      "179/223, train_loss: 0.1067, step time: 0.1106\n",
      "180/223, train_loss: 0.1081, step time: 0.0998\n",
      "181/223, train_loss: 0.1081, step time: 0.1046\n",
      "182/223, train_loss: 0.1008, step time: 0.1370\n",
      "183/223, train_loss: 0.0904, step time: 0.1363\n",
      "184/223, train_loss: 0.0931, step time: 0.0997\n",
      "185/223, train_loss: 0.0913, step time: 0.1111\n",
      "186/223, train_loss: 0.0977, step time: 0.1179\n",
      "187/223, train_loss: 0.1018, step time: 0.1087\n",
      "188/223, train_loss: 0.1122, step time: 0.1194\n",
      "189/223, train_loss: 0.1068, step time: 0.1432\n",
      "190/223, train_loss: 0.0967, step time: 0.1160\n",
      "191/223, train_loss: 0.1002, step time: 0.1103\n",
      "192/223, train_loss: 0.1108, step time: 0.1060\n",
      "193/223, train_loss: 0.1131, step time: 0.1062\n",
      "194/223, train_loss: 0.0941, step time: 0.1013\n",
      "195/223, train_loss: 0.1077, step time: 0.1002\n",
      "196/223, train_loss: 0.1204, step time: 0.1004\n",
      "197/223, train_loss: 0.1082, step time: 0.1105\n",
      "198/223, train_loss: 0.1208, step time: 0.1074\n",
      "199/223, train_loss: 0.0984, step time: 0.1003\n",
      "200/223, train_loss: 0.1009, step time: 0.0995\n",
      "201/223, train_loss: 0.0915, step time: 0.1136\n",
      "202/223, train_loss: 0.1063, step time: 0.1017\n",
      "203/223, train_loss: 0.1047, step time: 0.0998\n",
      "204/223, train_loss: 0.1043, step time: 0.1004\n",
      "205/223, train_loss: 0.1048, step time: 0.1030\n",
      "206/223, train_loss: 0.0997, step time: 0.1365\n",
      "207/223, train_loss: 0.0928, step time: 0.1047\n",
      "208/223, train_loss: 0.1083, step time: 0.1215\n",
      "209/223, train_loss: 0.0987, step time: 0.1120\n",
      "210/223, train_loss: 0.1106, step time: 0.1347\n",
      "211/223, train_loss: 0.1035, step time: 0.1238\n",
      "212/223, train_loss: 0.1017, step time: 0.1023\n",
      "213/223, train_loss: 0.1008, step time: 0.1154\n",
      "214/223, train_loss: 0.1128, step time: 0.1480\n",
      "215/223, train_loss: 0.0974, step time: 0.1178\n",
      "216/223, train_loss: 0.1084, step time: 0.1145\n",
      "217/223, train_loss: 0.0996, step time: 0.1119\n",
      "218/223, train_loss: 0.1022, step time: 0.1005\n",
      "219/223, train_loss: 0.0975, step time: 0.1001\n",
      "220/223, train_loss: 0.0999, step time: 0.1011\n",
      "221/223, train_loss: 0.0997, step time: 0.1032\n",
      "222/223, train_loss: 0.1087, step time: 0.1003\n",
      "223/223, train_loss: 0.1153, step time: 0.0999\n",
      "epoch 184 average loss: 0.1037\n",
      "time consuming of epoch 184 is: 90.7653\n",
      "----------\n",
      "epoch 185/300\n",
      "1/223, train_loss: 0.0965, step time: 0.1039\n",
      "2/223, train_loss: 0.1063, step time: 0.1030\n",
      "3/223, train_loss: 0.1108, step time: 0.1086\n",
      "4/223, train_loss: 0.1121, step time: 0.1056\n",
      "5/223, train_loss: 0.1067, step time: 0.1070\n",
      "6/223, train_loss: 0.1102, step time: 0.0993\n",
      "7/223, train_loss: 0.1046, step time: 0.0993\n",
      "8/223, train_loss: 0.1030, step time: 0.1225\n",
      "9/223, train_loss: 0.0969, step time: 0.1061\n",
      "10/223, train_loss: 0.0992, step time: 0.1066\n",
      "11/223, train_loss: 0.1010, step time: 0.1077\n",
      "12/223, train_loss: 0.0925, step time: 0.1017\n",
      "13/223, train_loss: 0.1072, step time: 0.1009\n",
      "14/223, train_loss: 0.1034, step time: 0.1011\n",
      "15/223, train_loss: 0.1070, step time: 0.1009\n",
      "16/223, train_loss: 0.3091, step time: 0.1011\n",
      "17/223, train_loss: 0.1119, step time: 0.1100\n",
      "18/223, train_loss: 0.1208, step time: 0.1171\n",
      "19/223, train_loss: 0.1074, step time: 0.1055\n",
      "20/223, train_loss: 0.0933, step time: 0.1093\n",
      "21/223, train_loss: 0.0972, step time: 0.1153\n",
      "22/223, train_loss: 0.1012, step time: 0.1176\n",
      "23/223, train_loss: 0.1131, step time: 0.1046\n",
      "24/223, train_loss: 0.0988, step time: 0.1035\n",
      "25/223, train_loss: 0.1103, step time: 0.1135\n",
      "26/223, train_loss: 0.0959, step time: 0.1041\n",
      "27/223, train_loss: 0.0936, step time: 0.1221\n",
      "28/223, train_loss: 0.1073, step time: 0.1068\n",
      "29/223, train_loss: 0.0980, step time: 0.1396\n",
      "30/223, train_loss: 0.1001, step time: 0.1006\n",
      "31/223, train_loss: 0.1209, step time: 0.1512\n",
      "32/223, train_loss: 0.1114, step time: 0.1091\n",
      "33/223, train_loss: 0.0979, step time: 0.1074\n",
      "34/223, train_loss: 0.1030, step time: 0.1104\n",
      "35/223, train_loss: 0.0911, step time: 0.1008\n",
      "36/223, train_loss: 0.0905, step time: 0.1003\n",
      "37/223, train_loss: 0.1101, step time: 0.1297\n",
      "38/223, train_loss: 0.1046, step time: 0.1007\n",
      "39/223, train_loss: 0.0952, step time: 0.1208\n",
      "40/223, train_loss: 0.0983, step time: 0.1005\n",
      "41/223, train_loss: 0.1054, step time: 0.1113\n",
      "42/223, train_loss: 0.1017, step time: 0.1136\n",
      "43/223, train_loss: 0.1034, step time: 0.1011\n",
      "44/223, train_loss: 0.1045, step time: 0.1004\n",
      "45/223, train_loss: 0.0946, step time: 0.1026\n",
      "46/223, train_loss: 0.0965, step time: 0.1033\n",
      "47/223, train_loss: 0.0986, step time: 0.1146\n",
      "48/223, train_loss: 0.0967, step time: 0.1015\n",
      "49/223, train_loss: 0.1009, step time: 0.1158\n",
      "50/223, train_loss: 0.0948, step time: 0.1439\n",
      "51/223, train_loss: 0.1153, step time: 0.1160\n",
      "52/223, train_loss: 0.0941, step time: 0.1097\n",
      "53/223, train_loss: 0.1097, step time: 0.1207\n",
      "54/223, train_loss: 0.1041, step time: 0.1306\n",
      "55/223, train_loss: 0.1022, step time: 0.1057\n",
      "56/223, train_loss: 0.1094, step time: 0.0998\n",
      "57/223, train_loss: 0.1023, step time: 0.1060\n",
      "58/223, train_loss: 0.1067, step time: 0.1005\n",
      "59/223, train_loss: 0.0953, step time: 0.1007\n",
      "60/223, train_loss: 0.1051, step time: 0.1015\n",
      "61/223, train_loss: 0.0959, step time: 0.1113\n",
      "62/223, train_loss: 0.0989, step time: 0.1235\n",
      "63/223, train_loss: 0.0975, step time: 0.1026\n",
      "64/223, train_loss: 0.1080, step time: 0.1017\n",
      "65/223, train_loss: 0.1039, step time: 0.1123\n",
      "66/223, train_loss: 0.0977, step time: 0.1057\n",
      "67/223, train_loss: 0.1052, step time: 0.1009\n",
      "68/223, train_loss: 0.1072, step time: 0.1013\n",
      "69/223, train_loss: 0.0875, step time: 0.1239\n",
      "70/223, train_loss: 0.0961, step time: 0.1003\n",
      "71/223, train_loss: 0.1019, step time: 0.1088\n",
      "72/223, train_loss: 0.0965, step time: 0.1013\n",
      "73/223, train_loss: 0.1034, step time: 0.1147\n",
      "74/223, train_loss: 0.0964, step time: 0.1083\n",
      "75/223, train_loss: 0.1161, step time: 0.1176\n",
      "76/223, train_loss: 0.1042, step time: 0.1231\n",
      "77/223, train_loss: 0.1029, step time: 0.1011\n",
      "78/223, train_loss: 0.0998, step time: 0.1157\n",
      "79/223, train_loss: 0.1032, step time: 0.1194\n",
      "80/223, train_loss: 0.0976, step time: 0.1081\n",
      "81/223, train_loss: 0.1031, step time: 0.1102\n",
      "82/223, train_loss: 0.0952, step time: 0.1062\n",
      "83/223, train_loss: 0.1000, step time: 0.1087\n",
      "84/223, train_loss: 0.1037, step time: 0.1161\n",
      "85/223, train_loss: 0.1017, step time: 0.1161\n",
      "86/223, train_loss: 0.0935, step time: 0.1067\n",
      "87/223, train_loss: 0.0994, step time: 0.1006\n",
      "88/223, train_loss: 0.1199, step time: 0.1007\n",
      "89/223, train_loss: 0.1037, step time: 0.1133\n",
      "90/223, train_loss: 0.1020, step time: 0.1005\n",
      "91/223, train_loss: 0.1008, step time: 0.1082\n",
      "92/223, train_loss: 0.1093, step time: 0.1151\n",
      "93/223, train_loss: 0.1005, step time: 0.1035\n",
      "94/223, train_loss: 0.1030, step time: 0.1155\n",
      "95/223, train_loss: 0.1037, step time: 0.1003\n",
      "96/223, train_loss: 0.1045, step time: 0.1163\n",
      "97/223, train_loss: 0.1005, step time: 0.0996\n",
      "98/223, train_loss: 0.0908, step time: 0.1193\n",
      "99/223, train_loss: 0.1241, step time: 0.1042\n",
      "100/223, train_loss: 0.0991, step time: 0.1009\n",
      "101/223, train_loss: 0.1016, step time: 0.1007\n",
      "102/223, train_loss: 0.1030, step time: 0.1115\n",
      "103/223, train_loss: 0.1066, step time: 0.1151\n",
      "104/223, train_loss: 0.0958, step time: 0.1001\n",
      "105/223, train_loss: 0.1024, step time: 0.1003\n",
      "106/223, train_loss: 0.0941, step time: 0.0997\n",
      "107/223, train_loss: 0.1033, step time: 0.1015\n",
      "108/223, train_loss: 0.1035, step time: 0.1028\n",
      "109/223, train_loss: 0.0966, step time: 0.1173\n",
      "110/223, train_loss: 0.1169, step time: 0.1072\n",
      "111/223, train_loss: 0.0962, step time: 0.1104\n",
      "112/223, train_loss: 0.1086, step time: 0.1369\n",
      "113/223, train_loss: 0.1061, step time: 0.1076\n",
      "114/223, train_loss: 0.0927, step time: 0.1133\n",
      "115/223, train_loss: 0.0989, step time: 0.1143\n",
      "116/223, train_loss: 0.1033, step time: 0.1112\n",
      "117/223, train_loss: 0.1089, step time: 0.1011\n",
      "118/223, train_loss: 0.0980, step time: 0.1320\n",
      "119/223, train_loss: 0.0942, step time: 0.1055\n",
      "120/223, train_loss: 0.1045, step time: 0.1387\n",
      "121/223, train_loss: 0.1165, step time: 0.1004\n",
      "122/223, train_loss: 0.1007, step time: 0.1018\n",
      "123/223, train_loss: 0.1081, step time: 0.1013\n",
      "124/223, train_loss: 0.1009, step time: 0.1217\n",
      "125/223, train_loss: 0.0865, step time: 0.1039\n",
      "126/223, train_loss: 0.1201, step time: 0.1241\n",
      "127/223, train_loss: 0.0973, step time: 0.1135\n",
      "128/223, train_loss: 0.1107, step time: 0.1063\n",
      "129/223, train_loss: 0.0984, step time: 0.1001\n",
      "130/223, train_loss: 0.0911, step time: 0.1355\n",
      "131/223, train_loss: 0.1084, step time: 0.1240\n",
      "132/223, train_loss: 0.0965, step time: 0.0997\n",
      "133/223, train_loss: 0.0995, step time: 0.1222\n",
      "134/223, train_loss: 0.0980, step time: 0.1157\n",
      "135/223, train_loss: 0.0977, step time: 0.1057\n",
      "136/223, train_loss: 0.1030, step time: 0.1102\n",
      "137/223, train_loss: 0.1026, step time: 0.1161\n",
      "138/223, train_loss: 0.0981, step time: 0.1219\n",
      "139/223, train_loss: 0.1083, step time: 0.1206\n",
      "140/223, train_loss: 0.0967, step time: 0.1086\n",
      "141/223, train_loss: 0.1008, step time: 0.1153\n",
      "142/223, train_loss: 0.1088, step time: 0.1033\n",
      "143/223, train_loss: 0.1008, step time: 0.1208\n",
      "144/223, train_loss: 0.0998, step time: 0.1090\n",
      "145/223, train_loss: 0.1094, step time: 0.1159\n",
      "146/223, train_loss: 0.1168, step time: 0.1302\n",
      "147/223, train_loss: 0.0960, step time: 0.1171\n",
      "148/223, train_loss: 0.1020, step time: 0.1085\n",
      "149/223, train_loss: 0.1055, step time: 0.1048\n",
      "150/223, train_loss: 0.1025, step time: 0.1085\n",
      "151/223, train_loss: 0.1122, step time: 0.1130\n",
      "152/223, train_loss: 0.1031, step time: 0.1073\n",
      "153/223, train_loss: 0.1004, step time: 0.1009\n",
      "154/223, train_loss: 0.1052, step time: 0.1184\n",
      "155/223, train_loss: 0.0987, step time: 0.1090\n",
      "156/223, train_loss: 0.1141, step time: 0.1100\n",
      "157/223, train_loss: 0.1031, step time: 0.1053\n",
      "158/223, train_loss: 0.0979, step time: 0.1066\n",
      "159/223, train_loss: 0.1084, step time: 0.1068\n",
      "160/223, train_loss: 0.1019, step time: 0.1238\n",
      "161/223, train_loss: 0.0932, step time: 0.1185\n",
      "162/223, train_loss: 0.0973, step time: 0.1033\n",
      "163/223, train_loss: 0.1135, step time: 0.1002\n",
      "164/223, train_loss: 0.1001, step time: 0.1133\n",
      "165/223, train_loss: 0.1108, step time: 0.1193\n",
      "166/223, train_loss: 0.0927, step time: 0.1148\n",
      "167/223, train_loss: 0.1119, step time: 0.1074\n",
      "168/223, train_loss: 0.0972, step time: 0.1012\n",
      "169/223, train_loss: 0.1090, step time: 0.1079\n",
      "170/223, train_loss: 0.0915, step time: 0.1060\n",
      "171/223, train_loss: 0.0938, step time: 0.1287\n",
      "172/223, train_loss: 0.1104, step time: 0.1230\n",
      "173/223, train_loss: 0.0995, step time: 0.1076\n",
      "174/223, train_loss: 0.1145, step time: 0.1006\n",
      "175/223, train_loss: 0.1019, step time: 0.1103\n",
      "176/223, train_loss: 0.1088, step time: 0.1074\n",
      "177/223, train_loss: 0.0979, step time: 0.1052\n",
      "178/223, train_loss: 0.1071, step time: 0.1151\n",
      "179/223, train_loss: 0.1088, step time: 0.1156\n",
      "180/223, train_loss: 0.1115, step time: 0.0994\n",
      "181/223, train_loss: 0.1088, step time: 0.0988\n",
      "182/223, train_loss: 0.1014, step time: 0.1055\n",
      "183/223, train_loss: 0.1079, step time: 0.1298\n",
      "184/223, train_loss: 0.0961, step time: 0.1012\n",
      "185/223, train_loss: 0.1088, step time: 0.1007\n",
      "186/223, train_loss: 0.1002, step time: 0.1002\n",
      "187/223, train_loss: 0.0986, step time: 0.1220\n",
      "188/223, train_loss: 0.1070, step time: 0.1002\n",
      "189/223, train_loss: 0.1000, step time: 0.1008\n",
      "190/223, train_loss: 0.1020, step time: 0.1063\n",
      "191/223, train_loss: 0.1041, step time: 0.1057\n",
      "192/223, train_loss: 0.0886, step time: 0.1011\n",
      "193/223, train_loss: 0.1130, step time: 0.1016\n",
      "194/223, train_loss: 0.0966, step time: 0.1004\n",
      "195/223, train_loss: 0.1032, step time: 0.1080\n",
      "196/223, train_loss: 0.0920, step time: 0.1132\n",
      "197/223, train_loss: 0.1094, step time: 0.1064\n",
      "198/223, train_loss: 0.0946, step time: 0.1000\n",
      "199/223, train_loss: 0.0951, step time: 0.1183\n",
      "200/223, train_loss: 0.1061, step time: 0.1157\n",
      "201/223, train_loss: 0.1181, step time: 0.1167\n",
      "202/223, train_loss: 0.1044, step time: 0.1002\n",
      "203/223, train_loss: 0.0960, step time: 0.1199\n",
      "204/223, train_loss: 0.1029, step time: 0.1167\n",
      "205/223, train_loss: 0.1005, step time: 0.1115\n",
      "206/223, train_loss: 0.1030, step time: 0.1157\n",
      "207/223, train_loss: 0.1050, step time: 0.1093\n",
      "208/223, train_loss: 0.1077, step time: 0.1233\n",
      "209/223, train_loss: 0.1073, step time: 0.1084\n",
      "210/223, train_loss: 0.1049, step time: 0.1254\n",
      "211/223, train_loss: 0.1102, step time: 0.1059\n",
      "212/223, train_loss: 0.1034, step time: 0.1175\n",
      "213/223, train_loss: 0.1004, step time: 0.1223\n",
      "214/223, train_loss: 0.0975, step time: 0.1118\n",
      "215/223, train_loss: 0.1077, step time: 0.1000\n",
      "216/223, train_loss: 0.0926, step time: 0.0998\n",
      "217/223, train_loss: 0.1004, step time: 0.1008\n",
      "218/223, train_loss: 0.0997, step time: 0.0993\n",
      "219/223, train_loss: 0.1039, step time: 0.0998\n",
      "220/223, train_loss: 0.0967, step time: 0.0997\n",
      "221/223, train_loss: 0.1063, step time: 0.1004\n",
      "222/223, train_loss: 0.1145, step time: 0.1105\n",
      "223/223, train_loss: 0.1092, step time: 0.1004\n",
      "epoch 185 average loss: 0.1037\n",
      "current epoch: 185 current mean dice: 0.8593 tc: 0.9210 wt: 0.8687 et: 0.7881\n",
      "best mean dice: 0.8600 at epoch: 170\n",
      "time consuming of epoch 185 is: 88.8033\n",
      "----------\n",
      "epoch 186/300\n",
      "1/223, train_loss: 0.1002, step time: 0.1072\n",
      "2/223, train_loss: 0.0999, step time: 0.1007\n",
      "3/223, train_loss: 0.1118, step time: 0.1013\n",
      "4/223, train_loss: 0.0987, step time: 0.1009\n",
      "5/223, train_loss: 0.0980, step time: 0.1175\n",
      "6/223, train_loss: 0.1124, step time: 0.1020\n",
      "7/223, train_loss: 0.1097, step time: 0.1264\n",
      "8/223, train_loss: 0.0928, step time: 0.1356\n",
      "9/223, train_loss: 0.0988, step time: 0.1186\n",
      "10/223, train_loss: 0.1053, step time: 0.1155\n",
      "11/223, train_loss: 0.0993, step time: 0.1222\n",
      "12/223, train_loss: 0.1102, step time: 0.1049\n",
      "13/223, train_loss: 0.1125, step time: 0.1259\n",
      "14/223, train_loss: 0.1056, step time: 0.1190\n",
      "15/223, train_loss: 0.0965, step time: 0.1000\n",
      "16/223, train_loss: 0.1131, step time: 0.1033\n",
      "17/223, train_loss: 0.0993, step time: 0.1221\n",
      "18/223, train_loss: 0.1033, step time: 0.1088\n",
      "19/223, train_loss: 0.0955, step time: 0.1024\n",
      "20/223, train_loss: 0.0897, step time: 0.1138\n",
      "21/223, train_loss: 0.1046, step time: 0.1026\n",
      "22/223, train_loss: 0.1097, step time: 0.1185\n",
      "23/223, train_loss: 0.1049, step time: 0.1134\n",
      "24/223, train_loss: 0.1104, step time: 0.1056\n",
      "25/223, train_loss: 0.0926, step time: 0.1003\n",
      "26/223, train_loss: 0.0962, step time: 0.1218\n",
      "27/223, train_loss: 0.1164, step time: 0.1192\n",
      "28/223, train_loss: 0.1033, step time: 0.1002\n",
      "29/223, train_loss: 0.1026, step time: 0.1118\n",
      "30/223, train_loss: 0.1129, step time: 0.1083\n",
      "31/223, train_loss: 0.0969, step time: 0.1200\n",
      "32/223, train_loss: 0.0870, step time: 0.1121\n",
      "33/223, train_loss: 0.1061, step time: 0.1160\n",
      "34/223, train_loss: 0.1099, step time: 0.1043\n",
      "35/223, train_loss: 0.0989, step time: 0.1258\n",
      "36/223, train_loss: 0.1005, step time: 0.1042\n",
      "37/223, train_loss: 0.1144, step time: 0.1266\n",
      "38/223, train_loss: 0.1082, step time: 0.1012\n",
      "39/223, train_loss: 0.0948, step time: 0.1171\n",
      "40/223, train_loss: 0.1050, step time: 0.1089\n",
      "41/223, train_loss: 0.1008, step time: 0.1098\n",
      "42/223, train_loss: 0.1005, step time: 0.1174\n",
      "43/223, train_loss: 0.1011, step time: 0.1094\n",
      "44/223, train_loss: 0.1057, step time: 0.1171\n",
      "45/223, train_loss: 0.0989, step time: 0.1300\n",
      "46/223, train_loss: 0.1183, step time: 0.1080\n",
      "47/223, train_loss: 0.0963, step time: 0.1006\n",
      "48/223, train_loss: 0.1047, step time: 0.1070\n",
      "49/223, train_loss: 0.0920, step time: 0.1151\n",
      "50/223, train_loss: 0.0978, step time: 0.1115\n",
      "51/223, train_loss: 0.1077, step time: 0.1289\n",
      "52/223, train_loss: 0.1056, step time: 0.1116\n",
      "53/223, train_loss: 0.0981, step time: 0.1165\n",
      "54/223, train_loss: 0.0996, step time: 0.1034\n",
      "55/223, train_loss: 0.1098, step time: 0.1171\n",
      "56/223, train_loss: 0.1043, step time: 0.1106\n",
      "57/223, train_loss: 0.1009, step time: 0.1104\n",
      "58/223, train_loss: 0.0983, step time: 0.1111\n",
      "59/223, train_loss: 0.0957, step time: 0.1235\n",
      "60/223, train_loss: 0.0957, step time: 0.1129\n",
      "61/223, train_loss: 0.1093, step time: 0.1001\n",
      "62/223, train_loss: 0.1072, step time: 0.0997\n",
      "63/223, train_loss: 0.1080, step time: 0.1180\n",
      "64/223, train_loss: 0.0917, step time: 0.1198\n",
      "65/223, train_loss: 0.0949, step time: 0.1143\n",
      "66/223, train_loss: 0.0958, step time: 0.1331\n",
      "67/223, train_loss: 0.0969, step time: 0.1362\n",
      "68/223, train_loss: 0.1020, step time: 0.1247\n",
      "69/223, train_loss: 0.0941, step time: 0.1032\n",
      "70/223, train_loss: 0.0967, step time: 0.1117\n",
      "71/223, train_loss: 0.2978, step time: 0.1160\n",
      "72/223, train_loss: 0.1045, step time: 0.1471\n",
      "73/223, train_loss: 0.1069, step time: 0.1109\n",
      "74/223, train_loss: 0.1030, step time: 0.1108\n",
      "75/223, train_loss: 0.1056, step time: 0.1001\n",
      "76/223, train_loss: 0.1043, step time: 0.1017\n",
      "77/223, train_loss: 0.0986, step time: 0.1157\n",
      "78/223, train_loss: 0.1010, step time: 0.1155\n",
      "79/223, train_loss: 0.1060, step time: 0.1083\n",
      "80/223, train_loss: 0.1114, step time: 0.1175\n",
      "81/223, train_loss: 0.0945, step time: 0.1076\n",
      "82/223, train_loss: 0.1122, step time: 0.1181\n",
      "83/223, train_loss: 0.1102, step time: 0.1176\n",
      "84/223, train_loss: 0.1204, step time: 0.1006\n",
      "85/223, train_loss: 0.1128, step time: 0.1105\n",
      "86/223, train_loss: 0.1143, step time: 0.1253\n",
      "87/223, train_loss: 0.0953, step time: 0.1317\n",
      "88/223, train_loss: 0.1066, step time: 0.1031\n",
      "89/223, train_loss: 0.1064, step time: 0.1190\n",
      "90/223, train_loss: 0.1016, step time: 0.1049\n",
      "91/223, train_loss: 0.0969, step time: 0.1068\n",
      "92/223, train_loss: 0.1143, step time: 0.1008\n",
      "93/223, train_loss: 0.0941, step time: 0.1072\n",
      "94/223, train_loss: 0.1007, step time: 0.1062\n",
      "95/223, train_loss: 0.1004, step time: 0.1009\n",
      "96/223, train_loss: 0.1054, step time: 0.1021\n",
      "97/223, train_loss: 0.0933, step time: 0.1143\n",
      "98/223, train_loss: 0.0984, step time: 0.1101\n",
      "99/223, train_loss: 0.0938, step time: 0.1156\n",
      "100/223, train_loss: 0.1103, step time: 0.1083\n",
      "101/223, train_loss: 0.1119, step time: 0.1080\n",
      "102/223, train_loss: 0.1072, step time: 0.1002\n",
      "103/223, train_loss: 0.1133, step time: 0.1033\n",
      "104/223, train_loss: 0.0966, step time: 0.1384\n",
      "105/223, train_loss: 0.1072, step time: 0.1125\n",
      "106/223, train_loss: 0.1101, step time: 0.1110\n",
      "107/223, train_loss: 0.0983, step time: 0.1072\n",
      "108/223, train_loss: 0.1003, step time: 0.1132\n",
      "109/223, train_loss: 0.1042, step time: 0.1182\n",
      "110/223, train_loss: 0.0949, step time: 0.1069\n",
      "111/223, train_loss: 0.1081, step time: 0.1416\n",
      "112/223, train_loss: 0.1104, step time: 0.1198\n",
      "113/223, train_loss: 0.1044, step time: 0.0987\n",
      "114/223, train_loss: 0.1058, step time: 0.1325\n",
      "115/223, train_loss: 0.1091, step time: 0.1100\n",
      "116/223, train_loss: 0.0957, step time: 0.1078\n",
      "117/223, train_loss: 0.1061, step time: 0.1009\n",
      "118/223, train_loss: 0.1002, step time: 0.0996\n",
      "119/223, train_loss: 0.0982, step time: 0.1027\n",
      "120/223, train_loss: 0.1179, step time: 0.0998\n",
      "121/223, train_loss: 0.1199, step time: 0.1159\n",
      "122/223, train_loss: 0.0981, step time: 0.1101\n",
      "123/223, train_loss: 0.0978, step time: 0.1248\n",
      "124/223, train_loss: 0.1081, step time: 0.1006\n",
      "125/223, train_loss: 0.0991, step time: 0.1098\n",
      "126/223, train_loss: 0.1069, step time: 0.1008\n",
      "127/223, train_loss: 0.0934, step time: 0.1112\n",
      "128/223, train_loss: 0.0956, step time: 0.1032\n",
      "129/223, train_loss: 0.0947, step time: 0.1227\n",
      "130/223, train_loss: 0.1144, step time: 0.1227\n",
      "131/223, train_loss: 0.1119, step time: 0.1002\n",
      "132/223, train_loss: 0.1078, step time: 0.1269\n",
      "133/223, train_loss: 0.0949, step time: 0.1242\n",
      "134/223, train_loss: 0.1100, step time: 0.1038\n",
      "135/223, train_loss: 0.0965, step time: 0.1394\n",
      "136/223, train_loss: 0.0990, step time: 0.1109\n",
      "137/223, train_loss: 0.1044, step time: 0.1135\n",
      "138/223, train_loss: 0.0957, step time: 0.1269\n",
      "139/223, train_loss: 0.0928, step time: 0.1235\n",
      "140/223, train_loss: 0.1038, step time: 0.1078\n",
      "141/223, train_loss: 0.0966, step time: 0.0994\n",
      "142/223, train_loss: 0.1149, step time: 0.1170\n",
      "143/223, train_loss: 0.0971, step time: 0.1029\n",
      "144/223, train_loss: 0.0937, step time: 0.1052\n",
      "145/223, train_loss: 0.1155, step time: 0.1061\n",
      "146/223, train_loss: 0.1126, step time: 0.1043\n",
      "147/223, train_loss: 0.0944, step time: 0.1096\n",
      "148/223, train_loss: 0.1026, step time: 0.1145\n",
      "149/223, train_loss: 0.1128, step time: 0.1058\n",
      "150/223, train_loss: 0.1095, step time: 0.1426\n",
      "151/223, train_loss: 0.0937, step time: 0.1128\n",
      "152/223, train_loss: 0.1210, step time: 0.1251\n",
      "153/223, train_loss: 0.0958, step time: 0.0996\n",
      "154/223, train_loss: 0.0967, step time: 0.1010\n",
      "155/223, train_loss: 0.0994, step time: 0.1224\n",
      "156/223, train_loss: 0.1048, step time: 0.1055\n",
      "157/223, train_loss: 0.0927, step time: 0.1144\n",
      "158/223, train_loss: 0.0904, step time: 0.1015\n",
      "159/223, train_loss: 0.1022, step time: 0.0998\n",
      "160/223, train_loss: 0.1104, step time: 0.1511\n",
      "161/223, train_loss: 0.0999, step time: 0.1082\n",
      "162/223, train_loss: 0.0999, step time: 0.1224\n",
      "163/223, train_loss: 0.1031, step time: 0.1100\n",
      "164/223, train_loss: 0.1180, step time: 0.1290\n",
      "165/223, train_loss: 0.1068, step time: 0.1001\n",
      "166/223, train_loss: 0.1074, step time: 0.1225\n",
      "167/223, train_loss: 0.1014, step time: 0.1063\n",
      "168/223, train_loss: 0.0959, step time: 0.1060\n",
      "169/223, train_loss: 0.1078, step time: 0.1202\n",
      "170/223, train_loss: 0.1026, step time: 0.1321\n",
      "171/223, train_loss: 0.1006, step time: 0.1164\n",
      "172/223, train_loss: 0.1039, step time: 0.1078\n",
      "173/223, train_loss: 0.0973, step time: 0.1081\n",
      "174/223, train_loss: 0.0938, step time: 0.1110\n",
      "175/223, train_loss: 0.0944, step time: 0.1448\n",
      "176/223, train_loss: 0.1022, step time: 0.1021\n",
      "177/223, train_loss: 0.0927, step time: 0.1083\n",
      "178/223, train_loss: 0.0971, step time: 0.1006\n",
      "179/223, train_loss: 0.1060, step time: 0.0998\n",
      "180/223, train_loss: 0.0987, step time: 0.0999\n",
      "181/223, train_loss: 0.1082, step time: 0.1099\n",
      "182/223, train_loss: 0.0986, step time: 0.1072\n",
      "183/223, train_loss: 0.1080, step time: 0.1085\n",
      "184/223, train_loss: 0.0906, step time: 0.1024\n",
      "185/223, train_loss: 0.1026, step time: 0.1258\n",
      "186/223, train_loss: 0.1044, step time: 0.1081\n",
      "187/223, train_loss: 0.1039, step time: 0.1076\n",
      "188/223, train_loss: 0.1117, step time: 0.1121\n",
      "189/223, train_loss: 0.1086, step time: 0.1160\n",
      "190/223, train_loss: 0.0941, step time: 0.1138\n",
      "191/223, train_loss: 0.1098, step time: 0.1092\n",
      "192/223, train_loss: 0.0936, step time: 0.1171\n",
      "193/223, train_loss: 0.0999, step time: 0.1109\n",
      "194/223, train_loss: 0.0975, step time: 0.0993\n",
      "195/223, train_loss: 0.0965, step time: 0.0995\n",
      "196/223, train_loss: 0.1089, step time: 0.0984\n",
      "197/223, train_loss: 0.1097, step time: 0.1022\n",
      "198/223, train_loss: 0.1031, step time: 0.1087\n",
      "199/223, train_loss: 0.1254, step time: 0.0991\n",
      "200/223, train_loss: 0.1135, step time: 0.0997\n",
      "201/223, train_loss: 0.1027, step time: 0.1128\n",
      "202/223, train_loss: 0.1121, step time: 0.1173\n",
      "203/223, train_loss: 0.1133, step time: 0.1603\n",
      "204/223, train_loss: 0.0952, step time: 0.1096\n",
      "205/223, train_loss: 0.0982, step time: 0.1288\n",
      "206/223, train_loss: 0.0985, step time: 0.1240\n",
      "207/223, train_loss: 0.1058, step time: 0.1115\n",
      "208/223, train_loss: 0.1021, step time: 0.1005\n",
      "209/223, train_loss: 0.0971, step time: 0.1060\n",
      "210/223, train_loss: 0.0952, step time: 0.1190\n",
      "211/223, train_loss: 0.0917, step time: 0.1171\n",
      "212/223, train_loss: 0.1088, step time: 0.1312\n",
      "213/223, train_loss: 0.0999, step time: 0.1010\n",
      "214/223, train_loss: 0.1053, step time: 0.1587\n",
      "215/223, train_loss: 0.0951, step time: 0.1008\n",
      "216/223, train_loss: 0.1200, step time: 0.1006\n",
      "217/223, train_loss: 0.1026, step time: 0.1105\n",
      "218/223, train_loss: 0.1033, step time: 0.1098\n",
      "219/223, train_loss: 0.0971, step time: 0.1093\n",
      "220/223, train_loss: 0.0988, step time: 0.1012\n",
      "221/223, train_loss: 0.0901, step time: 0.0989\n",
      "222/223, train_loss: 0.1081, step time: 0.0998\n",
      "223/223, train_loss: 0.1097, step time: 0.1002\n",
      "epoch 186 average loss: 0.1038\n",
      "time consuming of epoch 186 is: 86.5262\n",
      "----------\n",
      "epoch 187/300\n",
      "1/223, train_loss: 0.0974, step time: 0.1198\n",
      "2/223, train_loss: 0.1005, step time: 0.1003\n",
      "3/223, train_loss: 0.0996, step time: 0.1006\n",
      "4/223, train_loss: 0.0985, step time: 0.1049\n",
      "5/223, train_loss: 0.0915, step time: 0.1241\n",
      "6/223, train_loss: 0.1071, step time: 0.1008\n",
      "7/223, train_loss: 0.1055, step time: 0.1088\n",
      "8/223, train_loss: 0.1039, step time: 0.1006\n",
      "9/223, train_loss: 0.1087, step time: 0.1283\n",
      "10/223, train_loss: 0.1117, step time: 0.1108\n",
      "11/223, train_loss: 0.1030, step time: 0.1141\n",
      "12/223, train_loss: 0.1000, step time: 0.1218\n",
      "13/223, train_loss: 0.1169, step time: 0.1046\n",
      "14/223, train_loss: 0.0993, step time: 0.1017\n",
      "15/223, train_loss: 0.0994, step time: 0.1011\n",
      "16/223, train_loss: 0.0891, step time: 0.1245\n",
      "17/223, train_loss: 0.0916, step time: 0.1034\n",
      "18/223, train_loss: 0.0916, step time: 0.1119\n",
      "19/223, train_loss: 0.1113, step time: 0.1077\n",
      "20/223, train_loss: 0.1051, step time: 0.1026\n",
      "21/223, train_loss: 0.1043, step time: 0.1269\n",
      "22/223, train_loss: 0.0996, step time: 0.1149\n",
      "23/223, train_loss: 0.1062, step time: 0.1008\n",
      "24/223, train_loss: 0.1092, step time: 0.1008\n",
      "25/223, train_loss: 0.1023, step time: 0.1057\n",
      "26/223, train_loss: 0.0953, step time: 0.1091\n",
      "27/223, train_loss: 0.0940, step time: 0.1016\n",
      "28/223, train_loss: 0.1033, step time: 0.1008\n",
      "29/223, train_loss: 0.1135, step time: 0.1113\n",
      "30/223, train_loss: 0.0972, step time: 0.1175\n",
      "31/223, train_loss: 0.1013, step time: 0.1040\n",
      "32/223, train_loss: 0.1109, step time: 0.1011\n",
      "33/223, train_loss: 0.0994, step time: 0.1002\n",
      "34/223, train_loss: 0.1050, step time: 0.1057\n",
      "35/223, train_loss: 0.0931, step time: 0.1150\n",
      "36/223, train_loss: 0.0949, step time: 0.1000\n",
      "37/223, train_loss: 0.1045, step time: 0.1074\n",
      "38/223, train_loss: 0.0950, step time: 0.1105\n",
      "39/223, train_loss: 0.0969, step time: 0.1007\n",
      "40/223, train_loss: 0.1061, step time: 0.1011\n",
      "41/223, train_loss: 0.0902, step time: 0.1083\n",
      "42/223, train_loss: 0.1005, step time: 0.0998\n",
      "43/223, train_loss: 0.0977, step time: 0.1005\n",
      "44/223, train_loss: 0.0941, step time: 0.1003\n",
      "45/223, train_loss: 0.1203, step time: 0.1115\n",
      "46/223, train_loss: 0.1080, step time: 0.1007\n",
      "47/223, train_loss: 0.0972, step time: 0.1006\n",
      "48/223, train_loss: 0.1008, step time: 0.1033\n",
      "49/223, train_loss: 0.1131, step time: 0.1230\n",
      "50/223, train_loss: 0.1090, step time: 0.1213\n",
      "51/223, train_loss: 0.1111, step time: 0.1104\n",
      "52/223, train_loss: 0.0893, step time: 0.1004\n",
      "53/223, train_loss: 0.1007, step time: 0.1280\n",
      "54/223, train_loss: 0.1068, step time: 0.1158\n",
      "55/223, train_loss: 0.1020, step time: 0.1003\n",
      "56/223, train_loss: 0.1041, step time: 0.1018\n",
      "57/223, train_loss: 0.1096, step time: 0.1138\n",
      "58/223, train_loss: 0.1020, step time: 0.1135\n",
      "59/223, train_loss: 0.0992, step time: 0.1078\n",
      "60/223, train_loss: 0.1148, step time: 0.1119\n",
      "61/223, train_loss: 0.0993, step time: 0.1003\n",
      "62/223, train_loss: 0.0983, step time: 0.1050\n",
      "63/223, train_loss: 0.1061, step time: 0.1024\n",
      "64/223, train_loss: 0.0934, step time: 0.1005\n",
      "65/223, train_loss: 0.1111, step time: 0.1009\n",
      "66/223, train_loss: 0.0931, step time: 0.1192\n",
      "67/223, train_loss: 0.1131, step time: 0.1285\n",
      "68/223, train_loss: 0.1073, step time: 0.1239\n",
      "69/223, train_loss: 0.1046, step time: 0.1191\n",
      "70/223, train_loss: 0.1092, step time: 0.0999\n",
      "71/223, train_loss: 0.1024, step time: 0.1002\n",
      "72/223, train_loss: 0.0915, step time: 0.1092\n",
      "73/223, train_loss: 0.1038, step time: 0.1154\n",
      "74/223, train_loss: 0.1021, step time: 0.1028\n",
      "75/223, train_loss: 0.0955, step time: 0.1137\n",
      "76/223, train_loss: 0.1032, step time: 0.1165\n",
      "77/223, train_loss: 0.1019, step time: 0.1031\n",
      "78/223, train_loss: 0.1077, step time: 0.1158\n",
      "79/223, train_loss: 0.0969, step time: 0.1539\n",
      "80/223, train_loss: 0.1064, step time: 0.1319\n",
      "81/223, train_loss: 0.0933, step time: 0.1146\n",
      "82/223, train_loss: 0.1025, step time: 0.1104\n",
      "83/223, train_loss: 0.1020, step time: 0.1108\n",
      "84/223, train_loss: 0.0909, step time: 0.1115\n",
      "85/223, train_loss: 0.1019, step time: 0.1012\n",
      "86/223, train_loss: 0.1045, step time: 0.1129\n",
      "87/223, train_loss: 0.1108, step time: 0.1062\n",
      "88/223, train_loss: 0.0936, step time: 0.1002\n",
      "89/223, train_loss: 0.1120, step time: 0.1008\n",
      "90/223, train_loss: 0.1060, step time: 0.1003\n",
      "91/223, train_loss: 0.0933, step time: 0.1152\n",
      "92/223, train_loss: 0.0970, step time: 0.1008\n",
      "93/223, train_loss: 0.0924, step time: 0.1008\n",
      "94/223, train_loss: 0.1058, step time: 0.1177\n",
      "95/223, train_loss: 0.1059, step time: 0.1374\n",
      "96/223, train_loss: 0.1038, step time: 0.1243\n",
      "97/223, train_loss: 0.1127, step time: 0.1006\n",
      "98/223, train_loss: 0.1053, step time: 0.1041\n",
      "99/223, train_loss: 0.1113, step time: 0.1000\n",
      "100/223, train_loss: 0.1103, step time: 0.1005\n",
      "101/223, train_loss: 0.1015, step time: 0.1129\n",
      "102/223, train_loss: 0.0999, step time: 0.1278\n",
      "103/223, train_loss: 0.1013, step time: 0.1055\n",
      "104/223, train_loss: 0.0962, step time: 0.1022\n",
      "105/223, train_loss: 0.1023, step time: 0.1000\n",
      "106/223, train_loss: 0.0961, step time: 0.1174\n",
      "107/223, train_loss: 0.1038, step time: 0.1264\n",
      "108/223, train_loss: 0.1087, step time: 0.1127\n",
      "109/223, train_loss: 0.1223, step time: 0.1005\n",
      "110/223, train_loss: 0.1149, step time: 0.1104\n",
      "111/223, train_loss: 0.1188, step time: 0.1010\n",
      "112/223, train_loss: 0.0978, step time: 0.1001\n",
      "113/223, train_loss: 0.1042, step time: 0.1002\n",
      "114/223, train_loss: 0.1044, step time: 0.1315\n",
      "115/223, train_loss: 0.0942, step time: 0.1007\n",
      "116/223, train_loss: 0.0966, step time: 0.0998\n",
      "117/223, train_loss: 0.1068, step time: 0.1008\n",
      "118/223, train_loss: 0.1105, step time: 0.1091\n",
      "119/223, train_loss: 0.1034, step time: 0.1013\n",
      "120/223, train_loss: 0.1148, step time: 0.1014\n",
      "121/223, train_loss: 0.1004, step time: 0.1213\n",
      "122/223, train_loss: 0.1044, step time: 0.1037\n",
      "123/223, train_loss: 0.1077, step time: 0.1314\n",
      "124/223, train_loss: 0.1134, step time: 0.1250\n",
      "125/223, train_loss: 0.1035, step time: 0.1001\n",
      "126/223, train_loss: 0.0936, step time: 0.1088\n",
      "127/223, train_loss: 0.1009, step time: 0.1412\n",
      "128/223, train_loss: 0.1123, step time: 0.1004\n",
      "129/223, train_loss: 0.1046, step time: 0.1144\n",
      "130/223, train_loss: 0.1003, step time: 0.1242\n",
      "131/223, train_loss: 0.1043, step time: 0.1069\n",
      "132/223, train_loss: 0.0939, step time: 0.1089\n",
      "133/223, train_loss: 0.1038, step time: 0.1285\n",
      "134/223, train_loss: 0.0998, step time: 0.1044\n",
      "135/223, train_loss: 0.0909, step time: 0.1204\n",
      "136/223, train_loss: 0.1120, step time: 0.1008\n",
      "137/223, train_loss: 0.1090, step time: 0.1099\n",
      "138/223, train_loss: 0.1046, step time: 0.1074\n",
      "139/223, train_loss: 0.0992, step time: 0.1164\n",
      "140/223, train_loss: 0.1091, step time: 0.1190\n",
      "141/223, train_loss: 0.1023, step time: 0.1128\n",
      "142/223, train_loss: 0.1052, step time: 0.1062\n",
      "143/223, train_loss: 0.0951, step time: 0.1222\n",
      "144/223, train_loss: 0.1098, step time: 0.1096\n",
      "145/223, train_loss: 0.1171, step time: 0.0998\n",
      "146/223, train_loss: 0.0968, step time: 0.1129\n",
      "147/223, train_loss: 0.0988, step time: 0.1245\n",
      "148/223, train_loss: 0.0980, step time: 0.0997\n",
      "149/223, train_loss: 0.1043, step time: 0.1101\n",
      "150/223, train_loss: 0.1064, step time: 0.1074\n",
      "151/223, train_loss: 0.0984, step time: 0.1129\n",
      "152/223, train_loss: 0.0928, step time: 0.1187\n",
      "153/223, train_loss: 0.0901, step time: 0.1188\n",
      "154/223, train_loss: 0.1053, step time: 0.1223\n",
      "155/223, train_loss: 0.0982, step time: 0.1000\n",
      "156/223, train_loss: 0.1063, step time: 0.1160\n",
      "157/223, train_loss: 0.0979, step time: 0.1134\n",
      "158/223, train_loss: 0.1075, step time: 0.1183\n",
      "159/223, train_loss: 0.1024, step time: 0.1151\n",
      "160/223, train_loss: 0.1037, step time: 0.1052\n",
      "161/223, train_loss: 0.1023, step time: 0.1041\n",
      "162/223, train_loss: 0.0971, step time: 0.1070\n",
      "163/223, train_loss: 0.1010, step time: 0.1035\n",
      "164/223, train_loss: 0.0951, step time: 0.1041\n",
      "165/223, train_loss: 0.0929, step time: 0.1087\n",
      "166/223, train_loss: 0.1066, step time: 0.1072\n",
      "167/223, train_loss: 0.0948, step time: 0.1231\n",
      "168/223, train_loss: 0.1036, step time: 0.1145\n",
      "169/223, train_loss: 0.0979, step time: 0.1086\n",
      "170/223, train_loss: 0.0978, step time: 0.0996\n",
      "171/223, train_loss: 0.1003, step time: 0.1169\n",
      "172/223, train_loss: 0.0972, step time: 0.1129\n",
      "173/223, train_loss: 0.1041, step time: 0.1205\n",
      "174/223, train_loss: 0.0965, step time: 0.1323\n",
      "175/223, train_loss: 0.1067, step time: 0.1068\n",
      "176/223, train_loss: 0.1076, step time: 0.1096\n",
      "177/223, train_loss: 0.1248, step time: 0.1065\n",
      "178/223, train_loss: 0.3028, step time: 0.1195\n",
      "179/223, train_loss: 0.1102, step time: 0.0998\n",
      "180/223, train_loss: 0.1067, step time: 0.1105\n",
      "181/223, train_loss: 0.1004, step time: 0.1155\n",
      "182/223, train_loss: 0.0961, step time: 0.1082\n",
      "183/223, train_loss: 0.0966, step time: 0.1012\n",
      "184/223, train_loss: 0.0968, step time: 0.1224\n",
      "185/223, train_loss: 0.1101, step time: 0.1062\n",
      "186/223, train_loss: 0.0943, step time: 0.1174\n",
      "187/223, train_loss: 0.1007, step time: 0.1141\n",
      "188/223, train_loss: 0.1002, step time: 0.1048\n",
      "189/223, train_loss: 0.0904, step time: 0.1240\n",
      "190/223, train_loss: 0.1022, step time: 0.1065\n",
      "191/223, train_loss: 0.1084, step time: 0.1059\n",
      "192/223, train_loss: 0.1084, step time: 0.1079\n",
      "193/223, train_loss: 0.1056, step time: 0.1173\n",
      "194/223, train_loss: 0.1048, step time: 0.1036\n",
      "195/223, train_loss: 0.0997, step time: 0.1084\n",
      "196/223, train_loss: 0.1161, step time: 0.1120\n",
      "197/223, train_loss: 0.1096, step time: 0.1135\n",
      "198/223, train_loss: 0.0993, step time: 0.1274\n",
      "199/223, train_loss: 0.1083, step time: 0.1013\n",
      "200/223, train_loss: 0.0945, step time: 0.0987\n",
      "201/223, train_loss: 0.1097, step time: 0.0989\n",
      "202/223, train_loss: 0.0924, step time: 0.1146\n",
      "203/223, train_loss: 0.1077, step time: 0.1001\n",
      "204/223, train_loss: 0.1094, step time: 0.0996\n",
      "205/223, train_loss: 0.1097, step time: 0.0990\n",
      "206/223, train_loss: 0.1034, step time: 0.1052\n",
      "207/223, train_loss: 0.0921, step time: 0.1015\n",
      "208/223, train_loss: 0.1070, step time: 0.1104\n",
      "209/223, train_loss: 0.1080, step time: 0.1007\n",
      "210/223, train_loss: 0.1102, step time: 0.1168\n",
      "211/223, train_loss: 0.1105, step time: 0.1003\n",
      "212/223, train_loss: 0.1008, step time: 0.1227\n",
      "213/223, train_loss: 0.0945, step time: 0.1000\n",
      "214/223, train_loss: 0.0971, step time: 0.1124\n",
      "215/223, train_loss: 0.1058, step time: 0.1025\n",
      "216/223, train_loss: 0.0956, step time: 0.1050\n",
      "217/223, train_loss: 0.0985, step time: 0.1003\n",
      "218/223, train_loss: 0.1013, step time: 0.1000\n",
      "219/223, train_loss: 0.0993, step time: 0.1099\n",
      "220/223, train_loss: 0.1065, step time: 0.0986\n",
      "221/223, train_loss: 0.1019, step time: 0.0994\n",
      "222/223, train_loss: 0.1141, step time: 0.0998\n",
      "223/223, train_loss: 0.1025, step time: 0.0995\n",
      "epoch 187 average loss: 0.1036\n",
      "time consuming of epoch 187 is: 87.1357\n",
      "----------\n",
      "epoch 188/300\n",
      "1/223, train_loss: 0.1072, step time: 0.1025\n",
      "2/223, train_loss: 0.0965, step time: 0.0994\n",
      "3/223, train_loss: 0.1029, step time: 0.1086\n",
      "4/223, train_loss: 0.1003, step time: 0.1015\n",
      "5/223, train_loss: 0.1006, step time: 0.1113\n",
      "6/223, train_loss: 0.1028, step time: 0.1124\n",
      "7/223, train_loss: 0.1042, step time: 0.1040\n",
      "8/223, train_loss: 0.0947, step time: 0.1164\n",
      "9/223, train_loss: 0.1021, step time: 0.1093\n",
      "10/223, train_loss: 0.1060, step time: 0.1244\n",
      "11/223, train_loss: 0.0953, step time: 0.1099\n",
      "12/223, train_loss: 0.1015, step time: 0.1169\n",
      "13/223, train_loss: 0.1033, step time: 0.1064\n",
      "14/223, train_loss: 0.1087, step time: 0.1273\n",
      "15/223, train_loss: 0.1065, step time: 0.1187\n",
      "16/223, train_loss: 0.0997, step time: 0.1243\n",
      "17/223, train_loss: 0.0994, step time: 0.1083\n",
      "18/223, train_loss: 0.0957, step time: 0.1269\n",
      "19/223, train_loss: 0.1054, step time: 0.1080\n",
      "20/223, train_loss: 0.0958, step time: 0.1077\n",
      "21/223, train_loss: 0.1028, step time: 0.1215\n",
      "22/223, train_loss: 0.1041, step time: 0.1212\n",
      "23/223, train_loss: 0.1039, step time: 0.1012\n",
      "24/223, train_loss: 0.1073, step time: 0.1180\n",
      "25/223, train_loss: 0.1068, step time: 0.1226\n",
      "26/223, train_loss: 0.0940, step time: 0.1229\n",
      "27/223, train_loss: 0.0972, step time: 0.1065\n",
      "28/223, train_loss: 0.1086, step time: 0.1154\n",
      "29/223, train_loss: 0.1097, step time: 0.1025\n",
      "30/223, train_loss: 0.1082, step time: 0.1116\n",
      "31/223, train_loss: 0.0958, step time: 0.1116\n",
      "32/223, train_loss: 0.1032, step time: 0.1046\n",
      "33/223, train_loss: 0.1121, step time: 0.1107\n",
      "34/223, train_loss: 0.1058, step time: 0.1149\n",
      "35/223, train_loss: 0.0972, step time: 0.1056\n",
      "36/223, train_loss: 0.0938, step time: 0.1208\n",
      "37/223, train_loss: 0.1094, step time: 0.1225\n",
      "38/223, train_loss: 0.1045, step time: 0.1107\n",
      "39/223, train_loss: 0.1070, step time: 0.1235\n",
      "40/223, train_loss: 0.1013, step time: 0.1072\n",
      "41/223, train_loss: 0.1131, step time: 0.1115\n",
      "42/223, train_loss: 0.1002, step time: 0.1126\n",
      "43/223, train_loss: 0.0982, step time: 0.1245\n",
      "44/223, train_loss: 0.0912, step time: 0.1010\n",
      "45/223, train_loss: 0.0963, step time: 0.1270\n",
      "46/223, train_loss: 0.0939, step time: 0.1049\n",
      "47/223, train_loss: 0.1058, step time: 0.1389\n",
      "48/223, train_loss: 0.1080, step time: 0.1180\n",
      "49/223, train_loss: 0.1039, step time: 0.1196\n",
      "50/223, train_loss: 0.0945, step time: 0.1071\n",
      "51/223, train_loss: 0.1036, step time: 0.1270\n",
      "52/223, train_loss: 0.1027, step time: 0.1125\n",
      "53/223, train_loss: 0.1001, step time: 0.1097\n",
      "54/223, train_loss: 0.1000, step time: 0.1049\n",
      "55/223, train_loss: 0.0971, step time: 0.1040\n",
      "56/223, train_loss: 0.1181, step time: 0.1030\n",
      "57/223, train_loss: 0.1043, step time: 0.1112\n",
      "58/223, train_loss: 0.0977, step time: 0.1003\n",
      "59/223, train_loss: 0.1080, step time: 0.1013\n",
      "60/223, train_loss: 0.1032, step time: 0.1126\n",
      "61/223, train_loss: 0.1069, step time: 0.1134\n",
      "62/223, train_loss: 0.1102, step time: 0.1007\n",
      "63/223, train_loss: 0.1149, step time: 0.1023\n",
      "64/223, train_loss: 0.0950, step time: 0.1193\n",
      "65/223, train_loss: 0.0897, step time: 0.1218\n",
      "66/223, train_loss: 0.1113, step time: 0.1162\n",
      "67/223, train_loss: 0.0960, step time: 0.1174\n",
      "68/223, train_loss: 0.1020, step time: 0.1007\n",
      "69/223, train_loss: 0.0940, step time: 0.1017\n",
      "70/223, train_loss: 0.0964, step time: 0.1053\n",
      "71/223, train_loss: 0.1038, step time: 0.1158\n",
      "72/223, train_loss: 0.0998, step time: 0.1339\n",
      "73/223, train_loss: 0.1176, step time: 0.1148\n",
      "74/223, train_loss: 0.0952, step time: 0.1011\n",
      "75/223, train_loss: 0.1145, step time: 0.0999\n",
      "76/223, train_loss: 0.0918, step time: 0.1027\n",
      "77/223, train_loss: 0.0974, step time: 0.1233\n",
      "78/223, train_loss: 0.1114, step time: 0.1063\n",
      "79/223, train_loss: 0.1111, step time: 0.1176\n",
      "80/223, train_loss: 0.1001, step time: 0.1157\n",
      "81/223, train_loss: 0.1033, step time: 0.1012\n",
      "82/223, train_loss: 0.0993, step time: 0.0999\n",
      "83/223, train_loss: 0.1019, step time: 0.1161\n",
      "84/223, train_loss: 0.0998, step time: 0.1013\n",
      "85/223, train_loss: 0.1017, step time: 0.1195\n",
      "86/223, train_loss: 0.1008, step time: 0.1064\n",
      "87/223, train_loss: 0.1026, step time: 0.1045\n",
      "88/223, train_loss: 0.0948, step time: 0.1144\n",
      "89/223, train_loss: 0.1081, step time: 0.1062\n",
      "90/223, train_loss: 0.1099, step time: 0.1121\n",
      "91/223, train_loss: 0.1181, step time: 0.1010\n",
      "92/223, train_loss: 0.1103, step time: 0.1007\n",
      "93/223, train_loss: 0.0891, step time: 0.1079\n",
      "94/223, train_loss: 0.1061, step time: 0.1213\n",
      "95/223, train_loss: 0.1003, step time: 0.1095\n",
      "96/223, train_loss: 0.1058, step time: 0.1075\n",
      "97/223, train_loss: 0.1084, step time: 0.1149\n",
      "98/223, train_loss: 0.1008, step time: 0.0997\n",
      "99/223, train_loss: 0.1129, step time: 0.1003\n",
      "100/223, train_loss: 0.1086, step time: 0.1005\n",
      "101/223, train_loss: 0.1071, step time: 0.1071\n",
      "102/223, train_loss: 0.1034, step time: 0.1203\n",
      "103/223, train_loss: 0.1070, step time: 0.1204\n",
      "104/223, train_loss: 0.1001, step time: 0.1100\n",
      "105/223, train_loss: 0.0984, step time: 0.1131\n",
      "106/223, train_loss: 0.1004, step time: 0.1027\n",
      "107/223, train_loss: 0.0947, step time: 0.1255\n",
      "108/223, train_loss: 0.1003, step time: 0.1587\n",
      "109/223, train_loss: 0.1022, step time: 0.1009\n",
      "110/223, train_loss: 0.0992, step time: 0.1496\n",
      "111/223, train_loss: 0.0995, step time: 0.1161\n",
      "112/223, train_loss: 0.1039, step time: 0.1074\n",
      "113/223, train_loss: 0.1056, step time: 0.1137\n",
      "114/223, train_loss: 0.1060, step time: 0.1039\n",
      "115/223, train_loss: 0.1156, step time: 0.1022\n",
      "116/223, train_loss: 0.0961, step time: 0.1051\n",
      "117/223, train_loss: 0.0928, step time: 0.1121\n",
      "118/223, train_loss: 0.1132, step time: 0.1009\n",
      "119/223, train_loss: 0.1096, step time: 0.1005\n",
      "120/223, train_loss: 0.1037, step time: 0.1004\n",
      "121/223, train_loss: 0.1052, step time: 0.1414\n",
      "122/223, train_loss: 0.0940, step time: 0.1091\n",
      "123/223, train_loss: 0.1048, step time: 0.1138\n",
      "124/223, train_loss: 0.0989, step time: 0.1002\n",
      "125/223, train_loss: 0.1053, step time: 0.1103\n",
      "126/223, train_loss: 0.1073, step time: 0.1580\n",
      "127/223, train_loss: 0.0938, step time: 0.1252\n",
      "128/223, train_loss: 0.0970, step time: 0.1334\n",
      "129/223, train_loss: 0.1101, step time: 0.0995\n",
      "130/223, train_loss: 0.0977, step time: 0.1045\n",
      "131/223, train_loss: 0.1115, step time: 0.1168\n",
      "132/223, train_loss: 0.1014, step time: 0.1131\n",
      "133/223, train_loss: 0.1013, step time: 0.1107\n",
      "134/223, train_loss: 0.1012, step time: 0.1196\n",
      "135/223, train_loss: 0.1069, step time: 0.0995\n",
      "136/223, train_loss: 0.1073, step time: 0.1006\n",
      "137/223, train_loss: 0.1034, step time: 0.1132\n",
      "138/223, train_loss: 0.1012, step time: 0.0998\n",
      "139/223, train_loss: 0.1134, step time: 0.1003\n",
      "140/223, train_loss: 0.1041, step time: 0.1095\n",
      "141/223, train_loss: 0.1007, step time: 0.1016\n",
      "142/223, train_loss: 0.0996, step time: 0.1268\n",
      "143/223, train_loss: 0.1111, step time: 0.1016\n",
      "144/223, train_loss: 0.1018, step time: 0.0996\n",
      "145/223, train_loss: 0.1006, step time: 0.1399\n",
      "146/223, train_loss: 0.1164, step time: 0.1162\n",
      "147/223, train_loss: 0.1101, step time: 0.1140\n",
      "148/223, train_loss: 0.0996, step time: 0.0997\n",
      "149/223, train_loss: 0.0989, step time: 0.1169\n",
      "150/223, train_loss: 0.0983, step time: 0.1240\n",
      "151/223, train_loss: 0.1021, step time: 0.1349\n",
      "152/223, train_loss: 0.1032, step time: 0.1166\n",
      "153/223, train_loss: 0.1109, step time: 0.1123\n",
      "154/223, train_loss: 0.1018, step time: 0.1152\n",
      "155/223, train_loss: 0.1104, step time: 0.1213\n",
      "156/223, train_loss: 0.1093, step time: 0.1270\n",
      "157/223, train_loss: 0.1164, step time: 0.1004\n",
      "158/223, train_loss: 0.1084, step time: 0.1066\n",
      "159/223, train_loss: 0.0991, step time: 0.1197\n",
      "160/223, train_loss: 0.0932, step time: 0.1091\n",
      "161/223, train_loss: 0.1065, step time: 0.1012\n",
      "162/223, train_loss: 0.0885, step time: 0.1177\n",
      "163/223, train_loss: 0.1056, step time: 0.1178\n",
      "164/223, train_loss: 0.1030, step time: 0.1078\n",
      "165/223, train_loss: 0.0892, step time: 0.1218\n",
      "166/223, train_loss: 0.1012, step time: 0.1009\n",
      "167/223, train_loss: 0.1088, step time: 0.1150\n",
      "168/223, train_loss: 0.1010, step time: 0.1069\n",
      "169/223, train_loss: 0.0996, step time: 0.1275\n",
      "170/223, train_loss: 0.1017, step time: 0.1108\n",
      "171/223, train_loss: 0.0949, step time: 0.1182\n",
      "172/223, train_loss: 0.0959, step time: 0.1125\n",
      "173/223, train_loss: 0.1193, step time: 0.1052\n",
      "174/223, train_loss: 0.1040, step time: 0.1075\n",
      "175/223, train_loss: 0.0979, step time: 0.1202\n",
      "176/223, train_loss: 0.0950, step time: 0.1017\n",
      "177/223, train_loss: 0.0989, step time: 0.1079\n",
      "178/223, train_loss: 0.0940, step time: 0.1194\n",
      "179/223, train_loss: 0.1055, step time: 0.1239\n",
      "180/223, train_loss: 0.1046, step time: 0.1048\n",
      "181/223, train_loss: 0.1061, step time: 0.1258\n",
      "182/223, train_loss: 0.1017, step time: 0.1177\n",
      "183/223, train_loss: 0.1083, step time: 0.1098\n",
      "184/223, train_loss: 0.1129, step time: 0.1152\n",
      "185/223, train_loss: 0.3057, step time: 0.1151\n",
      "186/223, train_loss: 0.1023, step time: 0.1169\n",
      "187/223, train_loss: 0.1046, step time: 0.1309\n",
      "188/223, train_loss: 0.1141, step time: 0.1003\n",
      "189/223, train_loss: 0.0925, step time: 0.1252\n",
      "190/223, train_loss: 0.1108, step time: 0.1216\n",
      "191/223, train_loss: 0.1143, step time: 0.1065\n",
      "192/223, train_loss: 0.1091, step time: 0.1102\n",
      "193/223, train_loss: 0.0943, step time: 0.1159\n",
      "194/223, train_loss: 0.0987, step time: 0.1223\n",
      "195/223, train_loss: 0.1088, step time: 0.1065\n",
      "196/223, train_loss: 0.0929, step time: 0.1148\n",
      "197/223, train_loss: 0.1072, step time: 0.1130\n",
      "198/223, train_loss: 0.1092, step time: 0.1004\n",
      "199/223, train_loss: 0.1065, step time: 0.0997\n",
      "200/223, train_loss: 0.0911, step time: 0.1083\n",
      "201/223, train_loss: 0.0881, step time: 0.1180\n",
      "202/223, train_loss: 0.1006, step time: 0.1331\n",
      "203/223, train_loss: 0.0979, step time: 0.1351\n",
      "204/223, train_loss: 0.1011, step time: 0.1032\n",
      "205/223, train_loss: 0.0941, step time: 0.1209\n",
      "206/223, train_loss: 0.1001, step time: 0.1004\n",
      "207/223, train_loss: 0.0995, step time: 0.1001\n",
      "208/223, train_loss: 0.0970, step time: 0.1139\n",
      "209/223, train_loss: 0.1037, step time: 0.1071\n",
      "210/223, train_loss: 0.1111, step time: 0.1139\n",
      "211/223, train_loss: 0.0892, step time: 0.1065\n",
      "212/223, train_loss: 0.1053, step time: 0.0997\n",
      "213/223, train_loss: 0.0984, step time: 0.1415\n",
      "214/223, train_loss: 0.1000, step time: 0.1068\n",
      "215/223, train_loss: 0.1021, step time: 0.1214\n",
      "216/223, train_loss: 0.0936, step time: 0.1325\n",
      "217/223, train_loss: 0.1058, step time: 0.1039\n",
      "218/223, train_loss: 0.1117, step time: 0.1002\n",
      "219/223, train_loss: 0.1007, step time: 0.0993\n",
      "220/223, train_loss: 0.1008, step time: 0.0993\n",
      "221/223, train_loss: 0.0974, step time: 0.0993\n",
      "222/223, train_loss: 0.0968, step time: 0.0995\n",
      "223/223, train_loss: 0.0935, step time: 0.0994\n",
      "epoch 188 average loss: 0.1036\n",
      "time consuming of epoch 188 is: 86.7342\n",
      "----------\n",
      "epoch 189/300\n",
      "1/223, train_loss: 0.1060, step time: 0.1062\n",
      "2/223, train_loss: 0.1075, step time: 0.1137\n",
      "3/223, train_loss: 0.0987, step time: 0.0999\n",
      "4/223, train_loss: 0.1110, step time: 0.1102\n",
      "5/223, train_loss: 0.0908, step time: 0.1219\n",
      "6/223, train_loss: 0.0956, step time: 0.1265\n",
      "7/223, train_loss: 0.1013, step time: 0.1508\n",
      "8/223, train_loss: 0.1071, step time: 0.1129\n",
      "9/223, train_loss: 0.0936, step time: 0.1033\n",
      "10/223, train_loss: 0.1061, step time: 0.1323\n",
      "11/223, train_loss: 0.1064, step time: 0.1387\n",
      "12/223, train_loss: 0.1004, step time: 0.1092\n",
      "13/223, train_loss: 0.1045, step time: 0.1129\n",
      "14/223, train_loss: 0.1014, step time: 0.1344\n",
      "15/223, train_loss: 0.0989, step time: 0.1418\n",
      "16/223, train_loss: 0.0981, step time: 0.1453\n",
      "17/223, train_loss: 0.0999, step time: 0.1138\n",
      "18/223, train_loss: 0.1037, step time: 0.1136\n",
      "19/223, train_loss: 0.1171, step time: 0.1093\n",
      "20/223, train_loss: 0.0985, step time: 0.1012\n",
      "21/223, train_loss: 0.0993, step time: 0.1074\n",
      "22/223, train_loss: 0.0978, step time: 0.1038\n",
      "23/223, train_loss: 0.1021, step time: 0.1071\n",
      "24/223, train_loss: 0.0965, step time: 0.1071\n",
      "25/223, train_loss: 0.1180, step time: 0.1084\n",
      "26/223, train_loss: 0.1080, step time: 0.1016\n",
      "27/223, train_loss: 0.1083, step time: 0.1113\n",
      "28/223, train_loss: 0.1145, step time: 0.1012\n",
      "29/223, train_loss: 0.0987, step time: 0.1075\n",
      "30/223, train_loss: 0.1083, step time: 0.1314\n",
      "31/223, train_loss: 0.1002, step time: 0.1157\n",
      "32/223, train_loss: 0.0955, step time: 0.1208\n",
      "33/223, train_loss: 0.1040, step time: 0.1044\n",
      "34/223, train_loss: 0.1070, step time: 0.1119\n",
      "35/223, train_loss: 0.0940, step time: 0.1251\n",
      "36/223, train_loss: 0.1014, step time: 0.1178\n",
      "37/223, train_loss: 0.0937, step time: 0.1007\n",
      "38/223, train_loss: 0.0928, step time: 0.0996\n",
      "39/223, train_loss: 0.0975, step time: 0.1003\n",
      "40/223, train_loss: 0.1119, step time: 0.1009\n",
      "41/223, train_loss: 0.1041, step time: 0.1060\n",
      "42/223, train_loss: 0.0953, step time: 0.1005\n",
      "43/223, train_loss: 0.0974, step time: 0.1004\n",
      "44/223, train_loss: 0.1024, step time: 0.1008\n",
      "45/223, train_loss: 0.1153, step time: 0.1160\n",
      "46/223, train_loss: 0.0979, step time: 0.1051\n",
      "47/223, train_loss: 0.1038, step time: 0.1008\n",
      "48/223, train_loss: 0.0961, step time: 0.1010\n",
      "49/223, train_loss: 0.0944, step time: 0.1179\n",
      "50/223, train_loss: 0.1025, step time: 0.1006\n",
      "51/223, train_loss: 0.1001, step time: 0.0999\n",
      "52/223, train_loss: 0.0985, step time: 0.1003\n",
      "53/223, train_loss: 0.0909, step time: 0.1204\n",
      "54/223, train_loss: 0.1081, step time: 0.1007\n",
      "55/223, train_loss: 0.1002, step time: 0.1008\n",
      "56/223, train_loss: 0.1024, step time: 0.0999\n",
      "57/223, train_loss: 0.0920, step time: 0.1037\n",
      "58/223, train_loss: 0.1083, step time: 0.0993\n",
      "59/223, train_loss: 0.1063, step time: 0.1071\n",
      "60/223, train_loss: 0.1047, step time: 0.1013\n",
      "61/223, train_loss: 0.1037, step time: 0.1034\n",
      "62/223, train_loss: 0.1023, step time: 0.1070\n",
      "63/223, train_loss: 0.1027, step time: 0.1053\n",
      "64/223, train_loss: 0.1085, step time: 0.1084\n",
      "65/223, train_loss: 0.0936, step time: 0.1055\n",
      "66/223, train_loss: 0.1151, step time: 0.1008\n",
      "67/223, train_loss: 0.0943, step time: 0.0999\n",
      "68/223, train_loss: 0.1074, step time: 0.1003\n",
      "69/223, train_loss: 0.1166, step time: 0.1114\n",
      "70/223, train_loss: 0.1010, step time: 0.1001\n",
      "71/223, train_loss: 0.0940, step time: 0.1008\n",
      "72/223, train_loss: 0.0951, step time: 0.1005\n",
      "73/223, train_loss: 0.1008, step time: 0.0998\n",
      "74/223, train_loss: 0.0938, step time: 0.1080\n",
      "75/223, train_loss: 0.1034, step time: 0.1098\n",
      "76/223, train_loss: 0.1188, step time: 0.1146\n",
      "77/223, train_loss: 0.1041, step time: 0.1087\n",
      "78/223, train_loss: 0.1105, step time: 0.1005\n",
      "79/223, train_loss: 0.1044, step time: 0.1007\n",
      "80/223, train_loss: 0.1023, step time: 0.0999\n",
      "81/223, train_loss: 0.0999, step time: 0.1001\n",
      "82/223, train_loss: 0.0937, step time: 0.1117\n",
      "83/223, train_loss: 0.1052, step time: 0.1172\n",
      "84/223, train_loss: 0.0945, step time: 0.1008\n",
      "85/223, train_loss: 0.0972, step time: 0.1356\n",
      "86/223, train_loss: 0.0923, step time: 0.1116\n",
      "87/223, train_loss: 0.1050, step time: 0.1181\n",
      "88/223, train_loss: 0.0970, step time: 0.1151\n",
      "89/223, train_loss: 0.1087, step time: 0.1056\n",
      "90/223, train_loss: 0.0980, step time: 0.1008\n",
      "91/223, train_loss: 0.1034, step time: 0.1002\n",
      "92/223, train_loss: 0.0947, step time: 0.1005\n",
      "93/223, train_loss: 0.1021, step time: 0.1164\n",
      "94/223, train_loss: 0.0992, step time: 0.1184\n",
      "95/223, train_loss: 0.1053, step time: 0.1110\n",
      "96/223, train_loss: 0.0963, step time: 0.1454\n",
      "97/223, train_loss: 0.1073, step time: 0.1104\n",
      "98/223, train_loss: 0.1095, step time: 0.1117\n",
      "99/223, train_loss: 0.0995, step time: 0.1152\n",
      "100/223, train_loss: 0.1179, step time: 0.1265\n",
      "101/223, train_loss: 0.0998, step time: 0.1142\n",
      "102/223, train_loss: 0.0965, step time: 0.1178\n",
      "103/223, train_loss: 0.1024, step time: 0.1179\n",
      "104/223, train_loss: 0.0881, step time: 0.1183\n",
      "105/223, train_loss: 0.1226, step time: 0.1038\n",
      "106/223, train_loss: 0.1037, step time: 0.1200\n",
      "107/223, train_loss: 0.1004, step time: 0.1132\n",
      "108/223, train_loss: 0.1073, step time: 0.1202\n",
      "109/223, train_loss: 0.0924, step time: 0.1161\n",
      "110/223, train_loss: 0.1044, step time: 0.1094\n",
      "111/223, train_loss: 0.1086, step time: 0.1140\n",
      "112/223, train_loss: 0.0960, step time: 0.1116\n",
      "113/223, train_loss: 0.0981, step time: 0.1126\n",
      "114/223, train_loss: 0.0961, step time: 0.1169\n",
      "115/223, train_loss: 0.1147, step time: 0.1109\n",
      "116/223, train_loss: 0.1088, step time: 0.1075\n",
      "117/223, train_loss: 0.1032, step time: 0.1260\n",
      "118/223, train_loss: 0.1100, step time: 0.1094\n",
      "119/223, train_loss: 0.1052, step time: 0.1089\n",
      "120/223, train_loss: 0.0966, step time: 0.1141\n",
      "121/223, train_loss: 0.1230, step time: 0.1010\n",
      "122/223, train_loss: 0.0907, step time: 0.1114\n",
      "123/223, train_loss: 0.0942, step time: 0.1158\n",
      "124/223, train_loss: 0.1029, step time: 0.1218\n",
      "125/223, train_loss: 0.1031, step time: 0.1007\n",
      "126/223, train_loss: 0.1051, step time: 0.1110\n",
      "127/223, train_loss: 0.1044, step time: 0.1087\n",
      "128/223, train_loss: 0.1187, step time: 0.1163\n",
      "129/223, train_loss: 0.0931, step time: 0.1087\n",
      "130/223, train_loss: 0.0971, step time: 0.1192\n",
      "131/223, train_loss: 0.1069, step time: 0.1170\n",
      "132/223, train_loss: 0.0995, step time: 0.1181\n",
      "133/223, train_loss: 0.0978, step time: 0.1130\n",
      "134/223, train_loss: 0.1125, step time: 0.1009\n",
      "135/223, train_loss: 0.1256, step time: 0.1200\n",
      "136/223, train_loss: 0.1015, step time: 0.1056\n",
      "137/223, train_loss: 0.1067, step time: 0.1010\n",
      "138/223, train_loss: 0.1038, step time: 0.1123\n",
      "139/223, train_loss: 0.1007, step time: 0.1071\n",
      "140/223, train_loss: 0.0971, step time: 0.1191\n",
      "141/223, train_loss: 0.1075, step time: 0.1225\n",
      "142/223, train_loss: 0.0958, step time: 0.0999\n",
      "143/223, train_loss: 0.0995, step time: 0.1046\n",
      "144/223, train_loss: 0.1016, step time: 0.1170\n",
      "145/223, train_loss: 0.0995, step time: 0.1252\n",
      "146/223, train_loss: 0.1120, step time: 0.1055\n",
      "147/223, train_loss: 0.1147, step time: 0.1215\n",
      "148/223, train_loss: 0.0964, step time: 0.1164\n",
      "149/223, train_loss: 0.1006, step time: 0.1174\n",
      "150/223, train_loss: 0.1060, step time: 0.1011\n",
      "151/223, train_loss: 0.1056, step time: 0.1182\n",
      "152/223, train_loss: 0.0968, step time: 0.1003\n",
      "153/223, train_loss: 0.0992, step time: 0.0994\n",
      "154/223, train_loss: 0.0969, step time: 0.1056\n",
      "155/223, train_loss: 0.0954, step time: 0.1051\n",
      "156/223, train_loss: 0.1021, step time: 0.1000\n",
      "157/223, train_loss: 0.1127, step time: 0.1100\n",
      "158/223, train_loss: 0.0909, step time: 0.1148\n",
      "159/223, train_loss: 0.0922, step time: 0.1028\n",
      "160/223, train_loss: 0.0911, step time: 0.1003\n",
      "161/223, train_loss: 0.1051, step time: 0.1129\n",
      "162/223, train_loss: 0.0933, step time: 0.1203\n",
      "163/223, train_loss: 0.1064, step time: 0.1047\n",
      "164/223, train_loss: 0.1022, step time: 0.1087\n",
      "165/223, train_loss: 0.0984, step time: 0.0999\n",
      "166/223, train_loss: 0.0973, step time: 0.1132\n",
      "167/223, train_loss: 0.1015, step time: 0.1154\n",
      "168/223, train_loss: 0.1124, step time: 0.1011\n",
      "169/223, train_loss: 0.0966, step time: 0.1067\n",
      "170/223, train_loss: 0.0929, step time: 0.1071\n",
      "171/223, train_loss: 0.1034, step time: 0.1037\n",
      "172/223, train_loss: 0.1011, step time: 0.1040\n",
      "173/223, train_loss: 0.1039, step time: 0.1080\n",
      "174/223, train_loss: 0.1000, step time: 0.1269\n",
      "175/223, train_loss: 0.1107, step time: 0.1156\n",
      "176/223, train_loss: 0.0957, step time: 0.1106\n",
      "177/223, train_loss: 0.1139, step time: 0.1211\n",
      "178/223, train_loss: 0.0971, step time: 0.1100\n",
      "179/223, train_loss: 0.1056, step time: 0.0999\n",
      "180/223, train_loss: 0.1038, step time: 0.1457\n",
      "181/223, train_loss: 0.0989, step time: 0.1003\n",
      "182/223, train_loss: 0.1000, step time: 0.1111\n",
      "183/223, train_loss: 0.1048, step time: 0.1034\n",
      "184/223, train_loss: 0.1084, step time: 0.1293\n",
      "185/223, train_loss: 0.1044, step time: 0.1136\n",
      "186/223, train_loss: 0.0957, step time: 0.1215\n",
      "187/223, train_loss: 0.1066, step time: 0.1182\n",
      "188/223, train_loss: 0.0966, step time: 0.0990\n",
      "189/223, train_loss: 0.0993, step time: 0.0996\n",
      "190/223, train_loss: 0.1042, step time: 0.1042\n",
      "191/223, train_loss: 0.0923, step time: 0.1160\n",
      "192/223, train_loss: 0.1219, step time: 0.1372\n",
      "193/223, train_loss: 0.0994, step time: 0.1140\n",
      "194/223, train_loss: 0.1028, step time: 0.1152\n",
      "195/223, train_loss: 0.0896, step time: 0.1423\n",
      "196/223, train_loss: 0.0894, step time: 0.1042\n",
      "197/223, train_loss: 0.1019, step time: 0.1046\n",
      "198/223, train_loss: 0.1096, step time: 0.1353\n",
      "199/223, train_loss: 0.3055, step time: 0.1294\n",
      "200/223, train_loss: 0.1095, step time: 0.1005\n",
      "201/223, train_loss: 0.0990, step time: 0.1003\n",
      "202/223, train_loss: 0.1157, step time: 0.1000\n",
      "203/223, train_loss: 0.1050, step time: 0.1006\n",
      "204/223, train_loss: 0.1036, step time: 0.1004\n",
      "205/223, train_loss: 0.1127, step time: 0.1005\n",
      "206/223, train_loss: 0.0940, step time: 0.1042\n",
      "207/223, train_loss: 0.0938, step time: 0.1108\n",
      "208/223, train_loss: 0.1022, step time: 0.1042\n",
      "209/223, train_loss: 0.1005, step time: 0.1176\n",
      "210/223, train_loss: 0.1087, step time: 0.1262\n",
      "211/223, train_loss: 0.1063, step time: 0.1241\n",
      "212/223, train_loss: 0.1195, step time: 0.1035\n",
      "213/223, train_loss: 0.0971, step time: 0.1089\n",
      "214/223, train_loss: 0.1009, step time: 0.1179\n",
      "215/223, train_loss: 0.1056, step time: 0.1103\n",
      "216/223, train_loss: 0.0955, step time: 0.1257\n",
      "217/223, train_loss: 0.0964, step time: 0.1017\n",
      "218/223, train_loss: 0.1070, step time: 0.1012\n",
      "219/223, train_loss: 0.0977, step time: 0.1045\n",
      "220/223, train_loss: 0.1137, step time: 0.1003\n",
      "221/223, train_loss: 0.1131, step time: 0.0999\n",
      "222/223, train_loss: 0.1112, step time: 0.0991\n",
      "223/223, train_loss: 0.0883, step time: 0.0994\n",
      "epoch 189 average loss: 0.1033\n",
      "time consuming of epoch 189 is: 87.3970\n",
      "----------\n",
      "epoch 190/300\n",
      "1/223, train_loss: 0.1068, step time: 0.1133\n",
      "2/223, train_loss: 0.0980, step time: 0.1122\n",
      "3/223, train_loss: 0.0966, step time: 0.1260\n",
      "4/223, train_loss: 0.0940, step time: 0.1062\n",
      "5/223, train_loss: 0.0937, step time: 0.1012\n",
      "6/223, train_loss: 0.0995, step time: 0.1090\n",
      "7/223, train_loss: 0.1082, step time: 0.0995\n",
      "8/223, train_loss: 0.0961, step time: 0.1001\n",
      "9/223, train_loss: 0.0917, step time: 0.1163\n",
      "10/223, train_loss: 0.1069, step time: 0.1135\n",
      "11/223, train_loss: 0.1074, step time: 0.1121\n",
      "12/223, train_loss: 0.0994, step time: 0.1275\n",
      "13/223, train_loss: 0.1114, step time: 0.1081\n",
      "14/223, train_loss: 0.0901, step time: 0.1117\n",
      "15/223, train_loss: 0.1057, step time: 0.1096\n",
      "16/223, train_loss: 0.1004, step time: 0.1092\n",
      "17/223, train_loss: 0.1105, step time: 0.1084\n",
      "18/223, train_loss: 0.1161, step time: 0.1008\n",
      "19/223, train_loss: 0.0993, step time: 0.1109\n",
      "20/223, train_loss: 0.0975, step time: 0.1039\n",
      "21/223, train_loss: 0.1113, step time: 0.1208\n",
      "22/223, train_loss: 0.0890, step time: 0.1010\n",
      "23/223, train_loss: 0.1008, step time: 0.1015\n",
      "24/223, train_loss: 0.0972, step time: 0.1168\n",
      "25/223, train_loss: 0.1049, step time: 0.1109\n",
      "26/223, train_loss: 0.1125, step time: 0.1150\n",
      "27/223, train_loss: 0.1046, step time: 0.1004\n",
      "28/223, train_loss: 0.0980, step time: 0.1178\n",
      "29/223, train_loss: 0.1010, step time: 0.1180\n",
      "30/223, train_loss: 0.1209, step time: 0.1075\n",
      "31/223, train_loss: 0.0913, step time: 0.1002\n",
      "32/223, train_loss: 0.0965, step time: 0.1015\n",
      "33/223, train_loss: 0.0974, step time: 0.1106\n",
      "34/223, train_loss: 0.0961, step time: 0.1170\n",
      "35/223, train_loss: 0.1003, step time: 0.1325\n",
      "36/223, train_loss: 0.1031, step time: 0.1444\n",
      "37/223, train_loss: 0.1100, step time: 0.1134\n",
      "38/223, train_loss: 0.0946, step time: 0.1220\n",
      "39/223, train_loss: 0.1005, step time: 0.1190\n",
      "40/223, train_loss: 0.1002, step time: 0.1176\n",
      "41/223, train_loss: 0.0994, step time: 0.1131\n",
      "42/223, train_loss: 0.1138, step time: 0.1258\n",
      "43/223, train_loss: 0.1142, step time: 0.1085\n",
      "44/223, train_loss: 0.1043, step time: 0.1147\n",
      "45/223, train_loss: 0.1038, step time: 0.1050\n",
      "46/223, train_loss: 0.1101, step time: 0.1169\n",
      "47/223, train_loss: 0.0980, step time: 0.1375\n",
      "48/223, train_loss: 0.1138, step time: 0.1044\n",
      "49/223, train_loss: 0.1030, step time: 0.1141\n",
      "50/223, train_loss: 0.0982, step time: 0.1157\n",
      "51/223, train_loss: 0.1021, step time: 0.0998\n",
      "52/223, train_loss: 0.0902, step time: 0.1006\n",
      "53/223, train_loss: 0.0915, step time: 0.0999\n",
      "54/223, train_loss: 0.1064, step time: 0.1203\n",
      "55/223, train_loss: 0.1118, step time: 0.1047\n",
      "56/223, train_loss: 0.1097, step time: 0.1215\n",
      "57/223, train_loss: 0.1109, step time: 0.1003\n",
      "58/223, train_loss: 0.1115, step time: 0.1036\n",
      "59/223, train_loss: 0.0993, step time: 0.1443\n",
      "60/223, train_loss: 0.1135, step time: 0.1131\n",
      "61/223, train_loss: 0.1057, step time: 0.1058\n",
      "62/223, train_loss: 0.1065, step time: 0.1217\n",
      "63/223, train_loss: 0.0999, step time: 0.1075\n",
      "64/223, train_loss: 0.1100, step time: 0.1001\n",
      "65/223, train_loss: 0.0999, step time: 0.0996\n",
      "66/223, train_loss: 0.1002, step time: 0.1187\n",
      "67/223, train_loss: 0.1001, step time: 0.1263\n",
      "68/223, train_loss: 0.1141, step time: 0.1140\n",
      "69/223, train_loss: 0.1053, step time: 0.1158\n",
      "70/223, train_loss: 0.1143, step time: 0.1000\n",
      "71/223, train_loss: 0.1022, step time: 0.1294\n",
      "72/223, train_loss: 0.1024, step time: 0.1169\n",
      "73/223, train_loss: 0.0910, step time: 0.1077\n",
      "74/223, train_loss: 0.1085, step time: 0.1275\n",
      "75/223, train_loss: 0.1051, step time: 0.1009\n",
      "76/223, train_loss: 0.1026, step time: 0.1000\n",
      "77/223, train_loss: 0.1123, step time: 0.1071\n",
      "78/223, train_loss: 0.0984, step time: 0.1164\n",
      "79/223, train_loss: 0.1005, step time: 0.1087\n",
      "80/223, train_loss: 0.0955, step time: 0.1197\n",
      "81/223, train_loss: 0.0964, step time: 0.1232\n",
      "82/223, train_loss: 0.0941, step time: 0.1090\n",
      "83/223, train_loss: 0.0992, step time: 0.1149\n",
      "84/223, train_loss: 0.1181, step time: 0.1756\n",
      "85/223, train_loss: 0.1023, step time: 0.1013\n",
      "86/223, train_loss: 0.0937, step time: 0.1054\n",
      "87/223, train_loss: 0.0992, step time: 0.1871\n",
      "88/223, train_loss: 0.1031, step time: 0.1211\n",
      "89/223, train_loss: 0.0998, step time: 0.1149\n",
      "90/223, train_loss: 0.1001, step time: 0.1031\n",
      "91/223, train_loss: 0.0926, step time: 0.1125\n",
      "92/223, train_loss: 0.0957, step time: 0.1215\n",
      "93/223, train_loss: 0.0973, step time: 0.1127\n",
      "94/223, train_loss: 0.0987, step time: 0.1064\n",
      "95/223, train_loss: 0.1068, step time: 0.1122\n",
      "96/223, train_loss: 0.0993, step time: 0.1514\n",
      "97/223, train_loss: 0.0967, step time: 0.1326\n",
      "98/223, train_loss: 0.0988, step time: 0.1129\n",
      "99/223, train_loss: 0.1066, step time: 0.1096\n",
      "100/223, train_loss: 0.0934, step time: 0.1131\n",
      "101/223, train_loss: 0.0973, step time: 0.1463\n",
      "102/223, train_loss: 0.0996, step time: 0.0995\n",
      "103/223, train_loss: 0.0940, step time: 0.1053\n",
      "104/223, train_loss: 0.1016, step time: 0.1097\n",
      "105/223, train_loss: 0.1096, step time: 0.1687\n",
      "106/223, train_loss: 0.1019, step time: 0.1027\n",
      "107/223, train_loss: 0.1061, step time: 0.1175\n",
      "108/223, train_loss: 0.0963, step time: 0.1198\n",
      "109/223, train_loss: 0.0910, step time: 0.1082\n",
      "110/223, train_loss: 0.0932, step time: 0.0998\n",
      "111/223, train_loss: 0.0967, step time: 0.1130\n",
      "112/223, train_loss: 0.1110, step time: 0.1083\n",
      "113/223, train_loss: 0.1016, step time: 0.1220\n",
      "114/223, train_loss: 0.1119, step time: 0.1035\n",
      "115/223, train_loss: 0.1037, step time: 0.1199\n",
      "116/223, train_loss: 0.1104, step time: 0.1193\n",
      "117/223, train_loss: 0.1006, step time: 0.1080\n",
      "118/223, train_loss: 0.1093, step time: 0.0998\n",
      "119/223, train_loss: 0.1043, step time: 0.1099\n",
      "120/223, train_loss: 0.1053, step time: 0.1070\n",
      "121/223, train_loss: 0.0980, step time: 0.1130\n",
      "122/223, train_loss: 0.0992, step time: 0.0993\n",
      "123/223, train_loss: 0.1118, step time: 0.1012\n",
      "124/223, train_loss: 0.1099, step time: 0.1017\n",
      "125/223, train_loss: 0.0979, step time: 0.1007\n",
      "126/223, train_loss: 0.0969, step time: 0.1003\n",
      "127/223, train_loss: 0.0913, step time: 0.1008\n",
      "128/223, train_loss: 0.0943, step time: 0.1010\n",
      "129/223, train_loss: 0.1024, step time: 0.1006\n",
      "130/223, train_loss: 0.0933, step time: 0.1035\n",
      "131/223, train_loss: 0.1063, step time: 0.1168\n",
      "132/223, train_loss: 0.1138, step time: 0.1176\n",
      "133/223, train_loss: 0.1010, step time: 0.1177\n",
      "134/223, train_loss: 0.1095, step time: 0.1062\n",
      "135/223, train_loss: 0.1019, step time: 0.1055\n",
      "136/223, train_loss: 0.1032, step time: 0.1090\n",
      "137/223, train_loss: 0.1018, step time: 0.1216\n",
      "138/223, train_loss: 0.0996, step time: 0.1108\n",
      "139/223, train_loss: 0.1012, step time: 0.1080\n",
      "140/223, train_loss: 0.1140, step time: 0.1057\n",
      "141/223, train_loss: 0.1035, step time: 0.1165\n",
      "142/223, train_loss: 0.1047, step time: 0.1019\n",
      "143/223, train_loss: 0.0998, step time: 0.1115\n",
      "144/223, train_loss: 0.1073, step time: 0.1032\n",
      "145/223, train_loss: 0.0993, step time: 0.1032\n",
      "146/223, train_loss: 0.1149, step time: 0.1332\n",
      "147/223, train_loss: 0.0913, step time: 0.1060\n",
      "148/223, train_loss: 0.1009, step time: 0.1154\n",
      "149/223, train_loss: 0.0947, step time: 0.1122\n",
      "150/223, train_loss: 0.1011, step time: 0.1016\n",
      "151/223, train_loss: 0.1017, step time: 0.1004\n",
      "152/223, train_loss: 0.0915, step time: 0.1478\n",
      "153/223, train_loss: 0.1122, step time: 0.0994\n",
      "154/223, train_loss: 0.0976, step time: 0.1548\n",
      "155/223, train_loss: 0.1058, step time: 0.1152\n",
      "156/223, train_loss: 0.1159, step time: 0.1110\n",
      "157/223, train_loss: 0.0955, step time: 0.1001\n",
      "158/223, train_loss: 0.1103, step time: 0.1047\n",
      "159/223, train_loss: 0.0973, step time: 0.1154\n",
      "160/223, train_loss: 0.0937, step time: 0.1457\n",
      "161/223, train_loss: 0.1124, step time: 0.1227\n",
      "162/223, train_loss: 0.1045, step time: 0.0995\n",
      "163/223, train_loss: 0.0942, step time: 0.1029\n",
      "164/223, train_loss: 0.1046, step time: 0.1045\n",
      "165/223, train_loss: 0.1022, step time: 0.1187\n",
      "166/223, train_loss: 0.1026, step time: 0.1138\n",
      "167/223, train_loss: 0.0997, step time: 0.1353\n",
      "168/223, train_loss: 0.0925, step time: 0.1430\n",
      "169/223, train_loss: 0.1028, step time: 0.0991\n",
      "170/223, train_loss: 0.1076, step time: 0.1136\n",
      "171/223, train_loss: 0.1201, step time: 0.1016\n",
      "172/223, train_loss: 0.1241, step time: 0.1584\n",
      "173/223, train_loss: 0.0964, step time: 0.1155\n",
      "174/223, train_loss: 0.0985, step time: 0.1226\n",
      "175/223, train_loss: 0.1049, step time: 0.0993\n",
      "176/223, train_loss: 0.0910, step time: 0.1581\n",
      "177/223, train_loss: 0.1011, step time: 0.1294\n",
      "178/223, train_loss: 0.1029, step time: 0.1350\n",
      "179/223, train_loss: 0.0942, step time: 0.1004\n",
      "180/223, train_loss: 0.0946, step time: 0.1001\n",
      "181/223, train_loss: 0.1175, step time: 0.1042\n",
      "182/223, train_loss: 0.1039, step time: 0.1216\n",
      "183/223, train_loss: 0.0997, step time: 0.1124\n",
      "184/223, train_loss: 0.0974, step time: 0.1317\n",
      "185/223, train_loss: 0.0955, step time: 0.1241\n",
      "186/223, train_loss: 0.1160, step time: 0.1002\n",
      "187/223, train_loss: 0.0993, step time: 0.1000\n",
      "188/223, train_loss: 0.1035, step time: 0.1010\n",
      "189/223, train_loss: 0.1062, step time: 0.1010\n",
      "190/223, train_loss: 0.1200, step time: 0.1013\n",
      "191/223, train_loss: 0.1062, step time: 0.1031\n",
      "192/223, train_loss: 0.1107, step time: 0.1021\n",
      "193/223, train_loss: 0.0942, step time: 0.1004\n",
      "194/223, train_loss: 0.1037, step time: 0.0998\n",
      "195/223, train_loss: 0.0926, step time: 0.0983\n",
      "196/223, train_loss: 0.0971, step time: 0.0994\n",
      "197/223, train_loss: 0.0908, step time: 0.0988\n",
      "198/223, train_loss: 0.0904, step time: 0.1123\n",
      "199/223, train_loss: 0.0986, step time: 0.1086\n",
      "200/223, train_loss: 0.0954, step time: 0.1339\n",
      "201/223, train_loss: 0.0999, step time: 0.1185\n",
      "202/223, train_loss: 0.0908, step time: 0.0992\n",
      "203/223, train_loss: 0.0946, step time: 0.1045\n",
      "204/223, train_loss: 0.1072, step time: 0.1414\n",
      "205/223, train_loss: 0.0992, step time: 0.1139\n",
      "206/223, train_loss: 0.1030, step time: 0.1086\n",
      "207/223, train_loss: 0.1001, step time: 0.1145\n",
      "208/223, train_loss: 0.0965, step time: 0.1004\n",
      "209/223, train_loss: 0.1051, step time: 0.0998\n",
      "210/223, train_loss: 0.3112, step time: 0.1229\n",
      "211/223, train_loss: 0.1031, step time: 0.1160\n",
      "212/223, train_loss: 0.0962, step time: 0.1132\n",
      "213/223, train_loss: 0.0918, step time: 0.1020\n",
      "214/223, train_loss: 0.0903, step time: 0.1142\n",
      "215/223, train_loss: 0.1040, step time: 0.1004\n",
      "216/223, train_loss: 0.1133, step time: 0.0990\n",
      "217/223, train_loss: 0.0986, step time: 0.1008\n",
      "218/223, train_loss: 0.1020, step time: 0.1006\n",
      "219/223, train_loss: 0.1110, step time: 0.1014\n",
      "220/223, train_loss: 0.1110, step time: 0.1022\n",
      "221/223, train_loss: 0.1111, step time: 0.1003\n",
      "222/223, train_loss: 0.1048, step time: 0.0998\n",
      "223/223, train_loss: 0.1088, step time: 0.0992\n",
      "epoch 190 average loss: 0.1032\n",
      "current epoch: 190 current mean dice: 0.8599 tc: 0.9213 wt: 0.8695 et: 0.7889\n",
      "best mean dice: 0.8600 at epoch: 170\n",
      "time consuming of epoch 190 is: 89.9039\n",
      "----------\n",
      "epoch 191/300\n",
      "1/223, train_loss: 0.0984, step time: 0.1075\n",
      "2/223, train_loss: 0.1109, step time: 0.1013\n",
      "3/223, train_loss: 0.0965, step time: 0.1003\n",
      "4/223, train_loss: 0.0958, step time: 0.0999\n",
      "5/223, train_loss: 0.1002, step time: 0.1019\n",
      "6/223, train_loss: 0.0986, step time: 0.1091\n",
      "7/223, train_loss: 0.1118, step time: 0.1033\n",
      "8/223, train_loss: 0.0987, step time: 0.1151\n",
      "9/223, train_loss: 0.0968, step time: 0.1145\n",
      "10/223, train_loss: 0.0996, step time: 0.1141\n",
      "11/223, train_loss: 0.1092, step time: 0.0996\n",
      "12/223, train_loss: 0.0983, step time: 0.1158\n",
      "13/223, train_loss: 0.0993, step time: 0.1002\n",
      "14/223, train_loss: 0.1041, step time: 0.1027\n",
      "15/223, train_loss: 0.1194, step time: 0.1165\n",
      "16/223, train_loss: 0.0930, step time: 0.1118\n",
      "17/223, train_loss: 0.1000, step time: 0.1180\n",
      "18/223, train_loss: 0.0955, step time: 0.1034\n",
      "19/223, train_loss: 0.1135, step time: 0.1023\n",
      "20/223, train_loss: 0.0948, step time: 0.1236\n",
      "21/223, train_loss: 0.0962, step time: 0.1059\n",
      "22/223, train_loss: 0.0951, step time: 0.1107\n",
      "23/223, train_loss: 0.1082, step time: 0.1150\n",
      "24/223, train_loss: 0.1009, step time: 0.1321\n",
      "25/223, train_loss: 0.0951, step time: 0.1257\n",
      "26/223, train_loss: 0.1105, step time: 0.1192\n",
      "27/223, train_loss: 0.1005, step time: 0.1134\n",
      "28/223, train_loss: 0.1041, step time: 0.1008\n",
      "29/223, train_loss: 0.1026, step time: 0.1273\n",
      "30/223, train_loss: 0.1014, step time: 0.1123\n",
      "31/223, train_loss: 0.1015, step time: 0.1060\n",
      "32/223, train_loss: 0.0950, step time: 0.1204\n",
      "33/223, train_loss: 0.1044, step time: 0.1412\n",
      "34/223, train_loss: 0.1157, step time: 0.1230\n",
      "35/223, train_loss: 0.0986, step time: 0.1013\n",
      "36/223, train_loss: 0.1050, step time: 0.1146\n",
      "37/223, train_loss: 0.0991, step time: 0.0996\n",
      "38/223, train_loss: 0.0931, step time: 0.1082\n",
      "39/223, train_loss: 0.0990, step time: 0.1018\n",
      "40/223, train_loss: 0.0964, step time: 0.1081\n",
      "41/223, train_loss: 0.0997, step time: 0.1186\n",
      "42/223, train_loss: 0.1012, step time: 0.1189\n",
      "43/223, train_loss: 0.0940, step time: 0.0999\n",
      "44/223, train_loss: 0.1002, step time: 0.1004\n",
      "45/223, train_loss: 0.1000, step time: 0.1085\n",
      "46/223, train_loss: 0.1037, step time: 0.1090\n",
      "47/223, train_loss: 0.1055, step time: 0.1110\n",
      "48/223, train_loss: 0.1056, step time: 0.1199\n",
      "49/223, train_loss: 0.0908, step time: 0.1085\n",
      "50/223, train_loss: 0.0990, step time: 0.0998\n",
      "51/223, train_loss: 0.0948, step time: 0.0999\n",
      "52/223, train_loss: 0.1069, step time: 0.1030\n",
      "53/223, train_loss: 0.1089, step time: 0.1095\n",
      "54/223, train_loss: 0.0971, step time: 0.1039\n",
      "55/223, train_loss: 0.1051, step time: 0.1174\n",
      "56/223, train_loss: 0.0949, step time: 0.1013\n",
      "57/223, train_loss: 0.1035, step time: 0.1104\n",
      "58/223, train_loss: 0.1049, step time: 0.0997\n",
      "59/223, train_loss: 0.0941, step time: 0.1344\n",
      "60/223, train_loss: 0.0953, step time: 0.1248\n",
      "61/223, train_loss: 0.1010, step time: 0.1008\n",
      "62/223, train_loss: 0.0982, step time: 0.1092\n",
      "63/223, train_loss: 0.1049, step time: 0.1005\n",
      "64/223, train_loss: 0.1110, step time: 0.1000\n",
      "65/223, train_loss: 0.1148, step time: 0.1063\n",
      "66/223, train_loss: 0.0978, step time: 0.1003\n",
      "67/223, train_loss: 0.1112, step time: 0.1003\n",
      "68/223, train_loss: 0.1191, step time: 0.1077\n",
      "69/223, train_loss: 0.1143, step time: 0.1014\n",
      "70/223, train_loss: 0.1111, step time: 0.0999\n",
      "71/223, train_loss: 0.1134, step time: 0.1126\n",
      "72/223, train_loss: 0.1056, step time: 0.1042\n",
      "73/223, train_loss: 0.1045, step time: 0.1067\n",
      "74/223, train_loss: 0.0877, step time: 0.1006\n",
      "75/223, train_loss: 0.1057, step time: 0.1006\n",
      "76/223, train_loss: 0.1060, step time: 0.1018\n",
      "77/223, train_loss: 0.0980, step time: 0.1151\n",
      "78/223, train_loss: 0.0974, step time: 0.1008\n",
      "79/223, train_loss: 0.0954, step time: 0.1006\n",
      "80/223, train_loss: 0.0958, step time: 0.1023\n",
      "81/223, train_loss: 0.0930, step time: 0.1144\n",
      "82/223, train_loss: 0.1047, step time: 0.1003\n",
      "83/223, train_loss: 0.1067, step time: 0.1009\n",
      "84/223, train_loss: 0.1096, step time: 0.1316\n",
      "85/223, train_loss: 0.1058, step time: 0.1102\n",
      "86/223, train_loss: 0.1041, step time: 0.1073\n",
      "87/223, train_loss: 0.1128, step time: 0.1103\n",
      "88/223, train_loss: 0.1066, step time: 0.1016\n",
      "89/223, train_loss: 0.1074, step time: 0.1112\n",
      "90/223, train_loss: 0.0964, step time: 0.1012\n",
      "91/223, train_loss: 0.1038, step time: 0.1099\n",
      "92/223, train_loss: 0.1042, step time: 0.1121\n",
      "93/223, train_loss: 0.0934, step time: 0.1079\n",
      "94/223, train_loss: 0.1042, step time: 0.1261\n",
      "95/223, train_loss: 0.1068, step time: 0.1207\n",
      "96/223, train_loss: 0.0956, step time: 0.1233\n",
      "97/223, train_loss: 0.1061, step time: 0.1068\n",
      "98/223, train_loss: 0.1114, step time: 0.1006\n",
      "99/223, train_loss: 0.1030, step time: 0.1034\n",
      "100/223, train_loss: 0.1040, step time: 0.1003\n",
      "101/223, train_loss: 0.1110, step time: 0.1130\n",
      "102/223, train_loss: 0.0984, step time: 0.1177\n",
      "103/223, train_loss: 0.1006, step time: 0.1393\n",
      "104/223, train_loss: 0.1014, step time: 0.1390\n",
      "105/223, train_loss: 0.1074, step time: 0.1005\n",
      "106/223, train_loss: 0.1147, step time: 0.1005\n",
      "107/223, train_loss: 0.1270, step time: 0.1006\n",
      "108/223, train_loss: 0.0954, step time: 0.1005\n",
      "109/223, train_loss: 0.1097, step time: 0.1009\n",
      "110/223, train_loss: 0.1010, step time: 0.1103\n",
      "111/223, train_loss: 0.1009, step time: 0.1009\n",
      "112/223, train_loss: 0.1061, step time: 0.1004\n",
      "113/223, train_loss: 0.1051, step time: 0.1111\n",
      "114/223, train_loss: 0.0972, step time: 0.1006\n",
      "115/223, train_loss: 0.1125, step time: 0.1021\n",
      "116/223, train_loss: 0.1142, step time: 0.1002\n",
      "117/223, train_loss: 0.0932, step time: 0.1047\n",
      "118/223, train_loss: 0.1134, step time: 0.1174\n",
      "119/223, train_loss: 0.1074, step time: 0.1267\n",
      "120/223, train_loss: 0.0975, step time: 0.1381\n",
      "121/223, train_loss: 0.0955, step time: 0.1211\n",
      "122/223, train_loss: 0.0989, step time: 0.1080\n",
      "123/223, train_loss: 0.0932, step time: 0.1291\n",
      "124/223, train_loss: 0.1072, step time: 0.1007\n",
      "125/223, train_loss: 0.0924, step time: 0.1091\n",
      "126/223, train_loss: 0.1029, step time: 0.1001\n",
      "127/223, train_loss: 0.1032, step time: 0.1001\n",
      "128/223, train_loss: 0.1043, step time: 0.1224\n",
      "129/223, train_loss: 0.0967, step time: 0.0998\n",
      "130/223, train_loss: 0.0957, step time: 0.1105\n",
      "131/223, train_loss: 0.0997, step time: 0.1003\n",
      "132/223, train_loss: 0.1093, step time: 0.1122\n",
      "133/223, train_loss: 0.1074, step time: 0.1003\n",
      "134/223, train_loss: 0.0981, step time: 0.1016\n",
      "135/223, train_loss: 0.1009, step time: 0.1099\n",
      "136/223, train_loss: 0.1047, step time: 0.1097\n",
      "137/223, train_loss: 0.1036, step time: 0.1006\n",
      "138/223, train_loss: 0.1110, step time: 0.1063\n",
      "139/223, train_loss: 0.1041, step time: 0.1133\n",
      "140/223, train_loss: 0.1001, step time: 0.1234\n",
      "141/223, train_loss: 0.1018, step time: 0.1090\n",
      "142/223, train_loss: 0.1153, step time: 0.1082\n",
      "143/223, train_loss: 0.1111, step time: 0.1001\n",
      "144/223, train_loss: 0.1156, step time: 0.1199\n",
      "145/223, train_loss: 0.1115, step time: 0.1196\n",
      "146/223, train_loss: 0.1070, step time: 0.1074\n",
      "147/223, train_loss: 0.0933, step time: 0.1173\n",
      "148/223, train_loss: 0.0995, step time: 0.1155\n",
      "149/223, train_loss: 0.1046, step time: 0.1149\n",
      "150/223, train_loss: 0.0935, step time: 0.1103\n",
      "151/223, train_loss: 0.0975, step time: 0.1050\n",
      "152/223, train_loss: 0.1026, step time: 0.1566\n",
      "153/223, train_loss: 0.1098, step time: 0.1157\n",
      "154/223, train_loss: 0.0978, step time: 0.1073\n",
      "155/223, train_loss: 0.1011, step time: 0.1166\n",
      "156/223, train_loss: 0.0966, step time: 0.1298\n",
      "157/223, train_loss: 0.1051, step time: 0.1103\n",
      "158/223, train_loss: 0.1075, step time: 0.1002\n",
      "159/223, train_loss: 0.1039, step time: 0.1180\n",
      "160/223, train_loss: 0.0995, step time: 0.1128\n",
      "161/223, train_loss: 0.1023, step time: 0.1007\n",
      "162/223, train_loss: 0.0944, step time: 0.1001\n",
      "163/223, train_loss: 0.0974, step time: 0.1240\n",
      "164/223, train_loss: 0.1047, step time: 0.1032\n",
      "165/223, train_loss: 0.1002, step time: 0.1089\n",
      "166/223, train_loss: 0.1000, step time: 0.1145\n",
      "167/223, train_loss: 0.1160, step time: 0.1251\n",
      "168/223, train_loss: 0.1090, step time: 0.0998\n",
      "169/223, train_loss: 0.0986, step time: 0.1049\n",
      "170/223, train_loss: 0.1035, step time: 0.1088\n",
      "171/223, train_loss: 0.1172, step time: 0.1102\n",
      "172/223, train_loss: 0.0979, step time: 0.1095\n",
      "173/223, train_loss: 0.1032, step time: 0.1092\n",
      "174/223, train_loss: 0.0988, step time: 0.1045\n",
      "175/223, train_loss: 0.0928, step time: 0.1222\n",
      "176/223, train_loss: 0.1086, step time: 0.1045\n",
      "177/223, train_loss: 0.1094, step time: 0.1078\n",
      "178/223, train_loss: 0.1058, step time: 0.1308\n",
      "179/223, train_loss: 0.0962, step time: 0.1424\n",
      "180/223, train_loss: 0.1017, step time: 0.1477\n",
      "181/223, train_loss: 0.0959, step time: 0.0998\n",
      "182/223, train_loss: 0.1196, step time: 0.1011\n",
      "183/223, train_loss: 0.1022, step time: 0.1011\n",
      "184/223, train_loss: 0.1089, step time: 0.1173\n",
      "185/223, train_loss: 0.0933, step time: 0.1125\n",
      "186/223, train_loss: 0.0922, step time: 0.1256\n",
      "187/223, train_loss: 0.1034, step time: 0.1277\n",
      "188/223, train_loss: 0.0939, step time: 0.1018\n",
      "189/223, train_loss: 0.0965, step time: 0.1543\n",
      "190/223, train_loss: 0.1095, step time: 0.1256\n",
      "191/223, train_loss: 0.1097, step time: 0.1000\n",
      "192/223, train_loss: 0.1016, step time: 0.1012\n",
      "193/223, train_loss: 0.0944, step time: 0.1085\n",
      "194/223, train_loss: 0.0952, step time: 0.1112\n",
      "195/223, train_loss: 0.0981, step time: 0.1209\n",
      "196/223, train_loss: 0.1041, step time: 0.1011\n",
      "197/223, train_loss: 0.1043, step time: 0.1077\n",
      "198/223, train_loss: 0.1010, step time: 0.1096\n",
      "199/223, train_loss: 0.3044, step time: 0.1041\n",
      "200/223, train_loss: 0.0966, step time: 0.1043\n",
      "201/223, train_loss: 0.1090, step time: 0.1199\n",
      "202/223, train_loss: 0.0942, step time: 0.1001\n",
      "203/223, train_loss: 0.1047, step time: 0.1055\n",
      "204/223, train_loss: 0.1075, step time: 0.1217\n",
      "205/223, train_loss: 0.1049, step time: 0.1224\n",
      "206/223, train_loss: 0.1056, step time: 0.0992\n",
      "207/223, train_loss: 0.0988, step time: 0.0986\n",
      "208/223, train_loss: 0.0978, step time: 0.0989\n",
      "209/223, train_loss: 0.0945, step time: 0.1103\n",
      "210/223, train_loss: 0.1048, step time: 0.1006\n",
      "211/223, train_loss: 0.1064, step time: 0.1010\n",
      "212/223, train_loss: 0.0964, step time: 0.1115\n",
      "213/223, train_loss: 0.1160, step time: 0.1033\n",
      "214/223, train_loss: 0.1006, step time: 0.1063\n",
      "215/223, train_loss: 0.0971, step time: 0.1003\n",
      "216/223, train_loss: 0.0951, step time: 0.1005\n",
      "217/223, train_loss: 0.1019, step time: 0.0995\n",
      "218/223, train_loss: 0.0933, step time: 0.1016\n",
      "219/223, train_loss: 0.0954, step time: 0.1162\n",
      "220/223, train_loss: 0.0952, step time: 0.1066\n",
      "221/223, train_loss: 0.0978, step time: 0.1004\n",
      "222/223, train_loss: 0.1037, step time: 0.1006\n",
      "223/223, train_loss: 0.1062, step time: 0.1001\n",
      "epoch 191 average loss: 0.1034\n",
      "time consuming of epoch 191 is: 88.8037\n",
      "----------\n",
      "epoch 192/300\n",
      "1/223, train_loss: 0.1124, step time: 0.1028\n",
      "2/223, train_loss: 0.0976, step time: 0.1003\n",
      "3/223, train_loss: 0.1044, step time: 0.1257\n",
      "4/223, train_loss: 0.0999, step time: 0.1000\n",
      "5/223, train_loss: 0.1054, step time: 0.1006\n",
      "6/223, train_loss: 0.0949, step time: 0.1008\n",
      "7/223, train_loss: 0.0969, step time: 0.1096\n",
      "8/223, train_loss: 0.1167, step time: 0.1061\n",
      "9/223, train_loss: 0.0930, step time: 0.1004\n",
      "10/223, train_loss: 0.1023, step time: 0.1169\n",
      "11/223, train_loss: 0.0935, step time: 0.1311\n",
      "12/223, train_loss: 0.1025, step time: 0.1102\n",
      "13/223, train_loss: 0.0988, step time: 0.0989\n",
      "14/223, train_loss: 0.0922, step time: 0.1021\n",
      "15/223, train_loss: 0.1001, step time: 0.2000\n",
      "16/223, train_loss: 0.0995, step time: 0.1055\n",
      "17/223, train_loss: 0.0947, step time: 0.1086\n",
      "18/223, train_loss: 0.0947, step time: 0.1066\n",
      "19/223, train_loss: 0.0962, step time: 0.1211\n",
      "20/223, train_loss: 0.1123, step time: 0.1076\n",
      "21/223, train_loss: 0.1034, step time: 0.1209\n",
      "22/223, train_loss: 0.1058, step time: 0.1035\n",
      "23/223, train_loss: 0.1107, step time: 0.1043\n",
      "24/223, train_loss: 0.0995, step time: 0.1137\n",
      "25/223, train_loss: 0.1077, step time: 0.1159\n",
      "26/223, train_loss: 0.1144, step time: 0.1051\n",
      "27/223, train_loss: 0.1129, step time: 0.1216\n",
      "28/223, train_loss: 0.1018, step time: 0.1234\n",
      "29/223, train_loss: 0.1140, step time: 0.1019\n",
      "30/223, train_loss: 0.1048, step time: 0.1231\n",
      "31/223, train_loss: 0.1034, step time: 0.1179\n",
      "32/223, train_loss: 0.1132, step time: 0.1118\n",
      "33/223, train_loss: 0.0994, step time: 0.1002\n",
      "34/223, train_loss: 0.1069, step time: 0.1478\n",
      "35/223, train_loss: 0.1005, step time: 0.1162\n",
      "36/223, train_loss: 0.1075, step time: 0.0998\n",
      "37/223, train_loss: 0.1030, step time: 0.1201\n",
      "38/223, train_loss: 0.1111, step time: 0.1027\n",
      "39/223, train_loss: 0.0993, step time: 0.0995\n",
      "40/223, train_loss: 0.0890, step time: 0.1136\n",
      "41/223, train_loss: 0.1017, step time: 0.1003\n",
      "42/223, train_loss: 0.0933, step time: 0.1007\n",
      "43/223, train_loss: 0.1022, step time: 0.1024\n",
      "44/223, train_loss: 0.0967, step time: 0.0990\n",
      "45/223, train_loss: 0.1004, step time: 0.1119\n",
      "46/223, train_loss: 0.0999, step time: 0.1075\n",
      "47/223, train_loss: 0.1109, step time: 0.1003\n",
      "48/223, train_loss: 0.1020, step time: 0.1133\n",
      "49/223, train_loss: 0.0977, step time: 0.1202\n",
      "50/223, train_loss: 0.1042, step time: 0.1120\n",
      "51/223, train_loss: 0.0947, step time: 0.1127\n",
      "52/223, train_loss: 0.0875, step time: 0.1084\n",
      "53/223, train_loss: 0.0911, step time: 0.1157\n",
      "54/223, train_loss: 0.0949, step time: 0.1138\n",
      "55/223, train_loss: 0.1021, step time: 0.1016\n",
      "56/223, train_loss: 0.0947, step time: 0.1002\n",
      "57/223, train_loss: 0.0884, step time: 0.1020\n",
      "58/223, train_loss: 0.1196, step time: 0.1036\n",
      "59/223, train_loss: 0.1082, step time: 0.1025\n",
      "60/223, train_loss: 0.1029, step time: 0.1003\n",
      "61/223, train_loss: 0.0963, step time: 0.1215\n",
      "62/223, train_loss: 0.1062, step time: 0.1077\n",
      "63/223, train_loss: 0.1008, step time: 0.1342\n",
      "64/223, train_loss: 0.1006, step time: 0.1148\n",
      "65/223, train_loss: 0.0985, step time: 0.1011\n",
      "66/223, train_loss: 0.0938, step time: 0.1273\n",
      "67/223, train_loss: 0.1049, step time: 0.1077\n",
      "68/223, train_loss: 0.0978, step time: 0.1051\n",
      "69/223, train_loss: 0.0978, step time: 0.1207\n",
      "70/223, train_loss: 0.1114, step time: 0.1089\n",
      "71/223, train_loss: 0.1045, step time: 0.1182\n",
      "72/223, train_loss: 0.1040, step time: 0.1103\n",
      "73/223, train_loss: 0.1161, step time: 0.1064\n",
      "74/223, train_loss: 0.1122, step time: 0.1111\n",
      "75/223, train_loss: 0.1012, step time: 0.1052\n",
      "76/223, train_loss: 0.1054, step time: 0.1055\n",
      "77/223, train_loss: 0.1101, step time: 0.1051\n",
      "78/223, train_loss: 0.0951, step time: 0.1060\n",
      "79/223, train_loss: 0.0887, step time: 0.1085\n",
      "80/223, train_loss: 0.0991, step time: 0.1514\n",
      "81/223, train_loss: 0.0977, step time: 0.1022\n",
      "82/223, train_loss: 0.1131, step time: 0.1148\n",
      "83/223, train_loss: 0.0956, step time: 0.1019\n",
      "84/223, train_loss: 0.1102, step time: 0.1191\n",
      "85/223, train_loss: 0.1000, step time: 0.1193\n",
      "86/223, train_loss: 0.0991, step time: 0.1147\n",
      "87/223, train_loss: 0.0910, step time: 0.1292\n",
      "88/223, train_loss: 0.1097, step time: 0.1178\n",
      "89/223, train_loss: 0.1073, step time: 0.1263\n",
      "90/223, train_loss: 0.0989, step time: 0.1235\n",
      "91/223, train_loss: 0.1046, step time: 0.1006\n",
      "92/223, train_loss: 0.0992, step time: 0.1015\n",
      "93/223, train_loss: 0.1031, step time: 0.1176\n",
      "94/223, train_loss: 0.1083, step time: 0.1165\n",
      "95/223, train_loss: 0.1095, step time: 0.1316\n",
      "96/223, train_loss: 0.1073, step time: 0.1187\n",
      "97/223, train_loss: 0.1100, step time: 0.1158\n",
      "98/223, train_loss: 0.0992, step time: 0.1167\n",
      "99/223, train_loss: 0.0954, step time: 0.1159\n",
      "100/223, train_loss: 0.1070, step time: 0.1015\n",
      "101/223, train_loss: 0.1044, step time: 0.1326\n",
      "102/223, train_loss: 0.1065, step time: 0.1154\n",
      "103/223, train_loss: 0.0912, step time: 0.1052\n",
      "104/223, train_loss: 0.1118, step time: 0.1156\n",
      "105/223, train_loss: 0.1164, step time: 0.1195\n",
      "106/223, train_loss: 0.0999, step time: 0.1003\n",
      "107/223, train_loss: 0.1153, step time: 0.1006\n",
      "108/223, train_loss: 0.1145, step time: 0.1009\n",
      "109/223, train_loss: 0.0921, step time: 0.1062\n",
      "110/223, train_loss: 0.1049, step time: 0.1006\n",
      "111/223, train_loss: 0.1040, step time: 0.1053\n",
      "112/223, train_loss: 0.0999, step time: 0.1007\n",
      "113/223, train_loss: 0.0935, step time: 0.0999\n",
      "114/223, train_loss: 0.0968, step time: 0.1130\n",
      "115/223, train_loss: 0.1034, step time: 0.1141\n",
      "116/223, train_loss: 0.1033, step time: 0.1210\n",
      "117/223, train_loss: 0.1133, step time: 0.1081\n",
      "118/223, train_loss: 0.1079, step time: 0.1167\n",
      "119/223, train_loss: 0.1094, step time: 0.1001\n",
      "120/223, train_loss: 0.1060, step time: 0.1064\n",
      "121/223, train_loss: 0.0922, step time: 0.1176\n",
      "122/223, train_loss: 0.1024, step time: 0.1226\n",
      "123/223, train_loss: 0.0956, step time: 0.1008\n",
      "124/223, train_loss: 0.0935, step time: 0.1034\n",
      "125/223, train_loss: 0.1019, step time: 0.1053\n",
      "126/223, train_loss: 0.1105, step time: 0.1080\n",
      "127/223, train_loss: 0.1056, step time: 0.1164\n",
      "128/223, train_loss: 0.0998, step time: 0.1067\n",
      "129/223, train_loss: 0.1002, step time: 0.1056\n",
      "130/223, train_loss: 0.1126, step time: 0.1012\n",
      "131/223, train_loss: 0.0945, step time: 0.0999\n",
      "132/223, train_loss: 0.0916, step time: 0.1031\n",
      "133/223, train_loss: 0.0997, step time: 0.1008\n",
      "134/223, train_loss: 0.0998, step time: 0.1001\n",
      "135/223, train_loss: 0.0988, step time: 0.1006\n",
      "136/223, train_loss: 0.0933, step time: 0.1006\n",
      "137/223, train_loss: 0.1030, step time: 0.1126\n",
      "138/223, train_loss: 0.1158, step time: 0.1200\n",
      "139/223, train_loss: 0.1006, step time: 0.1163\n",
      "140/223, train_loss: 0.1076, step time: 0.1087\n",
      "141/223, train_loss: 0.1047, step time: 0.1173\n",
      "142/223, train_loss: 0.1083, step time: 0.1426\n",
      "143/223, train_loss: 0.0999, step time: 0.1166\n",
      "144/223, train_loss: 0.0948, step time: 0.1228\n",
      "145/223, train_loss: 0.0924, step time: 0.1060\n",
      "146/223, train_loss: 0.0961, step time: 0.1207\n",
      "147/223, train_loss: 0.1021, step time: 0.1206\n",
      "148/223, train_loss: 0.0971, step time: 0.1001\n",
      "149/223, train_loss: 0.0922, step time: 0.1196\n",
      "150/223, train_loss: 0.1085, step time: 0.1087\n",
      "151/223, train_loss: 0.0962, step time: 0.1273\n",
      "152/223, train_loss: 0.0884, step time: 0.1140\n",
      "153/223, train_loss: 0.0961, step time: 0.1074\n",
      "154/223, train_loss: 0.1117, step time: 0.1170\n",
      "155/223, train_loss: 0.1072, step time: 0.1122\n",
      "156/223, train_loss: 0.1049, step time: 0.1234\n",
      "157/223, train_loss: 0.1048, step time: 0.1328\n",
      "158/223, train_loss: 0.1118, step time: 0.1323\n",
      "159/223, train_loss: 0.1024, step time: 0.1200\n",
      "160/223, train_loss: 0.0981, step time: 0.1173\n",
      "161/223, train_loss: 0.1062, step time: 0.1182\n",
      "162/223, train_loss: 0.1044, step time: 0.1251\n",
      "163/223, train_loss: 0.1000, step time: 0.1235\n",
      "164/223, train_loss: 0.1029, step time: 0.1007\n",
      "165/223, train_loss: 0.1075, step time: 0.1075\n",
      "166/223, train_loss: 0.0982, step time: 0.1063\n",
      "167/223, train_loss: 0.0945, step time: 0.1115\n",
      "168/223, train_loss: 0.1028, step time: 0.1003\n",
      "169/223, train_loss: 0.1123, step time: 0.1209\n",
      "170/223, train_loss: 0.0953, step time: 0.1351\n",
      "171/223, train_loss: 0.1027, step time: 0.1042\n",
      "172/223, train_loss: 0.1009, step time: 0.1050\n",
      "173/223, train_loss: 0.1007, step time: 0.1050\n",
      "174/223, train_loss: 0.0898, step time: 0.1122\n",
      "175/223, train_loss: 0.0906, step time: 0.1045\n",
      "176/223, train_loss: 0.0987, step time: 0.1328\n",
      "177/223, train_loss: 0.0992, step time: 0.1009\n",
      "178/223, train_loss: 0.1026, step time: 0.1107\n",
      "179/223, train_loss: 0.1094, step time: 0.1005\n",
      "180/223, train_loss: 0.0982, step time: 0.1034\n",
      "181/223, train_loss: 0.0933, step time: 0.1081\n",
      "182/223, train_loss: 0.0977, step time: 0.1227\n",
      "183/223, train_loss: 0.2966, step time: 0.1039\n",
      "184/223, train_loss: 0.1063, step time: 0.1009\n",
      "185/223, train_loss: 0.0987, step time: 0.1137\n",
      "186/223, train_loss: 0.0971, step time: 0.1002\n",
      "187/223, train_loss: 0.1190, step time: 0.1006\n",
      "188/223, train_loss: 0.1074, step time: 0.1328\n",
      "189/223, train_loss: 0.1073, step time: 0.0999\n",
      "190/223, train_loss: 0.1003, step time: 0.1108\n",
      "191/223, train_loss: 0.1113, step time: 0.1042\n",
      "192/223, train_loss: 0.1023, step time: 0.1027\n",
      "193/223, train_loss: 0.1003, step time: 0.1196\n",
      "194/223, train_loss: 0.1046, step time: 0.1048\n",
      "195/223, train_loss: 0.0918, step time: 0.1081\n",
      "196/223, train_loss: 0.1160, step time: 0.1074\n",
      "197/223, train_loss: 0.0940, step time: 0.1156\n",
      "198/223, train_loss: 0.1079, step time: 0.0996\n",
      "199/223, train_loss: 0.1092, step time: 0.1002\n",
      "200/223, train_loss: 0.0954, step time: 0.1009\n",
      "201/223, train_loss: 0.0977, step time: 0.1176\n",
      "202/223, train_loss: 0.1022, step time: 0.1108\n",
      "203/223, train_loss: 0.1077, step time: 0.1036\n",
      "204/223, train_loss: 0.0956, step time: 0.1034\n",
      "205/223, train_loss: 0.0998, step time: 0.1122\n",
      "206/223, train_loss: 0.0981, step time: 0.1032\n",
      "207/223, train_loss: 0.0973, step time: 0.1021\n",
      "208/223, train_loss: 0.0977, step time: 0.1001\n",
      "209/223, train_loss: 0.1004, step time: 0.1118\n",
      "210/223, train_loss: 0.1007, step time: 0.1103\n",
      "211/223, train_loss: 0.0947, step time: 0.1323\n",
      "212/223, train_loss: 0.1084, step time: 0.0998\n",
      "213/223, train_loss: 0.0988, step time: 0.1065\n",
      "214/223, train_loss: 0.1060, step time: 0.1220\n",
      "215/223, train_loss: 0.1088, step time: 0.1046\n",
      "216/223, train_loss: 0.1100, step time: 0.1003\n",
      "217/223, train_loss: 0.1052, step time: 0.1049\n",
      "218/223, train_loss: 0.1011, step time: 0.1105\n",
      "219/223, train_loss: 0.0973, step time: 0.1018\n",
      "220/223, train_loss: 0.1092, step time: 0.1004\n",
      "221/223, train_loss: 0.1025, step time: 0.0994\n",
      "222/223, train_loss: 0.1124, step time: 0.0997\n",
      "223/223, train_loss: 0.1001, step time: 0.0997\n",
      "epoch 192 average loss: 0.1030\n",
      "time consuming of epoch 192 is: 90.5860\n",
      "----------\n",
      "epoch 193/300\n",
      "1/223, train_loss: 0.1073, step time: 0.1099\n",
      "2/223, train_loss: 0.1153, step time: 0.1060\n",
      "3/223, train_loss: 0.1034, step time: 0.1011\n",
      "4/223, train_loss: 0.1005, step time: 0.1008\n",
      "5/223, train_loss: 0.1054, step time: 0.1171\n",
      "6/223, train_loss: 0.0877, step time: 0.1062\n",
      "7/223, train_loss: 0.0974, step time: 0.1100\n",
      "8/223, train_loss: 0.1002, step time: 0.1166\n",
      "9/223, train_loss: 0.0973, step time: 0.1043\n",
      "10/223, train_loss: 0.0924, step time: 0.1004\n",
      "11/223, train_loss: 0.1036, step time: 0.1006\n",
      "12/223, train_loss: 0.1092, step time: 0.1351\n",
      "13/223, train_loss: 0.0991, step time: 0.1267\n",
      "14/223, train_loss: 0.0933, step time: 0.1100\n",
      "15/223, train_loss: 0.1071, step time: 0.1021\n",
      "16/223, train_loss: 0.0989, step time: 0.1052\n",
      "17/223, train_loss: 0.1017, step time: 0.1000\n",
      "18/223, train_loss: 0.1079, step time: 0.0999\n",
      "19/223, train_loss: 0.0977, step time: 0.1198\n",
      "20/223, train_loss: 0.1013, step time: 0.1249\n",
      "21/223, train_loss: 0.1055, step time: 0.1091\n",
      "22/223, train_loss: 0.1036, step time: 0.1279\n",
      "23/223, train_loss: 0.1071, step time: 0.1396\n",
      "24/223, train_loss: 0.0978, step time: 0.1001\n",
      "25/223, train_loss: 0.0979, step time: 0.1019\n",
      "26/223, train_loss: 0.1074, step time: 0.1132\n",
      "27/223, train_loss: 0.1071, step time: 0.1211\n",
      "28/223, train_loss: 0.1034, step time: 0.1116\n",
      "29/223, train_loss: 0.1136, step time: 0.1170\n",
      "30/223, train_loss: 0.1025, step time: 0.1086\n",
      "31/223, train_loss: 0.1023, step time: 0.1068\n",
      "32/223, train_loss: 0.1032, step time: 0.1020\n",
      "33/223, train_loss: 0.1010, step time: 0.1223\n",
      "34/223, train_loss: 0.1078, step time: 0.1220\n",
      "35/223, train_loss: 0.1007, step time: 0.1004\n",
      "36/223, train_loss: 0.0904, step time: 0.1033\n",
      "37/223, train_loss: 0.1196, step time: 0.1058\n",
      "38/223, train_loss: 0.1004, step time: 0.1136\n",
      "39/223, train_loss: 0.1006, step time: 0.1183\n",
      "40/223, train_loss: 0.1011, step time: 0.1007\n",
      "41/223, train_loss: 0.1105, step time: 0.1101\n",
      "42/223, train_loss: 0.0997, step time: 0.1004\n",
      "43/223, train_loss: 0.1028, step time: 0.1110\n",
      "44/223, train_loss: 0.1056, step time: 0.1046\n",
      "45/223, train_loss: 0.1176, step time: 0.1069\n",
      "46/223, train_loss: 0.1130, step time: 0.1000\n",
      "47/223, train_loss: 0.1019, step time: 0.1003\n",
      "48/223, train_loss: 0.0969, step time: 0.1137\n",
      "49/223, train_loss: 0.1084, step time: 0.1119\n",
      "50/223, train_loss: 0.1170, step time: 0.1148\n",
      "51/223, train_loss: 0.0910, step time: 0.1055\n",
      "52/223, train_loss: 0.1138, step time: 0.1006\n",
      "53/223, train_loss: 0.0976, step time: 0.1114\n",
      "54/223, train_loss: 0.0926, step time: 0.1488\n",
      "55/223, train_loss: 0.1059, step time: 0.1180\n",
      "56/223, train_loss: 0.0997, step time: 0.1073\n",
      "57/223, train_loss: 0.1017, step time: 0.1383\n",
      "58/223, train_loss: 0.1195, step time: 0.1128\n",
      "59/223, train_loss: 0.0979, step time: 0.1237\n",
      "60/223, train_loss: 0.0946, step time: 0.1007\n",
      "61/223, train_loss: 0.1179, step time: 0.1067\n",
      "62/223, train_loss: 0.1021, step time: 0.1204\n",
      "63/223, train_loss: 0.0995, step time: 0.1184\n",
      "64/223, train_loss: 0.0903, step time: 0.1287\n",
      "65/223, train_loss: 0.0998, step time: 0.1098\n",
      "66/223, train_loss: 0.0972, step time: 0.1109\n",
      "67/223, train_loss: 0.1011, step time: 0.1140\n",
      "68/223, train_loss: 0.0963, step time: 0.1042\n",
      "69/223, train_loss: 0.1031, step time: 0.1153\n",
      "70/223, train_loss: 0.0970, step time: 0.1441\n",
      "71/223, train_loss: 0.0966, step time: 0.1298\n",
      "72/223, train_loss: 0.1062, step time: 0.1334\n",
      "73/223, train_loss: 0.0921, step time: 0.1053\n",
      "74/223, train_loss: 0.0925, step time: 0.1048\n",
      "75/223, train_loss: 0.0953, step time: 0.1006\n",
      "76/223, train_loss: 0.0931, step time: 0.1555\n",
      "77/223, train_loss: 0.1070, step time: 0.1257\n",
      "78/223, train_loss: 0.1230, step time: 0.1025\n",
      "79/223, train_loss: 0.1039, step time: 0.1001\n",
      "80/223, train_loss: 0.1099, step time: 0.1005\n",
      "81/223, train_loss: 0.0970, step time: 0.1013\n",
      "82/223, train_loss: 0.0963, step time: 0.1382\n",
      "83/223, train_loss: 0.0915, step time: 0.1375\n",
      "84/223, train_loss: 0.1036, step time: 0.1014\n",
      "85/223, train_loss: 0.1077, step time: 0.1131\n",
      "86/223, train_loss: 0.0968, step time: 0.1141\n",
      "87/223, train_loss: 0.0985, step time: 0.1177\n",
      "88/223, train_loss: 0.1027, step time: 0.1026\n",
      "89/223, train_loss: 0.1007, step time: 0.1033\n",
      "90/223, train_loss: 0.1005, step time: 0.0995\n",
      "91/223, train_loss: 0.0968, step time: 0.1006\n",
      "92/223, train_loss: 0.1035, step time: 0.1033\n",
      "93/223, train_loss: 0.1135, step time: 0.1007\n",
      "94/223, train_loss: 0.1118, step time: 0.1112\n",
      "95/223, train_loss: 0.0973, step time: 0.1251\n",
      "96/223, train_loss: 0.1078, step time: 0.1131\n",
      "97/223, train_loss: 0.1004, step time: 0.1477\n",
      "98/223, train_loss: 0.0955, step time: 0.1321\n",
      "99/223, train_loss: 0.1166, step time: 0.1295\n",
      "100/223, train_loss: 0.0973, step time: 0.1096\n",
      "101/223, train_loss: 0.1062, step time: 0.1075\n",
      "102/223, train_loss: 0.0988, step time: 0.1189\n",
      "103/223, train_loss: 0.0930, step time: 0.1016\n",
      "104/223, train_loss: 0.1073, step time: 0.0994\n",
      "105/223, train_loss: 0.1124, step time: 0.1293\n",
      "106/223, train_loss: 0.1029, step time: 0.1212\n",
      "107/223, train_loss: 0.1023, step time: 0.1026\n",
      "108/223, train_loss: 0.0957, step time: 0.1125\n",
      "109/223, train_loss: 0.0978, step time: 0.1179\n",
      "110/223, train_loss: 0.1137, step time: 0.1079\n",
      "111/223, train_loss: 0.1082, step time: 0.1004\n",
      "112/223, train_loss: 0.1089, step time: 0.1012\n",
      "113/223, train_loss: 0.0986, step time: 0.1058\n",
      "114/223, train_loss: 0.1035, step time: 0.1075\n",
      "115/223, train_loss: 0.0950, step time: 0.1005\n",
      "116/223, train_loss: 0.0990, step time: 0.1121\n",
      "117/223, train_loss: 0.0999, step time: 0.1058\n",
      "118/223, train_loss: 0.0947, step time: 0.1140\n",
      "119/223, train_loss: 0.0911, step time: 0.1202\n",
      "120/223, train_loss: 0.1065, step time: 0.0996\n",
      "121/223, train_loss: 0.1141, step time: 0.1127\n",
      "122/223, train_loss: 0.0932, step time: 0.1528\n",
      "123/223, train_loss: 0.0994, step time: 0.1124\n",
      "124/223, train_loss: 0.0979, step time: 0.1105\n",
      "125/223, train_loss: 0.1002, step time: 0.1227\n",
      "126/223, train_loss: 0.1088, step time: 0.1212\n",
      "127/223, train_loss: 0.1030, step time: 0.1099\n",
      "128/223, train_loss: 0.1165, step time: 0.1102\n",
      "129/223, train_loss: 0.1134, step time: 0.1163\n",
      "130/223, train_loss: 0.0976, step time: 0.1206\n",
      "131/223, train_loss: 0.1098, step time: 0.1346\n",
      "132/223, train_loss: 0.0939, step time: 0.1007\n",
      "133/223, train_loss: 0.1062, step time: 0.1120\n",
      "134/223, train_loss: 0.1064, step time: 0.1233\n",
      "135/223, train_loss: 0.1009, step time: 0.1102\n",
      "136/223, train_loss: 0.0932, step time: 0.1244\n",
      "137/223, train_loss: 0.1013, step time: 0.1230\n",
      "138/223, train_loss: 0.0985, step time: 0.1114\n",
      "139/223, train_loss: 0.1225, step time: 0.1231\n",
      "140/223, train_loss: 0.1111, step time: 0.1093\n",
      "141/223, train_loss: 0.0973, step time: 0.1058\n",
      "142/223, train_loss: 0.0974, step time: 0.1006\n",
      "143/223, train_loss: 0.2955, step time: 0.1088\n",
      "144/223, train_loss: 0.1108, step time: 0.1070\n",
      "145/223, train_loss: 0.1102, step time: 0.1083\n",
      "146/223, train_loss: 0.1011, step time: 0.1011\n",
      "147/223, train_loss: 0.1045, step time: 0.1048\n",
      "148/223, train_loss: 0.0990, step time: 0.1126\n",
      "149/223, train_loss: 0.1017, step time: 0.1171\n",
      "150/223, train_loss: 0.0892, step time: 0.1192\n",
      "151/223, train_loss: 0.1034, step time: 0.1386\n",
      "152/223, train_loss: 0.0989, step time: 0.1140\n",
      "153/223, train_loss: 0.1055, step time: 0.1137\n",
      "154/223, train_loss: 0.1047, step time: 0.1007\n",
      "155/223, train_loss: 0.1049, step time: 0.1002\n",
      "156/223, train_loss: 0.1030, step time: 0.1006\n",
      "157/223, train_loss: 0.1029, step time: 0.1002\n",
      "158/223, train_loss: 0.1026, step time: 0.1009\n",
      "159/223, train_loss: 0.0960, step time: 0.1007\n",
      "160/223, train_loss: 0.0994, step time: 0.1013\n",
      "161/223, train_loss: 0.1086, step time: 0.1154\n",
      "162/223, train_loss: 0.0965, step time: 0.1171\n",
      "163/223, train_loss: 0.1024, step time: 0.1118\n",
      "164/223, train_loss: 0.1017, step time: 0.1010\n",
      "165/223, train_loss: 0.1143, step time: 0.1154\n",
      "166/223, train_loss: 0.1048, step time: 0.1005\n",
      "167/223, train_loss: 0.1045, step time: 0.1001\n",
      "168/223, train_loss: 0.0972, step time: 0.1009\n",
      "169/223, train_loss: 0.0988, step time: 0.1397\n",
      "170/223, train_loss: 0.1073, step time: 0.1065\n",
      "171/223, train_loss: 0.0986, step time: 0.1000\n",
      "172/223, train_loss: 0.0972, step time: 0.1068\n",
      "173/223, train_loss: 0.0997, step time: 0.1004\n",
      "174/223, train_loss: 0.1157, step time: 0.1003\n",
      "175/223, train_loss: 0.0967, step time: 0.1017\n",
      "176/223, train_loss: 0.1044, step time: 0.0991\n",
      "177/223, train_loss: 0.0899, step time: 0.1225\n",
      "178/223, train_loss: 0.1162, step time: 0.1110\n",
      "179/223, train_loss: 0.1010, step time: 0.1085\n",
      "180/223, train_loss: 0.1031, step time: 0.1007\n",
      "181/223, train_loss: 0.1017, step time: 0.1203\n",
      "182/223, train_loss: 0.0926, step time: 0.1170\n",
      "183/223, train_loss: 0.1028, step time: 0.1705\n",
      "184/223, train_loss: 0.0872, step time: 0.1123\n",
      "185/223, train_loss: 0.0991, step time: 0.0996\n",
      "186/223, train_loss: 0.0960, step time: 0.0995\n",
      "187/223, train_loss: 0.1029, step time: 0.0993\n",
      "188/223, train_loss: 0.1016, step time: 0.1276\n",
      "189/223, train_loss: 0.0957, step time: 0.1005\n",
      "190/223, train_loss: 0.1020, step time: 0.1150\n",
      "191/223, train_loss: 0.0996, step time: 0.1428\n",
      "192/223, train_loss: 0.1026, step time: 0.1369\n",
      "193/223, train_loss: 0.1095, step time: 0.1142\n",
      "194/223, train_loss: 0.0949, step time: 0.1220\n",
      "195/223, train_loss: 0.0938, step time: 0.1086\n",
      "196/223, train_loss: 0.1083, step time: 0.1005\n",
      "197/223, train_loss: 0.1050, step time: 0.1001\n",
      "198/223, train_loss: 0.0900, step time: 0.1000\n",
      "199/223, train_loss: 0.1077, step time: 0.1183\n",
      "200/223, train_loss: 0.1024, step time: 0.1101\n",
      "201/223, train_loss: 0.1052, step time: 0.1009\n",
      "202/223, train_loss: 0.0979, step time: 0.1015\n",
      "203/223, train_loss: 0.1006, step time: 0.1148\n",
      "204/223, train_loss: 0.1011, step time: 0.1005\n",
      "205/223, train_loss: 0.0981, step time: 0.1244\n",
      "206/223, train_loss: 0.1079, step time: 0.1229\n",
      "207/223, train_loss: 0.1019, step time: 0.1188\n",
      "208/223, train_loss: 0.0994, step time: 0.1169\n",
      "209/223, train_loss: 0.1077, step time: 0.1055\n",
      "210/223, train_loss: 0.0950, step time: 0.1014\n",
      "211/223, train_loss: 0.1000, step time: 0.1002\n",
      "212/223, train_loss: 0.1060, step time: 0.1041\n",
      "213/223, train_loss: 0.1003, step time: 0.0993\n",
      "214/223, train_loss: 0.0907, step time: 0.1065\n",
      "215/223, train_loss: 0.0962, step time: 0.1108\n",
      "216/223, train_loss: 0.0947, step time: 0.1002\n",
      "217/223, train_loss: 0.1041, step time: 0.1008\n",
      "218/223, train_loss: 0.1108, step time: 0.1004\n",
      "219/223, train_loss: 0.0986, step time: 0.1016\n",
      "220/223, train_loss: 0.1028, step time: 0.1001\n",
      "221/223, train_loss: 0.1008, step time: 0.1001\n",
      "222/223, train_loss: 0.1101, step time: 0.0996\n",
      "223/223, train_loss: 0.0958, step time: 0.1003\n",
      "epoch 193 average loss: 0.1030\n",
      "time consuming of epoch 193 is: 88.2905\n",
      "----------\n",
      "epoch 194/300\n",
      "1/223, train_loss: 0.0933, step time: 0.1139\n",
      "2/223, train_loss: 0.1081, step time: 0.1137\n",
      "3/223, train_loss: 0.0917, step time: 0.1009\n",
      "4/223, train_loss: 0.0984, step time: 0.1003\n",
      "5/223, train_loss: 0.1066, step time: 0.1086\n",
      "6/223, train_loss: 0.1004, step time: 0.0995\n",
      "7/223, train_loss: 0.1046, step time: 0.1196\n",
      "8/223, train_loss: 0.0988, step time: 0.1061\n",
      "9/223, train_loss: 0.1071, step time: 0.1019\n",
      "10/223, train_loss: 0.1140, step time: 0.1318\n",
      "11/223, train_loss: 0.0996, step time: 0.1160\n",
      "12/223, train_loss: 0.1036, step time: 0.1212\n",
      "13/223, train_loss: 0.1031, step time: 0.1084\n",
      "14/223, train_loss: 0.1020, step time: 0.1078\n",
      "15/223, train_loss: 0.0979, step time: 0.1581\n",
      "16/223, train_loss: 0.0995, step time: 0.1236\n",
      "17/223, train_loss: 0.1105, step time: 0.1148\n",
      "18/223, train_loss: 0.1078, step time: 0.1148\n",
      "19/223, train_loss: 0.0988, step time: 0.1440\n",
      "20/223, train_loss: 0.0884, step time: 0.1049\n",
      "21/223, train_loss: 0.0932, step time: 0.1053\n",
      "22/223, train_loss: 0.1013, step time: 0.1370\n",
      "23/223, train_loss: 0.1025, step time: 0.1100\n",
      "24/223, train_loss: 0.1005, step time: 0.1002\n",
      "25/223, train_loss: 0.1040, step time: 0.1108\n",
      "26/223, train_loss: 0.1003, step time: 0.1018\n",
      "27/223, train_loss: 0.0991, step time: 0.1004\n",
      "28/223, train_loss: 0.0976, step time: 0.1198\n",
      "29/223, train_loss: 0.1049, step time: 0.1035\n",
      "30/223, train_loss: 0.1037, step time: 0.1004\n",
      "31/223, train_loss: 0.1037, step time: 0.1004\n",
      "32/223, train_loss: 0.1077, step time: 0.1071\n",
      "33/223, train_loss: 0.1025, step time: 0.1188\n",
      "34/223, train_loss: 0.1098, step time: 0.0998\n",
      "35/223, train_loss: 0.1090, step time: 0.1006\n",
      "36/223, train_loss: 0.0980, step time: 0.1022\n",
      "37/223, train_loss: 0.1037, step time: 0.1377\n",
      "38/223, train_loss: 0.1101, step time: 0.1172\n",
      "39/223, train_loss: 0.1273, step time: 0.1183\n",
      "40/223, train_loss: 0.1067, step time: 0.0995\n",
      "41/223, train_loss: 0.0967, step time: 0.1127\n",
      "42/223, train_loss: 0.1044, step time: 0.1239\n",
      "43/223, train_loss: 0.0910, step time: 0.1007\n",
      "44/223, train_loss: 0.0903, step time: 0.1103\n",
      "45/223, train_loss: 0.0999, step time: 0.1181\n",
      "46/223, train_loss: 0.1057, step time: 0.1080\n",
      "47/223, train_loss: 0.1018, step time: 0.1024\n",
      "48/223, train_loss: 0.1049, step time: 0.1187\n",
      "49/223, train_loss: 0.1049, step time: 0.1241\n",
      "50/223, train_loss: 0.0945, step time: 0.1173\n",
      "51/223, train_loss: 0.0924, step time: 0.1003\n",
      "52/223, train_loss: 0.0967, step time: 0.1003\n",
      "53/223, train_loss: 0.1112, step time: 0.1131\n",
      "54/223, train_loss: 0.1005, step time: 0.1000\n",
      "55/223, train_loss: 0.0945, step time: 0.1282\n",
      "56/223, train_loss: 0.1106, step time: 0.1028\n",
      "57/223, train_loss: 0.1007, step time: 0.1245\n",
      "58/223, train_loss: 0.0929, step time: 0.1068\n",
      "59/223, train_loss: 0.0939, step time: 0.1418\n",
      "60/223, train_loss: 0.0966, step time: 0.0999\n",
      "61/223, train_loss: 0.0973, step time: 0.1146\n",
      "62/223, train_loss: 0.1058, step time: 0.1006\n",
      "63/223, train_loss: 0.1120, step time: 0.1018\n",
      "64/223, train_loss: 0.1143, step time: 0.1006\n",
      "65/223, train_loss: 0.0944, step time: 0.1167\n",
      "66/223, train_loss: 0.1006, step time: 0.1280\n",
      "67/223, train_loss: 0.0933, step time: 0.1162\n",
      "68/223, train_loss: 0.1027, step time: 0.1193\n",
      "69/223, train_loss: 0.0966, step time: 0.1107\n",
      "70/223, train_loss: 0.0969, step time: 0.1709\n",
      "71/223, train_loss: 0.1023, step time: 0.1337\n",
      "72/223, train_loss: 0.0954, step time: 0.0998\n",
      "73/223, train_loss: 0.0966, step time: 0.1106\n",
      "74/223, train_loss: 0.0914, step time: 0.1063\n",
      "75/223, train_loss: 0.1016, step time: 0.1005\n",
      "76/223, train_loss: 0.1002, step time: 0.1002\n",
      "77/223, train_loss: 0.1003, step time: 0.1128\n",
      "78/223, train_loss: 0.0959, step time: 0.1173\n",
      "79/223, train_loss: 0.0961, step time: 0.1010\n",
      "80/223, train_loss: 0.1022, step time: 0.1015\n",
      "81/223, train_loss: 0.1076, step time: 0.1091\n",
      "82/223, train_loss: 0.1029, step time: 0.1037\n",
      "83/223, train_loss: 0.1026, step time: 0.1004\n",
      "84/223, train_loss: 0.1084, step time: 0.1005\n",
      "85/223, train_loss: 0.1163, step time: 0.1113\n",
      "86/223, train_loss: 0.1006, step time: 0.1103\n",
      "87/223, train_loss: 0.0882, step time: 0.1250\n",
      "88/223, train_loss: 0.0992, step time: 0.1076\n",
      "89/223, train_loss: 0.0952, step time: 0.1186\n",
      "90/223, train_loss: 0.1100, step time: 0.1178\n",
      "91/223, train_loss: 0.0996, step time: 0.1055\n",
      "92/223, train_loss: 0.0932, step time: 0.1009\n",
      "93/223, train_loss: 0.0977, step time: 0.1194\n",
      "94/223, train_loss: 0.0993, step time: 0.1038\n",
      "95/223, train_loss: 0.1088, step time: 0.1115\n",
      "96/223, train_loss: 0.1088, step time: 0.1209\n",
      "97/223, train_loss: 0.1055, step time: 0.1133\n",
      "98/223, train_loss: 0.0946, step time: 0.1109\n",
      "99/223, train_loss: 0.0928, step time: 0.1003\n",
      "100/223, train_loss: 0.0975, step time: 0.1017\n",
      "101/223, train_loss: 0.1012, step time: 0.1169\n",
      "102/223, train_loss: 0.0932, step time: 0.1190\n",
      "103/223, train_loss: 0.0871, step time: 0.1089\n",
      "104/223, train_loss: 0.1098, step time: 0.1008\n",
      "105/223, train_loss: 0.1085, step time: 0.1071\n",
      "106/223, train_loss: 0.0949, step time: 0.1071\n",
      "107/223, train_loss: 0.1038, step time: 0.1154\n",
      "108/223, train_loss: 0.1068, step time: 0.1152\n",
      "109/223, train_loss: 0.1016, step time: 0.1077\n",
      "110/223, train_loss: 0.0997, step time: 0.1258\n",
      "111/223, train_loss: 0.1050, step time: 0.1198\n",
      "112/223, train_loss: 0.1115, step time: 0.1142\n",
      "113/223, train_loss: 0.0970, step time: 0.1185\n",
      "114/223, train_loss: 0.1054, step time: 0.1147\n",
      "115/223, train_loss: 0.0938, step time: 0.1103\n",
      "116/223, train_loss: 0.0885, step time: 0.1033\n",
      "117/223, train_loss: 0.1026, step time: 0.1032\n",
      "118/223, train_loss: 0.1101, step time: 0.1011\n",
      "119/223, train_loss: 0.0921, step time: 0.1477\n",
      "120/223, train_loss: 0.0991, step time: 0.1209\n",
      "121/223, train_loss: 0.1010, step time: 0.1133\n",
      "122/223, train_loss: 0.0950, step time: 0.1077\n",
      "123/223, train_loss: 0.1058, step time: 0.1090\n",
      "124/223, train_loss: 0.1004, step time: 0.1324\n",
      "125/223, train_loss: 0.1070, step time: 0.1281\n",
      "126/223, train_loss: 0.0959, step time: 0.1099\n",
      "127/223, train_loss: 0.0956, step time: 0.1002\n",
      "128/223, train_loss: 0.1070, step time: 0.1036\n",
      "129/223, train_loss: 0.1043, step time: 0.1061\n",
      "130/223, train_loss: 0.1129, step time: 0.1178\n",
      "131/223, train_loss: 0.0994, step time: 0.0995\n",
      "132/223, train_loss: 0.0957, step time: 0.1013\n",
      "133/223, train_loss: 0.1043, step time: 0.1078\n",
      "134/223, train_loss: 0.1052, step time: 0.1004\n",
      "135/223, train_loss: 0.1015, step time: 0.0998\n",
      "136/223, train_loss: 0.0984, step time: 0.0999\n",
      "137/223, train_loss: 0.1124, step time: 0.1141\n",
      "138/223, train_loss: 0.1025, step time: 0.1117\n",
      "139/223, train_loss: 0.1041, step time: 0.1195\n",
      "140/223, train_loss: 0.1024, step time: 0.1107\n",
      "141/223, train_loss: 0.3059, step time: 0.1218\n",
      "142/223, train_loss: 0.1043, step time: 0.1159\n",
      "143/223, train_loss: 0.1069, step time: 0.1109\n",
      "144/223, train_loss: 0.0968, step time: 0.1111\n",
      "145/223, train_loss: 0.1023, step time: 0.1170\n",
      "146/223, train_loss: 0.1002, step time: 0.1103\n",
      "147/223, train_loss: 0.1100, step time: 0.1143\n",
      "148/223, train_loss: 0.1103, step time: 0.1146\n",
      "149/223, train_loss: 0.0974, step time: 0.1138\n",
      "150/223, train_loss: 0.0949, step time: 0.1207\n",
      "151/223, train_loss: 0.1040, step time: 0.1106\n",
      "152/223, train_loss: 0.1081, step time: 0.1302\n",
      "153/223, train_loss: 0.1097, step time: 0.1083\n",
      "154/223, train_loss: 0.1171, step time: 0.1134\n",
      "155/223, train_loss: 0.1115, step time: 0.1259\n",
      "156/223, train_loss: 0.0979, step time: 0.1004\n",
      "157/223, train_loss: 0.1040, step time: 0.1069\n",
      "158/223, train_loss: 0.1017, step time: 0.1213\n",
      "159/223, train_loss: 0.1024, step time: 0.1269\n",
      "160/223, train_loss: 0.1049, step time: 0.1143\n",
      "161/223, train_loss: 0.0875, step time: 0.1107\n",
      "162/223, train_loss: 0.1055, step time: 0.1000\n",
      "163/223, train_loss: 0.1066, step time: 0.1004\n",
      "164/223, train_loss: 0.1083, step time: 0.1567\n",
      "165/223, train_loss: 0.1103, step time: 0.1173\n",
      "166/223, train_loss: 0.0999, step time: 0.1071\n",
      "167/223, train_loss: 0.1031, step time: 0.1267\n",
      "168/223, train_loss: 0.0944, step time: 0.1246\n",
      "169/223, train_loss: 0.0929, step time: 0.1189\n",
      "170/223, train_loss: 0.1051, step time: 0.1170\n",
      "171/223, train_loss: 0.1024, step time: 0.1105\n",
      "172/223, train_loss: 0.1055, step time: 0.1095\n",
      "173/223, train_loss: 0.1090, step time: 0.1100\n",
      "174/223, train_loss: 0.1093, step time: 0.1179\n",
      "175/223, train_loss: 0.1090, step time: 0.1109\n",
      "176/223, train_loss: 0.1058, step time: 0.0999\n",
      "177/223, train_loss: 0.0938, step time: 0.1116\n",
      "178/223, train_loss: 0.0963, step time: 0.1017\n",
      "179/223, train_loss: 0.1075, step time: 0.1163\n",
      "180/223, train_loss: 0.0950, step time: 0.1022\n",
      "181/223, train_loss: 0.1048, step time: 0.1109\n",
      "182/223, train_loss: 0.1027, step time: 0.1161\n",
      "183/223, train_loss: 0.1070, step time: 0.0999\n",
      "184/223, train_loss: 0.0980, step time: 0.1001\n",
      "185/223, train_loss: 0.1170, step time: 0.1066\n",
      "186/223, train_loss: 0.1070, step time: 0.1085\n",
      "187/223, train_loss: 0.0981, step time: 0.1104\n",
      "188/223, train_loss: 0.0978, step time: 0.1006\n",
      "189/223, train_loss: 0.1189, step time: 0.1052\n",
      "190/223, train_loss: 0.1167, step time: 0.1086\n",
      "191/223, train_loss: 0.1044, step time: 0.1031\n",
      "192/223, train_loss: 0.0995, step time: 0.1040\n",
      "193/223, train_loss: 0.1105, step time: 0.1114\n",
      "194/223, train_loss: 0.0940, step time: 0.1153\n",
      "195/223, train_loss: 0.1032, step time: 0.1094\n",
      "196/223, train_loss: 0.1125, step time: 0.1023\n",
      "197/223, train_loss: 0.1104, step time: 0.1112\n",
      "198/223, train_loss: 0.1094, step time: 0.1128\n",
      "199/223, train_loss: 0.1031, step time: 0.1057\n",
      "200/223, train_loss: 0.1073, step time: 0.1006\n",
      "201/223, train_loss: 0.0939, step time: 0.1133\n",
      "202/223, train_loss: 0.0970, step time: 0.1080\n",
      "203/223, train_loss: 0.0952, step time: 0.1023\n",
      "204/223, train_loss: 0.1101, step time: 0.1074\n",
      "205/223, train_loss: 0.0960, step time: 0.1125\n",
      "206/223, train_loss: 0.0967, step time: 0.1028\n",
      "207/223, train_loss: 0.0948, step time: 0.1002\n",
      "208/223, train_loss: 0.1105, step time: 0.1030\n",
      "209/223, train_loss: 0.0940, step time: 0.1050\n",
      "210/223, train_loss: 0.0954, step time: 0.1181\n",
      "211/223, train_loss: 0.0944, step time: 0.1086\n",
      "212/223, train_loss: 0.0912, step time: 0.1102\n",
      "213/223, train_loss: 0.1067, step time: 0.1061\n",
      "214/223, train_loss: 0.0955, step time: 0.1104\n",
      "215/223, train_loss: 0.1139, step time: 0.1108\n",
      "216/223, train_loss: 0.0982, step time: 0.1504\n",
      "217/223, train_loss: 0.0991, step time: 0.0990\n",
      "218/223, train_loss: 0.1048, step time: 0.0998\n",
      "219/223, train_loss: 0.1079, step time: 0.0986\n",
      "220/223, train_loss: 0.1043, step time: 0.0995\n",
      "221/223, train_loss: 0.0990, step time: 0.0996\n",
      "222/223, train_loss: 0.1047, step time: 0.1001\n",
      "223/223, train_loss: 0.0949, step time: 0.0992\n",
      "epoch 194 average loss: 0.1029\n",
      "time consuming of epoch 194 is: 85.9939\n",
      "----------\n",
      "epoch 195/300\n",
      "1/223, train_loss: 0.1000, step time: 0.1011\n",
      "2/223, train_loss: 0.1009, step time: 0.1235\n",
      "3/223, train_loss: 0.0998, step time: 0.1117\n",
      "4/223, train_loss: 0.1065, step time: 0.1394\n",
      "5/223, train_loss: 0.0913, step time: 0.1151\n",
      "6/223, train_loss: 0.0999, step time: 0.1112\n",
      "7/223, train_loss: 0.1033, step time: 0.1081\n",
      "8/223, train_loss: 0.1080, step time: 0.1126\n",
      "9/223, train_loss: 0.1010, step time: 0.1033\n",
      "10/223, train_loss: 0.0980, step time: 0.1018\n",
      "11/223, train_loss: 0.0938, step time: 0.1233\n",
      "12/223, train_loss: 0.1169, step time: 0.1146\n",
      "13/223, train_loss: 0.1083, step time: 0.1017\n",
      "14/223, train_loss: 0.1001, step time: 0.1076\n",
      "15/223, train_loss: 0.1051, step time: 0.1172\n",
      "16/223, train_loss: 0.0995, step time: 0.1188\n",
      "17/223, train_loss: 0.1195, step time: 0.1191\n",
      "18/223, train_loss: 0.1022, step time: 0.1156\n",
      "19/223, train_loss: 0.0935, step time: 0.1216\n",
      "20/223, train_loss: 0.1009, step time: 0.1119\n",
      "21/223, train_loss: 0.1094, step time: 0.1104\n",
      "22/223, train_loss: 0.1110, step time: 0.1155\n",
      "23/223, train_loss: 0.1043, step time: 0.1175\n",
      "24/223, train_loss: 0.0861, step time: 0.1165\n",
      "25/223, train_loss: 0.0981, step time: 0.1124\n",
      "26/223, train_loss: 0.0990, step time: 0.1004\n",
      "27/223, train_loss: 0.1089, step time: 0.1002\n",
      "28/223, train_loss: 0.0930, step time: 0.1025\n",
      "29/223, train_loss: 0.1029, step time: 0.1134\n",
      "30/223, train_loss: 0.1126, step time: 0.1149\n",
      "31/223, train_loss: 0.0933, step time: 0.1005\n",
      "32/223, train_loss: 0.1020, step time: 0.1008\n",
      "33/223, train_loss: 0.1072, step time: 0.1016\n",
      "34/223, train_loss: 0.0942, step time: 0.0994\n",
      "35/223, train_loss: 0.1089, step time: 0.1009\n",
      "36/223, train_loss: 0.0977, step time: 0.1002\n",
      "37/223, train_loss: 0.1044, step time: 0.1127\n",
      "38/223, train_loss: 0.1054, step time: 0.1140\n",
      "39/223, train_loss: 0.0935, step time: 0.1005\n",
      "40/223, train_loss: 0.1082, step time: 0.1021\n",
      "41/223, train_loss: 0.1182, step time: 0.1177\n",
      "42/223, train_loss: 0.1177, step time: 0.1198\n",
      "43/223, train_loss: 0.1186, step time: 0.1008\n",
      "44/223, train_loss: 0.1051, step time: 0.1539\n",
      "45/223, train_loss: 0.1039, step time: 0.1183\n",
      "46/223, train_loss: 0.1001, step time: 0.1024\n",
      "47/223, train_loss: 0.1028, step time: 0.0998\n",
      "48/223, train_loss: 0.1074, step time: 0.1003\n",
      "49/223, train_loss: 0.0983, step time: 0.1088\n",
      "50/223, train_loss: 0.1151, step time: 0.1031\n",
      "51/223, train_loss: 0.1067, step time: 0.1116\n",
      "52/223, train_loss: 0.0935, step time: 0.1195\n",
      "53/223, train_loss: 0.1092, step time: 0.1175\n",
      "54/223, train_loss: 0.1135, step time: 0.1116\n",
      "55/223, train_loss: 0.1130, step time: 0.1018\n",
      "56/223, train_loss: 0.0989, step time: 0.1264\n",
      "57/223, train_loss: 0.0948, step time: 0.1011\n",
      "58/223, train_loss: 0.0960, step time: 0.1066\n",
      "59/223, train_loss: 0.1006, step time: 0.1070\n",
      "60/223, train_loss: 0.1035, step time: 0.1005\n",
      "61/223, train_loss: 0.1087, step time: 0.1021\n",
      "62/223, train_loss: 0.1159, step time: 0.1157\n",
      "63/223, train_loss: 0.0943, step time: 0.1036\n",
      "64/223, train_loss: 0.0951, step time: 0.1371\n",
      "65/223, train_loss: 0.1254, step time: 0.1055\n",
      "66/223, train_loss: 0.1015, step time: 0.1131\n",
      "67/223, train_loss: 0.0951, step time: 0.1005\n",
      "68/223, train_loss: 0.0940, step time: 0.0997\n",
      "69/223, train_loss: 0.0903, step time: 0.1078\n",
      "70/223, train_loss: 0.1020, step time: 0.1083\n",
      "71/223, train_loss: 0.0944, step time: 0.1011\n",
      "72/223, train_loss: 0.1055, step time: 0.1190\n",
      "73/223, train_loss: 0.1078, step time: 0.1006\n",
      "74/223, train_loss: 0.1046, step time: 0.1052\n",
      "75/223, train_loss: 0.1050, step time: 0.1010\n",
      "76/223, train_loss: 0.0989, step time: 0.1011\n",
      "77/223, train_loss: 0.0975, step time: 0.1183\n",
      "78/223, train_loss: 0.1013, step time: 0.1010\n",
      "79/223, train_loss: 0.0967, step time: 0.1060\n",
      "80/223, train_loss: 0.0949, step time: 0.1204\n",
      "81/223, train_loss: 0.1039, step time: 0.1098\n",
      "82/223, train_loss: 0.0992, step time: 0.0997\n",
      "83/223, train_loss: 0.1115, step time: 0.1008\n",
      "84/223, train_loss: 0.1101, step time: 0.1298\n",
      "85/223, train_loss: 0.1034, step time: 0.1274\n",
      "86/223, train_loss: 0.1159, step time: 0.1222\n",
      "87/223, train_loss: 0.0977, step time: 0.0994\n",
      "88/223, train_loss: 0.1060, step time: 0.1158\n",
      "89/223, train_loss: 0.0964, step time: 0.1090\n",
      "90/223, train_loss: 0.1084, step time: 0.1060\n",
      "91/223, train_loss: 0.1158, step time: 0.1155\n",
      "92/223, train_loss: 0.1022, step time: 0.1065\n",
      "93/223, train_loss: 0.1056, step time: 0.1101\n",
      "94/223, train_loss: 0.1186, step time: 0.1325\n",
      "95/223, train_loss: 0.1019, step time: 0.1115\n",
      "96/223, train_loss: 0.0999, step time: 0.1272\n",
      "97/223, train_loss: 0.1002, step time: 0.1013\n",
      "98/223, train_loss: 0.1053, step time: 0.1176\n",
      "99/223, train_loss: 0.1006, step time: 0.1020\n",
      "100/223, train_loss: 0.1052, step time: 0.1003\n",
      "101/223, train_loss: 0.1009, step time: 0.1020\n",
      "102/223, train_loss: 0.1020, step time: 0.1064\n",
      "103/223, train_loss: 0.0991, step time: 0.0999\n",
      "104/223, train_loss: 0.1116, step time: 0.1005\n",
      "105/223, train_loss: 0.1053, step time: 0.1029\n",
      "106/223, train_loss: 0.0965, step time: 0.1118\n",
      "107/223, train_loss: 0.1095, step time: 0.1420\n",
      "108/223, train_loss: 0.0995, step time: 0.1010\n",
      "109/223, train_loss: 0.0885, step time: 0.1000\n",
      "110/223, train_loss: 0.0964, step time: 0.1166\n",
      "111/223, train_loss: 0.0979, step time: 0.1069\n",
      "112/223, train_loss: 0.0983, step time: 0.1481\n",
      "113/223, train_loss: 0.0969, step time: 0.1123\n",
      "114/223, train_loss: 0.1003, step time: 0.1069\n",
      "115/223, train_loss: 0.0925, step time: 0.1069\n",
      "116/223, train_loss: 0.1041, step time: 0.1277\n",
      "117/223, train_loss: 0.1072, step time: 0.1395\n",
      "118/223, train_loss: 0.1015, step time: 0.1069\n",
      "119/223, train_loss: 0.0889, step time: 0.1096\n",
      "120/223, train_loss: 0.1018, step time: 0.1509\n",
      "121/223, train_loss: 0.1114, step time: 0.1244\n",
      "122/223, train_loss: 0.0924, step time: 0.1154\n",
      "123/223, train_loss: 0.0906, step time: 0.0994\n",
      "124/223, train_loss: 0.0967, step time: 0.1220\n",
      "125/223, train_loss: 0.0970, step time: 0.1069\n",
      "126/223, train_loss: 0.0972, step time: 0.1025\n",
      "127/223, train_loss: 0.0980, step time: 0.1398\n",
      "128/223, train_loss: 0.0971, step time: 0.1276\n",
      "129/223, train_loss: 0.0998, step time: 0.1076\n",
      "130/223, train_loss: 0.1004, step time: 0.1202\n",
      "131/223, train_loss: 0.0992, step time: 0.1360\n",
      "132/223, train_loss: 0.1048, step time: 0.1005\n",
      "133/223, train_loss: 0.1084, step time: 0.1012\n",
      "134/223, train_loss: 0.1069, step time: 0.1007\n",
      "135/223, train_loss: 0.0948, step time: 0.1240\n",
      "136/223, train_loss: 0.0973, step time: 0.1383\n",
      "137/223, train_loss: 0.1117, step time: 0.0995\n",
      "138/223, train_loss: 0.1029, step time: 0.1109\n",
      "139/223, train_loss: 0.0880, step time: 0.1145\n",
      "140/223, train_loss: 0.0954, step time: 0.1054\n",
      "141/223, train_loss: 0.0994, step time: 0.1030\n",
      "142/223, train_loss: 0.1023, step time: 0.1769\n",
      "143/223, train_loss: 0.0949, step time: 0.1065\n",
      "144/223, train_loss: 0.1007, step time: 0.1005\n",
      "145/223, train_loss: 0.0949, step time: 0.1013\n",
      "146/223, train_loss: 0.0978, step time: 0.1014\n",
      "147/223, train_loss: 0.1063, step time: 0.0996\n",
      "148/223, train_loss: 0.1038, step time: 0.1084\n",
      "149/223, train_loss: 0.1042, step time: 0.1004\n",
      "150/223, train_loss: 0.1029, step time: 0.1279\n",
      "151/223, train_loss: 0.1008, step time: 0.1081\n",
      "152/223, train_loss: 0.1029, step time: 0.0997\n",
      "153/223, train_loss: 0.1076, step time: 0.0999\n",
      "154/223, train_loss: 0.0989, step time: 0.1057\n",
      "155/223, train_loss: 0.0907, step time: 0.1207\n",
      "156/223, train_loss: 0.1078, step time: 0.1047\n",
      "157/223, train_loss: 0.1038, step time: 0.0992\n",
      "158/223, train_loss: 0.1030, step time: 0.1056\n",
      "159/223, train_loss: 0.1007, step time: 0.1086\n",
      "160/223, train_loss: 0.0958, step time: 0.1596\n",
      "161/223, train_loss: 0.0955, step time: 0.1208\n",
      "162/223, train_loss: 0.1020, step time: 0.1137\n",
      "163/223, train_loss: 0.1072, step time: 0.1050\n",
      "164/223, train_loss: 0.0985, step time: 0.1255\n",
      "165/223, train_loss: 0.0944, step time: 0.1172\n",
      "166/223, train_loss: 0.1137, step time: 0.1154\n",
      "167/223, train_loss: 0.1048, step time: 0.1218\n",
      "168/223, train_loss: 0.1012, step time: 0.1255\n",
      "169/223, train_loss: 0.0972, step time: 0.1142\n",
      "170/223, train_loss: 0.0992, step time: 0.1030\n",
      "171/223, train_loss: 0.1018, step time: 0.1144\n",
      "172/223, train_loss: 0.1100, step time: 0.1068\n",
      "173/223, train_loss: 0.0964, step time: 0.1005\n",
      "174/223, train_loss: 0.1054, step time: 0.1086\n",
      "175/223, train_loss: 0.1018, step time: 0.1082\n",
      "176/223, train_loss: 0.1002, step time: 0.1198\n",
      "177/223, train_loss: 0.1108, step time: 0.1365\n",
      "178/223, train_loss: 0.1041, step time: 0.1006\n",
      "179/223, train_loss: 0.0950, step time: 0.1034\n",
      "180/223, train_loss: 0.1046, step time: 0.1145\n",
      "181/223, train_loss: 0.0912, step time: 0.1214\n",
      "182/223, train_loss: 0.0995, step time: 0.1227\n",
      "183/223, train_loss: 0.1061, step time: 0.1167\n",
      "184/223, train_loss: 0.2962, step time: 0.1175\n",
      "185/223, train_loss: 0.1015, step time: 0.1391\n",
      "186/223, train_loss: 0.1074, step time: 0.1082\n",
      "187/223, train_loss: 0.1020, step time: 0.1000\n",
      "188/223, train_loss: 0.0927, step time: 0.1111\n",
      "189/223, train_loss: 0.1005, step time: 0.1107\n",
      "190/223, train_loss: 0.1060, step time: 0.1153\n",
      "191/223, train_loss: 0.1095, step time: 0.1289\n",
      "192/223, train_loss: 0.1090, step time: 0.1083\n",
      "193/223, train_loss: 0.1058, step time: 0.1006\n",
      "194/223, train_loss: 0.1051, step time: 0.1179\n",
      "195/223, train_loss: 0.1069, step time: 0.1270\n",
      "196/223, train_loss: 0.0964, step time: 0.1249\n",
      "197/223, train_loss: 0.0948, step time: 0.1007\n",
      "198/223, train_loss: 0.1000, step time: 0.1159\n",
      "199/223, train_loss: 0.1031, step time: 0.0999\n",
      "200/223, train_loss: 0.0997, step time: 0.1008\n",
      "201/223, train_loss: 0.1000, step time: 0.1000\n",
      "202/223, train_loss: 0.1057, step time: 0.1139\n",
      "203/223, train_loss: 0.1025, step time: 0.1081\n",
      "204/223, train_loss: 0.1004, step time: 0.1019\n",
      "205/223, train_loss: 0.1071, step time: 0.1053\n",
      "206/223, train_loss: 0.0938, step time: 0.1496\n",
      "207/223, train_loss: 0.0990, step time: 0.1107\n",
      "208/223, train_loss: 0.0940, step time: 0.1234\n",
      "209/223, train_loss: 0.1129, step time: 0.1008\n",
      "210/223, train_loss: 0.0979, step time: 0.1008\n",
      "211/223, train_loss: 0.1026, step time: 0.1008\n",
      "212/223, train_loss: 0.0939, step time: 0.1004\n",
      "213/223, train_loss: 0.1113, step time: 0.1005\n",
      "214/223, train_loss: 0.0999, step time: 0.1001\n",
      "215/223, train_loss: 0.1000, step time: 0.1078\n",
      "216/223, train_loss: 0.0902, step time: 0.1101\n",
      "217/223, train_loss: 0.0989, step time: 0.1162\n",
      "218/223, train_loss: 0.0977, step time: 0.1072\n",
      "219/223, train_loss: 0.1025, step time: 0.1017\n",
      "220/223, train_loss: 0.1175, step time: 0.1005\n",
      "221/223, train_loss: 0.0961, step time: 0.0995\n",
      "222/223, train_loss: 0.0994, step time: 0.0990\n",
      "223/223, train_loss: 0.1084, step time: 0.0994\n",
      "epoch 195 average loss: 0.1030\n",
      "current epoch: 195 current mean dice: 0.8597 tc: 0.9213 wt: 0.8690 et: 0.7887\n",
      "best mean dice: 0.8600 at epoch: 170\n",
      "time consuming of epoch 195 is: 91.3201\n",
      "----------\n",
      "epoch 196/300\n",
      "1/223, train_loss: 0.1189, step time: 0.1004\n",
      "2/223, train_loss: 0.1025, step time: 0.1083\n",
      "3/223, train_loss: 0.0952, step time: 0.1009\n",
      "4/223, train_loss: 0.1043, step time: 0.1064\n",
      "5/223, train_loss: 0.0962, step time: 0.1216\n",
      "6/223, train_loss: 0.1110, step time: 0.1076\n",
      "7/223, train_loss: 0.0948, step time: 0.1056\n",
      "8/223, train_loss: 0.1005, step time: 0.1333\n",
      "9/223, train_loss: 0.0900, step time: 0.1129\n",
      "10/223, train_loss: 0.1039, step time: 0.1230\n",
      "11/223, train_loss: 0.1086, step time: 0.1085\n",
      "12/223, train_loss: 0.0978, step time: 0.1098\n",
      "13/223, train_loss: 0.1071, step time: 0.1004\n",
      "14/223, train_loss: 0.0967, step time: 0.1426\n",
      "15/223, train_loss: 0.1146, step time: 0.1056\n",
      "16/223, train_loss: 0.0938, step time: 0.1020\n",
      "17/223, train_loss: 0.1055, step time: 0.1105\n",
      "18/223, train_loss: 0.0890, step time: 0.1028\n",
      "19/223, train_loss: 0.1204, step time: 0.1012\n",
      "20/223, train_loss: 0.1025, step time: 0.1006\n",
      "21/223, train_loss: 0.1009, step time: 0.1004\n",
      "22/223, train_loss: 0.0940, step time: 0.1023\n",
      "23/223, train_loss: 0.0993, step time: 0.1107\n",
      "24/223, train_loss: 0.1073, step time: 0.1068\n",
      "25/223, train_loss: 0.1012, step time: 0.1014\n",
      "26/223, train_loss: 0.0934, step time: 0.1133\n",
      "27/223, train_loss: 0.1034, step time: 0.1035\n",
      "28/223, train_loss: 0.0945, step time: 0.1046\n",
      "29/223, train_loss: 0.0983, step time: 0.1070\n",
      "30/223, train_loss: 0.0942, step time: 0.1166\n",
      "31/223, train_loss: 0.0925, step time: 0.1113\n",
      "32/223, train_loss: 0.0905, step time: 0.1096\n",
      "33/223, train_loss: 0.1072, step time: 0.1124\n",
      "34/223, train_loss: 0.1044, step time: 0.1042\n",
      "35/223, train_loss: 0.1181, step time: 0.1081\n",
      "36/223, train_loss: 0.1022, step time: 0.1040\n",
      "37/223, train_loss: 0.1022, step time: 0.1077\n",
      "38/223, train_loss: 0.1004, step time: 0.1053\n",
      "39/223, train_loss: 0.1013, step time: 0.1090\n",
      "40/223, train_loss: 0.1039, step time: 0.0999\n",
      "41/223, train_loss: 0.0964, step time: 0.1125\n",
      "42/223, train_loss: 0.1017, step time: 0.1066\n",
      "43/223, train_loss: 0.1047, step time: 0.1000\n",
      "44/223, train_loss: 0.0996, step time: 0.1004\n",
      "45/223, train_loss: 0.1081, step time: 0.1149\n",
      "46/223, train_loss: 0.0957, step time: 0.1003\n",
      "47/223, train_loss: 0.0988, step time: 0.1150\n",
      "48/223, train_loss: 0.1037, step time: 0.1006\n",
      "49/223, train_loss: 0.1032, step time: 0.1123\n",
      "50/223, train_loss: 0.1044, step time: 0.1183\n",
      "51/223, train_loss: 0.1118, step time: 0.1069\n",
      "52/223, train_loss: 0.0943, step time: 0.0999\n",
      "53/223, train_loss: 0.0955, step time: 0.1084\n",
      "54/223, train_loss: 0.0966, step time: 0.1143\n",
      "55/223, train_loss: 0.1106, step time: 0.1074\n",
      "56/223, train_loss: 0.1109, step time: 0.1003\n",
      "57/223, train_loss: 0.0932, step time: 0.1139\n",
      "58/223, train_loss: 0.1072, step time: 0.0995\n",
      "59/223, train_loss: 0.0991, step time: 0.1284\n",
      "60/223, train_loss: 0.1083, step time: 0.1045\n",
      "61/223, train_loss: 0.1144, step time: 0.1188\n",
      "62/223, train_loss: 0.1035, step time: 0.1164\n",
      "63/223, train_loss: 0.1038, step time: 0.1234\n",
      "64/223, train_loss: 0.0964, step time: 0.1007\n",
      "65/223, train_loss: 0.1020, step time: 0.1010\n",
      "66/223, train_loss: 0.1119, step time: 0.1074\n",
      "67/223, train_loss: 0.1065, step time: 0.1090\n",
      "68/223, train_loss: 0.1079, step time: 0.1010\n",
      "69/223, train_loss: 0.0966, step time: 0.1188\n",
      "70/223, train_loss: 0.0933, step time: 0.1078\n",
      "71/223, train_loss: 0.0964, step time: 0.1011\n",
      "72/223, train_loss: 0.0888, step time: 0.1137\n",
      "73/223, train_loss: 0.0873, step time: 0.1149\n",
      "74/223, train_loss: 0.1149, step time: 0.1166\n",
      "75/223, train_loss: 0.0998, step time: 0.1082\n",
      "76/223, train_loss: 0.1010, step time: 0.1001\n",
      "77/223, train_loss: 0.1069, step time: 0.1087\n",
      "78/223, train_loss: 0.1046, step time: 0.0998\n",
      "79/223, train_loss: 0.1020, step time: 0.1277\n",
      "80/223, train_loss: 0.1020, step time: 0.1141\n",
      "81/223, train_loss: 0.1023, step time: 0.1039\n",
      "82/223, train_loss: 0.0883, step time: 0.1005\n",
      "83/223, train_loss: 0.1137, step time: 0.1118\n",
      "84/223, train_loss: 0.0996, step time: 0.1206\n",
      "85/223, train_loss: 0.0928, step time: 0.1142\n",
      "86/223, train_loss: 0.0969, step time: 0.1099\n",
      "87/223, train_loss: 0.1112, step time: 0.1460\n",
      "88/223, train_loss: 0.1082, step time: 0.1229\n",
      "89/223, train_loss: 0.1118, step time: 0.1079\n",
      "90/223, train_loss: 0.1052, step time: 0.1082\n",
      "91/223, train_loss: 0.1035, step time: 0.1239\n",
      "92/223, train_loss: 0.1111, step time: 0.1171\n",
      "93/223, train_loss: 0.0996, step time: 0.1087\n",
      "94/223, train_loss: 0.1162, step time: 0.1027\n",
      "95/223, train_loss: 0.0993, step time: 0.1078\n",
      "96/223, train_loss: 0.0922, step time: 0.1004\n",
      "97/223, train_loss: 0.1106, step time: 0.1075\n",
      "98/223, train_loss: 0.1119, step time: 0.1237\n",
      "99/223, train_loss: 0.0937, step time: 0.1219\n",
      "100/223, train_loss: 0.1003, step time: 0.1193\n",
      "101/223, train_loss: 0.0941, step time: 0.1168\n",
      "102/223, train_loss: 0.0997, step time: 0.1243\n",
      "103/223, train_loss: 0.1076, step time: 0.1018\n",
      "104/223, train_loss: 0.0941, step time: 0.1003\n",
      "105/223, train_loss: 0.1008, step time: 0.0998\n",
      "106/223, train_loss: 0.0984, step time: 0.0998\n",
      "107/223, train_loss: 0.1031, step time: 0.0995\n",
      "108/223, train_loss: 0.1038, step time: 0.1003\n",
      "109/223, train_loss: 0.0992, step time: 0.1000\n",
      "110/223, train_loss: 0.0934, step time: 0.0994\n",
      "111/223, train_loss: 0.0993, step time: 0.0998\n",
      "112/223, train_loss: 0.0954, step time: 0.1225\n",
      "113/223, train_loss: 0.1035, step time: 0.1155\n",
      "114/223, train_loss: 0.0972, step time: 0.1130\n",
      "115/223, train_loss: 0.1049, step time: 0.1212\n",
      "116/223, train_loss: 0.1041, step time: 0.1051\n",
      "117/223, train_loss: 0.0979, step time: 0.1039\n",
      "118/223, train_loss: 0.0956, step time: 0.1002\n",
      "119/223, train_loss: 0.0969, step time: 0.1002\n",
      "120/223, train_loss: 0.2980, step time: 0.1188\n",
      "121/223, train_loss: 0.0974, step time: 0.1095\n",
      "122/223, train_loss: 0.0978, step time: 0.1575\n",
      "123/223, train_loss: 0.0936, step time: 0.1252\n",
      "124/223, train_loss: 0.0940, step time: 0.1144\n",
      "125/223, train_loss: 0.1064, step time: 0.1141\n",
      "126/223, train_loss: 0.0984, step time: 0.0995\n",
      "127/223, train_loss: 0.1138, step time: 0.1011\n",
      "128/223, train_loss: 0.0998, step time: 0.1014\n",
      "129/223, train_loss: 0.1035, step time: 0.1005\n",
      "130/223, train_loss: 0.1039, step time: 0.1004\n",
      "131/223, train_loss: 0.0973, step time: 0.1003\n",
      "132/223, train_loss: 0.0980, step time: 0.1008\n",
      "133/223, train_loss: 0.1060, step time: 0.1045\n",
      "134/223, train_loss: 0.1141, step time: 0.1096\n",
      "135/223, train_loss: 0.1004, step time: 0.1085\n",
      "136/223, train_loss: 0.0920, step time: 0.1187\n",
      "137/223, train_loss: 0.1027, step time: 0.1109\n",
      "138/223, train_loss: 0.1018, step time: 0.1029\n",
      "139/223, train_loss: 0.0985, step time: 0.1303\n",
      "140/223, train_loss: 0.1026, step time: 0.1223\n",
      "141/223, train_loss: 0.1035, step time: 0.1066\n",
      "142/223, train_loss: 0.1116, step time: 0.0988\n",
      "143/223, train_loss: 0.1117, step time: 0.0986\n",
      "144/223, train_loss: 0.0996, step time: 0.0985\n",
      "145/223, train_loss: 0.1037, step time: 0.1088\n",
      "146/223, train_loss: 0.1079, step time: 0.1000\n",
      "147/223, train_loss: 0.0954, step time: 0.0993\n",
      "148/223, train_loss: 0.1019, step time: 0.1011\n",
      "149/223, train_loss: 0.1074, step time: 0.1299\n",
      "150/223, train_loss: 0.0968, step time: 0.1375\n",
      "151/223, train_loss: 0.1038, step time: 0.1323\n",
      "152/223, train_loss: 0.0994, step time: 0.1275\n",
      "153/223, train_loss: 0.1076, step time: 0.1050\n",
      "154/223, train_loss: 0.0907, step time: 0.1160\n",
      "155/223, train_loss: 0.0955, step time: 0.1425\n",
      "156/223, train_loss: 0.1036, step time: 0.1102\n",
      "157/223, train_loss: 0.1119, step time: 0.1088\n",
      "158/223, train_loss: 0.1059, step time: 0.1011\n",
      "159/223, train_loss: 0.0939, step time: 0.1197\n",
      "160/223, train_loss: 0.1072, step time: 0.1149\n",
      "161/223, train_loss: 0.1226, step time: 0.1104\n",
      "162/223, train_loss: 0.1010, step time: 0.1064\n",
      "163/223, train_loss: 0.1059, step time: 0.1446\n",
      "164/223, train_loss: 0.0980, step time: 0.1003\n",
      "165/223, train_loss: 0.1032, step time: 0.1261\n",
      "166/223, train_loss: 0.1107, step time: 0.1170\n",
      "167/223, train_loss: 0.1158, step time: 0.1155\n",
      "168/223, train_loss: 0.0929, step time: 0.1014\n",
      "169/223, train_loss: 0.0949, step time: 0.1109\n",
      "170/223, train_loss: 0.0995, step time: 0.1083\n",
      "171/223, train_loss: 0.0969, step time: 0.1125\n",
      "172/223, train_loss: 0.0899, step time: 0.1069\n",
      "173/223, train_loss: 0.0901, step time: 0.1123\n",
      "174/223, train_loss: 0.1026, step time: 0.1070\n",
      "175/223, train_loss: 0.1040, step time: 0.1000\n",
      "176/223, train_loss: 0.0987, step time: 0.1011\n",
      "177/223, train_loss: 0.1025, step time: 0.1147\n",
      "178/223, train_loss: 0.1072, step time: 0.1070\n",
      "179/223, train_loss: 0.0910, step time: 0.1218\n",
      "180/223, train_loss: 0.0952, step time: 0.1111\n",
      "181/223, train_loss: 0.1020, step time: 0.1129\n",
      "182/223, train_loss: 0.1126, step time: 0.1092\n",
      "183/223, train_loss: 0.0894, step time: 0.1324\n",
      "184/223, train_loss: 0.1154, step time: 0.1001\n",
      "185/223, train_loss: 0.0982, step time: 0.1003\n",
      "186/223, train_loss: 0.1008, step time: 0.1003\n",
      "187/223, train_loss: 0.1002, step time: 0.1008\n",
      "188/223, train_loss: 0.0982, step time: 0.1196\n",
      "189/223, train_loss: 0.1143, step time: 0.1057\n",
      "190/223, train_loss: 0.1028, step time: 0.1054\n",
      "191/223, train_loss: 0.1060, step time: 0.1024\n",
      "192/223, train_loss: 0.1039, step time: 0.1239\n",
      "193/223, train_loss: 0.1128, step time: 0.1137\n",
      "194/223, train_loss: 0.1047, step time: 0.1024\n",
      "195/223, train_loss: 0.0964, step time: 0.1038\n",
      "196/223, train_loss: 0.1062, step time: 0.1013\n",
      "197/223, train_loss: 0.0980, step time: 0.1111\n",
      "198/223, train_loss: 0.1061, step time: 0.1173\n",
      "199/223, train_loss: 0.1016, step time: 0.1239\n",
      "200/223, train_loss: 0.1081, step time: 0.1022\n",
      "201/223, train_loss: 0.0990, step time: 0.0991\n",
      "202/223, train_loss: 0.0924, step time: 0.0984\n",
      "203/223, train_loss: 0.0948, step time: 0.1036\n",
      "204/223, train_loss: 0.1093, step time: 0.1012\n",
      "205/223, train_loss: 0.0938, step time: 0.1133\n",
      "206/223, train_loss: 0.1169, step time: 0.1016\n",
      "207/223, train_loss: 0.0955, step time: 0.1086\n",
      "208/223, train_loss: 0.0977, step time: 0.1083\n",
      "209/223, train_loss: 0.1119, step time: 0.0996\n",
      "210/223, train_loss: 0.1092, step time: 0.1005\n",
      "211/223, train_loss: 0.1066, step time: 0.1037\n",
      "212/223, train_loss: 0.1132, step time: 0.1041\n",
      "213/223, train_loss: 0.1028, step time: 0.1062\n",
      "214/223, train_loss: 0.0997, step time: 0.1000\n",
      "215/223, train_loss: 0.0973, step time: 0.1013\n",
      "216/223, train_loss: 0.1083, step time: 0.1278\n",
      "217/223, train_loss: 0.1093, step time: 0.1007\n",
      "218/223, train_loss: 0.0920, step time: 0.1004\n",
      "219/223, train_loss: 0.1085, step time: 0.1003\n",
      "220/223, train_loss: 0.1078, step time: 0.1015\n",
      "221/223, train_loss: 0.1049, step time: 0.0994\n",
      "222/223, train_loss: 0.0996, step time: 0.0990\n",
      "223/223, train_loss: 0.0963, step time: 0.1000\n",
      "epoch 196 average loss: 0.1029\n",
      "time consuming of epoch 196 is: 90.0521\n",
      "----------\n",
      "epoch 197/300\n",
      "1/223, train_loss: 0.0971, step time: 0.1011\n",
      "2/223, train_loss: 0.1003, step time: 0.1065\n",
      "3/223, train_loss: 0.1198, step time: 0.1221\n",
      "4/223, train_loss: 0.3137, step time: 0.1110\n",
      "5/223, train_loss: 0.0988, step time: 0.1145\n",
      "6/223, train_loss: 0.0999, step time: 0.1137\n",
      "7/223, train_loss: 0.0976, step time: 0.1203\n",
      "8/223, train_loss: 0.1179, step time: 0.1412\n",
      "9/223, train_loss: 0.0966, step time: 0.1153\n",
      "10/223, train_loss: 0.1122, step time: 0.1220\n",
      "11/223, train_loss: 0.1101, step time: 0.1267\n",
      "12/223, train_loss: 0.1106, step time: 0.1283\n",
      "13/223, train_loss: 0.0929, step time: 0.1060\n",
      "14/223, train_loss: 0.0971, step time: 0.1005\n",
      "15/223, train_loss: 0.1007, step time: 0.1758\n",
      "16/223, train_loss: 0.0912, step time: 0.1180\n",
      "17/223, train_loss: 0.0993, step time: 0.1074\n",
      "18/223, train_loss: 0.0968, step time: 0.1006\n",
      "19/223, train_loss: 0.0985, step time: 0.1055\n",
      "20/223, train_loss: 0.0928, step time: 0.1009\n",
      "21/223, train_loss: 0.0972, step time: 0.1382\n",
      "22/223, train_loss: 0.0948, step time: 0.1142\n",
      "23/223, train_loss: 0.1066, step time: 0.1066\n",
      "24/223, train_loss: 0.1004, step time: 0.0997\n",
      "25/223, train_loss: 0.1042, step time: 0.0997\n",
      "26/223, train_loss: 0.1039, step time: 0.1274\n",
      "27/223, train_loss: 0.1027, step time: 0.1521\n",
      "28/223, train_loss: 0.1197, step time: 0.1242\n",
      "29/223, train_loss: 0.0921, step time: 0.1115\n",
      "30/223, train_loss: 0.1018, step time: 0.1056\n",
      "31/223, train_loss: 0.1026, step time: 0.1217\n",
      "32/223, train_loss: 0.0924, step time: 0.1023\n",
      "33/223, train_loss: 0.1129, step time: 0.1113\n",
      "34/223, train_loss: 0.0974, step time: 0.1325\n",
      "35/223, train_loss: 0.1050, step time: 0.0997\n",
      "36/223, train_loss: 0.0992, step time: 0.1091\n",
      "37/223, train_loss: 0.0940, step time: 0.1137\n",
      "38/223, train_loss: 0.1022, step time: 0.1197\n",
      "39/223, train_loss: 0.0942, step time: 0.1388\n",
      "40/223, train_loss: 0.1012, step time: 0.1134\n",
      "41/223, train_loss: 0.1112, step time: 0.1094\n",
      "42/223, train_loss: 0.0942, step time: 0.1140\n",
      "43/223, train_loss: 0.1040, step time: 0.1290\n",
      "44/223, train_loss: 0.1027, step time: 0.1214\n",
      "45/223, train_loss: 0.0957, step time: 0.1164\n",
      "46/223, train_loss: 0.1011, step time: 0.1254\n",
      "47/223, train_loss: 0.0893, step time: 0.1190\n",
      "48/223, train_loss: 0.1007, step time: 0.0996\n",
      "49/223, train_loss: 0.1074, step time: 0.1171\n",
      "50/223, train_loss: 0.0966, step time: 0.1231\n",
      "51/223, train_loss: 0.1070, step time: 0.1372\n",
      "52/223, train_loss: 0.0892, step time: 0.1045\n",
      "53/223, train_loss: 0.0951, step time: 0.1153\n",
      "54/223, train_loss: 0.0984, step time: 0.1051\n",
      "55/223, train_loss: 0.1126, step time: 0.1070\n",
      "56/223, train_loss: 0.0990, step time: 0.1158\n",
      "57/223, train_loss: 0.0999, step time: 0.1156\n",
      "58/223, train_loss: 0.1079, step time: 0.1017\n",
      "59/223, train_loss: 0.0982, step time: 0.1085\n",
      "60/223, train_loss: 0.0989, step time: 0.1138\n",
      "61/223, train_loss: 0.1012, step time: 0.1168\n",
      "62/223, train_loss: 0.1052, step time: 0.1352\n",
      "63/223, train_loss: 0.0993, step time: 0.1512\n",
      "64/223, train_loss: 0.1190, step time: 0.1004\n",
      "65/223, train_loss: 0.0910, step time: 0.1107\n",
      "66/223, train_loss: 0.0944, step time: 0.1011\n",
      "67/223, train_loss: 0.1020, step time: 0.1054\n",
      "68/223, train_loss: 0.1056, step time: 0.1076\n",
      "69/223, train_loss: 0.0935, step time: 0.1153\n",
      "70/223, train_loss: 0.1011, step time: 0.1003\n",
      "71/223, train_loss: 0.1091, step time: 0.1008\n",
      "72/223, train_loss: 0.0996, step time: 0.1004\n",
      "73/223, train_loss: 0.0958, step time: 0.1120\n",
      "74/223, train_loss: 0.1058, step time: 0.1113\n",
      "75/223, train_loss: 0.1026, step time: 0.1178\n",
      "76/223, train_loss: 0.1035, step time: 0.1150\n",
      "77/223, train_loss: 0.1067, step time: 0.1286\n",
      "78/223, train_loss: 0.1114, step time: 0.1366\n",
      "79/223, train_loss: 0.0918, step time: 0.0999\n",
      "80/223, train_loss: 0.0968, step time: 0.1002\n",
      "81/223, train_loss: 0.1012, step time: 0.1274\n",
      "82/223, train_loss: 0.1182, step time: 0.1288\n",
      "83/223, train_loss: 0.1061, step time: 0.1351\n",
      "84/223, train_loss: 0.0985, step time: 0.1053\n",
      "85/223, train_loss: 0.0990, step time: 0.1004\n",
      "86/223, train_loss: 0.0952, step time: 0.1007\n",
      "87/223, train_loss: 0.1004, step time: 0.1008\n",
      "88/223, train_loss: 0.1027, step time: 0.1110\n",
      "89/223, train_loss: 0.0920, step time: 0.1010\n",
      "90/223, train_loss: 0.1058, step time: 0.1080\n",
      "91/223, train_loss: 0.0935, step time: 0.1103\n",
      "92/223, train_loss: 0.0951, step time: 0.1047\n",
      "93/223, train_loss: 0.0977, step time: 0.1005\n",
      "94/223, train_loss: 0.1013, step time: 0.1004\n",
      "95/223, train_loss: 0.0970, step time: 0.0998\n",
      "96/223, train_loss: 0.1098, step time: 0.1057\n",
      "97/223, train_loss: 0.1005, step time: 0.1122\n",
      "98/223, train_loss: 0.0935, step time: 0.1111\n",
      "99/223, train_loss: 0.1061, step time: 0.1186\n",
      "100/223, train_loss: 0.1154, step time: 0.1156\n",
      "101/223, train_loss: 0.0985, step time: 0.1382\n",
      "102/223, train_loss: 0.1038, step time: 0.1569\n",
      "103/223, train_loss: 0.1064, step time: 0.1001\n",
      "104/223, train_loss: 0.1003, step time: 0.1001\n",
      "105/223, train_loss: 0.0999, step time: 0.1134\n",
      "106/223, train_loss: 0.1005, step time: 0.1159\n",
      "107/223, train_loss: 0.0924, step time: 0.1149\n",
      "108/223, train_loss: 0.0981, step time: 0.1003\n",
      "109/223, train_loss: 0.0987, step time: 0.1210\n",
      "110/223, train_loss: 0.1110, step time: 0.1195\n",
      "111/223, train_loss: 0.0999, step time: 0.1005\n",
      "112/223, train_loss: 0.1018, step time: 0.1030\n",
      "113/223, train_loss: 0.1014, step time: 0.1137\n",
      "114/223, train_loss: 0.1162, step time: 0.1167\n",
      "115/223, train_loss: 0.0959, step time: 0.1007\n",
      "116/223, train_loss: 0.1091, step time: 0.1005\n",
      "117/223, train_loss: 0.1056, step time: 0.1155\n",
      "118/223, train_loss: 0.1016, step time: 0.1005\n",
      "119/223, train_loss: 0.1061, step time: 0.1007\n",
      "120/223, train_loss: 0.1080, step time: 0.1129\n",
      "121/223, train_loss: 0.1010, step time: 0.1071\n",
      "122/223, train_loss: 0.1011, step time: 0.1075\n",
      "123/223, train_loss: 0.0932, step time: 0.1051\n",
      "124/223, train_loss: 0.1009, step time: 0.1003\n",
      "125/223, train_loss: 0.1125, step time: 0.1197\n",
      "126/223, train_loss: 0.0934, step time: 0.1039\n",
      "127/223, train_loss: 0.0910, step time: 0.1116\n",
      "128/223, train_loss: 0.1092, step time: 0.1043\n",
      "129/223, train_loss: 0.0999, step time: 0.1103\n",
      "130/223, train_loss: 0.1013, step time: 0.1107\n",
      "131/223, train_loss: 0.0967, step time: 0.1222\n",
      "132/223, train_loss: 0.1032, step time: 0.1149\n",
      "133/223, train_loss: 0.0946, step time: 0.1258\n",
      "134/223, train_loss: 0.1045, step time: 0.1367\n",
      "135/223, train_loss: 0.1078, step time: 0.1157\n",
      "136/223, train_loss: 0.1067, step time: 0.1040\n",
      "137/223, train_loss: 0.1095, step time: 0.1111\n",
      "138/223, train_loss: 0.0991, step time: 0.1086\n",
      "139/223, train_loss: 0.1002, step time: 0.1076\n",
      "140/223, train_loss: 0.1180, step time: 0.1004\n",
      "141/223, train_loss: 0.1034, step time: 0.1127\n",
      "142/223, train_loss: 0.0936, step time: 0.1483\n",
      "143/223, train_loss: 0.1038, step time: 0.1136\n",
      "144/223, train_loss: 0.0935, step time: 0.1105\n",
      "145/223, train_loss: 0.0993, step time: 0.1003\n",
      "146/223, train_loss: 0.0957, step time: 0.1008\n",
      "147/223, train_loss: 0.1091, step time: 0.1055\n",
      "148/223, train_loss: 0.1000, step time: 0.1004\n",
      "149/223, train_loss: 0.0942, step time: 0.1135\n",
      "150/223, train_loss: 0.0949, step time: 0.1090\n",
      "151/223, train_loss: 0.0954, step time: 0.1024\n",
      "152/223, train_loss: 0.1084, step time: 0.1000\n",
      "153/223, train_loss: 0.0977, step time: 0.0998\n",
      "154/223, train_loss: 0.0958, step time: 0.1048\n",
      "155/223, train_loss: 0.0885, step time: 0.1396\n",
      "156/223, train_loss: 0.1008, step time: 0.1000\n",
      "157/223, train_loss: 0.1055, step time: 0.1082\n",
      "158/223, train_loss: 0.1051, step time: 0.1433\n",
      "159/223, train_loss: 0.1007, step time: 0.1252\n",
      "160/223, train_loss: 0.0930, step time: 0.1007\n",
      "161/223, train_loss: 0.1112, step time: 0.1252\n",
      "162/223, train_loss: 0.0947, step time: 0.1088\n",
      "163/223, train_loss: 0.1057, step time: 0.1352\n",
      "164/223, train_loss: 0.1011, step time: 0.1108\n",
      "165/223, train_loss: 0.1080, step time: 0.1192\n",
      "166/223, train_loss: 0.1014, step time: 0.1219\n",
      "167/223, train_loss: 0.1008, step time: 0.1103\n",
      "168/223, train_loss: 0.1034, step time: 0.1011\n",
      "169/223, train_loss: 0.1011, step time: 0.1069\n",
      "170/223, train_loss: 0.1098, step time: 0.1005\n",
      "171/223, train_loss: 0.1114, step time: 0.1015\n",
      "172/223, train_loss: 0.1113, step time: 0.1125\n",
      "173/223, train_loss: 0.1071, step time: 0.1004\n",
      "174/223, train_loss: 0.1052, step time: 0.1013\n",
      "175/223, train_loss: 0.0983, step time: 0.1006\n",
      "176/223, train_loss: 0.1055, step time: 0.1026\n",
      "177/223, train_loss: 0.0938, step time: 0.1090\n",
      "178/223, train_loss: 0.1023, step time: 0.1246\n",
      "179/223, train_loss: 0.0886, step time: 0.1100\n",
      "180/223, train_loss: 0.0994, step time: 0.1191\n",
      "181/223, train_loss: 0.1003, step time: 0.1145\n",
      "182/223, train_loss: 0.1004, step time: 0.1061\n",
      "183/223, train_loss: 0.0921, step time: 0.1207\n",
      "184/223, train_loss: 0.1031, step time: 0.1141\n",
      "185/223, train_loss: 0.0980, step time: 0.1002\n",
      "186/223, train_loss: 0.0965, step time: 0.1142\n",
      "187/223, train_loss: 0.0960, step time: 0.1012\n",
      "188/223, train_loss: 0.0949, step time: 0.1113\n",
      "189/223, train_loss: 0.0957, step time: 0.1181\n",
      "190/223, train_loss: 0.1017, step time: 0.1175\n",
      "191/223, train_loss: 0.1051, step time: 0.1112\n",
      "192/223, train_loss: 0.1182, step time: 0.1086\n",
      "193/223, train_loss: 0.1056, step time: 0.1002\n",
      "194/223, train_loss: 0.0926, step time: 0.1193\n",
      "195/223, train_loss: 0.0955, step time: 0.1010\n",
      "196/223, train_loss: 0.1118, step time: 0.1041\n",
      "197/223, train_loss: 0.1072, step time: 0.1016\n",
      "198/223, train_loss: 0.1105, step time: 0.1214\n",
      "199/223, train_loss: 0.0974, step time: 0.1217\n",
      "200/223, train_loss: 0.1033, step time: 0.1190\n",
      "201/223, train_loss: 0.1067, step time: 0.1204\n",
      "202/223, train_loss: 0.1066, step time: 0.1381\n",
      "203/223, train_loss: 0.1061, step time: 0.0999\n",
      "204/223, train_loss: 0.1023, step time: 0.1218\n",
      "205/223, train_loss: 0.1118, step time: 0.1239\n",
      "206/223, train_loss: 0.1011, step time: 0.1224\n",
      "207/223, train_loss: 0.0965, step time: 0.1120\n",
      "208/223, train_loss: 0.1051, step time: 0.1008\n",
      "209/223, train_loss: 0.1024, step time: 0.1168\n",
      "210/223, train_loss: 0.0999, step time: 0.1466\n",
      "211/223, train_loss: 0.1044, step time: 0.1176\n",
      "212/223, train_loss: 0.1116, step time: 0.1045\n",
      "213/223, train_loss: 0.1017, step time: 0.1084\n",
      "214/223, train_loss: 0.1086, step time: 0.1126\n",
      "215/223, train_loss: 0.1194, step time: 0.1777\n",
      "216/223, train_loss: 0.1012, step time: 0.1207\n",
      "217/223, train_loss: 0.1119, step time: 0.1020\n",
      "218/223, train_loss: 0.1053, step time: 0.0997\n",
      "219/223, train_loss: 0.0934, step time: 0.1004\n",
      "220/223, train_loss: 0.1073, step time: 0.1022\n",
      "221/223, train_loss: 0.0926, step time: 0.0992\n",
      "222/223, train_loss: 0.0994, step time: 0.0997\n",
      "223/223, train_loss: 0.1101, step time: 0.0996\n",
      "epoch 197 average loss: 0.1027\n",
      "time consuming of epoch 197 is: 89.8713\n",
      "----------\n",
      "epoch 198/300\n",
      "1/223, train_loss: 0.1179, step time: 0.1059\n",
      "2/223, train_loss: 0.0947, step time: 0.1125\n",
      "3/223, train_loss: 0.0996, step time: 0.1232\n",
      "4/223, train_loss: 0.1054, step time: 0.1083\n",
      "5/223, train_loss: 0.0970, step time: 0.1074\n",
      "6/223, train_loss: 0.1062, step time: 0.1458\n",
      "7/223, train_loss: 0.1018, step time: 0.1340\n",
      "8/223, train_loss: 0.1042, step time: 0.1003\n",
      "9/223, train_loss: 0.1071, step time: 0.1240\n",
      "10/223, train_loss: 0.1192, step time: 0.1314\n",
      "11/223, train_loss: 0.0942, step time: 0.1011\n",
      "12/223, train_loss: 0.1043, step time: 0.1009\n",
      "13/223, train_loss: 0.1060, step time: 0.1036\n",
      "14/223, train_loss: 0.1016, step time: 0.1266\n",
      "15/223, train_loss: 0.1026, step time: 0.1002\n",
      "16/223, train_loss: 0.1001, step time: 0.1000\n",
      "17/223, train_loss: 0.0947, step time: 0.1081\n",
      "18/223, train_loss: 0.0950, step time: 0.1282\n",
      "19/223, train_loss: 0.1021, step time: 0.1059\n",
      "20/223, train_loss: 0.1019, step time: 0.1074\n",
      "21/223, train_loss: 0.1019, step time: 0.1156\n",
      "22/223, train_loss: 0.1270, step time: 0.1003\n",
      "23/223, train_loss: 0.0961, step time: 0.0998\n",
      "24/223, train_loss: 0.0976, step time: 0.1014\n",
      "25/223, train_loss: 0.1016, step time: 0.1114\n",
      "26/223, train_loss: 0.1122, step time: 0.1053\n",
      "27/223, train_loss: 0.1015, step time: 0.1175\n",
      "28/223, train_loss: 0.1083, step time: 0.1495\n",
      "29/223, train_loss: 0.1103, step time: 0.1114\n",
      "30/223, train_loss: 0.1040, step time: 0.1049\n",
      "31/223, train_loss: 0.1092, step time: 0.1341\n",
      "32/223, train_loss: 0.1022, step time: 0.1184\n",
      "33/223, train_loss: 0.0994, step time: 0.1070\n",
      "34/223, train_loss: 0.0996, step time: 0.1139\n",
      "35/223, train_loss: 0.0910, step time: 0.1132\n",
      "36/223, train_loss: 0.0957, step time: 0.1011\n",
      "37/223, train_loss: 0.1000, step time: 0.1099\n",
      "38/223, train_loss: 0.1095, step time: 0.1021\n",
      "39/223, train_loss: 0.0957, step time: 0.1009\n",
      "40/223, train_loss: 0.1123, step time: 0.1024\n",
      "41/223, train_loss: 0.1013, step time: 0.0997\n",
      "42/223, train_loss: 0.1022, step time: 0.1122\n",
      "43/223, train_loss: 0.1155, step time: 0.1153\n",
      "44/223, train_loss: 0.1042, step time: 0.1303\n",
      "45/223, train_loss: 0.0946, step time: 0.1440\n",
      "46/223, train_loss: 0.0956, step time: 0.1271\n",
      "47/223, train_loss: 0.0925, step time: 0.1218\n",
      "48/223, train_loss: 0.1069, step time: 0.1002\n",
      "49/223, train_loss: 0.0950, step time: 0.1140\n",
      "50/223, train_loss: 0.0978, step time: 0.1282\n",
      "51/223, train_loss: 0.0921, step time: 0.1036\n",
      "52/223, train_loss: 0.0959, step time: 0.1020\n",
      "53/223, train_loss: 0.1020, step time: 0.1167\n",
      "54/223, train_loss: 0.1117, step time: 0.1046\n",
      "55/223, train_loss: 0.1034, step time: 0.1084\n",
      "56/223, train_loss: 0.0945, step time: 0.1138\n",
      "57/223, train_loss: 0.0914, step time: 0.1141\n",
      "58/223, train_loss: 0.1116, step time: 0.1003\n",
      "59/223, train_loss: 0.0997, step time: 0.0999\n",
      "60/223, train_loss: 0.1067, step time: 0.1000\n",
      "61/223, train_loss: 0.0929, step time: 0.1254\n",
      "62/223, train_loss: 0.1047, step time: 0.1099\n",
      "63/223, train_loss: 0.1040, step time: 0.1289\n",
      "64/223, train_loss: 0.1094, step time: 0.1233\n",
      "65/223, train_loss: 0.1015, step time: 0.1146\n",
      "66/223, train_loss: 0.0965, step time: 0.1012\n",
      "67/223, train_loss: 0.0984, step time: 0.1006\n",
      "68/223, train_loss: 0.0999, step time: 0.1019\n",
      "69/223, train_loss: 0.0973, step time: 0.1129\n",
      "70/223, train_loss: 0.1028, step time: 0.1134\n",
      "71/223, train_loss: 0.0973, step time: 0.1130\n",
      "72/223, train_loss: 0.0977, step time: 0.1005\n",
      "73/223, train_loss: 0.0977, step time: 0.1167\n",
      "74/223, train_loss: 0.1006, step time: 0.1031\n",
      "75/223, train_loss: 0.0949, step time: 0.1019\n",
      "76/223, train_loss: 0.1118, step time: 0.1010\n",
      "77/223, train_loss: 0.1019, step time: 0.1087\n",
      "78/223, train_loss: 0.0873, step time: 0.1012\n",
      "79/223, train_loss: 0.0980, step time: 0.0998\n",
      "80/223, train_loss: 0.0910, step time: 0.1038\n",
      "81/223, train_loss: 0.1044, step time: 0.1072\n",
      "82/223, train_loss: 0.1014, step time: 0.1007\n",
      "83/223, train_loss: 0.2949, step time: 0.1022\n",
      "84/223, train_loss: 0.1047, step time: 0.1036\n",
      "85/223, train_loss: 0.1021, step time: 0.1109\n",
      "86/223, train_loss: 0.1039, step time: 0.1185\n",
      "87/223, train_loss: 0.1025, step time: 0.1285\n",
      "88/223, train_loss: 0.0953, step time: 0.0993\n",
      "89/223, train_loss: 0.1107, step time: 0.1167\n",
      "90/223, train_loss: 0.0937, step time: 0.1395\n",
      "91/223, train_loss: 0.1043, step time: 0.1061\n",
      "92/223, train_loss: 0.1070, step time: 0.1004\n",
      "93/223, train_loss: 0.1002, step time: 0.1001\n",
      "94/223, train_loss: 0.1154, step time: 0.1078\n",
      "95/223, train_loss: 0.0929, step time: 0.1052\n",
      "96/223, train_loss: 0.0982, step time: 0.1123\n",
      "97/223, train_loss: 0.1006, step time: 0.1015\n",
      "98/223, train_loss: 0.0991, step time: 0.1009\n",
      "99/223, train_loss: 0.0957, step time: 0.1624\n",
      "100/223, train_loss: 0.1075, step time: 0.1063\n",
      "101/223, train_loss: 0.1049, step time: 0.1166\n",
      "102/223, train_loss: 0.0877, step time: 0.1049\n",
      "103/223, train_loss: 0.0924, step time: 0.1110\n",
      "104/223, train_loss: 0.1004, step time: 0.1036\n",
      "105/223, train_loss: 0.1082, step time: 0.1341\n",
      "106/223, train_loss: 0.1105, step time: 0.0995\n",
      "107/223, train_loss: 0.1040, step time: 0.1010\n",
      "108/223, train_loss: 0.0986, step time: 0.1011\n",
      "109/223, train_loss: 0.1090, step time: 0.1009\n",
      "110/223, train_loss: 0.1094, step time: 0.1392\n",
      "111/223, train_loss: 0.0988, step time: 0.1182\n",
      "112/223, train_loss: 0.0927, step time: 0.1205\n",
      "113/223, train_loss: 0.0951, step time: 0.1018\n",
      "114/223, train_loss: 0.0990, step time: 0.1060\n",
      "115/223, train_loss: 0.0912, step time: 0.1071\n",
      "116/223, train_loss: 0.1118, step time: 0.1197\n",
      "117/223, train_loss: 0.1080, step time: 0.1077\n",
      "118/223, train_loss: 0.0947, step time: 0.1155\n",
      "119/223, train_loss: 0.1067, step time: 0.1308\n",
      "120/223, train_loss: 0.0968, step time: 0.1056\n",
      "121/223, train_loss: 0.1205, step time: 0.1064\n",
      "122/223, train_loss: 0.0969, step time: 0.1094\n",
      "123/223, train_loss: 0.0931, step time: 0.1098\n",
      "124/223, train_loss: 0.0942, step time: 0.1278\n",
      "125/223, train_loss: 0.0994, step time: 0.1082\n",
      "126/223, train_loss: 0.1002, step time: 0.1307\n",
      "127/223, train_loss: 0.1068, step time: 0.1109\n",
      "128/223, train_loss: 0.0966, step time: 0.1006\n",
      "129/223, train_loss: 0.1003, step time: 0.1315\n",
      "130/223, train_loss: 0.1016, step time: 0.1191\n",
      "131/223, train_loss: 0.1027, step time: 0.1091\n",
      "132/223, train_loss: 0.0984, step time: 0.1152\n",
      "133/223, train_loss: 0.1128, step time: 0.1117\n",
      "134/223, train_loss: 0.1004, step time: 0.1426\n",
      "135/223, train_loss: 0.1024, step time: 0.1159\n",
      "136/223, train_loss: 0.1047, step time: 0.1125\n",
      "137/223, train_loss: 0.1001, step time: 0.1273\n",
      "138/223, train_loss: 0.1080, step time: 0.1068\n",
      "139/223, train_loss: 0.1004, step time: 0.1224\n",
      "140/223, train_loss: 0.1082, step time: 0.1004\n",
      "141/223, train_loss: 0.1025, step time: 0.1166\n",
      "142/223, train_loss: 0.1097, step time: 0.1028\n",
      "143/223, train_loss: 0.1084, step time: 0.1039\n",
      "144/223, train_loss: 0.0960, step time: 0.1002\n",
      "145/223, train_loss: 0.1010, step time: 0.1137\n",
      "146/223, train_loss: 0.1031, step time: 0.1097\n",
      "147/223, train_loss: 0.1063, step time: 0.1208\n",
      "148/223, train_loss: 0.1026, step time: 0.1193\n",
      "149/223, train_loss: 0.0965, step time: 0.1198\n",
      "150/223, train_loss: 0.1063, step time: 0.1127\n",
      "151/223, train_loss: 0.1033, step time: 0.1140\n",
      "152/223, train_loss: 0.1139, step time: 0.1028\n",
      "153/223, train_loss: 0.0895, step time: 0.1125\n",
      "154/223, train_loss: 0.1102, step time: 0.1807\n",
      "155/223, train_loss: 0.1044, step time: 0.0997\n",
      "156/223, train_loss: 0.1060, step time: 0.0996\n",
      "157/223, train_loss: 0.0939, step time: 0.1099\n",
      "158/223, train_loss: 0.1017, step time: 0.1254\n",
      "159/223, train_loss: 0.1064, step time: 0.1092\n",
      "160/223, train_loss: 0.1059, step time: 0.1393\n",
      "161/223, train_loss: 0.1017, step time: 0.1107\n",
      "162/223, train_loss: 0.0999, step time: 0.1120\n",
      "163/223, train_loss: 0.0913, step time: 0.1132\n",
      "164/223, train_loss: 0.1098, step time: 0.1735\n",
      "165/223, train_loss: 0.0926, step time: 0.1000\n",
      "166/223, train_loss: 0.0964, step time: 0.1070\n",
      "167/223, train_loss: 0.1018, step time: 0.1263\n",
      "168/223, train_loss: 0.0946, step time: 0.1017\n",
      "169/223, train_loss: 0.1004, step time: 0.1312\n",
      "170/223, train_loss: 0.1021, step time: 0.0996\n",
      "171/223, train_loss: 0.0935, step time: 0.1108\n",
      "172/223, train_loss: 0.1018, step time: 0.1003\n",
      "173/223, train_loss: 0.1056, step time: 0.1004\n",
      "174/223, train_loss: 0.0959, step time: 0.1007\n",
      "175/223, train_loss: 0.0959, step time: 0.1047\n",
      "176/223, train_loss: 0.1085, step time: 0.1178\n",
      "177/223, train_loss: 0.0977, step time: 0.1287\n",
      "178/223, train_loss: 0.0967, step time: 0.1094\n",
      "179/223, train_loss: 0.0925, step time: 0.1004\n",
      "180/223, train_loss: 0.0942, step time: 0.1028\n",
      "181/223, train_loss: 0.0998, step time: 0.1147\n",
      "182/223, train_loss: 0.1029, step time: 0.1146\n",
      "183/223, train_loss: 0.0986, step time: 0.1327\n",
      "184/223, train_loss: 0.1023, step time: 0.1063\n",
      "185/223, train_loss: 0.1052, step time: 0.1348\n",
      "186/223, train_loss: 0.1046, step time: 0.1078\n",
      "187/223, train_loss: 0.1142, step time: 0.1237\n",
      "188/223, train_loss: 0.1061, step time: 0.1005\n",
      "189/223, train_loss: 0.1072, step time: 0.1295\n",
      "190/223, train_loss: 0.0957, step time: 0.1055\n",
      "191/223, train_loss: 0.0924, step time: 0.1114\n",
      "192/223, train_loss: 0.0977, step time: 0.1048\n",
      "193/223, train_loss: 0.1011, step time: 0.1042\n",
      "194/223, train_loss: 0.1040, step time: 0.1045\n",
      "195/223, train_loss: 0.1057, step time: 0.1024\n",
      "196/223, train_loss: 0.1083, step time: 0.1096\n",
      "197/223, train_loss: 0.0992, step time: 0.1188\n",
      "198/223, train_loss: 0.1081, step time: 0.1182\n",
      "199/223, train_loss: 0.1134, step time: 0.1236\n",
      "200/223, train_loss: 0.1085, step time: 0.1201\n",
      "201/223, train_loss: 0.0949, step time: 0.1056\n",
      "202/223, train_loss: 0.1073, step time: 0.1099\n",
      "203/223, train_loss: 0.0999, step time: 0.1168\n",
      "204/223, train_loss: 0.1070, step time: 0.1083\n",
      "205/223, train_loss: 0.1124, step time: 0.1077\n",
      "206/223, train_loss: 0.0943, step time: 0.1019\n",
      "207/223, train_loss: 0.1082, step time: 0.1018\n",
      "208/223, train_loss: 0.1039, step time: 0.1018\n",
      "209/223, train_loss: 0.0962, step time: 0.1383\n",
      "210/223, train_loss: 0.0919, step time: 0.1233\n",
      "211/223, train_loss: 0.0981, step time: 0.1307\n",
      "212/223, train_loss: 0.1111, step time: 0.1152\n",
      "213/223, train_loss: 0.1094, step time: 0.1526\n",
      "214/223, train_loss: 0.1050, step time: 0.1152\n",
      "215/223, train_loss: 0.1056, step time: 0.1189\n",
      "216/223, train_loss: 0.1102, step time: 0.1015\n",
      "217/223, train_loss: 0.1206, step time: 0.1019\n",
      "218/223, train_loss: 0.1108, step time: 0.1005\n",
      "219/223, train_loss: 0.0995, step time: 0.1003\n",
      "220/223, train_loss: 0.0930, step time: 0.1001\n",
      "221/223, train_loss: 0.0957, step time: 0.0998\n",
      "222/223, train_loss: 0.1070, step time: 0.0995\n",
      "223/223, train_loss: 0.1138, step time: 0.1004\n",
      "epoch 198 average loss: 0.1029\n",
      "time consuming of epoch 198 is: 87.6929\n",
      "----------\n",
      "epoch 199/300\n",
      "1/223, train_loss: 0.0987, step time: 0.1073\n",
      "2/223, train_loss: 0.1149, step time: 0.1191\n",
      "3/223, train_loss: 0.0933, step time: 0.1164\n",
      "4/223, train_loss: 0.0956, step time: 0.1017\n",
      "5/223, train_loss: 0.0976, step time: 0.1197\n",
      "6/223, train_loss: 0.1033, step time: 0.1088\n",
      "7/223, train_loss: 0.0925, step time: 0.1438\n",
      "8/223, train_loss: 0.0942, step time: 0.1135\n",
      "9/223, train_loss: 0.1111, step time: 0.1227\n",
      "10/223, train_loss: 0.1005, step time: 0.1246\n",
      "11/223, train_loss: 0.1088, step time: 0.1060\n",
      "12/223, train_loss: 0.1027, step time: 0.1007\n",
      "13/223, train_loss: 0.1079, step time: 0.1168\n",
      "14/223, train_loss: 0.0932, step time: 0.1076\n",
      "15/223, train_loss: 0.1076, step time: 0.1303\n",
      "16/223, train_loss: 0.1098, step time: 0.1024\n",
      "17/223, train_loss: 0.0961, step time: 0.1172\n",
      "18/223, train_loss: 0.1078, step time: 0.1166\n",
      "19/223, train_loss: 0.1106, step time: 0.1054\n",
      "20/223, train_loss: 0.0926, step time: 0.1327\n",
      "21/223, train_loss: 0.0966, step time: 0.1127\n",
      "22/223, train_loss: 0.1072, step time: 0.1270\n",
      "23/223, train_loss: 0.1031, step time: 0.1070\n",
      "24/223, train_loss: 0.1148, step time: 0.1338\n",
      "25/223, train_loss: 0.0927, step time: 0.1047\n",
      "26/223, train_loss: 0.0963, step time: 0.1051\n",
      "27/223, train_loss: 0.1112, step time: 0.1487\n",
      "28/223, train_loss: 0.0968, step time: 0.1046\n",
      "29/223, train_loss: 0.0943, step time: 0.1260\n",
      "30/223, train_loss: 0.0997, step time: 0.1100\n",
      "31/223, train_loss: 0.1054, step time: 0.1016\n",
      "32/223, train_loss: 0.1070, step time: 0.1105\n",
      "33/223, train_loss: 0.1096, step time: 0.1138\n",
      "34/223, train_loss: 0.0978, step time: 0.1401\n",
      "35/223, train_loss: 0.1017, step time: 0.1192\n",
      "36/223, train_loss: 0.0991, step time: 0.1120\n",
      "37/223, train_loss: 0.1005, step time: 0.1047\n",
      "38/223, train_loss: 0.0943, step time: 0.1134\n",
      "39/223, train_loss: 0.1065, step time: 0.1093\n",
      "40/223, train_loss: 0.0958, step time: 0.1000\n",
      "41/223, train_loss: 0.0992, step time: 0.1035\n",
      "42/223, train_loss: 0.1100, step time: 0.1089\n",
      "43/223, train_loss: 0.1077, step time: 0.1173\n",
      "44/223, train_loss: 0.0947, step time: 0.1067\n",
      "45/223, train_loss: 0.0941, step time: 0.1127\n",
      "46/223, train_loss: 0.1053, step time: 0.1129\n",
      "47/223, train_loss: 0.1040, step time: 0.1056\n",
      "48/223, train_loss: 0.1118, step time: 0.1242\n",
      "49/223, train_loss: 0.1045, step time: 0.1136\n",
      "50/223, train_loss: 0.1039, step time: 0.1061\n",
      "51/223, train_loss: 0.0914, step time: 0.1093\n",
      "52/223, train_loss: 0.0942, step time: 0.1366\n",
      "53/223, train_loss: 0.1121, step time: 0.1173\n",
      "54/223, train_loss: 0.1056, step time: 0.1106\n",
      "55/223, train_loss: 0.0979, step time: 0.1061\n",
      "56/223, train_loss: 0.0962, step time: 0.1192\n",
      "57/223, train_loss: 0.0938, step time: 0.1089\n",
      "58/223, train_loss: 0.1095, step time: 0.1091\n",
      "59/223, train_loss: 0.1102, step time: 0.1139\n",
      "60/223, train_loss: 0.0998, step time: 0.1132\n",
      "61/223, train_loss: 0.1071, step time: 0.1019\n",
      "62/223, train_loss: 0.0991, step time: 0.1091\n",
      "63/223, train_loss: 0.1032, step time: 0.1327\n",
      "64/223, train_loss: 0.1127, step time: 0.1176\n",
      "65/223, train_loss: 0.1035, step time: 0.0999\n",
      "66/223, train_loss: 0.0961, step time: 0.1203\n",
      "67/223, train_loss: 0.0979, step time: 0.1457\n",
      "68/223, train_loss: 0.0994, step time: 0.1201\n",
      "69/223, train_loss: 0.1002, step time: 0.1098\n",
      "70/223, train_loss: 0.1108, step time: 0.1142\n",
      "71/223, train_loss: 0.1000, step time: 0.1138\n",
      "72/223, train_loss: 0.0954, step time: 0.1103\n",
      "73/223, train_loss: 0.1015, step time: 0.1185\n",
      "74/223, train_loss: 0.1004, step time: 0.1067\n",
      "75/223, train_loss: 0.1043, step time: 0.1004\n",
      "76/223, train_loss: 0.1070, step time: 0.1197\n",
      "77/223, train_loss: 0.0948, step time: 0.0995\n",
      "78/223, train_loss: 0.1033, step time: 0.1039\n",
      "79/223, train_loss: 0.1015, step time: 0.1106\n",
      "80/223, train_loss: 0.1028, step time: 0.1112\n",
      "81/223, train_loss: 0.0928, step time: 0.1137\n",
      "82/223, train_loss: 0.1343, step time: 0.1224\n",
      "83/223, train_loss: 0.0949, step time: 0.1067\n",
      "84/223, train_loss: 0.1130, step time: 0.1147\n",
      "85/223, train_loss: 0.1007, step time: 0.1052\n",
      "86/223, train_loss: 0.1061, step time: 0.1087\n",
      "87/223, train_loss: 0.0980, step time: 0.1068\n",
      "88/223, train_loss: 0.1053, step time: 0.1285\n",
      "89/223, train_loss: 0.0911, step time: 0.1174\n",
      "90/223, train_loss: 0.1015, step time: 0.0987\n",
      "91/223, train_loss: 0.1001, step time: 0.1059\n",
      "92/223, train_loss: 0.0979, step time: 0.1021\n",
      "93/223, train_loss: 0.0939, step time: 0.1124\n",
      "94/223, train_loss: 0.1098, step time: 0.1169\n",
      "95/223, train_loss: 0.0976, step time: 0.0997\n",
      "96/223, train_loss: 0.0995, step time: 0.1003\n",
      "97/223, train_loss: 0.0983, step time: 0.1063\n",
      "98/223, train_loss: 0.0951, step time: 0.1044\n",
      "99/223, train_loss: 0.0883, step time: 0.1003\n",
      "100/223, train_loss: 0.1077, step time: 0.1040\n",
      "101/223, train_loss: 0.0988, step time: 0.1386\n",
      "102/223, train_loss: 0.1049, step time: 0.1017\n",
      "103/223, train_loss: 0.1021, step time: 0.0992\n",
      "104/223, train_loss: 0.1007, step time: 0.1018\n",
      "105/223, train_loss: 0.1159, step time: 0.1439\n",
      "106/223, train_loss: 0.1042, step time: 0.1105\n",
      "107/223, train_loss: 0.1040, step time: 0.1006\n",
      "108/223, train_loss: 0.1006, step time: 0.1000\n",
      "109/223, train_loss: 0.1072, step time: 0.1001\n",
      "110/223, train_loss: 0.0993, step time: 0.0997\n",
      "111/223, train_loss: 0.0996, step time: 0.1018\n",
      "112/223, train_loss: 0.1136, step time: 0.1010\n",
      "113/223, train_loss: 0.1058, step time: 0.1229\n",
      "114/223, train_loss: 0.0991, step time: 0.1090\n",
      "115/223, train_loss: 0.1091, step time: 0.1128\n",
      "116/223, train_loss: 0.1096, step time: 0.1060\n",
      "117/223, train_loss: 0.0949, step time: 0.1212\n",
      "118/223, train_loss: 0.0894, step time: 0.1009\n",
      "119/223, train_loss: 0.0954, step time: 0.1466\n",
      "120/223, train_loss: 0.0991, step time: 0.1003\n",
      "121/223, train_loss: 0.0915, step time: 0.1003\n",
      "122/223, train_loss: 0.1096, step time: 0.1060\n",
      "123/223, train_loss: 0.1094, step time: 0.1030\n",
      "124/223, train_loss: 0.1096, step time: 0.0999\n",
      "125/223, train_loss: 0.1048, step time: 0.1141\n",
      "126/223, train_loss: 0.0911, step time: 0.1313\n",
      "127/223, train_loss: 0.0937, step time: 0.1203\n",
      "128/223, train_loss: 0.1049, step time: 0.1114\n",
      "129/223, train_loss: 0.1043, step time: 0.1008\n",
      "130/223, train_loss: 0.0905, step time: 0.1021\n",
      "131/223, train_loss: 0.1021, step time: 0.1006\n",
      "132/223, train_loss: 0.1052, step time: 0.0997\n",
      "133/223, train_loss: 0.0937, step time: 0.1082\n",
      "134/223, train_loss: 0.0850, step time: 0.1028\n",
      "135/223, train_loss: 0.3066, step time: 0.1002\n",
      "136/223, train_loss: 0.1043, step time: 0.1086\n",
      "137/223, train_loss: 0.0977, step time: 0.1197\n",
      "138/223, train_loss: 0.1028, step time: 0.1048\n",
      "139/223, train_loss: 0.0987, step time: 0.1171\n",
      "140/223, train_loss: 0.1057, step time: 0.1214\n",
      "141/223, train_loss: 0.1092, step time: 0.1028\n",
      "142/223, train_loss: 0.1021, step time: 0.1180\n",
      "143/223, train_loss: 0.0939, step time: 0.1256\n",
      "144/223, train_loss: 0.0910, step time: 0.1000\n",
      "145/223, train_loss: 0.1007, step time: 0.1024\n",
      "146/223, train_loss: 0.1028, step time: 0.1000\n",
      "147/223, train_loss: 0.1050, step time: 0.1104\n",
      "148/223, train_loss: 0.0959, step time: 0.1184\n",
      "149/223, train_loss: 0.1192, step time: 0.0999\n",
      "150/223, train_loss: 0.0953, step time: 0.1122\n",
      "151/223, train_loss: 0.1079, step time: 0.1012\n",
      "152/223, train_loss: 0.1008, step time: 0.1117\n",
      "153/223, train_loss: 0.0992, step time: 0.1171\n",
      "154/223, train_loss: 0.0995, step time: 0.1064\n",
      "155/223, train_loss: 0.0984, step time: 0.1004\n",
      "156/223, train_loss: 0.1076, step time: 0.1118\n",
      "157/223, train_loss: 0.1000, step time: 0.1005\n",
      "158/223, train_loss: 0.0959, step time: 0.1084\n",
      "159/223, train_loss: 0.1005, step time: 0.1050\n",
      "160/223, train_loss: 0.0914, step time: 0.1188\n",
      "161/223, train_loss: 0.1010, step time: 0.1005\n",
      "162/223, train_loss: 0.1039, step time: 0.1001\n",
      "163/223, train_loss: 0.0960, step time: 0.1000\n",
      "164/223, train_loss: 0.1023, step time: 0.1015\n",
      "165/223, train_loss: 0.1070, step time: 0.1025\n",
      "166/223, train_loss: 0.0956, step time: 0.0996\n",
      "167/223, train_loss: 0.1093, step time: 0.1193\n",
      "168/223, train_loss: 0.1131, step time: 0.1164\n",
      "169/223, train_loss: 0.1009, step time: 0.1002\n",
      "170/223, train_loss: 0.0959, step time: 0.1029\n",
      "171/223, train_loss: 0.1266, step time: 0.1001\n",
      "172/223, train_loss: 0.0955, step time: 0.1011\n",
      "173/223, train_loss: 0.0990, step time: 0.1357\n",
      "174/223, train_loss: 0.1022, step time: 0.1108\n",
      "175/223, train_loss: 0.1085, step time: 0.1001\n",
      "176/223, train_loss: 0.1051, step time: 0.1001\n",
      "177/223, train_loss: 0.1081, step time: 0.1001\n",
      "178/223, train_loss: 0.0956, step time: 0.1170\n",
      "179/223, train_loss: 0.0979, step time: 0.1006\n",
      "180/223, train_loss: 0.1021, step time: 0.1001\n",
      "181/223, train_loss: 0.1000, step time: 0.1014\n",
      "182/223, train_loss: 0.0979, step time: 0.0994\n",
      "183/223, train_loss: 0.1056, step time: 0.1003\n",
      "184/223, train_loss: 0.1051, step time: 0.1007\n",
      "185/223, train_loss: 0.1026, step time: 0.1030\n",
      "186/223, train_loss: 0.1024, step time: 0.1079\n",
      "187/223, train_loss: 0.1102, step time: 0.1131\n",
      "188/223, train_loss: 0.1057, step time: 0.2332\n",
      "189/223, train_loss: 0.1051, step time: 0.1195\n",
      "190/223, train_loss: 0.1182, step time: 0.1002\n",
      "191/223, train_loss: 0.0935, step time: 0.1004\n",
      "192/223, train_loss: 0.0944, step time: 0.1008\n",
      "193/223, train_loss: 0.0946, step time: 0.1085\n",
      "194/223, train_loss: 0.1076, step time: 0.1112\n",
      "195/223, train_loss: 0.0988, step time: 0.1029\n",
      "196/223, train_loss: 0.1001, step time: 0.1096\n",
      "197/223, train_loss: 0.0985, step time: 0.1122\n",
      "198/223, train_loss: 0.0916, step time: 0.1127\n",
      "199/223, train_loss: 0.0998, step time: 0.1219\n",
      "200/223, train_loss: 0.1023, step time: 0.1369\n",
      "201/223, train_loss: 0.1059, step time: 0.1191\n",
      "202/223, train_loss: 0.0871, step time: 0.1022\n",
      "203/223, train_loss: 0.1143, step time: 0.1165\n",
      "204/223, train_loss: 0.0935, step time: 0.1104\n",
      "205/223, train_loss: 0.0976, step time: 0.1009\n",
      "206/223, train_loss: 0.1030, step time: 0.1133\n",
      "207/223, train_loss: 0.1060, step time: 0.1181\n",
      "208/223, train_loss: 0.0991, step time: 0.1127\n",
      "209/223, train_loss: 0.1041, step time: 0.1043\n",
      "210/223, train_loss: 0.0983, step time: 0.1187\n",
      "211/223, train_loss: 0.0947, step time: 0.1125\n",
      "212/223, train_loss: 0.1066, step time: 0.1288\n",
      "213/223, train_loss: 0.0973, step time: 0.1031\n",
      "214/223, train_loss: 0.1017, step time: 0.1111\n",
      "215/223, train_loss: 0.1050, step time: 0.1108\n",
      "216/223, train_loss: 0.1013, step time: 0.1268\n",
      "217/223, train_loss: 0.1163, step time: 0.1078\n",
      "218/223, train_loss: 0.1064, step time: 0.1014\n",
      "219/223, train_loss: 0.0997, step time: 0.1026\n",
      "220/223, train_loss: 0.0895, step time: 0.1062\n",
      "221/223, train_loss: 0.1125, step time: 0.0991\n",
      "222/223, train_loss: 0.0923, step time: 0.1000\n",
      "223/223, train_loss: 0.1048, step time: 0.0999\n",
      "epoch 199 average loss: 0.1027\n",
      "time consuming of epoch 199 is: 87.5419\n",
      "----------\n",
      "epoch 200/300\n",
      "1/223, train_loss: 0.1061, step time: 0.1158\n",
      "2/223, train_loss: 0.0965, step time: 0.1058\n",
      "3/223, train_loss: 0.0974, step time: 0.1035\n",
      "4/223, train_loss: 0.1036, step time: 0.1173\n",
      "5/223, train_loss: 0.0970, step time: 0.1135\n",
      "6/223, train_loss: 0.0993, step time: 0.1025\n",
      "7/223, train_loss: 0.0961, step time: 0.1326\n",
      "8/223, train_loss: 0.1031, step time: 0.1155\n",
      "9/223, train_loss: 0.1039, step time: 0.1004\n",
      "10/223, train_loss: 0.1015, step time: 0.1009\n",
      "11/223, train_loss: 0.0951, step time: 0.1007\n",
      "12/223, train_loss: 0.1100, step time: 0.1286\n",
      "13/223, train_loss: 0.0989, step time: 0.1063\n",
      "14/223, train_loss: 0.0974, step time: 0.1005\n",
      "15/223, train_loss: 0.1164, step time: 0.1009\n",
      "16/223, train_loss: 0.1063, step time: 0.1133\n",
      "17/223, train_loss: 0.0935, step time: 0.1109\n",
      "18/223, train_loss: 0.1023, step time: 0.1064\n",
      "19/223, train_loss: 0.1025, step time: 0.1020\n",
      "20/223, train_loss: 0.1140, step time: 0.1193\n",
      "21/223, train_loss: 0.1031, step time: 0.1153\n",
      "22/223, train_loss: 0.0994, step time: 0.1134\n",
      "23/223, train_loss: 0.1027, step time: 0.1016\n",
      "24/223, train_loss: 0.1117, step time: 0.1009\n",
      "25/223, train_loss: 0.0994, step time: 0.1210\n",
      "26/223, train_loss: 0.0965, step time: 0.1124\n",
      "27/223, train_loss: 0.1042, step time: 0.1193\n",
      "28/223, train_loss: 0.0955, step time: 0.1005\n",
      "29/223, train_loss: 0.1079, step time: 0.1038\n",
      "30/223, train_loss: 0.0903, step time: 0.1069\n",
      "31/223, train_loss: 0.1020, step time: 0.1015\n",
      "32/223, train_loss: 0.0895, step time: 0.1104\n",
      "33/223, train_loss: 0.0983, step time: 0.1001\n",
      "34/223, train_loss: 0.1004, step time: 0.1002\n",
      "35/223, train_loss: 0.0899, step time: 0.1008\n",
      "36/223, train_loss: 0.0996, step time: 0.1009\n",
      "37/223, train_loss: 0.0962, step time: 0.1167\n",
      "38/223, train_loss: 0.1065, step time: 0.1001\n",
      "39/223, train_loss: 0.1042, step time: 0.1081\n",
      "40/223, train_loss: 0.0944, step time: 0.1003\n",
      "41/223, train_loss: 0.0963, step time: 0.1550\n",
      "42/223, train_loss: 0.1098, step time: 0.1054\n",
      "43/223, train_loss: 0.0890, step time: 0.1018\n",
      "44/223, train_loss: 0.1033, step time: 0.1464\n",
      "45/223, train_loss: 0.3036, step time: 0.1002\n",
      "46/223, train_loss: 0.1133, step time: 0.1038\n",
      "47/223, train_loss: 0.1072, step time: 0.1164\n",
      "48/223, train_loss: 0.0937, step time: 0.1704\n",
      "49/223, train_loss: 0.1099, step time: 0.1113\n",
      "50/223, train_loss: 0.1048, step time: 0.1004\n",
      "51/223, train_loss: 0.0997, step time: 0.1009\n",
      "52/223, train_loss: 0.1001, step time: 0.1211\n",
      "53/223, train_loss: 0.1098, step time: 0.1060\n",
      "54/223, train_loss: 0.1191, step time: 0.1063\n",
      "55/223, train_loss: 0.1026, step time: 0.1139\n",
      "56/223, train_loss: 0.0898, step time: 0.1001\n",
      "57/223, train_loss: 0.1072, step time: 0.1195\n",
      "58/223, train_loss: 0.1004, step time: 0.1019\n",
      "59/223, train_loss: 0.0959, step time: 0.1188\n",
      "60/223, train_loss: 0.1032, step time: 0.1051\n",
      "61/223, train_loss: 0.0870, step time: 0.1168\n",
      "62/223, train_loss: 0.0944, step time: 0.1093\n",
      "63/223, train_loss: 0.1009, step time: 0.1004\n",
      "64/223, train_loss: 0.1051, step time: 0.1007\n",
      "65/223, train_loss: 0.1062, step time: 0.1029\n",
      "66/223, train_loss: 0.0957, step time: 0.1122\n",
      "67/223, train_loss: 0.0935, step time: 0.1041\n",
      "68/223, train_loss: 0.0996, step time: 0.1252\n",
      "69/223, train_loss: 0.1068, step time: 0.1130\n",
      "70/223, train_loss: 0.1024, step time: 0.1035\n",
      "71/223, train_loss: 0.0983, step time: 0.1003\n",
      "72/223, train_loss: 0.0982, step time: 0.1005\n",
      "73/223, train_loss: 0.1025, step time: 0.1151\n",
      "74/223, train_loss: 0.1125, step time: 0.1021\n",
      "75/223, train_loss: 0.0974, step time: 0.1109\n",
      "76/223, train_loss: 0.0985, step time: 0.1451\n",
      "77/223, train_loss: 0.1034, step time: 0.1091\n",
      "78/223, train_loss: 0.1077, step time: 0.1079\n",
      "79/223, train_loss: 0.1037, step time: 0.1039\n",
      "80/223, train_loss: 0.1053, step time: 0.1268\n",
      "81/223, train_loss: 0.1195, step time: 0.1281\n",
      "82/223, train_loss: 0.1056, step time: 0.1002\n",
      "83/223, train_loss: 0.1176, step time: 0.1003\n",
      "84/223, train_loss: 0.1021, step time: 0.0996\n",
      "85/223, train_loss: 0.1109, step time: 0.1001\n",
      "86/223, train_loss: 0.1131, step time: 0.1147\n",
      "87/223, train_loss: 0.0969, step time: 0.1193\n",
      "88/223, train_loss: 0.1109, step time: 0.1070\n",
      "89/223, train_loss: 0.0887, step time: 0.1045\n",
      "90/223, train_loss: 0.1021, step time: 0.1164\n",
      "91/223, train_loss: 0.1017, step time: 0.1096\n",
      "92/223, train_loss: 0.0976, step time: 0.1000\n",
      "93/223, train_loss: 0.0949, step time: 0.0985\n",
      "94/223, train_loss: 0.1125, step time: 0.1108\n",
      "95/223, train_loss: 0.0960, step time: 0.1064\n",
      "96/223, train_loss: 0.1068, step time: 0.1010\n",
      "97/223, train_loss: 0.1111, step time: 0.1251\n",
      "98/223, train_loss: 0.1015, step time: 0.1006\n",
      "99/223, train_loss: 0.0990, step time: 0.1001\n",
      "100/223, train_loss: 0.1063, step time: 0.1000\n",
      "101/223, train_loss: 0.1116, step time: 0.1093\n",
      "102/223, train_loss: 0.1058, step time: 0.1061\n",
      "103/223, train_loss: 0.1101, step time: 0.1003\n",
      "104/223, train_loss: 0.1098, step time: 0.1078\n",
      "105/223, train_loss: 0.0974, step time: 0.1035\n",
      "106/223, train_loss: 0.0976, step time: 0.1070\n",
      "107/223, train_loss: 0.0966, step time: 0.1005\n",
      "108/223, train_loss: 0.1008, step time: 0.1372\n",
      "109/223, train_loss: 0.0955, step time: 0.1055\n",
      "110/223, train_loss: 0.1106, step time: 0.1139\n",
      "111/223, train_loss: 0.0984, step time: 0.1010\n",
      "112/223, train_loss: 0.0960, step time: 0.1269\n",
      "113/223, train_loss: 0.0899, step time: 0.1083\n",
      "114/223, train_loss: 0.0937, step time: 0.1224\n",
      "115/223, train_loss: 0.0998, step time: 0.0999\n",
      "116/223, train_loss: 0.1099, step time: 0.1022\n",
      "117/223, train_loss: 0.1049, step time: 0.1227\n",
      "118/223, train_loss: 0.0990, step time: 0.0995\n",
      "119/223, train_loss: 0.1017, step time: 0.1244\n",
      "120/223, train_loss: 0.0960, step time: 0.1032\n",
      "121/223, train_loss: 0.0954, step time: 0.1140\n",
      "122/223, train_loss: 0.1019, step time: 0.1144\n",
      "123/223, train_loss: 0.1001, step time: 0.1222\n",
      "124/223, train_loss: 0.0921, step time: 0.1269\n",
      "125/223, train_loss: 0.1051, step time: 0.1066\n",
      "126/223, train_loss: 0.1036, step time: 0.1117\n",
      "127/223, train_loss: 0.0977, step time: 0.1001\n",
      "128/223, train_loss: 0.1037, step time: 0.1002\n",
      "129/223, train_loss: 0.0920, step time: 0.1248\n",
      "130/223, train_loss: 0.0935, step time: 0.1135\n",
      "131/223, train_loss: 0.0946, step time: 0.1011\n",
      "132/223, train_loss: 0.0992, step time: 0.1140\n",
      "133/223, train_loss: 0.0941, step time: 0.1174\n",
      "134/223, train_loss: 0.1067, step time: 0.1234\n",
      "135/223, train_loss: 0.0940, step time: 0.1097\n",
      "136/223, train_loss: 0.0988, step time: 0.1151\n",
      "137/223, train_loss: 0.0933, step time: 0.1244\n",
      "138/223, train_loss: 0.1155, step time: 0.1044\n",
      "139/223, train_loss: 0.1042, step time: 0.1089\n",
      "140/223, train_loss: 0.0997, step time: 0.1048\n",
      "141/223, train_loss: 0.1064, step time: 0.1047\n",
      "142/223, train_loss: 0.1131, step time: 0.1160\n",
      "143/223, train_loss: 0.1071, step time: 0.1093\n",
      "144/223, train_loss: 0.0985, step time: 0.1174\n",
      "145/223, train_loss: 0.0942, step time: 0.1023\n",
      "146/223, train_loss: 0.1082, step time: 0.1377\n",
      "147/223, train_loss: 0.0872, step time: 0.1116\n",
      "148/223, train_loss: 0.1015, step time: 0.1388\n",
      "149/223, train_loss: 0.1073, step time: 0.1014\n",
      "150/223, train_loss: 0.0966, step time: 0.1386\n",
      "151/223, train_loss: 0.1078, step time: 0.1046\n",
      "152/223, train_loss: 0.0991, step time: 0.1193\n",
      "153/223, train_loss: 0.1094, step time: 0.1044\n",
      "154/223, train_loss: 0.0940, step time: 0.1179\n",
      "155/223, train_loss: 0.0877, step time: 0.1167\n",
      "156/223, train_loss: 0.1106, step time: 0.1064\n",
      "157/223, train_loss: 0.1036, step time: 0.1172\n",
      "158/223, train_loss: 0.1147, step time: 0.0992\n",
      "159/223, train_loss: 0.1080, step time: 0.1093\n",
      "160/223, train_loss: 0.1054, step time: 0.1217\n",
      "161/223, train_loss: 0.1060, step time: 0.1154\n",
      "162/223, train_loss: 0.1089, step time: 0.1173\n",
      "163/223, train_loss: 0.0896, step time: 0.1072\n",
      "164/223, train_loss: 0.1023, step time: 0.1090\n",
      "165/223, train_loss: 0.1031, step time: 0.1070\n",
      "166/223, train_loss: 0.1000, step time: 0.1053\n",
      "167/223, train_loss: 0.1040, step time: 0.1009\n",
      "168/223, train_loss: 0.1035, step time: 0.1135\n",
      "169/223, train_loss: 0.1052, step time: 0.1344\n",
      "170/223, train_loss: 0.0961, step time: 0.1408\n",
      "171/223, train_loss: 0.1071, step time: 0.1002\n",
      "172/223, train_loss: 0.0945, step time: 0.1001\n",
      "173/223, train_loss: 0.1075, step time: 0.1219\n",
      "174/223, train_loss: 0.0963, step time: 0.1010\n",
      "175/223, train_loss: 0.1108, step time: 0.1008\n",
      "176/223, train_loss: 0.0967, step time: 0.1001\n",
      "177/223, train_loss: 0.1085, step time: 0.1094\n",
      "178/223, train_loss: 0.1026, step time: 0.1005\n",
      "179/223, train_loss: 0.0951, step time: 0.1030\n",
      "180/223, train_loss: 0.1017, step time: 0.1003\n",
      "181/223, train_loss: 0.1015, step time: 0.1133\n",
      "182/223, train_loss: 0.1054, step time: 0.1001\n",
      "183/223, train_loss: 0.1119, step time: 0.1123\n",
      "184/223, train_loss: 0.0977, step time: 0.1284\n",
      "185/223, train_loss: 0.0959, step time: 0.1006\n",
      "186/223, train_loss: 0.0997, step time: 0.0999\n",
      "187/223, train_loss: 0.0925, step time: 0.1000\n",
      "188/223, train_loss: 0.0995, step time: 0.0996\n",
      "189/223, train_loss: 0.1012, step time: 0.1332\n",
      "190/223, train_loss: 0.1006, step time: 0.1125\n",
      "191/223, train_loss: 0.1022, step time: 0.1022\n",
      "192/223, train_loss: 0.1039, step time: 0.1091\n",
      "193/223, train_loss: 0.0980, step time: 0.1004\n",
      "194/223, train_loss: 0.0950, step time: 0.1149\n",
      "195/223, train_loss: 0.1112, step time: 0.1111\n",
      "196/223, train_loss: 0.0948, step time: 0.1224\n",
      "197/223, train_loss: 0.0887, step time: 0.1066\n",
      "198/223, train_loss: 0.1053, step time: 0.1078\n",
      "199/223, train_loss: 0.0949, step time: 0.1000\n",
      "200/223, train_loss: 0.1081, step time: 0.1175\n",
      "201/223, train_loss: 0.0939, step time: 0.1110\n",
      "202/223, train_loss: 0.0970, step time: 0.1020\n",
      "203/223, train_loss: 0.1109, step time: 0.1146\n",
      "204/223, train_loss: 0.0898, step time: 0.1032\n",
      "205/223, train_loss: 0.0992, step time: 0.1012\n",
      "206/223, train_loss: 0.1005, step time: 0.1030\n",
      "207/223, train_loss: 0.0936, step time: 0.1110\n",
      "208/223, train_loss: 0.1061, step time: 0.1123\n",
      "209/223, train_loss: 0.0964, step time: 0.1107\n",
      "210/223, train_loss: 0.1064, step time: 0.1047\n",
      "211/223, train_loss: 0.0981, step time: 0.1001\n",
      "212/223, train_loss: 0.1029, step time: 0.1093\n",
      "213/223, train_loss: 0.0901, step time: 0.1216\n",
      "214/223, train_loss: 0.0942, step time: 0.1227\n",
      "215/223, train_loss: 0.1057, step time: 0.1047\n",
      "216/223, train_loss: 0.1019, step time: 0.1115\n",
      "217/223, train_loss: 0.1025, step time: 0.1146\n",
      "218/223, train_loss: 0.1107, step time: 0.1006\n",
      "219/223, train_loss: 0.0967, step time: 0.1028\n",
      "220/223, train_loss: 0.1164, step time: 0.0999\n",
      "221/223, train_loss: 0.1087, step time: 0.1021\n",
      "222/223, train_loss: 0.1154, step time: 0.1004\n",
      "223/223, train_loss: 0.1113, step time: 0.1004\n",
      "epoch 200 average loss: 0.1026\n",
      "saved new best metric model\n",
      "current epoch: 200 current mean dice: 0.8601 tc: 0.9215 wt: 0.8696 et: 0.7891\n",
      "best mean dice: 0.8601 at epoch: 200\n",
      "time consuming of epoch 200 is: 92.2894\n",
      "----------\n",
      "epoch 201/300\n",
      "1/223, train_loss: 0.1033, step time: 0.1015\n",
      "2/223, train_loss: 0.1005, step time: 0.1076\n",
      "3/223, train_loss: 0.1152, step time: 0.1002\n",
      "4/223, train_loss: 0.1155, step time: 0.1165\n",
      "5/223, train_loss: 0.1022, step time: 0.1156\n",
      "6/223, train_loss: 0.1069, step time: 0.1061\n",
      "7/223, train_loss: 0.1050, step time: 0.1110\n",
      "8/223, train_loss: 0.0948, step time: 0.0995\n",
      "9/223, train_loss: 0.1187, step time: 0.1186\n",
      "10/223, train_loss: 0.0953, step time: 0.1126\n",
      "11/223, train_loss: 0.1036, step time: 0.1293\n",
      "12/223, train_loss: 0.0948, step time: 0.1009\n",
      "13/223, train_loss: 0.1063, step time: 0.1151\n",
      "14/223, train_loss: 0.1047, step time: 0.1252\n",
      "15/223, train_loss: 0.1132, step time: 0.1181\n",
      "16/223, train_loss: 0.1088, step time: 0.0999\n",
      "17/223, train_loss: 0.1002, step time: 0.1235\n",
      "18/223, train_loss: 0.0959, step time: 0.1151\n",
      "19/223, train_loss: 0.1020, step time: 0.1351\n",
      "20/223, train_loss: 0.1102, step time: 0.1006\n",
      "21/223, train_loss: 0.1152, step time: 0.1065\n",
      "22/223, train_loss: 0.0974, step time: 0.1266\n",
      "23/223, train_loss: 0.1063, step time: 0.1074\n",
      "24/223, train_loss: 0.1019, step time: 0.1009\n",
      "25/223, train_loss: 0.1085, step time: 0.1104\n",
      "26/223, train_loss: 0.1128, step time: 0.1018\n",
      "27/223, train_loss: 0.1121, step time: 0.1000\n",
      "28/223, train_loss: 0.0956, step time: 0.1036\n",
      "29/223, train_loss: 0.1014, step time: 0.1034\n",
      "30/223, train_loss: 0.0935, step time: 0.1051\n",
      "31/223, train_loss: 0.0980, step time: 0.1052\n",
      "32/223, train_loss: 0.0997, step time: 0.1026\n",
      "33/223, train_loss: 0.0962, step time: 0.1079\n",
      "34/223, train_loss: 0.0842, step time: 0.1259\n",
      "35/223, train_loss: 0.1035, step time: 0.1016\n",
      "36/223, train_loss: 0.0971, step time: 0.1007\n",
      "37/223, train_loss: 0.1087, step time: 0.1011\n",
      "38/223, train_loss: 0.1047, step time: 0.1199\n",
      "39/223, train_loss: 0.1010, step time: 0.1111\n",
      "40/223, train_loss: 0.1062, step time: 0.1018\n",
      "41/223, train_loss: 0.1045, step time: 0.1119\n",
      "42/223, train_loss: 0.1033, step time: 0.1002\n",
      "43/223, train_loss: 0.1120, step time: 0.1010\n",
      "44/223, train_loss: 0.0950, step time: 0.1006\n",
      "45/223, train_loss: 0.1003, step time: 0.1153\n",
      "46/223, train_loss: 0.1017, step time: 0.1120\n",
      "47/223, train_loss: 0.0927, step time: 0.1143\n",
      "48/223, train_loss: 0.1048, step time: 0.1094\n",
      "49/223, train_loss: 0.1011, step time: 0.1203\n",
      "50/223, train_loss: 0.0935, step time: 0.1010\n",
      "51/223, train_loss: 0.1036, step time: 0.1012\n",
      "52/223, train_loss: 0.1151, step time: 0.1009\n",
      "53/223, train_loss: 0.1005, step time: 0.1031\n",
      "54/223, train_loss: 0.1036, step time: 0.0998\n",
      "55/223, train_loss: 0.0998, step time: 0.1007\n",
      "56/223, train_loss: 0.0892, step time: 0.1013\n",
      "57/223, train_loss: 0.1007, step time: 0.1173\n",
      "58/223, train_loss: 0.1063, step time: 0.1012\n",
      "59/223, train_loss: 0.1017, step time: 0.1021\n",
      "60/223, train_loss: 0.0940, step time: 0.1043\n",
      "61/223, train_loss: 0.1052, step time: 0.1129\n",
      "62/223, train_loss: 0.1025, step time: 0.1659\n",
      "63/223, train_loss: 0.0985, step time: 0.1264\n",
      "64/223, train_loss: 0.0987, step time: 0.1006\n",
      "65/223, train_loss: 0.1183, step time: 0.1058\n",
      "66/223, train_loss: 0.1042, step time: 0.1007\n",
      "67/223, train_loss: 0.1072, step time: 0.1002\n",
      "68/223, train_loss: 0.1177, step time: 0.1150\n",
      "69/223, train_loss: 0.0981, step time: 0.1059\n",
      "70/223, train_loss: 0.1028, step time: 0.1398\n",
      "71/223, train_loss: 0.1037, step time: 0.1141\n",
      "72/223, train_loss: 0.0971, step time: 0.1167\n",
      "73/223, train_loss: 0.0994, step time: 0.1046\n",
      "74/223, train_loss: 0.0989, step time: 0.1012\n",
      "75/223, train_loss: 0.1087, step time: 0.1107\n",
      "76/223, train_loss: 0.0963, step time: 0.1085\n",
      "77/223, train_loss: 0.1028, step time: 0.1034\n",
      "78/223, train_loss: 0.1053, step time: 0.1109\n",
      "79/223, train_loss: 0.1005, step time: 0.1169\n",
      "80/223, train_loss: 0.0981, step time: 0.1151\n",
      "81/223, train_loss: 0.1065, step time: 0.0997\n",
      "82/223, train_loss: 0.0950, step time: 0.1029\n",
      "83/223, train_loss: 0.1154, step time: 0.1116\n",
      "84/223, train_loss: 0.1012, step time: 0.1155\n",
      "85/223, train_loss: 0.1021, step time: 0.1395\n",
      "86/223, train_loss: 0.1048, step time: 0.1022\n",
      "87/223, train_loss: 0.0972, step time: 0.1328\n",
      "88/223, train_loss: 0.1006, step time: 0.1003\n",
      "89/223, train_loss: 0.1072, step time: 0.1032\n",
      "90/223, train_loss: 0.0890, step time: 0.1014\n",
      "91/223, train_loss: 0.1085, step time: 0.1005\n",
      "92/223, train_loss: 0.1067, step time: 0.1200\n",
      "93/223, train_loss: 0.1000, step time: 0.1057\n",
      "94/223, train_loss: 0.0925, step time: 0.1396\n",
      "95/223, train_loss: 0.0955, step time: 0.1137\n",
      "96/223, train_loss: 0.0986, step time: 0.1007\n",
      "97/223, train_loss: 0.1001, step time: 0.1241\n",
      "98/223, train_loss: 0.1031, step time: 0.1093\n",
      "99/223, train_loss: 0.0994, step time: 0.1368\n",
      "100/223, train_loss: 0.1045, step time: 0.1075\n",
      "101/223, train_loss: 0.0974, step time: 0.1203\n",
      "102/223, train_loss: 0.1040, step time: 0.1092\n",
      "103/223, train_loss: 0.1087, step time: 0.1018\n",
      "104/223, train_loss: 0.1100, step time: 0.1020\n",
      "105/223, train_loss: 0.0923, step time: 0.1146\n",
      "106/223, train_loss: 0.1077, step time: 0.1138\n",
      "107/223, train_loss: 0.0996, step time: 0.1274\n",
      "108/223, train_loss: 0.0952, step time: 0.1172\n",
      "109/223, train_loss: 0.1037, step time: 0.1136\n",
      "110/223, train_loss: 0.1098, step time: 0.1122\n",
      "111/223, train_loss: 0.1071, step time: 0.1142\n",
      "112/223, train_loss: 0.1022, step time: 0.1106\n",
      "113/223, train_loss: 0.1094, step time: 0.1195\n",
      "114/223, train_loss: 0.1122, step time: 0.1096\n",
      "115/223, train_loss: 0.0931, step time: 0.1108\n",
      "116/223, train_loss: 0.0913, step time: 0.1120\n",
      "117/223, train_loss: 0.0926, step time: 0.1006\n",
      "118/223, train_loss: 0.0998, step time: 0.1373\n",
      "119/223, train_loss: 0.1184, step time: 0.1034\n",
      "120/223, train_loss: 0.1041, step time: 0.1007\n",
      "121/223, train_loss: 0.0975, step time: 0.1058\n",
      "122/223, train_loss: 0.1039, step time: 0.1003\n",
      "123/223, train_loss: 0.0998, step time: 0.1198\n",
      "124/223, train_loss: 0.0935, step time: 0.1206\n",
      "125/223, train_loss: 0.0918, step time: 0.1122\n",
      "126/223, train_loss: 0.0932, step time: 0.1029\n",
      "127/223, train_loss: 0.0947, step time: 0.1097\n",
      "128/223, train_loss: 0.0922, step time: 0.1025\n",
      "129/223, train_loss: 0.0896, step time: 0.1090\n",
      "130/223, train_loss: 0.0978, step time: 0.1062\n",
      "131/223, train_loss: 0.0950, step time: 0.1173\n",
      "132/223, train_loss: 0.0921, step time: 0.1316\n",
      "133/223, train_loss: 0.1096, step time: 0.1068\n",
      "134/223, train_loss: 0.1152, step time: 0.1179\n",
      "135/223, train_loss: 0.0879, step time: 0.1245\n",
      "136/223, train_loss: 0.1076, step time: 0.1099\n",
      "137/223, train_loss: 0.0960, step time: 0.1054\n",
      "138/223, train_loss: 0.0962, step time: 0.1382\n",
      "139/223, train_loss: 0.0958, step time: 0.1321\n",
      "140/223, train_loss: 0.0993, step time: 0.1062\n",
      "141/223, train_loss: 0.1023, step time: 0.1110\n",
      "142/223, train_loss: 0.0987, step time: 0.1100\n",
      "143/223, train_loss: 0.0998, step time: 0.1261\n",
      "144/223, train_loss: 0.1044, step time: 0.1120\n",
      "145/223, train_loss: 0.1118, step time: 0.1403\n",
      "146/223, train_loss: 0.0924, step time: 0.1162\n",
      "147/223, train_loss: 0.1007, step time: 0.1226\n",
      "148/223, train_loss: 0.0991, step time: 0.1007\n",
      "149/223, train_loss: 0.1022, step time: 0.1038\n",
      "150/223, train_loss: 0.0998, step time: 0.1252\n",
      "151/223, train_loss: 0.0999, step time: 0.1164\n",
      "152/223, train_loss: 0.1142, step time: 0.1136\n",
      "153/223, train_loss: 0.0927, step time: 0.1234\n",
      "154/223, train_loss: 0.0926, step time: 0.1293\n",
      "155/223, train_loss: 0.1037, step time: 0.1001\n",
      "156/223, train_loss: 0.1006, step time: 0.1071\n",
      "157/223, train_loss: 0.0965, step time: 0.1484\n",
      "158/223, train_loss: 0.0950, step time: 0.1027\n",
      "159/223, train_loss: 0.1024, step time: 0.1104\n",
      "160/223, train_loss: 0.0945, step time: 0.1098\n",
      "161/223, train_loss: 0.0960, step time: 0.1016\n",
      "162/223, train_loss: 0.1012, step time: 0.1403\n",
      "163/223, train_loss: 0.1013, step time: 0.1067\n",
      "164/223, train_loss: 0.1133, step time: 0.1067\n",
      "165/223, train_loss: 0.1003, step time: 0.1030\n",
      "166/223, train_loss: 0.1091, step time: 0.1141\n",
      "167/223, train_loss: 0.0995, step time: 0.1233\n",
      "168/223, train_loss: 0.1035, step time: 0.0998\n",
      "169/223, train_loss: 0.1024, step time: 0.0999\n",
      "170/223, train_loss: 0.1007, step time: 0.1005\n",
      "171/223, train_loss: 0.1204, step time: 0.1003\n",
      "172/223, train_loss: 0.0986, step time: 0.1005\n",
      "173/223, train_loss: 0.0974, step time: 0.1164\n",
      "174/223, train_loss: 0.1082, step time: 0.1130\n",
      "175/223, train_loss: 0.0942, step time: 0.1302\n",
      "176/223, train_loss: 0.1117, step time: 0.1111\n",
      "177/223, train_loss: 0.0976, step time: 0.0996\n",
      "178/223, train_loss: 0.1020, step time: 0.1153\n",
      "179/223, train_loss: 0.1105, step time: 0.1010\n",
      "180/223, train_loss: 0.1077, step time: 0.1101\n",
      "181/223, train_loss: 0.0973, step time: 0.1202\n",
      "182/223, train_loss: 0.0968, step time: 0.1114\n",
      "183/223, train_loss: 0.0965, step time: 0.1050\n",
      "184/223, train_loss: 0.0980, step time: 0.1224\n",
      "185/223, train_loss: 0.0928, step time: 0.1119\n",
      "186/223, train_loss: 0.1057, step time: 0.1106\n",
      "187/223, train_loss: 0.1231, step time: 0.1000\n",
      "188/223, train_loss: 0.0955, step time: 0.1087\n",
      "189/223, train_loss: 0.1131, step time: 0.1149\n",
      "190/223, train_loss: 0.1018, step time: 0.1027\n",
      "191/223, train_loss: 0.1005, step time: 0.1188\n",
      "192/223, train_loss: 0.1044, step time: 0.1016\n",
      "193/223, train_loss: 0.0934, step time: 0.1088\n",
      "194/223, train_loss: 0.0913, step time: 0.0998\n",
      "195/223, train_loss: 0.1007, step time: 0.1002\n",
      "196/223, train_loss: 0.0964, step time: 0.1010\n",
      "197/223, train_loss: 0.1088, step time: 0.1004\n",
      "198/223, train_loss: 0.1017, step time: 0.1004\n",
      "199/223, train_loss: 0.0921, step time: 0.1809\n",
      "200/223, train_loss: 0.1012, step time: 0.1046\n",
      "201/223, train_loss: 0.1012, step time: 0.1047\n",
      "202/223, train_loss: 0.0956, step time: 0.1154\n",
      "203/223, train_loss: 0.0925, step time: 0.1135\n",
      "204/223, train_loss: 0.0966, step time: 0.1016\n",
      "205/223, train_loss: 0.1079, step time: 0.1066\n",
      "206/223, train_loss: 0.1047, step time: 0.1172\n",
      "207/223, train_loss: 0.1092, step time: 0.1002\n",
      "208/223, train_loss: 0.0965, step time: 0.1090\n",
      "209/223, train_loss: 0.1059, step time: 0.1218\n",
      "210/223, train_loss: 0.1082, step time: 0.1081\n",
      "211/223, train_loss: 0.0908, step time: 0.1030\n",
      "212/223, train_loss: 0.0995, step time: 0.1049\n",
      "213/223, train_loss: 0.0986, step time: 0.1076\n",
      "214/223, train_loss: 0.1028, step time: 0.1227\n",
      "215/223, train_loss: 0.2979, step time: 0.1172\n",
      "216/223, train_loss: 0.0951, step time: 0.0996\n",
      "217/223, train_loss: 0.1049, step time: 0.1043\n",
      "218/223, train_loss: 0.0971, step time: 0.1160\n",
      "219/223, train_loss: 0.1108, step time: 0.1293\n",
      "220/223, train_loss: 0.1078, step time: 0.0995\n",
      "221/223, train_loss: 0.0951, step time: 0.0990\n",
      "222/223, train_loss: 0.1002, step time: 0.0997\n",
      "223/223, train_loss: 0.1015, step time: 0.0997\n",
      "epoch 201 average loss: 0.1026\n",
      "time consuming of epoch 201 is: 90.0084\n",
      "----------\n",
      "epoch 202/300\n",
      "1/223, train_loss: 0.1010, step time: 0.1099\n",
      "2/223, train_loss: 0.1015, step time: 0.1230\n",
      "3/223, train_loss: 0.1122, step time: 0.1249\n",
      "4/223, train_loss: 0.1020, step time: 0.1099\n",
      "5/223, train_loss: 0.1075, step time: 0.1131\n",
      "6/223, train_loss: 0.1025, step time: 0.1056\n",
      "7/223, train_loss: 0.1026, step time: 0.1130\n",
      "8/223, train_loss: 0.0955, step time: 0.1394\n",
      "9/223, train_loss: 0.1023, step time: 0.1172\n",
      "10/223, train_loss: 0.1059, step time: 0.1017\n",
      "11/223, train_loss: 0.1086, step time: 0.1007\n",
      "12/223, train_loss: 0.0897, step time: 0.1175\n",
      "13/223, train_loss: 0.1030, step time: 0.1076\n",
      "14/223, train_loss: 0.1094, step time: 0.1089\n",
      "15/223, train_loss: 0.0989, step time: 0.1064\n",
      "16/223, train_loss: 0.1032, step time: 0.1008\n",
      "17/223, train_loss: 0.1123, step time: 0.1346\n",
      "18/223, train_loss: 0.0928, step time: 0.1418\n",
      "19/223, train_loss: 0.0970, step time: 0.1062\n",
      "20/223, train_loss: 0.1054, step time: 0.1217\n",
      "21/223, train_loss: 0.0929, step time: 0.1159\n",
      "22/223, train_loss: 0.0996, step time: 0.1005\n",
      "23/223, train_loss: 0.0956, step time: 0.1042\n",
      "24/223, train_loss: 0.1024, step time: 0.1230\n",
      "25/223, train_loss: 0.1071, step time: 0.1372\n",
      "26/223, train_loss: 0.1062, step time: 0.1424\n",
      "27/223, train_loss: 0.1006, step time: 0.1211\n",
      "28/223, train_loss: 0.1105, step time: 0.1020\n",
      "29/223, train_loss: 0.1083, step time: 0.1002\n",
      "30/223, train_loss: 0.1001, step time: 0.1220\n",
      "31/223, train_loss: 0.0978, step time: 0.1139\n",
      "32/223, train_loss: 0.0907, step time: 0.1344\n",
      "33/223, train_loss: 0.1022, step time: 0.1146\n",
      "34/223, train_loss: 0.1021, step time: 0.1034\n",
      "35/223, train_loss: 0.0930, step time: 0.1019\n",
      "36/223, train_loss: 0.0980, step time: 0.1008\n",
      "37/223, train_loss: 0.1027, step time: 0.1006\n",
      "38/223, train_loss: 0.1027, step time: 0.1139\n",
      "39/223, train_loss: 0.1008, step time: 0.1027\n",
      "40/223, train_loss: 0.1058, step time: 0.1096\n",
      "41/223, train_loss: 0.0993, step time: 0.1064\n",
      "42/223, train_loss: 0.0995, step time: 0.1086\n",
      "43/223, train_loss: 0.0903, step time: 0.1123\n",
      "44/223, train_loss: 0.1079, step time: 0.1032\n",
      "45/223, train_loss: 0.1099, step time: 0.1085\n",
      "46/223, train_loss: 0.0940, step time: 0.1088\n",
      "47/223, train_loss: 0.0971, step time: 0.1244\n",
      "48/223, train_loss: 0.1072, step time: 0.1034\n",
      "49/223, train_loss: 0.1000, step time: 0.1061\n",
      "50/223, train_loss: 0.0984, step time: 0.1125\n",
      "51/223, train_loss: 0.1020, step time: 0.1044\n",
      "52/223, train_loss: 0.1139, step time: 0.1099\n",
      "53/223, train_loss: 0.1012, step time: 0.1102\n",
      "54/223, train_loss: 0.0968, step time: 0.1122\n",
      "55/223, train_loss: 0.0969, step time: 0.1557\n",
      "56/223, train_loss: 0.0931, step time: 0.1060\n",
      "57/223, train_loss: 0.1019, step time: 0.1185\n",
      "58/223, train_loss: 0.0913, step time: 0.1124\n",
      "59/223, train_loss: 0.0930, step time: 0.1397\n",
      "60/223, train_loss: 0.1099, step time: 0.1062\n",
      "61/223, train_loss: 0.0933, step time: 0.1150\n",
      "62/223, train_loss: 0.1033, step time: 0.1090\n",
      "63/223, train_loss: 0.3046, step time: 0.1293\n",
      "64/223, train_loss: 0.1070, step time: 0.0996\n",
      "65/223, train_loss: 0.0928, step time: 0.1059\n",
      "66/223, train_loss: 0.0927, step time: 0.1145\n",
      "67/223, train_loss: 0.0966, step time: 0.1181\n",
      "68/223, train_loss: 0.0912, step time: 0.1068\n",
      "69/223, train_loss: 0.1027, step time: 0.1094\n",
      "70/223, train_loss: 0.1064, step time: 0.1072\n",
      "71/223, train_loss: 0.1076, step time: 0.1239\n",
      "72/223, train_loss: 0.1047, step time: 0.1062\n",
      "73/223, train_loss: 0.0948, step time: 0.1229\n",
      "74/223, train_loss: 0.1020, step time: 0.1083\n",
      "75/223, train_loss: 0.0993, step time: 0.1192\n",
      "76/223, train_loss: 0.1148, step time: 0.1031\n",
      "77/223, train_loss: 0.1096, step time: 0.1016\n",
      "78/223, train_loss: 0.0882, step time: 0.1001\n",
      "79/223, train_loss: 0.1055, step time: 0.1122\n",
      "80/223, train_loss: 0.1124, step time: 0.1017\n",
      "81/223, train_loss: 0.0954, step time: 0.1003\n",
      "82/223, train_loss: 0.1162, step time: 0.1002\n",
      "83/223, train_loss: 0.1017, step time: 0.0996\n",
      "84/223, train_loss: 0.1083, step time: 0.1212\n",
      "85/223, train_loss: 0.1178, step time: 0.1088\n",
      "86/223, train_loss: 0.1046, step time: 0.1002\n",
      "87/223, train_loss: 0.0952, step time: 0.0984\n",
      "88/223, train_loss: 0.0929, step time: 0.0993\n",
      "89/223, train_loss: 0.1022, step time: 0.1172\n",
      "90/223, train_loss: 0.1033, step time: 0.1171\n",
      "91/223, train_loss: 0.1172, step time: 0.1273\n",
      "92/223, train_loss: 0.0913, step time: 0.1033\n",
      "93/223, train_loss: 0.1069, step time: 0.1302\n",
      "94/223, train_loss: 0.1093, step time: 0.1431\n",
      "95/223, train_loss: 0.1051, step time: 0.1020\n",
      "96/223, train_loss: 0.0989, step time: 0.1011\n",
      "97/223, train_loss: 0.1028, step time: 0.1135\n",
      "98/223, train_loss: 0.1045, step time: 0.1258\n",
      "99/223, train_loss: 0.1072, step time: 0.1083\n",
      "100/223, train_loss: 0.0976, step time: 0.1006\n",
      "101/223, train_loss: 0.1026, step time: 0.1205\n",
      "102/223, train_loss: 0.1058, step time: 0.1198\n",
      "103/223, train_loss: 0.1008, step time: 0.1081\n",
      "104/223, train_loss: 0.1013, step time: 0.1055\n",
      "105/223, train_loss: 0.1009, step time: 0.1101\n",
      "106/223, train_loss: 0.0989, step time: 0.1004\n",
      "107/223, train_loss: 0.1030, step time: 0.1001\n",
      "108/223, train_loss: 0.0947, step time: 0.1165\n",
      "109/223, train_loss: 0.1038, step time: 0.1084\n",
      "110/223, train_loss: 0.1037, step time: 0.1223\n",
      "111/223, train_loss: 0.1002, step time: 0.1098\n",
      "112/223, train_loss: 0.1073, step time: 0.1231\n",
      "113/223, train_loss: 0.1085, step time: 0.1102\n",
      "114/223, train_loss: 0.0985, step time: 0.1068\n",
      "115/223, train_loss: 0.1006, step time: 0.1035\n",
      "116/223, train_loss: 0.1242, step time: 0.1169\n",
      "117/223, train_loss: 0.1024, step time: 0.1161\n",
      "118/223, train_loss: 0.0949, step time: 0.1086\n",
      "119/223, train_loss: 0.0966, step time: 0.1080\n",
      "120/223, train_loss: 0.1089, step time: 0.1009\n",
      "121/223, train_loss: 0.0993, step time: 0.1112\n",
      "122/223, train_loss: 0.1006, step time: 0.1443\n",
      "123/223, train_loss: 0.0987, step time: 0.1003\n",
      "124/223, train_loss: 0.1072, step time: 0.1009\n",
      "125/223, train_loss: 0.1037, step time: 0.1106\n",
      "126/223, train_loss: 0.1183, step time: 0.1051\n",
      "127/223, train_loss: 0.0991, step time: 0.1268\n",
      "128/223, train_loss: 0.0941, step time: 0.1126\n",
      "129/223, train_loss: 0.0986, step time: 0.1009\n",
      "130/223, train_loss: 0.0937, step time: 0.1190\n",
      "131/223, train_loss: 0.0933, step time: 0.1180\n",
      "132/223, train_loss: 0.0945, step time: 0.1005\n",
      "133/223, train_loss: 0.1217, step time: 0.1063\n",
      "134/223, train_loss: 0.1133, step time: 0.1007\n",
      "135/223, train_loss: 0.0929, step time: 0.1004\n",
      "136/223, train_loss: 0.1040, step time: 0.1207\n",
      "137/223, train_loss: 0.1003, step time: 0.1007\n",
      "138/223, train_loss: 0.0943, step time: 0.1042\n",
      "139/223, train_loss: 0.1010, step time: 0.1314\n",
      "140/223, train_loss: 0.1078, step time: 0.1127\n",
      "141/223, train_loss: 0.0905, step time: 0.1273\n",
      "142/223, train_loss: 0.1078, step time: 0.1003\n",
      "143/223, train_loss: 0.0962, step time: 0.1328\n",
      "144/223, train_loss: 0.0986, step time: 0.1200\n",
      "145/223, train_loss: 0.1008, step time: 0.1181\n",
      "146/223, train_loss: 0.1066, step time: 0.1412\n",
      "147/223, train_loss: 0.1009, step time: 0.0986\n",
      "148/223, train_loss: 0.0994, step time: 0.1255\n",
      "149/223, train_loss: 0.1041, step time: 0.1041\n",
      "150/223, train_loss: 0.0848, step time: 0.0995\n",
      "151/223, train_loss: 0.1069, step time: 0.1011\n",
      "152/223, train_loss: 0.0973, step time: 0.1015\n",
      "153/223, train_loss: 0.0964, step time: 0.1069\n",
      "154/223, train_loss: 0.1075, step time: 0.1031\n",
      "155/223, train_loss: 0.0904, step time: 0.1252\n",
      "156/223, train_loss: 0.1050, step time: 0.1004\n",
      "157/223, train_loss: 0.1075, step time: 0.1140\n",
      "158/223, train_loss: 0.1057, step time: 0.1097\n",
      "159/223, train_loss: 0.1108, step time: 0.1459\n",
      "160/223, train_loss: 0.0961, step time: 0.1623\n",
      "161/223, train_loss: 0.0897, step time: 0.1166\n",
      "162/223, train_loss: 0.1021, step time: 0.1307\n",
      "163/223, train_loss: 0.0993, step time: 0.1029\n",
      "164/223, train_loss: 0.1055, step time: 0.1008\n",
      "165/223, train_loss: 0.0961, step time: 0.1038\n",
      "166/223, train_loss: 0.1021, step time: 0.1051\n",
      "167/223, train_loss: 0.0952, step time: 0.1364\n",
      "168/223, train_loss: 0.0999, step time: 0.1064\n",
      "169/223, train_loss: 0.1124, step time: 0.1084\n",
      "170/223, train_loss: 0.1065, step time: 0.0992\n",
      "171/223, train_loss: 0.0938, step time: 0.0991\n",
      "172/223, train_loss: 0.0956, step time: 0.0989\n",
      "173/223, train_loss: 0.1115, step time: 0.1085\n",
      "174/223, train_loss: 0.1220, step time: 0.1143\n",
      "175/223, train_loss: 0.1142, step time: 0.0992\n",
      "176/223, train_loss: 0.1087, step time: 0.0984\n",
      "177/223, train_loss: 0.0970, step time: 0.1154\n",
      "178/223, train_loss: 0.0979, step time: 0.1251\n",
      "179/223, train_loss: 0.1069, step time: 0.1089\n",
      "180/223, train_loss: 0.0961, step time: 0.1005\n",
      "181/223, train_loss: 0.0995, step time: 0.1080\n",
      "182/223, train_loss: 0.1041, step time: 0.1196\n",
      "183/223, train_loss: 0.1074, step time: 0.1044\n",
      "184/223, train_loss: 0.1026, step time: 0.1047\n",
      "185/223, train_loss: 0.1082, step time: 0.1064\n",
      "186/223, train_loss: 0.0998, step time: 0.1017\n",
      "187/223, train_loss: 0.0905, step time: 0.1042\n",
      "188/223, train_loss: 0.1048, step time: 0.1095\n",
      "189/223, train_loss: 0.1026, step time: 0.1059\n",
      "190/223, train_loss: 0.1050, step time: 0.1205\n",
      "191/223, train_loss: 0.0970, step time: 0.1172\n",
      "192/223, train_loss: 0.1031, step time: 0.1005\n",
      "193/223, train_loss: 0.1060, step time: 0.1131\n",
      "194/223, train_loss: 0.0986, step time: 0.1399\n",
      "195/223, train_loss: 0.0918, step time: 0.1274\n",
      "196/223, train_loss: 0.1027, step time: 0.1135\n",
      "197/223, train_loss: 0.1003, step time: 0.1115\n",
      "198/223, train_loss: 0.0980, step time: 0.1073\n",
      "199/223, train_loss: 0.0935, step time: 0.1303\n",
      "200/223, train_loss: 0.1032, step time: 0.0998\n",
      "201/223, train_loss: 0.1063, step time: 0.0998\n",
      "202/223, train_loss: 0.1012, step time: 0.1021\n",
      "203/223, train_loss: 0.0974, step time: 0.1010\n",
      "204/223, train_loss: 0.0912, step time: 0.1010\n",
      "205/223, train_loss: 0.0974, step time: 0.0995\n",
      "206/223, train_loss: 0.1034, step time: 0.1002\n",
      "207/223, train_loss: 0.1058, step time: 0.1095\n",
      "208/223, train_loss: 0.1147, step time: 0.1206\n",
      "209/223, train_loss: 0.0920, step time: 0.1122\n",
      "210/223, train_loss: 0.0978, step time: 0.1748\n",
      "211/223, train_loss: 0.0876, step time: 0.1226\n",
      "212/223, train_loss: 0.1103, step time: 0.1103\n",
      "213/223, train_loss: 0.1078, step time: 0.1003\n",
      "214/223, train_loss: 0.0898, step time: 0.1127\n",
      "215/223, train_loss: 0.0953, step time: 0.1256\n",
      "216/223, train_loss: 0.0926, step time: 0.1000\n",
      "217/223, train_loss: 0.0954, step time: 0.0990\n",
      "218/223, train_loss: 0.1054, step time: 0.0996\n",
      "219/223, train_loss: 0.0968, step time: 0.1011\n",
      "220/223, train_loss: 0.1109, step time: 0.1086\n",
      "221/223, train_loss: 0.0969, step time: 0.1010\n",
      "222/223, train_loss: 0.1050, step time: 0.0995\n",
      "223/223, train_loss: 0.0953, step time: 0.0995\n",
      "epoch 202 average loss: 0.1025\n",
      "time consuming of epoch 202 is: 90.3255\n",
      "----------\n",
      "epoch 203/300\n",
      "1/223, train_loss: 0.0939, step time: 0.1110\n",
      "2/223, train_loss: 0.0952, step time: 0.1309\n",
      "3/223, train_loss: 0.0914, step time: 0.1155\n",
      "4/223, train_loss: 0.1077, step time: 0.1143\n",
      "5/223, train_loss: 0.0948, step time: 0.1144\n",
      "6/223, train_loss: 0.1065, step time: 0.1156\n",
      "7/223, train_loss: 0.0956, step time: 0.1012\n",
      "8/223, train_loss: 0.0899, step time: 0.1193\n",
      "9/223, train_loss: 0.1043, step time: 0.1133\n",
      "10/223, train_loss: 0.1122, step time: 0.1163\n",
      "11/223, train_loss: 0.1036, step time: 0.1001\n",
      "12/223, train_loss: 0.1066, step time: 0.1008\n",
      "13/223, train_loss: 0.0904, step time: 0.1060\n",
      "14/223, train_loss: 0.1041, step time: 0.1088\n",
      "15/223, train_loss: 0.0969, step time: 0.1201\n",
      "16/223, train_loss: 0.0981, step time: 0.1306\n",
      "17/223, train_loss: 0.1196, step time: 0.1008\n",
      "18/223, train_loss: 0.1089, step time: 0.1158\n",
      "19/223, train_loss: 0.0979, step time: 0.1119\n",
      "20/223, train_loss: 0.0974, step time: 0.1007\n",
      "21/223, train_loss: 0.0978, step time: 0.1087\n",
      "22/223, train_loss: 0.0918, step time: 0.1011\n",
      "23/223, train_loss: 0.0997, step time: 0.1018\n",
      "24/223, train_loss: 0.1012, step time: 0.1008\n",
      "25/223, train_loss: 0.1032, step time: 0.0994\n",
      "26/223, train_loss: 0.0991, step time: 0.1025\n",
      "27/223, train_loss: 0.1132, step time: 0.1004\n",
      "28/223, train_loss: 0.1033, step time: 0.1006\n",
      "29/223, train_loss: 0.1120, step time: 0.1003\n",
      "30/223, train_loss: 0.1062, step time: 0.1059\n",
      "31/223, train_loss: 0.0879, step time: 0.1000\n",
      "32/223, train_loss: 0.0896, step time: 0.1009\n",
      "33/223, train_loss: 0.1113, step time: 0.1128\n",
      "34/223, train_loss: 0.1165, step time: 0.1047\n",
      "35/223, train_loss: 0.0993, step time: 0.1132\n",
      "36/223, train_loss: 0.1070, step time: 0.1133\n",
      "37/223, train_loss: 0.1002, step time: 0.1148\n",
      "38/223, train_loss: 0.1053, step time: 0.1021\n",
      "39/223, train_loss: 0.1014, step time: 0.1059\n",
      "40/223, train_loss: 0.0924, step time: 0.1303\n",
      "41/223, train_loss: 0.1105, step time: 0.1070\n",
      "42/223, train_loss: 0.0931, step time: 0.1002\n",
      "43/223, train_loss: 0.1062, step time: 0.1146\n",
      "44/223, train_loss: 0.0970, step time: 0.1393\n",
      "45/223, train_loss: 0.1080, step time: 0.1097\n",
      "46/223, train_loss: 0.1033, step time: 0.1128\n",
      "47/223, train_loss: 0.0993, step time: 0.1150\n",
      "48/223, train_loss: 0.0943, step time: 0.1099\n",
      "49/223, train_loss: 0.1002, step time: 0.1189\n",
      "50/223, train_loss: 0.0935, step time: 0.1080\n",
      "51/223, train_loss: 0.0875, step time: 0.1123\n",
      "52/223, train_loss: 0.0909, step time: 0.1155\n",
      "53/223, train_loss: 0.0918, step time: 0.1005\n",
      "54/223, train_loss: 0.1137, step time: 0.1049\n",
      "55/223, train_loss: 0.1045, step time: 0.1074\n",
      "56/223, train_loss: 0.1102, step time: 0.1070\n",
      "57/223, train_loss: 0.3047, step time: 0.1109\n",
      "58/223, train_loss: 0.1066, step time: 0.1168\n",
      "59/223, train_loss: 0.0993, step time: 0.1077\n",
      "60/223, train_loss: 0.1040, step time: 0.1235\n",
      "61/223, train_loss: 0.1083, step time: 0.1063\n",
      "62/223, train_loss: 0.0972, step time: 0.1047\n",
      "63/223, train_loss: 0.1005, step time: 0.1135\n",
      "64/223, train_loss: 0.1147, step time: 0.1068\n",
      "65/223, train_loss: 0.1076, step time: 0.1064\n",
      "66/223, train_loss: 0.0991, step time: 0.1049\n",
      "67/223, train_loss: 0.0900, step time: 0.1296\n",
      "68/223, train_loss: 0.1155, step time: 0.1089\n",
      "69/223, train_loss: 0.1001, step time: 0.1000\n",
      "70/223, train_loss: 0.1071, step time: 0.1133\n",
      "71/223, train_loss: 0.0973, step time: 0.1055\n",
      "72/223, train_loss: 0.1000, step time: 0.1198\n",
      "73/223, train_loss: 0.1121, step time: 0.1122\n",
      "74/223, train_loss: 0.1037, step time: 0.1159\n",
      "75/223, train_loss: 0.0934, step time: 0.1118\n",
      "76/223, train_loss: 0.0916, step time: 0.1090\n",
      "77/223, train_loss: 0.1040, step time: 0.1181\n",
      "78/223, train_loss: 0.0973, step time: 0.1178\n",
      "79/223, train_loss: 0.0964, step time: 0.1067\n",
      "80/223, train_loss: 0.1006, step time: 0.1083\n",
      "81/223, train_loss: 0.0994, step time: 0.1042\n",
      "82/223, train_loss: 0.1017, step time: 0.1052\n",
      "83/223, train_loss: 0.0959, step time: 0.1044\n",
      "84/223, train_loss: 0.0980, step time: 0.1124\n",
      "85/223, train_loss: 0.0980, step time: 0.0996\n",
      "86/223, train_loss: 0.0920, step time: 0.1085\n",
      "87/223, train_loss: 0.1048, step time: 0.1071\n",
      "88/223, train_loss: 0.0900, step time: 0.1168\n",
      "89/223, train_loss: 0.0985, step time: 0.1135\n",
      "90/223, train_loss: 0.0986, step time: 0.1130\n",
      "91/223, train_loss: 0.0961, step time: 0.1139\n",
      "92/223, train_loss: 0.1121, step time: 0.1204\n",
      "93/223, train_loss: 0.1047, step time: 0.1067\n",
      "94/223, train_loss: 0.0921, step time: 0.1029\n",
      "95/223, train_loss: 0.1081, step time: 0.1032\n",
      "96/223, train_loss: 0.0945, step time: 0.1217\n",
      "97/223, train_loss: 0.0879, step time: 0.0998\n",
      "98/223, train_loss: 0.1035, step time: 0.1092\n",
      "99/223, train_loss: 0.1050, step time: 0.1204\n",
      "100/223, train_loss: 0.1042, step time: 0.1311\n",
      "101/223, train_loss: 0.0974, step time: 0.1165\n",
      "102/223, train_loss: 0.1062, step time: 0.1106\n",
      "103/223, train_loss: 0.0900, step time: 0.1135\n",
      "104/223, train_loss: 0.0938, step time: 0.1099\n",
      "105/223, train_loss: 0.1029, step time: 0.1008\n",
      "106/223, train_loss: 0.1034, step time: 0.1044\n",
      "107/223, train_loss: 0.0951, step time: 0.1228\n",
      "108/223, train_loss: 0.1096, step time: 0.1202\n",
      "109/223, train_loss: 0.1071, step time: 0.1184\n",
      "110/223, train_loss: 0.0945, step time: 0.1117\n",
      "111/223, train_loss: 0.1018, step time: 0.1080\n",
      "112/223, train_loss: 0.0922, step time: 0.1105\n",
      "113/223, train_loss: 0.1131, step time: 0.1153\n",
      "114/223, train_loss: 0.1059, step time: 0.1179\n",
      "115/223, train_loss: 0.1120, step time: 0.1030\n",
      "116/223, train_loss: 0.0961, step time: 0.1089\n",
      "117/223, train_loss: 0.0902, step time: 0.1065\n",
      "118/223, train_loss: 0.1042, step time: 0.1142\n",
      "119/223, train_loss: 0.0973, step time: 0.1110\n",
      "120/223, train_loss: 0.0928, step time: 0.1222\n",
      "121/223, train_loss: 0.0921, step time: 0.1141\n",
      "122/223, train_loss: 0.1097, step time: 0.1180\n",
      "123/223, train_loss: 0.0977, step time: 0.1049\n",
      "124/223, train_loss: 0.0980, step time: 0.1094\n",
      "125/223, train_loss: 0.0954, step time: 0.1061\n",
      "126/223, train_loss: 0.0901, step time: 0.1146\n",
      "127/223, train_loss: 0.0972, step time: 0.1113\n",
      "128/223, train_loss: 0.0955, step time: 0.1170\n",
      "129/223, train_loss: 0.0939, step time: 0.1085\n",
      "130/223, train_loss: 0.1053, step time: 0.1194\n",
      "131/223, train_loss: 0.1117, step time: 0.1258\n",
      "132/223, train_loss: 0.1001, step time: 0.1060\n",
      "133/223, train_loss: 0.1045, step time: 0.1172\n",
      "134/223, train_loss: 0.1030, step time: 0.1067\n",
      "135/223, train_loss: 0.1030, step time: 0.1041\n",
      "136/223, train_loss: 0.1018, step time: 0.1109\n",
      "137/223, train_loss: 0.0933, step time: 0.1145\n",
      "138/223, train_loss: 0.0971, step time: 0.1127\n",
      "139/223, train_loss: 0.1127, step time: 0.1093\n",
      "140/223, train_loss: 0.0962, step time: 0.1213\n",
      "141/223, train_loss: 0.1035, step time: 0.1098\n",
      "142/223, train_loss: 0.1046, step time: 0.1200\n",
      "143/223, train_loss: 0.0999, step time: 0.1170\n",
      "144/223, train_loss: 0.0914, step time: 0.1271\n",
      "145/223, train_loss: 0.0995, step time: 0.1145\n",
      "146/223, train_loss: 0.1104, step time: 0.1136\n",
      "147/223, train_loss: 0.1112, step time: 0.1181\n",
      "148/223, train_loss: 0.0965, step time: 0.1203\n",
      "149/223, train_loss: 0.1030, step time: 0.0997\n",
      "150/223, train_loss: 0.0989, step time: 0.1065\n",
      "151/223, train_loss: 0.0950, step time: 0.1207\n",
      "152/223, train_loss: 0.1016, step time: 0.1351\n",
      "153/223, train_loss: 0.1054, step time: 0.1327\n",
      "154/223, train_loss: 0.0964, step time: 0.1144\n",
      "155/223, train_loss: 0.0938, step time: 0.1262\n",
      "156/223, train_loss: 0.1058, step time: 0.1150\n",
      "157/223, train_loss: 0.1087, step time: 0.1077\n",
      "158/223, train_loss: 0.1014, step time: 0.1167\n",
      "159/223, train_loss: 0.0917, step time: 0.1394\n",
      "160/223, train_loss: 0.1187, step time: 0.1464\n",
      "161/223, train_loss: 0.1056, step time: 0.1219\n",
      "162/223, train_loss: 0.1153, step time: 0.0991\n",
      "163/223, train_loss: 0.0929, step time: 0.0993\n",
      "164/223, train_loss: 0.1106, step time: 0.0999\n",
      "165/223, train_loss: 0.0971, step time: 0.0999\n",
      "166/223, train_loss: 0.0978, step time: 0.1007\n",
      "167/223, train_loss: 0.0916, step time: 0.1001\n",
      "168/223, train_loss: 0.1065, step time: 0.1005\n",
      "169/223, train_loss: 0.1036, step time: 0.1201\n",
      "170/223, train_loss: 0.0958, step time: 0.1123\n",
      "171/223, train_loss: 0.0956, step time: 0.1362\n",
      "172/223, train_loss: 0.1061, step time: 0.1095\n",
      "173/223, train_loss: 0.1062, step time: 0.1016\n",
      "174/223, train_loss: 0.0996, step time: 0.1050\n",
      "175/223, train_loss: 0.1016, step time: 0.0997\n",
      "176/223, train_loss: 0.0949, step time: 0.0995\n",
      "177/223, train_loss: 0.1022, step time: 0.1103\n",
      "178/223, train_loss: 0.0940, step time: 0.1183\n",
      "179/223, train_loss: 0.1098, step time: 0.1135\n",
      "180/223, train_loss: 0.1080, step time: 0.1018\n",
      "181/223, train_loss: 0.0991, step time: 0.1237\n",
      "182/223, train_loss: 0.1111, step time: 0.1227\n",
      "183/223, train_loss: 0.1121, step time: 0.1047\n",
      "184/223, train_loss: 0.1044, step time: 0.1174\n",
      "185/223, train_loss: 0.0985, step time: 0.1123\n",
      "186/223, train_loss: 0.1046, step time: 0.1194\n",
      "187/223, train_loss: 0.1087, step time: 0.0998\n",
      "188/223, train_loss: 0.1143, step time: 0.1025\n",
      "189/223, train_loss: 0.1083, step time: 0.1087\n",
      "190/223, train_loss: 0.0926, step time: 0.1069\n",
      "191/223, train_loss: 0.1069, step time: 0.0996\n",
      "192/223, train_loss: 0.1074, step time: 0.1223\n",
      "193/223, train_loss: 0.1079, step time: 0.1157\n",
      "194/223, train_loss: 0.0929, step time: 0.1095\n",
      "195/223, train_loss: 0.1092, step time: 0.1137\n",
      "196/223, train_loss: 0.1052, step time: 0.1034\n",
      "197/223, train_loss: 0.1098, step time: 0.0983\n",
      "198/223, train_loss: 0.0956, step time: 0.1079\n",
      "199/223, train_loss: 0.1081, step time: 0.1076\n",
      "200/223, train_loss: 0.1022, step time: 0.1179\n",
      "201/223, train_loss: 0.0935, step time: 0.1038\n",
      "202/223, train_loss: 0.0999, step time: 0.1163\n",
      "203/223, train_loss: 0.1134, step time: 0.1274\n",
      "204/223, train_loss: 0.1049, step time: 0.1004\n",
      "205/223, train_loss: 0.0999, step time: 0.1020\n",
      "206/223, train_loss: 0.1153, step time: 0.1090\n",
      "207/223, train_loss: 0.1138, step time: 0.1143\n",
      "208/223, train_loss: 0.0961, step time: 0.1355\n",
      "209/223, train_loss: 0.0968, step time: 0.1156\n",
      "210/223, train_loss: 0.1119, step time: 0.1268\n",
      "211/223, train_loss: 0.1102, step time: 0.1281\n",
      "212/223, train_loss: 0.0927, step time: 0.1527\n",
      "213/223, train_loss: 0.0968, step time: 0.1004\n",
      "214/223, train_loss: 0.1083, step time: 0.1008\n",
      "215/223, train_loss: 0.1029, step time: 0.1152\n",
      "216/223, train_loss: 0.1033, step time: 0.1276\n",
      "217/223, train_loss: 0.1115, step time: 0.1079\n",
      "218/223, train_loss: 0.0860, step time: 0.0997\n",
      "219/223, train_loss: 0.1125, step time: 0.1002\n",
      "220/223, train_loss: 0.0882, step time: 0.0992\n",
      "221/223, train_loss: 0.0945, step time: 0.0996\n",
      "222/223, train_loss: 0.0993, step time: 0.1003\n",
      "223/223, train_loss: 0.0971, step time: 0.0998\n",
      "epoch 203 average loss: 0.1023\n",
      "time consuming of epoch 203 is: 88.9845\n",
      "----------\n",
      "epoch 204/300\n",
      "1/223, train_loss: 0.0863, step time: 0.1018\n",
      "2/223, train_loss: 0.1136, step time: 0.1035\n",
      "3/223, train_loss: 0.0908, step time: 0.1091\n",
      "4/223, train_loss: 0.0936, step time: 0.1290\n",
      "5/223, train_loss: 0.1055, step time: 0.1155\n",
      "6/223, train_loss: 0.0987, step time: 0.1137\n",
      "7/223, train_loss: 0.0917, step time: 0.1095\n",
      "8/223, train_loss: 0.0968, step time: 0.1006\n",
      "9/223, train_loss: 0.1057, step time: 0.1141\n",
      "10/223, train_loss: 0.0972, step time: 0.1218\n",
      "11/223, train_loss: 0.0945, step time: 0.1181\n",
      "12/223, train_loss: 0.1047, step time: 0.1121\n",
      "13/223, train_loss: 0.1023, step time: 0.1005\n",
      "14/223, train_loss: 0.1037, step time: 0.1001\n",
      "15/223, train_loss: 0.1039, step time: 0.0998\n",
      "16/223, train_loss: 0.1066, step time: 0.1325\n",
      "17/223, train_loss: 0.0996, step time: 0.1006\n",
      "18/223, train_loss: 0.0988, step time: 0.0998\n",
      "19/223, train_loss: 0.1093, step time: 0.1005\n",
      "20/223, train_loss: 0.1056, step time: 0.1071\n",
      "21/223, train_loss: 0.0880, step time: 0.1196\n",
      "22/223, train_loss: 0.0989, step time: 0.1057\n",
      "23/223, train_loss: 0.1008, step time: 0.1229\n",
      "24/223, train_loss: 0.0987, step time: 0.1010\n",
      "25/223, train_loss: 0.0997, step time: 0.1257\n",
      "26/223, train_loss: 0.1023, step time: 0.1235\n",
      "27/223, train_loss: 0.1199, step time: 0.1321\n",
      "28/223, train_loss: 0.0999, step time: 0.1006\n",
      "29/223, train_loss: 0.1052, step time: 0.1040\n",
      "30/223, train_loss: 0.1050, step time: 0.1005\n",
      "31/223, train_loss: 0.1031, step time: 0.1003\n",
      "32/223, train_loss: 0.0982, step time: 0.1026\n",
      "33/223, train_loss: 0.1047, step time: 0.1100\n",
      "34/223, train_loss: 0.0995, step time: 0.1136\n",
      "35/223, train_loss: 0.1049, step time: 0.1007\n",
      "36/223, train_loss: 0.1095, step time: 0.1003\n",
      "37/223, train_loss: 0.0948, step time: 0.1130\n",
      "38/223, train_loss: 0.1011, step time: 0.1002\n",
      "39/223, train_loss: 0.1014, step time: 0.1008\n",
      "40/223, train_loss: 0.1022, step time: 0.1127\n",
      "41/223, train_loss: 0.1005, step time: 0.1244\n",
      "42/223, train_loss: 0.0968, step time: 0.1258\n",
      "43/223, train_loss: 0.0999, step time: 0.1093\n",
      "44/223, train_loss: 0.0992, step time: 0.1003\n",
      "45/223, train_loss: 0.1022, step time: 0.1013\n",
      "46/223, train_loss: 0.1003, step time: 0.1035\n",
      "47/223, train_loss: 0.2912, step time: 0.1007\n",
      "48/223, train_loss: 0.0986, step time: 0.1017\n",
      "49/223, train_loss: 0.0941, step time: 0.1131\n",
      "50/223, train_loss: 0.1029, step time: 0.1167\n",
      "51/223, train_loss: 0.0981, step time: 0.1243\n",
      "52/223, train_loss: 0.0987, step time: 0.1109\n",
      "53/223, train_loss: 0.0969, step time: 0.1061\n",
      "54/223, train_loss: 0.0958, step time: 0.1207\n",
      "55/223, train_loss: 0.0943, step time: 0.1162\n",
      "56/223, train_loss: 0.1063, step time: 0.1075\n",
      "57/223, train_loss: 0.0988, step time: 0.1219\n",
      "58/223, train_loss: 0.1005, step time: 0.1192\n",
      "59/223, train_loss: 0.0949, step time: 0.1118\n",
      "60/223, train_loss: 0.1064, step time: 0.1025\n",
      "61/223, train_loss: 0.0865, step time: 0.1087\n",
      "62/223, train_loss: 0.0977, step time: 0.1093\n",
      "63/223, train_loss: 0.1009, step time: 0.1067\n",
      "64/223, train_loss: 0.1087, step time: 0.1170\n",
      "65/223, train_loss: 0.1031, step time: 0.1002\n",
      "66/223, train_loss: 0.1050, step time: 0.1106\n",
      "67/223, train_loss: 0.0985, step time: 0.1212\n",
      "68/223, train_loss: 0.1052, step time: 0.1030\n",
      "69/223, train_loss: 0.0988, step time: 0.1091\n",
      "70/223, train_loss: 0.0932, step time: 0.1368\n",
      "71/223, train_loss: 0.0965, step time: 0.1273\n",
      "72/223, train_loss: 0.1125, step time: 0.1057\n",
      "73/223, train_loss: 0.1137, step time: 0.1193\n",
      "74/223, train_loss: 0.1024, step time: 0.1154\n",
      "75/223, train_loss: 0.1000, step time: 0.1314\n",
      "76/223, train_loss: 0.0940, step time: 0.1139\n",
      "77/223, train_loss: 0.1084, step time: 0.1007\n",
      "78/223, train_loss: 0.1057, step time: 0.1015\n",
      "79/223, train_loss: 0.0941, step time: 0.1148\n",
      "80/223, train_loss: 0.1077, step time: 0.1005\n",
      "81/223, train_loss: 0.1103, step time: 0.1061\n",
      "82/223, train_loss: 0.0964, step time: 0.1116\n",
      "83/223, train_loss: 0.0998, step time: 0.1149\n",
      "84/223, train_loss: 0.0947, step time: 0.1087\n",
      "85/223, train_loss: 0.0931, step time: 0.1188\n",
      "86/223, train_loss: 0.1148, step time: 0.1160\n",
      "87/223, train_loss: 0.1024, step time: 0.1131\n",
      "88/223, train_loss: 0.0986, step time: 0.1047\n",
      "89/223, train_loss: 0.1025, step time: 0.1100\n",
      "90/223, train_loss: 0.1131, step time: 0.1196\n",
      "91/223, train_loss: 0.1261, step time: 0.1215\n",
      "92/223, train_loss: 0.1117, step time: 0.1144\n",
      "93/223, train_loss: 0.1139, step time: 0.1082\n",
      "94/223, train_loss: 0.0944, step time: 0.1231\n",
      "95/223, train_loss: 0.1085, step time: 0.1003\n",
      "96/223, train_loss: 0.0935, step time: 0.1059\n",
      "97/223, train_loss: 0.0928, step time: 0.1073\n",
      "98/223, train_loss: 0.1058, step time: 0.1118\n",
      "99/223, train_loss: 0.0992, step time: 0.1265\n",
      "100/223, train_loss: 0.1043, step time: 0.1180\n",
      "101/223, train_loss: 0.0973, step time: 0.1205\n",
      "102/223, train_loss: 0.1056, step time: 0.1102\n",
      "103/223, train_loss: 0.1045, step time: 0.1442\n",
      "104/223, train_loss: 0.1065, step time: 0.1171\n",
      "105/223, train_loss: 0.1046, step time: 0.1139\n",
      "106/223, train_loss: 0.0887, step time: 0.1168\n",
      "107/223, train_loss: 0.0895, step time: 0.1059\n",
      "108/223, train_loss: 0.1049, step time: 0.1149\n",
      "109/223, train_loss: 0.0964, step time: 0.1189\n",
      "110/223, train_loss: 0.1168, step time: 0.0997\n",
      "111/223, train_loss: 0.0985, step time: 0.1045\n",
      "112/223, train_loss: 0.1162, step time: 0.1009\n",
      "113/223, train_loss: 0.0916, step time: 0.1073\n",
      "114/223, train_loss: 0.0913, step time: 0.1036\n",
      "115/223, train_loss: 0.1024, step time: 0.1002\n",
      "116/223, train_loss: 0.1014, step time: 0.1036\n",
      "117/223, train_loss: 0.1044, step time: 0.1083\n",
      "118/223, train_loss: 0.0929, step time: 0.1011\n",
      "119/223, train_loss: 0.1005, step time: 0.1002\n",
      "120/223, train_loss: 0.1051, step time: 0.1077\n",
      "121/223, train_loss: 0.0951, step time: 0.1062\n",
      "122/223, train_loss: 0.1032, step time: 0.1108\n",
      "123/223, train_loss: 0.1094, step time: 0.1006\n",
      "124/223, train_loss: 0.0974, step time: 0.1059\n",
      "125/223, train_loss: 0.0895, step time: 0.1156\n",
      "126/223, train_loss: 0.1016, step time: 0.1034\n",
      "127/223, train_loss: 0.1061, step time: 0.1193\n",
      "128/223, train_loss: 0.0984, step time: 0.1077\n",
      "129/223, train_loss: 0.0974, step time: 0.1124\n",
      "130/223, train_loss: 0.1188, step time: 0.1208\n",
      "131/223, train_loss: 0.0944, step time: 0.1030\n",
      "132/223, train_loss: 0.1048, step time: 0.1182\n",
      "133/223, train_loss: 0.0932, step time: 0.1011\n",
      "134/223, train_loss: 0.0960, step time: 0.0998\n",
      "135/223, train_loss: 0.1068, step time: 0.1003\n",
      "136/223, train_loss: 0.1075, step time: 0.1006\n",
      "137/223, train_loss: 0.1057, step time: 0.1005\n",
      "138/223, train_loss: 0.0988, step time: 0.1137\n",
      "139/223, train_loss: 0.1087, step time: 0.1264\n",
      "140/223, train_loss: 0.1061, step time: 0.1569\n",
      "141/223, train_loss: 0.1056, step time: 0.1199\n",
      "142/223, train_loss: 0.1017, step time: 0.1304\n",
      "143/223, train_loss: 0.1138, step time: 0.1006\n",
      "144/223, train_loss: 0.0991, step time: 0.1059\n",
      "145/223, train_loss: 0.0977, step time: 0.1125\n",
      "146/223, train_loss: 0.1149, step time: 0.1324\n",
      "147/223, train_loss: 0.1119, step time: 0.1355\n",
      "148/223, train_loss: 0.1042, step time: 0.1019\n",
      "149/223, train_loss: 0.1018, step time: 0.1092\n",
      "150/223, train_loss: 0.0986, step time: 0.1070\n",
      "151/223, train_loss: 0.0977, step time: 0.1316\n",
      "152/223, train_loss: 0.1173, step time: 0.1042\n",
      "153/223, train_loss: 0.0904, step time: 0.1419\n",
      "154/223, train_loss: 0.1062, step time: 0.1139\n",
      "155/223, train_loss: 0.0959, step time: 0.1283\n",
      "156/223, train_loss: 0.0972, step time: 0.1076\n",
      "157/223, train_loss: 0.1008, step time: 0.1044\n",
      "158/223, train_loss: 0.0963, step time: 0.1081\n",
      "159/223, train_loss: 0.1006, step time: 0.1023\n",
      "160/223, train_loss: 0.0984, step time: 0.1020\n",
      "161/223, train_loss: 0.1002, step time: 0.1118\n",
      "162/223, train_loss: 0.1004, step time: 0.1209\n",
      "163/223, train_loss: 0.1049, step time: 0.1130\n",
      "164/223, train_loss: 0.1049, step time: 0.1129\n",
      "165/223, train_loss: 0.0981, step time: 0.1048\n",
      "166/223, train_loss: 0.1023, step time: 0.1072\n",
      "167/223, train_loss: 0.0932, step time: 0.1114\n",
      "168/223, train_loss: 0.1006, step time: 0.1062\n",
      "169/223, train_loss: 0.1069, step time: 0.1146\n",
      "170/223, train_loss: 0.1022, step time: 0.1302\n",
      "171/223, train_loss: 0.1050, step time: 0.1361\n",
      "172/223, train_loss: 0.1013, step time: 0.1156\n",
      "173/223, train_loss: 0.1054, step time: 0.1117\n",
      "174/223, train_loss: 0.1012, step time: 0.1097\n",
      "175/223, train_loss: 0.1019, step time: 0.1028\n",
      "176/223, train_loss: 0.0976, step time: 0.1116\n",
      "177/223, train_loss: 0.1001, step time: 0.1069\n",
      "178/223, train_loss: 0.0955, step time: 0.1172\n",
      "179/223, train_loss: 0.1058, step time: 0.1414\n",
      "180/223, train_loss: 0.1057, step time: 0.1226\n",
      "181/223, train_loss: 0.0974, step time: 0.1063\n",
      "182/223, train_loss: 0.1151, step time: 0.1398\n",
      "183/223, train_loss: 0.1041, step time: 0.1133\n",
      "184/223, train_loss: 0.0959, step time: 0.1074\n",
      "185/223, train_loss: 0.1017, step time: 0.1130\n",
      "186/223, train_loss: 0.1015, step time: 0.1088\n",
      "187/223, train_loss: 0.0985, step time: 0.1001\n",
      "188/223, train_loss: 0.1117, step time: 0.1021\n",
      "189/223, train_loss: 0.1039, step time: 0.1142\n",
      "190/223, train_loss: 0.1110, step time: 0.1276\n",
      "191/223, train_loss: 0.0924, step time: 0.1224\n",
      "192/223, train_loss: 0.1030, step time: 0.1054\n",
      "193/223, train_loss: 0.1036, step time: 0.1134\n",
      "194/223, train_loss: 0.1049, step time: 0.1071\n",
      "195/223, train_loss: 0.0903, step time: 0.1069\n",
      "196/223, train_loss: 0.0960, step time: 0.1158\n",
      "197/223, train_loss: 0.1071, step time: 0.1007\n",
      "198/223, train_loss: 0.1024, step time: 0.1029\n",
      "199/223, train_loss: 0.1037, step time: 0.1014\n",
      "200/223, train_loss: 0.1003, step time: 0.1003\n",
      "201/223, train_loss: 0.0945, step time: 0.1086\n",
      "202/223, train_loss: 0.0930, step time: 0.1015\n",
      "203/223, train_loss: 0.1135, step time: 0.1011\n",
      "204/223, train_loss: 0.1040, step time: 0.1379\n",
      "205/223, train_loss: 0.0955, step time: 0.1090\n",
      "206/223, train_loss: 0.0925, step time: 0.1196\n",
      "207/223, train_loss: 0.1029, step time: 0.1036\n",
      "208/223, train_loss: 0.1015, step time: 0.1013\n",
      "209/223, train_loss: 0.1016, step time: 0.1125\n",
      "210/223, train_loss: 0.0998, step time: 0.1339\n",
      "211/223, train_loss: 0.1046, step time: 0.1222\n",
      "212/223, train_loss: 0.1165, step time: 0.1265\n",
      "213/223, train_loss: 0.1007, step time: 0.1021\n",
      "214/223, train_loss: 0.0991, step time: 0.1102\n",
      "215/223, train_loss: 0.1065, step time: 0.1110\n",
      "216/223, train_loss: 0.1133, step time: 0.1074\n",
      "217/223, train_loss: 0.0992, step time: 0.1017\n",
      "218/223, train_loss: 0.0891, step time: 0.1054\n",
      "219/223, train_loss: 0.0961, step time: 0.1048\n",
      "220/223, train_loss: 0.0954, step time: 0.1094\n",
      "221/223, train_loss: 0.1004, step time: 0.0994\n",
      "222/223, train_loss: 0.0957, step time: 0.0988\n",
      "223/223, train_loss: 0.1013, step time: 0.1000\n",
      "epoch 204 average loss: 0.1024\n",
      "time consuming of epoch 204 is: 86.9571\n",
      "----------\n",
      "epoch 205/300\n",
      "1/223, train_loss: 0.1038, step time: 0.1048\n",
      "2/223, train_loss: 0.0960, step time: 0.1035\n",
      "3/223, train_loss: 0.1020, step time: 0.1154\n",
      "4/223, train_loss: 0.0994, step time: 0.1130\n",
      "5/223, train_loss: 0.0976, step time: 0.1095\n",
      "6/223, train_loss: 0.1199, step time: 0.1176\n",
      "7/223, train_loss: 0.1084, step time: 0.1162\n",
      "8/223, train_loss: 0.0992, step time: 0.0995\n",
      "9/223, train_loss: 0.0934, step time: 0.1021\n",
      "10/223, train_loss: 0.0958, step time: 0.1035\n",
      "11/223, train_loss: 0.0954, step time: 0.1190\n",
      "12/223, train_loss: 0.0927, step time: 0.1019\n",
      "13/223, train_loss: 0.0939, step time: 0.1011\n",
      "14/223, train_loss: 0.1032, step time: 0.1150\n",
      "15/223, train_loss: 0.0992, step time: 0.1128\n",
      "16/223, train_loss: 0.1000, step time: 0.1004\n",
      "17/223, train_loss: 0.0990, step time: 0.0990\n",
      "18/223, train_loss: 0.0912, step time: 0.1141\n",
      "19/223, train_loss: 0.1050, step time: 0.1065\n",
      "20/223, train_loss: 0.0945, step time: 0.1252\n",
      "21/223, train_loss: 0.0980, step time: 0.1157\n",
      "22/223, train_loss: 0.1014, step time: 0.1142\n",
      "23/223, train_loss: 0.0954, step time: 0.1236\n",
      "24/223, train_loss: 0.1027, step time: 0.1070\n",
      "25/223, train_loss: 0.1028, step time: 0.1112\n",
      "26/223, train_loss: 0.1117, step time: 0.1077\n",
      "27/223, train_loss: 0.1009, step time: 0.1134\n",
      "28/223, train_loss: 0.1085, step time: 0.1112\n",
      "29/223, train_loss: 0.0996, step time: 0.1088\n",
      "30/223, train_loss: 0.1201, step time: 0.1119\n",
      "31/223, train_loss: 0.1044, step time: 0.1097\n",
      "32/223, train_loss: 0.1043, step time: 0.1161\n",
      "33/223, train_loss: 0.1005, step time: 0.1006\n",
      "34/223, train_loss: 0.0943, step time: 0.1064\n",
      "35/223, train_loss: 0.1022, step time: 0.1186\n",
      "36/223, train_loss: 0.0941, step time: 0.1048\n",
      "37/223, train_loss: 0.1059, step time: 0.1222\n",
      "38/223, train_loss: 0.0895, step time: 0.0999\n",
      "39/223, train_loss: 0.0912, step time: 0.0997\n",
      "40/223, train_loss: 0.1017, step time: 0.1003\n",
      "41/223, train_loss: 0.0926, step time: 0.1005\n",
      "42/223, train_loss: 0.1032, step time: 0.0990\n",
      "43/223, train_loss: 0.0904, step time: 0.1001\n",
      "44/223, train_loss: 0.0858, step time: 0.0999\n",
      "45/223, train_loss: 0.0958, step time: 0.1027\n",
      "46/223, train_loss: 0.1030, step time: 0.0996\n",
      "47/223, train_loss: 0.1004, step time: 0.1002\n",
      "48/223, train_loss: 0.1091, step time: 0.1254\n",
      "49/223, train_loss: 0.1038, step time: 0.1004\n",
      "50/223, train_loss: 0.1041, step time: 0.1090\n",
      "51/223, train_loss: 0.0971, step time: 0.1257\n",
      "52/223, train_loss: 0.1032, step time: 0.1145\n",
      "53/223, train_loss: 0.1055, step time: 0.1078\n",
      "54/223, train_loss: 0.0943, step time: 0.1065\n",
      "55/223, train_loss: 0.1025, step time: 0.1207\n",
      "56/223, train_loss: 0.0960, step time: 0.1295\n",
      "57/223, train_loss: 0.1038, step time: 0.1121\n",
      "58/223, train_loss: 0.0904, step time: 0.1074\n",
      "59/223, train_loss: 0.1104, step time: 0.1205\n",
      "60/223, train_loss: 0.1025, step time: 0.1232\n",
      "61/223, train_loss: 0.0921, step time: 0.1004\n",
      "62/223, train_loss: 0.0956, step time: 0.1116\n",
      "63/223, train_loss: 0.0907, step time: 0.1176\n",
      "64/223, train_loss: 0.0974, step time: 0.1412\n",
      "65/223, train_loss: 0.0997, step time: 0.1059\n",
      "66/223, train_loss: 0.1093, step time: 0.1113\n",
      "67/223, train_loss: 0.0981, step time: 0.1155\n",
      "68/223, train_loss: 0.1019, step time: 0.1069\n",
      "69/223, train_loss: 0.0924, step time: 0.1107\n",
      "70/223, train_loss: 0.1110, step time: 0.1104\n",
      "71/223, train_loss: 0.0910, step time: 0.1227\n",
      "72/223, train_loss: 0.1158, step time: 0.1010\n",
      "73/223, train_loss: 0.0920, step time: 0.0991\n",
      "74/223, train_loss: 0.1072, step time: 0.0997\n",
      "75/223, train_loss: 0.1013, step time: 0.1338\n",
      "76/223, train_loss: 0.1000, step time: 0.1127\n",
      "77/223, train_loss: 0.0912, step time: 0.1108\n",
      "78/223, train_loss: 0.1112, step time: 0.1080\n",
      "79/223, train_loss: 0.0945, step time: 0.1153\n",
      "80/223, train_loss: 0.1072, step time: 0.1256\n",
      "81/223, train_loss: 0.1044, step time: 0.1030\n",
      "82/223, train_loss: 0.1008, step time: 0.1206\n",
      "83/223, train_loss: 0.0991, step time: 0.1101\n",
      "84/223, train_loss: 0.0962, step time: 0.1535\n",
      "85/223, train_loss: 0.1018, step time: 0.1068\n",
      "86/223, train_loss: 0.0859, step time: 0.1333\n",
      "87/223, train_loss: 0.1010, step time: 0.1211\n",
      "88/223, train_loss: 0.1058, step time: 0.1053\n",
      "89/223, train_loss: 0.0998, step time: 0.1000\n",
      "90/223, train_loss: 0.1010, step time: 0.1344\n",
      "91/223, train_loss: 0.0997, step time: 0.0995\n",
      "92/223, train_loss: 0.1132, step time: 0.1022\n",
      "93/223, train_loss: 0.1123, step time: 0.1479\n",
      "94/223, train_loss: 0.1078, step time: 0.1115\n",
      "95/223, train_loss: 0.1006, step time: 0.1070\n",
      "96/223, train_loss: 0.1068, step time: 0.1088\n",
      "97/223, train_loss: 0.1027, step time: 0.1019\n",
      "98/223, train_loss: 0.0936, step time: 0.1143\n",
      "99/223, train_loss: 0.0991, step time: 0.1106\n",
      "100/223, train_loss: 0.0968, step time: 0.1211\n",
      "101/223, train_loss: 0.1055, step time: 0.1015\n",
      "102/223, train_loss: 0.0896, step time: 0.1093\n",
      "103/223, train_loss: 0.1012, step time: 0.1129\n",
      "104/223, train_loss: 0.1080, step time: 0.1231\n",
      "105/223, train_loss: 0.1000, step time: 0.1161\n",
      "106/223, train_loss: 0.1116, step time: 0.1225\n",
      "107/223, train_loss: 0.1006, step time: 0.1403\n",
      "108/223, train_loss: 0.0944, step time: 0.1277\n",
      "109/223, train_loss: 0.1091, step time: 0.1185\n",
      "110/223, train_loss: 0.0928, step time: 0.1014\n",
      "111/223, train_loss: 0.1065, step time: 0.1102\n",
      "112/223, train_loss: 0.1042, step time: 0.1160\n",
      "113/223, train_loss: 0.0932, step time: 0.1109\n",
      "114/223, train_loss: 0.1068, step time: 0.1198\n",
      "115/223, train_loss: 0.1004, step time: 0.1118\n",
      "116/223, train_loss: 0.1113, step time: 0.1099\n",
      "117/223, train_loss: 0.1003, step time: 0.1067\n",
      "118/223, train_loss: 0.1084, step time: 0.1053\n",
      "119/223, train_loss: 0.1079, step time: 0.1254\n",
      "120/223, train_loss: 0.0984, step time: 0.0998\n",
      "121/223, train_loss: 0.1019, step time: 0.1230\n",
      "122/223, train_loss: 0.0931, step time: 0.1000\n",
      "123/223, train_loss: 0.0895, step time: 0.1112\n",
      "124/223, train_loss: 0.1005, step time: 0.1240\n",
      "125/223, train_loss: 0.1096, step time: 0.1399\n",
      "126/223, train_loss: 0.0910, step time: 0.1099\n",
      "127/223, train_loss: 0.1075, step time: 0.1208\n",
      "128/223, train_loss: 0.1031, step time: 0.1052\n",
      "129/223, train_loss: 0.0919, step time: 0.1267\n",
      "130/223, train_loss: 0.0960, step time: 0.1069\n",
      "131/223, train_loss: 0.1107, step time: 0.1001\n",
      "132/223, train_loss: 0.1027, step time: 0.1005\n",
      "133/223, train_loss: 0.1147, step time: 0.0998\n",
      "134/223, train_loss: 0.0955, step time: 0.1003\n",
      "135/223, train_loss: 0.0986, step time: 0.1000\n",
      "136/223, train_loss: 0.0987, step time: 0.1079\n",
      "137/223, train_loss: 0.0972, step time: 0.1019\n",
      "138/223, train_loss: 0.1052, step time: 0.1017\n",
      "139/223, train_loss: 0.0983, step time: 0.1124\n",
      "140/223, train_loss: 0.1099, step time: 0.1103\n",
      "141/223, train_loss: 0.0966, step time: 0.1002\n",
      "142/223, train_loss: 0.1033, step time: 0.1145\n",
      "143/223, train_loss: 0.1000, step time: 0.1147\n",
      "144/223, train_loss: 0.1078, step time: 0.1006\n",
      "145/223, train_loss: 0.1058, step time: 0.1002\n",
      "146/223, train_loss: 0.0980, step time: 0.1280\n",
      "147/223, train_loss: 0.0970, step time: 0.1145\n",
      "148/223, train_loss: 0.1119, step time: 0.1008\n",
      "149/223, train_loss: 0.1127, step time: 0.1008\n",
      "150/223, train_loss: 0.0927, step time: 0.1094\n",
      "151/223, train_loss: 0.1085, step time: 0.1006\n",
      "152/223, train_loss: 0.1038, step time: 0.0993\n",
      "153/223, train_loss: 0.1043, step time: 0.1057\n",
      "154/223, train_loss: 0.1078, step time: 0.1010\n",
      "155/223, train_loss: 0.0940, step time: 0.1001\n",
      "156/223, train_loss: 0.0955, step time: 0.1007\n",
      "157/223, train_loss: 0.1032, step time: 0.1002\n",
      "158/223, train_loss: 0.1011, step time: 0.1007\n",
      "159/223, train_loss: 0.1013, step time: 0.1186\n",
      "160/223, train_loss: 0.1067, step time: 0.1344\n",
      "161/223, train_loss: 0.1089, step time: 0.1002\n",
      "162/223, train_loss: 0.1034, step time: 0.1007\n",
      "163/223, train_loss: 0.0970, step time: 0.1005\n",
      "164/223, train_loss: 0.1083, step time: 0.1005\n",
      "165/223, train_loss: 0.1039, step time: 0.1223\n",
      "166/223, train_loss: 0.0956, step time: 0.1000\n",
      "167/223, train_loss: 0.1011, step time: 0.1025\n",
      "168/223, train_loss: 0.1160, step time: 0.1003\n",
      "169/223, train_loss: 0.0934, step time: 0.0997\n",
      "170/223, train_loss: 0.0981, step time: 0.0989\n",
      "171/223, train_loss: 0.0974, step time: 0.1025\n",
      "172/223, train_loss: 0.1077, step time: 0.1091\n",
      "173/223, train_loss: 0.1153, step time: 0.1008\n",
      "174/223, train_loss: 0.0957, step time: 0.0995\n",
      "175/223, train_loss: 0.1028, step time: 0.0996\n",
      "176/223, train_loss: 0.1184, step time: 0.1186\n",
      "177/223, train_loss: 0.0961, step time: 0.1018\n",
      "178/223, train_loss: 0.1012, step time: 0.1038\n",
      "179/223, train_loss: 0.1122, step time: 0.1002\n",
      "180/223, train_loss: 0.1151, step time: 0.1137\n",
      "181/223, train_loss: 0.0918, step time: 0.1151\n",
      "182/223, train_loss: 0.1035, step time: 0.1123\n",
      "183/223, train_loss: 0.1004, step time: 0.1013\n",
      "184/223, train_loss: 0.1001, step time: 0.1005\n",
      "185/223, train_loss: 0.1096, step time: 0.1062\n",
      "186/223, train_loss: 0.1096, step time: 0.1001\n",
      "187/223, train_loss: 0.0973, step time: 0.1017\n",
      "188/223, train_loss: 0.0950, step time: 0.1006\n",
      "189/223, train_loss: 0.1053, step time: 0.1046\n",
      "190/223, train_loss: 0.0994, step time: 0.1043\n",
      "191/223, train_loss: 0.1052, step time: 0.1177\n",
      "192/223, train_loss: 0.0947, step time: 0.1299\n",
      "193/223, train_loss: 0.0992, step time: 0.1012\n",
      "194/223, train_loss: 0.1079, step time: 0.1099\n",
      "195/223, train_loss: 0.0940, step time: 0.1240\n",
      "196/223, train_loss: 0.1061, step time: 0.1430\n",
      "197/223, train_loss: 0.1071, step time: 0.1005\n",
      "198/223, train_loss: 0.0937, step time: 0.1098\n",
      "199/223, train_loss: 0.0961, step time: 0.1080\n",
      "200/223, train_loss: 0.1073, step time: 0.1224\n",
      "201/223, train_loss: 0.1063, step time: 0.1006\n",
      "202/223, train_loss: 0.0926, step time: 0.1122\n",
      "203/223, train_loss: 0.0981, step time: 0.1431\n",
      "204/223, train_loss: 0.1011, step time: 0.1091\n",
      "205/223, train_loss: 0.0989, step time: 0.0993\n",
      "206/223, train_loss: 0.0947, step time: 0.1329\n",
      "207/223, train_loss: 0.0932, step time: 0.1232\n",
      "208/223, train_loss: 0.0970, step time: 0.1156\n",
      "209/223, train_loss: 0.1195, step time: 0.0996\n",
      "210/223, train_loss: 0.0931, step time: 0.1114\n",
      "211/223, train_loss: 0.0931, step time: 0.1232\n",
      "212/223, train_loss: 0.0924, step time: 0.1428\n",
      "213/223, train_loss: 0.0984, step time: 0.1013\n",
      "214/223, train_loss: 0.1175, step time: 0.0999\n",
      "215/223, train_loss: 0.1066, step time: 0.0999\n",
      "216/223, train_loss: 0.1044, step time: 0.0998\n",
      "217/223, train_loss: 0.2949, step time: 0.1046\n",
      "218/223, train_loss: 0.1090, step time: 0.0998\n",
      "219/223, train_loss: 0.0957, step time: 0.1001\n",
      "220/223, train_loss: 0.1008, step time: 0.1003\n",
      "221/223, train_loss: 0.1029, step time: 0.0997\n",
      "222/223, train_loss: 0.1098, step time: 0.1003\n",
      "223/223, train_loss: 0.1015, step time: 0.0998\n",
      "epoch 205 average loss: 0.1022\n",
      "saved new best metric model\n",
      "current epoch: 205 current mean dice: 0.8605 tc: 0.9216 wt: 0.8705 et: 0.7895\n",
      "best mean dice: 0.8605 at epoch: 205\n",
      "time consuming of epoch 205 is: 100.3371\n",
      "----------\n",
      "epoch 206/300\n",
      "1/223, train_loss: 0.1131, step time: 0.1033\n",
      "2/223, train_loss: 0.0956, step time: 0.1002\n",
      "3/223, train_loss: 0.1013, step time: 0.1012\n",
      "4/223, train_loss: 0.1101, step time: 0.1075\n",
      "5/223, train_loss: 0.1037, step time: 0.0998\n",
      "6/223, train_loss: 0.1014, step time: 0.0997\n",
      "7/223, train_loss: 0.1067, step time: 0.0996\n",
      "8/223, train_loss: 0.0957, step time: 0.1008\n",
      "9/223, train_loss: 0.1077, step time: 0.0993\n",
      "10/223, train_loss: 0.1005, step time: 0.0989\n",
      "11/223, train_loss: 0.1047, step time: 0.1001\n",
      "12/223, train_loss: 0.0935, step time: 0.1266\n",
      "13/223, train_loss: 0.1018, step time: 0.1001\n",
      "14/223, train_loss: 0.0954, step time: 0.0993\n",
      "15/223, train_loss: 0.1009, step time: 0.1008\n",
      "16/223, train_loss: 0.1174, step time: 0.1302\n",
      "17/223, train_loss: 0.0991, step time: 0.1153\n",
      "18/223, train_loss: 0.1068, step time: 0.1194\n",
      "19/223, train_loss: 0.0945, step time: 0.1132\n",
      "20/223, train_loss: 0.0904, step time: 0.1162\n",
      "21/223, train_loss: 0.1005, step time: 0.1135\n",
      "22/223, train_loss: 0.1042, step time: 0.1074\n",
      "23/223, train_loss: 0.1078, step time: 0.1169\n",
      "24/223, train_loss: 0.0981, step time: 0.1029\n",
      "25/223, train_loss: 0.0888, step time: 0.1200\n",
      "26/223, train_loss: 0.0932, step time: 0.1143\n",
      "27/223, train_loss: 0.1015, step time: 0.1657\n",
      "28/223, train_loss: 0.1021, step time: 0.1012\n",
      "29/223, train_loss: 0.1027, step time: 0.0988\n",
      "30/223, train_loss: 0.0997, step time: 0.1146\n",
      "31/223, train_loss: 0.0964, step time: 0.1151\n",
      "32/223, train_loss: 0.1004, step time: 0.1150\n",
      "33/223, train_loss: 0.1105, step time: 0.1118\n",
      "34/223, train_loss: 0.1022, step time: 0.0997\n",
      "35/223, train_loss: 0.1023, step time: 0.1109\n",
      "36/223, train_loss: 0.0918, step time: 0.1100\n",
      "37/223, train_loss: 0.1007, step time: 0.1254\n",
      "38/223, train_loss: 0.0942, step time: 0.1227\n",
      "39/223, train_loss: 0.1183, step time: 0.1011\n",
      "40/223, train_loss: 0.1017, step time: 0.1166\n",
      "41/223, train_loss: 0.1021, step time: 0.1065\n",
      "42/223, train_loss: 0.1144, step time: 0.1238\n",
      "43/223, train_loss: 0.1099, step time: 0.1044\n",
      "44/223, train_loss: 0.1079, step time: 0.0998\n",
      "45/223, train_loss: 0.0915, step time: 0.1000\n",
      "46/223, train_loss: 0.1013, step time: 0.1096\n",
      "47/223, train_loss: 0.1056, step time: 0.0996\n",
      "48/223, train_loss: 0.0980, step time: 0.1133\n",
      "49/223, train_loss: 0.0951, step time: 0.1148\n",
      "50/223, train_loss: 0.1092, step time: 0.1155\n",
      "51/223, train_loss: 0.0907, step time: 0.1137\n",
      "52/223, train_loss: 0.0903, step time: 0.0998\n",
      "53/223, train_loss: 0.1043, step time: 0.1136\n",
      "54/223, train_loss: 0.0922, step time: 0.1534\n",
      "55/223, train_loss: 0.1007, step time: 0.1497\n",
      "56/223, train_loss: 0.0967, step time: 0.1148\n",
      "57/223, train_loss: 0.1085, step time: 0.1059\n",
      "58/223, train_loss: 0.1014, step time: 0.1168\n",
      "59/223, train_loss: 0.0945, step time: 0.1007\n",
      "60/223, train_loss: 0.1044, step time: 0.1010\n",
      "61/223, train_loss: 0.1040, step time: 0.1339\n",
      "62/223, train_loss: 0.0983, step time: 0.1037\n",
      "63/223, train_loss: 0.1092, step time: 0.1331\n",
      "64/223, train_loss: 0.0967, step time: 0.1100\n",
      "65/223, train_loss: 0.0966, step time: 0.1099\n",
      "66/223, train_loss: 0.1065, step time: 0.1325\n",
      "67/223, train_loss: 0.0976, step time: 0.1309\n",
      "68/223, train_loss: 0.0930, step time: 0.1003\n",
      "69/223, train_loss: 0.1063, step time: 0.1063\n",
      "70/223, train_loss: 0.1013, step time: 0.1146\n",
      "71/223, train_loss: 0.1059, step time: 0.1141\n",
      "72/223, train_loss: 0.0923, step time: 0.1006\n",
      "73/223, train_loss: 0.0907, step time: 0.1091\n",
      "74/223, train_loss: 0.1020, step time: 0.1033\n",
      "75/223, train_loss: 0.0976, step time: 0.1003\n",
      "76/223, train_loss: 0.1113, step time: 0.1036\n",
      "77/223, train_loss: 0.0918, step time: 0.1055\n",
      "78/223, train_loss: 0.0964, step time: 0.1003\n",
      "79/223, train_loss: 0.0902, step time: 0.1008\n",
      "80/223, train_loss: 0.1047, step time: 0.1001\n",
      "81/223, train_loss: 0.1036, step time: 0.1105\n",
      "82/223, train_loss: 0.0983, step time: 0.1002\n",
      "83/223, train_loss: 0.1006, step time: 0.1068\n",
      "84/223, train_loss: 0.1141, step time: 0.1009\n",
      "85/223, train_loss: 0.1094, step time: 0.1008\n",
      "86/223, train_loss: 0.1066, step time: 0.1038\n",
      "87/223, train_loss: 0.0917, step time: 0.1231\n",
      "88/223, train_loss: 0.0923, step time: 0.1193\n",
      "89/223, train_loss: 0.1010, step time: 0.1125\n",
      "90/223, train_loss: 0.0982, step time: 0.1012\n",
      "91/223, train_loss: 0.1086, step time: 0.1312\n",
      "92/223, train_loss: 0.1092, step time: 0.1302\n",
      "93/223, train_loss: 0.1025, step time: 0.1037\n",
      "94/223, train_loss: 0.0979, step time: 0.1004\n",
      "95/223, train_loss: 0.1154, step time: 0.1008\n",
      "96/223, train_loss: 0.0854, step time: 0.1024\n",
      "97/223, train_loss: 0.1114, step time: 0.1040\n",
      "98/223, train_loss: 0.1011, step time: 0.1008\n",
      "99/223, train_loss: 0.0945, step time: 0.1066\n",
      "100/223, train_loss: 0.1074, step time: 0.1035\n",
      "101/223, train_loss: 0.1022, step time: 0.1157\n",
      "102/223, train_loss: 0.0979, step time: 0.1008\n",
      "103/223, train_loss: 0.1090, step time: 0.1179\n",
      "104/223, train_loss: 0.1078, step time: 0.1032\n",
      "105/223, train_loss: 0.1043, step time: 0.1173\n",
      "106/223, train_loss: 0.0910, step time: 0.1076\n",
      "107/223, train_loss: 0.1028, step time: 0.1030\n",
      "108/223, train_loss: 0.0990, step time: 0.1226\n",
      "109/223, train_loss: 0.1089, step time: 0.1136\n",
      "110/223, train_loss: 0.1047, step time: 0.1027\n",
      "111/223, train_loss: 0.0956, step time: 0.1066\n",
      "112/223, train_loss: 0.0937, step time: 0.1233\n",
      "113/223, train_loss: 0.0926, step time: 0.1139\n",
      "114/223, train_loss: 0.1080, step time: 0.1092\n",
      "115/223, train_loss: 0.1060, step time: 0.1087\n",
      "116/223, train_loss: 0.1082, step time: 0.1190\n",
      "117/223, train_loss: 0.0969, step time: 0.1167\n",
      "118/223, train_loss: 0.1022, step time: 0.1017\n",
      "119/223, train_loss: 0.0910, step time: 0.1296\n",
      "120/223, train_loss: 0.1007, step time: 0.1129\n",
      "121/223, train_loss: 0.1020, step time: 0.1127\n",
      "122/223, train_loss: 0.0994, step time: 0.1130\n",
      "123/223, train_loss: 0.1027, step time: 0.1293\n",
      "124/223, train_loss: 0.1011, step time: 0.1120\n",
      "125/223, train_loss: 0.1060, step time: 0.1115\n",
      "126/223, train_loss: 0.1026, step time: 0.1194\n",
      "127/223, train_loss: 0.0912, step time: 0.1143\n",
      "128/223, train_loss: 0.0942, step time: 0.1208\n",
      "129/223, train_loss: 0.0936, step time: 0.1167\n",
      "130/223, train_loss: 0.1082, step time: 0.1046\n",
      "131/223, train_loss: 0.0996, step time: 0.1003\n",
      "132/223, train_loss: 0.0989, step time: 0.1046\n",
      "133/223, train_loss: 0.0989, step time: 0.1207\n",
      "134/223, train_loss: 0.0941, step time: 0.1002\n",
      "135/223, train_loss: 0.1048, step time: 0.1003\n",
      "136/223, train_loss: 0.0992, step time: 0.1005\n",
      "137/223, train_loss: 0.0996, step time: 0.1366\n",
      "138/223, train_loss: 0.0972, step time: 0.1360\n",
      "139/223, train_loss: 0.1164, step time: 0.1004\n",
      "140/223, train_loss: 0.1074, step time: 0.1003\n",
      "141/223, train_loss: 0.0993, step time: 0.1113\n",
      "142/223, train_loss: 0.1029, step time: 0.1031\n",
      "143/223, train_loss: 0.1029, step time: 0.1164\n",
      "144/223, train_loss: 0.1111, step time: 0.1004\n",
      "145/223, train_loss: 0.0966, step time: 0.1008\n",
      "146/223, train_loss: 0.1008, step time: 0.1002\n",
      "147/223, train_loss: 0.1147, step time: 0.1109\n",
      "148/223, train_loss: 0.0912, step time: 0.1038\n",
      "149/223, train_loss: 0.1107, step time: 0.1096\n",
      "150/223, train_loss: 0.1021, step time: 0.1169\n",
      "151/223, train_loss: 0.1009, step time: 0.1239\n",
      "152/223, train_loss: 0.1039, step time: 0.1022\n",
      "153/223, train_loss: 0.1027, step time: 0.1075\n",
      "154/223, train_loss: 0.1104, step time: 0.1284\n",
      "155/223, train_loss: 0.1042, step time: 0.1404\n",
      "156/223, train_loss: 0.0963, step time: 0.1254\n",
      "157/223, train_loss: 0.1019, step time: 0.1005\n",
      "158/223, train_loss: 0.0929, step time: 0.1010\n",
      "159/223, train_loss: 0.0933, step time: 0.1003\n",
      "160/223, train_loss: 0.0980, step time: 0.1102\n",
      "161/223, train_loss: 0.1046, step time: 0.1043\n",
      "162/223, train_loss: 0.0902, step time: 0.1107\n",
      "163/223, train_loss: 0.1046, step time: 0.1122\n",
      "164/223, train_loss: 0.1004, step time: 0.1062\n",
      "165/223, train_loss: 0.1088, step time: 0.1067\n",
      "166/223, train_loss: 0.0983, step time: 0.1099\n",
      "167/223, train_loss: 0.0980, step time: 0.1129\n",
      "168/223, train_loss: 0.0997, step time: 0.1529\n",
      "169/223, train_loss: 0.1043, step time: 0.1003\n",
      "170/223, train_loss: 0.0986, step time: 0.0990\n",
      "171/223, train_loss: 0.1045, step time: 0.1000\n",
      "172/223, train_loss: 0.0994, step time: 0.1018\n",
      "173/223, train_loss: 0.1080, step time: 0.1001\n",
      "174/223, train_loss: 0.1024, step time: 0.0991\n",
      "175/223, train_loss: 0.2969, step time: 0.1005\n",
      "176/223, train_loss: 0.1043, step time: 0.0997\n",
      "177/223, train_loss: 0.1033, step time: 0.0992\n",
      "178/223, train_loss: 0.1084, step time: 0.0993\n",
      "179/223, train_loss: 0.1073, step time: 0.1005\n",
      "180/223, train_loss: 0.0951, step time: 0.1152\n",
      "181/223, train_loss: 0.0959, step time: 0.0991\n",
      "182/223, train_loss: 0.1044, step time: 0.0999\n",
      "183/223, train_loss: 0.0945, step time: 0.1217\n",
      "184/223, train_loss: 0.1087, step time: 0.1188\n",
      "185/223, train_loss: 0.1124, step time: 0.0999\n",
      "186/223, train_loss: 0.0974, step time: 0.1014\n",
      "187/223, train_loss: 0.1006, step time: 0.1053\n",
      "188/223, train_loss: 0.0896, step time: 0.1053\n",
      "189/223, train_loss: 0.1099, step time: 0.1084\n",
      "190/223, train_loss: 0.0898, step time: 0.1038\n",
      "191/223, train_loss: 0.1029, step time: 0.0996\n",
      "192/223, train_loss: 0.0980, step time: 0.1123\n",
      "193/223, train_loss: 0.0926, step time: 0.1029\n",
      "194/223, train_loss: 0.1077, step time: 0.1009\n",
      "195/223, train_loss: 0.1046, step time: 0.1103\n",
      "196/223, train_loss: 0.1091, step time: 0.1329\n",
      "197/223, train_loss: 0.1029, step time: 0.1334\n",
      "198/223, train_loss: 0.0969, step time: 0.1102\n",
      "199/223, train_loss: 0.1070, step time: 0.1131\n",
      "200/223, train_loss: 0.1055, step time: 0.1046\n",
      "201/223, train_loss: 0.0925, step time: 0.1041\n",
      "202/223, train_loss: 0.0909, step time: 0.1128\n",
      "203/223, train_loss: 0.1004, step time: 0.1303\n",
      "204/223, train_loss: 0.1032, step time: 0.1356\n",
      "205/223, train_loss: 0.0972, step time: 0.0999\n",
      "206/223, train_loss: 0.0960, step time: 0.1006\n",
      "207/223, train_loss: 0.0938, step time: 0.1232\n",
      "208/223, train_loss: 0.1112, step time: 0.1010\n",
      "209/223, train_loss: 0.1004, step time: 0.1069\n",
      "210/223, train_loss: 0.1013, step time: 0.0992\n",
      "211/223, train_loss: 0.1101, step time: 0.0986\n",
      "212/223, train_loss: 0.1065, step time: 0.0989\n",
      "213/223, train_loss: 0.1027, step time: 0.1162\n",
      "214/223, train_loss: 0.1011, step time: 0.1166\n",
      "215/223, train_loss: 0.0941, step time: 0.1082\n",
      "216/223, train_loss: 0.1060, step time: 0.1019\n",
      "217/223, train_loss: 0.1052, step time: 0.0999\n",
      "218/223, train_loss: 0.0916, step time: 0.1006\n",
      "219/223, train_loss: 0.1145, step time: 0.1000\n",
      "220/223, train_loss: 0.1097, step time: 0.1184\n",
      "221/223, train_loss: 0.1041, step time: 0.0997\n",
      "222/223, train_loss: 0.0927, step time: 0.0994\n",
      "223/223, train_loss: 0.1049, step time: 0.0994\n",
      "epoch 206 average loss: 0.1022\n",
      "time consuming of epoch 206 is: 94.0315\n",
      "----------\n",
      "epoch 207/300\n",
      "1/223, train_loss: 0.1126, step time: 0.1140\n",
      "2/223, train_loss: 0.1006, step time: 0.1104\n",
      "3/223, train_loss: 0.0998, step time: 0.1046\n",
      "4/223, train_loss: 0.1042, step time: 0.1106\n",
      "5/223, train_loss: 0.0949, step time: 0.1027\n",
      "6/223, train_loss: 0.0949, step time: 0.1003\n",
      "7/223, train_loss: 0.1027, step time: 0.0998\n",
      "8/223, train_loss: 0.0882, step time: 0.1331\n",
      "9/223, train_loss: 0.1015, step time: 0.0999\n",
      "10/223, train_loss: 0.0977, step time: 0.1000\n",
      "11/223, train_loss: 0.0952, step time: 0.1010\n",
      "12/223, train_loss: 0.1011, step time: 0.1042\n",
      "13/223, train_loss: 0.1052, step time: 0.1000\n",
      "14/223, train_loss: 0.1023, step time: 0.1003\n",
      "15/223, train_loss: 0.0990, step time: 0.1008\n",
      "16/223, train_loss: 0.1000, step time: 0.1432\n",
      "17/223, train_loss: 0.1045, step time: 0.0997\n",
      "18/223, train_loss: 0.1087, step time: 0.1000\n",
      "19/223, train_loss: 0.0965, step time: 0.1001\n",
      "20/223, train_loss: 0.1004, step time: 0.1033\n",
      "21/223, train_loss: 0.0954, step time: 0.1001\n",
      "22/223, train_loss: 0.1062, step time: 0.0998\n",
      "23/223, train_loss: 0.0951, step time: 0.1002\n",
      "24/223, train_loss: 0.1110, step time: 0.1001\n",
      "25/223, train_loss: 0.0954, step time: 0.1005\n",
      "26/223, train_loss: 0.1009, step time: 0.0999\n",
      "27/223, train_loss: 0.1002, step time: 0.1005\n",
      "28/223, train_loss: 0.1047, step time: 0.1024\n",
      "29/223, train_loss: 0.0872, step time: 0.0999\n",
      "30/223, train_loss: 0.0964, step time: 0.1005\n",
      "31/223, train_loss: 0.1102, step time: 0.1006\n",
      "32/223, train_loss: 0.0943, step time: 0.1279\n",
      "33/223, train_loss: 0.1020, step time: 0.1162\n",
      "34/223, train_loss: 0.1038, step time: 0.1154\n",
      "35/223, train_loss: 0.1145, step time: 0.1315\n",
      "36/223, train_loss: 0.1005, step time: 0.1268\n",
      "37/223, train_loss: 0.0891, step time: 0.1053\n",
      "38/223, train_loss: 0.1160, step time: 0.1003\n",
      "39/223, train_loss: 0.1015, step time: 0.1003\n",
      "40/223, train_loss: 0.2942, step time: 0.1060\n",
      "41/223, train_loss: 0.1008, step time: 0.1303\n",
      "42/223, train_loss: 0.0931, step time: 0.1008\n",
      "43/223, train_loss: 0.0967, step time: 0.0995\n",
      "44/223, train_loss: 0.0983, step time: 0.1139\n",
      "45/223, train_loss: 0.0927, step time: 0.1109\n",
      "46/223, train_loss: 0.0989, step time: 0.1259\n",
      "47/223, train_loss: 0.1063, step time: 0.1270\n",
      "48/223, train_loss: 0.1075, step time: 0.1097\n",
      "49/223, train_loss: 0.1103, step time: 0.1005\n",
      "50/223, train_loss: 0.1028, step time: 0.1127\n",
      "51/223, train_loss: 0.1172, step time: 0.1295\n",
      "52/223, train_loss: 0.0956, step time: 0.1028\n",
      "53/223, train_loss: 0.1030, step time: 0.1074\n",
      "54/223, train_loss: 0.1140, step time: 0.1068\n",
      "55/223, train_loss: 0.1018, step time: 0.1002\n",
      "56/223, train_loss: 0.0990, step time: 0.1012\n",
      "57/223, train_loss: 0.1054, step time: 0.1063\n",
      "58/223, train_loss: 0.1032, step time: 0.0998\n",
      "59/223, train_loss: 0.0983, step time: 0.1004\n",
      "60/223, train_loss: 0.1013, step time: 0.1008\n",
      "61/223, train_loss: 0.0988, step time: 0.1154\n",
      "62/223, train_loss: 0.1101, step time: 0.1011\n",
      "63/223, train_loss: 0.0991, step time: 0.1012\n",
      "64/223, train_loss: 0.1015, step time: 0.1109\n",
      "65/223, train_loss: 0.0958, step time: 0.1109\n",
      "66/223, train_loss: 0.0932, step time: 0.1003\n",
      "67/223, train_loss: 0.1029, step time: 0.1036\n",
      "68/223, train_loss: 0.0973, step time: 0.1025\n",
      "69/223, train_loss: 0.1055, step time: 0.1148\n",
      "70/223, train_loss: 0.0922, step time: 0.1465\n",
      "71/223, train_loss: 0.1011, step time: 0.1113\n",
      "72/223, train_loss: 0.0971, step time: 0.1431\n",
      "73/223, train_loss: 0.0907, step time: 0.1130\n",
      "74/223, train_loss: 0.0967, step time: 0.1052\n",
      "75/223, train_loss: 0.1008, step time: 0.1129\n",
      "76/223, train_loss: 0.1227, step time: 0.1172\n",
      "77/223, train_loss: 0.1023, step time: 0.1385\n",
      "78/223, train_loss: 0.0975, step time: 0.1002\n",
      "79/223, train_loss: 0.0929, step time: 0.1107\n",
      "80/223, train_loss: 0.0886, step time: 0.1450\n",
      "81/223, train_loss: 0.1077, step time: 0.1148\n",
      "82/223, train_loss: 0.1076, step time: 0.1198\n",
      "83/223, train_loss: 0.0910, step time: 0.1192\n",
      "84/223, train_loss: 0.0981, step time: 0.1013\n",
      "85/223, train_loss: 0.1031, step time: 0.1162\n",
      "86/223, train_loss: 0.0979, step time: 0.1011\n",
      "87/223, train_loss: 0.1030, step time: 0.1003\n",
      "88/223, train_loss: 0.1094, step time: 0.1146\n",
      "89/223, train_loss: 0.0968, step time: 0.1158\n",
      "90/223, train_loss: 0.1089, step time: 0.1942\n",
      "91/223, train_loss: 0.1032, step time: 0.1338\n",
      "92/223, train_loss: 0.0910, step time: 0.1006\n",
      "93/223, train_loss: 0.0999, step time: 0.1150\n",
      "94/223, train_loss: 0.0990, step time: 0.1103\n",
      "95/223, train_loss: 0.1020, step time: 0.1098\n",
      "96/223, train_loss: 0.1108, step time: 0.1578\n",
      "97/223, train_loss: 0.1097, step time: 0.1322\n",
      "98/223, train_loss: 0.0935, step time: 0.1321\n",
      "99/223, train_loss: 0.0986, step time: 0.1180\n",
      "100/223, train_loss: 0.0925, step time: 0.1203\n",
      "101/223, train_loss: 0.1129, step time: 0.1006\n",
      "102/223, train_loss: 0.1006, step time: 0.1013\n",
      "103/223, train_loss: 0.1006, step time: 0.1008\n",
      "104/223, train_loss: 0.1069, step time: 0.1005\n",
      "105/223, train_loss: 0.1190, step time: 0.1162\n",
      "106/223, train_loss: 0.1069, step time: 0.1221\n",
      "107/223, train_loss: 0.1050, step time: 0.1223\n",
      "108/223, train_loss: 0.0936, step time: 0.1002\n",
      "109/223, train_loss: 0.1082, step time: 0.0988\n",
      "110/223, train_loss: 0.0982, step time: 0.1120\n",
      "111/223, train_loss: 0.1046, step time: 0.1296\n",
      "112/223, train_loss: 0.0976, step time: 0.1431\n",
      "113/223, train_loss: 0.1069, step time: 0.1151\n",
      "114/223, train_loss: 0.1048, step time: 0.1163\n",
      "115/223, train_loss: 0.1042, step time: 0.1359\n",
      "116/223, train_loss: 0.0909, step time: 0.0998\n",
      "117/223, train_loss: 0.0934, step time: 0.1121\n",
      "118/223, train_loss: 0.1097, step time: 0.0999\n",
      "119/223, train_loss: 0.0896, step time: 0.1258\n",
      "120/223, train_loss: 0.1011, step time: 0.1170\n",
      "121/223, train_loss: 0.0988, step time: 0.1101\n",
      "122/223, train_loss: 0.0972, step time: 0.1194\n",
      "123/223, train_loss: 0.1085, step time: 0.1099\n",
      "124/223, train_loss: 0.0996, step time: 0.1047\n",
      "125/223, train_loss: 0.1034, step time: 0.1043\n",
      "126/223, train_loss: 0.1015, step time: 0.1104\n",
      "127/223, train_loss: 0.1024, step time: 0.1002\n",
      "128/223, train_loss: 0.0984, step time: 0.1030\n",
      "129/223, train_loss: 0.1077, step time: 0.1151\n",
      "130/223, train_loss: 0.1019, step time: 0.1222\n",
      "131/223, train_loss: 0.1144, step time: 0.1172\n",
      "132/223, train_loss: 0.1016, step time: 0.1003\n",
      "133/223, train_loss: 0.0943, step time: 0.1269\n",
      "134/223, train_loss: 0.0932, step time: 0.1226\n",
      "135/223, train_loss: 0.0966, step time: 0.1047\n",
      "136/223, train_loss: 0.1006, step time: 0.1125\n",
      "137/223, train_loss: 0.1109, step time: 0.1064\n",
      "138/223, train_loss: 0.0903, step time: 0.1240\n",
      "139/223, train_loss: 0.0977, step time: 0.1137\n",
      "140/223, train_loss: 0.1044, step time: 0.1065\n",
      "141/223, train_loss: 0.0931, step time: 0.1162\n",
      "142/223, train_loss: 0.1076, step time: 0.1145\n",
      "143/223, train_loss: 0.1039, step time: 0.1251\n",
      "144/223, train_loss: 0.1158, step time: 0.1099\n",
      "145/223, train_loss: 0.1031, step time: 0.1152\n",
      "146/223, train_loss: 0.1076, step time: 0.1070\n",
      "147/223, train_loss: 0.1042, step time: 0.1081\n",
      "148/223, train_loss: 0.1001, step time: 0.1011\n",
      "149/223, train_loss: 0.0919, step time: 0.1056\n",
      "150/223, train_loss: 0.0981, step time: 0.1581\n",
      "151/223, train_loss: 0.1047, step time: 0.1302\n",
      "152/223, train_loss: 0.1063, step time: 0.1157\n",
      "153/223, train_loss: 0.0983, step time: 0.1122\n",
      "154/223, train_loss: 0.0967, step time: 0.1004\n",
      "155/223, train_loss: 0.0947, step time: 0.1071\n",
      "156/223, train_loss: 0.0971, step time: 0.1016\n",
      "157/223, train_loss: 0.1103, step time: 0.1104\n",
      "158/223, train_loss: 0.1113, step time: 0.1081\n",
      "159/223, train_loss: 0.0939, step time: 0.1005\n",
      "160/223, train_loss: 0.1012, step time: 0.1014\n",
      "161/223, train_loss: 0.0923, step time: 0.1080\n",
      "162/223, train_loss: 0.1031, step time: 0.1060\n",
      "163/223, train_loss: 0.1082, step time: 0.1012\n",
      "164/223, train_loss: 0.1021, step time: 0.1021\n",
      "165/223, train_loss: 0.1001, step time: 0.1159\n",
      "166/223, train_loss: 0.1033, step time: 0.1097\n",
      "167/223, train_loss: 0.1124, step time: 0.1104\n",
      "168/223, train_loss: 0.1047, step time: 0.1115\n",
      "169/223, train_loss: 0.0964, step time: 0.1063\n",
      "170/223, train_loss: 0.1064, step time: 0.1052\n",
      "171/223, train_loss: 0.0931, step time: 0.1111\n",
      "172/223, train_loss: 0.1045, step time: 0.1145\n",
      "173/223, train_loss: 0.0920, step time: 0.1588\n",
      "174/223, train_loss: 0.1019, step time: 0.1108\n",
      "175/223, train_loss: 0.0919, step time: 0.1001\n",
      "176/223, train_loss: 0.1032, step time: 0.1005\n",
      "177/223, train_loss: 0.0975, step time: 0.1386\n",
      "178/223, train_loss: 0.0974, step time: 0.1045\n",
      "179/223, train_loss: 0.1024, step time: 0.1203\n",
      "180/223, train_loss: 0.0925, step time: 0.1001\n",
      "181/223, train_loss: 0.0964, step time: 0.1146\n",
      "182/223, train_loss: 0.1021, step time: 0.1114\n",
      "183/223, train_loss: 0.1154, step time: 0.1127\n",
      "184/223, train_loss: 0.1016, step time: 0.1006\n",
      "185/223, train_loss: 0.1103, step time: 0.1087\n",
      "186/223, train_loss: 0.0919, step time: 0.1005\n",
      "187/223, train_loss: 0.0960, step time: 0.1121\n",
      "188/223, train_loss: 0.1032, step time: 0.1230\n",
      "189/223, train_loss: 0.1094, step time: 0.1590\n",
      "190/223, train_loss: 0.1046, step time: 0.1340\n",
      "191/223, train_loss: 0.1027, step time: 0.1009\n",
      "192/223, train_loss: 0.1013, step time: 0.1031\n",
      "193/223, train_loss: 0.1042, step time: 0.1056\n",
      "194/223, train_loss: 0.1025, step time: 0.1000\n",
      "195/223, train_loss: 0.1060, step time: 0.1005\n",
      "196/223, train_loss: 0.0909, step time: 0.1160\n",
      "197/223, train_loss: 0.0942, step time: 0.1069\n",
      "198/223, train_loss: 0.1036, step time: 0.0992\n",
      "199/223, train_loss: 0.1059, step time: 0.1175\n",
      "200/223, train_loss: 0.0926, step time: 0.1060\n",
      "201/223, train_loss: 0.1107, step time: 0.1177\n",
      "202/223, train_loss: 0.1197, step time: 0.1126\n",
      "203/223, train_loss: 0.1018, step time: 0.1246\n",
      "204/223, train_loss: 0.0959, step time: 0.1108\n",
      "205/223, train_loss: 0.0959, step time: 0.1178\n",
      "206/223, train_loss: 0.0981, step time: 0.1002\n",
      "207/223, train_loss: 0.1094, step time: 0.1002\n",
      "208/223, train_loss: 0.0974, step time: 0.1184\n",
      "209/223, train_loss: 0.1023, step time: 0.1091\n",
      "210/223, train_loss: 0.1079, step time: 0.1495\n",
      "211/223, train_loss: 0.1116, step time: 0.1240\n",
      "212/223, train_loss: 0.1107, step time: 0.1008\n",
      "213/223, train_loss: 0.0909, step time: 0.1107\n",
      "214/223, train_loss: 0.0966, step time: 0.1194\n",
      "215/223, train_loss: 0.1021, step time: 0.1048\n",
      "216/223, train_loss: 0.1116, step time: 0.1260\n",
      "217/223, train_loss: 0.0947, step time: 0.1006\n",
      "218/223, train_loss: 0.1057, step time: 0.1003\n",
      "219/223, train_loss: 0.0933, step time: 0.0998\n",
      "220/223, train_loss: 0.1045, step time: 0.1083\n",
      "221/223, train_loss: 0.0928, step time: 0.0999\n",
      "222/223, train_loss: 0.0954, step time: 0.1003\n",
      "223/223, train_loss: 0.1011, step time: 0.1004\n",
      "epoch 207 average loss: 0.1022\n",
      "time consuming of epoch 207 is: 95.3161\n",
      "----------\n",
      "epoch 208/300\n",
      "1/223, train_loss: 0.1001, step time: 0.1072\n",
      "2/223, train_loss: 0.0931, step time: 0.1066\n",
      "3/223, train_loss: 0.0968, step time: 0.1174\n",
      "4/223, train_loss: 0.0977, step time: 0.1065\n",
      "5/223, train_loss: 0.1289, step time: 0.1135\n",
      "6/223, train_loss: 0.1074, step time: 0.1091\n",
      "7/223, train_loss: 0.1017, step time: 0.1252\n",
      "8/223, train_loss: 0.1027, step time: 0.1097\n",
      "9/223, train_loss: 0.1011, step time: 0.1009\n",
      "10/223, train_loss: 0.1051, step time: 0.1203\n",
      "11/223, train_loss: 0.1041, step time: 0.1098\n",
      "12/223, train_loss: 0.0943, step time: 0.1301\n",
      "13/223, train_loss: 0.1051, step time: 0.1010\n",
      "14/223, train_loss: 0.1085, step time: 0.1154\n",
      "15/223, train_loss: 0.1038, step time: 0.1064\n",
      "16/223, train_loss: 0.1189, step time: 0.1209\n",
      "17/223, train_loss: 0.1007, step time: 0.1047\n",
      "18/223, train_loss: 0.1002, step time: 0.1174\n",
      "19/223, train_loss: 0.0916, step time: 0.1049\n",
      "20/223, train_loss: 0.1033, step time: 0.1299\n",
      "21/223, train_loss: 0.0992, step time: 0.1151\n",
      "22/223, train_loss: 0.0980, step time: 0.1130\n",
      "23/223, train_loss: 0.0944, step time: 0.1259\n",
      "24/223, train_loss: 0.1183, step time: 0.1180\n",
      "25/223, train_loss: 0.1102, step time: 0.1137\n",
      "26/223, train_loss: 0.0996, step time: 0.1246\n",
      "27/223, train_loss: 0.0941, step time: 0.1061\n",
      "28/223, train_loss: 0.0904, step time: 0.1107\n",
      "29/223, train_loss: 0.0992, step time: 0.1016\n",
      "30/223, train_loss: 0.1048, step time: 0.1081\n",
      "31/223, train_loss: 0.1101, step time: 0.1015\n",
      "32/223, train_loss: 0.1056, step time: 0.1198\n",
      "33/223, train_loss: 0.0954, step time: 0.1112\n",
      "34/223, train_loss: 0.0986, step time: 0.1105\n",
      "35/223, train_loss: 0.1033, step time: 0.1078\n",
      "36/223, train_loss: 0.1058, step time: 0.1106\n",
      "37/223, train_loss: 0.1015, step time: 0.1111\n",
      "38/223, train_loss: 0.1015, step time: 0.1171\n",
      "39/223, train_loss: 0.0931, step time: 0.1065\n",
      "40/223, train_loss: 0.1023, step time: 0.1059\n",
      "41/223, train_loss: 0.1108, step time: 0.1106\n",
      "42/223, train_loss: 0.0972, step time: 0.1096\n",
      "43/223, train_loss: 0.1064, step time: 0.1068\n",
      "44/223, train_loss: 0.1007, step time: 0.1245\n",
      "45/223, train_loss: 0.1038, step time: 0.1134\n",
      "46/223, train_loss: 0.0918, step time: 0.1095\n",
      "47/223, train_loss: 0.0919, step time: 0.1018\n",
      "48/223, train_loss: 0.0948, step time: 0.1230\n",
      "49/223, train_loss: 0.0954, step time: 0.1003\n",
      "50/223, train_loss: 0.0948, step time: 0.1151\n",
      "51/223, train_loss: 0.0985, step time: 0.1127\n",
      "52/223, train_loss: 0.0957, step time: 0.1236\n",
      "53/223, train_loss: 0.0959, step time: 0.1139\n",
      "54/223, train_loss: 0.0983, step time: 0.1121\n",
      "55/223, train_loss: 0.1026, step time: 0.1054\n",
      "56/223, train_loss: 0.0954, step time: 0.1203\n",
      "57/223, train_loss: 0.0916, step time: 0.1044\n",
      "58/223, train_loss: 0.1019, step time: 0.1015\n",
      "59/223, train_loss: 0.0922, step time: 0.1088\n",
      "60/223, train_loss: 0.0926, step time: 0.1100\n",
      "61/223, train_loss: 0.0983, step time: 0.1036\n",
      "62/223, train_loss: 0.1017, step time: 0.1002\n",
      "63/223, train_loss: 0.1032, step time: 0.1001\n",
      "64/223, train_loss: 0.1063, step time: 0.1039\n",
      "65/223, train_loss: 0.1036, step time: 0.0996\n",
      "66/223, train_loss: 0.1066, step time: 0.1074\n",
      "67/223, train_loss: 0.0961, step time: 0.1154\n",
      "68/223, train_loss: 0.0979, step time: 0.1206\n",
      "69/223, train_loss: 0.1130, step time: 0.1212\n",
      "70/223, train_loss: 0.0906, step time: 0.1004\n",
      "71/223, train_loss: 0.1041, step time: 0.1005\n",
      "72/223, train_loss: 0.0954, step time: 0.1045\n",
      "73/223, train_loss: 0.1005, step time: 0.1170\n",
      "74/223, train_loss: 0.0944, step time: 0.1115\n",
      "75/223, train_loss: 0.1032, step time: 0.1102\n",
      "76/223, train_loss: 0.1002, step time: 0.1125\n",
      "77/223, train_loss: 0.0936, step time: 0.1137\n",
      "78/223, train_loss: 0.1017, step time: 0.1001\n",
      "79/223, train_loss: 0.0997, step time: 0.1005\n",
      "80/223, train_loss: 0.1054, step time: 0.1051\n",
      "81/223, train_loss: 0.0914, step time: 0.1015\n",
      "82/223, train_loss: 0.0974, step time: 0.1192\n",
      "83/223, train_loss: 0.1004, step time: 0.1140\n",
      "84/223, train_loss: 0.0943, step time: 0.1488\n",
      "85/223, train_loss: 0.1001, step time: 0.1068\n",
      "86/223, train_loss: 0.1006, step time: 0.1221\n",
      "87/223, train_loss: 0.1096, step time: 0.1547\n",
      "88/223, train_loss: 0.1016, step time: 0.1580\n",
      "89/223, train_loss: 0.1098, step time: 0.1018\n",
      "90/223, train_loss: 0.0983, step time: 0.0999\n",
      "91/223, train_loss: 0.1080, step time: 0.0995\n",
      "92/223, train_loss: 0.0927, step time: 0.0995\n",
      "93/223, train_loss: 0.1044, step time: 0.1025\n",
      "94/223, train_loss: 0.0939, step time: 0.1072\n",
      "95/223, train_loss: 0.0978, step time: 0.0998\n",
      "96/223, train_loss: 0.1149, step time: 0.0994\n",
      "97/223, train_loss: 0.1016, step time: 0.1106\n",
      "98/223, train_loss: 0.1068, step time: 0.1025\n",
      "99/223, train_loss: 0.0980, step time: 0.1302\n",
      "100/223, train_loss: 0.0932, step time: 0.1048\n",
      "101/223, train_loss: 0.1027, step time: 0.1017\n",
      "102/223, train_loss: 0.0935, step time: 0.1117\n",
      "103/223, train_loss: 0.0993, step time: 0.1140\n",
      "104/223, train_loss: 0.1103, step time: 0.1589\n",
      "105/223, train_loss: 0.1005, step time: 0.1101\n",
      "106/223, train_loss: 0.1051, step time: 0.1169\n",
      "107/223, train_loss: 0.1012, step time: 0.1110\n",
      "108/223, train_loss: 0.0971, step time: 0.1053\n",
      "109/223, train_loss: 0.0977, step time: 0.1204\n",
      "110/223, train_loss: 0.1042, step time: 0.1058\n",
      "111/223, train_loss: 0.1009, step time: 0.1378\n",
      "112/223, train_loss: 0.1041, step time: 0.1229\n",
      "113/223, train_loss: 0.1097, step time: 0.1071\n",
      "114/223, train_loss: 0.1087, step time: 0.1142\n",
      "115/223, train_loss: 0.0951, step time: 0.1054\n",
      "116/223, train_loss: 0.1013, step time: 0.1066\n",
      "117/223, train_loss: 0.1157, step time: 0.1243\n",
      "118/223, train_loss: 0.0891, step time: 0.1129\n",
      "119/223, train_loss: 0.1010, step time: 0.1209\n",
      "120/223, train_loss: 0.1017, step time: 0.1084\n",
      "121/223, train_loss: 0.1106, step time: 0.1089\n",
      "122/223, train_loss: 0.0936, step time: 0.1113\n",
      "123/223, train_loss: 0.1019, step time: 0.1045\n",
      "124/223, train_loss: 0.3071, step time: 0.1011\n",
      "125/223, train_loss: 0.0902, step time: 0.1011\n",
      "126/223, train_loss: 0.0946, step time: 0.1050\n",
      "127/223, train_loss: 0.1129, step time: 0.1003\n",
      "128/223, train_loss: 0.1017, step time: 0.1255\n",
      "129/223, train_loss: 0.0933, step time: 0.1000\n",
      "130/223, train_loss: 0.1085, step time: 0.1173\n",
      "131/223, train_loss: 0.0999, step time: 0.1078\n",
      "132/223, train_loss: 0.0935, step time: 0.1196\n",
      "133/223, train_loss: 0.0961, step time: 0.1323\n",
      "134/223, train_loss: 0.0931, step time: 0.1137\n",
      "135/223, train_loss: 0.0923, step time: 0.1280\n",
      "136/223, train_loss: 0.0995, step time: 0.1281\n",
      "137/223, train_loss: 0.0953, step time: 0.1360\n",
      "138/223, train_loss: 0.1143, step time: 0.1099\n",
      "139/223, train_loss: 0.1041, step time: 0.1314\n",
      "140/223, train_loss: 0.0962, step time: 0.1016\n",
      "141/223, train_loss: 0.1055, step time: 0.1003\n",
      "142/223, train_loss: 0.1098, step time: 0.1138\n",
      "143/223, train_loss: 0.0966, step time: 0.1421\n",
      "144/223, train_loss: 0.0964, step time: 0.1177\n",
      "145/223, train_loss: 0.1021, step time: 0.1370\n",
      "146/223, train_loss: 0.0946, step time: 0.1192\n",
      "147/223, train_loss: 0.1122, step time: 0.1155\n",
      "148/223, train_loss: 0.1093, step time: 0.1292\n",
      "149/223, train_loss: 0.1080, step time: 0.1017\n",
      "150/223, train_loss: 0.0950, step time: 0.1241\n",
      "151/223, train_loss: 0.1008, step time: 0.1099\n",
      "152/223, train_loss: 0.0991, step time: 0.1203\n",
      "153/223, train_loss: 0.1031, step time: 0.1034\n",
      "154/223, train_loss: 0.0989, step time: 0.1010\n",
      "155/223, train_loss: 0.0922, step time: 0.1008\n",
      "156/223, train_loss: 0.1169, step time: 0.1010\n",
      "157/223, train_loss: 0.1154, step time: 0.1116\n",
      "158/223, train_loss: 0.1010, step time: 0.1146\n",
      "159/223, train_loss: 0.0989, step time: 0.1294\n",
      "160/223, train_loss: 0.0976, step time: 0.1261\n",
      "161/223, train_loss: 0.0932, step time: 0.1150\n",
      "162/223, train_loss: 0.1035, step time: 0.1126\n",
      "163/223, train_loss: 0.1058, step time: 0.1055\n",
      "164/223, train_loss: 0.1147, step time: 0.1157\n",
      "165/223, train_loss: 0.0960, step time: 0.1147\n",
      "166/223, train_loss: 0.0921, step time: 0.1182\n",
      "167/223, train_loss: 0.1092, step time: 0.1154\n",
      "168/223, train_loss: 0.1094, step time: 0.1136\n",
      "169/223, train_loss: 0.1033, step time: 0.1031\n",
      "170/223, train_loss: 0.1045, step time: 0.1169\n",
      "171/223, train_loss: 0.0973, step time: 0.1017\n",
      "172/223, train_loss: 0.0942, step time: 0.1247\n",
      "173/223, train_loss: 0.0959, step time: 0.1153\n",
      "174/223, train_loss: 0.0960, step time: 0.1150\n",
      "175/223, train_loss: 0.0988, step time: 0.1172\n",
      "176/223, train_loss: 0.0959, step time: 0.1141\n",
      "177/223, train_loss: 0.0993, step time: 0.1035\n",
      "178/223, train_loss: 0.1047, step time: 0.1114\n",
      "179/223, train_loss: 0.1048, step time: 0.1405\n",
      "180/223, train_loss: 0.1082, step time: 0.1148\n",
      "181/223, train_loss: 0.1018, step time: 0.1238\n",
      "182/223, train_loss: 0.0992, step time: 0.1065\n",
      "183/223, train_loss: 0.1141, step time: 0.1037\n",
      "184/223, train_loss: 0.0939, step time: 0.1208\n",
      "185/223, train_loss: 0.0965, step time: 0.1257\n",
      "186/223, train_loss: 0.1047, step time: 0.0987\n",
      "187/223, train_loss: 0.0995, step time: 0.0982\n",
      "188/223, train_loss: 0.1020, step time: 0.0988\n",
      "189/223, train_loss: 0.1028, step time: 0.0986\n",
      "190/223, train_loss: 0.0970, step time: 0.1050\n",
      "191/223, train_loss: 0.1086, step time: 0.1091\n",
      "192/223, train_loss: 0.1113, step time: 0.1053\n",
      "193/223, train_loss: 0.1057, step time: 0.1011\n",
      "194/223, train_loss: 0.0911, step time: 0.1105\n",
      "195/223, train_loss: 0.1051, step time: 0.1381\n",
      "196/223, train_loss: 0.0951, step time: 0.1003\n",
      "197/223, train_loss: 0.0998, step time: 0.1004\n",
      "198/223, train_loss: 0.1074, step time: 0.1005\n",
      "199/223, train_loss: 0.1121, step time: 0.1007\n",
      "200/223, train_loss: 0.0975, step time: 0.1006\n",
      "201/223, train_loss: 0.1124, step time: 0.1031\n",
      "202/223, train_loss: 0.1126, step time: 0.1340\n",
      "203/223, train_loss: 0.0988, step time: 0.1037\n",
      "204/223, train_loss: 0.0935, step time: 0.1578\n",
      "205/223, train_loss: 0.0935, step time: 0.1105\n",
      "206/223, train_loss: 0.0978, step time: 0.1111\n",
      "207/223, train_loss: 0.0986, step time: 0.1093\n",
      "208/223, train_loss: 0.1074, step time: 0.1310\n",
      "209/223, train_loss: 0.1098, step time: 0.1121\n",
      "210/223, train_loss: 0.1036, step time: 0.1164\n",
      "211/223, train_loss: 0.1118, step time: 0.1221\n",
      "212/223, train_loss: 0.0899, step time: 0.1070\n",
      "213/223, train_loss: 0.0980, step time: 0.1112\n",
      "214/223, train_loss: 0.1006, step time: 0.1131\n",
      "215/223, train_loss: 0.0946, step time: 0.1064\n",
      "216/223, train_loss: 0.0919, step time: 0.1249\n",
      "217/223, train_loss: 0.0922, step time: 0.1283\n",
      "218/223, train_loss: 0.0956, step time: 0.1006\n",
      "219/223, train_loss: 0.1048, step time: 0.1002\n",
      "220/223, train_loss: 0.0932, step time: 0.1448\n",
      "221/223, train_loss: 0.0921, step time: 0.0993\n",
      "222/223, train_loss: 0.1132, step time: 0.0999\n",
      "223/223, train_loss: 0.0989, step time: 0.1001\n",
      "epoch 208 average loss: 0.1020\n",
      "time consuming of epoch 208 is: 88.4542\n",
      "----------\n",
      "epoch 209/300\n",
      "1/223, train_loss: 0.1019, step time: 0.1238\n",
      "2/223, train_loss: 0.0916, step time: 0.1001\n",
      "3/223, train_loss: 0.0941, step time: 0.1056\n",
      "4/223, train_loss: 0.1030, step time: 0.1262\n",
      "5/223, train_loss: 0.1118, step time: 0.1191\n",
      "6/223, train_loss: 0.1018, step time: 0.1082\n",
      "7/223, train_loss: 0.1022, step time: 0.1156\n",
      "8/223, train_loss: 0.1109, step time: 0.1489\n",
      "9/223, train_loss: 0.1030, step time: 0.1004\n",
      "10/223, train_loss: 0.1112, step time: 0.1185\n",
      "11/223, train_loss: 0.0935, step time: 0.1179\n",
      "12/223, train_loss: 0.0907, step time: 0.1009\n",
      "13/223, train_loss: 0.0994, step time: 0.1275\n",
      "14/223, train_loss: 0.0930, step time: 0.0999\n",
      "15/223, train_loss: 0.0959, step time: 0.1167\n",
      "16/223, train_loss: 0.0946, step time: 0.1010\n",
      "17/223, train_loss: 0.0972, step time: 0.1227\n",
      "18/223, train_loss: 0.1000, step time: 0.1266\n",
      "19/223, train_loss: 0.0958, step time: 0.1012\n",
      "20/223, train_loss: 0.0947, step time: 0.1138\n",
      "21/223, train_loss: 0.1091, step time: 0.1039\n",
      "22/223, train_loss: 0.0954, step time: 0.1047\n",
      "23/223, train_loss: 0.1135, step time: 0.1007\n",
      "24/223, train_loss: 0.0984, step time: 0.1004\n",
      "25/223, train_loss: 0.1066, step time: 0.1100\n",
      "26/223, train_loss: 0.1025, step time: 0.1003\n",
      "27/223, train_loss: 0.0989, step time: 0.1474\n",
      "28/223, train_loss: 0.1055, step time: 0.1023\n",
      "29/223, train_loss: 0.0924, step time: 0.1069\n",
      "30/223, train_loss: 0.0985, step time: 0.1085\n",
      "31/223, train_loss: 0.1105, step time: 0.1041\n",
      "32/223, train_loss: 0.1089, step time: 0.1003\n",
      "33/223, train_loss: 0.0970, step time: 0.1324\n",
      "34/223, train_loss: 0.0952, step time: 0.1146\n",
      "35/223, train_loss: 0.1012, step time: 0.1006\n",
      "36/223, train_loss: 0.1068, step time: 0.1010\n",
      "37/223, train_loss: 0.1029, step time: 0.1336\n",
      "38/223, train_loss: 0.1003, step time: 0.1850\n",
      "39/223, train_loss: 0.0877, step time: 0.1210\n",
      "40/223, train_loss: 0.0933, step time: 0.1003\n",
      "41/223, train_loss: 0.1029, step time: 0.1162\n",
      "42/223, train_loss: 0.0984, step time: 0.1004\n",
      "43/223, train_loss: 0.0980, step time: 0.1141\n",
      "44/223, train_loss: 0.0996, step time: 0.1011\n",
      "45/223, train_loss: 0.1102, step time: 0.1124\n",
      "46/223, train_loss: 0.0983, step time: 0.1031\n",
      "47/223, train_loss: 0.1115, step time: 0.1250\n",
      "48/223, train_loss: 0.1077, step time: 0.1053\n",
      "49/223, train_loss: 0.1120, step time: 0.1114\n",
      "50/223, train_loss: 0.0952, step time: 0.1207\n",
      "51/223, train_loss: 0.0983, step time: 0.1014\n",
      "52/223, train_loss: 0.0917, step time: 0.1041\n",
      "53/223, train_loss: 0.0912, step time: 0.0997\n",
      "54/223, train_loss: 0.0903, step time: 0.1420\n",
      "55/223, train_loss: 0.1051, step time: 0.1274\n",
      "56/223, train_loss: 0.0895, step time: 0.1103\n",
      "57/223, train_loss: 0.1025, step time: 0.1174\n",
      "58/223, train_loss: 0.1036, step time: 0.1217\n",
      "59/223, train_loss: 0.0967, step time: 0.1017\n",
      "60/223, train_loss: 0.1045, step time: 0.1025\n",
      "61/223, train_loss: 0.1012, step time: 0.1131\n",
      "62/223, train_loss: 0.0942, step time: 0.1085\n",
      "63/223, train_loss: 0.0913, step time: 0.1137\n",
      "64/223, train_loss: 0.0998, step time: 0.1131\n",
      "65/223, train_loss: 0.1120, step time: 0.1047\n",
      "66/223, train_loss: 0.1031, step time: 0.1017\n",
      "67/223, train_loss: 0.1129, step time: 0.1006\n",
      "68/223, train_loss: 0.1008, step time: 0.0999\n",
      "69/223, train_loss: 0.1105, step time: 0.1146\n",
      "70/223, train_loss: 0.1075, step time: 0.1153\n",
      "71/223, train_loss: 0.0904, step time: 0.1011\n",
      "72/223, train_loss: 0.1097, step time: 0.1231\n",
      "73/223, train_loss: 0.1092, step time: 0.1047\n",
      "74/223, train_loss: 0.0919, step time: 0.1021\n",
      "75/223, train_loss: 0.1104, step time: 0.1319\n",
      "76/223, train_loss: 0.1118, step time: 0.1227\n",
      "77/223, train_loss: 0.1009, step time: 0.1033\n",
      "78/223, train_loss: 0.0965, step time: 0.1155\n",
      "79/223, train_loss: 0.1038, step time: 0.1086\n",
      "80/223, train_loss: 0.0941, step time: 0.1011\n",
      "81/223, train_loss: 0.1023, step time: 0.1121\n",
      "82/223, train_loss: 0.1023, step time: 0.1127\n",
      "83/223, train_loss: 0.0980, step time: 0.1002\n",
      "84/223, train_loss: 0.1054, step time: 0.1008\n",
      "85/223, train_loss: 0.1021, step time: 0.1075\n",
      "86/223, train_loss: 0.1015, step time: 0.1027\n",
      "87/223, train_loss: 0.0955, step time: 0.1106\n",
      "88/223, train_loss: 0.0964, step time: 0.1148\n",
      "89/223, train_loss: 0.1181, step time: 0.1298\n",
      "90/223, train_loss: 0.1011, step time: 0.1410\n",
      "91/223, train_loss: 0.0935, step time: 0.1073\n",
      "92/223, train_loss: 0.1067, step time: 0.1010\n",
      "93/223, train_loss: 0.1001, step time: 0.1180\n",
      "94/223, train_loss: 0.1012, step time: 0.1010\n",
      "95/223, train_loss: 0.1013, step time: 0.1015\n",
      "96/223, train_loss: 0.1112, step time: 0.1275\n",
      "97/223, train_loss: 0.1004, step time: 0.1190\n",
      "98/223, train_loss: 0.1082, step time: 0.1007\n",
      "99/223, train_loss: 0.0936, step time: 0.1000\n",
      "100/223, train_loss: 0.1058, step time: 0.1014\n",
      "101/223, train_loss: 0.1028, step time: 0.1150\n",
      "102/223, train_loss: 0.1075, step time: 0.1043\n",
      "103/223, train_loss: 0.0991, step time: 0.1010\n",
      "104/223, train_loss: 0.1042, step time: 0.1054\n",
      "105/223, train_loss: 0.1012, step time: 0.1001\n",
      "106/223, train_loss: 0.1016, step time: 0.1447\n",
      "107/223, train_loss: 0.1112, step time: 0.1626\n",
      "108/223, train_loss: 0.1001, step time: 0.1162\n",
      "109/223, train_loss: 0.1006, step time: 0.1180\n",
      "110/223, train_loss: 0.1096, step time: 0.1047\n",
      "111/223, train_loss: 0.0966, step time: 0.1134\n",
      "112/223, train_loss: 0.0912, step time: 0.1173\n",
      "113/223, train_loss: 0.3014, step time: 0.1008\n",
      "114/223, train_loss: 0.1009, step time: 0.1276\n",
      "115/223, train_loss: 0.0915, step time: 0.1052\n",
      "116/223, train_loss: 0.0946, step time: 0.1032\n",
      "117/223, train_loss: 0.1170, step time: 0.1142\n",
      "118/223, train_loss: 0.0974, step time: 0.1155\n",
      "119/223, train_loss: 0.0966, step time: 0.1098\n",
      "120/223, train_loss: 0.0953, step time: 0.1058\n",
      "121/223, train_loss: 0.0858, step time: 0.1107\n",
      "122/223, train_loss: 0.0968, step time: 0.1260\n",
      "123/223, train_loss: 0.1106, step time: 0.1115\n",
      "124/223, train_loss: 0.0925, step time: 0.0992\n",
      "125/223, train_loss: 0.1083, step time: 0.1007\n",
      "126/223, train_loss: 0.0984, step time: 0.1516\n",
      "127/223, train_loss: 0.0978, step time: 0.1311\n",
      "128/223, train_loss: 0.1136, step time: 0.1013\n",
      "129/223, train_loss: 0.0984, step time: 0.1098\n",
      "130/223, train_loss: 0.1020, step time: 0.1076\n",
      "131/223, train_loss: 0.1012, step time: 0.1134\n",
      "132/223, train_loss: 0.0883, step time: 0.1246\n",
      "133/223, train_loss: 0.1047, step time: 0.1078\n",
      "134/223, train_loss: 0.1006, step time: 0.1130\n",
      "135/223, train_loss: 0.1035, step time: 0.1010\n",
      "136/223, train_loss: 0.0974, step time: 0.1246\n",
      "137/223, train_loss: 0.0973, step time: 0.1158\n",
      "138/223, train_loss: 0.0991, step time: 0.1178\n",
      "139/223, train_loss: 0.0920, step time: 0.1167\n",
      "140/223, train_loss: 0.1029, step time: 0.1172\n",
      "141/223, train_loss: 0.0989, step time: 0.1229\n",
      "142/223, train_loss: 0.0963, step time: 0.1186\n",
      "143/223, train_loss: 0.1064, step time: 0.1274\n",
      "144/223, train_loss: 0.0954, step time: 0.1000\n",
      "145/223, train_loss: 0.1088, step time: 0.1059\n",
      "146/223, train_loss: 0.1043, step time: 0.1113\n",
      "147/223, train_loss: 0.1066, step time: 0.1062\n",
      "148/223, train_loss: 0.1081, step time: 0.1126\n",
      "149/223, train_loss: 0.0948, step time: 0.1267\n",
      "150/223, train_loss: 0.0998, step time: 0.1264\n",
      "151/223, train_loss: 0.0963, step time: 0.1010\n",
      "152/223, train_loss: 0.1032, step time: 0.1044\n",
      "153/223, train_loss: 0.1016, step time: 0.1092\n",
      "154/223, train_loss: 0.1123, step time: 0.1157\n",
      "155/223, train_loss: 0.0867, step time: 0.1057\n",
      "156/223, train_loss: 0.1033, step time: 0.1134\n",
      "157/223, train_loss: 0.0958, step time: 0.1106\n",
      "158/223, train_loss: 0.0974, step time: 0.1284\n",
      "159/223, train_loss: 0.1119, step time: 0.1066\n",
      "160/223, train_loss: 0.1147, step time: 0.1148\n",
      "161/223, train_loss: 0.0988, step time: 0.1139\n",
      "162/223, train_loss: 0.0998, step time: 0.1136\n",
      "163/223, train_loss: 0.0918, step time: 0.1095\n",
      "164/223, train_loss: 0.0868, step time: 0.1146\n",
      "165/223, train_loss: 0.1054, step time: 0.1119\n",
      "166/223, train_loss: 0.1133, step time: 0.1142\n",
      "167/223, train_loss: 0.0905, step time: 0.1208\n",
      "168/223, train_loss: 0.1058, step time: 0.1066\n",
      "169/223, train_loss: 0.1004, step time: 0.1103\n",
      "170/223, train_loss: 0.1011, step time: 0.1129\n",
      "171/223, train_loss: 0.0976, step time: 0.1003\n",
      "172/223, train_loss: 0.0997, step time: 0.1030\n",
      "173/223, train_loss: 0.1156, step time: 0.1161\n",
      "174/223, train_loss: 0.1043, step time: 0.1050\n",
      "175/223, train_loss: 0.0945, step time: 0.1012\n",
      "176/223, train_loss: 0.0983, step time: 0.1065\n",
      "177/223, train_loss: 0.0944, step time: 0.1207\n",
      "178/223, train_loss: 0.0967, step time: 0.1065\n",
      "179/223, train_loss: 0.1006, step time: 0.0998\n",
      "180/223, train_loss: 0.0942, step time: 0.1146\n",
      "181/223, train_loss: 0.1071, step time: 0.1150\n",
      "182/223, train_loss: 0.1118, step time: 0.1100\n",
      "183/223, train_loss: 0.0998, step time: 0.1142\n",
      "184/223, train_loss: 0.0896, step time: 0.1033\n",
      "185/223, train_loss: 0.1034, step time: 0.1164\n",
      "186/223, train_loss: 0.0886, step time: 0.1259\n",
      "187/223, train_loss: 0.1055, step time: 0.1048\n",
      "188/223, train_loss: 0.1009, step time: 0.0992\n",
      "189/223, train_loss: 0.1042, step time: 0.1133\n",
      "190/223, train_loss: 0.0971, step time: 0.1219\n",
      "191/223, train_loss: 0.0967, step time: 0.1031\n",
      "192/223, train_loss: 0.0959, step time: 0.1174\n",
      "193/223, train_loss: 0.0884, step time: 0.1219\n",
      "194/223, train_loss: 0.0938, step time: 0.1317\n",
      "195/223, train_loss: 0.0998, step time: 0.1104\n",
      "196/223, train_loss: 0.0920, step time: 0.1063\n",
      "197/223, train_loss: 0.1123, step time: 0.1182\n",
      "198/223, train_loss: 0.1052, step time: 0.0994\n",
      "199/223, train_loss: 0.1004, step time: 0.1008\n",
      "200/223, train_loss: 0.0989, step time: 0.1199\n",
      "201/223, train_loss: 0.1029, step time: 0.1142\n",
      "202/223, train_loss: 0.0992, step time: 0.1127\n",
      "203/223, train_loss: 0.1059, step time: 0.1241\n",
      "204/223, train_loss: 0.0985, step time: 0.1182\n",
      "205/223, train_loss: 0.1176, step time: 0.1131\n",
      "206/223, train_loss: 0.0967, step time: 0.1149\n",
      "207/223, train_loss: 0.1015, step time: 0.1097\n",
      "208/223, train_loss: 0.1104, step time: 0.0991\n",
      "209/223, train_loss: 0.1096, step time: 0.1051\n",
      "210/223, train_loss: 0.1007, step time: 0.1188\n",
      "211/223, train_loss: 0.1004, step time: 0.1101\n",
      "212/223, train_loss: 0.0925, step time: 0.1007\n",
      "213/223, train_loss: 0.1121, step time: 0.0995\n",
      "214/223, train_loss: 0.1075, step time: 0.0991\n",
      "215/223, train_loss: 0.1029, step time: 0.0994\n",
      "216/223, train_loss: 0.1010, step time: 0.1014\n",
      "217/223, train_loss: 0.1041, step time: 0.1322\n",
      "218/223, train_loss: 0.1107, step time: 0.1133\n",
      "219/223, train_loss: 0.1107, step time: 0.1185\n",
      "220/223, train_loss: 0.1064, step time: 0.0992\n",
      "221/223, train_loss: 0.1084, step time: 0.0991\n",
      "222/223, train_loss: 0.0942, step time: 0.0988\n",
      "223/223, train_loss: 0.1028, step time: 0.0995\n",
      "epoch 209 average loss: 0.1020\n",
      "time consuming of epoch 209 is: 86.5809\n",
      "----------\n",
      "epoch 210/300\n",
      "1/223, train_loss: 0.1109, step time: 0.1062\n",
      "2/223, train_loss: 0.1068, step time: 0.0995\n",
      "3/223, train_loss: 0.1143, step time: 0.0998\n",
      "4/223, train_loss: 0.1045, step time: 0.0999\n",
      "5/223, train_loss: 0.1093, step time: 0.0999\n",
      "6/223, train_loss: 0.1116, step time: 0.0996\n",
      "7/223, train_loss: 0.1033, step time: 0.0998\n",
      "8/223, train_loss: 0.1130, step time: 0.0996\n",
      "9/223, train_loss: 0.1089, step time: 0.0994\n",
      "10/223, train_loss: 0.1128, step time: 0.0999\n",
      "11/223, train_loss: 0.0963, step time: 0.1102\n",
      "12/223, train_loss: 0.0901, step time: 0.1127\n",
      "13/223, train_loss: 0.0894, step time: 0.1080\n",
      "14/223, train_loss: 0.1069, step time: 0.1088\n",
      "15/223, train_loss: 0.1044, step time: 0.1147\n",
      "16/223, train_loss: 0.0947, step time: 0.1073\n",
      "17/223, train_loss: 0.1021, step time: 0.1154\n",
      "18/223, train_loss: 0.1064, step time: 0.1183\n",
      "19/223, train_loss: 0.0962, step time: 0.1087\n",
      "20/223, train_loss: 0.0946, step time: 0.1286\n",
      "21/223, train_loss: 0.1014, step time: 0.1055\n",
      "22/223, train_loss: 0.1034, step time: 0.1153\n",
      "23/223, train_loss: 0.1104, step time: 0.0995\n",
      "24/223, train_loss: 0.1030, step time: 0.1007\n",
      "25/223, train_loss: 0.0968, step time: 0.1239\n",
      "26/223, train_loss: 0.0991, step time: 0.1050\n",
      "27/223, train_loss: 0.0941, step time: 0.1271\n",
      "28/223, train_loss: 0.0973, step time: 0.1270\n",
      "29/223, train_loss: 0.1002, step time: 0.1269\n",
      "30/223, train_loss: 0.1149, step time: 0.1263\n",
      "31/223, train_loss: 0.1106, step time: 0.1405\n",
      "32/223, train_loss: 0.1026, step time: 0.1118\n",
      "33/223, train_loss: 0.0956, step time: 0.1119\n",
      "34/223, train_loss: 0.1065, step time: 0.1015\n",
      "35/223, train_loss: 0.0893, step time: 0.1002\n",
      "36/223, train_loss: 0.1038, step time: 0.1003\n",
      "37/223, train_loss: 0.0905, step time: 0.1124\n",
      "38/223, train_loss: 0.2998, step time: 0.1074\n",
      "39/223, train_loss: 0.1097, step time: 0.1091\n",
      "40/223, train_loss: 0.0965, step time: 0.1066\n",
      "41/223, train_loss: 0.0998, step time: 0.1002\n",
      "42/223, train_loss: 0.1079, step time: 0.1011\n",
      "43/223, train_loss: 0.0955, step time: 0.1003\n",
      "44/223, train_loss: 0.1030, step time: 0.1005\n",
      "45/223, train_loss: 0.1055, step time: 0.1200\n",
      "46/223, train_loss: 0.1048, step time: 0.1183\n",
      "47/223, train_loss: 0.0992, step time: 0.1002\n",
      "48/223, train_loss: 0.0915, step time: 0.1022\n",
      "49/223, train_loss: 0.1079, step time: 0.1001\n",
      "50/223, train_loss: 0.1054, step time: 0.1110\n",
      "51/223, train_loss: 0.0895, step time: 0.0998\n",
      "52/223, train_loss: 0.1005, step time: 0.1349\n",
      "53/223, train_loss: 0.1117, step time: 0.1193\n",
      "54/223, train_loss: 0.0889, step time: 0.1079\n",
      "55/223, train_loss: 0.1007, step time: 0.1212\n",
      "56/223, train_loss: 0.0979, step time: 0.1081\n",
      "57/223, train_loss: 0.0950, step time: 0.1036\n",
      "58/223, train_loss: 0.1056, step time: 0.1182\n",
      "59/223, train_loss: 0.1046, step time: 0.0994\n",
      "60/223, train_loss: 0.1051, step time: 0.0992\n",
      "61/223, train_loss: 0.0965, step time: 0.1106\n",
      "62/223, train_loss: 0.1063, step time: 0.1152\n",
      "63/223, train_loss: 0.0995, step time: 0.1133\n",
      "64/223, train_loss: 0.1002, step time: 0.1046\n",
      "65/223, train_loss: 0.0976, step time: 0.1067\n",
      "66/223, train_loss: 0.1121, step time: 0.1024\n",
      "67/223, train_loss: 0.1063, step time: 0.1003\n",
      "68/223, train_loss: 0.0981, step time: 0.1014\n",
      "69/223, train_loss: 0.0941, step time: 0.1149\n",
      "70/223, train_loss: 0.1058, step time: 0.1077\n",
      "71/223, train_loss: 0.0953, step time: 0.1067\n",
      "72/223, train_loss: 0.0858, step time: 0.1056\n",
      "73/223, train_loss: 0.0936, step time: 0.1062\n",
      "74/223, train_loss: 0.0972, step time: 0.1103\n",
      "75/223, train_loss: 0.0979, step time: 0.1003\n",
      "76/223, train_loss: 0.0934, step time: 0.1123\n",
      "77/223, train_loss: 0.0917, step time: 0.1113\n",
      "78/223, train_loss: 0.1024, step time: 0.1138\n",
      "79/223, train_loss: 0.1031, step time: 0.1314\n",
      "80/223, train_loss: 0.1130, step time: 0.1131\n",
      "81/223, train_loss: 0.1059, step time: 0.1108\n",
      "82/223, train_loss: 0.0937, step time: 0.1042\n",
      "83/223, train_loss: 0.0981, step time: 0.1034\n",
      "84/223, train_loss: 0.0959, step time: 0.1090\n",
      "85/223, train_loss: 0.0946, step time: 0.1133\n",
      "86/223, train_loss: 0.1012, step time: 0.1081\n",
      "87/223, train_loss: 0.1035, step time: 0.1102\n",
      "88/223, train_loss: 0.1049, step time: 0.1057\n",
      "89/223, train_loss: 0.1061, step time: 0.1310\n",
      "90/223, train_loss: 0.0996, step time: 0.1047\n",
      "91/223, train_loss: 0.1059, step time: 0.1003\n",
      "92/223, train_loss: 0.0938, step time: 0.1015\n",
      "93/223, train_loss: 0.0890, step time: 0.0995\n",
      "94/223, train_loss: 0.1076, step time: 0.1009\n",
      "95/223, train_loss: 0.1054, step time: 0.1009\n",
      "96/223, train_loss: 0.1053, step time: 0.1010\n",
      "97/223, train_loss: 0.1098, step time: 0.1006\n",
      "98/223, train_loss: 0.1037, step time: 0.1010\n",
      "99/223, train_loss: 0.0963, step time: 0.1163\n",
      "100/223, train_loss: 0.1020, step time: 0.1065\n",
      "101/223, train_loss: 0.1103, step time: 0.0997\n",
      "102/223, train_loss: 0.1042, step time: 0.0999\n",
      "103/223, train_loss: 0.1055, step time: 0.1005\n",
      "104/223, train_loss: 0.1026, step time: 0.1011\n",
      "105/223, train_loss: 0.0980, step time: 0.1004\n",
      "106/223, train_loss: 0.1025, step time: 0.1003\n",
      "107/223, train_loss: 0.1023, step time: 0.1008\n",
      "108/223, train_loss: 0.1097, step time: 0.1005\n",
      "109/223, train_loss: 0.1031, step time: 0.1071\n",
      "110/223, train_loss: 0.0981, step time: 0.1122\n",
      "111/223, train_loss: 0.1000, step time: 0.1303\n",
      "112/223, train_loss: 0.0939, step time: 0.1122\n",
      "113/223, train_loss: 0.0966, step time: 0.1119\n",
      "114/223, train_loss: 0.0998, step time: 0.1207\n",
      "115/223, train_loss: 0.0924, step time: 0.1167\n",
      "116/223, train_loss: 0.1086, step time: 0.1003\n",
      "117/223, train_loss: 0.0941, step time: 0.1135\n",
      "118/223, train_loss: 0.1058, step time: 0.1167\n",
      "119/223, train_loss: 0.1137, step time: 0.1143\n",
      "120/223, train_loss: 0.0917, step time: 0.1137\n",
      "121/223, train_loss: 0.1001, step time: 0.1154\n",
      "122/223, train_loss: 0.1001, step time: 0.1024\n",
      "123/223, train_loss: 0.0923, step time: 0.1129\n",
      "124/223, train_loss: 0.0942, step time: 0.1094\n",
      "125/223, train_loss: 0.0919, step time: 0.1168\n",
      "126/223, train_loss: 0.1006, step time: 0.1069\n",
      "127/223, train_loss: 0.0951, step time: 0.1004\n",
      "128/223, train_loss: 0.1019, step time: 0.1061\n",
      "129/223, train_loss: 0.1014, step time: 0.1053\n",
      "130/223, train_loss: 0.0946, step time: 0.1118\n",
      "131/223, train_loss: 0.0940, step time: 0.1009\n",
      "132/223, train_loss: 0.1060, step time: 0.1185\n",
      "133/223, train_loss: 0.0939, step time: 0.1146\n",
      "134/223, train_loss: 0.0988, step time: 0.1017\n",
      "135/223, train_loss: 0.1157, step time: 0.0993\n",
      "136/223, train_loss: 0.1030, step time: 0.1133\n",
      "137/223, train_loss: 0.1047, step time: 0.1141\n",
      "138/223, train_loss: 0.0902, step time: 0.1268\n",
      "139/223, train_loss: 0.0926, step time: 0.1008\n",
      "140/223, train_loss: 0.1023, step time: 0.1001\n",
      "141/223, train_loss: 0.0927, step time: 0.0999\n",
      "142/223, train_loss: 0.0983, step time: 0.0990\n",
      "143/223, train_loss: 0.0956, step time: 0.1027\n",
      "144/223, train_loss: 0.1031, step time: 0.1166\n",
      "145/223, train_loss: 0.0932, step time: 0.1163\n",
      "146/223, train_loss: 0.0946, step time: 0.1089\n",
      "147/223, train_loss: 0.0900, step time: 0.1163\n",
      "148/223, train_loss: 0.0892, step time: 0.1092\n",
      "149/223, train_loss: 0.1011, step time: 0.1146\n",
      "150/223, train_loss: 0.0959, step time: 0.1016\n",
      "151/223, train_loss: 0.0953, step time: 0.1174\n",
      "152/223, train_loss: 0.0929, step time: 0.1104\n",
      "153/223, train_loss: 0.1029, step time: 0.1273\n",
      "154/223, train_loss: 0.1043, step time: 0.1056\n",
      "155/223, train_loss: 0.1043, step time: 0.1186\n",
      "156/223, train_loss: 0.1059, step time: 0.0999\n",
      "157/223, train_loss: 0.1005, step time: 0.1070\n",
      "158/223, train_loss: 0.0997, step time: 0.1161\n",
      "159/223, train_loss: 0.1004, step time: 0.1177\n",
      "160/223, train_loss: 0.1013, step time: 0.1119\n",
      "161/223, train_loss: 0.0950, step time: 0.1071\n",
      "162/223, train_loss: 0.1008, step time: 0.1085\n",
      "163/223, train_loss: 0.1099, step time: 0.1209\n",
      "164/223, train_loss: 0.0994, step time: 0.1078\n",
      "165/223, train_loss: 0.1076, step time: 0.1090\n",
      "166/223, train_loss: 0.0994, step time: 0.1094\n",
      "167/223, train_loss: 0.1066, step time: 0.1171\n",
      "168/223, train_loss: 0.0969, step time: 0.1087\n",
      "169/223, train_loss: 0.0951, step time: 0.1360\n",
      "170/223, train_loss: 0.0956, step time: 0.1110\n",
      "171/223, train_loss: 0.1094, step time: 0.1115\n",
      "172/223, train_loss: 0.0975, step time: 0.1188\n",
      "173/223, train_loss: 0.0993, step time: 0.1241\n",
      "174/223, train_loss: 0.0997, step time: 0.1060\n",
      "175/223, train_loss: 0.1034, step time: 0.1138\n",
      "176/223, train_loss: 0.0911, step time: 0.1147\n",
      "177/223, train_loss: 0.1005, step time: 0.1055\n",
      "178/223, train_loss: 0.1181, step time: 0.1099\n",
      "179/223, train_loss: 0.1010, step time: 0.1269\n",
      "180/223, train_loss: 0.1084, step time: 0.1027\n",
      "181/223, train_loss: 0.0991, step time: 0.1138\n",
      "182/223, train_loss: 0.0939, step time: 0.1195\n",
      "183/223, train_loss: 0.0920, step time: 0.1277\n",
      "184/223, train_loss: 0.1142, step time: 0.1258\n",
      "185/223, train_loss: 0.1031, step time: 0.1109\n",
      "186/223, train_loss: 0.0995, step time: 0.1019\n",
      "187/223, train_loss: 0.1044, step time: 0.1053\n",
      "188/223, train_loss: 0.0907, step time: 0.1101\n",
      "189/223, train_loss: 0.1026, step time: 0.1134\n",
      "190/223, train_loss: 0.0995, step time: 0.1039\n",
      "191/223, train_loss: 0.0939, step time: 0.1044\n",
      "192/223, train_loss: 0.0922, step time: 0.1092\n",
      "193/223, train_loss: 0.0858, step time: 0.1139\n",
      "194/223, train_loss: 0.1000, step time: 0.1001\n",
      "195/223, train_loss: 0.1059, step time: 0.1082\n",
      "196/223, train_loss: 0.0981, step time: 0.1167\n",
      "197/223, train_loss: 0.1152, step time: 0.1137\n",
      "198/223, train_loss: 0.0969, step time: 0.1113\n",
      "199/223, train_loss: 0.0991, step time: 0.1096\n",
      "200/223, train_loss: 0.1206, step time: 0.1328\n",
      "201/223, train_loss: 0.1107, step time: 0.1130\n",
      "202/223, train_loss: 0.1022, step time: 0.1038\n",
      "203/223, train_loss: 0.0997, step time: 0.1141\n",
      "204/223, train_loss: 0.1004, step time: 0.1043\n",
      "205/223, train_loss: 0.0928, step time: 0.1211\n",
      "206/223, train_loss: 0.1108, step time: 0.0991\n",
      "207/223, train_loss: 0.1114, step time: 0.1003\n",
      "208/223, train_loss: 0.1025, step time: 0.1072\n",
      "209/223, train_loss: 0.0974, step time: 0.1101\n",
      "210/223, train_loss: 0.1068, step time: 0.1007\n",
      "211/223, train_loss: 0.1084, step time: 0.1011\n",
      "212/223, train_loss: 0.1125, step time: 0.1173\n",
      "213/223, train_loss: 0.1244, step time: 0.1162\n",
      "214/223, train_loss: 0.1074, step time: 0.1257\n",
      "215/223, train_loss: 0.1047, step time: 0.1095\n",
      "216/223, train_loss: 0.0911, step time: 0.1005\n",
      "217/223, train_loss: 0.1131, step time: 0.1164\n",
      "218/223, train_loss: 0.1066, step time: 0.1138\n",
      "219/223, train_loss: 0.1002, step time: 0.1013\n",
      "220/223, train_loss: 0.1023, step time: 0.1151\n",
      "221/223, train_loss: 0.0970, step time: 0.1000\n",
      "222/223, train_loss: 0.1025, step time: 0.0986\n",
      "223/223, train_loss: 0.0964, step time: 0.1001\n",
      "epoch 210 average loss: 0.1020\n",
      "current epoch: 210 current mean dice: 0.8605 tc: 0.9219 wt: 0.8706 et: 0.7890\n",
      "best mean dice: 0.8605 at epoch: 205\n",
      "time consuming of epoch 210 is: 94.1843\n",
      "----------\n",
      "epoch 211/300\n",
      "1/223, train_loss: 0.1058, step time: 0.1041\n",
      "2/223, train_loss: 0.1075, step time: 0.1311\n",
      "3/223, train_loss: 0.1083, step time: 0.1202\n",
      "4/223, train_loss: 0.0906, step time: 0.1209\n",
      "5/223, train_loss: 0.0994, step time: 0.1329\n",
      "6/223, train_loss: 0.0972, step time: 0.1163\n",
      "7/223, train_loss: 0.0937, step time: 0.1159\n",
      "8/223, train_loss: 0.1051, step time: 0.1064\n",
      "9/223, train_loss: 0.0988, step time: 0.1039\n",
      "10/223, train_loss: 0.0972, step time: 0.1277\n",
      "11/223, train_loss: 0.1161, step time: 0.1111\n",
      "12/223, train_loss: 0.0889, step time: 0.1126\n",
      "13/223, train_loss: 0.0895, step time: 0.0997\n",
      "14/223, train_loss: 0.0920, step time: 0.1098\n",
      "15/223, train_loss: 0.0955, step time: 0.1120\n",
      "16/223, train_loss: 0.0936, step time: 0.1143\n",
      "17/223, train_loss: 0.0952, step time: 0.1101\n",
      "18/223, train_loss: 0.1075, step time: 0.1024\n",
      "19/223, train_loss: 0.0986, step time: 0.1285\n",
      "20/223, train_loss: 0.1127, step time: 0.1123\n",
      "21/223, train_loss: 0.1004, step time: 0.0998\n",
      "22/223, train_loss: 0.0890, step time: 0.1123\n",
      "23/223, train_loss: 0.0968, step time: 0.1271\n",
      "24/223, train_loss: 0.2933, step time: 0.1020\n",
      "25/223, train_loss: 0.0969, step time: 0.1123\n",
      "26/223, train_loss: 0.1203, step time: 0.1129\n",
      "27/223, train_loss: 0.0982, step time: 0.1183\n",
      "28/223, train_loss: 0.1005, step time: 0.1154\n",
      "29/223, train_loss: 0.1060, step time: 0.1158\n",
      "30/223, train_loss: 0.0966, step time: 0.1120\n",
      "31/223, train_loss: 0.1013, step time: 0.1150\n",
      "32/223, train_loss: 0.1039, step time: 0.1135\n",
      "33/223, train_loss: 0.1125, step time: 0.1144\n",
      "34/223, train_loss: 0.0931, step time: 0.1005\n",
      "35/223, train_loss: 0.1109, step time: 0.1017\n",
      "36/223, train_loss: 0.1172, step time: 0.1156\n",
      "37/223, train_loss: 0.1015, step time: 0.1053\n",
      "38/223, train_loss: 0.1001, step time: 0.1093\n",
      "39/223, train_loss: 0.1045, step time: 0.1222\n",
      "40/223, train_loss: 0.0972, step time: 0.1379\n",
      "41/223, train_loss: 0.1063, step time: 0.1096\n",
      "42/223, train_loss: 0.0998, step time: 0.1010\n",
      "43/223, train_loss: 0.0985, step time: 0.1065\n",
      "44/223, train_loss: 0.1020, step time: 0.1288\n",
      "45/223, train_loss: 0.1122, step time: 0.1171\n",
      "46/223, train_loss: 0.1089, step time: 0.1007\n",
      "47/223, train_loss: 0.0928, step time: 0.1077\n",
      "48/223, train_loss: 0.0961, step time: 0.1066\n",
      "49/223, train_loss: 0.1016, step time: 0.1211\n",
      "50/223, train_loss: 0.1128, step time: 0.1187\n",
      "51/223, train_loss: 0.0974, step time: 0.1372\n",
      "52/223, train_loss: 0.0957, step time: 0.1067\n",
      "53/223, train_loss: 0.0986, step time: 0.1147\n",
      "54/223, train_loss: 0.0955, step time: 0.1063\n",
      "55/223, train_loss: 0.0946, step time: 0.1125\n",
      "56/223, train_loss: 0.0987, step time: 0.1216\n",
      "57/223, train_loss: 0.1020, step time: 0.1146\n",
      "58/223, train_loss: 0.0979, step time: 0.1005\n",
      "59/223, train_loss: 0.1008, step time: 0.1022\n",
      "60/223, train_loss: 0.1056, step time: 0.1177\n",
      "61/223, train_loss: 0.0995, step time: 0.1171\n",
      "62/223, train_loss: 0.0945, step time: 0.1090\n",
      "63/223, train_loss: 0.0978, step time: 0.1069\n",
      "64/223, train_loss: 0.1082, step time: 0.1174\n",
      "65/223, train_loss: 0.0886, step time: 0.1150\n",
      "66/223, train_loss: 0.1003, step time: 0.1001\n",
      "67/223, train_loss: 0.0922, step time: 0.1004\n",
      "68/223, train_loss: 0.0954, step time: 0.1163\n",
      "69/223, train_loss: 0.0964, step time: 0.1343\n",
      "70/223, train_loss: 0.1074, step time: 0.1163\n",
      "71/223, train_loss: 0.1037, step time: 0.1143\n",
      "72/223, train_loss: 0.1009, step time: 0.1000\n",
      "73/223, train_loss: 0.1074, step time: 0.1044\n",
      "74/223, train_loss: 0.1077, step time: 0.1152\n",
      "75/223, train_loss: 0.0911, step time: 0.1001\n",
      "76/223, train_loss: 0.1153, step time: 0.1053\n",
      "77/223, train_loss: 0.1016, step time: 0.1035\n",
      "78/223, train_loss: 0.1032, step time: 0.1206\n",
      "79/223, train_loss: 0.1077, step time: 0.1261\n",
      "80/223, train_loss: 0.1080, step time: 0.1005\n",
      "81/223, train_loss: 0.1033, step time: 0.1096\n",
      "82/223, train_loss: 0.0984, step time: 0.1024\n",
      "83/223, train_loss: 0.0989, step time: 0.1004\n",
      "84/223, train_loss: 0.0910, step time: 0.1023\n",
      "85/223, train_loss: 0.1000, step time: 0.1124\n",
      "86/223, train_loss: 0.0905, step time: 0.1218\n",
      "87/223, train_loss: 0.0935, step time: 0.1639\n",
      "88/223, train_loss: 0.1013, step time: 0.1091\n",
      "89/223, train_loss: 0.1086, step time: 0.1424\n",
      "90/223, train_loss: 0.1095, step time: 0.1096\n",
      "91/223, train_loss: 0.0969, step time: 0.1098\n",
      "92/223, train_loss: 0.1044, step time: 0.1259\n",
      "93/223, train_loss: 0.0966, step time: 0.1094\n",
      "94/223, train_loss: 0.0953, step time: 0.1090\n",
      "95/223, train_loss: 0.1073, step time: 0.1232\n",
      "96/223, train_loss: 0.0936, step time: 0.1138\n",
      "97/223, train_loss: 0.0976, step time: 0.1246\n",
      "98/223, train_loss: 0.1034, step time: 0.1107\n",
      "99/223, train_loss: 0.0890, step time: 0.1186\n",
      "100/223, train_loss: 0.0914, step time: 0.1138\n",
      "101/223, train_loss: 0.0984, step time: 0.1082\n",
      "102/223, train_loss: 0.1050, step time: 0.1317\n",
      "103/223, train_loss: 0.1011, step time: 0.1131\n",
      "104/223, train_loss: 0.0943, step time: 0.1073\n",
      "105/223, train_loss: 0.1028, step time: 0.1218\n",
      "106/223, train_loss: 0.0993, step time: 0.1135\n",
      "107/223, train_loss: 0.0973, step time: 0.1501\n",
      "108/223, train_loss: 0.0978, step time: 0.1125\n",
      "109/223, train_loss: 0.0976, step time: 0.1341\n",
      "110/223, train_loss: 0.1076, step time: 0.1016\n",
      "111/223, train_loss: 0.0983, step time: 0.1206\n",
      "112/223, train_loss: 0.0993, step time: 0.1029\n",
      "113/223, train_loss: 0.1008, step time: 0.1231\n",
      "114/223, train_loss: 0.1024, step time: 0.1213\n",
      "115/223, train_loss: 0.0914, step time: 0.1002\n",
      "116/223, train_loss: 0.0979, step time: 0.1013\n",
      "117/223, train_loss: 0.1082, step time: 0.1182\n",
      "118/223, train_loss: 0.0904, step time: 0.1025\n",
      "119/223, train_loss: 0.1014, step time: 0.1290\n",
      "120/223, train_loss: 0.1020, step time: 0.1159\n",
      "121/223, train_loss: 0.1007, step time: 0.1099\n",
      "122/223, train_loss: 0.0953, step time: 0.1007\n",
      "123/223, train_loss: 0.1018, step time: 0.1205\n",
      "124/223, train_loss: 0.1098, step time: 0.1236\n",
      "125/223, train_loss: 0.0986, step time: 0.1002\n",
      "126/223, train_loss: 0.0936, step time: 0.1229\n",
      "127/223, train_loss: 0.1103, step time: 0.1014\n",
      "128/223, train_loss: 0.1019, step time: 0.1002\n",
      "129/223, train_loss: 0.1178, step time: 0.1078\n",
      "130/223, train_loss: 0.0978, step time: 0.1030\n",
      "131/223, train_loss: 0.1005, step time: 0.1026\n",
      "132/223, train_loss: 0.1033, step time: 0.1038\n",
      "133/223, train_loss: 0.1014, step time: 0.1074\n",
      "134/223, train_loss: 0.0980, step time: 0.1245\n",
      "135/223, train_loss: 0.0987, step time: 0.1132\n",
      "136/223, train_loss: 0.0926, step time: 0.1295\n",
      "137/223, train_loss: 0.1105, step time: 0.1097\n",
      "138/223, train_loss: 0.0911, step time: 0.1103\n",
      "139/223, train_loss: 0.0988, step time: 0.1166\n",
      "140/223, train_loss: 0.1116, step time: 0.1091\n",
      "141/223, train_loss: 0.1108, step time: 0.1103\n",
      "142/223, train_loss: 0.1002, step time: 0.1067\n",
      "143/223, train_loss: 0.1103, step time: 0.1011\n",
      "144/223, train_loss: 0.0957, step time: 0.1035\n",
      "145/223, train_loss: 0.0936, step time: 0.1006\n",
      "146/223, train_loss: 0.0984, step time: 0.0996\n",
      "147/223, train_loss: 0.1050, step time: 0.1177\n",
      "148/223, train_loss: 0.1119, step time: 0.1519\n",
      "149/223, train_loss: 0.0962, step time: 0.0998\n",
      "150/223, train_loss: 0.1023, step time: 0.0986\n",
      "151/223, train_loss: 0.0901, step time: 0.0993\n",
      "152/223, train_loss: 0.0931, step time: 0.1003\n",
      "153/223, train_loss: 0.1040, step time: 0.0993\n",
      "154/223, train_loss: 0.1188, step time: 0.1008\n",
      "155/223, train_loss: 0.1056, step time: 0.1013\n",
      "156/223, train_loss: 0.1043, step time: 0.1092\n",
      "157/223, train_loss: 0.0976, step time: 0.1010\n",
      "158/223, train_loss: 0.1000, step time: 0.1002\n",
      "159/223, train_loss: 0.1070, step time: 0.1009\n",
      "160/223, train_loss: 0.0941, step time: 0.1044\n",
      "161/223, train_loss: 0.1016, step time: 0.1281\n",
      "162/223, train_loss: 0.0968, step time: 0.1009\n",
      "163/223, train_loss: 0.0997, step time: 0.0999\n",
      "164/223, train_loss: 0.0936, step time: 0.1249\n",
      "165/223, train_loss: 0.0927, step time: 0.1011\n",
      "166/223, train_loss: 0.0979, step time: 0.1012\n",
      "167/223, train_loss: 0.0969, step time: 0.1173\n",
      "168/223, train_loss: 0.0985, step time: 0.1443\n",
      "169/223, train_loss: 0.1104, step time: 0.1060\n",
      "170/223, train_loss: 0.1077, step time: 0.1155\n",
      "171/223, train_loss: 0.1023, step time: 0.1260\n",
      "172/223, train_loss: 0.1154, step time: 0.1009\n",
      "173/223, train_loss: 0.0965, step time: 0.1063\n",
      "174/223, train_loss: 0.1026, step time: 0.1311\n",
      "175/223, train_loss: 0.0949, step time: 0.1006\n",
      "176/223, train_loss: 0.1017, step time: 0.1003\n",
      "177/223, train_loss: 0.0998, step time: 0.1119\n",
      "178/223, train_loss: 0.1018, step time: 0.0995\n",
      "179/223, train_loss: 0.1082, step time: 0.1001\n",
      "180/223, train_loss: 0.1025, step time: 0.1040\n",
      "181/223, train_loss: 0.1020, step time: 0.1100\n",
      "182/223, train_loss: 0.0987, step time: 0.1156\n",
      "183/223, train_loss: 0.1052, step time: 0.1025\n",
      "184/223, train_loss: 0.0974, step time: 0.1154\n",
      "185/223, train_loss: 0.0889, step time: 0.1011\n",
      "186/223, train_loss: 0.1104, step time: 0.1321\n",
      "187/223, train_loss: 0.1025, step time: 0.1002\n",
      "188/223, train_loss: 0.1098, step time: 0.1098\n",
      "189/223, train_loss: 0.0929, step time: 0.1577\n",
      "190/223, train_loss: 0.0844, step time: 0.1242\n",
      "191/223, train_loss: 0.0917, step time: 0.1032\n",
      "192/223, train_loss: 0.1081, step time: 0.1123\n",
      "193/223, train_loss: 0.1002, step time: 0.1134\n",
      "194/223, train_loss: 0.1017, step time: 0.0997\n",
      "195/223, train_loss: 0.1102, step time: 0.1198\n",
      "196/223, train_loss: 0.1000, step time: 0.1011\n",
      "197/223, train_loss: 0.1110, step time: 0.0998\n",
      "198/223, train_loss: 0.0949, step time: 0.1034\n",
      "199/223, train_loss: 0.0975, step time: 0.1003\n",
      "200/223, train_loss: 0.1016, step time: 0.1230\n",
      "201/223, train_loss: 0.0906, step time: 0.1008\n",
      "202/223, train_loss: 0.0991, step time: 0.1364\n",
      "203/223, train_loss: 0.1202, step time: 0.1005\n",
      "204/223, train_loss: 0.1027, step time: 0.1058\n",
      "205/223, train_loss: 0.1027, step time: 0.1119\n",
      "206/223, train_loss: 0.0937, step time: 0.1381\n",
      "207/223, train_loss: 0.1067, step time: 0.1135\n",
      "208/223, train_loss: 0.1202, step time: 0.1283\n",
      "209/223, train_loss: 0.0975, step time: 0.1179\n",
      "210/223, train_loss: 0.1062, step time: 0.1178\n",
      "211/223, train_loss: 0.0864, step time: 0.1060\n",
      "212/223, train_loss: 0.1014, step time: 0.1024\n",
      "213/223, train_loss: 0.1030, step time: 0.1092\n",
      "214/223, train_loss: 0.1081, step time: 0.1149\n",
      "215/223, train_loss: 0.1021, step time: 0.1020\n",
      "216/223, train_loss: 0.0934, step time: 0.1001\n",
      "217/223, train_loss: 0.1039, step time: 0.1005\n",
      "218/223, train_loss: 0.1151, step time: 0.1002\n",
      "219/223, train_loss: 0.0984, step time: 0.1133\n",
      "220/223, train_loss: 0.1061, step time: 0.1003\n",
      "221/223, train_loss: 0.0990, step time: 0.0995\n",
      "222/223, train_loss: 0.1079, step time: 0.0994\n",
      "223/223, train_loss: 0.1109, step time: 0.0998\n",
      "epoch 211 average loss: 0.1019\n",
      "time consuming of epoch 211 is: 90.1802\n",
      "----------\n",
      "epoch 212/300\n",
      "1/223, train_loss: 0.0856, step time: 0.1136\n",
      "2/223, train_loss: 0.1100, step time: 0.1200\n",
      "3/223, train_loss: 0.0948, step time: 0.1097\n",
      "4/223, train_loss: 0.1022, step time: 0.0997\n",
      "5/223, train_loss: 0.1020, step time: 0.1222\n",
      "6/223, train_loss: 0.0977, step time: 0.1638\n",
      "7/223, train_loss: 0.1105, step time: 0.1179\n",
      "8/223, train_loss: 0.0986, step time: 0.1000\n",
      "9/223, train_loss: 0.1208, step time: 0.1088\n",
      "10/223, train_loss: 0.0997, step time: 0.1141\n",
      "11/223, train_loss: 0.1009, step time: 0.1244\n",
      "12/223, train_loss: 0.1050, step time: 0.1145\n",
      "13/223, train_loss: 0.1032, step time: 0.1119\n",
      "14/223, train_loss: 0.1067, step time: 0.1376\n",
      "15/223, train_loss: 0.0976, step time: 0.1232\n",
      "16/223, train_loss: 0.0920, step time: 0.1098\n",
      "17/223, train_loss: 0.1025, step time: 0.1110\n",
      "18/223, train_loss: 0.1036, step time: 0.1146\n",
      "19/223, train_loss: 0.1039, step time: 0.1345\n",
      "20/223, train_loss: 0.0974, step time: 0.1089\n",
      "21/223, train_loss: 0.1051, step time: 0.1185\n",
      "22/223, train_loss: 0.1008, step time: 0.1315\n",
      "23/223, train_loss: 0.1054, step time: 0.1350\n",
      "24/223, train_loss: 0.0990, step time: 0.1158\n",
      "25/223, train_loss: 0.0886, step time: 0.1050\n",
      "26/223, train_loss: 0.1041, step time: 0.1432\n",
      "27/223, train_loss: 0.0999, step time: 0.1371\n",
      "28/223, train_loss: 0.0941, step time: 0.1112\n",
      "29/223, train_loss: 0.1007, step time: 0.1150\n",
      "30/223, train_loss: 0.1016, step time: 0.1123\n",
      "31/223, train_loss: 0.1028, step time: 0.1352\n",
      "32/223, train_loss: 0.0916, step time: 0.1128\n",
      "33/223, train_loss: 0.0920, step time: 0.1044\n",
      "34/223, train_loss: 0.1096, step time: 0.1100\n",
      "35/223, train_loss: 0.0975, step time: 0.1418\n",
      "36/223, train_loss: 0.1053, step time: 0.1226\n",
      "37/223, train_loss: 0.0946, step time: 0.1096\n",
      "38/223, train_loss: 0.1104, step time: 0.1031\n",
      "39/223, train_loss: 0.0986, step time: 0.1313\n",
      "40/223, train_loss: 0.1095, step time: 0.0999\n",
      "41/223, train_loss: 0.1020, step time: 0.1178\n",
      "42/223, train_loss: 0.1063, step time: 0.1351\n",
      "43/223, train_loss: 0.0973, step time: 0.0996\n",
      "44/223, train_loss: 0.1123, step time: 0.0991\n",
      "45/223, train_loss: 0.1173, step time: 0.1154\n",
      "46/223, train_loss: 0.1028, step time: 0.1152\n",
      "47/223, train_loss: 0.0981, step time: 0.1703\n",
      "48/223, train_loss: 0.1008, step time: 0.0992\n",
      "49/223, train_loss: 0.0969, step time: 0.1168\n",
      "50/223, train_loss: 0.1016, step time: 0.1543\n",
      "51/223, train_loss: 0.0895, step time: 0.1259\n",
      "52/223, train_loss: 0.1005, step time: 0.1264\n",
      "53/223, train_loss: 0.1103, step time: 0.1051\n",
      "54/223, train_loss: 0.0921, step time: 0.1095\n",
      "55/223, train_loss: 0.0964, step time: 0.1153\n",
      "56/223, train_loss: 0.0928, step time: 0.1053\n",
      "57/223, train_loss: 0.0967, step time: 0.1096\n",
      "58/223, train_loss: 0.1054, step time: 0.1477\n",
      "59/223, train_loss: 0.0916, step time: 0.1074\n",
      "60/223, train_loss: 0.0987, step time: 0.1125\n",
      "61/223, train_loss: 0.0913, step time: 0.1123\n",
      "62/223, train_loss: 0.0956, step time: 0.1079\n",
      "63/223, train_loss: 0.0906, step time: 0.1108\n",
      "64/223, train_loss: 0.0990, step time: 0.1325\n",
      "65/223, train_loss: 0.1087, step time: 0.1127\n",
      "66/223, train_loss: 0.1024, step time: 0.1096\n",
      "67/223, train_loss: 0.0947, step time: 0.1705\n",
      "68/223, train_loss: 0.1142, step time: 0.1081\n",
      "69/223, train_loss: 0.1064, step time: 0.1149\n",
      "70/223, train_loss: 0.0973, step time: 0.1013\n",
      "71/223, train_loss: 0.1067, step time: 0.1005\n",
      "72/223, train_loss: 0.1067, step time: 0.1155\n",
      "73/223, train_loss: 0.1081, step time: 0.1080\n",
      "74/223, train_loss: 0.1036, step time: 0.1111\n",
      "75/223, train_loss: 0.0937, step time: 0.1347\n",
      "76/223, train_loss: 0.1026, step time: 0.1095\n",
      "77/223, train_loss: 0.0954, step time: 0.1129\n",
      "78/223, train_loss: 0.0938, step time: 0.0998\n",
      "79/223, train_loss: 0.1132, step time: 0.1075\n",
      "80/223, train_loss: 0.1039, step time: 0.1090\n",
      "81/223, train_loss: 0.0939, step time: 0.1092\n",
      "82/223, train_loss: 0.1084, step time: 0.1123\n",
      "83/223, train_loss: 0.1059, step time: 0.1170\n",
      "84/223, train_loss: 0.0967, step time: 0.1061\n",
      "85/223, train_loss: 0.0928, step time: 0.1148\n",
      "86/223, train_loss: 0.1030, step time: 0.1071\n",
      "87/223, train_loss: 0.0959, step time: 0.1202\n",
      "88/223, train_loss: 0.1184, step time: 0.1410\n",
      "89/223, train_loss: 0.1118, step time: 0.1293\n",
      "90/223, train_loss: 0.1020, step time: 0.1002\n",
      "91/223, train_loss: 0.1062, step time: 0.1003\n",
      "92/223, train_loss: 0.0895, step time: 0.1025\n",
      "93/223, train_loss: 0.1056, step time: 0.1000\n",
      "94/223, train_loss: 0.0976, step time: 0.1088\n",
      "95/223, train_loss: 0.0999, step time: 0.1091\n",
      "96/223, train_loss: 0.1009, step time: 0.1012\n",
      "97/223, train_loss: 0.1038, step time: 0.1138\n",
      "98/223, train_loss: 0.1016, step time: 0.1163\n",
      "99/223, train_loss: 0.1052, step time: 0.1018\n",
      "100/223, train_loss: 0.0899, step time: 0.1119\n",
      "101/223, train_loss: 0.0948, step time: 0.1119\n",
      "102/223, train_loss: 0.0917, step time: 0.1321\n",
      "103/223, train_loss: 0.1065, step time: 0.1149\n",
      "104/223, train_loss: 0.0955, step time: 0.1065\n",
      "105/223, train_loss: 0.0971, step time: 0.1115\n",
      "106/223, train_loss: 0.0957, step time: 0.1125\n",
      "107/223, train_loss: 0.0964, step time: 0.1260\n",
      "108/223, train_loss: 0.0953, step time: 0.1065\n",
      "109/223, train_loss: 0.1157, step time: 0.1207\n",
      "110/223, train_loss: 0.0943, step time: 0.1102\n",
      "111/223, train_loss: 0.0979, step time: 0.1265\n",
      "112/223, train_loss: 0.0980, step time: 0.1106\n",
      "113/223, train_loss: 0.0934, step time: 0.1166\n",
      "114/223, train_loss: 0.1044, step time: 0.1095\n",
      "115/223, train_loss: 0.0928, step time: 0.1187\n",
      "116/223, train_loss: 0.1015, step time: 0.1000\n",
      "117/223, train_loss: 0.1026, step time: 0.1128\n",
      "118/223, train_loss: 0.0996, step time: 0.1071\n",
      "119/223, train_loss: 0.0998, step time: 0.1235\n",
      "120/223, train_loss: 0.0959, step time: 0.1144\n",
      "121/223, train_loss: 0.1040, step time: 0.1147\n",
      "122/223, train_loss: 0.1010, step time: 0.0995\n",
      "123/223, train_loss: 0.0937, step time: 0.1041\n",
      "124/223, train_loss: 0.1011, step time: 0.1120\n",
      "125/223, train_loss: 0.0943, step time: 0.1149\n",
      "126/223, train_loss: 0.0989, step time: 0.1005\n",
      "127/223, train_loss: 0.1028, step time: 0.1262\n",
      "128/223, train_loss: 0.1008, step time: 0.1172\n",
      "129/223, train_loss: 0.0989, step time: 0.0991\n",
      "130/223, train_loss: 0.0993, step time: 0.0993\n",
      "131/223, train_loss: 0.0982, step time: 0.0996\n",
      "132/223, train_loss: 0.1056, step time: 0.1049\n",
      "133/223, train_loss: 0.1003, step time: 0.1091\n",
      "134/223, train_loss: 0.0911, step time: 0.1173\n",
      "135/223, train_loss: 0.0957, step time: 0.1008\n",
      "136/223, train_loss: 0.1071, step time: 0.1027\n",
      "137/223, train_loss: 0.0929, step time: 0.1520\n",
      "138/223, train_loss: 0.1078, step time: 0.1085\n",
      "139/223, train_loss: 0.0916, step time: 0.1000\n",
      "140/223, train_loss: 0.1143, step time: 0.1005\n",
      "141/223, train_loss: 0.0985, step time: 0.1202\n",
      "142/223, train_loss: 0.0948, step time: 0.1264\n",
      "143/223, train_loss: 0.0998, step time: 0.1069\n",
      "144/223, train_loss: 0.0987, step time: 0.1150\n",
      "145/223, train_loss: 0.0938, step time: 0.1066\n",
      "146/223, train_loss: 0.1001, step time: 0.1099\n",
      "147/223, train_loss: 0.0996, step time: 0.1004\n",
      "148/223, train_loss: 0.1093, step time: 0.1154\n",
      "149/223, train_loss: 0.1068, step time: 0.1211\n",
      "150/223, train_loss: 0.0968, step time: 0.1112\n",
      "151/223, train_loss: 0.1072, step time: 0.1176\n",
      "152/223, train_loss: 0.1058, step time: 0.1070\n",
      "153/223, train_loss: 0.0993, step time: 0.1130\n",
      "154/223, train_loss: 0.1001, step time: 0.1000\n",
      "155/223, train_loss: 0.1033, step time: 0.1001\n",
      "156/223, train_loss: 0.1161, step time: 0.1101\n",
      "157/223, train_loss: 0.0992, step time: 0.1084\n",
      "158/223, train_loss: 0.0899, step time: 0.1192\n",
      "159/223, train_loss: 0.1061, step time: 0.1354\n",
      "160/223, train_loss: 0.1036, step time: 0.1064\n",
      "161/223, train_loss: 0.1010, step time: 0.1219\n",
      "162/223, train_loss: 0.1030, step time: 0.1181\n",
      "163/223, train_loss: 0.1016, step time: 0.1348\n",
      "164/223, train_loss: 0.0855, step time: 0.1163\n",
      "165/223, train_loss: 0.0981, step time: 0.1121\n",
      "166/223, train_loss: 0.1015, step time: 0.1151\n",
      "167/223, train_loss: 0.1080, step time: 0.1218\n",
      "168/223, train_loss: 0.0925, step time: 0.1210\n",
      "169/223, train_loss: 0.0989, step time: 0.1120\n",
      "170/223, train_loss: 0.0879, step time: 0.1211\n",
      "171/223, train_loss: 0.1049, step time: 0.1418\n",
      "172/223, train_loss: 0.1092, step time: 0.1178\n",
      "173/223, train_loss: 0.1006, step time: 0.0997\n",
      "174/223, train_loss: 0.1107, step time: 0.1004\n",
      "175/223, train_loss: 0.0979, step time: 0.1011\n",
      "176/223, train_loss: 0.1081, step time: 0.1209\n",
      "177/223, train_loss: 0.1139, step time: 0.1019\n",
      "178/223, train_loss: 0.1000, step time: 0.1378\n",
      "179/223, train_loss: 0.0970, step time: 0.1081\n",
      "180/223, train_loss: 0.0964, step time: 0.1224\n",
      "181/223, train_loss: 0.1132, step time: 0.1097\n",
      "182/223, train_loss: 0.1073, step time: 0.1283\n",
      "183/223, train_loss: 0.1080, step time: 0.1142\n",
      "184/223, train_loss: 0.1040, step time: 0.1337\n",
      "185/223, train_loss: 0.1031, step time: 0.1144\n",
      "186/223, train_loss: 0.1085, step time: 0.1258\n",
      "187/223, train_loss: 0.1037, step time: 0.1258\n",
      "188/223, train_loss: 0.1060, step time: 0.1012\n",
      "189/223, train_loss: 0.1045, step time: 0.1086\n",
      "190/223, train_loss: 0.0979, step time: 0.1062\n",
      "191/223, train_loss: 0.0923, step time: 0.1226\n",
      "192/223, train_loss: 0.0967, step time: 0.1078\n",
      "193/223, train_loss: 0.1073, step time: 0.1213\n",
      "194/223, train_loss: 0.1102, step time: 0.1284\n",
      "195/223, train_loss: 0.0933, step time: 0.1098\n",
      "196/223, train_loss: 0.0871, step time: 0.1124\n",
      "197/223, train_loss: 0.1107, step time: 0.0986\n",
      "198/223, train_loss: 0.0932, step time: 0.0991\n",
      "199/223, train_loss: 0.1059, step time: 0.0992\n",
      "200/223, train_loss: 0.0958, step time: 0.0990\n",
      "201/223, train_loss: 0.0961, step time: 0.1049\n",
      "202/223, train_loss: 0.1060, step time: 0.1007\n",
      "203/223, train_loss: 0.1085, step time: 0.1264\n",
      "204/223, train_loss: 0.1006, step time: 0.1054\n",
      "205/223, train_loss: 0.0920, step time: 0.1167\n",
      "206/223, train_loss: 0.1038, step time: 0.1050\n",
      "207/223, train_loss: 0.1013, step time: 0.1121\n",
      "208/223, train_loss: 0.1014, step time: 0.0985\n",
      "209/223, train_loss: 0.2950, step time: 0.1084\n",
      "210/223, train_loss: 0.1077, step time: 0.1471\n",
      "211/223, train_loss: 0.0889, step time: 0.1198\n",
      "212/223, train_loss: 0.1026, step time: 0.1094\n",
      "213/223, train_loss: 0.1004, step time: 0.1113\n",
      "214/223, train_loss: 0.1006, step time: 0.0993\n",
      "215/223, train_loss: 0.0934, step time: 0.1130\n",
      "216/223, train_loss: 0.1002, step time: 0.1039\n",
      "217/223, train_loss: 0.0974, step time: 0.1011\n",
      "218/223, train_loss: 0.0933, step time: 0.0999\n",
      "219/223, train_loss: 0.1026, step time: 0.0993\n",
      "220/223, train_loss: 0.0990, step time: 0.0998\n",
      "221/223, train_loss: 0.1104, step time: 0.1002\n",
      "222/223, train_loss: 0.1064, step time: 0.0991\n",
      "223/223, train_loss: 0.1008, step time: 0.1005\n",
      "epoch 212 average loss: 0.1017\n",
      "time consuming of epoch 212 is: 90.6157\n",
      "----------\n",
      "epoch 213/300\n",
      "1/223, train_loss: 0.1045, step time: 0.1026\n",
      "2/223, train_loss: 0.1002, step time: 0.1002\n",
      "3/223, train_loss: 0.1022, step time: 0.1007\n",
      "4/223, train_loss: 0.0979, step time: 0.1054\n",
      "5/223, train_loss: 0.0994, step time: 0.1261\n",
      "6/223, train_loss: 0.1207, step time: 0.1104\n",
      "7/223, train_loss: 0.0997, step time: 0.1406\n",
      "8/223, train_loss: 0.1106, step time: 0.1001\n",
      "9/223, train_loss: 0.1017, step time: 0.1005\n",
      "10/223, train_loss: 0.0927, step time: 0.1173\n",
      "11/223, train_loss: 0.1069, step time: 0.1465\n",
      "12/223, train_loss: 0.1019, step time: 0.1008\n",
      "13/223, train_loss: 0.1044, step time: 0.1071\n",
      "14/223, train_loss: 0.1062, step time: 0.1008\n",
      "15/223, train_loss: 0.0973, step time: 0.1132\n",
      "16/223, train_loss: 0.0978, step time: 0.1032\n",
      "17/223, train_loss: 0.0926, step time: 0.1727\n",
      "18/223, train_loss: 0.0949, step time: 0.1393\n",
      "19/223, train_loss: 0.0895, step time: 0.1059\n",
      "20/223, train_loss: 0.1059, step time: 0.0997\n",
      "21/223, train_loss: 0.0990, step time: 0.0999\n",
      "22/223, train_loss: 0.1070, step time: 0.1009\n",
      "23/223, train_loss: 0.1016, step time: 0.1002\n",
      "24/223, train_loss: 0.1067, step time: 0.1010\n",
      "25/223, train_loss: 0.0954, step time: 0.0998\n",
      "26/223, train_loss: 0.1066, step time: 0.0999\n",
      "27/223, train_loss: 0.0994, step time: 0.0997\n",
      "28/223, train_loss: 0.0973, step time: 0.0999\n",
      "29/223, train_loss: 0.1028, step time: 0.1000\n",
      "30/223, train_loss: 0.0918, step time: 0.1001\n",
      "31/223, train_loss: 0.0976, step time: 0.0995\n",
      "32/223, train_loss: 0.1074, step time: 0.1215\n",
      "33/223, train_loss: 0.0972, step time: 0.1002\n",
      "34/223, train_loss: 0.0935, step time: 0.0997\n",
      "35/223, train_loss: 0.0946, step time: 0.0997\n",
      "36/223, train_loss: 0.1021, step time: 0.1022\n",
      "37/223, train_loss: 0.0957, step time: 0.0996\n",
      "38/223, train_loss: 0.1053, step time: 0.0997\n",
      "39/223, train_loss: 0.1126, step time: 0.0997\n",
      "40/223, train_loss: 0.0962, step time: 0.1154\n",
      "41/223, train_loss: 0.1087, step time: 0.0989\n",
      "42/223, train_loss: 0.1021, step time: 0.0998\n",
      "43/223, train_loss: 0.0958, step time: 0.0990\n",
      "44/223, train_loss: 0.1039, step time: 0.1070\n",
      "45/223, train_loss: 0.0883, step time: 0.1006\n",
      "46/223, train_loss: 0.0977, step time: 0.1001\n",
      "47/223, train_loss: 0.0880, step time: 0.1002\n",
      "48/223, train_loss: 0.1026, step time: 0.1327\n",
      "49/223, train_loss: 0.0991, step time: 0.1004\n",
      "50/223, train_loss: 0.1023, step time: 0.1033\n",
      "51/223, train_loss: 0.1041, step time: 0.1002\n",
      "52/223, train_loss: 0.0969, step time: 0.1560\n",
      "53/223, train_loss: 0.0979, step time: 0.1179\n",
      "54/223, train_loss: 0.1010, step time: 0.1070\n",
      "55/223, train_loss: 0.1054, step time: 0.1171\n",
      "56/223, train_loss: 0.0883, step time: 0.1135\n",
      "57/223, train_loss: 0.1065, step time: 0.1172\n",
      "58/223, train_loss: 0.0972, step time: 0.1111\n",
      "59/223, train_loss: 0.0889, step time: 0.1143\n",
      "60/223, train_loss: 0.1081, step time: 0.1428\n",
      "61/223, train_loss: 0.1089, step time: 0.1195\n",
      "62/223, train_loss: 0.1027, step time: 0.1106\n",
      "63/223, train_loss: 0.0905, step time: 0.1162\n",
      "64/223, train_loss: 0.1008, step time: 0.1057\n",
      "65/223, train_loss: 0.1119, step time: 0.1037\n",
      "66/223, train_loss: 0.1048, step time: 0.1001\n",
      "67/223, train_loss: 0.1081, step time: 0.1104\n",
      "68/223, train_loss: 0.0943, step time: 0.1245\n",
      "69/223, train_loss: 0.1170, step time: 0.1054\n",
      "70/223, train_loss: 0.1042, step time: 0.1023\n",
      "71/223, train_loss: 0.1052, step time: 0.1056\n",
      "72/223, train_loss: 0.0977, step time: 0.1236\n",
      "73/223, train_loss: 0.1040, step time: 0.1089\n",
      "74/223, train_loss: 0.1063, step time: 0.1084\n",
      "75/223, train_loss: 0.0960, step time: 0.1214\n",
      "76/223, train_loss: 0.1030, step time: 0.1130\n",
      "77/223, train_loss: 0.1085, step time: 0.1006\n",
      "78/223, train_loss: 0.1020, step time: 0.1210\n",
      "79/223, train_loss: 0.1075, step time: 0.1083\n",
      "80/223, train_loss: 0.1186, step time: 0.1097\n",
      "81/223, train_loss: 0.1087, step time: 0.1115\n",
      "82/223, train_loss: 0.0919, step time: 0.1231\n",
      "83/223, train_loss: 0.0889, step time: 0.1380\n",
      "84/223, train_loss: 0.0989, step time: 0.1265\n",
      "85/223, train_loss: 0.1022, step time: 0.1267\n",
      "86/223, train_loss: 0.0916, step time: 0.1086\n",
      "87/223, train_loss: 0.0991, step time: 0.1385\n",
      "88/223, train_loss: 0.0956, step time: 0.1235\n",
      "89/223, train_loss: 0.0985, step time: 0.1020\n",
      "90/223, train_loss: 0.0982, step time: 0.0997\n",
      "91/223, train_loss: 0.1024, step time: 0.1177\n",
      "92/223, train_loss: 0.1009, step time: 0.1227\n",
      "93/223, train_loss: 0.1032, step time: 0.1073\n",
      "94/223, train_loss: 0.1034, step time: 0.1110\n",
      "95/223, train_loss: 0.0945, step time: 0.1297\n",
      "96/223, train_loss: 0.1004, step time: 0.1156\n",
      "97/223, train_loss: 0.1028, step time: 0.0999\n",
      "98/223, train_loss: 0.0920, step time: 0.1009\n",
      "99/223, train_loss: 0.0976, step time: 0.1008\n",
      "100/223, train_loss: 0.0967, step time: 0.1738\n",
      "101/223, train_loss: 0.0959, step time: 0.1213\n",
      "102/223, train_loss: 0.0964, step time: 0.1166\n",
      "103/223, train_loss: 0.1036, step time: 0.1115\n",
      "104/223, train_loss: 0.0949, step time: 0.1013\n",
      "105/223, train_loss: 0.0920, step time: 0.1061\n",
      "106/223, train_loss: 0.1032, step time: 0.1025\n",
      "107/223, train_loss: 0.1117, step time: 0.1203\n",
      "108/223, train_loss: 0.1101, step time: 0.1005\n",
      "109/223, train_loss: 0.0939, step time: 0.1370\n",
      "110/223, train_loss: 0.0929, step time: 0.1274\n",
      "111/223, train_loss: 0.1002, step time: 0.1216\n",
      "112/223, train_loss: 0.0944, step time: 0.1107\n",
      "113/223, train_loss: 0.1007, step time: 0.1079\n",
      "114/223, train_loss: 0.1029, step time: 0.1288\n",
      "115/223, train_loss: 0.1071, step time: 0.1145\n",
      "116/223, train_loss: 0.0900, step time: 0.1279\n",
      "117/223, train_loss: 0.0949, step time: 0.1289\n",
      "118/223, train_loss: 0.0921, step time: 0.1185\n",
      "119/223, train_loss: 0.1000, step time: 0.1181\n",
      "120/223, train_loss: 0.0959, step time: 0.1144\n",
      "121/223, train_loss: 0.0936, step time: 0.1132\n",
      "122/223, train_loss: 0.0922, step time: 0.1121\n",
      "123/223, train_loss: 0.1143, step time: 0.1413\n",
      "124/223, train_loss: 0.0952, step time: 0.1063\n",
      "125/223, train_loss: 0.1078, step time: 0.1135\n",
      "126/223, train_loss: 0.1055, step time: 0.1403\n",
      "127/223, train_loss: 0.0905, step time: 0.1200\n",
      "128/223, train_loss: 0.1065, step time: 0.1140\n",
      "129/223, train_loss: 0.1031, step time: 0.1269\n",
      "130/223, train_loss: 0.1062, step time: 0.1198\n",
      "131/223, train_loss: 0.1127, step time: 0.1175\n",
      "132/223, train_loss: 0.3001, step time: 0.1063\n",
      "133/223, train_loss: 0.0905, step time: 0.1179\n",
      "134/223, train_loss: 0.0997, step time: 0.1004\n",
      "135/223, train_loss: 0.0940, step time: 0.0997\n",
      "136/223, train_loss: 0.1112, step time: 0.1020\n",
      "137/223, train_loss: 0.0992, step time: 0.1164\n",
      "138/223, train_loss: 0.0952, step time: 0.0988\n",
      "139/223, train_loss: 0.0994, step time: 0.1035\n",
      "140/223, train_loss: 0.0934, step time: 0.1008\n",
      "141/223, train_loss: 0.0997, step time: 0.1201\n",
      "142/223, train_loss: 0.0976, step time: 0.1003\n",
      "143/223, train_loss: 0.1077, step time: 0.1009\n",
      "144/223, train_loss: 0.1125, step time: 0.1156\n",
      "145/223, train_loss: 0.1100, step time: 0.1495\n",
      "146/223, train_loss: 0.0978, step time: 0.1305\n",
      "147/223, train_loss: 0.1001, step time: 0.1008\n",
      "148/223, train_loss: 0.0898, step time: 0.1116\n",
      "149/223, train_loss: 0.0957, step time: 0.1223\n",
      "150/223, train_loss: 0.0988, step time: 0.1063\n",
      "151/223, train_loss: 0.1010, step time: 0.1228\n",
      "152/223, train_loss: 0.1107, step time: 0.1202\n",
      "153/223, train_loss: 0.1027, step time: 0.1153\n",
      "154/223, train_loss: 0.0988, step time: 0.1046\n",
      "155/223, train_loss: 0.1092, step time: 0.1076\n",
      "156/223, train_loss: 0.0939, step time: 0.1320\n",
      "157/223, train_loss: 0.1034, step time: 0.1085\n",
      "158/223, train_loss: 0.0940, step time: 0.1142\n",
      "159/223, train_loss: 0.1041, step time: 0.1349\n",
      "160/223, train_loss: 0.1003, step time: 0.1104\n",
      "161/223, train_loss: 0.0999, step time: 0.1320\n",
      "162/223, train_loss: 0.1076, step time: 0.1250\n",
      "163/223, train_loss: 0.1046, step time: 0.1253\n",
      "164/223, train_loss: 0.1055, step time: 0.1045\n",
      "165/223, train_loss: 0.1018, step time: 0.1049\n",
      "166/223, train_loss: 0.1062, step time: 0.1074\n",
      "167/223, train_loss: 0.0959, step time: 0.1293\n",
      "168/223, train_loss: 0.1033, step time: 0.1043\n",
      "169/223, train_loss: 0.0954, step time: 0.1120\n",
      "170/223, train_loss: 0.0962, step time: 0.1127\n",
      "171/223, train_loss: 0.0975, step time: 0.1158\n",
      "172/223, train_loss: 0.0973, step time: 0.1173\n",
      "173/223, train_loss: 0.0961, step time: 0.1109\n",
      "174/223, train_loss: 0.0886, step time: 0.1055\n",
      "175/223, train_loss: 0.0923, step time: 0.1219\n",
      "176/223, train_loss: 0.1061, step time: 0.1147\n",
      "177/223, train_loss: 0.1015, step time: 0.1268\n",
      "178/223, train_loss: 0.0939, step time: 0.1335\n",
      "179/223, train_loss: 0.0967, step time: 0.1418\n",
      "180/223, train_loss: 0.0957, step time: 0.1001\n",
      "181/223, train_loss: 0.1000, step time: 0.1118\n",
      "182/223, train_loss: 0.1066, step time: 0.1227\n",
      "183/223, train_loss: 0.1095, step time: 0.1115\n",
      "184/223, train_loss: 0.0975, step time: 0.1474\n",
      "185/223, train_loss: 0.1133, step time: 0.0997\n",
      "186/223, train_loss: 0.1144, step time: 0.1003\n",
      "187/223, train_loss: 0.1123, step time: 0.1006\n",
      "188/223, train_loss: 0.1021, step time: 0.1091\n",
      "189/223, train_loss: 0.1017, step time: 0.1007\n",
      "190/223, train_loss: 0.0951, step time: 0.0988\n",
      "191/223, train_loss: 0.0996, step time: 0.1170\n",
      "192/223, train_loss: 0.0910, step time: 0.1128\n",
      "193/223, train_loss: 0.1004, step time: 0.0997\n",
      "194/223, train_loss: 0.1109, step time: 0.1099\n",
      "195/223, train_loss: 0.1062, step time: 0.1897\n",
      "196/223, train_loss: 0.1106, step time: 0.1066\n",
      "197/223, train_loss: 0.1070, step time: 0.1138\n",
      "198/223, train_loss: 0.0966, step time: 0.1360\n",
      "199/223, train_loss: 0.0973, step time: 0.1159\n",
      "200/223, train_loss: 0.0985, step time: 0.1164\n",
      "201/223, train_loss: 0.1018, step time: 0.1089\n",
      "202/223, train_loss: 0.1115, step time: 0.1055\n",
      "203/223, train_loss: 0.1141, step time: 0.1250\n",
      "204/223, train_loss: 0.0989, step time: 0.1060\n",
      "205/223, train_loss: 0.1024, step time: 0.1013\n",
      "206/223, train_loss: 0.0975, step time: 0.1059\n",
      "207/223, train_loss: 0.1043, step time: 0.1003\n",
      "208/223, train_loss: 0.0912, step time: 0.1232\n",
      "209/223, train_loss: 0.0910, step time: 0.0993\n",
      "210/223, train_loss: 0.1027, step time: 0.1144\n",
      "211/223, train_loss: 0.1117, step time: 0.1205\n",
      "212/223, train_loss: 0.1118, step time: 0.1163\n",
      "213/223, train_loss: 0.0905, step time: 0.1059\n",
      "214/223, train_loss: 0.0979, step time: 0.1154\n",
      "215/223, train_loss: 0.1087, step time: 0.1316\n",
      "216/223, train_loss: 0.0967, step time: 0.1022\n",
      "217/223, train_loss: 0.1000, step time: 0.1012\n",
      "218/223, train_loss: 0.0976, step time: 0.1007\n",
      "219/223, train_loss: 0.1002, step time: 0.1158\n",
      "220/223, train_loss: 0.0979, step time: 0.0991\n",
      "221/223, train_loss: 0.0954, step time: 0.0992\n",
      "222/223, train_loss: 0.1008, step time: 0.0997\n",
      "223/223, train_loss: 0.0936, step time: 0.1001\n",
      "epoch 213 average loss: 0.1016\n",
      "time consuming of epoch 213 is: 94.4127\n",
      "----------\n",
      "epoch 214/300\n",
      "1/223, train_loss: 0.1004, step time: 0.1141\n",
      "2/223, train_loss: 0.1084, step time: 0.1044\n",
      "3/223, train_loss: 0.1060, step time: 0.1176\n",
      "4/223, train_loss: 0.1003, step time: 0.1197\n",
      "5/223, train_loss: 0.1037, step time: 0.1085\n",
      "6/223, train_loss: 0.1012, step time: 0.1063\n",
      "7/223, train_loss: 0.1018, step time: 0.1157\n",
      "8/223, train_loss: 0.0954, step time: 0.1175\n",
      "9/223, train_loss: 0.1114, step time: 0.1159\n",
      "10/223, train_loss: 0.1032, step time: 0.1090\n",
      "11/223, train_loss: 0.0921, step time: 0.1141\n",
      "12/223, train_loss: 0.0910, step time: 0.0997\n",
      "13/223, train_loss: 0.1091, step time: 0.1152\n",
      "14/223, train_loss: 0.1000, step time: 0.1169\n",
      "15/223, train_loss: 0.0931, step time: 0.1026\n",
      "16/223, train_loss: 0.0901, step time: 0.1124\n",
      "17/223, train_loss: 0.1071, step time: 0.1079\n",
      "18/223, train_loss: 0.1085, step time: 0.1015\n",
      "19/223, train_loss: 0.1121, step time: 0.1313\n",
      "20/223, train_loss: 0.0964, step time: 0.1200\n",
      "21/223, train_loss: 0.0972, step time: 0.0995\n",
      "22/223, train_loss: 0.1095, step time: 0.1000\n",
      "23/223, train_loss: 0.0976, step time: 0.1247\n",
      "24/223, train_loss: 0.0963, step time: 0.1083\n",
      "25/223, train_loss: 0.1018, step time: 0.1217\n",
      "26/223, train_loss: 0.1014, step time: 0.0999\n",
      "27/223, train_loss: 0.1006, step time: 0.1037\n",
      "28/223, train_loss: 0.1131, step time: 0.1044\n",
      "29/223, train_loss: 0.0987, step time: 0.1039\n",
      "30/223, train_loss: 0.1055, step time: 0.1180\n",
      "31/223, train_loss: 0.1004, step time: 0.1126\n",
      "32/223, train_loss: 0.1071, step time: 0.1002\n",
      "33/223, train_loss: 0.1067, step time: 0.1382\n",
      "34/223, train_loss: 0.0931, step time: 0.1184\n",
      "35/223, train_loss: 0.1056, step time: 0.1070\n",
      "36/223, train_loss: 0.1021, step time: 0.1051\n",
      "37/223, train_loss: 0.1020, step time: 0.1219\n",
      "38/223, train_loss: 0.1073, step time: 0.1054\n",
      "39/223, train_loss: 0.0911, step time: 0.1229\n",
      "40/223, train_loss: 0.1013, step time: 0.1211\n",
      "41/223, train_loss: 0.1056, step time: 0.1125\n",
      "42/223, train_loss: 0.0968, step time: 0.0999\n",
      "43/223, train_loss: 0.1028, step time: 0.1183\n",
      "44/223, train_loss: 0.1047, step time: 0.1012\n",
      "45/223, train_loss: 0.0849, step time: 0.1313\n",
      "46/223, train_loss: 0.1074, step time: 0.1363\n",
      "47/223, train_loss: 0.1004, step time: 0.1251\n",
      "48/223, train_loss: 0.1037, step time: 0.1005\n",
      "49/223, train_loss: 0.0951, step time: 0.1063\n",
      "50/223, train_loss: 0.0905, step time: 0.0995\n",
      "51/223, train_loss: 0.0999, step time: 0.1227\n",
      "52/223, train_loss: 0.0973, step time: 0.1325\n",
      "53/223, train_loss: 0.0886, step time: 0.1006\n",
      "54/223, train_loss: 0.1045, step time: 0.1009\n",
      "55/223, train_loss: 0.1092, step time: 0.1010\n",
      "56/223, train_loss: 0.1003, step time: 0.1008\n",
      "57/223, train_loss: 0.0900, step time: 0.1158\n",
      "58/223, train_loss: 0.1046, step time: 0.1115\n",
      "59/223, train_loss: 0.0962, step time: 0.1128\n",
      "60/223, train_loss: 0.0975, step time: 0.1289\n",
      "61/223, train_loss: 0.0952, step time: 0.1331\n",
      "62/223, train_loss: 0.0944, step time: 0.1184\n",
      "63/223, train_loss: 0.0930, step time: 0.1000\n",
      "64/223, train_loss: 0.1003, step time: 0.1101\n",
      "65/223, train_loss: 0.1069, step time: 0.1241\n",
      "66/223, train_loss: 0.0917, step time: 0.1161\n",
      "67/223, train_loss: 0.1190, step time: 0.1160\n",
      "68/223, train_loss: 0.1032, step time: 0.0998\n",
      "69/223, train_loss: 0.0991, step time: 0.1080\n",
      "70/223, train_loss: 0.0969, step time: 0.1106\n",
      "71/223, train_loss: 0.1006, step time: 0.0999\n",
      "72/223, train_loss: 0.0956, step time: 0.1367\n",
      "73/223, train_loss: 0.1043, step time: 0.1002\n",
      "74/223, train_loss: 0.1119, step time: 0.1000\n",
      "75/223, train_loss: 0.1041, step time: 0.1275\n",
      "76/223, train_loss: 0.0950, step time: 0.1067\n",
      "77/223, train_loss: 0.1072, step time: 0.1083\n",
      "78/223, train_loss: 0.0998, step time: 0.1080\n",
      "79/223, train_loss: 0.1075, step time: 0.1229\n",
      "80/223, train_loss: 0.1044, step time: 0.1033\n",
      "81/223, train_loss: 0.0982, step time: 0.1215\n",
      "82/223, train_loss: 0.1099, step time: 0.1213\n",
      "83/223, train_loss: 0.0994, step time: 0.0999\n",
      "84/223, train_loss: 0.1128, step time: 0.1032\n",
      "85/223, train_loss: 0.1050, step time: 0.1201\n",
      "86/223, train_loss: 0.1032, step time: 0.1065\n",
      "87/223, train_loss: 0.1035, step time: 0.1196\n",
      "88/223, train_loss: 0.1045, step time: 0.1155\n",
      "89/223, train_loss: 0.1196, step time: 0.1137\n",
      "90/223, train_loss: 0.1192, step time: 0.1057\n",
      "91/223, train_loss: 0.0903, step time: 0.1293\n",
      "92/223, train_loss: 0.1005, step time: 0.1009\n",
      "93/223, train_loss: 0.1016, step time: 0.1067\n",
      "94/223, train_loss: 0.1046, step time: 0.1355\n",
      "95/223, train_loss: 0.0933, step time: 0.1297\n",
      "96/223, train_loss: 0.0979, step time: 0.0999\n",
      "97/223, train_loss: 0.1081, step time: 0.1216\n",
      "98/223, train_loss: 0.1027, step time: 0.1243\n",
      "99/223, train_loss: 0.1114, step time: 0.1004\n",
      "100/223, train_loss: 0.0981, step time: 0.1076\n",
      "101/223, train_loss: 0.0885, step time: 0.1130\n",
      "102/223, train_loss: 0.0945, step time: 0.0999\n",
      "103/223, train_loss: 0.0954, step time: 0.1000\n",
      "104/223, train_loss: 0.1066, step time: 0.1140\n",
      "105/223, train_loss: 0.0915, step time: 0.1088\n",
      "106/223, train_loss: 0.0956, step time: 0.1129\n",
      "107/223, train_loss: 0.1100, step time: 0.1068\n",
      "108/223, train_loss: 0.1077, step time: 0.1136\n",
      "109/223, train_loss: 0.1080, step time: 0.1001\n",
      "110/223, train_loss: 0.1024, step time: 0.1005\n",
      "111/223, train_loss: 0.1072, step time: 0.1488\n",
      "112/223, train_loss: 0.0958, step time: 0.1014\n",
      "113/223, train_loss: 0.1029, step time: 0.1049\n",
      "114/223, train_loss: 0.1016, step time: 0.1018\n",
      "115/223, train_loss: 0.0961, step time: 0.1122\n",
      "116/223, train_loss: 0.0943, step time: 0.1017\n",
      "117/223, train_loss: 0.1043, step time: 0.0995\n",
      "118/223, train_loss: 0.1109, step time: 0.1013\n",
      "119/223, train_loss: 0.0997, step time: 0.1520\n",
      "120/223, train_loss: 0.0969, step time: 0.1001\n",
      "121/223, train_loss: 0.0917, step time: 0.1006\n",
      "122/223, train_loss: 0.1123, step time: 0.1166\n",
      "123/223, train_loss: 0.0994, step time: 0.1058\n",
      "124/223, train_loss: 0.0942, step time: 0.1147\n",
      "125/223, train_loss: 0.0923, step time: 0.1054\n",
      "126/223, train_loss: 0.1038, step time: 0.1220\n",
      "127/223, train_loss: 0.1071, step time: 0.1350\n",
      "128/223, train_loss: 0.0992, step time: 0.1153\n",
      "129/223, train_loss: 0.0988, step time: 0.1145\n",
      "130/223, train_loss: 0.0917, step time: 0.1101\n",
      "131/223, train_loss: 0.0917, step time: 0.1003\n",
      "132/223, train_loss: 0.1075, step time: 0.1194\n",
      "133/223, train_loss: 0.0916, step time: 0.1001\n",
      "134/223, train_loss: 0.2982, step time: 0.1093\n",
      "135/223, train_loss: 0.0932, step time: 0.1096\n",
      "136/223, train_loss: 0.0885, step time: 0.1075\n",
      "137/223, train_loss: 0.0900, step time: 0.1086\n",
      "138/223, train_loss: 0.0899, step time: 0.1115\n",
      "139/223, train_loss: 0.0974, step time: 0.1262\n",
      "140/223, train_loss: 0.1054, step time: 0.1003\n",
      "141/223, train_loss: 0.1099, step time: 0.1107\n",
      "142/223, train_loss: 0.1112, step time: 0.1000\n",
      "143/223, train_loss: 0.1016, step time: 0.1250\n",
      "144/223, train_loss: 0.1093, step time: 0.1173\n",
      "145/223, train_loss: 0.0986, step time: 0.1128\n",
      "146/223, train_loss: 0.0897, step time: 0.1019\n",
      "147/223, train_loss: 0.1082, step time: 0.0983\n",
      "148/223, train_loss: 0.0999, step time: 0.0992\n",
      "149/223, train_loss: 0.1001, step time: 0.1156\n",
      "150/223, train_loss: 0.0989, step time: 0.1094\n",
      "151/223, train_loss: 0.1077, step time: 0.1080\n",
      "152/223, train_loss: 0.0975, step time: 0.1111\n",
      "153/223, train_loss: 0.0950, step time: 0.0997\n",
      "154/223, train_loss: 0.1046, step time: 0.1117\n",
      "155/223, train_loss: 0.0972, step time: 0.1024\n",
      "156/223, train_loss: 0.1100, step time: 0.1012\n",
      "157/223, train_loss: 0.1011, step time: 0.1135\n",
      "158/223, train_loss: 0.1022, step time: 0.1106\n",
      "159/223, train_loss: 0.1107, step time: 0.1265\n",
      "160/223, train_loss: 0.1053, step time: 0.1105\n",
      "161/223, train_loss: 0.0887, step time: 0.1159\n",
      "162/223, train_loss: 0.1112, step time: 0.1275\n",
      "163/223, train_loss: 0.1058, step time: 0.1441\n",
      "164/223, train_loss: 0.0997, step time: 0.1086\n",
      "165/223, train_loss: 0.1023, step time: 0.0994\n",
      "166/223, train_loss: 0.1003, step time: 0.1000\n",
      "167/223, train_loss: 0.1079, step time: 0.1080\n",
      "168/223, train_loss: 0.1015, step time: 0.1058\n",
      "169/223, train_loss: 0.0990, step time: 0.1206\n",
      "170/223, train_loss: 0.0933, step time: 0.1381\n",
      "171/223, train_loss: 0.1019, step time: 0.1022\n",
      "172/223, train_loss: 0.0988, step time: 0.1083\n",
      "173/223, train_loss: 0.0999, step time: 0.1133\n",
      "174/223, train_loss: 0.1067, step time: 0.1191\n",
      "175/223, train_loss: 0.0972, step time: 0.1107\n",
      "176/223, train_loss: 0.1060, step time: 0.1299\n",
      "177/223, train_loss: 0.1015, step time: 0.1015\n",
      "178/223, train_loss: 0.1066, step time: 0.1006\n",
      "179/223, train_loss: 0.0937, step time: 0.1045\n",
      "180/223, train_loss: 0.1017, step time: 0.1219\n",
      "181/223, train_loss: 0.1119, step time: 0.1015\n",
      "182/223, train_loss: 0.1014, step time: 0.1576\n",
      "183/223, train_loss: 0.0967, step time: 0.1569\n",
      "184/223, train_loss: 0.0973, step time: 0.0998\n",
      "185/223, train_loss: 0.0963, step time: 0.0999\n",
      "186/223, train_loss: 0.0961, step time: 0.1001\n",
      "187/223, train_loss: 0.1072, step time: 0.1218\n",
      "188/223, train_loss: 0.1027, step time: 0.1003\n",
      "189/223, train_loss: 0.1001, step time: 0.1049\n",
      "190/223, train_loss: 0.0982, step time: 0.1002\n",
      "191/223, train_loss: 0.0966, step time: 0.0991\n",
      "192/223, train_loss: 0.1078, step time: 0.1004\n",
      "193/223, train_loss: 0.0959, step time: 0.1003\n",
      "194/223, train_loss: 0.0944, step time: 0.1056\n",
      "195/223, train_loss: 0.0929, step time: 0.1003\n",
      "196/223, train_loss: 0.1008, step time: 0.1112\n",
      "197/223, train_loss: 0.0998, step time: 0.1089\n",
      "198/223, train_loss: 0.0989, step time: 0.1257\n",
      "199/223, train_loss: 0.0991, step time: 0.0998\n",
      "200/223, train_loss: 0.0903, step time: 0.1109\n",
      "201/223, train_loss: 0.1065, step time: 0.1059\n",
      "202/223, train_loss: 0.0997, step time: 0.1212\n",
      "203/223, train_loss: 0.1107, step time: 0.1316\n",
      "204/223, train_loss: 0.1068, step time: 0.1060\n",
      "205/223, train_loss: 0.1035, step time: 0.1182\n",
      "206/223, train_loss: 0.0963, step time: 0.1164\n",
      "207/223, train_loss: 0.1067, step time: 0.1010\n",
      "208/223, train_loss: 0.1033, step time: 0.1013\n",
      "209/223, train_loss: 0.0971, step time: 0.0991\n",
      "210/223, train_loss: 0.0985, step time: 0.1105\n",
      "211/223, train_loss: 0.0964, step time: 0.1115\n",
      "212/223, train_loss: 0.1069, step time: 0.1128\n",
      "213/223, train_loss: 0.0973, step time: 0.1114\n",
      "214/223, train_loss: 0.0885, step time: 0.1094\n",
      "215/223, train_loss: 0.0935, step time: 0.1198\n",
      "216/223, train_loss: 0.0948, step time: 0.1088\n",
      "217/223, train_loss: 0.0940, step time: 0.1108\n",
      "218/223, train_loss: 0.0893, step time: 0.0985\n",
      "219/223, train_loss: 0.0960, step time: 0.0990\n",
      "220/223, train_loss: 0.0947, step time: 0.1012\n",
      "221/223, train_loss: 0.0978, step time: 0.1002\n",
      "222/223, train_loss: 0.1006, step time: 0.0996\n",
      "223/223, train_loss: 0.0976, step time: 0.1000\n",
      "epoch 214 average loss: 0.1016\n",
      "time consuming of epoch 214 is: 89.9286\n",
      "----------\n",
      "epoch 215/300\n",
      "1/223, train_loss: 0.0949, step time: 0.1065\n",
      "2/223, train_loss: 0.1052, step time: 0.1011\n",
      "3/223, train_loss: 0.1014, step time: 0.1145\n",
      "4/223, train_loss: 0.0955, step time: 0.1282\n",
      "5/223, train_loss: 0.1057, step time: 0.1105\n",
      "6/223, train_loss: 0.0988, step time: 0.1218\n",
      "7/223, train_loss: 0.1045, step time: 0.1399\n",
      "8/223, train_loss: 0.1076, step time: 0.1199\n",
      "9/223, train_loss: 0.1145, step time: 0.1070\n",
      "10/223, train_loss: 0.0946, step time: 0.1204\n",
      "11/223, train_loss: 0.0965, step time: 0.1263\n",
      "12/223, train_loss: 0.1089, step time: 0.1004\n",
      "13/223, train_loss: 0.0952, step time: 0.1533\n",
      "14/223, train_loss: 0.1083, step time: 0.1251\n",
      "15/223, train_loss: 0.0933, step time: 0.1284\n",
      "16/223, train_loss: 0.0960, step time: 0.1155\n",
      "17/223, train_loss: 0.1069, step time: 0.1165\n",
      "18/223, train_loss: 0.0942, step time: 0.1083\n",
      "19/223, train_loss: 0.1021, step time: 0.1237\n",
      "20/223, train_loss: 0.0954, step time: 0.1079\n",
      "21/223, train_loss: 0.1136, step time: 0.1110\n",
      "22/223, train_loss: 0.1045, step time: 0.1200\n",
      "23/223, train_loss: 0.0942, step time: 0.1172\n",
      "24/223, train_loss: 0.0907, step time: 0.1000\n",
      "25/223, train_loss: 0.0968, step time: 0.1174\n",
      "26/223, train_loss: 0.1100, step time: 0.1045\n",
      "27/223, train_loss: 0.1052, step time: 0.1290\n",
      "28/223, train_loss: 0.0963, step time: 0.1009\n",
      "29/223, train_loss: 0.0940, step time: 0.1380\n",
      "30/223, train_loss: 0.0900, step time: 0.1014\n",
      "31/223, train_loss: 0.1053, step time: 0.1216\n",
      "32/223, train_loss: 0.1102, step time: 0.1025\n",
      "33/223, train_loss: 0.1066, step time: 0.0999\n",
      "34/223, train_loss: 0.0961, step time: 0.1096\n",
      "35/223, train_loss: 0.1014, step time: 0.1365\n",
      "36/223, train_loss: 0.0961, step time: 0.0999\n",
      "37/223, train_loss: 0.1004, step time: 0.1066\n",
      "38/223, train_loss: 0.1114, step time: 0.1784\n",
      "39/223, train_loss: 0.0996, step time: 0.0999\n",
      "40/223, train_loss: 0.0971, step time: 0.1010\n",
      "41/223, train_loss: 0.1082, step time: 0.1004\n",
      "42/223, train_loss: 0.0898, step time: 0.1015\n",
      "43/223, train_loss: 0.0937, step time: 0.1002\n",
      "44/223, train_loss: 0.0986, step time: 0.1006\n",
      "45/223, train_loss: 0.1009, step time: 0.1017\n",
      "46/223, train_loss: 0.1111, step time: 0.1303\n",
      "47/223, train_loss: 0.0987, step time: 0.1161\n",
      "48/223, train_loss: 0.0985, step time: 0.1002\n",
      "49/223, train_loss: 0.0984, step time: 0.1003\n",
      "50/223, train_loss: 0.0861, step time: 0.1001\n",
      "51/223, train_loss: 0.1001, step time: 0.1131\n",
      "52/223, train_loss: 0.0995, step time: 0.1150\n",
      "53/223, train_loss: 0.0940, step time: 0.1558\n",
      "54/223, train_loss: 0.1125, step time: 0.1225\n",
      "55/223, train_loss: 0.1085, step time: 0.1134\n",
      "56/223, train_loss: 0.0983, step time: 0.1202\n",
      "57/223, train_loss: 0.1001, step time: 0.0994\n",
      "58/223, train_loss: 0.0986, step time: 0.1310\n",
      "59/223, train_loss: 0.0958, step time: 0.0999\n",
      "60/223, train_loss: 0.0973, step time: 0.1109\n",
      "61/223, train_loss: 0.1071, step time: 0.1059\n",
      "62/223, train_loss: 0.1058, step time: 0.1384\n",
      "63/223, train_loss: 0.1014, step time: 0.1288\n",
      "64/223, train_loss: 0.0927, step time: 0.1178\n",
      "65/223, train_loss: 0.1048, step time: 0.1141\n",
      "66/223, train_loss: 0.0976, step time: 0.1111\n",
      "67/223, train_loss: 0.0951, step time: 0.1120\n",
      "68/223, train_loss: 0.1015, step time: 0.1037\n",
      "69/223, train_loss: 0.0940, step time: 0.0995\n",
      "70/223, train_loss: 0.1143, step time: 0.1061\n",
      "71/223, train_loss: 0.0998, step time: 0.1149\n",
      "72/223, train_loss: 0.0970, step time: 0.1244\n",
      "73/223, train_loss: 0.1019, step time: 0.1110\n",
      "74/223, train_loss: 0.0997, step time: 0.1055\n",
      "75/223, train_loss: 0.1026, step time: 0.1328\n",
      "76/223, train_loss: 0.1023, step time: 0.1136\n",
      "77/223, train_loss: 0.0980, step time: 0.1017\n",
      "78/223, train_loss: 0.1018, step time: 0.1000\n",
      "79/223, train_loss: 0.0914, step time: 0.1027\n",
      "80/223, train_loss: 0.0921, step time: 0.1013\n",
      "81/223, train_loss: 0.0935, step time: 0.1171\n",
      "82/223, train_loss: 0.1037, step time: 0.1003\n",
      "83/223, train_loss: 0.0935, step time: 0.1406\n",
      "84/223, train_loss: 0.0887, step time: 0.1068\n",
      "85/223, train_loss: 0.1151, step time: 0.1081\n",
      "86/223, train_loss: 0.0918, step time: 0.1162\n",
      "87/223, train_loss: 0.0954, step time: 0.1144\n",
      "88/223, train_loss: 0.0971, step time: 0.1087\n",
      "89/223, train_loss: 0.1072, step time: 0.0996\n",
      "90/223, train_loss: 0.0949, step time: 0.1002\n",
      "91/223, train_loss: 0.0918, step time: 0.1223\n",
      "92/223, train_loss: 0.1030, step time: 0.1243\n",
      "93/223, train_loss: 0.0960, step time: 0.1224\n",
      "94/223, train_loss: 0.0926, step time: 0.0998\n",
      "95/223, train_loss: 0.0943, step time: 0.1165\n",
      "96/223, train_loss: 0.0955, step time: 0.1152\n",
      "97/223, train_loss: 0.1100, step time: 0.1281\n",
      "98/223, train_loss: 0.1108, step time: 0.1260\n",
      "99/223, train_loss: 0.1068, step time: 0.1220\n",
      "100/223, train_loss: 0.1126, step time: 0.1070\n",
      "101/223, train_loss: 0.1060, step time: 0.1044\n",
      "102/223, train_loss: 0.1041, step time: 0.1079\n",
      "103/223, train_loss: 0.0887, step time: 0.1104\n",
      "104/223, train_loss: 0.0917, step time: 0.1118\n",
      "105/223, train_loss: 0.0996, step time: 0.1130\n",
      "106/223, train_loss: 0.1043, step time: 0.0997\n",
      "107/223, train_loss: 0.1048, step time: 0.1069\n",
      "108/223, train_loss: 0.0943, step time: 0.1075\n",
      "109/223, train_loss: 0.1073, step time: 0.1051\n",
      "110/223, train_loss: 0.2997, step time: 0.1016\n",
      "111/223, train_loss: 0.0983, step time: 0.1106\n",
      "112/223, train_loss: 0.0913, step time: 0.1111\n",
      "113/223, train_loss: 0.1067, step time: 0.1178\n",
      "114/223, train_loss: 0.1031, step time: 0.1260\n",
      "115/223, train_loss: 0.1016, step time: 0.1037\n",
      "116/223, train_loss: 0.1050, step time: 0.1126\n",
      "117/223, train_loss: 0.1145, step time: 0.1164\n",
      "118/223, train_loss: 0.1136, step time: 0.1622\n",
      "119/223, train_loss: 0.1156, step time: 0.1205\n",
      "120/223, train_loss: 0.0917, step time: 0.1077\n",
      "121/223, train_loss: 0.0981, step time: 0.1120\n",
      "122/223, train_loss: 0.1019, step time: 0.1008\n",
      "123/223, train_loss: 0.1019, step time: 0.1021\n",
      "124/223, train_loss: 0.1004, step time: 0.1211\n",
      "125/223, train_loss: 0.1050, step time: 0.1060\n",
      "126/223, train_loss: 0.1051, step time: 0.1095\n",
      "127/223, train_loss: 0.0929, step time: 0.1101\n",
      "128/223, train_loss: 0.1068, step time: 0.1168\n",
      "129/223, train_loss: 0.0885, step time: 0.1249\n",
      "130/223, train_loss: 0.1269, step time: 0.1126\n",
      "131/223, train_loss: 0.0948, step time: 0.1200\n",
      "132/223, train_loss: 0.1114, step time: 0.1087\n",
      "133/223, train_loss: 0.0915, step time: 0.1080\n",
      "134/223, train_loss: 0.1054, step time: 0.1136\n",
      "135/223, train_loss: 0.1069, step time: 0.1049\n",
      "136/223, train_loss: 0.0907, step time: 0.0998\n",
      "137/223, train_loss: 0.1007, step time: 0.1015\n",
      "138/223, train_loss: 0.0983, step time: 0.1004\n",
      "139/223, train_loss: 0.1008, step time: 0.1168\n",
      "140/223, train_loss: 0.0995, step time: 0.1003\n",
      "141/223, train_loss: 0.0921, step time: 0.1061\n",
      "142/223, train_loss: 0.1001, step time: 0.1056\n",
      "143/223, train_loss: 0.0984, step time: 0.1085\n",
      "144/223, train_loss: 0.0982, step time: 0.1164\n",
      "145/223, train_loss: 0.1067, step time: 0.1062\n",
      "146/223, train_loss: 0.1044, step time: 0.0991\n",
      "147/223, train_loss: 0.0947, step time: 0.0993\n",
      "148/223, train_loss: 0.0993, step time: 0.1232\n",
      "149/223, train_loss: 0.1030, step time: 0.1105\n",
      "150/223, train_loss: 0.0930, step time: 0.1351\n",
      "151/223, train_loss: 0.0968, step time: 0.1385\n",
      "152/223, train_loss: 0.0974, step time: 0.1148\n",
      "153/223, train_loss: 0.1070, step time: 0.1439\n",
      "154/223, train_loss: 0.1109, step time: 0.1278\n",
      "155/223, train_loss: 0.1053, step time: 0.1173\n",
      "156/223, train_loss: 0.0985, step time: 0.1019\n",
      "157/223, train_loss: 0.1127, step time: 0.1003\n",
      "158/223, train_loss: 0.0960, step time: 0.1051\n",
      "159/223, train_loss: 0.1065, step time: 0.1146\n",
      "160/223, train_loss: 0.0957, step time: 0.1093\n",
      "161/223, train_loss: 0.0906, step time: 0.1137\n",
      "162/223, train_loss: 0.0994, step time: 0.1171\n",
      "163/223, train_loss: 0.0967, step time: 0.1451\n",
      "164/223, train_loss: 0.1084, step time: 0.1064\n",
      "165/223, train_loss: 0.0900, step time: 0.1052\n",
      "166/223, train_loss: 0.0980, step time: 0.1048\n",
      "167/223, train_loss: 0.1036, step time: 0.0991\n",
      "168/223, train_loss: 0.0970, step time: 0.1005\n",
      "169/223, train_loss: 0.1119, step time: 0.1182\n",
      "170/223, train_loss: 0.1109, step time: 0.1007\n",
      "171/223, train_loss: 0.1009, step time: 0.1005\n",
      "172/223, train_loss: 0.0892, step time: 0.1035\n",
      "173/223, train_loss: 0.0918, step time: 0.1000\n",
      "174/223, train_loss: 0.1029, step time: 0.1050\n",
      "175/223, train_loss: 0.1083, step time: 0.1199\n",
      "176/223, train_loss: 0.0952, step time: 0.1103\n",
      "177/223, train_loss: 0.0878, step time: 0.1070\n",
      "178/223, train_loss: 0.0970, step time: 0.1230\n",
      "179/223, train_loss: 0.0978, step time: 0.1165\n",
      "180/223, train_loss: 0.1023, step time: 0.1050\n",
      "181/223, train_loss: 0.1037, step time: 0.1006\n",
      "182/223, train_loss: 0.0917, step time: 0.1083\n",
      "183/223, train_loss: 0.1060, step time: 0.1178\n",
      "184/223, train_loss: 0.1110, step time: 0.1157\n",
      "185/223, train_loss: 0.1097, step time: 0.1109\n",
      "186/223, train_loss: 0.1028, step time: 0.1129\n",
      "187/223, train_loss: 0.0998, step time: 0.1072\n",
      "188/223, train_loss: 0.1027, step time: 0.1068\n",
      "189/223, train_loss: 0.0932, step time: 0.1168\n",
      "190/223, train_loss: 0.0957, step time: 0.1222\n",
      "191/223, train_loss: 0.1049, step time: 0.1005\n",
      "192/223, train_loss: 0.1020, step time: 0.1215\n",
      "193/223, train_loss: 0.0938, step time: 0.1186\n",
      "194/223, train_loss: 0.0939, step time: 0.1087\n",
      "195/223, train_loss: 0.1086, step time: 0.1140\n",
      "196/223, train_loss: 0.1108, step time: 0.1005\n",
      "197/223, train_loss: 0.0929, step time: 0.1085\n",
      "198/223, train_loss: 0.1023, step time: 0.1089\n",
      "199/223, train_loss: 0.0974, step time: 0.1277\n",
      "200/223, train_loss: 0.1095, step time: 0.1056\n",
      "201/223, train_loss: 0.0987, step time: 0.1131\n",
      "202/223, train_loss: 0.1002, step time: 0.1089\n",
      "203/223, train_loss: 0.0985, step time: 0.1158\n",
      "204/223, train_loss: 0.1124, step time: 0.1147\n",
      "205/223, train_loss: 0.0980, step time: 0.1093\n",
      "206/223, train_loss: 0.1025, step time: 0.1066\n",
      "207/223, train_loss: 0.1066, step time: 0.1074\n",
      "208/223, train_loss: 0.1061, step time: 0.1020\n",
      "209/223, train_loss: 0.1058, step time: 0.1193\n",
      "210/223, train_loss: 0.0889, step time: 0.0992\n",
      "211/223, train_loss: 0.0939, step time: 0.0987\n",
      "212/223, train_loss: 0.0985, step time: 0.0995\n",
      "213/223, train_loss: 0.1015, step time: 0.1089\n",
      "214/223, train_loss: 0.1070, step time: 0.1013\n",
      "215/223, train_loss: 0.1082, step time: 0.0997\n",
      "216/223, train_loss: 0.1045, step time: 0.1009\n",
      "217/223, train_loss: 0.1018, step time: 0.1076\n",
      "218/223, train_loss: 0.1123, step time: 0.1011\n",
      "219/223, train_loss: 0.0920, step time: 0.1049\n",
      "220/223, train_loss: 0.1058, step time: 0.1002\n",
      "221/223, train_loss: 0.1030, step time: 0.1000\n",
      "222/223, train_loss: 0.0953, step time: 0.0988\n",
      "223/223, train_loss: 0.1035, step time: 0.0996\n",
      "epoch 215 average loss: 0.1016\n",
      "saved new best metric model\n",
      "current epoch: 215 current mean dice: 0.8609 tc: 0.9219 wt: 0.8705 et: 0.7902\n",
      "best mean dice: 0.8609 at epoch: 215\n",
      "time consuming of epoch 215 is: 91.2484\n",
      "----------\n",
      "epoch 216/300\n",
      "1/223, train_loss: 0.0914, step time: 0.1055\n",
      "2/223, train_loss: 0.1036, step time: 0.1004\n",
      "3/223, train_loss: 0.1011, step time: 0.1157\n",
      "4/223, train_loss: 0.1032, step time: 0.1283\n",
      "5/223, train_loss: 0.0975, step time: 0.1065\n",
      "6/223, train_loss: 0.0970, step time: 0.1060\n",
      "7/223, train_loss: 0.0915, step time: 0.1018\n",
      "8/223, train_loss: 0.1094, step time: 0.1056\n",
      "9/223, train_loss: 0.0948, step time: 0.1396\n",
      "10/223, train_loss: 0.0975, step time: 0.1005\n",
      "11/223, train_loss: 0.1046, step time: 0.1011\n",
      "12/223, train_loss: 0.0952, step time: 0.1058\n",
      "13/223, train_loss: 0.0992, step time: 0.1049\n",
      "14/223, train_loss: 0.1014, step time: 0.1128\n",
      "15/223, train_loss: 0.1101, step time: 0.1145\n",
      "16/223, train_loss: 0.0940, step time: 0.1008\n",
      "17/223, train_loss: 0.1140, step time: 0.1046\n",
      "18/223, train_loss: 0.1024, step time: 0.1070\n",
      "19/223, train_loss: 0.1033, step time: 0.1010\n",
      "20/223, train_loss: 0.1046, step time: 0.1007\n",
      "21/223, train_loss: 0.1021, step time: 0.1119\n",
      "22/223, train_loss: 0.1018, step time: 0.1002\n",
      "23/223, train_loss: 0.1022, step time: 0.1006\n",
      "24/223, train_loss: 0.0947, step time: 0.1009\n",
      "25/223, train_loss: 0.1011, step time: 0.1054\n",
      "26/223, train_loss: 0.0994, step time: 0.1003\n",
      "27/223, train_loss: 0.1064, step time: 0.1082\n",
      "28/223, train_loss: 0.1053, step time: 0.1053\n",
      "29/223, train_loss: 0.0960, step time: 0.0993\n",
      "30/223, train_loss: 0.0849, step time: 0.1038\n",
      "31/223, train_loss: 0.0943, step time: 0.1000\n",
      "32/223, train_loss: 0.0906, step time: 0.1119\n",
      "33/223, train_loss: 0.0942, step time: 0.1009\n",
      "34/223, train_loss: 0.0936, step time: 0.1128\n",
      "35/223, train_loss: 0.0949, step time: 0.1099\n",
      "36/223, train_loss: 0.0978, step time: 0.1260\n",
      "37/223, train_loss: 0.0975, step time: 0.1158\n",
      "38/223, train_loss: 0.0979, step time: 0.0998\n",
      "39/223, train_loss: 0.1046, step time: 0.1136\n",
      "40/223, train_loss: 0.0972, step time: 0.0992\n",
      "41/223, train_loss: 0.1019, step time: 0.1110\n",
      "42/223, train_loss: 0.0920, step time: 0.1044\n",
      "43/223, train_loss: 0.1077, step time: 0.1116\n",
      "44/223, train_loss: 0.1114, step time: 0.1155\n",
      "45/223, train_loss: 0.0886, step time: 0.1137\n",
      "46/223, train_loss: 0.1091, step time: 0.1101\n",
      "47/223, train_loss: 0.0941, step time: 0.1291\n",
      "48/223, train_loss: 0.0937, step time: 0.1158\n",
      "49/223, train_loss: 0.1003, step time: 0.1055\n",
      "50/223, train_loss: 0.0974, step time: 0.1066\n",
      "51/223, train_loss: 0.0989, step time: 0.0999\n",
      "52/223, train_loss: 0.1039, step time: 0.1004\n",
      "53/223, train_loss: 0.1102, step time: 0.1148\n",
      "54/223, train_loss: 0.1071, step time: 0.1401\n",
      "55/223, train_loss: 0.0923, step time: 0.1179\n",
      "56/223, train_loss: 0.1050, step time: 0.1132\n",
      "57/223, train_loss: 0.1109, step time: 0.1001\n",
      "58/223, train_loss: 0.1009, step time: 0.0991\n",
      "59/223, train_loss: 0.1114, step time: 0.1003\n",
      "60/223, train_loss: 0.0857, step time: 0.1332\n",
      "61/223, train_loss: 0.1035, step time: 0.1005\n",
      "62/223, train_loss: 0.0990, step time: 0.1001\n",
      "63/223, train_loss: 0.1077, step time: 0.1010\n",
      "64/223, train_loss: 0.1038, step time: 0.1098\n",
      "65/223, train_loss: 0.0974, step time: 0.1110\n",
      "66/223, train_loss: 0.0938, step time: 0.0997\n",
      "67/223, train_loss: 0.0950, step time: 0.1167\n",
      "68/223, train_loss: 0.1017, step time: 0.1089\n",
      "69/223, train_loss: 0.0921, step time: 0.1074\n",
      "70/223, train_loss: 0.1080, step time: 0.1002\n",
      "71/223, train_loss: 0.1100, step time: 0.1035\n",
      "72/223, train_loss: 0.0983, step time: 0.1147\n",
      "73/223, train_loss: 0.0931, step time: 0.1091\n",
      "74/223, train_loss: 0.1133, step time: 0.0999\n",
      "75/223, train_loss: 0.0947, step time: 0.1123\n",
      "76/223, train_loss: 0.0953, step time: 0.1227\n",
      "77/223, train_loss: 0.0991, step time: 0.1159\n",
      "78/223, train_loss: 0.0956, step time: 0.1105\n",
      "79/223, train_loss: 0.0989, step time: 0.1023\n",
      "80/223, train_loss: 0.0958, step time: 0.1006\n",
      "81/223, train_loss: 0.0940, step time: 0.1018\n",
      "82/223, train_loss: 0.0962, step time: 0.1115\n",
      "83/223, train_loss: 0.1001, step time: 0.1001\n",
      "84/223, train_loss: 0.1092, step time: 0.1110\n",
      "85/223, train_loss: 0.1137, step time: 0.1059\n",
      "86/223, train_loss: 0.0968, step time: 0.1124\n",
      "87/223, train_loss: 0.1054, step time: 0.1462\n",
      "88/223, train_loss: 0.0971, step time: 0.1129\n",
      "89/223, train_loss: 0.0933, step time: 0.1078\n",
      "90/223, train_loss: 0.0941, step time: 0.1008\n",
      "91/223, train_loss: 0.1029, step time: 0.1105\n",
      "92/223, train_loss: 0.1100, step time: 0.1005\n",
      "93/223, train_loss: 0.1035, step time: 0.1237\n",
      "94/223, train_loss: 0.2943, step time: 0.1015\n",
      "95/223, train_loss: 0.1047, step time: 0.1005\n",
      "96/223, train_loss: 0.0926, step time: 0.1090\n",
      "97/223, train_loss: 0.1024, step time: 0.1226\n",
      "98/223, train_loss: 0.1023, step time: 0.1004\n",
      "99/223, train_loss: 0.0931, step time: 0.1183\n",
      "100/223, train_loss: 0.0950, step time: 0.1058\n",
      "101/223, train_loss: 0.1008, step time: 0.1084\n",
      "102/223, train_loss: 0.0978, step time: 0.1145\n",
      "103/223, train_loss: 0.0980, step time: 0.1036\n",
      "104/223, train_loss: 0.0878, step time: 0.1121\n",
      "105/223, train_loss: 0.0993, step time: 0.1005\n",
      "106/223, train_loss: 0.1037, step time: 0.1195\n",
      "107/223, train_loss: 0.1051, step time: 0.1008\n",
      "108/223, train_loss: 0.1033, step time: 0.0997\n",
      "109/223, train_loss: 0.0963, step time: 0.1450\n",
      "110/223, train_loss: 0.0988, step time: 0.1014\n",
      "111/223, train_loss: 0.0976, step time: 0.1220\n",
      "112/223, train_loss: 0.1017, step time: 0.1181\n",
      "113/223, train_loss: 0.0938, step time: 0.1076\n",
      "114/223, train_loss: 0.1076, step time: 0.1123\n",
      "115/223, train_loss: 0.1009, step time: 0.1415\n",
      "116/223, train_loss: 0.1132, step time: 0.1122\n",
      "117/223, train_loss: 0.1138, step time: 0.1127\n",
      "118/223, train_loss: 0.0940, step time: 0.1007\n",
      "119/223, train_loss: 0.1013, step time: 0.1003\n",
      "120/223, train_loss: 0.0985, step time: 0.1050\n",
      "121/223, train_loss: 0.1037, step time: 0.1011\n",
      "122/223, train_loss: 0.1114, step time: 0.1217\n",
      "123/223, train_loss: 0.0977, step time: 0.1136\n",
      "124/223, train_loss: 0.1096, step time: 0.1267\n",
      "125/223, train_loss: 0.1017, step time: 0.1063\n",
      "126/223, train_loss: 0.1124, step time: 0.1099\n",
      "127/223, train_loss: 0.1109, step time: 0.1156\n",
      "128/223, train_loss: 0.0906, step time: 0.1023\n",
      "129/223, train_loss: 0.1005, step time: 0.1091\n",
      "130/223, train_loss: 0.0993, step time: 0.1130\n",
      "131/223, train_loss: 0.1033, step time: 0.1048\n",
      "132/223, train_loss: 0.0901, step time: 0.1013\n",
      "133/223, train_loss: 0.1041, step time: 0.1100\n",
      "134/223, train_loss: 0.0978, step time: 0.1067\n",
      "135/223, train_loss: 0.1026, step time: 0.1171\n",
      "136/223, train_loss: 0.1158, step time: 0.1013\n",
      "137/223, train_loss: 0.0995, step time: 0.1035\n",
      "138/223, train_loss: 0.1008, step time: 0.1057\n",
      "139/223, train_loss: 0.0997, step time: 0.1033\n",
      "140/223, train_loss: 0.1056, step time: 0.1110\n",
      "141/223, train_loss: 0.1027, step time: 0.1147\n",
      "142/223, train_loss: 0.0984, step time: 0.1045\n",
      "143/223, train_loss: 0.1039, step time: 0.1283\n",
      "144/223, train_loss: 0.0956, step time: 0.1315\n",
      "145/223, train_loss: 0.1023, step time: 0.1120\n",
      "146/223, train_loss: 0.1049, step time: 0.1144\n",
      "147/223, train_loss: 0.0991, step time: 0.1193\n",
      "148/223, train_loss: 0.1036, step time: 0.1181\n",
      "149/223, train_loss: 0.1077, step time: 0.1127\n",
      "150/223, train_loss: 0.1043, step time: 0.1005\n",
      "151/223, train_loss: 0.0941, step time: 0.1198\n",
      "152/223, train_loss: 0.1212, step time: 0.1014\n",
      "153/223, train_loss: 0.0932, step time: 0.1072\n",
      "154/223, train_loss: 0.1047, step time: 0.1109\n",
      "155/223, train_loss: 0.0965, step time: 0.1332\n",
      "156/223, train_loss: 0.0998, step time: 0.1106\n",
      "157/223, train_loss: 0.0998, step time: 0.1362\n",
      "158/223, train_loss: 0.1092, step time: 0.1103\n",
      "159/223, train_loss: 0.1135, step time: 0.1222\n",
      "160/223, train_loss: 0.0903, step time: 0.1159\n",
      "161/223, train_loss: 0.0947, step time: 0.1497\n",
      "162/223, train_loss: 0.0932, step time: 0.1081\n",
      "163/223, train_loss: 0.0944, step time: 0.1065\n",
      "164/223, train_loss: 0.0952, step time: 0.1100\n",
      "165/223, train_loss: 0.0902, step time: 0.1480\n",
      "166/223, train_loss: 0.1102, step time: 0.1080\n",
      "167/223, train_loss: 0.1105, step time: 0.1098\n",
      "168/223, train_loss: 0.0997, step time: 0.1337\n",
      "169/223, train_loss: 0.1046, step time: 0.1096\n",
      "170/223, train_loss: 0.1039, step time: 0.1022\n",
      "171/223, train_loss: 0.0930, step time: 0.1276\n",
      "172/223, train_loss: 0.0992, step time: 0.1188\n",
      "173/223, train_loss: 0.1029, step time: 0.1098\n",
      "174/223, train_loss: 0.1014, step time: 0.1177\n",
      "175/223, train_loss: 0.0914, step time: 0.0996\n",
      "176/223, train_loss: 0.1078, step time: 0.1256\n",
      "177/223, train_loss: 0.1038, step time: 0.1102\n",
      "178/223, train_loss: 0.1102, step time: 0.1136\n",
      "179/223, train_loss: 0.1119, step time: 0.1247\n",
      "180/223, train_loss: 0.0946, step time: 0.1356\n",
      "181/223, train_loss: 0.0883, step time: 0.1239\n",
      "182/223, train_loss: 0.0986, step time: 0.1089\n",
      "183/223, train_loss: 0.1022, step time: 0.1017\n",
      "184/223, train_loss: 0.1020, step time: 0.1436\n",
      "185/223, train_loss: 0.1002, step time: 0.1045\n",
      "186/223, train_loss: 0.0978, step time: 0.1008\n",
      "187/223, train_loss: 0.0895, step time: 0.1005\n",
      "188/223, train_loss: 0.0930, step time: 0.1117\n",
      "189/223, train_loss: 0.0923, step time: 0.1187\n",
      "190/223, train_loss: 0.1039, step time: 0.1087\n",
      "191/223, train_loss: 0.1124, step time: 0.1295\n",
      "192/223, train_loss: 0.0978, step time: 0.1461\n",
      "193/223, train_loss: 0.0991, step time: 0.1208\n",
      "194/223, train_loss: 0.0946, step time: 0.1097\n",
      "195/223, train_loss: 0.0960, step time: 0.1227\n",
      "196/223, train_loss: 0.1024, step time: 0.1015\n",
      "197/223, train_loss: 0.0956, step time: 0.1312\n",
      "198/223, train_loss: 0.1060, step time: 0.1152\n",
      "199/223, train_loss: 0.1011, step time: 0.1543\n",
      "200/223, train_loss: 0.0981, step time: 0.1108\n",
      "201/223, train_loss: 0.1158, step time: 0.1143\n",
      "202/223, train_loss: 0.0965, step time: 0.1081\n",
      "203/223, train_loss: 0.1153, step time: 0.0990\n",
      "204/223, train_loss: 0.1199, step time: 0.1090\n",
      "205/223, train_loss: 0.1063, step time: 0.0993\n",
      "206/223, train_loss: 0.1058, step time: 0.1254\n",
      "207/223, train_loss: 0.0941, step time: 0.1120\n",
      "208/223, train_loss: 0.0997, step time: 0.1242\n",
      "209/223, train_loss: 0.1100, step time: 0.1078\n",
      "210/223, train_loss: 0.0936, step time: 0.1046\n",
      "211/223, train_loss: 0.1042, step time: 0.1109\n",
      "212/223, train_loss: 0.1177, step time: 0.1013\n",
      "213/223, train_loss: 0.0988, step time: 0.1058\n",
      "214/223, train_loss: 0.0995, step time: 0.1153\n",
      "215/223, train_loss: 0.0969, step time: 0.1132\n",
      "216/223, train_loss: 0.1021, step time: 0.1152\n",
      "217/223, train_loss: 0.1171, step time: 0.1179\n",
      "218/223, train_loss: 0.0978, step time: 0.1005\n",
      "219/223, train_loss: 0.0984, step time: 0.1016\n",
      "220/223, train_loss: 0.0999, step time: 0.1003\n",
      "221/223, train_loss: 0.1017, step time: 0.1022\n",
      "222/223, train_loss: 0.0941, step time: 0.0998\n",
      "223/223, train_loss: 0.0944, step time: 0.0995\n",
      "epoch 216 average loss: 0.1017\n",
      "time consuming of epoch 216 is: 86.7292\n",
      "----------\n",
      "epoch 217/300\n",
      "1/223, train_loss: 0.1146, step time: 0.1022\n",
      "2/223, train_loss: 0.1053, step time: 0.1007\n",
      "3/223, train_loss: 0.0892, step time: 0.1003\n",
      "4/223, train_loss: 0.1094, step time: 0.1075\n",
      "5/223, train_loss: 0.1092, step time: 0.1054\n",
      "6/223, train_loss: 0.0984, step time: 0.1176\n",
      "7/223, train_loss: 0.1020, step time: 0.1063\n",
      "8/223, train_loss: 0.0941, step time: 0.1005\n",
      "9/223, train_loss: 0.1066, step time: 0.1074\n",
      "10/223, train_loss: 0.1041, step time: 0.1103\n",
      "11/223, train_loss: 0.1052, step time: 0.1228\n",
      "12/223, train_loss: 0.0949, step time: 0.1150\n",
      "13/223, train_loss: 0.1014, step time: 0.1052\n",
      "14/223, train_loss: 0.1089, step time: 0.1056\n",
      "15/223, train_loss: 0.1069, step time: 0.1240\n",
      "16/223, train_loss: 0.1031, step time: 0.1012\n",
      "17/223, train_loss: 0.1113, step time: 0.1176\n",
      "18/223, train_loss: 0.1053, step time: 0.1206\n",
      "19/223, train_loss: 0.0988, step time: 0.1289\n",
      "20/223, train_loss: 0.0979, step time: 0.1086\n",
      "21/223, train_loss: 0.1041, step time: 0.0998\n",
      "22/223, train_loss: 0.1123, step time: 0.1436\n",
      "23/223, train_loss: 0.0982, step time: 0.1227\n",
      "24/223, train_loss: 0.0981, step time: 0.1188\n",
      "25/223, train_loss: 0.0967, step time: 0.1126\n",
      "26/223, train_loss: 0.0960, step time: 0.1156\n",
      "27/223, train_loss: 0.1096, step time: 0.1076\n",
      "28/223, train_loss: 0.1025, step time: 0.1185\n",
      "29/223, train_loss: 0.1078, step time: 0.1129\n",
      "30/223, train_loss: 0.1011, step time: 0.1241\n",
      "31/223, train_loss: 0.1137, step time: 0.1063\n",
      "32/223, train_loss: 0.0952, step time: 0.1201\n",
      "33/223, train_loss: 0.1047, step time: 0.1128\n",
      "34/223, train_loss: 0.1038, step time: 0.1005\n",
      "35/223, train_loss: 0.0977, step time: 0.1189\n",
      "36/223, train_loss: 0.0992, step time: 0.1145\n",
      "37/223, train_loss: 0.0965, step time: 0.1171\n",
      "38/223, train_loss: 0.1078, step time: 0.1210\n",
      "39/223, train_loss: 0.0902, step time: 0.1034\n",
      "40/223, train_loss: 0.0996, step time: 0.1164\n",
      "41/223, train_loss: 0.0937, step time: 0.1079\n",
      "42/223, train_loss: 0.0926, step time: 0.1136\n",
      "43/223, train_loss: 0.1031, step time: 0.1100\n",
      "44/223, train_loss: 0.0950, step time: 0.1056\n",
      "45/223, train_loss: 0.1076, step time: 0.1007\n",
      "46/223, train_loss: 0.0995, step time: 0.1144\n",
      "47/223, train_loss: 0.0939, step time: 0.1004\n",
      "48/223, train_loss: 0.1015, step time: 0.0996\n",
      "49/223, train_loss: 0.0900, step time: 0.1242\n",
      "50/223, train_loss: 0.0972, step time: 0.1337\n",
      "51/223, train_loss: 0.0991, step time: 0.1067\n",
      "52/223, train_loss: 0.0914, step time: 0.1073\n",
      "53/223, train_loss: 0.1125, step time: 0.1082\n",
      "54/223, train_loss: 0.1041, step time: 0.1207\n",
      "55/223, train_loss: 0.1027, step time: 0.1116\n",
      "56/223, train_loss: 0.0985, step time: 0.1011\n",
      "57/223, train_loss: 0.0953, step time: 0.1487\n",
      "58/223, train_loss: 0.1042, step time: 0.1404\n",
      "59/223, train_loss: 0.0893, step time: 0.1184\n",
      "60/223, train_loss: 0.0998, step time: 0.1031\n",
      "61/223, train_loss: 0.0974, step time: 0.1168\n",
      "62/223, train_loss: 0.1092, step time: 0.1186\n",
      "63/223, train_loss: 0.0973, step time: 0.1176\n",
      "64/223, train_loss: 0.0996, step time: 0.1008\n",
      "65/223, train_loss: 0.1063, step time: 0.1135\n",
      "66/223, train_loss: 0.1074, step time: 0.1127\n",
      "67/223, train_loss: 0.0978, step time: 0.1416\n",
      "68/223, train_loss: 0.0959, step time: 0.1135\n",
      "69/223, train_loss: 0.1053, step time: 0.1237\n",
      "70/223, train_loss: 0.0964, step time: 0.1109\n",
      "71/223, train_loss: 0.0910, step time: 0.1685\n",
      "72/223, train_loss: 0.1096, step time: 0.1006\n",
      "73/223, train_loss: 0.1086, step time: 0.1216\n",
      "74/223, train_loss: 0.1014, step time: 0.1355\n",
      "75/223, train_loss: 0.1018, step time: 0.1092\n",
      "76/223, train_loss: 0.1039, step time: 0.1210\n",
      "77/223, train_loss: 0.0938, step time: 0.1176\n",
      "78/223, train_loss: 0.1172, step time: 0.1364\n",
      "79/223, train_loss: 0.1028, step time: 0.1336\n",
      "80/223, train_loss: 0.1083, step time: 0.1611\n",
      "81/223, train_loss: 0.1053, step time: 0.1065\n",
      "82/223, train_loss: 0.1007, step time: 0.1163\n",
      "83/223, train_loss: 0.1048, step time: 0.1654\n",
      "84/223, train_loss: 0.0888, step time: 0.1042\n",
      "85/223, train_loss: 0.1002, step time: 0.1041\n",
      "86/223, train_loss: 0.0957, step time: 0.1187\n",
      "87/223, train_loss: 0.0999, step time: 0.1070\n",
      "88/223, train_loss: 0.0984, step time: 0.1063\n",
      "89/223, train_loss: 0.1045, step time: 0.1126\n",
      "90/223, train_loss: 0.0983, step time: 0.1007\n",
      "91/223, train_loss: 0.0880, step time: 0.1019\n",
      "92/223, train_loss: 0.1034, step time: 0.1020\n",
      "93/223, train_loss: 0.0955, step time: 0.1083\n",
      "94/223, train_loss: 0.1101, step time: 0.1197\n",
      "95/223, train_loss: 0.0977, step time: 0.1091\n",
      "96/223, train_loss: 0.0992, step time: 0.1158\n",
      "97/223, train_loss: 0.0993, step time: 0.1023\n",
      "98/223, train_loss: 0.1025, step time: 0.1018\n",
      "99/223, train_loss: 0.0962, step time: 0.1103\n",
      "100/223, train_loss: 0.1059, step time: 0.1113\n",
      "101/223, train_loss: 0.0989, step time: 0.1072\n",
      "102/223, train_loss: 0.0908, step time: 0.1094\n",
      "103/223, train_loss: 0.0917, step time: 0.1146\n",
      "104/223, train_loss: 0.0953, step time: 0.1124\n",
      "105/223, train_loss: 0.0959, step time: 0.1075\n",
      "106/223, train_loss: 0.0889, step time: 0.1173\n",
      "107/223, train_loss: 0.1015, step time: 0.1048\n",
      "108/223, train_loss: 0.0862, step time: 0.1192\n",
      "109/223, train_loss: 0.0928, step time: 0.1046\n",
      "110/223, train_loss: 0.0881, step time: 0.1135\n",
      "111/223, train_loss: 0.1019, step time: 0.1285\n",
      "112/223, train_loss: 0.0967, step time: 0.1158\n",
      "113/223, train_loss: 0.0917, step time: 0.1017\n",
      "114/223, train_loss: 0.0897, step time: 0.1312\n",
      "115/223, train_loss: 0.1024, step time: 0.1139\n",
      "116/223, train_loss: 0.1023, step time: 0.1375\n",
      "117/223, train_loss: 0.1179, step time: 0.1317\n",
      "118/223, train_loss: 0.1023, step time: 0.1056\n",
      "119/223, train_loss: 0.0934, step time: 0.1180\n",
      "120/223, train_loss: 0.0925, step time: 0.1071\n",
      "121/223, train_loss: 0.1005, step time: 0.1074\n",
      "122/223, train_loss: 0.1046, step time: 0.1567\n",
      "123/223, train_loss: 0.1180, step time: 0.1119\n",
      "124/223, train_loss: 0.0921, step time: 0.1597\n",
      "125/223, train_loss: 0.0955, step time: 0.1177\n",
      "126/223, train_loss: 0.0938, step time: 0.1014\n",
      "127/223, train_loss: 0.1040, step time: 0.1010\n",
      "128/223, train_loss: 0.1149, step time: 0.1265\n",
      "129/223, train_loss: 0.0991, step time: 0.1088\n",
      "130/223, train_loss: 0.1041, step time: 0.1200\n",
      "131/223, train_loss: 0.1039, step time: 0.1342\n",
      "132/223, train_loss: 0.0982, step time: 0.1048\n",
      "133/223, train_loss: 0.1133, step time: 0.1024\n",
      "134/223, train_loss: 0.1107, step time: 0.1217\n",
      "135/223, train_loss: 0.1026, step time: 0.1554\n",
      "136/223, train_loss: 0.1030, step time: 0.1044\n",
      "137/223, train_loss: 0.0907, step time: 0.1164\n",
      "138/223, train_loss: 0.1019, step time: 0.1008\n",
      "139/223, train_loss: 0.1112, step time: 0.1095\n",
      "140/223, train_loss: 0.0967, step time: 0.1177\n",
      "141/223, train_loss: 0.1096, step time: 0.1061\n",
      "142/223, train_loss: 0.0976, step time: 0.1017\n",
      "143/223, train_loss: 0.1043, step time: 0.1715\n",
      "144/223, train_loss: 0.1009, step time: 0.1249\n",
      "145/223, train_loss: 0.0989, step time: 0.1124\n",
      "146/223, train_loss: 0.0986, step time: 0.1048\n",
      "147/223, train_loss: 0.0903, step time: 0.1241\n",
      "148/223, train_loss: 0.0931, step time: 0.0986\n",
      "149/223, train_loss: 0.0962, step time: 0.1092\n",
      "150/223, train_loss: 0.0981, step time: 0.1003\n",
      "151/223, train_loss: 0.1021, step time: 0.1015\n",
      "152/223, train_loss: 0.0983, step time: 0.1010\n",
      "153/223, train_loss: 0.0995, step time: 0.1159\n",
      "154/223, train_loss: 0.1044, step time: 0.1021\n",
      "155/223, train_loss: 0.1004, step time: 0.1080\n",
      "156/223, train_loss: 0.1063, step time: 0.1140\n",
      "157/223, train_loss: 0.1111, step time: 0.1225\n",
      "158/223, train_loss: 0.0987, step time: 0.1166\n",
      "159/223, train_loss: 0.1028, step time: 0.1440\n",
      "160/223, train_loss: 0.0980, step time: 0.1224\n",
      "161/223, train_loss: 0.1093, step time: 0.1018\n",
      "162/223, train_loss: 0.1041, step time: 0.1017\n",
      "163/223, train_loss: 0.0988, step time: 0.0993\n",
      "164/223, train_loss: 0.0888, step time: 0.1006\n",
      "165/223, train_loss: 0.0959, step time: 0.1213\n",
      "166/223, train_loss: 0.1044, step time: 0.1438\n",
      "167/223, train_loss: 0.1041, step time: 0.1184\n",
      "168/223, train_loss: 0.1030, step time: 0.0998\n",
      "169/223, train_loss: 0.1053, step time: 0.1057\n",
      "170/223, train_loss: 0.1014, step time: 0.1182\n",
      "171/223, train_loss: 0.0932, step time: 0.1058\n",
      "172/223, train_loss: 0.0943, step time: 0.1022\n",
      "173/223, train_loss: 0.1108, step time: 0.1074\n",
      "174/223, train_loss: 0.0904, step time: 0.1195\n",
      "175/223, train_loss: 0.1017, step time: 0.1056\n",
      "176/223, train_loss: 0.0932, step time: 0.1005\n",
      "177/223, train_loss: 0.1079, step time: 0.1003\n",
      "178/223, train_loss: 0.1010, step time: 0.1009\n",
      "179/223, train_loss: 0.1041, step time: 0.1131\n",
      "180/223, train_loss: 0.0956, step time: 0.1408\n",
      "181/223, train_loss: 0.1001, step time: 0.1008\n",
      "182/223, train_loss: 0.0933, step time: 0.1401\n",
      "183/223, train_loss: 0.1000, step time: 0.1261\n",
      "184/223, train_loss: 0.1121, step time: 0.1001\n",
      "185/223, train_loss: 0.1086, step time: 0.1154\n",
      "186/223, train_loss: 0.1144, step time: 0.1200\n",
      "187/223, train_loss: 0.0935, step time: 0.1202\n",
      "188/223, train_loss: 0.1077, step time: 0.1184\n",
      "189/223, train_loss: 0.1006, step time: 0.1001\n",
      "190/223, train_loss: 0.1081, step time: 0.1069\n",
      "191/223, train_loss: 0.1011, step time: 0.1186\n",
      "192/223, train_loss: 0.1037, step time: 0.1178\n",
      "193/223, train_loss: 0.0907, step time: 0.1010\n",
      "194/223, train_loss: 0.0981, step time: 0.1088\n",
      "195/223, train_loss: 0.1121, step time: 0.1022\n",
      "196/223, train_loss: 0.1082, step time: 0.1045\n",
      "197/223, train_loss: 0.0940, step time: 0.1101\n",
      "198/223, train_loss: 0.1030, step time: 0.0998\n",
      "199/223, train_loss: 0.1026, step time: 0.1006\n",
      "200/223, train_loss: 0.0959, step time: 0.1024\n",
      "201/223, train_loss: 0.1032, step time: 0.1394\n",
      "202/223, train_loss: 0.1069, step time: 0.1057\n",
      "203/223, train_loss: 0.0894, step time: 0.1098\n",
      "204/223, train_loss: 0.0939, step time: 0.1305\n",
      "205/223, train_loss: 0.1011, step time: 0.1202\n",
      "206/223, train_loss: 0.1046, step time: 0.1335\n",
      "207/223, train_loss: 0.1095, step time: 0.1569\n",
      "208/223, train_loss: 0.0931, step time: 0.1076\n",
      "209/223, train_loss: 0.1093, step time: 0.1194\n",
      "210/223, train_loss: 0.0938, step time: 0.1435\n",
      "211/223, train_loss: 0.1054, step time: 0.1113\n",
      "212/223, train_loss: 0.0995, step time: 0.1197\n",
      "213/223, train_loss: 0.0963, step time: 0.1155\n",
      "214/223, train_loss: 0.1074, step time: 0.1003\n",
      "215/223, train_loss: 0.0924, step time: 0.1221\n",
      "216/223, train_loss: 0.0930, step time: 0.1134\n",
      "217/223, train_loss: 0.3094, step time: 0.1127\n",
      "218/223, train_loss: 0.0948, step time: 0.1095\n",
      "219/223, train_loss: 0.0986, step time: 0.1070\n",
      "220/223, train_loss: 0.1016, step time: 0.1131\n",
      "221/223, train_loss: 0.0894, step time: 0.0989\n",
      "222/223, train_loss: 0.0974, step time: 0.0987\n",
      "223/223, train_loss: 0.0930, step time: 0.0992\n",
      "epoch 217 average loss: 0.1015\n",
      "time consuming of epoch 217 is: 87.6828\n",
      "----------\n",
      "epoch 218/300\n",
      "1/223, train_loss: 0.0988, step time: 0.1027\n",
      "2/223, train_loss: 0.0926, step time: 0.1128\n",
      "3/223, train_loss: 0.1027, step time: 0.1032\n",
      "4/223, train_loss: 0.0999, step time: 0.1034\n",
      "5/223, train_loss: 0.1096, step time: 0.1001\n",
      "6/223, train_loss: 0.1090, step time: 0.1133\n",
      "7/223, train_loss: 0.0924, step time: 0.1106\n",
      "8/223, train_loss: 0.1089, step time: 0.1121\n",
      "9/223, train_loss: 0.1028, step time: 0.1004\n",
      "10/223, train_loss: 0.1062, step time: 0.0999\n",
      "11/223, train_loss: 0.0938, step time: 0.1004\n",
      "12/223, train_loss: 0.0994, step time: 0.1135\n",
      "13/223, train_loss: 0.1023, step time: 0.1163\n",
      "14/223, train_loss: 0.1108, step time: 0.1103\n",
      "15/223, train_loss: 0.1038, step time: 0.1144\n",
      "16/223, train_loss: 0.0898, step time: 0.1001\n",
      "17/223, train_loss: 0.0906, step time: 0.1163\n",
      "18/223, train_loss: 0.1077, step time: 0.1205\n",
      "19/223, train_loss: 0.0934, step time: 0.1094\n",
      "20/223, train_loss: 0.0971, step time: 0.1066\n",
      "21/223, train_loss: 0.1052, step time: 0.1001\n",
      "22/223, train_loss: 0.1058, step time: 0.1070\n",
      "23/223, train_loss: 0.1127, step time: 0.1045\n",
      "24/223, train_loss: 0.0872, step time: 0.1017\n",
      "25/223, train_loss: 0.0991, step time: 0.1346\n",
      "26/223, train_loss: 0.1021, step time: 0.1277\n",
      "27/223, train_loss: 0.0966, step time: 0.1012\n",
      "28/223, train_loss: 0.0925, step time: 0.1010\n",
      "29/223, train_loss: 0.0953, step time: 0.1107\n",
      "30/223, train_loss: 0.0985, step time: 0.1012\n",
      "31/223, train_loss: 0.1006, step time: 0.1007\n",
      "32/223, train_loss: 0.1077, step time: 0.1003\n",
      "33/223, train_loss: 0.0951, step time: 0.1059\n",
      "34/223, train_loss: 0.1002, step time: 0.1016\n",
      "35/223, train_loss: 0.1002, step time: 0.1010\n",
      "36/223, train_loss: 0.0853, step time: 0.1118\n",
      "37/223, train_loss: 0.1084, step time: 0.1001\n",
      "38/223, train_loss: 0.0967, step time: 0.0999\n",
      "39/223, train_loss: 0.0959, step time: 0.0998\n",
      "40/223, train_loss: 0.0953, step time: 0.1094\n",
      "41/223, train_loss: 0.0942, step time: 0.1192\n",
      "42/223, train_loss: 0.1085, step time: 0.1308\n",
      "43/223, train_loss: 0.1072, step time: 0.1286\n",
      "44/223, train_loss: 0.1072, step time: 0.1036\n",
      "45/223, train_loss: 0.0959, step time: 0.1381\n",
      "46/223, train_loss: 0.1092, step time: 0.1177\n",
      "47/223, train_loss: 0.0903, step time: 0.1001\n",
      "48/223, train_loss: 0.1027, step time: 0.1004\n",
      "49/223, train_loss: 0.0997, step time: 0.1102\n",
      "50/223, train_loss: 0.1065, step time: 0.1183\n",
      "51/223, train_loss: 0.0878, step time: 0.1178\n",
      "52/223, train_loss: 0.0990, step time: 0.1204\n",
      "53/223, train_loss: 0.1032, step time: 0.1116\n",
      "54/223, train_loss: 0.1076, step time: 0.1201\n",
      "55/223, train_loss: 0.0928, step time: 0.1008\n",
      "56/223, train_loss: 0.0946, step time: 0.1184\n",
      "57/223, train_loss: 0.0902, step time: 0.1122\n",
      "58/223, train_loss: 0.1180, step time: 0.1123\n",
      "59/223, train_loss: 0.0903, step time: 0.1101\n",
      "60/223, train_loss: 0.0917, step time: 0.1092\n",
      "61/223, train_loss: 0.1015, step time: 0.1478\n",
      "62/223, train_loss: 0.0909, step time: 0.1356\n",
      "63/223, train_loss: 0.0935, step time: 0.1194\n",
      "64/223, train_loss: 0.0964, step time: 0.1009\n",
      "65/223, train_loss: 0.1027, step time: 0.1097\n",
      "66/223, train_loss: 0.1038, step time: 0.1443\n",
      "67/223, train_loss: 0.0974, step time: 0.1272\n",
      "68/223, train_loss: 0.0922, step time: 0.1101\n",
      "69/223, train_loss: 0.1136, step time: 0.1140\n",
      "70/223, train_loss: 0.1010, step time: 0.1240\n",
      "71/223, train_loss: 0.0889, step time: 0.1202\n",
      "72/223, train_loss: 0.1102, step time: 0.1011\n",
      "73/223, train_loss: 0.1104, step time: 0.1236\n",
      "74/223, train_loss: 0.0939, step time: 0.1139\n",
      "75/223, train_loss: 0.1065, step time: 0.1004\n",
      "76/223, train_loss: 0.0919, step time: 0.1063\n",
      "77/223, train_loss: 0.1116, step time: 0.1048\n",
      "78/223, train_loss: 0.0998, step time: 0.1392\n",
      "79/223, train_loss: 0.1035, step time: 0.1101\n",
      "80/223, train_loss: 0.0982, step time: 0.1006\n",
      "81/223, train_loss: 0.1030, step time: 0.1124\n",
      "82/223, train_loss: 0.0942, step time: 0.1077\n",
      "83/223, train_loss: 0.1131, step time: 0.1007\n",
      "84/223, train_loss: 0.0910, step time: 0.1054\n",
      "85/223, train_loss: 0.0930, step time: 0.1067\n",
      "86/223, train_loss: 0.0959, step time: 0.1204\n",
      "87/223, train_loss: 0.1106, step time: 0.1151\n",
      "88/223, train_loss: 0.1023, step time: 0.1010\n",
      "89/223, train_loss: 0.1110, step time: 0.1151\n",
      "90/223, train_loss: 0.1036, step time: 0.1142\n",
      "91/223, train_loss: 0.0952, step time: 0.1123\n",
      "92/223, train_loss: 0.1029, step time: 0.1001\n",
      "93/223, train_loss: 0.0938, step time: 0.1064\n",
      "94/223, train_loss: 0.0971, step time: 0.1006\n",
      "95/223, train_loss: 0.0922, step time: 0.1010\n",
      "96/223, train_loss: 0.1034, step time: 0.1070\n",
      "97/223, train_loss: 0.0964, step time: 0.1112\n",
      "98/223, train_loss: 0.1142, step time: 0.0999\n",
      "99/223, train_loss: 0.0994, step time: 0.1178\n",
      "100/223, train_loss: 0.0953, step time: 0.1017\n",
      "101/223, train_loss: 0.0974, step time: 0.1193\n",
      "102/223, train_loss: 0.1028, step time: 0.1007\n",
      "103/223, train_loss: 0.0978, step time: 0.1359\n",
      "104/223, train_loss: 0.0887, step time: 0.1167\n",
      "105/223, train_loss: 0.0948, step time: 0.1162\n",
      "106/223, train_loss: 0.0947, step time: 0.1109\n",
      "107/223, train_loss: 0.1114, step time: 0.1081\n",
      "108/223, train_loss: 0.1012, step time: 0.1107\n",
      "109/223, train_loss: 0.0954, step time: 0.1130\n",
      "110/223, train_loss: 0.0900, step time: 0.1057\n",
      "111/223, train_loss: 0.0969, step time: 0.1009\n",
      "112/223, train_loss: 0.0969, step time: 0.1010\n",
      "113/223, train_loss: 0.1043, step time: 0.1069\n",
      "114/223, train_loss: 0.0927, step time: 0.1405\n",
      "115/223, train_loss: 0.1113, step time: 0.1270\n",
      "116/223, train_loss: 0.3087, step time: 0.1003\n",
      "117/223, train_loss: 0.1053, step time: 0.1068\n",
      "118/223, train_loss: 0.0886, step time: 0.0999\n",
      "119/223, train_loss: 0.1104, step time: 0.1009\n",
      "120/223, train_loss: 0.1105, step time: 0.1163\n",
      "121/223, train_loss: 0.0951, step time: 0.1076\n",
      "122/223, train_loss: 0.0932, step time: 0.1052\n",
      "123/223, train_loss: 0.1005, step time: 0.1108\n",
      "124/223, train_loss: 0.0949, step time: 0.1003\n",
      "125/223, train_loss: 0.0949, step time: 0.1169\n",
      "126/223, train_loss: 0.1000, step time: 0.1003\n",
      "127/223, train_loss: 0.0996, step time: 0.1005\n",
      "128/223, train_loss: 0.1099, step time: 0.1085\n",
      "129/223, train_loss: 0.0898, step time: 0.1099\n",
      "130/223, train_loss: 0.0899, step time: 0.1064\n",
      "131/223, train_loss: 0.0992, step time: 0.1295\n",
      "132/223, train_loss: 0.1098, step time: 0.1031\n",
      "133/223, train_loss: 0.0959, step time: 0.1146\n",
      "134/223, train_loss: 0.1065, step time: 0.1046\n",
      "135/223, train_loss: 0.1005, step time: 0.1328\n",
      "136/223, train_loss: 0.1131, step time: 0.1120\n",
      "137/223, train_loss: 0.0948, step time: 0.1059\n",
      "138/223, train_loss: 0.1147, step time: 0.1191\n",
      "139/223, train_loss: 0.1049, step time: 0.1270\n",
      "140/223, train_loss: 0.1017, step time: 0.1069\n",
      "141/223, train_loss: 0.0913, step time: 0.1013\n",
      "142/223, train_loss: 0.1089, step time: 0.1009\n",
      "143/223, train_loss: 0.0948, step time: 0.1012\n",
      "144/223, train_loss: 0.1088, step time: 0.1551\n",
      "145/223, train_loss: 0.1054, step time: 0.1163\n",
      "146/223, train_loss: 0.0960, step time: 0.1350\n",
      "147/223, train_loss: 0.1018, step time: 0.1177\n",
      "148/223, train_loss: 0.0958, step time: 0.1003\n",
      "149/223, train_loss: 0.1007, step time: 0.1050\n",
      "150/223, train_loss: 0.1105, step time: 0.1006\n",
      "151/223, train_loss: 0.0985, step time: 0.1214\n",
      "152/223, train_loss: 0.1074, step time: 0.1307\n",
      "153/223, train_loss: 0.0984, step time: 0.1186\n",
      "154/223, train_loss: 0.1099, step time: 0.1108\n",
      "155/223, train_loss: 0.0898, step time: 0.1134\n",
      "156/223, train_loss: 0.1023, step time: 0.0998\n",
      "157/223, train_loss: 0.0929, step time: 0.1168\n",
      "158/223, train_loss: 0.0988, step time: 0.1303\n",
      "159/223, train_loss: 0.0999, step time: 0.1005\n",
      "160/223, train_loss: 0.0977, step time: 0.1032\n",
      "161/223, train_loss: 0.0999, step time: 0.1008\n",
      "162/223, train_loss: 0.1182, step time: 0.1279\n",
      "163/223, train_loss: 0.1045, step time: 0.1190\n",
      "164/223, train_loss: 0.0938, step time: 0.1120\n",
      "165/223, train_loss: 0.1144, step time: 0.1064\n",
      "166/223, train_loss: 0.1103, step time: 0.0990\n",
      "167/223, train_loss: 0.0964, step time: 0.0993\n",
      "168/223, train_loss: 0.1071, step time: 0.1326\n",
      "169/223, train_loss: 0.0896, step time: 0.1059\n",
      "170/223, train_loss: 0.1106, step time: 0.0999\n",
      "171/223, train_loss: 0.0907, step time: 0.1058\n",
      "172/223, train_loss: 0.1001, step time: 0.1083\n",
      "173/223, train_loss: 0.1045, step time: 0.1378\n",
      "174/223, train_loss: 0.1000, step time: 0.1274\n",
      "175/223, train_loss: 0.0934, step time: 0.1256\n",
      "176/223, train_loss: 0.1071, step time: 0.1090\n",
      "177/223, train_loss: 0.1032, step time: 0.1079\n",
      "178/223, train_loss: 0.1029, step time: 0.1124\n",
      "179/223, train_loss: 0.1029, step time: 0.1296\n",
      "180/223, train_loss: 0.1039, step time: 0.1237\n",
      "181/223, train_loss: 0.0988, step time: 0.1252\n",
      "182/223, train_loss: 0.1034, step time: 0.1291\n",
      "183/223, train_loss: 0.1052, step time: 0.1085\n",
      "184/223, train_loss: 0.0937, step time: 0.1518\n",
      "185/223, train_loss: 0.1044, step time: 0.1050\n",
      "186/223, train_loss: 0.1061, step time: 0.1124\n",
      "187/223, train_loss: 0.0922, step time: 0.1463\n",
      "188/223, train_loss: 0.0943, step time: 0.1007\n",
      "189/223, train_loss: 0.1028, step time: 0.1001\n",
      "190/223, train_loss: 0.0937, step time: 0.1139\n",
      "191/223, train_loss: 0.1121, step time: 0.1285\n",
      "192/223, train_loss: 0.0974, step time: 0.1078\n",
      "193/223, train_loss: 0.1051, step time: 0.1126\n",
      "194/223, train_loss: 0.0922, step time: 0.1157\n",
      "195/223, train_loss: 0.1062, step time: 0.1357\n",
      "196/223, train_loss: 0.0997, step time: 0.1021\n",
      "197/223, train_loss: 0.0998, step time: 0.1199\n",
      "198/223, train_loss: 0.1032, step time: 0.1032\n",
      "199/223, train_loss: 0.0998, step time: 0.1048\n",
      "200/223, train_loss: 0.0891, step time: 0.1363\n",
      "201/223, train_loss: 0.1159, step time: 0.1230\n",
      "202/223, train_loss: 0.1091, step time: 0.1233\n",
      "203/223, train_loss: 0.1051, step time: 0.1199\n",
      "204/223, train_loss: 0.0938, step time: 0.1202\n",
      "205/223, train_loss: 0.0993, step time: 0.1261\n",
      "206/223, train_loss: 0.1192, step time: 0.1150\n",
      "207/223, train_loss: 0.0959, step time: 0.1358\n",
      "208/223, train_loss: 0.0903, step time: 0.1162\n",
      "209/223, train_loss: 0.0986, step time: 0.1175\n",
      "210/223, train_loss: 0.0971, step time: 0.1200\n",
      "211/223, train_loss: 0.0976, step time: 0.1259\n",
      "212/223, train_loss: 0.1015, step time: 0.1204\n",
      "213/223, train_loss: 0.0916, step time: 0.0995\n",
      "214/223, train_loss: 0.1028, step time: 0.1022\n",
      "215/223, train_loss: 0.1006, step time: 0.1155\n",
      "216/223, train_loss: 0.1030, step time: 0.1081\n",
      "217/223, train_loss: 0.0990, step time: 0.1025\n",
      "218/223, train_loss: 0.0974, step time: 0.1439\n",
      "219/223, train_loss: 0.0979, step time: 0.1111\n",
      "220/223, train_loss: 0.1056, step time: 0.1028\n",
      "221/223, train_loss: 0.1069, step time: 0.0990\n",
      "222/223, train_loss: 0.0973, step time: 0.0994\n",
      "223/223, train_loss: 0.1182, step time: 0.1002\n",
      "epoch 218 average loss: 0.1014\n",
      "time consuming of epoch 218 is: 89.1080\n",
      "----------\n",
      "epoch 219/300\n",
      "1/223, train_loss: 0.0973, step time: 0.1014\n",
      "2/223, train_loss: 0.1075, step time: 0.1112\n",
      "3/223, train_loss: 0.0969, step time: 0.1206\n",
      "4/223, train_loss: 0.1028, step time: 0.1082\n",
      "5/223, train_loss: 0.1009, step time: 0.1133\n",
      "6/223, train_loss: 0.0961, step time: 0.1079\n",
      "7/223, train_loss: 0.1062, step time: 0.1584\n",
      "8/223, train_loss: 0.1105, step time: 0.1218\n",
      "9/223, train_loss: 0.0919, step time: 0.1004\n",
      "10/223, train_loss: 0.1002, step time: 0.1198\n",
      "11/223, train_loss: 0.0961, step time: 0.1456\n",
      "12/223, train_loss: 0.1039, step time: 0.1095\n",
      "13/223, train_loss: 0.1023, step time: 0.1141\n",
      "14/223, train_loss: 0.0873, step time: 0.1067\n",
      "15/223, train_loss: 0.1125, step time: 0.1094\n",
      "16/223, train_loss: 0.1024, step time: 0.1005\n",
      "17/223, train_loss: 0.0947, step time: 0.1081\n",
      "18/223, train_loss: 0.0980, step time: 0.1026\n",
      "19/223, train_loss: 0.1080, step time: 0.1207\n",
      "20/223, train_loss: 0.1007, step time: 0.1075\n",
      "21/223, train_loss: 0.1146, step time: 0.1090\n",
      "22/223, train_loss: 0.1150, step time: 0.1275\n",
      "23/223, train_loss: 0.1130, step time: 0.1024\n",
      "24/223, train_loss: 0.1007, step time: 0.1042\n",
      "25/223, train_loss: 0.1026, step time: 0.1214\n",
      "26/223, train_loss: 0.1033, step time: 0.1006\n",
      "27/223, train_loss: 0.1017, step time: 0.1140\n",
      "28/223, train_loss: 0.0970, step time: 0.1023\n",
      "29/223, train_loss: 0.1064, step time: 0.1073\n",
      "30/223, train_loss: 0.1001, step time: 0.1386\n",
      "31/223, train_loss: 0.1012, step time: 0.1251\n",
      "32/223, train_loss: 0.1088, step time: 0.1003\n",
      "33/223, train_loss: 0.0937, step time: 0.1147\n",
      "34/223, train_loss: 0.1063, step time: 0.1021\n",
      "35/223, train_loss: 0.0953, step time: 0.1357\n",
      "36/223, train_loss: 0.1062, step time: 0.1121\n",
      "37/223, train_loss: 0.0928, step time: 0.1075\n",
      "38/223, train_loss: 0.0914, step time: 0.1039\n",
      "39/223, train_loss: 0.0878, step time: 0.1631\n",
      "40/223, train_loss: 0.1089, step time: 0.1334\n",
      "41/223, train_loss: 0.0998, step time: 0.1217\n",
      "42/223, train_loss: 0.1076, step time: 0.1101\n",
      "43/223, train_loss: 0.1163, step time: 0.1121\n",
      "44/223, train_loss: 0.0979, step time: 0.1148\n",
      "45/223, train_loss: 0.0972, step time: 0.1248\n",
      "46/223, train_loss: 0.0966, step time: 0.1157\n",
      "47/223, train_loss: 0.1054, step time: 0.1078\n",
      "48/223, train_loss: 0.1136, step time: 0.1310\n",
      "49/223, train_loss: 0.1063, step time: 0.1046\n",
      "50/223, train_loss: 0.1114, step time: 0.1226\n",
      "51/223, train_loss: 0.0894, step time: 0.1085\n",
      "52/223, train_loss: 0.1037, step time: 0.1327\n",
      "53/223, train_loss: 0.0986, step time: 0.1343\n",
      "54/223, train_loss: 0.1013, step time: 0.1072\n",
      "55/223, train_loss: 0.1065, step time: 0.1086\n",
      "56/223, train_loss: 0.1012, step time: 0.1139\n",
      "57/223, train_loss: 0.1012, step time: 0.1052\n",
      "58/223, train_loss: 0.0930, step time: 0.1229\n",
      "59/223, train_loss: 0.1034, step time: 0.1163\n",
      "60/223, train_loss: 0.1031, step time: 0.1126\n",
      "61/223, train_loss: 0.0930, step time: 0.1008\n",
      "62/223, train_loss: 0.1038, step time: 0.1007\n",
      "63/223, train_loss: 0.0935, step time: 0.1008\n",
      "64/223, train_loss: 0.1001, step time: 0.1006\n",
      "65/223, train_loss: 0.1081, step time: 0.1006\n",
      "66/223, train_loss: 0.1011, step time: 0.1004\n",
      "67/223, train_loss: 0.1000, step time: 0.0999\n",
      "68/223, train_loss: 0.0950, step time: 0.1286\n",
      "69/223, train_loss: 0.1037, step time: 0.1076\n",
      "70/223, train_loss: 0.0979, step time: 0.1002\n",
      "71/223, train_loss: 0.0996, step time: 0.1006\n",
      "72/223, train_loss: 0.1027, step time: 0.1005\n",
      "73/223, train_loss: 0.1060, step time: 0.1114\n",
      "74/223, train_loss: 0.1071, step time: 0.1078\n",
      "75/223, train_loss: 0.0929, step time: 0.1006\n",
      "76/223, train_loss: 0.0985, step time: 0.1006\n",
      "77/223, train_loss: 0.0896, step time: 0.1068\n",
      "78/223, train_loss: 0.0882, step time: 0.1075\n",
      "79/223, train_loss: 0.1006, step time: 0.1069\n",
      "80/223, train_loss: 0.0937, step time: 0.1001\n",
      "81/223, train_loss: 0.1033, step time: 0.1097\n",
      "82/223, train_loss: 0.1020, step time: 0.1043\n",
      "83/223, train_loss: 0.0904, step time: 0.1003\n",
      "84/223, train_loss: 0.1124, step time: 0.1018\n",
      "85/223, train_loss: 0.1041, step time: 0.1376\n",
      "86/223, train_loss: 0.0983, step time: 0.1051\n",
      "87/223, train_loss: 0.1096, step time: 0.1024\n",
      "88/223, train_loss: 0.0950, step time: 0.1115\n",
      "89/223, train_loss: 0.0982, step time: 0.1050\n",
      "90/223, train_loss: 0.1001, step time: 0.1036\n",
      "91/223, train_loss: 0.1013, step time: 0.1084\n",
      "92/223, train_loss: 0.1145, step time: 0.1005\n",
      "93/223, train_loss: 0.1053, step time: 0.1061\n",
      "94/223, train_loss: 0.0969, step time: 0.1035\n",
      "95/223, train_loss: 0.0979, step time: 0.0990\n",
      "96/223, train_loss: 0.1091, step time: 0.0990\n",
      "97/223, train_loss: 0.0921, step time: 0.1108\n",
      "98/223, train_loss: 0.1026, step time: 0.1063\n",
      "99/223, train_loss: 0.0907, step time: 0.1344\n",
      "100/223, train_loss: 0.0982, step time: 0.1290\n",
      "101/223, train_loss: 0.0882, step time: 0.1062\n",
      "102/223, train_loss: 0.0948, step time: 0.1129\n",
      "103/223, train_loss: 0.0941, step time: 0.1065\n",
      "104/223, train_loss: 0.1112, step time: 0.1126\n",
      "105/223, train_loss: 0.1002, step time: 0.1220\n",
      "106/223, train_loss: 0.1062, step time: 0.1217\n",
      "107/223, train_loss: 0.0912, step time: 0.1067\n",
      "108/223, train_loss: 0.1089, step time: 0.1245\n",
      "109/223, train_loss: 0.0882, step time: 0.0990\n",
      "110/223, train_loss: 0.0943, step time: 0.1226\n",
      "111/223, train_loss: 0.0979, step time: 0.1162\n",
      "112/223, train_loss: 0.1051, step time: 0.1068\n",
      "113/223, train_loss: 0.1111, step time: 0.1107\n",
      "114/223, train_loss: 0.0865, step time: 0.1056\n",
      "115/223, train_loss: 0.1003, step time: 0.1286\n",
      "116/223, train_loss: 0.1011, step time: 0.1290\n",
      "117/223, train_loss: 0.0964, step time: 0.1118\n",
      "118/223, train_loss: 0.0962, step time: 0.1005\n",
      "119/223, train_loss: 0.1063, step time: 0.1001\n",
      "120/223, train_loss: 0.1067, step time: 0.1022\n",
      "121/223, train_loss: 0.0930, step time: 0.1047\n",
      "122/223, train_loss: 0.1013, step time: 0.1260\n",
      "123/223, train_loss: 0.0944, step time: 0.1058\n",
      "124/223, train_loss: 0.1119, step time: 0.1218\n",
      "125/223, train_loss: 0.1011, step time: 0.1015\n",
      "126/223, train_loss: 0.0922, step time: 0.1131\n",
      "127/223, train_loss: 0.1012, step time: 0.1335\n",
      "128/223, train_loss: 0.1001, step time: 0.1241\n",
      "129/223, train_loss: 0.0955, step time: 0.1490\n",
      "130/223, train_loss: 0.1052, step time: 0.1132\n",
      "131/223, train_loss: 0.0992, step time: 0.1144\n",
      "132/223, train_loss: 0.0903, step time: 0.1311\n",
      "133/223, train_loss: 0.1133, step time: 0.1010\n",
      "134/223, train_loss: 0.1018, step time: 0.1185\n",
      "135/223, train_loss: 0.1034, step time: 0.0998\n",
      "136/223, train_loss: 0.1058, step time: 0.1004\n",
      "137/223, train_loss: 0.1036, step time: 0.1000\n",
      "138/223, train_loss: 0.1030, step time: 0.1185\n",
      "139/223, train_loss: 0.0944, step time: 0.1175\n",
      "140/223, train_loss: 0.0954, step time: 0.1730\n",
      "141/223, train_loss: 0.1010, step time: 0.1089\n",
      "142/223, train_loss: 0.1013, step time: 0.1029\n",
      "143/223, train_loss: 0.0935, step time: 0.1142\n",
      "144/223, train_loss: 0.0985, step time: 0.1155\n",
      "145/223, train_loss: 0.0992, step time: 0.1272\n",
      "146/223, train_loss: 0.0965, step time: 0.1254\n",
      "147/223, train_loss: 0.1138, step time: 0.1006\n",
      "148/223, train_loss: 0.0874, step time: 0.1006\n",
      "149/223, train_loss: 0.0991, step time: 0.1099\n",
      "150/223, train_loss: 0.1108, step time: 0.1191\n",
      "151/223, train_loss: 0.1139, step time: 0.1132\n",
      "152/223, train_loss: 0.0880, step time: 0.1217\n",
      "153/223, train_loss: 0.0980, step time: 0.1001\n",
      "154/223, train_loss: 0.0952, step time: 0.1355\n",
      "155/223, train_loss: 0.1176, step time: 0.1050\n",
      "156/223, train_loss: 0.1032, step time: 0.1104\n",
      "157/223, train_loss: 0.1048, step time: 0.0993\n",
      "158/223, train_loss: 0.0912, step time: 0.1314\n",
      "159/223, train_loss: 0.0970, step time: 0.1082\n",
      "160/223, train_loss: 0.1029, step time: 0.1186\n",
      "161/223, train_loss: 0.0978, step time: 0.1004\n",
      "162/223, train_loss: 0.1020, step time: 0.1082\n",
      "163/223, train_loss: 0.1020, step time: 0.1064\n",
      "164/223, train_loss: 0.0962, step time: 0.1012\n",
      "165/223, train_loss: 0.0959, step time: 0.1059\n",
      "166/223, train_loss: 0.0989, step time: 0.1080\n",
      "167/223, train_loss: 0.0943, step time: 0.1343\n",
      "168/223, train_loss: 0.1017, step time: 0.1125\n",
      "169/223, train_loss: 0.1017, step time: 0.1174\n",
      "170/223, train_loss: 0.1136, step time: 0.1221\n",
      "171/223, train_loss: 0.1013, step time: 0.1126\n",
      "172/223, train_loss: 0.1105, step time: 0.1209\n",
      "173/223, train_loss: 0.0971, step time: 0.1019\n",
      "174/223, train_loss: 0.0995, step time: 0.1152\n",
      "175/223, train_loss: 0.0902, step time: 0.1005\n",
      "176/223, train_loss: 0.1000, step time: 0.0996\n",
      "177/223, train_loss: 0.1001, step time: 0.0994\n",
      "178/223, train_loss: 0.1042, step time: 0.1053\n",
      "179/223, train_loss: 0.1111, step time: 0.1009\n",
      "180/223, train_loss: 0.0935, step time: 0.1211\n",
      "181/223, train_loss: 0.1053, step time: 0.0999\n",
      "182/223, train_loss: 0.1073, step time: 0.1278\n",
      "183/223, train_loss: 0.1014, step time: 0.1095\n",
      "184/223, train_loss: 0.0941, step time: 0.1283\n",
      "185/223, train_loss: 0.0962, step time: 0.1008\n",
      "186/223, train_loss: 0.2957, step time: 0.1060\n",
      "187/223, train_loss: 0.1064, step time: 0.1158\n",
      "188/223, train_loss: 0.0953, step time: 0.1288\n",
      "189/223, train_loss: 0.0886, step time: 0.1074\n",
      "190/223, train_loss: 0.0994, step time: 0.1004\n",
      "191/223, train_loss: 0.1089, step time: 0.1107\n",
      "192/223, train_loss: 0.0940, step time: 0.1010\n",
      "193/223, train_loss: 0.0898, step time: 0.1072\n",
      "194/223, train_loss: 0.0920, step time: 0.1171\n",
      "195/223, train_loss: 0.0951, step time: 0.1078\n",
      "196/223, train_loss: 0.0939, step time: 0.1154\n",
      "197/223, train_loss: 0.0979, step time: 0.1090\n",
      "198/223, train_loss: 0.0922, step time: 0.1277\n",
      "199/223, train_loss: 0.1008, step time: 0.1056\n",
      "200/223, train_loss: 0.1045, step time: 0.1006\n",
      "201/223, train_loss: 0.1044, step time: 0.1051\n",
      "202/223, train_loss: 0.1008, step time: 0.1241\n",
      "203/223, train_loss: 0.1133, step time: 0.1000\n",
      "204/223, train_loss: 0.1124, step time: 0.1032\n",
      "205/223, train_loss: 0.0969, step time: 0.1007\n",
      "206/223, train_loss: 0.1099, step time: 0.1188\n",
      "207/223, train_loss: 0.0888, step time: 0.1155\n",
      "208/223, train_loss: 0.1044, step time: 0.1335\n",
      "209/223, train_loss: 0.0965, step time: 0.1131\n",
      "210/223, train_loss: 0.1089, step time: 0.1167\n",
      "211/223, train_loss: 0.1010, step time: 0.1117\n",
      "212/223, train_loss: 0.0945, step time: 0.1177\n",
      "213/223, train_loss: 0.0919, step time: 0.1079\n",
      "214/223, train_loss: 0.1024, step time: 0.1104\n",
      "215/223, train_loss: 0.0973, step time: 0.1055\n",
      "216/223, train_loss: 0.0920, step time: 0.1108\n",
      "217/223, train_loss: 0.1079, step time: 0.1005\n",
      "218/223, train_loss: 0.1068, step time: 0.1006\n",
      "219/223, train_loss: 0.1024, step time: 0.1002\n",
      "220/223, train_loss: 0.1035, step time: 0.0996\n",
      "221/223, train_loss: 0.0893, step time: 0.1002\n",
      "222/223, train_loss: 0.0957, step time: 0.0985\n",
      "223/223, train_loss: 0.1078, step time: 0.0993\n",
      "epoch 219 average loss: 0.1014\n",
      "time consuming of epoch 219 is: 87.9238\n",
      "----------\n",
      "epoch 220/300\n",
      "1/223, train_loss: 0.1000, step time: 0.1027\n",
      "2/223, train_loss: 0.0978, step time: 0.1071\n",
      "3/223, train_loss: 0.0919, step time: 0.1062\n",
      "4/223, train_loss: 0.1047, step time: 0.1128\n",
      "5/223, train_loss: 0.0981, step time: 0.1164\n",
      "6/223, train_loss: 0.1000, step time: 0.1005\n",
      "7/223, train_loss: 0.0980, step time: 0.1003\n",
      "8/223, train_loss: 0.0922, step time: 0.1002\n",
      "9/223, train_loss: 0.0927, step time: 0.1326\n",
      "10/223, train_loss: 0.0893, step time: 0.1052\n",
      "11/223, train_loss: 0.0988, step time: 0.1214\n",
      "12/223, train_loss: 0.0957, step time: 0.1064\n",
      "13/223, train_loss: 0.1104, step time: 0.1098\n",
      "14/223, train_loss: 0.1048, step time: 0.1055\n",
      "15/223, train_loss: 0.1038, step time: 0.1025\n",
      "16/223, train_loss: 0.1040, step time: 0.1004\n",
      "17/223, train_loss: 0.0998, step time: 0.1178\n",
      "18/223, train_loss: 0.1025, step time: 0.1007\n",
      "19/223, train_loss: 0.0992, step time: 0.1010\n",
      "20/223, train_loss: 0.0894, step time: 0.1005\n",
      "21/223, train_loss: 0.1023, step time: 0.1015\n",
      "22/223, train_loss: 0.0962, step time: 0.1074\n",
      "23/223, train_loss: 0.0981, step time: 0.1246\n",
      "24/223, train_loss: 0.0933, step time: 0.1107\n",
      "25/223, train_loss: 0.0943, step time: 0.1087\n",
      "26/223, train_loss: 0.0963, step time: 0.1137\n",
      "27/223, train_loss: 0.1026, step time: 0.1049\n",
      "28/223, train_loss: 0.1123, step time: 0.1172\n",
      "29/223, train_loss: 0.1005, step time: 0.1139\n",
      "30/223, train_loss: 0.0969, step time: 0.1140\n",
      "31/223, train_loss: 0.0903, step time: 0.1170\n",
      "32/223, train_loss: 0.0858, step time: 0.1313\n",
      "33/223, train_loss: 0.0943, step time: 0.1143\n",
      "34/223, train_loss: 0.0953, step time: 0.1164\n",
      "35/223, train_loss: 0.1009, step time: 0.1464\n",
      "36/223, train_loss: 0.0938, step time: 0.1083\n",
      "37/223, train_loss: 0.1164, step time: 0.1070\n",
      "38/223, train_loss: 0.1028, step time: 0.1129\n",
      "39/223, train_loss: 0.0904, step time: 0.1147\n",
      "40/223, train_loss: 0.1033, step time: 0.1155\n",
      "41/223, train_loss: 0.0942, step time: 0.1120\n",
      "42/223, train_loss: 0.1056, step time: 0.1015\n",
      "43/223, train_loss: 0.2976, step time: 0.1182\n",
      "44/223, train_loss: 0.0986, step time: 0.1121\n",
      "45/223, train_loss: 0.1094, step time: 0.1242\n",
      "46/223, train_loss: 0.0853, step time: 0.1044\n",
      "47/223, train_loss: 0.1129, step time: 0.1111\n",
      "48/223, train_loss: 0.1029, step time: 0.1004\n",
      "49/223, train_loss: 0.1041, step time: 0.1112\n",
      "50/223, train_loss: 0.1172, step time: 0.1142\n",
      "51/223, train_loss: 0.0946, step time: 0.1127\n",
      "52/223, train_loss: 0.0899, step time: 0.1264\n",
      "53/223, train_loss: 0.1136, step time: 0.1194\n",
      "54/223, train_loss: 0.0915, step time: 0.1135\n",
      "55/223, train_loss: 0.0999, step time: 0.1158\n",
      "56/223, train_loss: 0.1114, step time: 0.1114\n",
      "57/223, train_loss: 0.1052, step time: 0.1107\n",
      "58/223, train_loss: 0.1012, step time: 0.1009\n",
      "59/223, train_loss: 0.0952, step time: 0.1093\n",
      "60/223, train_loss: 0.1053, step time: 0.1366\n",
      "61/223, train_loss: 0.1000, step time: 0.1218\n",
      "62/223, train_loss: 0.0970, step time: 0.1001\n",
      "63/223, train_loss: 0.1024, step time: 0.1223\n",
      "64/223, train_loss: 0.0982, step time: 0.1574\n",
      "65/223, train_loss: 0.0894, step time: 0.1212\n",
      "66/223, train_loss: 0.1069, step time: 0.1102\n",
      "67/223, train_loss: 0.1093, step time: 0.1073\n",
      "68/223, train_loss: 0.0959, step time: 0.1024\n",
      "69/223, train_loss: 0.1223, step time: 0.1255\n",
      "70/223, train_loss: 0.0929, step time: 0.1138\n",
      "71/223, train_loss: 0.1191, step time: 0.1172\n",
      "72/223, train_loss: 0.0952, step time: 0.1135\n",
      "73/223, train_loss: 0.0996, step time: 0.1191\n",
      "74/223, train_loss: 0.0926, step time: 0.1137\n",
      "75/223, train_loss: 0.1107, step time: 0.1043\n",
      "76/223, train_loss: 0.1034, step time: 0.1203\n",
      "77/223, train_loss: 0.1031, step time: 0.1093\n",
      "78/223, train_loss: 0.0945, step time: 0.1141\n",
      "79/223, train_loss: 0.1017, step time: 0.1256\n",
      "80/223, train_loss: 0.0995, step time: 0.1177\n",
      "81/223, train_loss: 0.1035, step time: 0.0999\n",
      "82/223, train_loss: 0.1019, step time: 0.1003\n",
      "83/223, train_loss: 0.0866, step time: 0.1266\n",
      "84/223, train_loss: 0.1106, step time: 0.1003\n",
      "85/223, train_loss: 0.0913, step time: 0.1279\n",
      "86/223, train_loss: 0.1103, step time: 0.1344\n",
      "87/223, train_loss: 0.0998, step time: 0.1161\n",
      "88/223, train_loss: 0.1006, step time: 0.1009\n",
      "89/223, train_loss: 0.1073, step time: 0.1059\n",
      "90/223, train_loss: 0.0988, step time: 0.1017\n",
      "91/223, train_loss: 0.0926, step time: 0.1072\n",
      "92/223, train_loss: 0.0984, step time: 0.1021\n",
      "93/223, train_loss: 0.1078, step time: 0.1067\n",
      "94/223, train_loss: 0.1018, step time: 0.1115\n",
      "95/223, train_loss: 0.1078, step time: 0.1139\n",
      "96/223, train_loss: 0.1037, step time: 0.1053\n",
      "97/223, train_loss: 0.0995, step time: 0.1024\n",
      "98/223, train_loss: 0.1003, step time: 0.1054\n",
      "99/223, train_loss: 0.1069, step time: 0.1055\n",
      "100/223, train_loss: 0.0928, step time: 0.1139\n",
      "101/223, train_loss: 0.1105, step time: 0.1055\n",
      "102/223, train_loss: 0.1022, step time: 0.1186\n",
      "103/223, train_loss: 0.1005, step time: 0.1199\n",
      "104/223, train_loss: 0.1138, step time: 0.1168\n",
      "105/223, train_loss: 0.0898, step time: 0.1141\n",
      "106/223, train_loss: 0.0897, step time: 0.1113\n",
      "107/223, train_loss: 0.1110, step time: 0.1068\n",
      "108/223, train_loss: 0.1053, step time: 0.1290\n",
      "109/223, train_loss: 0.0915, step time: 0.1049\n",
      "110/223, train_loss: 0.1070, step time: 0.1077\n",
      "111/223, train_loss: 0.0914, step time: 0.1095\n",
      "112/223, train_loss: 0.1092, step time: 0.1255\n",
      "113/223, train_loss: 0.0995, step time: 0.1081\n",
      "114/223, train_loss: 0.0978, step time: 0.1224\n",
      "115/223, train_loss: 0.1033, step time: 0.1117\n",
      "116/223, train_loss: 0.1084, step time: 0.1201\n",
      "117/223, train_loss: 0.0920, step time: 0.1124\n",
      "118/223, train_loss: 0.1049, step time: 0.1082\n",
      "119/223, train_loss: 0.1076, step time: 0.1003\n",
      "120/223, train_loss: 0.0983, step time: 0.1292\n",
      "121/223, train_loss: 0.1069, step time: 0.1175\n",
      "122/223, train_loss: 0.1012, step time: 0.1117\n",
      "123/223, train_loss: 0.0921, step time: 0.1293\n",
      "124/223, train_loss: 0.0966, step time: 0.1081\n",
      "125/223, train_loss: 0.1030, step time: 0.1049\n",
      "126/223, train_loss: 0.1031, step time: 0.1191\n",
      "127/223, train_loss: 0.0873, step time: 0.1095\n",
      "128/223, train_loss: 0.1015, step time: 0.1276\n",
      "129/223, train_loss: 0.0996, step time: 0.1008\n",
      "130/223, train_loss: 0.1005, step time: 0.1048\n",
      "131/223, train_loss: 0.1084, step time: 0.1053\n",
      "132/223, train_loss: 0.0902, step time: 0.1089\n",
      "133/223, train_loss: 0.1039, step time: 0.1122\n",
      "134/223, train_loss: 0.0970, step time: 0.1107\n",
      "135/223, train_loss: 0.0958, step time: 0.1043\n",
      "136/223, train_loss: 0.1031, step time: 0.1130\n",
      "137/223, train_loss: 0.1039, step time: 0.1075\n",
      "138/223, train_loss: 0.1081, step time: 0.1006\n",
      "139/223, train_loss: 0.1021, step time: 0.1119\n",
      "140/223, train_loss: 0.0986, step time: 0.1218\n",
      "141/223, train_loss: 0.1036, step time: 0.1177\n",
      "142/223, train_loss: 0.1037, step time: 0.1070\n",
      "143/223, train_loss: 0.1101, step time: 0.1007\n",
      "144/223, train_loss: 0.0920, step time: 0.1362\n",
      "145/223, train_loss: 0.0934, step time: 0.1353\n",
      "146/223, train_loss: 0.0971, step time: 0.0994\n",
      "147/223, train_loss: 0.1043, step time: 0.1002\n",
      "148/223, train_loss: 0.1030, step time: 0.0999\n",
      "149/223, train_loss: 0.0986, step time: 0.1026\n",
      "150/223, train_loss: 0.0987, step time: 0.0999\n",
      "151/223, train_loss: 0.1117, step time: 0.1000\n",
      "152/223, train_loss: 0.0979, step time: 0.1033\n",
      "153/223, train_loss: 0.0958, step time: 0.1036\n",
      "154/223, train_loss: 0.1038, step time: 0.1129\n",
      "155/223, train_loss: 0.0894, step time: 0.1200\n",
      "156/223, train_loss: 0.1143, step time: 0.1008\n",
      "157/223, train_loss: 0.1022, step time: 0.1074\n",
      "158/223, train_loss: 0.1041, step time: 0.1039\n",
      "159/223, train_loss: 0.1009, step time: 0.1078\n",
      "160/223, train_loss: 0.0944, step time: 0.1058\n",
      "161/223, train_loss: 0.1117, step time: 0.1006\n",
      "162/223, train_loss: 0.1015, step time: 0.1182\n",
      "163/223, train_loss: 0.0869, step time: 0.1120\n",
      "164/223, train_loss: 0.1032, step time: 0.1012\n",
      "165/223, train_loss: 0.1065, step time: 0.1145\n",
      "166/223, train_loss: 0.0966, step time: 0.1185\n",
      "167/223, train_loss: 0.1116, step time: 0.1408\n",
      "168/223, train_loss: 0.1125, step time: 0.1002\n",
      "169/223, train_loss: 0.1029, step time: 0.1007\n",
      "170/223, train_loss: 0.1045, step time: 0.1152\n",
      "171/223, train_loss: 0.0956, step time: 0.1233\n",
      "172/223, train_loss: 0.0911, step time: 0.1024\n",
      "173/223, train_loss: 0.1072, step time: 0.1002\n",
      "174/223, train_loss: 0.0893, step time: 0.1007\n",
      "175/223, train_loss: 0.0921, step time: 0.1011\n",
      "176/223, train_loss: 0.0980, step time: 0.1045\n",
      "177/223, train_loss: 0.1078, step time: 0.1035\n",
      "178/223, train_loss: 0.1150, step time: 0.1161\n",
      "179/223, train_loss: 0.1052, step time: 0.1136\n",
      "180/223, train_loss: 0.0917, step time: 0.1070\n",
      "181/223, train_loss: 0.0986, step time: 0.1122\n",
      "182/223, train_loss: 0.1054, step time: 0.1054\n",
      "183/223, train_loss: 0.1053, step time: 0.1134\n",
      "184/223, train_loss: 0.1025, step time: 0.1268\n",
      "185/223, train_loss: 0.0962, step time: 0.1362\n",
      "186/223, train_loss: 0.0925, step time: 0.1163\n",
      "187/223, train_loss: 0.0909, step time: 0.1002\n",
      "188/223, train_loss: 0.0930, step time: 0.1034\n",
      "189/223, train_loss: 0.1027, step time: 0.1004\n",
      "190/223, train_loss: 0.1012, step time: 0.1163\n",
      "191/223, train_loss: 0.1014, step time: 0.1137\n",
      "192/223, train_loss: 0.1000, step time: 0.1133\n",
      "193/223, train_loss: 0.1060, step time: 0.1544\n",
      "194/223, train_loss: 0.0905, step time: 0.1151\n",
      "195/223, train_loss: 0.1016, step time: 0.1005\n",
      "196/223, train_loss: 0.0941, step time: 0.1174\n",
      "197/223, train_loss: 0.1115, step time: 0.1030\n",
      "198/223, train_loss: 0.0975, step time: 0.1001\n",
      "199/223, train_loss: 0.1100, step time: 0.1105\n",
      "200/223, train_loss: 0.1011, step time: 0.1005\n",
      "201/223, train_loss: 0.1009, step time: 0.1007\n",
      "202/223, train_loss: 0.1014, step time: 0.1173\n",
      "203/223, train_loss: 0.0966, step time: 0.1211\n",
      "204/223, train_loss: 0.0895, step time: 0.1191\n",
      "205/223, train_loss: 0.1044, step time: 0.1097\n",
      "206/223, train_loss: 0.0898, step time: 0.1018\n",
      "207/223, train_loss: 0.1061, step time: 0.1055\n",
      "208/223, train_loss: 0.0942, step time: 0.1073\n",
      "209/223, train_loss: 0.0973, step time: 0.1043\n",
      "210/223, train_loss: 0.0944, step time: 0.1145\n",
      "211/223, train_loss: 0.1030, step time: 0.1053\n",
      "212/223, train_loss: 0.0942, step time: 0.1156\n",
      "213/223, train_loss: 0.0939, step time: 0.1183\n",
      "214/223, train_loss: 0.0941, step time: 0.1110\n",
      "215/223, train_loss: 0.1078, step time: 0.1100\n",
      "216/223, train_loss: 0.1068, step time: 0.1133\n",
      "217/223, train_loss: 0.0990, step time: 0.1004\n",
      "218/223, train_loss: 0.0932, step time: 0.0999\n",
      "219/223, train_loss: 0.1101, step time: 0.1075\n",
      "220/223, train_loss: 0.0954, step time: 0.1015\n",
      "221/223, train_loss: 0.1024, step time: 0.1008\n",
      "222/223, train_loss: 0.1074, step time: 0.0998\n",
      "223/223, train_loss: 0.1027, step time: 0.0997\n",
      "epoch 220 average loss: 0.1014\n",
      "saved new best metric model\n",
      "current epoch: 220 current mean dice: 0.8609 tc: 0.9224 wt: 0.8702 et: 0.7901\n",
      "best mean dice: 0.8609 at epoch: 220\n",
      "time consuming of epoch 220 is: 91.9846\n",
      "----------\n",
      "epoch 221/300\n",
      "1/223, train_loss: 0.0905, step time: 0.1015\n",
      "2/223, train_loss: 0.1071, step time: 0.1008\n",
      "3/223, train_loss: 0.1021, step time: 0.1011\n",
      "4/223, train_loss: 0.0888, step time: 0.1003\n",
      "5/223, train_loss: 0.0948, step time: 0.1056\n",
      "6/223, train_loss: 0.0874, step time: 0.1000\n",
      "7/223, train_loss: 0.1058, step time: 0.1155\n",
      "8/223, train_loss: 0.0996, step time: 0.1006\n",
      "9/223, train_loss: 0.1076, step time: 0.1005\n",
      "10/223, train_loss: 0.1016, step time: 0.1060\n",
      "11/223, train_loss: 0.1118, step time: 0.1221\n",
      "12/223, train_loss: 0.1031, step time: 0.1186\n",
      "13/223, train_loss: 0.1164, step time: 0.1001\n",
      "14/223, train_loss: 0.1103, step time: 0.1007\n",
      "15/223, train_loss: 0.1026, step time: 0.1021\n",
      "16/223, train_loss: 0.0978, step time: 0.1022\n",
      "17/223, train_loss: 0.0931, step time: 0.1122\n",
      "18/223, train_loss: 0.1096, step time: 0.1004\n",
      "19/223, train_loss: 0.0919, step time: 0.1095\n",
      "20/223, train_loss: 0.0943, step time: 0.1207\n",
      "21/223, train_loss: 0.0972, step time: 0.1002\n",
      "22/223, train_loss: 0.1133, step time: 0.1011\n",
      "23/223, train_loss: 0.1078, step time: 0.1004\n",
      "24/223, train_loss: 0.0928, step time: 0.0998\n",
      "25/223, train_loss: 0.1070, step time: 0.1196\n",
      "26/223, train_loss: 0.0875, step time: 0.1035\n",
      "27/223, train_loss: 0.1068, step time: 0.1034\n",
      "28/223, train_loss: 0.0938, step time: 0.1097\n",
      "29/223, train_loss: 0.0971, step time: 0.1060\n",
      "30/223, train_loss: 0.1067, step time: 0.1157\n",
      "31/223, train_loss: 0.1025, step time: 0.1168\n",
      "32/223, train_loss: 0.0886, step time: 0.1191\n",
      "33/223, train_loss: 0.1151, step time: 0.1048\n",
      "34/223, train_loss: 0.1054, step time: 0.1076\n",
      "35/223, train_loss: 0.1041, step time: 0.1005\n",
      "36/223, train_loss: 0.1093, step time: 0.1006\n",
      "37/223, train_loss: 0.1051, step time: 0.1107\n",
      "38/223, train_loss: 0.1010, step time: 0.1005\n",
      "39/223, train_loss: 0.0991, step time: 0.0998\n",
      "40/223, train_loss: 0.0920, step time: 0.1008\n",
      "41/223, train_loss: 0.1009, step time: 0.1078\n",
      "42/223, train_loss: 0.0996, step time: 0.1091\n",
      "43/223, train_loss: 0.0964, step time: 0.1000\n",
      "44/223, train_loss: 0.1056, step time: 0.1010\n",
      "45/223, train_loss: 0.1033, step time: 0.1055\n",
      "46/223, train_loss: 0.0926, step time: 0.1048\n",
      "47/223, train_loss: 0.0921, step time: 0.1052\n",
      "48/223, train_loss: 0.0990, step time: 0.1307\n",
      "49/223, train_loss: 0.1001, step time: 0.1171\n",
      "50/223, train_loss: 0.1008, step time: 0.1147\n",
      "51/223, train_loss: 0.0989, step time: 0.1268\n",
      "52/223, train_loss: 0.1082, step time: 0.1195\n",
      "53/223, train_loss: 0.0935, step time: 0.1131\n",
      "54/223, train_loss: 0.0965, step time: 0.1117\n",
      "55/223, train_loss: 0.1071, step time: 0.1365\n",
      "56/223, train_loss: 0.1004, step time: 0.1394\n",
      "57/223, train_loss: 0.0896, step time: 0.1532\n",
      "58/223, train_loss: 0.1008, step time: 0.1042\n",
      "59/223, train_loss: 0.1026, step time: 0.1347\n",
      "60/223, train_loss: 0.0934, step time: 0.1255\n",
      "61/223, train_loss: 0.0966, step time: 0.1140\n",
      "62/223, train_loss: 0.0966, step time: 0.1150\n",
      "63/223, train_loss: 0.0941, step time: 0.1198\n",
      "64/223, train_loss: 0.1037, step time: 0.1220\n",
      "65/223, train_loss: 0.1042, step time: 0.1319\n",
      "66/223, train_loss: 0.1030, step time: 0.1136\n",
      "67/223, train_loss: 0.1046, step time: 0.1245\n",
      "68/223, train_loss: 0.0871, step time: 0.1127\n",
      "69/223, train_loss: 0.0954, step time: 0.1032\n",
      "70/223, train_loss: 0.0894, step time: 0.1168\n",
      "71/223, train_loss: 0.1048, step time: 0.1347\n",
      "72/223, train_loss: 0.0979, step time: 0.1089\n",
      "73/223, train_loss: 0.0994, step time: 0.1009\n",
      "74/223, train_loss: 0.0969, step time: 0.1373\n",
      "75/223, train_loss: 0.1098, step time: 0.1063\n",
      "76/223, train_loss: 0.0912, step time: 0.1128\n",
      "77/223, train_loss: 0.1109, step time: 0.1055\n",
      "78/223, train_loss: 0.1157, step time: 0.1058\n",
      "79/223, train_loss: 0.0970, step time: 0.1124\n",
      "80/223, train_loss: 0.1025, step time: 0.1216\n",
      "81/223, train_loss: 0.0954, step time: 0.1016\n",
      "82/223, train_loss: 0.0945, step time: 0.1151\n",
      "83/223, train_loss: 0.1087, step time: 0.1228\n",
      "84/223, train_loss: 0.0931, step time: 0.1004\n",
      "85/223, train_loss: 0.1016, step time: 0.1257\n",
      "86/223, train_loss: 0.1007, step time: 0.1277\n",
      "87/223, train_loss: 0.0994, step time: 0.1278\n",
      "88/223, train_loss: 0.0932, step time: 0.1214\n",
      "89/223, train_loss: 0.0890, step time: 0.1019\n",
      "90/223, train_loss: 0.0976, step time: 0.1086\n",
      "91/223, train_loss: 0.0958, step time: 0.1076\n",
      "92/223, train_loss: 0.1015, step time: 0.1133\n",
      "93/223, train_loss: 0.0948, step time: 0.1120\n",
      "94/223, train_loss: 0.0919, step time: 0.1008\n",
      "95/223, train_loss: 0.0880, step time: 0.1006\n",
      "96/223, train_loss: 0.1049, step time: 0.1417\n",
      "97/223, train_loss: 0.1032, step time: 0.1008\n",
      "98/223, train_loss: 0.0970, step time: 0.1070\n",
      "99/223, train_loss: 0.0958, step time: 0.0986\n",
      "100/223, train_loss: 0.0906, step time: 0.1367\n",
      "101/223, train_loss: 0.0885, step time: 0.1008\n",
      "102/223, train_loss: 0.1133, step time: 0.1036\n",
      "103/223, train_loss: 0.1018, step time: 0.1039\n",
      "104/223, train_loss: 0.1029, step time: 0.1137\n",
      "105/223, train_loss: 0.1011, step time: 0.1074\n",
      "106/223, train_loss: 0.0993, step time: 0.1097\n",
      "107/223, train_loss: 0.1031, step time: 0.1116\n",
      "108/223, train_loss: 0.0942, step time: 0.1069\n",
      "109/223, train_loss: 0.1010, step time: 0.1189\n",
      "110/223, train_loss: 0.0931, step time: 0.1000\n",
      "111/223, train_loss: 0.1039, step time: 0.1112\n",
      "112/223, train_loss: 0.0922, step time: 0.1338\n",
      "113/223, train_loss: 0.0948, step time: 0.1079\n",
      "114/223, train_loss: 0.1063, step time: 0.1073\n",
      "115/223, train_loss: 0.0958, step time: 0.1216\n",
      "116/223, train_loss: 0.0982, step time: 0.1373\n",
      "117/223, train_loss: 0.0971, step time: 0.1233\n",
      "118/223, train_loss: 0.0941, step time: 0.1104\n",
      "119/223, train_loss: 0.1075, step time: 0.1056\n",
      "120/223, train_loss: 0.1167, step time: 0.1195\n",
      "121/223, train_loss: 0.0880, step time: 0.1381\n",
      "122/223, train_loss: 0.0956, step time: 0.1255\n",
      "123/223, train_loss: 0.0963, step time: 0.1003\n",
      "124/223, train_loss: 0.1044, step time: 0.1003\n",
      "125/223, train_loss: 0.0993, step time: 0.1382\n",
      "126/223, train_loss: 0.1132, step time: 0.1176\n",
      "127/223, train_loss: 0.0998, step time: 0.1160\n",
      "128/223, train_loss: 0.3016, step time: 0.1254\n",
      "129/223, train_loss: 0.1027, step time: 0.1328\n",
      "130/223, train_loss: 0.0938, step time: 0.1088\n",
      "131/223, train_loss: 0.1001, step time: 0.1109\n",
      "132/223, train_loss: 0.0983, step time: 0.1017\n",
      "133/223, train_loss: 0.1051, step time: 0.1168\n",
      "134/223, train_loss: 0.0910, step time: 0.1148\n",
      "135/223, train_loss: 0.1165, step time: 0.1098\n",
      "136/223, train_loss: 0.1066, step time: 0.1399\n",
      "137/223, train_loss: 0.1104, step time: 0.1164\n",
      "138/223, train_loss: 0.0982, step time: 0.1232\n",
      "139/223, train_loss: 0.1066, step time: 0.1176\n",
      "140/223, train_loss: 0.1022, step time: 0.1197\n",
      "141/223, train_loss: 0.1045, step time: 0.1137\n",
      "142/223, train_loss: 0.0983, step time: 0.0999\n",
      "143/223, train_loss: 0.0990, step time: 0.1000\n",
      "144/223, train_loss: 0.0997, step time: 0.1063\n",
      "145/223, train_loss: 0.1025, step time: 0.1070\n",
      "146/223, train_loss: 0.0846, step time: 0.1221\n",
      "147/223, train_loss: 0.1025, step time: 0.1086\n",
      "148/223, train_loss: 0.1193, step time: 0.1093\n",
      "149/223, train_loss: 0.1075, step time: 0.1225\n",
      "150/223, train_loss: 0.0946, step time: 0.1160\n",
      "151/223, train_loss: 0.0996, step time: 0.1089\n",
      "152/223, train_loss: 0.1008, step time: 0.1366\n",
      "153/223, train_loss: 0.1013, step time: 0.1050\n",
      "154/223, train_loss: 0.1017, step time: 0.1213\n",
      "155/223, train_loss: 0.1016, step time: 0.1080\n",
      "156/223, train_loss: 0.0919, step time: 0.1054\n",
      "157/223, train_loss: 0.1101, step time: 0.1063\n",
      "158/223, train_loss: 0.0966, step time: 0.1076\n",
      "159/223, train_loss: 0.0947, step time: 0.0994\n",
      "160/223, train_loss: 0.1070, step time: 0.1009\n",
      "161/223, train_loss: 0.1217, step time: 0.1004\n",
      "162/223, train_loss: 0.0944, step time: 0.1051\n",
      "163/223, train_loss: 0.1015, step time: 0.1098\n",
      "164/223, train_loss: 0.0950, step time: 0.1192\n",
      "165/223, train_loss: 0.0994, step time: 0.1029\n",
      "166/223, train_loss: 0.1022, step time: 0.1095\n",
      "167/223, train_loss: 0.0944, step time: 0.1181\n",
      "168/223, train_loss: 0.1105, step time: 0.1036\n",
      "169/223, train_loss: 0.1042, step time: 0.1072\n",
      "170/223, train_loss: 0.1051, step time: 0.1174\n",
      "171/223, train_loss: 0.0944, step time: 0.1193\n",
      "172/223, train_loss: 0.1011, step time: 0.1093\n",
      "173/223, train_loss: 0.0910, step time: 0.1256\n",
      "174/223, train_loss: 0.0987, step time: 0.1127\n",
      "175/223, train_loss: 0.1038, step time: 0.0999\n",
      "176/223, train_loss: 0.1237, step time: 0.1000\n",
      "177/223, train_loss: 0.1031, step time: 0.1358\n",
      "178/223, train_loss: 0.0973, step time: 0.1177\n",
      "179/223, train_loss: 0.1000, step time: 0.1124\n",
      "180/223, train_loss: 0.1038, step time: 0.1056\n",
      "181/223, train_loss: 0.1159, step time: 0.1103\n",
      "182/223, train_loss: 0.1046, step time: 0.1431\n",
      "183/223, train_loss: 0.0931, step time: 0.1110\n",
      "184/223, train_loss: 0.1220, step time: 0.1168\n",
      "185/223, train_loss: 0.0996, step time: 0.1197\n",
      "186/223, train_loss: 0.0971, step time: 0.1063\n",
      "187/223, train_loss: 0.0922, step time: 0.1007\n",
      "188/223, train_loss: 0.1041, step time: 0.1167\n",
      "189/223, train_loss: 0.0927, step time: 0.1042\n",
      "190/223, train_loss: 0.0990, step time: 0.1267\n",
      "191/223, train_loss: 0.0923, step time: 0.1289\n",
      "192/223, train_loss: 0.1011, step time: 0.1042\n",
      "193/223, train_loss: 0.0912, step time: 0.1029\n",
      "194/223, train_loss: 0.0972, step time: 0.1119\n",
      "195/223, train_loss: 0.0991, step time: 0.1119\n",
      "196/223, train_loss: 0.1045, step time: 0.1133\n",
      "197/223, train_loss: 0.0921, step time: 0.1220\n",
      "198/223, train_loss: 0.1016, step time: 0.1163\n",
      "199/223, train_loss: 0.1072, step time: 0.1142\n",
      "200/223, train_loss: 0.0981, step time: 0.1138\n",
      "201/223, train_loss: 0.1006, step time: 0.1271\n",
      "202/223, train_loss: 0.0988, step time: 0.1151\n",
      "203/223, train_loss: 0.0978, step time: 0.1028\n",
      "204/223, train_loss: 0.0981, step time: 0.1252\n",
      "205/223, train_loss: 0.1056, step time: 0.1000\n",
      "206/223, train_loss: 0.0974, step time: 0.1072\n",
      "207/223, train_loss: 0.0977, step time: 0.1174\n",
      "208/223, train_loss: 0.1093, step time: 0.1175\n",
      "209/223, train_loss: 0.0967, step time: 0.1270\n",
      "210/223, train_loss: 0.0985, step time: 0.0994\n",
      "211/223, train_loss: 0.0904, step time: 0.1005\n",
      "212/223, train_loss: 0.0985, step time: 0.1015\n",
      "213/223, train_loss: 0.1025, step time: 0.1029\n",
      "214/223, train_loss: 0.0913, step time: 0.1002\n",
      "215/223, train_loss: 0.1042, step time: 0.0994\n",
      "216/223, train_loss: 0.1084, step time: 0.0993\n",
      "217/223, train_loss: 0.1100, step time: 0.1164\n",
      "218/223, train_loss: 0.1020, step time: 0.1000\n",
      "219/223, train_loss: 0.1075, step time: 0.1001\n",
      "220/223, train_loss: 0.0945, step time: 0.1001\n",
      "221/223, train_loss: 0.0983, step time: 0.1010\n",
      "222/223, train_loss: 0.0995, step time: 0.0997\n",
      "223/223, train_loss: 0.1014, step time: 0.0994\n",
      "epoch 221 average loss: 0.1012\n",
      "time consuming of epoch 221 is: 90.1322\n",
      "----------\n",
      "epoch 222/300\n",
      "1/223, train_loss: 0.1023, step time: 0.1074\n",
      "2/223, train_loss: 0.1088, step time: 0.1006\n",
      "3/223, train_loss: 0.0896, step time: 0.0997\n",
      "4/223, train_loss: 0.1075, step time: 0.1065\n",
      "5/223, train_loss: 0.0867, step time: 0.1186\n",
      "6/223, train_loss: 0.1033, step time: 0.0995\n",
      "7/223, train_loss: 0.0964, step time: 0.1003\n",
      "8/223, train_loss: 0.1030, step time: 0.1058\n",
      "9/223, train_loss: 0.1007, step time: 0.1106\n",
      "10/223, train_loss: 0.1032, step time: 0.1020\n",
      "11/223, train_loss: 0.1039, step time: 0.1165\n",
      "12/223, train_loss: 0.1011, step time: 0.1002\n",
      "13/223, train_loss: 0.0908, step time: 0.1254\n",
      "14/223, train_loss: 0.0973, step time: 0.1356\n",
      "15/223, train_loss: 0.1037, step time: 0.1006\n",
      "16/223, train_loss: 0.1003, step time: 0.1006\n",
      "17/223, train_loss: 0.0948, step time: 0.1041\n",
      "18/223, train_loss: 0.1043, step time: 0.1138\n",
      "19/223, train_loss: 0.0965, step time: 0.1154\n",
      "20/223, train_loss: 0.0993, step time: 0.1326\n",
      "21/223, train_loss: 0.0915, step time: 0.1199\n",
      "22/223, train_loss: 0.0965, step time: 0.1000\n",
      "23/223, train_loss: 0.1091, step time: 0.1123\n",
      "24/223, train_loss: 0.1002, step time: 0.1224\n",
      "25/223, train_loss: 0.0942, step time: 0.1091\n",
      "26/223, train_loss: 0.1087, step time: 0.1133\n",
      "27/223, train_loss: 0.0956, step time: 0.1181\n",
      "28/223, train_loss: 0.0918, step time: 0.1249\n",
      "29/223, train_loss: 0.0978, step time: 0.1140\n",
      "30/223, train_loss: 0.1033, step time: 0.1201\n",
      "31/223, train_loss: 0.1041, step time: 0.1178\n",
      "32/223, train_loss: 0.0951, step time: 0.1284\n",
      "33/223, train_loss: 0.0902, step time: 0.1407\n",
      "34/223, train_loss: 0.1129, step time: 0.1242\n",
      "35/223, train_loss: 0.0943, step time: 0.1416\n",
      "36/223, train_loss: 0.1004, step time: 0.1050\n",
      "37/223, train_loss: 0.1056, step time: 0.1103\n",
      "38/223, train_loss: 0.1035, step time: 0.1235\n",
      "39/223, train_loss: 0.0931, step time: 0.1049\n",
      "40/223, train_loss: 0.1017, step time: 0.1190\n",
      "41/223, train_loss: 0.0968, step time: 0.1097\n",
      "42/223, train_loss: 0.0936, step time: 0.0994\n",
      "43/223, train_loss: 0.1177, step time: 0.1164\n",
      "44/223, train_loss: 0.1001, step time: 0.1046\n",
      "45/223, train_loss: 0.0980, step time: 0.0999\n",
      "46/223, train_loss: 0.1056, step time: 0.1129\n",
      "47/223, train_loss: 0.1080, step time: 0.1121\n",
      "48/223, train_loss: 0.0938, step time: 0.1174\n",
      "49/223, train_loss: 0.1044, step time: 0.1073\n",
      "50/223, train_loss: 0.0919, step time: 0.1108\n",
      "51/223, train_loss: 0.1098, step time: 0.1088\n",
      "52/223, train_loss: 0.1118, step time: 0.1016\n",
      "53/223, train_loss: 0.0887, step time: 0.1168\n",
      "54/223, train_loss: 0.0857, step time: 0.1028\n",
      "55/223, train_loss: 0.1133, step time: 0.1097\n",
      "56/223, train_loss: 0.0939, step time: 0.1164\n",
      "57/223, train_loss: 0.0941, step time: 0.1079\n",
      "58/223, train_loss: 0.1101, step time: 0.1173\n",
      "59/223, train_loss: 0.0928, step time: 0.1049\n",
      "60/223, train_loss: 0.0985, step time: 0.1323\n",
      "61/223, train_loss: 0.1101, step time: 0.1115\n",
      "62/223, train_loss: 0.0974, step time: 0.1149\n",
      "63/223, train_loss: 0.1050, step time: 0.1065\n",
      "64/223, train_loss: 0.0942, step time: 0.1290\n",
      "65/223, train_loss: 0.1015, step time: 0.1020\n",
      "66/223, train_loss: 0.0918, step time: 0.1100\n",
      "67/223, train_loss: 0.0984, step time: 0.1063\n",
      "68/223, train_loss: 0.0948, step time: 0.1228\n",
      "69/223, train_loss: 0.1025, step time: 0.1238\n",
      "70/223, train_loss: 0.1024, step time: 0.1154\n",
      "71/223, train_loss: 0.1033, step time: 0.1049\n",
      "72/223, train_loss: 0.0989, step time: 0.1076\n",
      "73/223, train_loss: 0.0954, step time: 0.1225\n",
      "74/223, train_loss: 0.0939, step time: 0.1070\n",
      "75/223, train_loss: 0.1017, step time: 0.1131\n",
      "76/223, train_loss: 0.1125, step time: 0.1136\n",
      "77/223, train_loss: 0.0955, step time: 0.1213\n",
      "78/223, train_loss: 0.0965, step time: 0.1072\n",
      "79/223, train_loss: 0.0961, step time: 0.1086\n",
      "80/223, train_loss: 0.1040, step time: 0.1294\n",
      "81/223, train_loss: 0.0942, step time: 0.1065\n",
      "82/223, train_loss: 0.0974, step time: 0.1084\n",
      "83/223, train_loss: 0.0903, step time: 0.1017\n",
      "84/223, train_loss: 0.1013, step time: 0.1138\n",
      "85/223, train_loss: 0.1060, step time: 0.1046\n",
      "86/223, train_loss: 0.1031, step time: 0.1075\n",
      "87/223, train_loss: 0.1010, step time: 0.1020\n",
      "88/223, train_loss: 0.1022, step time: 0.1162\n",
      "89/223, train_loss: 0.1009, step time: 0.1004\n",
      "90/223, train_loss: 0.1019, step time: 0.1104\n",
      "91/223, train_loss: 0.0905, step time: 0.0991\n",
      "92/223, train_loss: 0.0941, step time: 0.0994\n",
      "93/223, train_loss: 0.0972, step time: 0.1191\n",
      "94/223, train_loss: 0.0990, step time: 0.1066\n",
      "95/223, train_loss: 0.0895, step time: 0.1045\n",
      "96/223, train_loss: 0.1055, step time: 0.1621\n",
      "97/223, train_loss: 0.1057, step time: 0.1218\n",
      "98/223, train_loss: 0.0930, step time: 0.1186\n",
      "99/223, train_loss: 0.0998, step time: 0.0992\n",
      "100/223, train_loss: 0.0970, step time: 0.0992\n",
      "101/223, train_loss: 0.0917, step time: 0.1152\n",
      "102/223, train_loss: 0.0981, step time: 0.1185\n",
      "103/223, train_loss: 0.0943, step time: 0.1102\n",
      "104/223, train_loss: 0.1182, step time: 0.0998\n",
      "105/223, train_loss: 0.0956, step time: 0.1046\n",
      "106/223, train_loss: 0.0921, step time: 0.1002\n",
      "107/223, train_loss: 0.1071, step time: 0.1004\n",
      "108/223, train_loss: 0.0961, step time: 0.1127\n",
      "109/223, train_loss: 0.0959, step time: 0.1002\n",
      "110/223, train_loss: 0.1200, step time: 0.1046\n",
      "111/223, train_loss: 0.0929, step time: 0.1291\n",
      "112/223, train_loss: 0.1005, step time: 0.1339\n",
      "113/223, train_loss: 0.2965, step time: 0.1040\n",
      "114/223, train_loss: 0.0955, step time: 0.1085\n",
      "115/223, train_loss: 0.1011, step time: 0.1364\n",
      "116/223, train_loss: 0.0944, step time: 0.1044\n",
      "117/223, train_loss: 0.1006, step time: 0.1122\n",
      "118/223, train_loss: 0.1096, step time: 0.1158\n",
      "119/223, train_loss: 0.0995, step time: 0.1248\n",
      "120/223, train_loss: 0.0952, step time: 0.1405\n",
      "121/223, train_loss: 0.0974, step time: 0.1082\n",
      "122/223, train_loss: 0.0966, step time: 0.1461\n",
      "123/223, train_loss: 0.1063, step time: 0.1167\n",
      "124/223, train_loss: 0.1059, step time: 0.1127\n",
      "125/223, train_loss: 0.1135, step time: 0.1167\n",
      "126/223, train_loss: 0.0927, step time: 0.0999\n",
      "127/223, train_loss: 0.0932, step time: 0.1042\n",
      "128/223, train_loss: 0.1037, step time: 0.1513\n",
      "129/223, train_loss: 0.1116, step time: 0.1106\n",
      "130/223, train_loss: 0.1058, step time: 0.1247\n",
      "131/223, train_loss: 0.0994, step time: 0.1266\n",
      "132/223, train_loss: 0.1061, step time: 0.1281\n",
      "133/223, train_loss: 0.0984, step time: 0.1215\n",
      "134/223, train_loss: 0.0962, step time: 0.1089\n",
      "135/223, train_loss: 0.1279, step time: 0.1291\n",
      "136/223, train_loss: 0.0950, step time: 0.1068\n",
      "137/223, train_loss: 0.1040, step time: 0.1207\n",
      "138/223, train_loss: 0.0905, step time: 0.1000\n",
      "139/223, train_loss: 0.1085, step time: 0.1044\n",
      "140/223, train_loss: 0.1050, step time: 0.1372\n",
      "141/223, train_loss: 0.0975, step time: 0.1084\n",
      "142/223, train_loss: 0.0985, step time: 0.1233\n",
      "143/223, train_loss: 0.1117, step time: 0.1117\n",
      "144/223, train_loss: 0.0986, step time: 0.1099\n",
      "145/223, train_loss: 0.1033, step time: 0.1252\n",
      "146/223, train_loss: 0.0959, step time: 0.1413\n",
      "147/223, train_loss: 0.0933, step time: 0.1183\n",
      "148/223, train_loss: 0.1059, step time: 0.1108\n",
      "149/223, train_loss: 0.1000, step time: 0.1107\n",
      "150/223, train_loss: 0.0991, step time: 0.0993\n",
      "151/223, train_loss: 0.0913, step time: 0.1003\n",
      "152/223, train_loss: 0.1006, step time: 0.1017\n",
      "153/223, train_loss: 0.1011, step time: 0.1006\n",
      "154/223, train_loss: 0.0936, step time: 0.1088\n",
      "155/223, train_loss: 0.1127, step time: 0.1022\n",
      "156/223, train_loss: 0.1062, step time: 0.1594\n",
      "157/223, train_loss: 0.1099, step time: 0.1433\n",
      "158/223, train_loss: 0.0914, step time: 0.1110\n",
      "159/223, train_loss: 0.1082, step time: 0.1157\n",
      "160/223, train_loss: 0.1085, step time: 0.1105\n",
      "161/223, train_loss: 0.1025, step time: 0.1127\n",
      "162/223, train_loss: 0.0888, step time: 0.1011\n",
      "163/223, train_loss: 0.1005, step time: 0.0996\n",
      "164/223, train_loss: 0.0965, step time: 0.1496\n",
      "165/223, train_loss: 0.1074, step time: 0.1068\n",
      "166/223, train_loss: 0.0969, step time: 0.1172\n",
      "167/223, train_loss: 0.0939, step time: 0.1069\n",
      "168/223, train_loss: 0.1110, step time: 0.1005\n",
      "169/223, train_loss: 0.0956, step time: 0.1067\n",
      "170/223, train_loss: 0.1004, step time: 0.1056\n",
      "171/223, train_loss: 0.0993, step time: 0.0999\n",
      "172/223, train_loss: 0.0953, step time: 0.1006\n",
      "173/223, train_loss: 0.1102, step time: 0.1010\n",
      "174/223, train_loss: 0.0998, step time: 0.1241\n",
      "175/223, train_loss: 0.1042, step time: 0.1144\n",
      "176/223, train_loss: 0.0998, step time: 0.1060\n",
      "177/223, train_loss: 0.0897, step time: 0.1213\n",
      "178/223, train_loss: 0.1109, step time: 0.1234\n",
      "179/223, train_loss: 0.0928, step time: 0.1268\n",
      "180/223, train_loss: 0.0970, step time: 0.1217\n",
      "181/223, train_loss: 0.1022, step time: 0.1088\n",
      "182/223, train_loss: 0.1027, step time: 0.1209\n",
      "183/223, train_loss: 0.0974, step time: 0.1435\n",
      "184/223, train_loss: 0.1162, step time: 0.1110\n",
      "185/223, train_loss: 0.0955, step time: 0.1096\n",
      "186/223, train_loss: 0.1041, step time: 0.1336\n",
      "187/223, train_loss: 0.0956, step time: 0.1246\n",
      "188/223, train_loss: 0.0988, step time: 0.1310\n",
      "189/223, train_loss: 0.1072, step time: 0.0991\n",
      "190/223, train_loss: 0.0969, step time: 0.1096\n",
      "191/223, train_loss: 0.0955, step time: 0.1177\n",
      "192/223, train_loss: 0.1073, step time: 0.1088\n",
      "193/223, train_loss: 0.1071, step time: 0.1365\n",
      "194/223, train_loss: 0.1070, step time: 0.1360\n",
      "195/223, train_loss: 0.0945, step time: 0.1178\n",
      "196/223, train_loss: 0.1154, step time: 0.1635\n",
      "197/223, train_loss: 0.0929, step time: 0.1493\n",
      "198/223, train_loss: 0.1036, step time: 0.1155\n",
      "199/223, train_loss: 0.1030, step time: 0.1101\n",
      "200/223, train_loss: 0.0911, step time: 0.1083\n",
      "201/223, train_loss: 0.1038, step time: 0.1385\n",
      "202/223, train_loss: 0.0962, step time: 0.1327\n",
      "203/223, train_loss: 0.0972, step time: 0.1133\n",
      "204/223, train_loss: 0.1043, step time: 0.1239\n",
      "205/223, train_loss: 0.0926, step time: 0.1557\n",
      "206/223, train_loss: 0.1065, step time: 0.1101\n",
      "207/223, train_loss: 0.1127, step time: 0.1092\n",
      "208/223, train_loss: 0.1013, step time: 0.1178\n",
      "209/223, train_loss: 0.0934, step time: 0.0998\n",
      "210/223, train_loss: 0.0981, step time: 0.1005\n",
      "211/223, train_loss: 0.1060, step time: 0.1003\n",
      "212/223, train_loss: 0.0906, step time: 0.1121\n",
      "213/223, train_loss: 0.0931, step time: 0.0998\n",
      "214/223, train_loss: 0.1119, step time: 0.0997\n",
      "215/223, train_loss: 0.0979, step time: 0.1007\n",
      "216/223, train_loss: 0.1024, step time: 0.1201\n",
      "217/223, train_loss: 0.1002, step time: 0.1005\n",
      "218/223, train_loss: 0.0953, step time: 0.1014\n",
      "219/223, train_loss: 0.0998, step time: 0.0996\n",
      "220/223, train_loss: 0.0926, step time: 0.1025\n",
      "221/223, train_loss: 0.1067, step time: 0.0996\n",
      "222/223, train_loss: 0.1044, step time: 0.0991\n",
      "223/223, train_loss: 0.1043, step time: 0.1005\n",
      "epoch 222 average loss: 0.1012\n",
      "time consuming of epoch 222 is: 88.7081\n",
      "----------\n",
      "epoch 223/300\n",
      "1/223, train_loss: 0.1028, step time: 0.1063\n",
      "2/223, train_loss: 0.1024, step time: 0.1112\n",
      "3/223, train_loss: 0.0884, step time: 0.1024\n",
      "4/223, train_loss: 0.1050, step time: 0.1012\n",
      "5/223, train_loss: 0.1026, step time: 0.1164\n",
      "6/223, train_loss: 0.1083, step time: 0.1133\n",
      "7/223, train_loss: 0.0914, step time: 0.1084\n",
      "8/223, train_loss: 0.0954, step time: 0.1332\n",
      "9/223, train_loss: 0.1026, step time: 0.1120\n",
      "10/223, train_loss: 0.1132, step time: 0.1138\n",
      "11/223, train_loss: 0.1031, step time: 0.1068\n",
      "12/223, train_loss: 0.0997, step time: 0.1037\n",
      "13/223, train_loss: 0.1083, step time: 0.1120\n",
      "14/223, train_loss: 0.1025, step time: 0.1086\n",
      "15/223, train_loss: 0.1028, step time: 0.1007\n",
      "16/223, train_loss: 0.0985, step time: 0.1018\n",
      "17/223, train_loss: 0.1094, step time: 0.1143\n",
      "18/223, train_loss: 0.1083, step time: 0.1137\n",
      "19/223, train_loss: 0.0985, step time: 0.1052\n",
      "20/223, train_loss: 0.0970, step time: 0.1058\n",
      "21/223, train_loss: 0.0997, step time: 0.1106\n",
      "22/223, train_loss: 0.0944, step time: 0.1006\n",
      "23/223, train_loss: 0.1005, step time: 0.1188\n",
      "24/223, train_loss: 0.1065, step time: 0.1356\n",
      "25/223, train_loss: 0.1048, step time: 0.1168\n",
      "26/223, train_loss: 0.0963, step time: 0.1062\n",
      "27/223, train_loss: 0.0982, step time: 0.1002\n",
      "28/223, train_loss: 0.0924, step time: 0.1183\n",
      "29/223, train_loss: 0.0876, step time: 0.1050\n",
      "30/223, train_loss: 0.1077, step time: 0.1061\n",
      "31/223, train_loss: 0.0964, step time: 0.1086\n",
      "32/223, train_loss: 0.0977, step time: 0.1193\n",
      "33/223, train_loss: 0.1044, step time: 0.1127\n",
      "34/223, train_loss: 0.1108, step time: 0.1003\n",
      "35/223, train_loss: 0.0942, step time: 0.1134\n",
      "36/223, train_loss: 0.0989, step time: 0.1477\n",
      "37/223, train_loss: 0.0987, step time: 0.1349\n",
      "38/223, train_loss: 0.0961, step time: 0.1058\n",
      "39/223, train_loss: 0.1088, step time: 0.1077\n",
      "40/223, train_loss: 0.0998, step time: 0.1204\n",
      "41/223, train_loss: 0.1025, step time: 0.1163\n",
      "42/223, train_loss: 0.0980, step time: 0.1327\n",
      "43/223, train_loss: 0.0950, step time: 0.1071\n",
      "44/223, train_loss: 0.1018, step time: 0.1065\n",
      "45/223, train_loss: 0.1013, step time: 0.1149\n",
      "46/223, train_loss: 0.0938, step time: 0.1002\n",
      "47/223, train_loss: 0.0884, step time: 0.1052\n",
      "48/223, train_loss: 0.1020, step time: 0.1297\n",
      "49/223, train_loss: 0.0964, step time: 0.1168\n",
      "50/223, train_loss: 0.1021, step time: 0.1131\n",
      "51/223, train_loss: 0.1061, step time: 0.1085\n",
      "52/223, train_loss: 0.1009, step time: 0.1103\n",
      "53/223, train_loss: 0.0997, step time: 0.1137\n",
      "54/223, train_loss: 0.0962, step time: 0.1016\n",
      "55/223, train_loss: 0.0952, step time: 0.1104\n",
      "56/223, train_loss: 0.1020, step time: 0.1072\n",
      "57/223, train_loss: 0.0968, step time: 0.1207\n",
      "58/223, train_loss: 0.1052, step time: 0.0996\n",
      "59/223, train_loss: 0.0963, step time: 0.1140\n",
      "60/223, train_loss: 0.1023, step time: 0.1262\n",
      "61/223, train_loss: 0.0998, step time: 0.1231\n",
      "62/223, train_loss: 0.1071, step time: 0.1131\n",
      "63/223, train_loss: 0.1029, step time: 0.1637\n",
      "64/223, train_loss: 0.0986, step time: 0.1209\n",
      "65/223, train_loss: 0.1028, step time: 0.1020\n",
      "66/223, train_loss: 0.1084, step time: 0.1058\n",
      "67/223, train_loss: 0.1017, step time: 0.1367\n",
      "68/223, train_loss: 0.0960, step time: 0.1072\n",
      "69/223, train_loss: 0.0875, step time: 0.1186\n",
      "70/223, train_loss: 0.0919, step time: 0.1160\n",
      "71/223, train_loss: 0.0922, step time: 0.1222\n",
      "72/223, train_loss: 0.0962, step time: 0.1104\n",
      "73/223, train_loss: 0.0962, step time: 0.1137\n",
      "74/223, train_loss: 0.1107, step time: 0.0995\n",
      "75/223, train_loss: 0.1097, step time: 0.1020\n",
      "76/223, train_loss: 0.1024, step time: 0.1002\n",
      "77/223, train_loss: 0.0874, step time: 0.1156\n",
      "78/223, train_loss: 0.0972, step time: 0.0992\n",
      "79/223, train_loss: 0.1015, step time: 0.1178\n",
      "80/223, train_loss: 0.1044, step time: 0.1229\n",
      "81/223, train_loss: 0.0998, step time: 0.1156\n",
      "82/223, train_loss: 0.1019, step time: 0.1055\n",
      "83/223, train_loss: 0.1107, step time: 0.1454\n",
      "84/223, train_loss: 0.1102, step time: 0.1181\n",
      "85/223, train_loss: 0.0955, step time: 0.1383\n",
      "86/223, train_loss: 0.1081, step time: 0.1144\n",
      "87/223, train_loss: 0.0967, step time: 0.1081\n",
      "88/223, train_loss: 0.0991, step time: 0.1032\n",
      "89/223, train_loss: 0.0900, step time: 0.1079\n",
      "90/223, train_loss: 0.1041, step time: 0.1066\n",
      "91/223, train_loss: 0.0974, step time: 0.1100\n",
      "92/223, train_loss: 0.1093, step time: 0.1015\n",
      "93/223, train_loss: 0.0909, step time: 0.1093\n",
      "94/223, train_loss: 0.1123, step time: 0.1295\n",
      "95/223, train_loss: 0.1087, step time: 0.1118\n",
      "96/223, train_loss: 0.0938, step time: 0.1556\n",
      "97/223, train_loss: 0.1070, step time: 0.1227\n",
      "98/223, train_loss: 0.1065, step time: 0.1144\n",
      "99/223, train_loss: 0.1024, step time: 0.1006\n",
      "100/223, train_loss: 0.0931, step time: 0.1009\n",
      "101/223, train_loss: 0.0997, step time: 0.1125\n",
      "102/223, train_loss: 0.1044, step time: 0.1118\n",
      "103/223, train_loss: 0.0983, step time: 0.1087\n",
      "104/223, train_loss: 0.1042, step time: 0.1010\n",
      "105/223, train_loss: 0.1020, step time: 0.1058\n",
      "106/223, train_loss: 0.1000, step time: 0.1118\n",
      "107/223, train_loss: 0.0970, step time: 0.1348\n",
      "108/223, train_loss: 0.1030, step time: 0.1150\n",
      "109/223, train_loss: 0.0992, step time: 0.1063\n",
      "110/223, train_loss: 0.1052, step time: 0.1002\n",
      "111/223, train_loss: 0.1001, step time: 0.1303\n",
      "112/223, train_loss: 0.0939, step time: 0.1253\n",
      "113/223, train_loss: 0.1093, step time: 0.1053\n",
      "114/223, train_loss: 0.0987, step time: 0.1270\n",
      "115/223, train_loss: 0.0944, step time: 0.1443\n",
      "116/223, train_loss: 0.1030, step time: 0.1140\n",
      "117/223, train_loss: 0.0963, step time: 0.1005\n",
      "118/223, train_loss: 0.1100, step time: 0.1043\n",
      "119/223, train_loss: 0.0992, step time: 0.1276\n",
      "120/223, train_loss: 0.1011, step time: 0.1354\n",
      "121/223, train_loss: 0.0961, step time: 0.1061\n",
      "122/223, train_loss: 0.0870, step time: 0.1050\n",
      "123/223, train_loss: 0.1139, step time: 0.1186\n",
      "124/223, train_loss: 0.0963, step time: 0.1043\n",
      "125/223, train_loss: 0.1079, step time: 0.1166\n",
      "126/223, train_loss: 0.0933, step time: 0.1047\n",
      "127/223, train_loss: 0.0988, step time: 0.0998\n",
      "128/223, train_loss: 0.0953, step time: 0.1066\n",
      "129/223, train_loss: 0.1071, step time: 0.1026\n",
      "130/223, train_loss: 0.1041, step time: 0.1286\n",
      "131/223, train_loss: 0.0912, step time: 0.1053\n",
      "132/223, train_loss: 0.0959, step time: 0.1158\n",
      "133/223, train_loss: 0.0990, step time: 0.0992\n",
      "134/223, train_loss: 0.1001, step time: 0.1125\n",
      "135/223, train_loss: 0.1082, step time: 0.1180\n",
      "136/223, train_loss: 0.0985, step time: 0.1249\n",
      "137/223, train_loss: 0.0889, step time: 0.1199\n",
      "138/223, train_loss: 0.1150, step time: 0.1118\n",
      "139/223, train_loss: 0.0966, step time: 0.1127\n",
      "140/223, train_loss: 0.1000, step time: 0.1250\n",
      "141/223, train_loss: 0.0962, step time: 0.1086\n",
      "142/223, train_loss: 0.0987, step time: 0.1063\n",
      "143/223, train_loss: 0.1041, step time: 0.1002\n",
      "144/223, train_loss: 0.0950, step time: 0.0998\n",
      "145/223, train_loss: 0.1021, step time: 0.0986\n",
      "146/223, train_loss: 0.1044, step time: 0.1040\n",
      "147/223, train_loss: 0.0933, step time: 0.1051\n",
      "148/223, train_loss: 0.1158, step time: 0.1099\n",
      "149/223, train_loss: 0.1012, step time: 0.1023\n",
      "150/223, train_loss: 0.0931, step time: 0.1135\n",
      "151/223, train_loss: 0.0948, step time: 0.0997\n",
      "152/223, train_loss: 0.0927, step time: 0.0990\n",
      "153/223, train_loss: 0.0991, step time: 0.0988\n",
      "154/223, train_loss: 0.1021, step time: 0.1132\n",
      "155/223, train_loss: 0.1044, step time: 0.1166\n",
      "156/223, train_loss: 0.0901, step time: 0.1084\n",
      "157/223, train_loss: 0.1078, step time: 0.1185\n",
      "158/223, train_loss: 0.1240, step time: 0.1135\n",
      "159/223, train_loss: 0.0946, step time: 0.1042\n",
      "160/223, train_loss: 0.1073, step time: 0.1237\n",
      "161/223, train_loss: 0.1035, step time: 0.1042\n",
      "162/223, train_loss: 0.0960, step time: 0.1175\n",
      "163/223, train_loss: 0.0960, step time: 0.1132\n",
      "164/223, train_loss: 0.0986, step time: 0.1081\n",
      "165/223, train_loss: 0.1146, step time: 0.1183\n",
      "166/223, train_loss: 0.1020, step time: 0.1021\n",
      "167/223, train_loss: 0.0998, step time: 0.0995\n",
      "168/223, train_loss: 0.0980, step time: 0.1116\n",
      "169/223, train_loss: 0.1019, step time: 0.1188\n",
      "170/223, train_loss: 0.1086, step time: 0.1013\n",
      "171/223, train_loss: 0.1059, step time: 0.1059\n",
      "172/223, train_loss: 0.0959, step time: 0.1052\n",
      "173/223, train_loss: 0.1030, step time: 0.1031\n",
      "174/223, train_loss: 0.0938, step time: 0.1131\n",
      "175/223, train_loss: 0.1003, step time: 0.1047\n",
      "176/223, train_loss: 0.1114, step time: 0.1059\n",
      "177/223, train_loss: 0.0999, step time: 0.1040\n",
      "178/223, train_loss: 0.1011, step time: 0.1129\n",
      "179/223, train_loss: 0.1100, step time: 0.1136\n",
      "180/223, train_loss: 0.1049, step time: 0.1094\n",
      "181/223, train_loss: 0.0892, step time: 0.1218\n",
      "182/223, train_loss: 0.1053, step time: 0.1120\n",
      "183/223, train_loss: 0.1020, step time: 0.1286\n",
      "184/223, train_loss: 0.1122, step time: 0.1056\n",
      "185/223, train_loss: 0.0985, step time: 0.1072\n",
      "186/223, train_loss: 0.1052, step time: 0.1219\n",
      "187/223, train_loss: 0.1011, step time: 0.1054\n",
      "188/223, train_loss: 0.1057, step time: 0.1331\n",
      "189/223, train_loss: 0.0969, step time: 0.1171\n",
      "190/223, train_loss: 0.0963, step time: 0.0998\n",
      "191/223, train_loss: 0.0999, step time: 0.1188\n",
      "192/223, train_loss: 0.0949, step time: 0.1063\n",
      "193/223, train_loss: 0.0949, step time: 0.1090\n",
      "194/223, train_loss: 0.0950, step time: 0.1069\n",
      "195/223, train_loss: 0.0918, step time: 0.1275\n",
      "196/223, train_loss: 0.1022, step time: 0.1509\n",
      "197/223, train_loss: 0.0984, step time: 0.1199\n",
      "198/223, train_loss: 0.0985, step time: 0.1063\n",
      "199/223, train_loss: 0.0959, step time: 0.1132\n",
      "200/223, train_loss: 0.0959, step time: 0.1251\n",
      "201/223, train_loss: 0.0898, step time: 0.1194\n",
      "202/223, train_loss: 0.0895, step time: 0.1071\n",
      "203/223, train_loss: 0.1070, step time: 0.1061\n",
      "204/223, train_loss: 0.0991, step time: 0.1121\n",
      "205/223, train_loss: 0.0978, step time: 0.1021\n",
      "206/223, train_loss: 0.0993, step time: 0.1114\n",
      "207/223, train_loss: 0.1022, step time: 0.1002\n",
      "208/223, train_loss: 0.0997, step time: 0.1220\n",
      "209/223, train_loss: 0.1034, step time: 0.1028\n",
      "210/223, train_loss: 0.1050, step time: 0.1005\n",
      "211/223, train_loss: 0.1060, step time: 0.1074\n",
      "212/223, train_loss: 0.2998, step time: 0.1039\n",
      "213/223, train_loss: 0.1058, step time: 0.1057\n",
      "214/223, train_loss: 0.0965, step time: 0.1109\n",
      "215/223, train_loss: 0.1027, step time: 0.1160\n",
      "216/223, train_loss: 0.0929, step time: 0.1014\n",
      "217/223, train_loss: 0.1174, step time: 0.1148\n",
      "218/223, train_loss: 0.0964, step time: 0.1017\n",
      "219/223, train_loss: 0.1015, step time: 0.1058\n",
      "220/223, train_loss: 0.1038, step time: 0.1017\n",
      "221/223, train_loss: 0.0975, step time: 0.0987\n",
      "222/223, train_loss: 0.0859, step time: 0.1004\n",
      "223/223, train_loss: 0.0872, step time: 0.1018\n",
      "epoch 223 average loss: 0.1013\n",
      "time consuming of epoch 223 is: 88.9964\n",
      "----------\n",
      "epoch 224/300\n",
      "1/223, train_loss: 0.0976, step time: 0.1125\n",
      "2/223, train_loss: 0.0927, step time: 0.0987\n",
      "3/223, train_loss: 0.1036, step time: 0.0990\n",
      "4/223, train_loss: 0.1046, step time: 0.0990\n",
      "5/223, train_loss: 0.1216, step time: 0.1125\n",
      "6/223, train_loss: 0.1021, step time: 0.1477\n",
      "7/223, train_loss: 0.0924, step time: 0.0998\n",
      "8/223, train_loss: 0.1053, step time: 0.1021\n",
      "9/223, train_loss: 0.1152, step time: 0.1198\n",
      "10/223, train_loss: 0.1043, step time: 0.1205\n",
      "11/223, train_loss: 0.0925, step time: 0.1118\n",
      "12/223, train_loss: 0.0984, step time: 0.1105\n",
      "13/223, train_loss: 0.0945, step time: 0.1126\n",
      "14/223, train_loss: 0.1022, step time: 0.1035\n",
      "15/223, train_loss: 0.1042, step time: 0.1037\n",
      "16/223, train_loss: 0.1059, step time: 0.0992\n",
      "17/223, train_loss: 0.0885, step time: 0.1059\n",
      "18/223, train_loss: 0.0943, step time: 0.1124\n",
      "19/223, train_loss: 0.1002, step time: 0.1109\n",
      "20/223, train_loss: 0.1083, step time: 0.0996\n",
      "21/223, train_loss: 0.1003, step time: 0.1277\n",
      "22/223, train_loss: 0.1215, step time: 0.1058\n",
      "23/223, train_loss: 0.1027, step time: 0.1108\n",
      "24/223, train_loss: 0.0951, step time: 0.1257\n",
      "25/223, train_loss: 0.0933, step time: 0.1077\n",
      "26/223, train_loss: 0.0943, step time: 0.1251\n",
      "27/223, train_loss: 0.0949, step time: 0.1035\n",
      "28/223, train_loss: 0.0914, step time: 0.1240\n",
      "29/223, train_loss: 0.1006, step time: 0.1132\n",
      "30/223, train_loss: 0.1059, step time: 0.1142\n",
      "31/223, train_loss: 0.0912, step time: 0.1178\n",
      "32/223, train_loss: 0.1042, step time: 0.1039\n",
      "33/223, train_loss: 0.0883, step time: 0.1265\n",
      "34/223, train_loss: 0.0972, step time: 0.1098\n",
      "35/223, train_loss: 0.1068, step time: 0.1065\n",
      "36/223, train_loss: 0.1079, step time: 0.1106\n",
      "37/223, train_loss: 0.1038, step time: 0.1002\n",
      "38/223, train_loss: 0.1071, step time: 0.0999\n",
      "39/223, train_loss: 0.0894, step time: 0.1060\n",
      "40/223, train_loss: 0.1050, step time: 0.1119\n",
      "41/223, train_loss: 0.1001, step time: 0.1207\n",
      "42/223, train_loss: 0.0955, step time: 0.1051\n",
      "43/223, train_loss: 0.0935, step time: 0.1077\n",
      "44/223, train_loss: 0.0987, step time: 0.1001\n",
      "45/223, train_loss: 0.1099, step time: 0.1101\n",
      "46/223, train_loss: 0.0938, step time: 0.1164\n",
      "47/223, train_loss: 0.1013, step time: 0.1189\n",
      "48/223, train_loss: 0.0992, step time: 0.1269\n",
      "49/223, train_loss: 0.1027, step time: 0.1137\n",
      "50/223, train_loss: 0.0920, step time: 0.1210\n",
      "51/223, train_loss: 0.0956, step time: 0.1071\n",
      "52/223, train_loss: 0.1057, step time: 0.1074\n",
      "53/223, train_loss: 0.0969, step time: 0.1237\n",
      "54/223, train_loss: 0.1162, step time: 0.1150\n",
      "55/223, train_loss: 0.0983, step time: 0.1077\n",
      "56/223, train_loss: 0.0933, step time: 0.1072\n",
      "57/223, train_loss: 0.0965, step time: 0.1021\n",
      "58/223, train_loss: 0.0954, step time: 0.1091\n",
      "59/223, train_loss: 0.1001, step time: 0.1327\n",
      "60/223, train_loss: 0.1042, step time: 0.1274\n",
      "61/223, train_loss: 0.0924, step time: 0.1010\n",
      "62/223, train_loss: 0.1066, step time: 0.1003\n",
      "63/223, train_loss: 0.0943, step time: 0.1217\n",
      "64/223, train_loss: 0.0991, step time: 0.1135\n",
      "65/223, train_loss: 0.0976, step time: 0.1276\n",
      "66/223, train_loss: 0.1029, step time: 0.1022\n",
      "67/223, train_loss: 0.0983, step time: 0.1221\n",
      "68/223, train_loss: 0.0986, step time: 0.1003\n",
      "69/223, train_loss: 0.0976, step time: 0.1132\n",
      "70/223, train_loss: 0.1039, step time: 0.1124\n",
      "71/223, train_loss: 0.0925, step time: 0.1500\n",
      "72/223, train_loss: 0.1066, step time: 0.1087\n",
      "73/223, train_loss: 0.0974, step time: 0.1370\n",
      "74/223, train_loss: 0.0998, step time: 0.1034\n",
      "75/223, train_loss: 0.1078, step time: 0.1042\n",
      "76/223, train_loss: 0.0899, step time: 0.1002\n",
      "77/223, train_loss: 0.0997, step time: 0.1050\n",
      "78/223, train_loss: 0.0949, step time: 0.1116\n",
      "79/223, train_loss: 0.1135, step time: 0.1111\n",
      "80/223, train_loss: 0.0984, step time: 0.1002\n",
      "81/223, train_loss: 0.0980, step time: 0.1009\n",
      "82/223, train_loss: 0.0946, step time: 0.1042\n",
      "83/223, train_loss: 0.0966, step time: 0.1001\n",
      "84/223, train_loss: 0.0901, step time: 0.1009\n",
      "85/223, train_loss: 0.0952, step time: 0.1168\n",
      "86/223, train_loss: 0.0946, step time: 0.0995\n",
      "87/223, train_loss: 0.0975, step time: 0.0990\n",
      "88/223, train_loss: 0.0948, step time: 0.1504\n",
      "89/223, train_loss: 0.0999, step time: 0.1033\n",
      "90/223, train_loss: 0.1071, step time: 0.1202\n",
      "91/223, train_loss: 0.1095, step time: 0.1087\n",
      "92/223, train_loss: 0.0927, step time: 0.1048\n",
      "93/223, train_loss: 0.1112, step time: 0.1105\n",
      "94/223, train_loss: 0.1084, step time: 0.1073\n",
      "95/223, train_loss: 0.1005, step time: 0.1111\n",
      "96/223, train_loss: 0.0969, step time: 0.1003\n",
      "97/223, train_loss: 0.1059, step time: 0.1085\n",
      "98/223, train_loss: 0.0944, step time: 0.1081\n",
      "99/223, train_loss: 0.1020, step time: 0.1146\n",
      "100/223, train_loss: 0.0940, step time: 0.1200\n",
      "101/223, train_loss: 0.1073, step time: 0.1051\n",
      "102/223, train_loss: 0.1104, step time: 0.1303\n",
      "103/223, train_loss: 0.0931, step time: 0.1418\n",
      "104/223, train_loss: 0.0996, step time: 0.1019\n",
      "105/223, train_loss: 0.0987, step time: 0.1385\n",
      "106/223, train_loss: 0.0974, step time: 0.1050\n",
      "107/223, train_loss: 0.1015, step time: 0.1216\n",
      "108/223, train_loss: 0.1140, step time: 0.1170\n",
      "109/223, train_loss: 0.1031, step time: 0.1136\n",
      "110/223, train_loss: 0.1059, step time: 0.1056\n",
      "111/223, train_loss: 0.0922, step time: 0.1158\n",
      "112/223, train_loss: 0.0938, step time: 0.1076\n",
      "113/223, train_loss: 0.1048, step time: 0.1474\n",
      "114/223, train_loss: 0.0983, step time: 0.1079\n",
      "115/223, train_loss: 0.0972, step time: 0.1184\n",
      "116/223, train_loss: 0.1111, step time: 0.1034\n",
      "117/223, train_loss: 0.1040, step time: 0.1037\n",
      "118/223, train_loss: 0.0996, step time: 0.1020\n",
      "119/223, train_loss: 0.1004, step time: 0.1155\n",
      "120/223, train_loss: 0.0972, step time: 0.1103\n",
      "121/223, train_loss: 0.1000, step time: 0.1157\n",
      "122/223, train_loss: 0.1088, step time: 0.1193\n",
      "123/223, train_loss: 0.0960, step time: 0.1123\n",
      "124/223, train_loss: 0.1071, step time: 0.1050\n",
      "125/223, train_loss: 0.0939, step time: 0.1098\n",
      "126/223, train_loss: 0.0980, step time: 0.1021\n",
      "127/223, train_loss: 0.1066, step time: 0.1068\n",
      "128/223, train_loss: 0.1000, step time: 0.1123\n",
      "129/223, train_loss: 0.0880, step time: 0.0993\n",
      "130/223, train_loss: 0.1150, step time: 0.1031\n",
      "131/223, train_loss: 0.0937, step time: 0.1310\n",
      "132/223, train_loss: 0.1029, step time: 0.1053\n",
      "133/223, train_loss: 0.1058, step time: 0.1079\n",
      "134/223, train_loss: 0.1057, step time: 0.1194\n",
      "135/223, train_loss: 0.1088, step time: 0.1151\n",
      "136/223, train_loss: 0.0964, step time: 0.1089\n",
      "137/223, train_loss: 0.1082, step time: 0.1219\n",
      "138/223, train_loss: 0.0972, step time: 0.1187\n",
      "139/223, train_loss: 0.0921, step time: 0.1193\n",
      "140/223, train_loss: 0.1131, step time: 0.1096\n",
      "141/223, train_loss: 0.0914, step time: 0.1128\n",
      "142/223, train_loss: 0.0964, step time: 0.1340\n",
      "143/223, train_loss: 0.1006, step time: 0.1095\n",
      "144/223, train_loss: 0.1154, step time: 0.1000\n",
      "145/223, train_loss: 0.0982, step time: 0.1129\n",
      "146/223, train_loss: 0.1007, step time: 0.1067\n",
      "147/223, train_loss: 0.1091, step time: 0.1081\n",
      "148/223, train_loss: 0.0967, step time: 0.1058\n",
      "149/223, train_loss: 0.0990, step time: 0.1131\n",
      "150/223, train_loss: 0.0902, step time: 0.1000\n",
      "151/223, train_loss: 0.0985, step time: 0.1344\n",
      "152/223, train_loss: 0.0949, step time: 0.1265\n",
      "153/223, train_loss: 0.1110, step time: 0.1441\n",
      "154/223, train_loss: 0.0978, step time: 0.1008\n",
      "155/223, train_loss: 0.0999, step time: 0.1245\n",
      "156/223, train_loss: 0.0949, step time: 0.0999\n",
      "157/223, train_loss: 0.0919, step time: 0.1127\n",
      "158/223, train_loss: 0.0894, step time: 0.0993\n",
      "159/223, train_loss: 0.1068, step time: 0.0993\n",
      "160/223, train_loss: 0.0995, step time: 0.0991\n",
      "161/223, train_loss: 0.1011, step time: 0.1000\n",
      "162/223, train_loss: 0.0971, step time: 0.1035\n",
      "163/223, train_loss: 0.0992, step time: 0.1008\n",
      "164/223, train_loss: 0.0951, step time: 0.0997\n",
      "165/223, train_loss: 0.0974, step time: 0.1000\n",
      "166/223, train_loss: 0.0972, step time: 0.1126\n",
      "167/223, train_loss: 0.1086, step time: 0.1015\n",
      "168/223, train_loss: 0.1064, step time: 0.1094\n",
      "169/223, train_loss: 0.2981, step time: 0.1247\n",
      "170/223, train_loss: 0.1018, step time: 0.1264\n",
      "171/223, train_loss: 0.0945, step time: 0.1114\n",
      "172/223, train_loss: 0.0925, step time: 0.1058\n",
      "173/223, train_loss: 0.0977, step time: 0.1001\n",
      "174/223, train_loss: 0.0991, step time: 0.1003\n",
      "175/223, train_loss: 0.1062, step time: 0.1251\n",
      "176/223, train_loss: 0.1069, step time: 0.1371\n",
      "177/223, train_loss: 0.1070, step time: 0.1124\n",
      "178/223, train_loss: 0.0970, step time: 0.1096\n",
      "179/223, train_loss: 0.0968, step time: 0.1118\n",
      "180/223, train_loss: 0.0957, step time: 0.1296\n",
      "181/223, train_loss: 0.1000, step time: 0.1174\n",
      "182/223, train_loss: 0.1031, step time: 0.1107\n",
      "183/223, train_loss: 0.1105, step time: 0.1004\n",
      "184/223, train_loss: 0.1001, step time: 0.1014\n",
      "185/223, train_loss: 0.1166, step time: 0.1047\n",
      "186/223, train_loss: 0.0907, step time: 0.1004\n",
      "187/223, train_loss: 0.1052, step time: 0.1177\n",
      "188/223, train_loss: 0.0987, step time: 0.1252\n",
      "189/223, train_loss: 0.0988, step time: 0.1134\n",
      "190/223, train_loss: 0.0868, step time: 0.1181\n",
      "191/223, train_loss: 0.0890, step time: 0.0999\n",
      "192/223, train_loss: 0.0995, step time: 0.0998\n",
      "193/223, train_loss: 0.0916, step time: 0.1126\n",
      "194/223, train_loss: 0.0935, step time: 0.1426\n",
      "195/223, train_loss: 0.1046, step time: 0.1096\n",
      "196/223, train_loss: 0.1070, step time: 0.1143\n",
      "197/223, train_loss: 0.0975, step time: 0.1178\n",
      "198/223, train_loss: 0.0948, step time: 0.1008\n",
      "199/223, train_loss: 0.1034, step time: 0.1007\n",
      "200/223, train_loss: 0.1114, step time: 0.1007\n",
      "201/223, train_loss: 0.1064, step time: 0.1287\n",
      "202/223, train_loss: 0.0886, step time: 0.1026\n",
      "203/223, train_loss: 0.1006, step time: 0.1028\n",
      "204/223, train_loss: 0.0887, step time: 0.1098\n",
      "205/223, train_loss: 0.0974, step time: 0.1388\n",
      "206/223, train_loss: 0.1074, step time: 0.1511\n",
      "207/223, train_loss: 0.0922, step time: 0.1285\n",
      "208/223, train_loss: 0.1054, step time: 0.0999\n",
      "209/223, train_loss: 0.0996, step time: 0.1103\n",
      "210/223, train_loss: 0.1014, step time: 0.1322\n",
      "211/223, train_loss: 0.1009, step time: 0.1092\n",
      "212/223, train_loss: 0.0980, step time: 0.1171\n",
      "213/223, train_loss: 0.1012, step time: 0.1022\n",
      "214/223, train_loss: 0.1132, step time: 0.1104\n",
      "215/223, train_loss: 0.1017, step time: 0.1048\n",
      "216/223, train_loss: 0.0871, step time: 0.1102\n",
      "217/223, train_loss: 0.1041, step time: 0.1119\n",
      "218/223, train_loss: 0.0890, step time: 0.1005\n",
      "219/223, train_loss: 0.0959, step time: 0.0999\n",
      "220/223, train_loss: 0.1038, step time: 0.1003\n",
      "221/223, train_loss: 0.1152, step time: 0.0987\n",
      "222/223, train_loss: 0.1084, step time: 0.0993\n",
      "223/223, train_loss: 0.1037, step time: 0.0999\n",
      "epoch 224 average loss: 0.1011\n",
      "time consuming of epoch 224 is: 89.3888\n",
      "----------\n",
      "epoch 225/300\n",
      "1/223, train_loss: 0.0959, step time: 0.1013\n",
      "2/223, train_loss: 0.1032, step time: 0.1128\n",
      "3/223, train_loss: 0.1077, step time: 0.1004\n",
      "4/223, train_loss: 0.1020, step time: 0.1012\n",
      "5/223, train_loss: 0.1062, step time: 0.1075\n",
      "6/223, train_loss: 0.0955, step time: 0.1133\n",
      "7/223, train_loss: 0.0931, step time: 0.1064\n",
      "8/223, train_loss: 0.1003, step time: 0.1303\n",
      "9/223, train_loss: 0.1006, step time: 0.1174\n",
      "10/223, train_loss: 0.0996, step time: 0.1356\n",
      "11/223, train_loss: 0.1001, step time: 0.1095\n",
      "12/223, train_loss: 0.1008, step time: 0.1298\n",
      "13/223, train_loss: 0.1000, step time: 0.1002\n",
      "14/223, train_loss: 0.0938, step time: 0.1309\n",
      "15/223, train_loss: 0.1049, step time: 0.1075\n",
      "16/223, train_loss: 0.0916, step time: 0.1013\n",
      "17/223, train_loss: 0.1100, step time: 0.1166\n",
      "18/223, train_loss: 0.0979, step time: 0.1057\n",
      "19/223, train_loss: 0.0941, step time: 0.1049\n",
      "20/223, train_loss: 0.1053, step time: 0.1393\n",
      "21/223, train_loss: 0.1026, step time: 0.1039\n",
      "22/223, train_loss: 0.0893, step time: 0.1174\n",
      "23/223, train_loss: 0.1138, step time: 0.1017\n",
      "24/223, train_loss: 0.1021, step time: 0.1007\n",
      "25/223, train_loss: 0.1079, step time: 0.1092\n",
      "26/223, train_loss: 0.1061, step time: 0.1080\n",
      "27/223, train_loss: 0.0914, step time: 0.1023\n",
      "28/223, train_loss: 0.0974, step time: 0.1105\n",
      "29/223, train_loss: 0.0966, step time: 0.1237\n",
      "30/223, train_loss: 0.1112, step time: 0.1157\n",
      "31/223, train_loss: 0.1059, step time: 0.1012\n",
      "32/223, train_loss: 0.1022, step time: 0.1024\n",
      "33/223, train_loss: 0.0999, step time: 0.1030\n",
      "34/223, train_loss: 0.1031, step time: 0.1291\n",
      "35/223, train_loss: 0.1079, step time: 0.1126\n",
      "36/223, train_loss: 0.1132, step time: 0.1001\n",
      "37/223, train_loss: 0.0946, step time: 0.1011\n",
      "38/223, train_loss: 0.0896, step time: 0.1083\n",
      "39/223, train_loss: 0.0917, step time: 0.1304\n",
      "40/223, train_loss: 0.1154, step time: 0.1049\n",
      "41/223, train_loss: 0.1127, step time: 0.1323\n",
      "42/223, train_loss: 0.1028, step time: 0.1091\n",
      "43/223, train_loss: 0.0997, step time: 0.0998\n",
      "44/223, train_loss: 0.1057, step time: 0.1002\n",
      "45/223, train_loss: 0.0972, step time: 0.1096\n",
      "46/223, train_loss: 0.1010, step time: 0.1212\n",
      "47/223, train_loss: 0.1016, step time: 0.1069\n",
      "48/223, train_loss: 0.1029, step time: 0.1007\n",
      "49/223, train_loss: 0.1098, step time: 0.1053\n",
      "50/223, train_loss: 0.0878, step time: 0.1076\n",
      "51/223, train_loss: 0.0966, step time: 0.1098\n",
      "52/223, train_loss: 0.1038, step time: 0.1018\n",
      "53/223, train_loss: 0.0912, step time: 0.1004\n",
      "54/223, train_loss: 0.1060, step time: 0.1068\n",
      "55/223, train_loss: 0.0933, step time: 0.0996\n",
      "56/223, train_loss: 0.0883, step time: 0.1183\n",
      "57/223, train_loss: 0.1025, step time: 0.1099\n",
      "58/223, train_loss: 0.1043, step time: 0.1011\n",
      "59/223, train_loss: 0.1098, step time: 0.1462\n",
      "60/223, train_loss: 0.1059, step time: 0.1051\n",
      "61/223, train_loss: 0.0921, step time: 0.1272\n",
      "62/223, train_loss: 0.1096, step time: 0.1139\n",
      "63/223, train_loss: 0.0941, step time: 0.1290\n",
      "64/223, train_loss: 0.0966, step time: 0.1050\n",
      "65/223, train_loss: 0.1021, step time: 0.1173\n",
      "66/223, train_loss: 0.1030, step time: 0.1148\n",
      "67/223, train_loss: 0.1029, step time: 0.1190\n",
      "68/223, train_loss: 0.1036, step time: 0.1119\n",
      "69/223, train_loss: 0.0963, step time: 0.1241\n",
      "70/223, train_loss: 0.1138, step time: 0.1064\n",
      "71/223, train_loss: 0.0926, step time: 0.1071\n",
      "72/223, train_loss: 0.1061, step time: 0.1184\n",
      "73/223, train_loss: 0.0967, step time: 0.1024\n",
      "74/223, train_loss: 0.0923, step time: 0.1249\n",
      "75/223, train_loss: 0.0956, step time: 0.1138\n",
      "76/223, train_loss: 0.1012, step time: 0.1307\n",
      "77/223, train_loss: 0.1082, step time: 0.1254\n",
      "78/223, train_loss: 0.0999, step time: 0.1072\n",
      "79/223, train_loss: 0.1053, step time: 0.1229\n",
      "80/223, train_loss: 0.0893, step time: 0.1653\n",
      "81/223, train_loss: 0.0982, step time: 0.1156\n",
      "82/223, train_loss: 0.0992, step time: 0.1072\n",
      "83/223, train_loss: 0.0876, step time: 0.0994\n",
      "84/223, train_loss: 0.0981, step time: 0.0990\n",
      "85/223, train_loss: 0.1041, step time: 0.0989\n",
      "86/223, train_loss: 0.0878, step time: 0.1080\n",
      "87/223, train_loss: 0.1004, step time: 0.1148\n",
      "88/223, train_loss: 0.0984, step time: 0.1053\n",
      "89/223, train_loss: 0.0968, step time: 0.0992\n",
      "90/223, train_loss: 0.0993, step time: 0.1110\n",
      "91/223, train_loss: 0.1001, step time: 0.1103\n",
      "92/223, train_loss: 0.0965, step time: 0.1084\n",
      "93/223, train_loss: 0.0911, step time: 0.1124\n",
      "94/223, train_loss: 0.1006, step time: 0.1115\n",
      "95/223, train_loss: 0.1073, step time: 0.1316\n",
      "96/223, train_loss: 0.1166, step time: 0.1100\n",
      "97/223, train_loss: 0.1099, step time: 0.1194\n",
      "98/223, train_loss: 0.0997, step time: 0.1149\n",
      "99/223, train_loss: 0.1035, step time: 0.1162\n",
      "100/223, train_loss: 0.0982, step time: 0.1329\n",
      "101/223, train_loss: 0.0993, step time: 0.1031\n",
      "102/223, train_loss: 0.0878, step time: 0.0993\n",
      "103/223, train_loss: 0.0953, step time: 0.0997\n",
      "104/223, train_loss: 0.0939, step time: 0.1066\n",
      "105/223, train_loss: 0.0946, step time: 0.1184\n",
      "106/223, train_loss: 0.1012, step time: 0.1149\n",
      "107/223, train_loss: 0.0917, step time: 0.1165\n",
      "108/223, train_loss: 0.0941, step time: 0.1126\n",
      "109/223, train_loss: 0.1004, step time: 0.1225\n",
      "110/223, train_loss: 0.0954, step time: 0.0997\n",
      "111/223, train_loss: 0.1059, step time: 0.1059\n",
      "112/223, train_loss: 0.0941, step time: 0.1264\n",
      "113/223, train_loss: 0.1016, step time: 0.1014\n",
      "114/223, train_loss: 0.1106, step time: 0.1004\n",
      "115/223, train_loss: 0.0863, step time: 0.1025\n",
      "116/223, train_loss: 0.0931, step time: 0.1009\n",
      "117/223, train_loss: 0.1022, step time: 0.1040\n",
      "118/223, train_loss: 0.0972, step time: 0.1003\n",
      "119/223, train_loss: 0.0937, step time: 0.1027\n",
      "120/223, train_loss: 0.0954, step time: 0.1008\n",
      "121/223, train_loss: 0.0935, step time: 0.1557\n",
      "122/223, train_loss: 0.1080, step time: 0.1004\n",
      "123/223, train_loss: 0.1062, step time: 0.1001\n",
      "124/223, train_loss: 0.1104, step time: 0.1000\n",
      "125/223, train_loss: 0.1021, step time: 0.1001\n",
      "126/223, train_loss: 0.0980, step time: 0.1013\n",
      "127/223, train_loss: 0.0937, step time: 0.1090\n",
      "128/223, train_loss: 0.0981, step time: 0.1010\n",
      "129/223, train_loss: 0.3115, step time: 0.1172\n",
      "130/223, train_loss: 0.1046, step time: 0.1074\n",
      "131/223, train_loss: 0.0909, step time: 0.1202\n",
      "132/223, train_loss: 0.1018, step time: 0.0992\n",
      "133/223, train_loss: 0.1027, step time: 0.1128\n",
      "134/223, train_loss: 0.0943, step time: 0.1054\n",
      "135/223, train_loss: 0.0948, step time: 0.1040\n",
      "136/223, train_loss: 0.1104, step time: 0.1413\n",
      "137/223, train_loss: 0.1031, step time: 0.1106\n",
      "138/223, train_loss: 0.1126, step time: 0.1410\n",
      "139/223, train_loss: 0.1053, step time: 0.1004\n",
      "140/223, train_loss: 0.0879, step time: 0.1021\n",
      "141/223, train_loss: 0.1078, step time: 0.1115\n",
      "142/223, train_loss: 0.0997, step time: 0.1034\n",
      "143/223, train_loss: 0.0969, step time: 0.1036\n",
      "144/223, train_loss: 0.1082, step time: 0.1083\n",
      "145/223, train_loss: 0.1069, step time: 0.1463\n",
      "146/223, train_loss: 0.1191, step time: 0.1006\n",
      "147/223, train_loss: 0.0964, step time: 0.1105\n",
      "148/223, train_loss: 0.1013, step time: 0.1014\n",
      "149/223, train_loss: 0.0935, step time: 0.1153\n",
      "150/223, train_loss: 0.0978, step time: 0.1244\n",
      "151/223, train_loss: 0.1089, step time: 0.1131\n",
      "152/223, train_loss: 0.1040, step time: 0.1043\n",
      "153/223, train_loss: 0.0914, step time: 0.0993\n",
      "154/223, train_loss: 0.1040, step time: 0.1055\n",
      "155/223, train_loss: 0.1012, step time: 0.1109\n",
      "156/223, train_loss: 0.0942, step time: 0.1036\n",
      "157/223, train_loss: 0.0948, step time: 0.1060\n",
      "158/223, train_loss: 0.0954, step time: 0.1145\n",
      "159/223, train_loss: 0.0999, step time: 0.1014\n",
      "160/223, train_loss: 0.0984, step time: 0.1004\n",
      "161/223, train_loss: 0.0880, step time: 0.1003\n",
      "162/223, train_loss: 0.1075, step time: 0.1033\n",
      "163/223, train_loss: 0.0950, step time: 0.1097\n",
      "164/223, train_loss: 0.0929, step time: 0.1021\n",
      "165/223, train_loss: 0.1061, step time: 0.1022\n",
      "166/223, train_loss: 0.1005, step time: 0.1144\n",
      "167/223, train_loss: 0.0973, step time: 0.1134\n",
      "168/223, train_loss: 0.1005, step time: 0.1007\n",
      "169/223, train_loss: 0.0914, step time: 0.1209\n",
      "170/223, train_loss: 0.0930, step time: 0.1015\n",
      "171/223, train_loss: 0.1004, step time: 0.1130\n",
      "172/223, train_loss: 0.1030, step time: 0.1038\n",
      "173/223, train_loss: 0.0966, step time: 0.1104\n",
      "174/223, train_loss: 0.1043, step time: 0.1141\n",
      "175/223, train_loss: 0.0917, step time: 0.1295\n",
      "176/223, train_loss: 0.0992, step time: 0.1209\n",
      "177/223, train_loss: 0.0991, step time: 0.1413\n",
      "178/223, train_loss: 0.1029, step time: 0.1027\n",
      "179/223, train_loss: 0.1003, step time: 0.0988\n",
      "180/223, train_loss: 0.0946, step time: 0.0993\n",
      "181/223, train_loss: 0.1084, step time: 0.1088\n",
      "182/223, train_loss: 0.0893, step time: 0.1031\n",
      "183/223, train_loss: 0.1069, step time: 0.1022\n",
      "184/223, train_loss: 0.0898, step time: 0.1056\n",
      "185/223, train_loss: 0.0895, step time: 0.1004\n",
      "186/223, train_loss: 0.0985, step time: 0.1109\n",
      "187/223, train_loss: 0.1038, step time: 0.1116\n",
      "188/223, train_loss: 0.1018, step time: 0.1024\n",
      "189/223, train_loss: 0.1026, step time: 0.1017\n",
      "190/223, train_loss: 0.0994, step time: 0.1097\n",
      "191/223, train_loss: 0.0988, step time: 0.1161\n",
      "192/223, train_loss: 0.0982, step time: 0.1256\n",
      "193/223, train_loss: 0.1080, step time: 0.1190\n",
      "194/223, train_loss: 0.1008, step time: 0.1119\n",
      "195/223, train_loss: 0.1004, step time: 0.1275\n",
      "196/223, train_loss: 0.0885, step time: 0.1249\n",
      "197/223, train_loss: 0.1086, step time: 0.1320\n",
      "198/223, train_loss: 0.1053, step time: 0.1163\n",
      "199/223, train_loss: 0.1034, step time: 0.1213\n",
      "200/223, train_loss: 0.0999, step time: 0.1007\n",
      "201/223, train_loss: 0.0945, step time: 0.1006\n",
      "202/223, train_loss: 0.0915, step time: 0.1009\n",
      "203/223, train_loss: 0.1040, step time: 0.1073\n",
      "204/223, train_loss: 0.1053, step time: 0.0998\n",
      "205/223, train_loss: 0.0966, step time: 0.0994\n",
      "206/223, train_loss: 0.1078, step time: 0.1005\n",
      "207/223, train_loss: 0.1030, step time: 0.1205\n",
      "208/223, train_loss: 0.1048, step time: 0.0999\n",
      "209/223, train_loss: 0.1020, step time: 0.0999\n",
      "210/223, train_loss: 0.0954, step time: 0.1003\n",
      "211/223, train_loss: 0.1090, step time: 0.1050\n",
      "212/223, train_loss: 0.0994, step time: 0.1035\n",
      "213/223, train_loss: 0.1042, step time: 0.1082\n",
      "214/223, train_loss: 0.1024, step time: 0.1160\n",
      "215/223, train_loss: 0.0969, step time: 0.1263\n",
      "216/223, train_loss: 0.1001, step time: 0.1010\n",
      "217/223, train_loss: 0.1091, step time: 0.1212\n",
      "218/223, train_loss: 0.1012, step time: 0.1144\n",
      "219/223, train_loss: 0.0943, step time: 0.1157\n",
      "220/223, train_loss: 0.0971, step time: 0.1006\n",
      "221/223, train_loss: 0.1001, step time: 0.0987\n",
      "222/223, train_loss: 0.1007, step time: 0.0992\n",
      "223/223, train_loss: 0.1057, step time: 0.0994\n",
      "epoch 225 average loss: 0.1011\n",
      "saved new best metric model\n",
      "current epoch: 225 current mean dice: 0.8615 tc: 0.9223 wt: 0.8712 et: 0.7909\n",
      "best mean dice: 0.8615 at epoch: 225\n",
      "time consuming of epoch 225 is: 100.1257\n",
      "----------\n",
      "epoch 226/300\n",
      "1/223, train_loss: 0.0958, step time: 0.1022\n",
      "2/223, train_loss: 0.1007, step time: 0.1001\n",
      "3/223, train_loss: 0.1016, step time: 0.1161\n",
      "4/223, train_loss: 0.1072, step time: 0.1177\n",
      "5/223, train_loss: 0.0967, step time: 0.1101\n",
      "6/223, train_loss: 0.0926, step time: 0.1052\n",
      "7/223, train_loss: 0.0858, step time: 0.1553\n",
      "8/223, train_loss: 0.0969, step time: 0.1055\n",
      "9/223, train_loss: 0.0928, step time: 0.1112\n",
      "10/223, train_loss: 0.0954, step time: 0.1029\n",
      "11/223, train_loss: 0.0935, step time: 0.1120\n",
      "12/223, train_loss: 0.0981, step time: 0.1013\n",
      "13/223, train_loss: 0.0971, step time: 0.1277\n",
      "14/223, train_loss: 0.0970, step time: 0.1039\n",
      "15/223, train_loss: 0.1100, step time: 0.1004\n",
      "16/223, train_loss: 0.1057, step time: 0.1073\n",
      "17/223, train_loss: 0.0997, step time: 0.1147\n",
      "18/223, train_loss: 0.1072, step time: 0.1153\n",
      "19/223, train_loss: 0.1086, step time: 0.1172\n",
      "20/223, train_loss: 0.0984, step time: 0.1004\n",
      "21/223, train_loss: 0.1101, step time: 0.1142\n",
      "22/223, train_loss: 0.0923, step time: 0.1000\n",
      "23/223, train_loss: 0.0976, step time: 0.1187\n",
      "24/223, train_loss: 0.0964, step time: 0.1017\n",
      "25/223, train_loss: 0.1096, step time: 0.1161\n",
      "26/223, train_loss: 0.1041, step time: 0.1080\n",
      "27/223, train_loss: 0.0898, step time: 0.1005\n",
      "28/223, train_loss: 0.0932, step time: 0.1003\n",
      "29/223, train_loss: 0.0995, step time: 0.1032\n",
      "30/223, train_loss: 0.0958, step time: 0.1334\n",
      "31/223, train_loss: 0.0994, step time: 0.1012\n",
      "32/223, train_loss: 0.0952, step time: 0.1074\n",
      "33/223, train_loss: 0.0942, step time: 0.1006\n",
      "34/223, train_loss: 0.1019, step time: 0.0996\n",
      "35/223, train_loss: 0.1002, step time: 0.1000\n",
      "36/223, train_loss: 0.0981, step time: 0.1123\n",
      "37/223, train_loss: 0.0906, step time: 0.1074\n",
      "38/223, train_loss: 0.1005, step time: 0.0999\n",
      "39/223, train_loss: 0.0904, step time: 0.1047\n",
      "40/223, train_loss: 0.1008, step time: 0.1239\n",
      "41/223, train_loss: 0.0912, step time: 0.1114\n",
      "42/223, train_loss: 0.0978, step time: 0.1143\n",
      "43/223, train_loss: 0.1062, step time: 0.1310\n",
      "44/223, train_loss: 0.0974, step time: 0.1191\n",
      "45/223, train_loss: 0.0995, step time: 0.1051\n",
      "46/223, train_loss: 0.1043, step time: 0.1061\n",
      "47/223, train_loss: 0.0971, step time: 0.0998\n",
      "48/223, train_loss: 0.0965, step time: 0.1099\n",
      "49/223, train_loss: 0.0901, step time: 0.1160\n",
      "50/223, train_loss: 0.0929, step time: 0.1003\n",
      "51/223, train_loss: 0.0906, step time: 0.1006\n",
      "52/223, train_loss: 0.0961, step time: 0.1345\n",
      "53/223, train_loss: 0.1180, step time: 0.1054\n",
      "54/223, train_loss: 0.1002, step time: 0.1005\n",
      "55/223, train_loss: 0.1032, step time: 0.1056\n",
      "56/223, train_loss: 0.0984, step time: 0.1019\n",
      "57/223, train_loss: 0.1025, step time: 0.1134\n",
      "58/223, train_loss: 0.0970, step time: 0.1076\n",
      "59/223, train_loss: 0.1070, step time: 0.1003\n",
      "60/223, train_loss: 0.0980, step time: 0.1075\n",
      "61/223, train_loss: 0.1012, step time: 0.1009\n",
      "62/223, train_loss: 0.0942, step time: 0.1223\n",
      "63/223, train_loss: 0.1005, step time: 0.1053\n",
      "64/223, train_loss: 0.0973, step time: 0.1006\n",
      "65/223, train_loss: 0.1010, step time: 0.1046\n",
      "66/223, train_loss: 0.0921, step time: 0.1092\n",
      "67/223, train_loss: 0.0937, step time: 0.1195\n",
      "68/223, train_loss: 0.1045, step time: 0.1168\n",
      "69/223, train_loss: 0.3075, step time: 0.1024\n",
      "70/223, train_loss: 0.0877, step time: 0.1213\n",
      "71/223, train_loss: 0.1044, step time: 0.1086\n",
      "72/223, train_loss: 0.1054, step time: 0.1001\n",
      "73/223, train_loss: 0.1033, step time: 0.1003\n",
      "74/223, train_loss: 0.1004, step time: 0.1029\n",
      "75/223, train_loss: 0.0989, step time: 0.1237\n",
      "76/223, train_loss: 0.1042, step time: 0.1007\n",
      "77/223, train_loss: 0.1009, step time: 0.1057\n",
      "78/223, train_loss: 0.0943, step time: 0.1042\n",
      "79/223, train_loss: 0.0950, step time: 0.0991\n",
      "80/223, train_loss: 0.0942, step time: 0.0988\n",
      "81/223, train_loss: 0.0892, step time: 0.1133\n",
      "82/223, train_loss: 0.1079, step time: 0.1197\n",
      "83/223, train_loss: 0.1081, step time: 0.1214\n",
      "84/223, train_loss: 0.1021, step time: 0.1125\n",
      "85/223, train_loss: 0.1071, step time: 0.1003\n",
      "86/223, train_loss: 0.0926, step time: 0.1226\n",
      "87/223, train_loss: 0.0920, step time: 0.1320\n",
      "88/223, train_loss: 0.0921, step time: 0.1029\n",
      "89/223, train_loss: 0.0986, step time: 0.1124\n",
      "90/223, train_loss: 0.1203, step time: 0.1078\n",
      "91/223, train_loss: 0.0979, step time: 0.1031\n",
      "92/223, train_loss: 0.0875, step time: 0.1055\n",
      "93/223, train_loss: 0.1061, step time: 0.1153\n",
      "94/223, train_loss: 0.1043, step time: 0.1088\n",
      "95/223, train_loss: 0.1054, step time: 0.1425\n",
      "96/223, train_loss: 0.0954, step time: 0.1003\n",
      "97/223, train_loss: 0.1069, step time: 0.1201\n",
      "98/223, train_loss: 0.1164, step time: 0.1045\n",
      "99/223, train_loss: 0.0976, step time: 0.1541\n",
      "100/223, train_loss: 0.0903, step time: 0.1131\n",
      "101/223, train_loss: 0.0980, step time: 0.1003\n",
      "102/223, train_loss: 0.1119, step time: 0.1163\n",
      "103/223, train_loss: 0.1041, step time: 0.1409\n",
      "104/223, train_loss: 0.1001, step time: 0.1186\n",
      "105/223, train_loss: 0.0940, step time: 0.1147\n",
      "106/223, train_loss: 0.0988, step time: 0.1103\n",
      "107/223, train_loss: 0.1078, step time: 0.1119\n",
      "108/223, train_loss: 0.0975, step time: 0.1007\n",
      "109/223, train_loss: 0.1006, step time: 0.1092\n",
      "110/223, train_loss: 0.0904, step time: 0.0999\n",
      "111/223, train_loss: 0.1047, step time: 0.1211\n",
      "112/223, train_loss: 0.0946, step time: 0.1001\n",
      "113/223, train_loss: 0.1094, step time: 0.1069\n",
      "114/223, train_loss: 0.1048, step time: 0.1237\n",
      "115/223, train_loss: 0.0905, step time: 0.1240\n",
      "116/223, train_loss: 0.1013, step time: 0.1066\n",
      "117/223, train_loss: 0.0911, step time: 0.1352\n",
      "118/223, train_loss: 0.1055, step time: 0.1047\n",
      "119/223, train_loss: 0.1011, step time: 0.1019\n",
      "120/223, train_loss: 0.0864, step time: 0.1006\n",
      "121/223, train_loss: 0.1070, step time: 0.1195\n",
      "122/223, train_loss: 0.1159, step time: 0.1005\n",
      "123/223, train_loss: 0.1028, step time: 0.1157\n",
      "124/223, train_loss: 0.1032, step time: 0.1004\n",
      "125/223, train_loss: 0.0924, step time: 0.1074\n",
      "126/223, train_loss: 0.0995, step time: 0.1177\n",
      "127/223, train_loss: 0.1112, step time: 0.1020\n",
      "128/223, train_loss: 0.0920, step time: 0.1020\n",
      "129/223, train_loss: 0.0976, step time: 0.1523\n",
      "130/223, train_loss: 0.0948, step time: 0.1166\n",
      "131/223, train_loss: 0.1056, step time: 0.1004\n",
      "132/223, train_loss: 0.0971, step time: 0.1134\n",
      "133/223, train_loss: 0.0934, step time: 0.1233\n",
      "134/223, train_loss: 0.0921, step time: 0.1462\n",
      "135/223, train_loss: 0.0935, step time: 0.1130\n",
      "136/223, train_loss: 0.0942, step time: 0.1000\n",
      "137/223, train_loss: 0.1028, step time: 0.1111\n",
      "138/223, train_loss: 0.1044, step time: 0.1230\n",
      "139/223, train_loss: 0.1064, step time: 0.1288\n",
      "140/223, train_loss: 0.0988, step time: 0.1084\n",
      "141/223, train_loss: 0.1031, step time: 0.1124\n",
      "142/223, train_loss: 0.1013, step time: 0.1154\n",
      "143/223, train_loss: 0.1097, step time: 0.1029\n",
      "144/223, train_loss: 0.1094, step time: 0.1343\n",
      "145/223, train_loss: 0.1112, step time: 0.1150\n",
      "146/223, train_loss: 0.1140, step time: 0.1033\n",
      "147/223, train_loss: 0.1129, step time: 0.1059\n",
      "148/223, train_loss: 0.1055, step time: 0.1108\n",
      "149/223, train_loss: 0.0937, step time: 0.1263\n",
      "150/223, train_loss: 0.0944, step time: 0.1117\n",
      "151/223, train_loss: 0.0970, step time: 0.1563\n",
      "152/223, train_loss: 0.0937, step time: 0.1281\n",
      "153/223, train_loss: 0.0991, step time: 0.1199\n",
      "154/223, train_loss: 0.0907, step time: 0.1170\n",
      "155/223, train_loss: 0.1050, step time: 0.1145\n",
      "156/223, train_loss: 0.1054, step time: 0.1021\n",
      "157/223, train_loss: 0.0928, step time: 0.1005\n",
      "158/223, train_loss: 0.1144, step time: 0.1006\n",
      "159/223, train_loss: 0.0969, step time: 0.1247\n",
      "160/223, train_loss: 0.0986, step time: 0.1009\n",
      "161/223, train_loss: 0.0936, step time: 0.1006\n",
      "162/223, train_loss: 0.0978, step time: 0.1007\n",
      "163/223, train_loss: 0.1080, step time: 0.1000\n",
      "164/223, train_loss: 0.1215, step time: 0.1004\n",
      "165/223, train_loss: 0.1019, step time: 0.1249\n",
      "166/223, train_loss: 0.1059, step time: 0.1463\n",
      "167/223, train_loss: 0.0989, step time: 0.1149\n",
      "168/223, train_loss: 0.0954, step time: 0.1004\n",
      "169/223, train_loss: 0.0953, step time: 0.1073\n",
      "170/223, train_loss: 0.0965, step time: 0.1022\n",
      "171/223, train_loss: 0.1081, step time: 0.1128\n",
      "172/223, train_loss: 0.0941, step time: 0.1028\n",
      "173/223, train_loss: 0.0915, step time: 0.1004\n",
      "174/223, train_loss: 0.0971, step time: 0.1000\n",
      "175/223, train_loss: 0.0971, step time: 0.1001\n",
      "176/223, train_loss: 0.0941, step time: 0.1045\n",
      "177/223, train_loss: 0.0994, step time: 0.0995\n",
      "178/223, train_loss: 0.1188, step time: 0.1324\n",
      "179/223, train_loss: 0.0962, step time: 0.1043\n",
      "180/223, train_loss: 0.0982, step time: 0.1226\n",
      "181/223, train_loss: 0.0979, step time: 0.1131\n",
      "182/223, train_loss: 0.1048, step time: 0.1286\n",
      "183/223, train_loss: 0.0932, step time: 0.1275\n",
      "184/223, train_loss: 0.1118, step time: 0.1181\n",
      "185/223, train_loss: 0.0970, step time: 0.1194\n",
      "186/223, train_loss: 0.0991, step time: 0.1271\n",
      "187/223, train_loss: 0.0982, step time: 0.1051\n",
      "188/223, train_loss: 0.1066, step time: 0.1256\n",
      "189/223, train_loss: 0.0994, step time: 0.1066\n",
      "190/223, train_loss: 0.1028, step time: 0.1009\n",
      "191/223, train_loss: 0.0959, step time: 0.1015\n",
      "192/223, train_loss: 0.0968, step time: 0.1010\n",
      "193/223, train_loss: 0.0946, step time: 0.0998\n",
      "194/223, train_loss: 0.0953, step time: 0.0999\n",
      "195/223, train_loss: 0.0904, step time: 0.0999\n",
      "196/223, train_loss: 0.1021, step time: 0.1085\n",
      "197/223, train_loss: 0.1049, step time: 0.1058\n",
      "198/223, train_loss: 0.0926, step time: 0.1096\n",
      "199/223, train_loss: 0.1068, step time: 0.1630\n",
      "200/223, train_loss: 0.0914, step time: 0.1298\n",
      "201/223, train_loss: 0.1147, step time: 0.1318\n",
      "202/223, train_loss: 0.0998, step time: 0.1082\n",
      "203/223, train_loss: 0.0831, step time: 0.1059\n",
      "204/223, train_loss: 0.0955, step time: 0.1048\n",
      "205/223, train_loss: 0.0912, step time: 0.1161\n",
      "206/223, train_loss: 0.1024, step time: 0.1207\n",
      "207/223, train_loss: 0.1039, step time: 0.1199\n",
      "208/223, train_loss: 0.1064, step time: 0.1059\n",
      "209/223, train_loss: 0.0966, step time: 0.0998\n",
      "210/223, train_loss: 0.0976, step time: 0.1004\n",
      "211/223, train_loss: 0.1023, step time: 0.1199\n",
      "212/223, train_loss: 0.0948, step time: 0.1187\n",
      "213/223, train_loss: 0.1035, step time: 0.0999\n",
      "214/223, train_loss: 0.1071, step time: 0.1324\n",
      "215/223, train_loss: 0.1047, step time: 0.1475\n",
      "216/223, train_loss: 0.1037, step time: 0.1122\n",
      "217/223, train_loss: 0.0984, step time: 0.0999\n",
      "218/223, train_loss: 0.1194, step time: 0.1149\n",
      "219/223, train_loss: 0.1022, step time: 0.1005\n",
      "220/223, train_loss: 0.1039, step time: 0.0994\n",
      "221/223, train_loss: 0.1040, step time: 0.0997\n",
      "222/223, train_loss: 0.1010, step time: 0.1000\n",
      "223/223, train_loss: 0.1032, step time: 0.0997\n",
      "epoch 226 average loss: 0.1009\n",
      "time consuming of epoch 226 is: 88.3670\n",
      "----------\n",
      "epoch 227/300\n",
      "1/223, train_loss: 0.0942, step time: 0.1016\n",
      "2/223, train_loss: 0.1007, step time: 0.1131\n",
      "3/223, train_loss: 0.1002, step time: 0.1115\n",
      "4/223, train_loss: 0.0945, step time: 0.1011\n",
      "5/223, train_loss: 0.1031, step time: 0.1148\n",
      "6/223, train_loss: 0.0983, step time: 0.1118\n",
      "7/223, train_loss: 0.0894, step time: 0.1392\n",
      "8/223, train_loss: 0.1103, step time: 0.1390\n",
      "9/223, train_loss: 0.0989, step time: 0.1085\n",
      "10/223, train_loss: 0.0993, step time: 0.1044\n",
      "11/223, train_loss: 0.0968, step time: 0.1147\n",
      "12/223, train_loss: 0.1053, step time: 0.1047\n",
      "13/223, train_loss: 0.0980, step time: 0.1055\n",
      "14/223, train_loss: 0.0963, step time: 0.1482\n",
      "15/223, train_loss: 0.1012, step time: 0.1282\n",
      "16/223, train_loss: 0.1011, step time: 0.1045\n",
      "17/223, train_loss: 0.0998, step time: 0.1012\n",
      "18/223, train_loss: 0.1029, step time: 0.1130\n",
      "19/223, train_loss: 0.0958, step time: 0.1097\n",
      "20/223, train_loss: 0.0930, step time: 0.1095\n",
      "21/223, train_loss: 0.1035, step time: 0.1086\n",
      "22/223, train_loss: 0.1135, step time: 0.1112\n",
      "23/223, train_loss: 0.1024, step time: 0.1215\n",
      "24/223, train_loss: 0.1063, step time: 0.1112\n",
      "25/223, train_loss: 0.0950, step time: 0.1062\n",
      "26/223, train_loss: 0.0914, step time: 0.1001\n",
      "27/223, train_loss: 0.0959, step time: 0.1053\n",
      "28/223, train_loss: 0.1037, step time: 0.1200\n",
      "29/223, train_loss: 0.3031, step time: 0.1035\n",
      "30/223, train_loss: 0.1086, step time: 0.1108\n",
      "31/223, train_loss: 0.0951, step time: 0.1003\n",
      "32/223, train_loss: 0.1060, step time: 0.1282\n",
      "33/223, train_loss: 0.0939, step time: 0.1069\n",
      "34/223, train_loss: 0.1022, step time: 0.1035\n",
      "35/223, train_loss: 0.1036, step time: 0.1176\n",
      "36/223, train_loss: 0.0975, step time: 0.1048\n",
      "37/223, train_loss: 0.0926, step time: 0.1103\n",
      "38/223, train_loss: 0.0951, step time: 0.1005\n",
      "39/223, train_loss: 0.1012, step time: 0.1157\n",
      "40/223, train_loss: 0.1067, step time: 0.1191\n",
      "41/223, train_loss: 0.0873, step time: 0.1160\n",
      "42/223, train_loss: 0.0908, step time: 0.1076\n",
      "43/223, train_loss: 0.1100, step time: 0.1146\n",
      "44/223, train_loss: 0.0939, step time: 0.1031\n",
      "45/223, train_loss: 0.0965, step time: 0.1153\n",
      "46/223, train_loss: 0.1090, step time: 0.1077\n",
      "47/223, train_loss: 0.1059, step time: 0.1217\n",
      "48/223, train_loss: 0.1006, step time: 0.1185\n",
      "49/223, train_loss: 0.1086, step time: 0.1142\n",
      "50/223, train_loss: 0.1054, step time: 0.1218\n",
      "51/223, train_loss: 0.0955, step time: 0.1201\n",
      "52/223, train_loss: 0.0958, step time: 0.0999\n",
      "53/223, train_loss: 0.1051, step time: 0.1134\n",
      "54/223, train_loss: 0.0950, step time: 0.1036\n",
      "55/223, train_loss: 0.1101, step time: 0.1196\n",
      "56/223, train_loss: 0.0982, step time: 0.1075\n",
      "57/223, train_loss: 0.0933, step time: 0.1044\n",
      "58/223, train_loss: 0.1000, step time: 0.1095\n",
      "59/223, train_loss: 0.1014, step time: 0.1272\n",
      "60/223, train_loss: 0.0946, step time: 0.1069\n",
      "61/223, train_loss: 0.1091, step time: 0.1209\n",
      "62/223, train_loss: 0.1011, step time: 0.1097\n",
      "63/223, train_loss: 0.0941, step time: 0.1060\n",
      "64/223, train_loss: 0.1054, step time: 0.1043\n",
      "65/223, train_loss: 0.0957, step time: 0.1086\n",
      "66/223, train_loss: 0.1081, step time: 0.1084\n",
      "67/223, train_loss: 0.1082, step time: 0.1096\n",
      "68/223, train_loss: 0.0975, step time: 0.1401\n",
      "69/223, train_loss: 0.1007, step time: 0.1008\n",
      "70/223, train_loss: 0.1025, step time: 0.1008\n",
      "71/223, train_loss: 0.1094, step time: 0.0984\n",
      "72/223, train_loss: 0.0964, step time: 0.1027\n",
      "73/223, train_loss: 0.0999, step time: 0.1161\n",
      "74/223, train_loss: 0.1079, step time: 0.1114\n",
      "75/223, train_loss: 0.0898, step time: 0.1073\n",
      "76/223, train_loss: 0.0942, step time: 0.1208\n",
      "77/223, train_loss: 0.1021, step time: 0.1052\n",
      "78/223, train_loss: 0.1095, step time: 0.1164\n",
      "79/223, train_loss: 0.1000, step time: 0.1267\n",
      "80/223, train_loss: 0.0931, step time: 0.1037\n",
      "81/223, train_loss: 0.0902, step time: 0.1107\n",
      "82/223, train_loss: 0.0992, step time: 0.1092\n",
      "83/223, train_loss: 0.0966, step time: 0.1034\n",
      "84/223, train_loss: 0.0882, step time: 0.1102\n",
      "85/223, train_loss: 0.1045, step time: 0.1050\n",
      "86/223, train_loss: 0.0989, step time: 0.1018\n",
      "87/223, train_loss: 0.0914, step time: 0.1035\n",
      "88/223, train_loss: 0.1018, step time: 0.1048\n",
      "89/223, train_loss: 0.1212, step time: 0.1003\n",
      "90/223, train_loss: 0.1030, step time: 0.1001\n",
      "91/223, train_loss: 0.1049, step time: 0.1086\n",
      "92/223, train_loss: 0.0897, step time: 0.1011\n",
      "93/223, train_loss: 0.0942, step time: 0.1196\n",
      "94/223, train_loss: 0.0993, step time: 0.0998\n",
      "95/223, train_loss: 0.1138, step time: 0.1004\n",
      "96/223, train_loss: 0.0997, step time: 0.1037\n",
      "97/223, train_loss: 0.0914, step time: 0.1029\n",
      "98/223, train_loss: 0.0924, step time: 0.1195\n",
      "99/223, train_loss: 0.0976, step time: 0.1052\n",
      "100/223, train_loss: 0.0906, step time: 0.1483\n",
      "101/223, train_loss: 0.1066, step time: 0.1095\n",
      "102/223, train_loss: 0.0958, step time: 0.1184\n",
      "103/223, train_loss: 0.0994, step time: 0.1031\n",
      "104/223, train_loss: 0.0947, step time: 0.1168\n",
      "105/223, train_loss: 0.1082, step time: 0.1198\n",
      "106/223, train_loss: 0.1122, step time: 0.1063\n",
      "107/223, train_loss: 0.1031, step time: 0.1101\n",
      "108/223, train_loss: 0.0902, step time: 0.1024\n",
      "109/223, train_loss: 0.1092, step time: 0.1061\n",
      "110/223, train_loss: 0.1035, step time: 0.1220\n",
      "111/223, train_loss: 0.0933, step time: 0.1316\n",
      "112/223, train_loss: 0.0893, step time: 0.1090\n",
      "113/223, train_loss: 0.1027, step time: 0.1154\n",
      "114/223, train_loss: 0.0974, step time: 0.1333\n",
      "115/223, train_loss: 0.0935, step time: 0.1257\n",
      "116/223, train_loss: 0.0972, step time: 0.1163\n",
      "117/223, train_loss: 0.0960, step time: 0.1152\n",
      "118/223, train_loss: 0.1115, step time: 0.1012\n",
      "119/223, train_loss: 0.0953, step time: 0.1013\n",
      "120/223, train_loss: 0.0926, step time: 0.1110\n",
      "121/223, train_loss: 0.1257, step time: 0.1128\n",
      "122/223, train_loss: 0.0915, step time: 0.1201\n",
      "123/223, train_loss: 0.1122, step time: 0.1414\n",
      "124/223, train_loss: 0.0979, step time: 0.1097\n",
      "125/223, train_loss: 0.0879, step time: 0.0996\n",
      "126/223, train_loss: 0.0896, step time: 0.1001\n",
      "127/223, train_loss: 0.0882, step time: 0.1004\n",
      "128/223, train_loss: 0.0935, step time: 0.1219\n",
      "129/223, train_loss: 0.0977, step time: 0.1001\n",
      "130/223, train_loss: 0.1114, step time: 0.1111\n",
      "131/223, train_loss: 0.0875, step time: 0.1015\n",
      "132/223, train_loss: 0.1003, step time: 0.1007\n",
      "133/223, train_loss: 0.1101, step time: 0.1093\n",
      "134/223, train_loss: 0.0963, step time: 0.1114\n",
      "135/223, train_loss: 0.1035, step time: 0.1018\n",
      "136/223, train_loss: 0.0873, step time: 0.1041\n",
      "137/223, train_loss: 0.1034, step time: 0.1044\n",
      "138/223, train_loss: 0.1053, step time: 0.1080\n",
      "139/223, train_loss: 0.0994, step time: 0.1115\n",
      "140/223, train_loss: 0.0959, step time: 0.1020\n",
      "141/223, train_loss: 0.1030, step time: 0.1154\n",
      "142/223, train_loss: 0.0982, step time: 0.0995\n",
      "143/223, train_loss: 0.0976, step time: 0.0996\n",
      "144/223, train_loss: 0.1051, step time: 0.1148\n",
      "145/223, train_loss: 0.0994, step time: 0.1056\n",
      "146/223, train_loss: 0.0926, step time: 0.1040\n",
      "147/223, train_loss: 0.0941, step time: 0.1172\n",
      "148/223, train_loss: 0.0987, step time: 0.1012\n",
      "149/223, train_loss: 0.0991, step time: 0.1004\n",
      "150/223, train_loss: 0.1133, step time: 0.1109\n",
      "151/223, train_loss: 0.1014, step time: 0.1295\n",
      "152/223, train_loss: 0.0945, step time: 0.1012\n",
      "153/223, train_loss: 0.1091, step time: 0.1008\n",
      "154/223, train_loss: 0.1059, step time: 0.1020\n",
      "155/223, train_loss: 0.1138, step time: 0.1012\n",
      "156/223, train_loss: 0.0920, step time: 0.1005\n",
      "157/223, train_loss: 0.1031, step time: 0.1534\n",
      "158/223, train_loss: 0.0997, step time: 0.1438\n",
      "159/223, train_loss: 0.1001, step time: 0.1166\n",
      "160/223, train_loss: 0.1003, step time: 0.1011\n",
      "161/223, train_loss: 0.1026, step time: 0.1011\n",
      "162/223, train_loss: 0.1186, step time: 0.1061\n",
      "163/223, train_loss: 0.0956, step time: 0.1008\n",
      "164/223, train_loss: 0.0958, step time: 0.1003\n",
      "165/223, train_loss: 0.1019, step time: 0.1114\n",
      "166/223, train_loss: 0.1018, step time: 0.1009\n",
      "167/223, train_loss: 0.1108, step time: 0.1114\n",
      "168/223, train_loss: 0.0963, step time: 0.1103\n",
      "169/223, train_loss: 0.0959, step time: 0.1069\n",
      "170/223, train_loss: 0.0935, step time: 0.1007\n",
      "171/223, train_loss: 0.1056, step time: 0.1127\n",
      "172/223, train_loss: 0.1013, step time: 0.1048\n",
      "173/223, train_loss: 0.0917, step time: 0.1029\n",
      "174/223, train_loss: 0.0960, step time: 0.1227\n",
      "175/223, train_loss: 0.1025, step time: 0.1009\n",
      "176/223, train_loss: 0.0934, step time: 0.1043\n",
      "177/223, train_loss: 0.1175, step time: 0.1143\n",
      "178/223, train_loss: 0.1022, step time: 0.1624\n",
      "179/223, train_loss: 0.0995, step time: 0.1037\n",
      "180/223, train_loss: 0.1035, step time: 0.1006\n",
      "181/223, train_loss: 0.1050, step time: 0.1265\n",
      "182/223, train_loss: 0.1056, step time: 0.1088\n",
      "183/223, train_loss: 0.1143, step time: 0.1010\n",
      "184/223, train_loss: 0.0995, step time: 0.1291\n",
      "185/223, train_loss: 0.0961, step time: 0.1606\n",
      "186/223, train_loss: 0.0894, step time: 0.0998\n",
      "187/223, train_loss: 0.0938, step time: 0.1140\n",
      "188/223, train_loss: 0.1038, step time: 0.1058\n",
      "189/223, train_loss: 0.0930, step time: 0.1046\n",
      "190/223, train_loss: 0.1092, step time: 0.1138\n",
      "191/223, train_loss: 0.1072, step time: 0.1031\n",
      "192/223, train_loss: 0.1008, step time: 0.1123\n",
      "193/223, train_loss: 0.0936, step time: 0.1138\n",
      "194/223, train_loss: 0.1045, step time: 0.1040\n",
      "195/223, train_loss: 0.1097, step time: 0.1420\n",
      "196/223, train_loss: 0.0990, step time: 0.1142\n",
      "197/223, train_loss: 0.0899, step time: 0.1046\n",
      "198/223, train_loss: 0.0944, step time: 0.1148\n",
      "199/223, train_loss: 0.0963, step time: 0.1014\n",
      "200/223, train_loss: 0.1097, step time: 0.1385\n",
      "201/223, train_loss: 0.1034, step time: 0.1036\n",
      "202/223, train_loss: 0.0904, step time: 0.1081\n",
      "203/223, train_loss: 0.0993, step time: 0.1011\n",
      "204/223, train_loss: 0.0966, step time: 0.1059\n",
      "205/223, train_loss: 0.0912, step time: 0.1177\n",
      "206/223, train_loss: 0.0989, step time: 0.1090\n",
      "207/223, train_loss: 0.1042, step time: 0.1125\n",
      "208/223, train_loss: 0.0967, step time: 0.1008\n",
      "209/223, train_loss: 0.0944, step time: 0.1107\n",
      "210/223, train_loss: 0.1010, step time: 0.1167\n",
      "211/223, train_loss: 0.1062, step time: 0.1184\n",
      "212/223, train_loss: 0.0902, step time: 0.1191\n",
      "213/223, train_loss: 0.1023, step time: 0.1128\n",
      "214/223, train_loss: 0.0929, step time: 0.1242\n",
      "215/223, train_loss: 0.1093, step time: 0.1179\n",
      "216/223, train_loss: 0.0999, step time: 0.1003\n",
      "217/223, train_loss: 0.1020, step time: 0.1003\n",
      "218/223, train_loss: 0.1004, step time: 0.1311\n",
      "219/223, train_loss: 0.1084, step time: 0.0998\n",
      "220/223, train_loss: 0.0995, step time: 0.1045\n",
      "221/223, train_loss: 0.0967, step time: 0.0992\n",
      "222/223, train_loss: 0.1055, step time: 0.0993\n",
      "223/223, train_loss: 0.0956, step time: 0.0986\n",
      "epoch 227 average loss: 0.1009\n",
      "time consuming of epoch 227 is: 88.6316\n",
      "----------\n",
      "epoch 228/300\n",
      "1/223, train_loss: 0.1117, step time: 0.1015\n",
      "2/223, train_loss: 0.0926, step time: 0.1031\n",
      "3/223, train_loss: 0.0985, step time: 0.1087\n",
      "4/223, train_loss: 0.0950, step time: 0.1143\n",
      "5/223, train_loss: 0.0917, step time: 0.1002\n",
      "6/223, train_loss: 0.1189, step time: 0.1211\n",
      "7/223, train_loss: 0.0955, step time: 0.1204\n",
      "8/223, train_loss: 0.0904, step time: 0.1149\n",
      "9/223, train_loss: 0.1075, step time: 0.1027\n",
      "10/223, train_loss: 0.1041, step time: 0.1095\n",
      "11/223, train_loss: 0.1030, step time: 0.1035\n",
      "12/223, train_loss: 0.1003, step time: 0.1001\n",
      "13/223, train_loss: 0.0909, step time: 0.1344\n",
      "14/223, train_loss: 0.1076, step time: 0.1266\n",
      "15/223, train_loss: 0.0989, step time: 0.1049\n",
      "16/223, train_loss: 0.1039, step time: 0.1006\n",
      "17/223, train_loss: 0.1051, step time: 0.1133\n",
      "18/223, train_loss: 0.0956, step time: 0.1005\n",
      "19/223, train_loss: 0.1204, step time: 0.1045\n",
      "20/223, train_loss: 0.0961, step time: 0.1275\n",
      "21/223, train_loss: 0.0971, step time: 0.1004\n",
      "22/223, train_loss: 0.0950, step time: 0.1005\n",
      "23/223, train_loss: 0.0980, step time: 0.1016\n",
      "24/223, train_loss: 0.0896, step time: 0.1053\n",
      "25/223, train_loss: 0.1057, step time: 0.1241\n",
      "26/223, train_loss: 0.1204, step time: 0.1010\n",
      "27/223, train_loss: 0.0919, step time: 0.1107\n",
      "28/223, train_loss: 0.0950, step time: 0.1078\n",
      "29/223, train_loss: 0.0937, step time: 0.1010\n",
      "30/223, train_loss: 0.1036, step time: 0.1189\n",
      "31/223, train_loss: 0.1027, step time: 0.1256\n",
      "32/223, train_loss: 0.1008, step time: 0.1573\n",
      "33/223, train_loss: 0.0902, step time: 0.1007\n",
      "34/223, train_loss: 0.1012, step time: 0.1091\n",
      "35/223, train_loss: 0.1030, step time: 0.1094\n",
      "36/223, train_loss: 0.1064, step time: 0.1098\n",
      "37/223, train_loss: 0.0986, step time: 0.1158\n",
      "38/223, train_loss: 0.1006, step time: 0.1199\n",
      "39/223, train_loss: 0.1048, step time: 0.1348\n",
      "40/223, train_loss: 0.1017, step time: 0.1119\n",
      "41/223, train_loss: 0.0967, step time: 0.0998\n",
      "42/223, train_loss: 0.3001, step time: 0.1289\n",
      "43/223, train_loss: 0.0983, step time: 0.1076\n",
      "44/223, train_loss: 0.1021, step time: 0.1227\n",
      "45/223, train_loss: 0.0916, step time: 0.1053\n",
      "46/223, train_loss: 0.1003, step time: 0.1114\n",
      "47/223, train_loss: 0.1046, step time: 0.1018\n",
      "48/223, train_loss: 0.1191, step time: 0.1058\n",
      "49/223, train_loss: 0.0872, step time: 0.1071\n",
      "50/223, train_loss: 0.1074, step time: 0.1336\n",
      "51/223, train_loss: 0.1001, step time: 0.1196\n",
      "52/223, train_loss: 0.0990, step time: 0.1055\n",
      "53/223, train_loss: 0.0927, step time: 0.1205\n",
      "54/223, train_loss: 0.0964, step time: 0.1203\n",
      "55/223, train_loss: 0.0960, step time: 0.1114\n",
      "56/223, train_loss: 0.1112, step time: 0.0990\n",
      "57/223, train_loss: 0.0964, step time: 0.0999\n",
      "58/223, train_loss: 0.1069, step time: 0.1103\n",
      "59/223, train_loss: 0.0978, step time: 0.1099\n",
      "60/223, train_loss: 0.0988, step time: 0.0984\n",
      "61/223, train_loss: 0.0957, step time: 0.0992\n",
      "62/223, train_loss: 0.1070, step time: 0.1273\n",
      "63/223, train_loss: 0.1026, step time: 0.1265\n",
      "64/223, train_loss: 0.0942, step time: 0.1002\n",
      "65/223, train_loss: 0.0947, step time: 0.1019\n",
      "66/223, train_loss: 0.0907, step time: 0.1007\n",
      "67/223, train_loss: 0.0972, step time: 0.1156\n",
      "68/223, train_loss: 0.0967, step time: 0.1072\n",
      "69/223, train_loss: 0.0935, step time: 0.1168\n",
      "70/223, train_loss: 0.1052, step time: 0.1220\n",
      "71/223, train_loss: 0.0886, step time: 0.1122\n",
      "72/223, train_loss: 0.1110, step time: 0.1430\n",
      "73/223, train_loss: 0.0873, step time: 0.1192\n",
      "74/223, train_loss: 0.0960, step time: 0.1029\n",
      "75/223, train_loss: 0.0977, step time: 0.1037\n",
      "76/223, train_loss: 0.0948, step time: 0.1101\n",
      "77/223, train_loss: 0.0962, step time: 0.1231\n",
      "78/223, train_loss: 0.0924, step time: 0.1209\n",
      "79/223, train_loss: 0.0926, step time: 0.1019\n",
      "80/223, train_loss: 0.0970, step time: 0.1532\n",
      "81/223, train_loss: 0.1100, step time: 0.1150\n",
      "82/223, train_loss: 0.1003, step time: 0.1138\n",
      "83/223, train_loss: 0.1068, step time: 0.1004\n",
      "84/223, train_loss: 0.1184, step time: 0.1003\n",
      "85/223, train_loss: 0.1047, step time: 0.1162\n",
      "86/223, train_loss: 0.0996, step time: 0.1366\n",
      "87/223, train_loss: 0.0961, step time: 0.1036\n",
      "88/223, train_loss: 0.0916, step time: 0.1103\n",
      "89/223, train_loss: 0.0932, step time: 0.1196\n",
      "90/223, train_loss: 0.1007, step time: 0.1101\n",
      "91/223, train_loss: 0.0969, step time: 0.1270\n",
      "92/223, train_loss: 0.1054, step time: 0.1084\n",
      "93/223, train_loss: 0.1003, step time: 0.1100\n",
      "94/223, train_loss: 0.1004, step time: 0.1163\n",
      "95/223, train_loss: 0.0966, step time: 0.1298\n",
      "96/223, train_loss: 0.1003, step time: 0.1252\n",
      "97/223, train_loss: 0.0925, step time: 0.1215\n",
      "98/223, train_loss: 0.0942, step time: 0.1001\n",
      "99/223, train_loss: 0.1053, step time: 0.1007\n",
      "100/223, train_loss: 0.1008, step time: 0.1026\n",
      "101/223, train_loss: 0.1124, step time: 0.1328\n",
      "102/223, train_loss: 0.0906, step time: 0.1344\n",
      "103/223, train_loss: 0.0983, step time: 0.1017\n",
      "104/223, train_loss: 0.1002, step time: 0.1372\n",
      "105/223, train_loss: 0.1142, step time: 0.1000\n",
      "106/223, train_loss: 0.1022, step time: 0.0994\n",
      "107/223, train_loss: 0.1097, step time: 0.0994\n",
      "108/223, train_loss: 0.1044, step time: 0.1047\n",
      "109/223, train_loss: 0.1006, step time: 0.1390\n",
      "110/223, train_loss: 0.0940, step time: 0.1233\n",
      "111/223, train_loss: 0.0969, step time: 0.1154\n",
      "112/223, train_loss: 0.0947, step time: 0.1129\n",
      "113/223, train_loss: 0.0975, step time: 0.1002\n",
      "114/223, train_loss: 0.1032, step time: 0.1001\n",
      "115/223, train_loss: 0.0910, step time: 0.0997\n",
      "116/223, train_loss: 0.0908, step time: 0.1024\n",
      "117/223, train_loss: 0.0971, step time: 0.1435\n",
      "118/223, train_loss: 0.0930, step time: 0.1130\n",
      "119/223, train_loss: 0.1111, step time: 0.1351\n",
      "120/223, train_loss: 0.0926, step time: 0.1037\n",
      "121/223, train_loss: 0.0898, step time: 0.1077\n",
      "122/223, train_loss: 0.0962, step time: 0.1105\n",
      "123/223, train_loss: 0.1009, step time: 0.1054\n",
      "124/223, train_loss: 0.1056, step time: 0.1003\n",
      "125/223, train_loss: 0.0951, step time: 0.1115\n",
      "126/223, train_loss: 0.1036, step time: 0.1107\n",
      "127/223, train_loss: 0.1082, step time: 0.1242\n",
      "128/223, train_loss: 0.1034, step time: 0.1272\n",
      "129/223, train_loss: 0.0916, step time: 0.1016\n",
      "130/223, train_loss: 0.0994, step time: 0.1162\n",
      "131/223, train_loss: 0.0955, step time: 0.1149\n",
      "132/223, train_loss: 0.0890, step time: 0.1075\n",
      "133/223, train_loss: 0.1033, step time: 0.1124\n",
      "134/223, train_loss: 0.0936, step time: 0.1064\n",
      "135/223, train_loss: 0.1070, step time: 0.1004\n",
      "136/223, train_loss: 0.0932, step time: 0.1003\n",
      "137/223, train_loss: 0.0934, step time: 0.1175\n",
      "138/223, train_loss: 0.0970, step time: 0.1200\n",
      "139/223, train_loss: 0.0945, step time: 0.1050\n",
      "140/223, train_loss: 0.1195, step time: 0.1086\n",
      "141/223, train_loss: 0.1092, step time: 0.1026\n",
      "142/223, train_loss: 0.1015, step time: 0.1063\n",
      "143/223, train_loss: 0.0997, step time: 0.1555\n",
      "144/223, train_loss: 0.0976, step time: 0.1221\n",
      "145/223, train_loss: 0.0931, step time: 0.1447\n",
      "146/223, train_loss: 0.0895, step time: 0.0988\n",
      "147/223, train_loss: 0.0968, step time: 0.1172\n",
      "148/223, train_loss: 0.0959, step time: 0.1175\n",
      "149/223, train_loss: 0.1025, step time: 0.1115\n",
      "150/223, train_loss: 0.0904, step time: 0.1067\n",
      "151/223, train_loss: 0.0976, step time: 0.1098\n",
      "152/223, train_loss: 0.0946, step time: 0.1267\n",
      "153/223, train_loss: 0.1054, step time: 0.1006\n",
      "154/223, train_loss: 0.0866, step time: 0.1127\n",
      "155/223, train_loss: 0.1018, step time: 0.1128\n",
      "156/223, train_loss: 0.0957, step time: 0.1041\n",
      "157/223, train_loss: 0.1091, step time: 0.1218\n",
      "158/223, train_loss: 0.1046, step time: 0.1249\n",
      "159/223, train_loss: 0.0931, step time: 0.1024\n",
      "160/223, train_loss: 0.0954, step time: 0.1003\n",
      "161/223, train_loss: 0.1081, step time: 0.1056\n",
      "162/223, train_loss: 0.1035, step time: 0.1148\n",
      "163/223, train_loss: 0.1030, step time: 0.1012\n",
      "164/223, train_loss: 0.0965, step time: 0.1020\n",
      "165/223, train_loss: 0.1088, step time: 0.1138\n",
      "166/223, train_loss: 0.1066, step time: 0.1008\n",
      "167/223, train_loss: 0.0969, step time: 0.1092\n",
      "168/223, train_loss: 0.0996, step time: 0.1251\n",
      "169/223, train_loss: 0.1062, step time: 0.1010\n",
      "170/223, train_loss: 0.1038, step time: 0.1156\n",
      "171/223, train_loss: 0.0892, step time: 0.1071\n",
      "172/223, train_loss: 0.0974, step time: 0.1144\n",
      "173/223, train_loss: 0.1105, step time: 0.1323\n",
      "174/223, train_loss: 0.1044, step time: 0.1001\n",
      "175/223, train_loss: 0.1017, step time: 0.1024\n",
      "176/223, train_loss: 0.1034, step time: 0.1127\n",
      "177/223, train_loss: 0.1010, step time: 0.1481\n",
      "178/223, train_loss: 0.1000, step time: 0.1048\n",
      "179/223, train_loss: 0.1067, step time: 0.1093\n",
      "180/223, train_loss: 0.1005, step time: 0.1159\n",
      "181/223, train_loss: 0.1090, step time: 0.1130\n",
      "182/223, train_loss: 0.0983, step time: 0.1225\n",
      "183/223, train_loss: 0.1077, step time: 0.1037\n",
      "184/223, train_loss: 0.1123, step time: 0.0995\n",
      "185/223, train_loss: 0.1123, step time: 0.1003\n",
      "186/223, train_loss: 0.1055, step time: 0.1284\n",
      "187/223, train_loss: 0.1026, step time: 0.1069\n",
      "188/223, train_loss: 0.1033, step time: 0.1093\n",
      "189/223, train_loss: 0.1019, step time: 0.1133\n",
      "190/223, train_loss: 0.0932, step time: 0.1057\n",
      "191/223, train_loss: 0.0941, step time: 0.1090\n",
      "192/223, train_loss: 0.1113, step time: 0.1049\n",
      "193/223, train_loss: 0.1036, step time: 0.1239\n",
      "194/223, train_loss: 0.0962, step time: 0.1074\n",
      "195/223, train_loss: 0.0888, step time: 0.1219\n",
      "196/223, train_loss: 0.0914, step time: 0.1163\n",
      "197/223, train_loss: 0.0887, step time: 0.1030\n",
      "198/223, train_loss: 0.0937, step time: 0.1078\n",
      "199/223, train_loss: 0.1033, step time: 0.1006\n",
      "200/223, train_loss: 0.1045, step time: 0.1356\n",
      "201/223, train_loss: 0.1071, step time: 0.1004\n",
      "202/223, train_loss: 0.1120, step time: 0.1149\n",
      "203/223, train_loss: 0.1065, step time: 0.1146\n",
      "204/223, train_loss: 0.1052, step time: 0.1001\n",
      "205/223, train_loss: 0.0895, step time: 0.1054\n",
      "206/223, train_loss: 0.1069, step time: 0.1031\n",
      "207/223, train_loss: 0.1046, step time: 0.1128\n",
      "208/223, train_loss: 0.0907, step time: 0.1053\n",
      "209/223, train_loss: 0.0990, step time: 0.1141\n",
      "210/223, train_loss: 0.0959, step time: 0.1132\n",
      "211/223, train_loss: 0.0953, step time: 0.1212\n",
      "212/223, train_loss: 0.1021, step time: 0.1110\n",
      "213/223, train_loss: 0.0995, step time: 0.1211\n",
      "214/223, train_loss: 0.0915, step time: 0.1208\n",
      "215/223, train_loss: 0.1075, step time: 0.0998\n",
      "216/223, train_loss: 0.0916, step time: 0.1213\n",
      "217/223, train_loss: 0.0997, step time: 0.1215\n",
      "218/223, train_loss: 0.1007, step time: 0.1002\n",
      "219/223, train_loss: 0.0960, step time: 0.0999\n",
      "220/223, train_loss: 0.1079, step time: 0.0999\n",
      "221/223, train_loss: 0.0931, step time: 0.1003\n",
      "222/223, train_loss: 0.0994, step time: 0.0999\n",
      "223/223, train_loss: 0.1002, step time: 0.0998\n",
      "epoch 228 average loss: 0.1008\n",
      "time consuming of epoch 228 is: 90.4109\n",
      "----------\n",
      "epoch 229/300\n",
      "1/223, train_loss: 0.1010, step time: 0.1095\n",
      "2/223, train_loss: 0.1065, step time: 0.1063\n",
      "3/223, train_loss: 0.0969, step time: 0.1126\n",
      "4/223, train_loss: 0.1102, step time: 0.1002\n",
      "5/223, train_loss: 0.1016, step time: 0.1127\n",
      "6/223, train_loss: 0.0977, step time: 0.1014\n",
      "7/223, train_loss: 0.0906, step time: 0.1199\n",
      "8/223, train_loss: 0.1077, step time: 0.1073\n",
      "9/223, train_loss: 0.1022, step time: 0.1110\n",
      "10/223, train_loss: 0.0929, step time: 0.1389\n",
      "11/223, train_loss: 0.0983, step time: 0.1139\n",
      "12/223, train_loss: 0.0889, step time: 0.1127\n",
      "13/223, train_loss: 0.0976, step time: 0.1126\n",
      "14/223, train_loss: 0.1044, step time: 0.1242\n",
      "15/223, train_loss: 0.1022, step time: 0.1271\n",
      "16/223, train_loss: 0.0971, step time: 0.1015\n",
      "17/223, train_loss: 0.0981, step time: 0.1067\n",
      "18/223, train_loss: 0.1074, step time: 0.1208\n",
      "19/223, train_loss: 0.0898, step time: 0.1077\n",
      "20/223, train_loss: 0.1003, step time: 0.1170\n",
      "21/223, train_loss: 0.0915, step time: 0.1013\n",
      "22/223, train_loss: 0.1071, step time: 0.1129\n",
      "23/223, train_loss: 0.1068, step time: 0.1263\n",
      "24/223, train_loss: 0.0951, step time: 0.1318\n",
      "25/223, train_loss: 0.0940, step time: 0.1119\n",
      "26/223, train_loss: 0.1120, step time: 0.1123\n",
      "27/223, train_loss: 0.0950, step time: 0.1211\n",
      "28/223, train_loss: 0.0943, step time: 0.1114\n",
      "29/223, train_loss: 0.0998, step time: 0.1210\n",
      "30/223, train_loss: 0.0963, step time: 0.1172\n",
      "31/223, train_loss: 0.1098, step time: 0.1486\n",
      "32/223, train_loss: 0.0938, step time: 0.1275\n",
      "33/223, train_loss: 0.1171, step time: 0.1028\n",
      "34/223, train_loss: 0.0938, step time: 0.1108\n",
      "35/223, train_loss: 0.0896, step time: 0.1136\n",
      "36/223, train_loss: 0.1005, step time: 0.1432\n",
      "37/223, train_loss: 0.0945, step time: 0.1017\n",
      "38/223, train_loss: 0.1083, step time: 0.1100\n",
      "39/223, train_loss: 0.1034, step time: 0.1016\n",
      "40/223, train_loss: 0.0944, step time: 0.1158\n",
      "41/223, train_loss: 0.0885, step time: 0.1179\n",
      "42/223, train_loss: 0.0997, step time: 0.1109\n",
      "43/223, train_loss: 0.1056, step time: 0.1020\n",
      "44/223, train_loss: 0.0882, step time: 0.1171\n",
      "45/223, train_loss: 0.0957, step time: 0.1052\n",
      "46/223, train_loss: 0.1108, step time: 0.1091\n",
      "47/223, train_loss: 0.0987, step time: 0.1081\n",
      "48/223, train_loss: 0.0947, step time: 0.1061\n",
      "49/223, train_loss: 0.1062, step time: 0.1139\n",
      "50/223, train_loss: 0.0950, step time: 0.1142\n",
      "51/223, train_loss: 0.1046, step time: 0.1408\n",
      "52/223, train_loss: 0.0977, step time: 0.1364\n",
      "53/223, train_loss: 0.0910, step time: 0.1066\n",
      "54/223, train_loss: 0.0989, step time: 0.1094\n",
      "55/223, train_loss: 0.0843, step time: 0.1227\n",
      "56/223, train_loss: 0.0978, step time: 0.1184\n",
      "57/223, train_loss: 0.0959, step time: 0.1214\n",
      "58/223, train_loss: 0.1025, step time: 0.1179\n",
      "59/223, train_loss: 0.0913, step time: 0.1149\n",
      "60/223, train_loss: 0.1008, step time: 0.1116\n",
      "61/223, train_loss: 0.1111, step time: 0.1197\n",
      "62/223, train_loss: 0.0909, step time: 0.1028\n",
      "63/223, train_loss: 0.1088, step time: 0.1121\n",
      "64/223, train_loss: 0.0952, step time: 0.1025\n",
      "65/223, train_loss: 0.0936, step time: 0.1190\n",
      "66/223, train_loss: 0.0887, step time: 0.1103\n",
      "67/223, train_loss: 0.1096, step time: 0.1009\n",
      "68/223, train_loss: 0.0930, step time: 0.1008\n",
      "69/223, train_loss: 0.1127, step time: 0.1159\n",
      "70/223, train_loss: 0.0997, step time: 0.1002\n",
      "71/223, train_loss: 0.0911, step time: 0.1001\n",
      "72/223, train_loss: 0.0909, step time: 0.1090\n",
      "73/223, train_loss: 0.1070, step time: 0.1181\n",
      "74/223, train_loss: 0.0985, step time: 0.1020\n",
      "75/223, train_loss: 0.1093, step time: 0.1411\n",
      "76/223, train_loss: 0.1075, step time: 0.1012\n",
      "77/223, train_loss: 0.1028, step time: 0.1001\n",
      "78/223, train_loss: 0.0935, step time: 0.0998\n",
      "79/223, train_loss: 0.0977, step time: 0.1294\n",
      "80/223, train_loss: 0.1044, step time: 0.1123\n",
      "81/223, train_loss: 0.1001, step time: 0.1083\n",
      "82/223, train_loss: 0.0897, step time: 0.1125\n",
      "83/223, train_loss: 0.1012, step time: 0.1181\n",
      "84/223, train_loss: 0.1037, step time: 0.1019\n",
      "85/223, train_loss: 0.1070, step time: 0.1097\n",
      "86/223, train_loss: 0.0949, step time: 0.1002\n",
      "87/223, train_loss: 0.1009, step time: 0.1228\n",
      "88/223, train_loss: 0.1012, step time: 0.1113\n",
      "89/223, train_loss: 0.0952, step time: 0.1016\n",
      "90/223, train_loss: 0.1073, step time: 0.1145\n",
      "91/223, train_loss: 0.0937, step time: 0.1192\n",
      "92/223, train_loss: 0.1030, step time: 0.0998\n",
      "93/223, train_loss: 0.0922, step time: 0.1047\n",
      "94/223, train_loss: 0.1025, step time: 0.1331\n",
      "95/223, train_loss: 0.0956, step time: 0.1242\n",
      "96/223, train_loss: 0.1074, step time: 0.1093\n",
      "97/223, train_loss: 0.0868, step time: 0.0994\n",
      "98/223, train_loss: 0.1008, step time: 0.1013\n",
      "99/223, train_loss: 0.0972, step time: 0.0999\n",
      "100/223, train_loss: 0.0908, step time: 0.1046\n",
      "101/223, train_loss: 0.1010, step time: 0.1040\n",
      "102/223, train_loss: 0.1016, step time: 0.1006\n",
      "103/223, train_loss: 0.0953, step time: 0.1229\n",
      "104/223, train_loss: 0.1002, step time: 0.1074\n",
      "105/223, train_loss: 0.1133, step time: 0.1223\n",
      "106/223, train_loss: 0.0934, step time: 0.1413\n",
      "107/223, train_loss: 0.0967, step time: 0.1497\n",
      "108/223, train_loss: 0.1036, step time: 0.1133\n",
      "109/223, train_loss: 0.1016, step time: 0.1082\n",
      "110/223, train_loss: 0.0977, step time: 0.1383\n",
      "111/223, train_loss: 0.0963, step time: 0.1005\n",
      "112/223, train_loss: 0.0990, step time: 0.1023\n",
      "113/223, train_loss: 0.0939, step time: 0.1020\n",
      "114/223, train_loss: 0.0962, step time: 0.1054\n",
      "115/223, train_loss: 0.1009, step time: 0.1100\n",
      "116/223, train_loss: 0.1073, step time: 0.1081\n",
      "117/223, train_loss: 0.1011, step time: 0.1119\n",
      "118/223, train_loss: 0.1054, step time: 0.1103\n",
      "119/223, train_loss: 0.0976, step time: 0.1272\n",
      "120/223, train_loss: 0.0938, step time: 0.1136\n",
      "121/223, train_loss: 0.1012, step time: 0.1436\n",
      "122/223, train_loss: 0.1045, step time: 0.1060\n",
      "123/223, train_loss: 0.1089, step time: 0.1014\n",
      "124/223, train_loss: 0.0942, step time: 0.1004\n",
      "125/223, train_loss: 0.0859, step time: 0.1123\n",
      "126/223, train_loss: 0.0964, step time: 0.1002\n",
      "127/223, train_loss: 0.0998, step time: 0.0997\n",
      "128/223, train_loss: 0.1102, step time: 0.1183\n",
      "129/223, train_loss: 0.1203, step time: 0.1084\n",
      "130/223, train_loss: 0.0899, step time: 0.1009\n",
      "131/223, train_loss: 0.1005, step time: 0.1013\n",
      "132/223, train_loss: 0.0944, step time: 0.1015\n",
      "133/223, train_loss: 0.1079, step time: 0.1074\n",
      "134/223, train_loss: 0.1102, step time: 0.1128\n",
      "135/223, train_loss: 0.1024, step time: 0.1083\n",
      "136/223, train_loss: 0.0975, step time: 0.1009\n",
      "137/223, train_loss: 0.1027, step time: 0.1082\n",
      "138/223, train_loss: 0.1122, step time: 0.1427\n",
      "139/223, train_loss: 0.1015, step time: 0.1336\n",
      "140/223, train_loss: 0.1015, step time: 0.1032\n",
      "141/223, train_loss: 0.0905, step time: 0.1216\n",
      "142/223, train_loss: 0.0940, step time: 0.1134\n",
      "143/223, train_loss: 0.1016, step time: 0.1057\n",
      "144/223, train_loss: 0.1073, step time: 0.1002\n",
      "145/223, train_loss: 0.1063, step time: 0.1098\n",
      "146/223, train_loss: 0.1085, step time: 0.1136\n",
      "147/223, train_loss: 0.1037, step time: 0.1001\n",
      "148/223, train_loss: 0.0905, step time: 0.1001\n",
      "149/223, train_loss: 0.1114, step time: 0.1218\n",
      "150/223, train_loss: 0.1051, step time: 0.1143\n",
      "151/223, train_loss: 0.1031, step time: 0.1104\n",
      "152/223, train_loss: 0.0918, step time: 0.1010\n",
      "153/223, train_loss: 0.0964, step time: 0.1200\n",
      "154/223, train_loss: 0.0961, step time: 0.1104\n",
      "155/223, train_loss: 0.1018, step time: 0.1029\n",
      "156/223, train_loss: 0.0908, step time: 0.1151\n",
      "157/223, train_loss: 0.0942, step time: 0.1009\n",
      "158/223, train_loss: 0.1038, step time: 0.1004\n",
      "159/223, train_loss: 0.0928, step time: 0.1012\n",
      "160/223, train_loss: 0.0946, step time: 0.1013\n",
      "161/223, train_loss: 0.0964, step time: 0.1104\n",
      "162/223, train_loss: 0.1017, step time: 0.1335\n",
      "163/223, train_loss: 0.0956, step time: 0.1156\n",
      "164/223, train_loss: 0.0932, step time: 0.1008\n",
      "165/223, train_loss: 0.1050, step time: 0.1197\n",
      "166/223, train_loss: 0.0998, step time: 0.1345\n",
      "167/223, train_loss: 0.0926, step time: 0.1250\n",
      "168/223, train_loss: 0.0965, step time: 0.1101\n",
      "169/223, train_loss: 0.1017, step time: 0.1089\n",
      "170/223, train_loss: 0.0927, step time: 0.1050\n",
      "171/223, train_loss: 0.1015, step time: 0.1261\n",
      "172/223, train_loss: 0.1072, step time: 0.1007\n",
      "173/223, train_loss: 0.0962, step time: 0.1201\n",
      "174/223, train_loss: 0.0942, step time: 0.1001\n",
      "175/223, train_loss: 0.1122, step time: 0.1077\n",
      "176/223, train_loss: 0.1092, step time: 0.1282\n",
      "177/223, train_loss: 0.1024, step time: 0.1543\n",
      "178/223, train_loss: 0.1123, step time: 0.1003\n",
      "179/223, train_loss: 0.0994, step time: 0.1010\n",
      "180/223, train_loss: 0.0914, step time: 0.1010\n",
      "181/223, train_loss: 0.0890, step time: 0.1034\n",
      "182/223, train_loss: 0.1061, step time: 0.1054\n",
      "183/223, train_loss: 0.1189, step time: 0.1451\n",
      "184/223, train_loss: 0.1052, step time: 0.1133\n",
      "185/223, train_loss: 0.0991, step time: 0.1172\n",
      "186/223, train_loss: 0.0966, step time: 0.1027\n",
      "187/223, train_loss: 0.0998, step time: 0.0999\n",
      "188/223, train_loss: 0.0953, step time: 0.1066\n",
      "189/223, train_loss: 0.0913, step time: 0.1077\n",
      "190/223, train_loss: 0.0923, step time: 0.1124\n",
      "191/223, train_loss: 0.0977, step time: 0.1256\n",
      "192/223, train_loss: 0.1059, step time: 0.1314\n",
      "193/223, train_loss: 0.0913, step time: 0.1201\n",
      "194/223, train_loss: 0.1028, step time: 0.1005\n",
      "195/223, train_loss: 0.1062, step time: 0.1288\n",
      "196/223, train_loss: 0.0985, step time: 0.1006\n",
      "197/223, train_loss: 0.0960, step time: 0.1246\n",
      "198/223, train_loss: 0.1089, step time: 0.1026\n",
      "199/223, train_loss: 0.0951, step time: 0.1194\n",
      "200/223, train_loss: 0.1071, step time: 0.1123\n",
      "201/223, train_loss: 0.1046, step time: 0.1231\n",
      "202/223, train_loss: 0.1102, step time: 0.1093\n",
      "203/223, train_loss: 0.1047, step time: 0.1116\n",
      "204/223, train_loss: 0.1027, step time: 0.1154\n",
      "205/223, train_loss: 0.0945, step time: 0.1002\n",
      "206/223, train_loss: 0.0986, step time: 0.0995\n",
      "207/223, train_loss: 0.0946, step time: 0.0997\n",
      "208/223, train_loss: 0.0967, step time: 0.1061\n",
      "209/223, train_loss: 0.3019, step time: 0.0994\n",
      "210/223, train_loss: 0.0987, step time: 0.1373\n",
      "211/223, train_loss: 0.1029, step time: 0.1008\n",
      "212/223, train_loss: 0.1000, step time: 0.1007\n",
      "213/223, train_loss: 0.1009, step time: 0.1000\n",
      "214/223, train_loss: 0.0983, step time: 0.1000\n",
      "215/223, train_loss: 0.1068, step time: 0.1193\n",
      "216/223, train_loss: 0.1043, step time: 0.1125\n",
      "217/223, train_loss: 0.1025, step time: 0.1159\n",
      "218/223, train_loss: 0.1100, step time: 0.1000\n",
      "219/223, train_loss: 0.1107, step time: 0.0998\n",
      "220/223, train_loss: 0.0947, step time: 0.1020\n",
      "221/223, train_loss: 0.0952, step time: 0.1008\n",
      "222/223, train_loss: 0.1122, step time: 0.0996\n",
      "223/223, train_loss: 0.1010, step time: 0.1008\n",
      "epoch 229 average loss: 0.1008\n",
      "time consuming of epoch 229 is: 89.3882\n",
      "----------\n",
      "epoch 230/300\n",
      "1/223, train_loss: 0.1027, step time: 0.1010\n",
      "2/223, train_loss: 0.1048, step time: 0.1012\n",
      "3/223, train_loss: 0.0974, step time: 0.1012\n",
      "4/223, train_loss: 0.1040, step time: 0.1022\n",
      "5/223, train_loss: 0.1102, step time: 0.1052\n",
      "6/223, train_loss: 0.1138, step time: 0.1145\n",
      "7/223, train_loss: 0.1143, step time: 0.1010\n",
      "8/223, train_loss: 0.3045, step time: 0.1173\n",
      "9/223, train_loss: 0.0965, step time: 0.0996\n",
      "10/223, train_loss: 0.1022, step time: 0.1059\n",
      "11/223, train_loss: 0.1094, step time: 0.1068\n",
      "12/223, train_loss: 0.1027, step time: 0.1150\n",
      "13/223, train_loss: 0.1030, step time: 0.1277\n",
      "14/223, train_loss: 0.0991, step time: 0.1027\n",
      "15/223, train_loss: 0.0934, step time: 0.1284\n",
      "16/223, train_loss: 0.1028, step time: 0.1004\n",
      "17/223, train_loss: 0.1082, step time: 0.1258\n",
      "18/223, train_loss: 0.1034, step time: 0.1039\n",
      "19/223, train_loss: 0.1017, step time: 0.1738\n",
      "20/223, train_loss: 0.1093, step time: 0.1173\n",
      "21/223, train_loss: 0.1015, step time: 0.1008\n",
      "22/223, train_loss: 0.1091, step time: 0.1211\n",
      "23/223, train_loss: 0.1063, step time: 0.1290\n",
      "24/223, train_loss: 0.0922, step time: 0.1118\n",
      "25/223, train_loss: 0.0958, step time: 0.1000\n",
      "26/223, train_loss: 0.0982, step time: 0.1635\n",
      "27/223, train_loss: 0.0925, step time: 0.1015\n",
      "28/223, train_loss: 0.1000, step time: 0.1105\n",
      "29/223, train_loss: 0.0994, step time: 0.1092\n",
      "30/223, train_loss: 0.0900, step time: 0.1224\n",
      "31/223, train_loss: 0.0906, step time: 0.1133\n",
      "32/223, train_loss: 0.0986, step time: 0.1005\n",
      "33/223, train_loss: 0.0993, step time: 0.1148\n",
      "34/223, train_loss: 0.1009, step time: 0.1184\n",
      "35/223, train_loss: 0.1041, step time: 0.1003\n",
      "36/223, train_loss: 0.1175, step time: 0.1068\n",
      "37/223, train_loss: 0.0991, step time: 0.1106\n",
      "38/223, train_loss: 0.0954, step time: 0.1264\n",
      "39/223, train_loss: 0.0917, step time: 0.1003\n",
      "40/223, train_loss: 0.1058, step time: 0.1004\n",
      "41/223, train_loss: 0.1043, step time: 0.1081\n",
      "42/223, train_loss: 0.1001, step time: 0.1053\n",
      "43/223, train_loss: 0.0979, step time: 0.1055\n",
      "44/223, train_loss: 0.1019, step time: 0.1110\n",
      "45/223, train_loss: 0.1027, step time: 0.1138\n",
      "46/223, train_loss: 0.0935, step time: 0.1279\n",
      "47/223, train_loss: 0.0972, step time: 0.1119\n",
      "48/223, train_loss: 0.0971, step time: 0.1001\n",
      "49/223, train_loss: 0.0967, step time: 0.1002\n",
      "50/223, train_loss: 0.1048, step time: 0.1074\n",
      "51/223, train_loss: 0.1017, step time: 0.1055\n",
      "52/223, train_loss: 0.0991, step time: 0.0997\n",
      "53/223, train_loss: 0.1010, step time: 0.1022\n",
      "54/223, train_loss: 0.0890, step time: 0.1145\n",
      "55/223, train_loss: 0.1026, step time: 0.1223\n",
      "56/223, train_loss: 0.1043, step time: 0.1009\n",
      "57/223, train_loss: 0.0968, step time: 0.1180\n",
      "58/223, train_loss: 0.0879, step time: 0.1894\n",
      "59/223, train_loss: 0.0988, step time: 0.1011\n",
      "60/223, train_loss: 0.1050, step time: 0.1143\n",
      "61/223, train_loss: 0.0949, step time: 0.1285\n",
      "62/223, train_loss: 0.0906, step time: 0.1567\n",
      "63/223, train_loss: 0.1097, step time: 0.1171\n",
      "64/223, train_loss: 0.0941, step time: 0.1118\n",
      "65/223, train_loss: 0.1097, step time: 0.1026\n",
      "66/223, train_loss: 0.0948, step time: 0.1094\n",
      "67/223, train_loss: 0.1069, step time: 0.1104\n",
      "68/223, train_loss: 0.1004, step time: 0.1000\n",
      "69/223, train_loss: 0.1008, step time: 0.1002\n",
      "70/223, train_loss: 0.1034, step time: 0.1180\n",
      "71/223, train_loss: 0.1033, step time: 0.1006\n",
      "72/223, train_loss: 0.1001, step time: 0.1001\n",
      "73/223, train_loss: 0.0984, step time: 0.1205\n",
      "74/223, train_loss: 0.1109, step time: 0.1360\n",
      "75/223, train_loss: 0.0957, step time: 0.1384\n",
      "76/223, train_loss: 0.0940, step time: 0.0995\n",
      "77/223, train_loss: 0.1105, step time: 0.1047\n",
      "78/223, train_loss: 0.1008, step time: 0.0999\n",
      "79/223, train_loss: 0.0978, step time: 0.1149\n",
      "80/223, train_loss: 0.0968, step time: 0.1431\n",
      "81/223, train_loss: 0.0937, step time: 0.1269\n",
      "82/223, train_loss: 0.1002, step time: 0.1187\n",
      "83/223, train_loss: 0.1005, step time: 0.1160\n",
      "84/223, train_loss: 0.0935, step time: 0.1100\n",
      "85/223, train_loss: 0.0956, step time: 0.0996\n",
      "86/223, train_loss: 0.1010, step time: 0.1000\n",
      "87/223, train_loss: 0.0951, step time: 0.1089\n",
      "88/223, train_loss: 0.0895, step time: 0.1012\n",
      "89/223, train_loss: 0.0981, step time: 0.1011\n",
      "90/223, train_loss: 0.1038, step time: 0.1126\n",
      "91/223, train_loss: 0.1162, step time: 0.1007\n",
      "92/223, train_loss: 0.0952, step time: 0.1006\n",
      "93/223, train_loss: 0.0921, step time: 0.0998\n",
      "94/223, train_loss: 0.0928, step time: 0.1149\n",
      "95/223, train_loss: 0.1027, step time: 0.1220\n",
      "96/223, train_loss: 0.0985, step time: 0.1364\n",
      "97/223, train_loss: 0.0886, step time: 0.1006\n",
      "98/223, train_loss: 0.0953, step time: 0.1000\n",
      "99/223, train_loss: 0.1109, step time: 0.1004\n",
      "100/223, train_loss: 0.1056, step time: 0.1118\n",
      "101/223, train_loss: 0.0912, step time: 0.1010\n",
      "102/223, train_loss: 0.0940, step time: 0.1104\n",
      "103/223, train_loss: 0.1116, step time: 0.0998\n",
      "104/223, train_loss: 0.0864, step time: 0.1326\n",
      "105/223, train_loss: 0.0862, step time: 0.1136\n",
      "106/223, train_loss: 0.1103, step time: 0.1019\n",
      "107/223, train_loss: 0.1023, step time: 0.1082\n",
      "108/223, train_loss: 0.0961, step time: 0.1347\n",
      "109/223, train_loss: 0.0901, step time: 0.1001\n",
      "110/223, train_loss: 0.0855, step time: 0.1005\n",
      "111/223, train_loss: 0.1048, step time: 0.0997\n",
      "112/223, train_loss: 0.1083, step time: 0.0993\n",
      "113/223, train_loss: 0.1017, step time: 0.0996\n",
      "114/223, train_loss: 0.0966, step time: 0.1235\n",
      "115/223, train_loss: 0.0979, step time: 0.0993\n",
      "116/223, train_loss: 0.0954, step time: 0.1000\n",
      "117/223, train_loss: 0.0948, step time: 0.0997\n",
      "118/223, train_loss: 0.0971, step time: 0.1002\n",
      "119/223, train_loss: 0.1020, step time: 0.0995\n",
      "120/223, train_loss: 0.0966, step time: 0.0994\n",
      "121/223, train_loss: 0.0952, step time: 0.0996\n",
      "122/223, train_loss: 0.0896, step time: 0.1005\n",
      "123/223, train_loss: 0.1004, step time: 0.1005\n",
      "124/223, train_loss: 0.0991, step time: 0.0998\n",
      "125/223, train_loss: 0.1008, step time: 0.1004\n",
      "126/223, train_loss: 0.0984, step time: 0.1082\n",
      "127/223, train_loss: 0.1000, step time: 0.1071\n",
      "128/223, train_loss: 0.0990, step time: 0.1298\n",
      "129/223, train_loss: 0.0935, step time: 0.1002\n",
      "130/223, train_loss: 0.1023, step time: 0.1001\n",
      "131/223, train_loss: 0.1104, step time: 0.1079\n",
      "132/223, train_loss: 0.1066, step time: 0.1008\n",
      "133/223, train_loss: 0.1027, step time: 0.1097\n",
      "134/223, train_loss: 0.0971, step time: 0.1031\n",
      "135/223, train_loss: 0.0972, step time: 0.1247\n",
      "136/223, train_loss: 0.1037, step time: 0.1069\n",
      "137/223, train_loss: 0.0980, step time: 0.1033\n",
      "138/223, train_loss: 0.1009, step time: 0.1098\n",
      "139/223, train_loss: 0.1044, step time: 0.1134\n",
      "140/223, train_loss: 0.0925, step time: 0.1274\n",
      "141/223, train_loss: 0.1039, step time: 0.1063\n",
      "142/223, train_loss: 0.0988, step time: 0.1132\n",
      "143/223, train_loss: 0.1085, step time: 0.1089\n",
      "144/223, train_loss: 0.0967, step time: 0.1007\n",
      "145/223, train_loss: 0.1072, step time: 0.1079\n",
      "146/223, train_loss: 0.1080, step time: 0.1039\n",
      "147/223, train_loss: 0.0914, step time: 0.1135\n",
      "148/223, train_loss: 0.0984, step time: 0.1467\n",
      "149/223, train_loss: 0.1202, step time: 0.1011\n",
      "150/223, train_loss: 0.0887, step time: 0.1002\n",
      "151/223, train_loss: 0.1075, step time: 0.1219\n",
      "152/223, train_loss: 0.0973, step time: 0.1172\n",
      "153/223, train_loss: 0.1048, step time: 0.1181\n",
      "154/223, train_loss: 0.1031, step time: 0.1167\n",
      "155/223, train_loss: 0.0898, step time: 0.1198\n",
      "156/223, train_loss: 0.0939, step time: 0.1368\n",
      "157/223, train_loss: 0.0971, step time: 0.1029\n",
      "158/223, train_loss: 0.1025, step time: 0.1066\n",
      "159/223, train_loss: 0.0958, step time: 0.1204\n",
      "160/223, train_loss: 0.0989, step time: 0.1086\n",
      "161/223, train_loss: 0.0937, step time: 0.1176\n",
      "162/223, train_loss: 0.0908, step time: 0.1103\n",
      "163/223, train_loss: 0.0999, step time: 0.1136\n",
      "164/223, train_loss: 0.0942, step time: 0.1023\n",
      "165/223, train_loss: 0.0976, step time: 0.1164\n",
      "166/223, train_loss: 0.1117, step time: 0.1381\n",
      "167/223, train_loss: 0.0913, step time: 0.1180\n",
      "168/223, train_loss: 0.1187, step time: 0.1043\n",
      "169/223, train_loss: 0.1300, step time: 0.1269\n",
      "170/223, train_loss: 0.1097, step time: 0.1247\n",
      "171/223, train_loss: 0.0958, step time: 0.1116\n",
      "172/223, train_loss: 0.1023, step time: 0.1004\n",
      "173/223, train_loss: 0.1045, step time: 0.1007\n",
      "174/223, train_loss: 0.0842, step time: 0.1017\n",
      "175/223, train_loss: 0.1040, step time: 0.1199\n",
      "176/223, train_loss: 0.0966, step time: 0.1165\n",
      "177/223, train_loss: 0.0904, step time: 0.1196\n",
      "178/223, train_loss: 0.1107, step time: 0.1128\n",
      "179/223, train_loss: 0.0952, step time: 0.1080\n",
      "180/223, train_loss: 0.0940, step time: 0.1199\n",
      "181/223, train_loss: 0.0941, step time: 0.1054\n",
      "182/223, train_loss: 0.1086, step time: 0.0997\n",
      "183/223, train_loss: 0.1104, step time: 0.1246\n",
      "184/223, train_loss: 0.0949, step time: 0.1017\n",
      "185/223, train_loss: 0.0951, step time: 0.1015\n",
      "186/223, train_loss: 0.1095, step time: 0.1037\n",
      "187/223, train_loss: 0.0963, step time: 0.1053\n",
      "188/223, train_loss: 0.1087, step time: 0.1144\n",
      "189/223, train_loss: 0.1011, step time: 0.1013\n",
      "190/223, train_loss: 0.0918, step time: 0.1364\n",
      "191/223, train_loss: 0.1037, step time: 0.1029\n",
      "192/223, train_loss: 0.0981, step time: 0.1096\n",
      "193/223, train_loss: 0.0920, step time: 0.0994\n",
      "194/223, train_loss: 0.0969, step time: 0.1287\n",
      "195/223, train_loss: 0.0921, step time: 0.1133\n",
      "196/223, train_loss: 0.0935, step time: 0.1087\n",
      "197/223, train_loss: 0.1125, step time: 0.1217\n",
      "198/223, train_loss: 0.0910, step time: 0.1033\n",
      "199/223, train_loss: 0.0909, step time: 0.1135\n",
      "200/223, train_loss: 0.0974, step time: 0.1012\n",
      "201/223, train_loss: 0.0976, step time: 0.1054\n",
      "202/223, train_loss: 0.1048, step time: 0.1112\n",
      "203/223, train_loss: 0.1107, step time: 0.1113\n",
      "204/223, train_loss: 0.0977, step time: 0.1160\n",
      "205/223, train_loss: 0.1083, step time: 0.1105\n",
      "206/223, train_loss: 0.1045, step time: 0.1250\n",
      "207/223, train_loss: 0.1002, step time: 0.1137\n",
      "208/223, train_loss: 0.0976, step time: 0.1232\n",
      "209/223, train_loss: 0.0891, step time: 0.1010\n",
      "210/223, train_loss: 0.0998, step time: 0.1121\n",
      "211/223, train_loss: 0.1079, step time: 0.1132\n",
      "212/223, train_loss: 0.1015, step time: 0.1323\n",
      "213/223, train_loss: 0.0982, step time: 0.1169\n",
      "214/223, train_loss: 0.0996, step time: 0.1118\n",
      "215/223, train_loss: 0.0997, step time: 0.1125\n",
      "216/223, train_loss: 0.1002, step time: 0.1001\n",
      "217/223, train_loss: 0.1075, step time: 0.1005\n",
      "218/223, train_loss: 0.0943, step time: 0.1253\n",
      "219/223, train_loss: 0.0933, step time: 0.0998\n",
      "220/223, train_loss: 0.0949, step time: 0.0999\n",
      "221/223, train_loss: 0.0993, step time: 0.1004\n",
      "222/223, train_loss: 0.0951, step time: 0.1108\n",
      "223/223, train_loss: 0.1007, step time: 0.0987\n",
      "epoch 230 average loss: 0.1008\n",
      "saved new best metric model\n",
      "current epoch: 230 current mean dice: 0.8619 tc: 0.9225 wt: 0.8715 et: 0.7916\n",
      "best mean dice: 0.8619 at epoch: 230\n",
      "time consuming of epoch 230 is: 94.9367\n",
      "----------\n",
      "epoch 231/300\n",
      "1/223, train_loss: 0.1005, step time: 0.1006\n",
      "2/223, train_loss: 0.0983, step time: 0.1259\n",
      "3/223, train_loss: 0.1011, step time: 0.1285\n",
      "4/223, train_loss: 0.1000, step time: 0.1135\n",
      "5/223, train_loss: 0.0970, step time: 0.1045\n",
      "6/223, train_loss: 0.0916, step time: 0.1107\n",
      "7/223, train_loss: 0.1094, step time: 0.1042\n",
      "8/223, train_loss: 0.0958, step time: 0.1124\n",
      "9/223, train_loss: 0.1040, step time: 0.1154\n",
      "10/223, train_loss: 0.0966, step time: 0.1143\n",
      "11/223, train_loss: 0.0976, step time: 0.1240\n",
      "12/223, train_loss: 0.1046, step time: 0.1149\n",
      "13/223, train_loss: 0.0892, step time: 0.1060\n",
      "14/223, train_loss: 0.0922, step time: 0.1252\n",
      "15/223, train_loss: 0.1106, step time: 0.1029\n",
      "16/223, train_loss: 0.0959, step time: 0.1098\n",
      "17/223, train_loss: 0.1064, step time: 0.1029\n",
      "18/223, train_loss: 0.0962, step time: 0.1093\n",
      "19/223, train_loss: 0.0962, step time: 0.1096\n",
      "20/223, train_loss: 0.0932, step time: 0.1091\n",
      "21/223, train_loss: 0.1071, step time: 0.1042\n",
      "22/223, train_loss: 0.1000, step time: 0.1037\n",
      "23/223, train_loss: 0.1129, step time: 0.1025\n",
      "24/223, train_loss: 0.1008, step time: 0.1202\n",
      "25/223, train_loss: 0.0928, step time: 0.1055\n",
      "26/223, train_loss: 0.1000, step time: 0.1062\n",
      "27/223, train_loss: 0.1129, step time: 0.1190\n",
      "28/223, train_loss: 0.0912, step time: 0.1213\n",
      "29/223, train_loss: 0.1047, step time: 0.1061\n",
      "30/223, train_loss: 0.0922, step time: 0.1393\n",
      "31/223, train_loss: 0.0936, step time: 0.1112\n",
      "32/223, train_loss: 0.1017, step time: 0.1013\n",
      "33/223, train_loss: 0.1019, step time: 0.1079\n",
      "34/223, train_loss: 0.1020, step time: 0.1224\n",
      "35/223, train_loss: 0.1026, step time: 0.1480\n",
      "36/223, train_loss: 0.0927, step time: 0.1221\n",
      "37/223, train_loss: 0.1042, step time: 0.1058\n",
      "38/223, train_loss: 0.0966, step time: 0.1007\n",
      "39/223, train_loss: 0.1060, step time: 0.1125\n",
      "40/223, train_loss: 0.2957, step time: 0.1081\n",
      "41/223, train_loss: 0.1011, step time: 0.1156\n",
      "42/223, train_loss: 0.0941, step time: 0.1774\n",
      "43/223, train_loss: 0.0923, step time: 0.1373\n",
      "44/223, train_loss: 0.1069, step time: 0.1074\n",
      "45/223, train_loss: 0.1062, step time: 0.1319\n",
      "46/223, train_loss: 0.1047, step time: 0.1117\n",
      "47/223, train_loss: 0.1085, step time: 0.1200\n",
      "48/223, train_loss: 0.1000, step time: 0.1208\n",
      "49/223, train_loss: 0.1016, step time: 0.1275\n",
      "50/223, train_loss: 0.1051, step time: 0.1017\n",
      "51/223, train_loss: 0.0997, step time: 0.1145\n",
      "52/223, train_loss: 0.1014, step time: 0.1152\n",
      "53/223, train_loss: 0.1102, step time: 0.1109\n",
      "54/223, train_loss: 0.1012, step time: 0.1141\n",
      "55/223, train_loss: 0.0938, step time: 0.1264\n",
      "56/223, train_loss: 0.0967, step time: 0.1185\n",
      "57/223, train_loss: 0.1046, step time: 0.1045\n",
      "58/223, train_loss: 0.1045, step time: 0.1153\n",
      "59/223, train_loss: 0.0970, step time: 0.1401\n",
      "60/223, train_loss: 0.1024, step time: 0.1124\n",
      "61/223, train_loss: 0.0884, step time: 0.1136\n",
      "62/223, train_loss: 0.1138, step time: 0.1049\n",
      "63/223, train_loss: 0.0971, step time: 0.1157\n",
      "64/223, train_loss: 0.1105, step time: 0.1057\n",
      "65/223, train_loss: 0.1045, step time: 0.1002\n",
      "66/223, train_loss: 0.1083, step time: 0.1005\n",
      "67/223, train_loss: 0.0891, step time: 0.1095\n",
      "68/223, train_loss: 0.0950, step time: 0.1356\n",
      "69/223, train_loss: 0.1114, step time: 0.0999\n",
      "70/223, train_loss: 0.0996, step time: 0.1103\n",
      "71/223, train_loss: 0.0936, step time: 0.1230\n",
      "72/223, train_loss: 0.1001, step time: 0.1042\n",
      "73/223, train_loss: 0.1030, step time: 0.1212\n",
      "74/223, train_loss: 0.0948, step time: 0.1168\n",
      "75/223, train_loss: 0.0997, step time: 0.1003\n",
      "76/223, train_loss: 0.1092, step time: 0.1020\n",
      "77/223, train_loss: 0.0998, step time: 0.1426\n",
      "78/223, train_loss: 0.0903, step time: 0.1006\n",
      "79/223, train_loss: 0.0891, step time: 0.1003\n",
      "80/223, train_loss: 0.0911, step time: 0.1193\n",
      "81/223, train_loss: 0.1031, step time: 0.1106\n",
      "82/223, train_loss: 0.1015, step time: 0.1229\n",
      "83/223, train_loss: 0.1005, step time: 0.1121\n",
      "84/223, train_loss: 0.0940, step time: 0.1173\n",
      "85/223, train_loss: 0.1048, step time: 0.1192\n",
      "86/223, train_loss: 0.0898, step time: 0.1024\n",
      "87/223, train_loss: 0.0990, step time: 0.1004\n",
      "88/223, train_loss: 0.1010, step time: 0.0999\n",
      "89/223, train_loss: 0.0894, step time: 0.1058\n",
      "90/223, train_loss: 0.0930, step time: 0.1092\n",
      "91/223, train_loss: 0.1020, step time: 0.1203\n",
      "92/223, train_loss: 0.0939, step time: 0.1289\n",
      "93/223, train_loss: 0.0988, step time: 0.1131\n",
      "94/223, train_loss: 0.0944, step time: 0.1065\n",
      "95/223, train_loss: 0.1003, step time: 0.1196\n",
      "96/223, train_loss: 0.0946, step time: 0.1129\n",
      "97/223, train_loss: 0.1032, step time: 0.1103\n",
      "98/223, train_loss: 0.1022, step time: 0.1175\n",
      "99/223, train_loss: 0.1087, step time: 0.0999\n",
      "100/223, train_loss: 0.0945, step time: 0.1200\n",
      "101/223, train_loss: 0.0967, step time: 0.1008\n",
      "102/223, train_loss: 0.1113, step time: 0.1005\n",
      "103/223, train_loss: 0.1050, step time: 0.1604\n",
      "104/223, train_loss: 0.0935, step time: 0.1294\n",
      "105/223, train_loss: 0.1083, step time: 0.1130\n",
      "106/223, train_loss: 0.0996, step time: 0.1388\n",
      "107/223, train_loss: 0.0956, step time: 0.1128\n",
      "108/223, train_loss: 0.0890, step time: 0.0988\n",
      "109/223, train_loss: 0.0985, step time: 0.1314\n",
      "110/223, train_loss: 0.0918, step time: 0.1121\n",
      "111/223, train_loss: 0.0882, step time: 0.1090\n",
      "112/223, train_loss: 0.0935, step time: 0.1079\n",
      "113/223, train_loss: 0.0933, step time: 0.1108\n",
      "114/223, train_loss: 0.1121, step time: 0.1103\n",
      "115/223, train_loss: 0.0965, step time: 0.1022\n",
      "116/223, train_loss: 0.0893, step time: 0.1010\n",
      "117/223, train_loss: 0.1050, step time: 0.1215\n",
      "118/223, train_loss: 0.1059, step time: 0.1148\n",
      "119/223, train_loss: 0.1007, step time: 0.1160\n",
      "120/223, train_loss: 0.0963, step time: 0.1150\n",
      "121/223, train_loss: 0.1073, step time: 0.1091\n",
      "122/223, train_loss: 0.0916, step time: 0.1134\n",
      "123/223, train_loss: 0.0948, step time: 0.1220\n",
      "124/223, train_loss: 0.0948, step time: 0.1021\n",
      "125/223, train_loss: 0.0915, step time: 0.1148\n",
      "126/223, train_loss: 0.1145, step time: 0.1113\n",
      "127/223, train_loss: 0.0984, step time: 0.1121\n",
      "128/223, train_loss: 0.1003, step time: 0.1349\n",
      "129/223, train_loss: 0.1093, step time: 0.1221\n",
      "130/223, train_loss: 0.1076, step time: 0.1099\n",
      "131/223, train_loss: 0.0962, step time: 0.1091\n",
      "132/223, train_loss: 0.1075, step time: 0.1209\n",
      "133/223, train_loss: 0.1093, step time: 0.1113\n",
      "134/223, train_loss: 0.0917, step time: 0.1325\n",
      "135/223, train_loss: 0.1001, step time: 0.1016\n",
      "136/223, train_loss: 0.1009, step time: 0.0994\n",
      "137/223, train_loss: 0.1120, step time: 0.1089\n",
      "138/223, train_loss: 0.0849, step time: 0.1388\n",
      "139/223, train_loss: 0.0991, step time: 0.1086\n",
      "140/223, train_loss: 0.1039, step time: 0.1468\n",
      "141/223, train_loss: 0.0910, step time: 0.1159\n",
      "142/223, train_loss: 0.0921, step time: 0.1120\n",
      "143/223, train_loss: 0.0969, step time: 0.1297\n",
      "144/223, train_loss: 0.0949, step time: 0.1072\n",
      "145/223, train_loss: 0.0989, step time: 0.1137\n",
      "146/223, train_loss: 0.0927, step time: 0.1001\n",
      "147/223, train_loss: 0.1056, step time: 0.1221\n",
      "148/223, train_loss: 0.0955, step time: 0.1106\n",
      "149/223, train_loss: 0.0926, step time: 0.1045\n",
      "150/223, train_loss: 0.1129, step time: 0.1290\n",
      "151/223, train_loss: 0.0963, step time: 0.1078\n",
      "152/223, train_loss: 0.0944, step time: 0.1094\n",
      "153/223, train_loss: 0.0977, step time: 0.1158\n",
      "154/223, train_loss: 0.1053, step time: 0.1100\n",
      "155/223, train_loss: 0.1073, step time: 0.1167\n",
      "156/223, train_loss: 0.1018, step time: 0.1223\n",
      "157/223, train_loss: 0.0976, step time: 0.1128\n",
      "158/223, train_loss: 0.0985, step time: 0.1060\n",
      "159/223, train_loss: 0.0947, step time: 0.1234\n",
      "160/223, train_loss: 0.0960, step time: 0.1181\n",
      "161/223, train_loss: 0.0952, step time: 0.1104\n",
      "162/223, train_loss: 0.1052, step time: 0.1046\n",
      "163/223, train_loss: 0.0922, step time: 0.1141\n",
      "164/223, train_loss: 0.0988, step time: 0.1117\n",
      "165/223, train_loss: 0.1040, step time: 0.1070\n",
      "166/223, train_loss: 0.1110, step time: 0.0999\n",
      "167/223, train_loss: 0.0976, step time: 0.1296\n",
      "168/223, train_loss: 0.1089, step time: 0.1274\n",
      "169/223, train_loss: 0.1078, step time: 0.1125\n",
      "170/223, train_loss: 0.0971, step time: 0.1011\n",
      "171/223, train_loss: 0.1044, step time: 0.1028\n",
      "172/223, train_loss: 0.1015, step time: 0.1001\n",
      "173/223, train_loss: 0.1010, step time: 0.1067\n",
      "174/223, train_loss: 0.0927, step time: 0.1005\n",
      "175/223, train_loss: 0.0980, step time: 0.1005\n",
      "176/223, train_loss: 0.1000, step time: 0.1232\n",
      "177/223, train_loss: 0.0903, step time: 0.1005\n",
      "178/223, train_loss: 0.0962, step time: 0.1146\n",
      "179/223, train_loss: 0.0992, step time: 0.1093\n",
      "180/223, train_loss: 0.0906, step time: 0.1017\n",
      "181/223, train_loss: 0.1052, step time: 0.1241\n",
      "182/223, train_loss: 0.0982, step time: 0.1228\n",
      "183/223, train_loss: 0.0951, step time: 0.0997\n",
      "184/223, train_loss: 0.0975, step time: 0.1014\n",
      "185/223, train_loss: 0.0944, step time: 0.1112\n",
      "186/223, train_loss: 0.1024, step time: 0.1002\n",
      "187/223, train_loss: 0.0933, step time: 0.1344\n",
      "188/223, train_loss: 0.1000, step time: 0.1008\n",
      "189/223, train_loss: 0.1243, step time: 0.1256\n",
      "190/223, train_loss: 0.1031, step time: 0.1104\n",
      "191/223, train_loss: 0.1111, step time: 0.0998\n",
      "192/223, train_loss: 0.1057, step time: 0.1007\n",
      "193/223, train_loss: 0.1011, step time: 0.1047\n",
      "194/223, train_loss: 0.0948, step time: 0.1107\n",
      "195/223, train_loss: 0.0926, step time: 0.1267\n",
      "196/223, train_loss: 0.0983, step time: 0.1028\n",
      "197/223, train_loss: 0.1057, step time: 0.1286\n",
      "198/223, train_loss: 0.1101, step time: 0.1067\n",
      "199/223, train_loss: 0.1071, step time: 0.1071\n",
      "200/223, train_loss: 0.0917, step time: 0.1072\n",
      "201/223, train_loss: 0.0889, step time: 0.1082\n",
      "202/223, train_loss: 0.0985, step time: 0.1013\n",
      "203/223, train_loss: 0.0917, step time: 0.1154\n",
      "204/223, train_loss: 0.1059, step time: 0.1000\n",
      "205/223, train_loss: 0.0890, step time: 0.1125\n",
      "206/223, train_loss: 0.0972, step time: 0.1138\n",
      "207/223, train_loss: 0.0989, step time: 0.1480\n",
      "208/223, train_loss: 0.1039, step time: 0.1171\n",
      "209/223, train_loss: 0.1079, step time: 0.1043\n",
      "210/223, train_loss: 0.0955, step time: 0.1090\n",
      "211/223, train_loss: 0.1044, step time: 0.1023\n",
      "212/223, train_loss: 0.0947, step time: 0.1110\n",
      "213/223, train_loss: 0.0942, step time: 0.0995\n",
      "214/223, train_loss: 0.1054, step time: 0.0999\n",
      "215/223, train_loss: 0.0956, step time: 0.1005\n",
      "216/223, train_loss: 0.1055, step time: 0.1244\n",
      "217/223, train_loss: 0.1108, step time: 0.1014\n",
      "218/223, train_loss: 0.1102, step time: 0.1000\n",
      "219/223, train_loss: 0.1035, step time: 0.1089\n",
      "220/223, train_loss: 0.0935, step time: 0.1012\n",
      "221/223, train_loss: 0.0948, step time: 0.0997\n",
      "222/223, train_loss: 0.1181, step time: 0.1001\n",
      "223/223, train_loss: 0.0945, step time: 0.1019\n",
      "epoch 231 average loss: 0.1007\n",
      "time consuming of epoch 231 is: 87.5528\n",
      "----------\n",
      "epoch 232/300\n",
      "1/223, train_loss: 0.1012, step time: 0.1023\n",
      "2/223, train_loss: 0.0984, step time: 0.1125\n",
      "3/223, train_loss: 0.0977, step time: 0.1191\n",
      "4/223, train_loss: 0.1084, step time: 0.1048\n",
      "5/223, train_loss: 0.0882, step time: 0.1136\n",
      "6/223, train_loss: 0.1084, step time: 0.1220\n",
      "7/223, train_loss: 0.0990, step time: 0.1025\n",
      "8/223, train_loss: 0.0958, step time: 0.1181\n",
      "9/223, train_loss: 0.1022, step time: 0.1070\n",
      "10/223, train_loss: 0.1113, step time: 0.1004\n",
      "11/223, train_loss: 0.0971, step time: 0.1181\n",
      "12/223, train_loss: 0.0918, step time: 0.1012\n",
      "13/223, train_loss: 0.0960, step time: 0.1279\n",
      "14/223, train_loss: 0.1047, step time: 0.1021\n",
      "15/223, train_loss: 0.0954, step time: 0.1047\n",
      "16/223, train_loss: 0.1050, step time: 0.1005\n",
      "17/223, train_loss: 0.1039, step time: 0.1087\n",
      "18/223, train_loss: 0.0966, step time: 0.1076\n",
      "19/223, train_loss: 0.1210, step time: 0.1229\n",
      "20/223, train_loss: 0.1004, step time: 0.1120\n",
      "21/223, train_loss: 0.1027, step time: 0.1389\n",
      "22/223, train_loss: 0.0900, step time: 0.1411\n",
      "23/223, train_loss: 0.1028, step time: 0.0997\n",
      "24/223, train_loss: 0.0906, step time: 0.1010\n",
      "25/223, train_loss: 0.0942, step time: 0.1084\n",
      "26/223, train_loss: 0.0969, step time: 0.1041\n",
      "27/223, train_loss: 0.1020, step time: 0.1118\n",
      "28/223, train_loss: 0.1020, step time: 0.1086\n",
      "29/223, train_loss: 0.0945, step time: 0.1043\n",
      "30/223, train_loss: 0.0977, step time: 0.1204\n",
      "31/223, train_loss: 0.1008, step time: 0.1197\n",
      "32/223, train_loss: 0.1087, step time: 0.1039\n",
      "33/223, train_loss: 0.0918, step time: 0.1205\n",
      "34/223, train_loss: 0.1049, step time: 0.1269\n",
      "35/223, train_loss: 0.1060, step time: 0.1165\n",
      "36/223, train_loss: 0.1112, step time: 0.0996\n",
      "37/223, train_loss: 0.1027, step time: 0.1095\n",
      "38/223, train_loss: 0.0936, step time: 0.1016\n",
      "39/223, train_loss: 0.1009, step time: 0.1039\n",
      "40/223, train_loss: 0.1058, step time: 0.1491\n",
      "41/223, train_loss: 0.1018, step time: 0.1137\n",
      "42/223, train_loss: 0.0994, step time: 0.1122\n",
      "43/223, train_loss: 0.1078, step time: 0.1289\n",
      "44/223, train_loss: 0.0945, step time: 0.0991\n",
      "45/223, train_loss: 0.1010, step time: 0.0997\n",
      "46/223, train_loss: 0.0948, step time: 0.1053\n",
      "47/223, train_loss: 0.1001, step time: 0.1308\n",
      "48/223, train_loss: 0.0928, step time: 0.0995\n",
      "49/223, train_loss: 0.0948, step time: 0.1190\n",
      "50/223, train_loss: 0.1049, step time: 0.1098\n",
      "51/223, train_loss: 0.0877, step time: 0.1183\n",
      "52/223, train_loss: 0.0856, step time: 0.1036\n",
      "53/223, train_loss: 0.0945, step time: 0.1215\n",
      "54/223, train_loss: 0.0974, step time: 0.1001\n",
      "55/223, train_loss: 0.0999, step time: 0.1122\n",
      "56/223, train_loss: 0.1028, step time: 0.1204\n",
      "57/223, train_loss: 0.0991, step time: 0.1415\n",
      "58/223, train_loss: 0.1056, step time: 0.1015\n",
      "59/223, train_loss: 0.1040, step time: 0.1006\n",
      "60/223, train_loss: 0.0886, step time: 0.1068\n",
      "61/223, train_loss: 0.1011, step time: 0.1084\n",
      "62/223, train_loss: 0.0988, step time: 0.1002\n",
      "63/223, train_loss: 0.0901, step time: 0.0997\n",
      "64/223, train_loss: 0.0892, step time: 0.1125\n",
      "65/223, train_loss: 0.1039, step time: 0.1008\n",
      "66/223, train_loss: 0.1010, step time: 0.1244\n",
      "67/223, train_loss: 0.0891, step time: 0.1129\n",
      "68/223, train_loss: 0.0907, step time: 0.1143\n",
      "69/223, train_loss: 0.0986, step time: 0.1081\n",
      "70/223, train_loss: 0.1014, step time: 0.1096\n",
      "71/223, train_loss: 0.0966, step time: 0.1140\n",
      "72/223, train_loss: 0.0921, step time: 0.1144\n",
      "73/223, train_loss: 0.0936, step time: 0.1203\n",
      "74/223, train_loss: 0.0939, step time: 0.1078\n",
      "75/223, train_loss: 0.0973, step time: 0.1158\n",
      "76/223, train_loss: 0.1096, step time: 0.1159\n",
      "77/223, train_loss: 0.3081, step time: 0.1203\n",
      "78/223, train_loss: 0.1123, step time: 0.1073\n",
      "79/223, train_loss: 0.1017, step time: 0.1093\n",
      "80/223, train_loss: 0.1017, step time: 0.1052\n",
      "81/223, train_loss: 0.1050, step time: 0.1358\n",
      "82/223, train_loss: 0.1075, step time: 0.1107\n",
      "83/223, train_loss: 0.0992, step time: 0.1039\n",
      "84/223, train_loss: 0.0997, step time: 0.1307\n",
      "85/223, train_loss: 0.1002, step time: 0.0997\n",
      "86/223, train_loss: 0.0961, step time: 0.0996\n",
      "87/223, train_loss: 0.0902, step time: 0.1005\n",
      "88/223, train_loss: 0.0930, step time: 0.1009\n",
      "89/223, train_loss: 0.0893, step time: 0.0989\n",
      "90/223, train_loss: 0.0928, step time: 0.0989\n",
      "91/223, train_loss: 0.1330, step time: 0.0982\n",
      "92/223, train_loss: 0.1127, step time: 0.1080\n",
      "93/223, train_loss: 0.0889, step time: 0.0994\n",
      "94/223, train_loss: 0.1015, step time: 0.0992\n",
      "95/223, train_loss: 0.1031, step time: 0.0998\n",
      "96/223, train_loss: 0.1048, step time: 0.1005\n",
      "97/223, train_loss: 0.0886, step time: 0.1000\n",
      "98/223, train_loss: 0.1019, step time: 0.0993\n",
      "99/223, train_loss: 0.0940, step time: 0.0996\n",
      "100/223, train_loss: 0.0959, step time: 0.1003\n",
      "101/223, train_loss: 0.1014, step time: 0.1102\n",
      "102/223, train_loss: 0.0959, step time: 0.0992\n",
      "103/223, train_loss: 0.0937, step time: 0.0994\n",
      "104/223, train_loss: 0.1123, step time: 0.0993\n",
      "105/223, train_loss: 0.0957, step time: 0.1088\n",
      "106/223, train_loss: 0.1040, step time: 0.1003\n",
      "107/223, train_loss: 0.1015, step time: 0.0989\n",
      "108/223, train_loss: 0.1134, step time: 0.1099\n",
      "109/223, train_loss: 0.0964, step time: 0.1358\n",
      "110/223, train_loss: 0.0897, step time: 0.1006\n",
      "111/223, train_loss: 0.1008, step time: 0.1004\n",
      "112/223, train_loss: 0.0956, step time: 0.1009\n",
      "113/223, train_loss: 0.0968, step time: 0.1190\n",
      "114/223, train_loss: 0.1090, step time: 0.1151\n",
      "115/223, train_loss: 0.0979, step time: 0.1206\n",
      "116/223, train_loss: 0.0924, step time: 0.1138\n",
      "117/223, train_loss: 0.1055, step time: 0.0997\n",
      "118/223, train_loss: 0.0998, step time: 0.0994\n",
      "119/223, train_loss: 0.1041, step time: 0.0985\n",
      "120/223, train_loss: 0.0983, step time: 0.1076\n",
      "121/223, train_loss: 0.1014, step time: 0.1201\n",
      "122/223, train_loss: 0.0930, step time: 0.1816\n",
      "123/223, train_loss: 0.0993, step time: 0.1073\n",
      "124/223, train_loss: 0.1029, step time: 0.1011\n",
      "125/223, train_loss: 0.0993, step time: 0.1175\n",
      "126/223, train_loss: 0.1068, step time: 0.1086\n",
      "127/223, train_loss: 0.1006, step time: 0.1000\n",
      "128/223, train_loss: 0.1023, step time: 0.1005\n",
      "129/223, train_loss: 0.1004, step time: 0.1406\n",
      "130/223, train_loss: 0.1026, step time: 0.1120\n",
      "131/223, train_loss: 0.1013, step time: 0.1008\n",
      "132/223, train_loss: 0.1004, step time: 0.1122\n",
      "133/223, train_loss: 0.1025, step time: 0.1069\n",
      "134/223, train_loss: 0.1021, step time: 0.1003\n",
      "135/223, train_loss: 0.0939, step time: 0.1131\n",
      "136/223, train_loss: 0.0883, step time: 0.1071\n",
      "137/223, train_loss: 0.1133, step time: 0.1002\n",
      "138/223, train_loss: 0.1044, step time: 0.1000\n",
      "139/223, train_loss: 0.0981, step time: 0.0999\n",
      "140/223, train_loss: 0.0924, step time: 0.1184\n",
      "141/223, train_loss: 0.1001, step time: 0.1125\n",
      "142/223, train_loss: 0.1026, step time: 0.1059\n",
      "143/223, train_loss: 0.0963, step time: 0.1426\n",
      "144/223, train_loss: 0.1094, step time: 0.1006\n",
      "145/223, train_loss: 0.0997, step time: 0.1169\n",
      "146/223, train_loss: 0.0889, step time: 0.1066\n",
      "147/223, train_loss: 0.1003, step time: 0.0999\n",
      "148/223, train_loss: 0.1005, step time: 0.1012\n",
      "149/223, train_loss: 0.0938, step time: 0.1045\n",
      "150/223, train_loss: 0.0970, step time: 0.1120\n",
      "151/223, train_loss: 0.1154, step time: 0.1272\n",
      "152/223, train_loss: 0.0941, step time: 0.1001\n",
      "153/223, train_loss: 0.1050, step time: 0.1217\n",
      "154/223, train_loss: 0.0946, step time: 0.1403\n",
      "155/223, train_loss: 0.0987, step time: 0.1296\n",
      "156/223, train_loss: 0.1081, step time: 0.1069\n",
      "157/223, train_loss: 0.1065, step time: 0.1166\n",
      "158/223, train_loss: 0.1034, step time: 0.1137\n",
      "159/223, train_loss: 0.1128, step time: 0.1089\n",
      "160/223, train_loss: 0.0920, step time: 0.1166\n",
      "161/223, train_loss: 0.0958, step time: 0.1133\n",
      "162/223, train_loss: 0.1029, step time: 0.1184\n",
      "163/223, train_loss: 0.0995, step time: 0.1323\n",
      "164/223, train_loss: 0.1062, step time: 0.1148\n",
      "165/223, train_loss: 0.0905, step time: 0.1159\n",
      "166/223, train_loss: 0.1032, step time: 0.1208\n",
      "167/223, train_loss: 0.0990, step time: 0.1214\n",
      "168/223, train_loss: 0.1049, step time: 0.1414\n",
      "169/223, train_loss: 0.0940, step time: 0.1000\n",
      "170/223, train_loss: 0.0940, step time: 0.0992\n",
      "171/223, train_loss: 0.1129, step time: 0.1000\n",
      "172/223, train_loss: 0.1056, step time: 0.1073\n",
      "173/223, train_loss: 0.1137, step time: 0.0997\n",
      "174/223, train_loss: 0.0982, step time: 0.1000\n",
      "175/223, train_loss: 0.0882, step time: 0.0997\n",
      "176/223, train_loss: 0.0937, step time: 0.1326\n",
      "177/223, train_loss: 0.0970, step time: 0.0997\n",
      "178/223, train_loss: 0.1140, step time: 0.0993\n",
      "179/223, train_loss: 0.1030, step time: 0.1012\n",
      "180/223, train_loss: 0.0948, step time: 0.1014\n",
      "181/223, train_loss: 0.1021, step time: 0.0994\n",
      "182/223, train_loss: 0.1017, step time: 0.0984\n",
      "183/223, train_loss: 0.0982, step time: 0.0999\n",
      "184/223, train_loss: 0.0975, step time: 0.1191\n",
      "185/223, train_loss: 0.1038, step time: 0.0997\n",
      "186/223, train_loss: 0.0970, step time: 0.1044\n",
      "187/223, train_loss: 0.0992, step time: 0.1374\n",
      "188/223, train_loss: 0.1067, step time: 0.1157\n",
      "189/223, train_loss: 0.1033, step time: 0.1169\n",
      "190/223, train_loss: 0.0946, step time: 0.1170\n",
      "191/223, train_loss: 0.1033, step time: 0.1044\n",
      "192/223, train_loss: 0.1047, step time: 0.1031\n",
      "193/223, train_loss: 0.1008, step time: 0.1116\n",
      "194/223, train_loss: 0.1095, step time: 0.1137\n",
      "195/223, train_loss: 0.0908, step time: 0.1263\n",
      "196/223, train_loss: 0.0943, step time: 0.1077\n",
      "197/223, train_loss: 0.1039, step time: 0.1173\n",
      "198/223, train_loss: 0.0913, step time: 0.1098\n",
      "199/223, train_loss: 0.1026, step time: 0.1153\n",
      "200/223, train_loss: 0.1039, step time: 0.1108\n",
      "201/223, train_loss: 0.0939, step time: 0.1039\n",
      "202/223, train_loss: 0.0989, step time: 0.1049\n",
      "203/223, train_loss: 0.1057, step time: 0.0986\n",
      "204/223, train_loss: 0.0903, step time: 0.0991\n",
      "205/223, train_loss: 0.1047, step time: 0.1381\n",
      "206/223, train_loss: 0.0992, step time: 0.1026\n",
      "207/223, train_loss: 0.0876, step time: 0.1195\n",
      "208/223, train_loss: 0.0942, step time: 0.1148\n",
      "209/223, train_loss: 0.1017, step time: 0.1102\n",
      "210/223, train_loss: 0.1065, step time: 0.1071\n",
      "211/223, train_loss: 0.1017, step time: 0.0997\n",
      "212/223, train_loss: 0.1175, step time: 0.1001\n",
      "213/223, train_loss: 0.1016, step time: 0.1265\n",
      "214/223, train_loss: 0.0946, step time: 0.1392\n",
      "215/223, train_loss: 0.0998, step time: 0.1348\n",
      "216/223, train_loss: 0.0995, step time: 0.1000\n",
      "217/223, train_loss: 0.0965, step time: 0.1020\n",
      "218/223, train_loss: 0.0984, step time: 0.1004\n",
      "219/223, train_loss: 0.0939, step time: 0.1064\n",
      "220/223, train_loss: 0.0952, step time: 0.1238\n",
      "221/223, train_loss: 0.0942, step time: 0.1003\n",
      "222/223, train_loss: 0.1027, step time: 0.1003\n",
      "223/223, train_loss: 0.0990, step time: 0.0998\n",
      "epoch 232 average loss: 0.1008\n",
      "time consuming of epoch 232 is: 96.1068\n",
      "----------\n",
      "epoch 233/300\n",
      "1/223, train_loss: 0.1034, step time: 0.1172\n",
      "2/223, train_loss: 0.0976, step time: 0.1006\n",
      "3/223, train_loss: 0.0991, step time: 0.1016\n",
      "4/223, train_loss: 0.1121, step time: 0.1014\n",
      "5/223, train_loss: 0.0927, step time: 0.1120\n",
      "6/223, train_loss: 0.1047, step time: 0.1220\n",
      "7/223, train_loss: 0.1021, step time: 0.1370\n",
      "8/223, train_loss: 0.1067, step time: 0.1207\n",
      "9/223, train_loss: 0.3053, step time: 0.1005\n",
      "10/223, train_loss: 0.0897, step time: 0.1191\n",
      "11/223, train_loss: 0.1031, step time: 0.1004\n",
      "12/223, train_loss: 0.0976, step time: 0.1023\n",
      "13/223, train_loss: 0.1003, step time: 0.1007\n",
      "14/223, train_loss: 0.0896, step time: 0.1153\n",
      "15/223, train_loss: 0.0949, step time: 0.1159\n",
      "16/223, train_loss: 0.0988, step time: 0.1179\n",
      "17/223, train_loss: 0.0902, step time: 0.1063\n",
      "18/223, train_loss: 0.0976, step time: 0.1122\n",
      "19/223, train_loss: 0.0865, step time: 0.1121\n",
      "20/223, train_loss: 0.0946, step time: 0.1144\n",
      "21/223, train_loss: 0.0921, step time: 0.1255\n",
      "22/223, train_loss: 0.1180, step time: 0.1001\n",
      "23/223, train_loss: 0.1003, step time: 0.1163\n",
      "24/223, train_loss: 0.0970, step time: 0.1091\n",
      "25/223, train_loss: 0.0920, step time: 0.1108\n",
      "26/223, train_loss: 0.0923, step time: 0.1117\n",
      "27/223, train_loss: 0.0995, step time: 0.1027\n",
      "28/223, train_loss: 0.1007, step time: 0.1142\n",
      "29/223, train_loss: 0.0919, step time: 0.1054\n",
      "30/223, train_loss: 0.0946, step time: 0.1124\n",
      "31/223, train_loss: 0.0999, step time: 0.1074\n",
      "32/223, train_loss: 0.0931, step time: 0.1002\n",
      "33/223, train_loss: 0.1120, step time: 0.1091\n",
      "34/223, train_loss: 0.0990, step time: 0.1007\n",
      "35/223, train_loss: 0.0949, step time: 0.1106\n",
      "36/223, train_loss: 0.1141, step time: 0.1021\n",
      "37/223, train_loss: 0.1026, step time: 0.0998\n",
      "38/223, train_loss: 0.0975, step time: 0.1142\n",
      "39/223, train_loss: 0.0911, step time: 0.1004\n",
      "40/223, train_loss: 0.1066, step time: 0.1001\n",
      "41/223, train_loss: 0.0939, step time: 0.1080\n",
      "42/223, train_loss: 0.0964, step time: 0.1005\n",
      "43/223, train_loss: 0.1003, step time: 0.1008\n",
      "44/223, train_loss: 0.0992, step time: 0.1007\n",
      "45/223, train_loss: 0.0948, step time: 0.1119\n",
      "46/223, train_loss: 0.1034, step time: 0.0998\n",
      "47/223, train_loss: 0.1043, step time: 0.1012\n",
      "48/223, train_loss: 0.0987, step time: 0.1006\n",
      "49/223, train_loss: 0.1117, step time: 0.1144\n",
      "50/223, train_loss: 0.1000, step time: 0.1001\n",
      "51/223, train_loss: 0.0975, step time: 0.0999\n",
      "52/223, train_loss: 0.1069, step time: 0.0993\n",
      "53/223, train_loss: 0.1119, step time: 0.1130\n",
      "54/223, train_loss: 0.1015, step time: 0.1138\n",
      "55/223, train_loss: 0.0940, step time: 0.1014\n",
      "56/223, train_loss: 0.0949, step time: 0.1185\n",
      "57/223, train_loss: 0.1011, step time: 0.1213\n",
      "58/223, train_loss: 0.0924, step time: 0.0998\n",
      "59/223, train_loss: 0.0893, step time: 0.1094\n",
      "60/223, train_loss: 0.0919, step time: 0.1002\n",
      "61/223, train_loss: 0.0911, step time: 0.1230\n",
      "62/223, train_loss: 0.1016, step time: 0.1216\n",
      "63/223, train_loss: 0.1083, step time: 0.1195\n",
      "64/223, train_loss: 0.0978, step time: 0.1267\n",
      "65/223, train_loss: 0.1052, step time: 0.1582\n",
      "66/223, train_loss: 0.0940, step time: 0.1150\n",
      "67/223, train_loss: 0.1046, step time: 0.1486\n",
      "68/223, train_loss: 0.0884, step time: 0.1563\n",
      "69/223, train_loss: 0.0965, step time: 0.1087\n",
      "70/223, train_loss: 0.1010, step time: 0.1126\n",
      "71/223, train_loss: 0.1056, step time: 0.1164\n",
      "72/223, train_loss: 0.0878, step time: 0.1376\n",
      "73/223, train_loss: 0.1020, step time: 0.1322\n",
      "74/223, train_loss: 0.0958, step time: 0.1012\n",
      "75/223, train_loss: 0.1052, step time: 0.1002\n",
      "76/223, train_loss: 0.0957, step time: 0.1012\n",
      "77/223, train_loss: 0.0976, step time: 0.1375\n",
      "78/223, train_loss: 0.0950, step time: 0.1121\n",
      "79/223, train_loss: 0.0999, step time: 0.1055\n",
      "80/223, train_loss: 0.1006, step time: 0.1251\n",
      "81/223, train_loss: 0.1047, step time: 0.1384\n",
      "82/223, train_loss: 0.0881, step time: 0.1144\n",
      "83/223, train_loss: 0.1005, step time: 0.1086\n",
      "84/223, train_loss: 0.0992, step time: 0.1098\n",
      "85/223, train_loss: 0.0969, step time: 0.1187\n",
      "86/223, train_loss: 0.0872, step time: 0.1052\n",
      "87/223, train_loss: 0.1030, step time: 0.1006\n",
      "88/223, train_loss: 0.0933, step time: 0.1072\n",
      "89/223, train_loss: 0.0963, step time: 0.1154\n",
      "90/223, train_loss: 0.0897, step time: 0.1075\n",
      "91/223, train_loss: 0.1010, step time: 0.1291\n",
      "92/223, train_loss: 0.0950, step time: 0.1001\n",
      "93/223, train_loss: 0.0943, step time: 0.1130\n",
      "94/223, train_loss: 0.0937, step time: 0.1208\n",
      "95/223, train_loss: 0.1007, step time: 0.1246\n",
      "96/223, train_loss: 0.1052, step time: 0.1243\n",
      "97/223, train_loss: 0.0939, step time: 0.1008\n",
      "98/223, train_loss: 0.0953, step time: 0.1200\n",
      "99/223, train_loss: 0.1066, step time: 0.1142\n",
      "100/223, train_loss: 0.0990, step time: 0.1141\n",
      "101/223, train_loss: 0.1038, step time: 0.1001\n",
      "102/223, train_loss: 0.1051, step time: 0.1121\n",
      "103/223, train_loss: 0.1063, step time: 0.1124\n",
      "104/223, train_loss: 0.1092, step time: 0.1057\n",
      "105/223, train_loss: 0.1027, step time: 0.1214\n",
      "106/223, train_loss: 0.0974, step time: 0.1111\n",
      "107/223, train_loss: 0.0955, step time: 0.1221\n",
      "108/223, train_loss: 0.1064, step time: 0.1206\n",
      "109/223, train_loss: 0.1041, step time: 0.1143\n",
      "110/223, train_loss: 0.1162, step time: 0.1175\n",
      "111/223, train_loss: 0.1095, step time: 0.1098\n",
      "112/223, train_loss: 0.1038, step time: 0.1243\n",
      "113/223, train_loss: 0.0912, step time: 0.1248\n",
      "114/223, train_loss: 0.1148, step time: 0.1056\n",
      "115/223, train_loss: 0.0962, step time: 0.1150\n",
      "116/223, train_loss: 0.1066, step time: 0.1277\n",
      "117/223, train_loss: 0.0903, step time: 0.1198\n",
      "118/223, train_loss: 0.1007, step time: 0.1011\n",
      "119/223, train_loss: 0.0997, step time: 0.1011\n",
      "120/223, train_loss: 0.0958, step time: 0.1010\n",
      "121/223, train_loss: 0.1002, step time: 0.1084\n",
      "122/223, train_loss: 0.1138, step time: 0.1109\n",
      "123/223, train_loss: 0.1001, step time: 0.1034\n",
      "124/223, train_loss: 0.1061, step time: 0.1220\n",
      "125/223, train_loss: 0.0926, step time: 0.1074\n",
      "126/223, train_loss: 0.1035, step time: 0.1190\n",
      "127/223, train_loss: 0.1006, step time: 0.1130\n",
      "128/223, train_loss: 0.1070, step time: 0.1294\n",
      "129/223, train_loss: 0.1035, step time: 0.1337\n",
      "130/223, train_loss: 0.1037, step time: 0.1153\n",
      "131/223, train_loss: 0.1131, step time: 0.1121\n",
      "132/223, train_loss: 0.0980, step time: 0.1370\n",
      "133/223, train_loss: 0.0843, step time: 0.1207\n",
      "134/223, train_loss: 0.0907, step time: 0.1003\n",
      "135/223, train_loss: 0.0914, step time: 0.1089\n",
      "136/223, train_loss: 0.0968, step time: 0.1032\n",
      "137/223, train_loss: 0.1006, step time: 0.1218\n",
      "138/223, train_loss: 0.1044, step time: 0.1014\n",
      "139/223, train_loss: 0.1050, step time: 0.1077\n",
      "140/223, train_loss: 0.0972, step time: 0.1020\n",
      "141/223, train_loss: 0.1000, step time: 0.1214\n",
      "142/223, train_loss: 0.1008, step time: 0.1148\n",
      "143/223, train_loss: 0.1011, step time: 0.1012\n",
      "144/223, train_loss: 0.1115, step time: 0.1007\n",
      "145/223, train_loss: 0.0879, step time: 0.1127\n",
      "146/223, train_loss: 0.1077, step time: 0.1105\n",
      "147/223, train_loss: 0.1026, step time: 0.1003\n",
      "148/223, train_loss: 0.1020, step time: 0.1003\n",
      "149/223, train_loss: 0.0922, step time: 0.1000\n",
      "150/223, train_loss: 0.0979, step time: 0.1078\n",
      "151/223, train_loss: 0.1064, step time: 0.1114\n",
      "152/223, train_loss: 0.1054, step time: 0.1001\n",
      "153/223, train_loss: 0.0976, step time: 0.1167\n",
      "154/223, train_loss: 0.0929, step time: 0.1122\n",
      "155/223, train_loss: 0.1017, step time: 0.1081\n",
      "156/223, train_loss: 0.1028, step time: 0.1196\n",
      "157/223, train_loss: 0.1017, step time: 0.1038\n",
      "158/223, train_loss: 0.1104, step time: 0.1047\n",
      "159/223, train_loss: 0.1014, step time: 0.1026\n",
      "160/223, train_loss: 0.1168, step time: 0.1102\n",
      "161/223, train_loss: 0.0945, step time: 0.1027\n",
      "162/223, train_loss: 0.0974, step time: 0.1144\n",
      "163/223, train_loss: 0.0912, step time: 0.1005\n",
      "164/223, train_loss: 0.1018, step time: 0.1000\n",
      "165/223, train_loss: 0.0979, step time: 0.1067\n",
      "166/223, train_loss: 0.1009, step time: 0.1107\n",
      "167/223, train_loss: 0.1015, step time: 0.1218\n",
      "168/223, train_loss: 0.0916, step time: 0.1163\n",
      "169/223, train_loss: 0.0954, step time: 0.1297\n",
      "170/223, train_loss: 0.1031, step time: 0.1137\n",
      "171/223, train_loss: 0.0976, step time: 0.1148\n",
      "172/223, train_loss: 0.0963, step time: 0.1753\n",
      "173/223, train_loss: 0.1045, step time: 0.1275\n",
      "174/223, train_loss: 0.1013, step time: 0.1264\n",
      "175/223, train_loss: 0.1033, step time: 0.1140\n",
      "176/223, train_loss: 0.1047, step time: 0.1060\n",
      "177/223, train_loss: 0.1040, step time: 0.1128\n",
      "178/223, train_loss: 0.0977, step time: 0.1164\n",
      "179/223, train_loss: 0.0976, step time: 0.1093\n",
      "180/223, train_loss: 0.0963, step time: 0.1001\n",
      "181/223, train_loss: 0.0939, step time: 0.1340\n",
      "182/223, train_loss: 0.1054, step time: 0.1319\n",
      "183/223, train_loss: 0.0970, step time: 0.1001\n",
      "184/223, train_loss: 0.1015, step time: 0.1000\n",
      "185/223, train_loss: 0.0937, step time: 0.1177\n",
      "186/223, train_loss: 0.1062, step time: 0.1237\n",
      "187/223, train_loss: 0.0921, step time: 0.1102\n",
      "188/223, train_loss: 0.0912, step time: 0.1080\n",
      "189/223, train_loss: 0.1008, step time: 0.1002\n",
      "190/223, train_loss: 0.1027, step time: 0.1137\n",
      "191/223, train_loss: 0.1047, step time: 0.1329\n",
      "192/223, train_loss: 0.0908, step time: 0.1164\n",
      "193/223, train_loss: 0.1005, step time: 0.1401\n",
      "194/223, train_loss: 0.1001, step time: 0.1109\n",
      "195/223, train_loss: 0.1134, step time: 0.1102\n",
      "196/223, train_loss: 0.0951, step time: 0.1074\n",
      "197/223, train_loss: 0.1079, step time: 0.1401\n",
      "198/223, train_loss: 0.0971, step time: 0.1047\n",
      "199/223, train_loss: 0.0977, step time: 0.1094\n",
      "200/223, train_loss: 0.0972, step time: 0.1095\n",
      "201/223, train_loss: 0.1056, step time: 0.1009\n",
      "202/223, train_loss: 0.0960, step time: 0.1217\n",
      "203/223, train_loss: 0.1095, step time: 0.1141\n",
      "204/223, train_loss: 0.0991, step time: 0.1198\n",
      "205/223, train_loss: 0.0903, step time: 0.1125\n",
      "206/223, train_loss: 0.1088, step time: 0.1179\n",
      "207/223, train_loss: 0.0914, step time: 0.1061\n",
      "208/223, train_loss: 0.0971, step time: 0.1287\n",
      "209/223, train_loss: 0.0993, step time: 0.1101\n",
      "210/223, train_loss: 0.1027, step time: 0.1229\n",
      "211/223, train_loss: 0.0947, step time: 0.1250\n",
      "212/223, train_loss: 0.1055, step time: 0.1163\n",
      "213/223, train_loss: 0.1135, step time: 0.0999\n",
      "214/223, train_loss: 0.0957, step time: 0.1177\n",
      "215/223, train_loss: 0.1150, step time: 0.1051\n",
      "216/223, train_loss: 0.0955, step time: 0.1056\n",
      "217/223, train_loss: 0.1017, step time: 0.1006\n",
      "218/223, train_loss: 0.0933, step time: 0.1009\n",
      "219/223, train_loss: 0.0967, step time: 0.1014\n",
      "220/223, train_loss: 0.0890, step time: 0.1017\n",
      "221/223, train_loss: 0.1000, step time: 0.1014\n",
      "222/223, train_loss: 0.0915, step time: 0.0992\n",
      "223/223, train_loss: 0.1048, step time: 0.1002\n",
      "epoch 233 average loss: 0.1006\n",
      "time consuming of epoch 233 is: 87.4195\n",
      "----------\n",
      "epoch 234/300\n",
      "1/223, train_loss: 0.0998, step time: 0.1072\n",
      "2/223, train_loss: 0.0937, step time: 0.1126\n",
      "3/223, train_loss: 0.0925, step time: 0.1155\n",
      "4/223, train_loss: 0.1087, step time: 0.1077\n",
      "5/223, train_loss: 0.1042, step time: 0.1070\n",
      "6/223, train_loss: 0.0987, step time: 0.1226\n",
      "7/223, train_loss: 0.1080, step time: 0.1327\n",
      "8/223, train_loss: 0.1003, step time: 0.1294\n",
      "9/223, train_loss: 0.1030, step time: 0.1135\n",
      "10/223, train_loss: 0.1018, step time: 0.1051\n",
      "11/223, train_loss: 0.1069, step time: 0.1158\n",
      "12/223, train_loss: 0.0998, step time: 0.1012\n",
      "13/223, train_loss: 0.1082, step time: 0.1157\n",
      "14/223, train_loss: 0.0927, step time: 0.1085\n",
      "15/223, train_loss: 0.0909, step time: 0.1220\n",
      "16/223, train_loss: 0.1009, step time: 0.1074\n",
      "17/223, train_loss: 0.1015, step time: 0.1059\n",
      "18/223, train_loss: 0.0956, step time: 0.1310\n",
      "19/223, train_loss: 0.1098, step time: 0.1217\n",
      "20/223, train_loss: 0.0984, step time: 0.1126\n",
      "21/223, train_loss: 0.1091, step time: 0.1110\n",
      "22/223, train_loss: 0.1006, step time: 0.1089\n",
      "23/223, train_loss: 0.0946, step time: 0.1161\n",
      "24/223, train_loss: 0.1031, step time: 0.1202\n",
      "25/223, train_loss: 0.0981, step time: 0.1166\n",
      "26/223, train_loss: 0.0940, step time: 0.1098\n",
      "27/223, train_loss: 0.1076, step time: 0.1133\n",
      "28/223, train_loss: 0.0962, step time: 0.1315\n",
      "29/223, train_loss: 0.0879, step time: 0.0997\n",
      "30/223, train_loss: 0.0987, step time: 0.0988\n",
      "31/223, train_loss: 0.1033, step time: 0.0995\n",
      "32/223, train_loss: 0.1051, step time: 0.1006\n",
      "33/223, train_loss: 0.1011, step time: 0.0994\n",
      "34/223, train_loss: 0.0997, step time: 0.0993\n",
      "35/223, train_loss: 0.1043, step time: 0.1003\n",
      "36/223, train_loss: 0.1008, step time: 0.1036\n",
      "37/223, train_loss: 0.1053, step time: 0.0991\n",
      "38/223, train_loss: 0.0936, step time: 0.0989\n",
      "39/223, train_loss: 0.0986, step time: 0.1005\n",
      "40/223, train_loss: 0.1057, step time: 0.1256\n",
      "41/223, train_loss: 0.0990, step time: 0.1000\n",
      "42/223, train_loss: 0.1054, step time: 0.0999\n",
      "43/223, train_loss: 0.0860, step time: 0.1007\n",
      "44/223, train_loss: 0.1008, step time: 0.1100\n",
      "45/223, train_loss: 0.1013, step time: 0.1000\n",
      "46/223, train_loss: 0.1052, step time: 0.0994\n",
      "47/223, train_loss: 0.1171, step time: 0.0995\n",
      "48/223, train_loss: 0.0965, step time: 0.1151\n",
      "49/223, train_loss: 0.1061, step time: 0.1003\n",
      "50/223, train_loss: 0.1071, step time: 0.1009\n",
      "51/223, train_loss: 0.0915, step time: 0.1007\n",
      "52/223, train_loss: 0.0965, step time: 0.1211\n",
      "53/223, train_loss: 0.1023, step time: 0.1221\n",
      "54/223, train_loss: 0.0982, step time: 0.1143\n",
      "55/223, train_loss: 0.0960, step time: 0.1268\n",
      "56/223, train_loss: 0.1004, step time: 0.1168\n",
      "57/223, train_loss: 0.0932, step time: 0.1056\n",
      "58/223, train_loss: 0.0928, step time: 0.1288\n",
      "59/223, train_loss: 0.0962, step time: 0.1219\n",
      "60/223, train_loss: 0.0932, step time: 0.1005\n",
      "61/223, train_loss: 0.0951, step time: 0.1217\n",
      "62/223, train_loss: 0.0911, step time: 0.1111\n",
      "63/223, train_loss: 0.1135, step time: 0.1425\n",
      "64/223, train_loss: 0.1081, step time: 0.1144\n",
      "65/223, train_loss: 0.1121, step time: 0.1114\n",
      "66/223, train_loss: 0.0911, step time: 0.1170\n",
      "67/223, train_loss: 0.0985, step time: 0.1123\n",
      "68/223, train_loss: 0.1025, step time: 0.1023\n",
      "69/223, train_loss: 0.1024, step time: 0.1096\n",
      "70/223, train_loss: 0.0968, step time: 0.1056\n",
      "71/223, train_loss: 0.1008, step time: 0.1058\n",
      "72/223, train_loss: 0.0915, step time: 0.1352\n",
      "73/223, train_loss: 0.0965, step time: 0.1020\n",
      "74/223, train_loss: 0.2974, step time: 0.1286\n",
      "75/223, train_loss: 0.0972, step time: 0.1119\n",
      "76/223, train_loss: 0.1055, step time: 0.1006\n",
      "77/223, train_loss: 0.0989, step time: 0.1019\n",
      "78/223, train_loss: 0.0886, step time: 0.1048\n",
      "79/223, train_loss: 0.0938, step time: 0.1238\n",
      "80/223, train_loss: 0.1095, step time: 0.1172\n",
      "81/223, train_loss: 0.1095, step time: 0.1091\n",
      "82/223, train_loss: 0.0971, step time: 0.1280\n",
      "83/223, train_loss: 0.0995, step time: 0.1158\n",
      "84/223, train_loss: 0.0925, step time: 0.1029\n",
      "85/223, train_loss: 0.0985, step time: 0.1112\n",
      "86/223, train_loss: 0.0964, step time: 0.1115\n",
      "87/223, train_loss: 0.1041, step time: 0.1133\n",
      "88/223, train_loss: 0.0953, step time: 0.1470\n",
      "89/223, train_loss: 0.0916, step time: 0.1134\n",
      "90/223, train_loss: 0.0948, step time: 0.1197\n",
      "91/223, train_loss: 0.1005, step time: 0.1281\n",
      "92/223, train_loss: 0.0902, step time: 0.1094\n",
      "93/223, train_loss: 0.0957, step time: 0.1221\n",
      "94/223, train_loss: 0.0932, step time: 0.1219\n",
      "95/223, train_loss: 0.0990, step time: 0.1325\n",
      "96/223, train_loss: 0.1156, step time: 0.1069\n",
      "97/223, train_loss: 0.1019, step time: 0.1011\n",
      "98/223, train_loss: 0.1005, step time: 0.1086\n",
      "99/223, train_loss: 0.1111, step time: 0.1006\n",
      "100/223, train_loss: 0.0955, step time: 0.0997\n",
      "101/223, train_loss: 0.0893, step time: 0.1140\n",
      "102/223, train_loss: 0.0983, step time: 0.1103\n",
      "103/223, train_loss: 0.1138, step time: 0.0995\n",
      "104/223, train_loss: 0.0952, step time: 0.0998\n",
      "105/223, train_loss: 0.1029, step time: 0.1129\n",
      "106/223, train_loss: 0.0971, step time: 0.1044\n",
      "107/223, train_loss: 0.0930, step time: 0.1193\n",
      "108/223, train_loss: 0.1033, step time: 0.1009\n",
      "109/223, train_loss: 0.1157, step time: 0.1183\n",
      "110/223, train_loss: 0.1040, step time: 0.1239\n",
      "111/223, train_loss: 0.0964, step time: 0.1238\n",
      "112/223, train_loss: 0.1003, step time: 0.1058\n",
      "113/223, train_loss: 0.0986, step time: 0.1128\n",
      "114/223, train_loss: 0.1045, step time: 0.1107\n",
      "115/223, train_loss: 0.0911, step time: 0.1003\n",
      "116/223, train_loss: 0.1026, step time: 0.1213\n",
      "117/223, train_loss: 0.1144, step time: 0.1122\n",
      "118/223, train_loss: 0.0899, step time: 0.1225\n",
      "119/223, train_loss: 0.1036, step time: 0.1009\n",
      "120/223, train_loss: 0.0929, step time: 0.1032\n",
      "121/223, train_loss: 0.1151, step time: 0.1156\n",
      "122/223, train_loss: 0.0983, step time: 0.1098\n",
      "123/223, train_loss: 0.0976, step time: 0.1032\n",
      "124/223, train_loss: 0.0934, step time: 0.1002\n",
      "125/223, train_loss: 0.0893, step time: 0.1161\n",
      "126/223, train_loss: 0.1049, step time: 0.1146\n",
      "127/223, train_loss: 0.1029, step time: 0.1130\n",
      "128/223, train_loss: 0.1086, step time: 0.1223\n",
      "129/223, train_loss: 0.0918, step time: 0.1082\n",
      "130/223, train_loss: 0.0915, step time: 0.1096\n",
      "131/223, train_loss: 0.0953, step time: 0.1000\n",
      "132/223, train_loss: 0.1050, step time: 0.1163\n",
      "133/223, train_loss: 0.0859, step time: 0.1150\n",
      "134/223, train_loss: 0.0936, step time: 0.1092\n",
      "135/223, train_loss: 0.0891, step time: 0.1096\n",
      "136/223, train_loss: 0.0996, step time: 0.1056\n",
      "137/223, train_loss: 0.1069, step time: 0.1092\n",
      "138/223, train_loss: 0.1020, step time: 0.1033\n",
      "139/223, train_loss: 0.1047, step time: 0.1061\n",
      "140/223, train_loss: 0.0894, step time: 0.1134\n",
      "141/223, train_loss: 0.0938, step time: 0.1054\n",
      "142/223, train_loss: 0.1025, step time: 0.1148\n",
      "143/223, train_loss: 0.0887, step time: 0.1019\n",
      "144/223, train_loss: 0.0915, step time: 0.1119\n",
      "145/223, train_loss: 0.1034, step time: 0.1093\n",
      "146/223, train_loss: 0.0917, step time: 0.1123\n",
      "147/223, train_loss: 0.1017, step time: 0.1114\n",
      "148/223, train_loss: 0.1039, step time: 0.1046\n",
      "149/223, train_loss: 0.0882, step time: 0.1003\n",
      "150/223, train_loss: 0.0931, step time: 0.1075\n",
      "151/223, train_loss: 0.1102, step time: 0.1310\n",
      "152/223, train_loss: 0.1009, step time: 0.1029\n",
      "153/223, train_loss: 0.1094, step time: 0.1123\n",
      "154/223, train_loss: 0.0942, step time: 0.1058\n",
      "155/223, train_loss: 0.0888, step time: 0.1002\n",
      "156/223, train_loss: 0.1082, step time: 0.1009\n",
      "157/223, train_loss: 0.0882, step time: 0.1126\n",
      "158/223, train_loss: 0.0940, step time: 0.1011\n",
      "159/223, train_loss: 0.0968, step time: 0.1001\n",
      "160/223, train_loss: 0.0966, step time: 0.1024\n",
      "161/223, train_loss: 0.1030, step time: 0.1133\n",
      "162/223, train_loss: 0.0939, step time: 0.1180\n",
      "163/223, train_loss: 0.0952, step time: 0.1433\n",
      "164/223, train_loss: 0.0986, step time: 0.1157\n",
      "165/223, train_loss: 0.0941, step time: 0.1214\n",
      "166/223, train_loss: 0.0981, step time: 0.1084\n",
      "167/223, train_loss: 0.1029, step time: 0.1164\n",
      "168/223, train_loss: 0.1055, step time: 0.1099\n",
      "169/223, train_loss: 0.0872, step time: 0.1024\n",
      "170/223, train_loss: 0.0933, step time: 0.1238\n",
      "171/223, train_loss: 0.0962, step time: 0.1117\n",
      "172/223, train_loss: 0.1062, step time: 0.1155\n",
      "173/223, train_loss: 0.0946, step time: 0.1148\n",
      "174/223, train_loss: 0.0974, step time: 0.1016\n",
      "175/223, train_loss: 0.1070, step time: 0.1006\n",
      "176/223, train_loss: 0.1113, step time: 0.1008\n",
      "177/223, train_loss: 0.0969, step time: 0.1163\n",
      "178/223, train_loss: 0.0999, step time: 0.1236\n",
      "179/223, train_loss: 0.1182, step time: 0.1122\n",
      "180/223, train_loss: 0.0994, step time: 0.1006\n",
      "181/223, train_loss: 0.1069, step time: 0.1042\n",
      "182/223, train_loss: 0.1093, step time: 0.1085\n",
      "183/223, train_loss: 0.1017, step time: 0.1002\n",
      "184/223, train_loss: 0.1043, step time: 0.1078\n",
      "185/223, train_loss: 0.0944, step time: 0.1247\n",
      "186/223, train_loss: 0.0976, step time: 0.0999\n",
      "187/223, train_loss: 0.1045, step time: 0.1022\n",
      "188/223, train_loss: 0.1074, step time: 0.1243\n",
      "189/223, train_loss: 0.1077, step time: 0.1156\n",
      "190/223, train_loss: 0.0934, step time: 0.1073\n",
      "191/223, train_loss: 0.1119, step time: 0.1224\n",
      "192/223, train_loss: 0.1014, step time: 0.1011\n",
      "193/223, train_loss: 0.1059, step time: 0.1039\n",
      "194/223, train_loss: 0.1083, step time: 0.0994\n",
      "195/223, train_loss: 0.0941, step time: 0.1317\n",
      "196/223, train_loss: 0.0890, step time: 0.1045\n",
      "197/223, train_loss: 0.0983, step time: 0.1075\n",
      "198/223, train_loss: 0.1020, step time: 0.1001\n",
      "199/223, train_loss: 0.1033, step time: 0.1211\n",
      "200/223, train_loss: 0.0937, step time: 0.1536\n",
      "201/223, train_loss: 0.1064, step time: 0.1003\n",
      "202/223, train_loss: 0.0908, step time: 0.0995\n",
      "203/223, train_loss: 0.0954, step time: 0.1171\n",
      "204/223, train_loss: 0.0997, step time: 0.1060\n",
      "205/223, train_loss: 0.1006, step time: 0.1063\n",
      "206/223, train_loss: 0.1079, step time: 0.1214\n",
      "207/223, train_loss: 0.0981, step time: 0.1327\n",
      "208/223, train_loss: 0.1030, step time: 0.1029\n",
      "209/223, train_loss: 0.1071, step time: 0.1105\n",
      "210/223, train_loss: 0.0937, step time: 0.1006\n",
      "211/223, train_loss: 0.0989, step time: 0.1207\n",
      "212/223, train_loss: 0.0935, step time: 0.1236\n",
      "213/223, train_loss: 0.0983, step time: 0.1146\n",
      "214/223, train_loss: 0.0921, step time: 0.1071\n",
      "215/223, train_loss: 0.1102, step time: 0.1124\n",
      "216/223, train_loss: 0.1055, step time: 0.1120\n",
      "217/223, train_loss: 0.0911, step time: 0.1021\n",
      "218/223, train_loss: 0.0898, step time: 0.1003\n",
      "219/223, train_loss: 0.0947, step time: 0.1003\n",
      "220/223, train_loss: 0.1107, step time: 0.1002\n",
      "221/223, train_loss: 0.1104, step time: 0.1000\n",
      "222/223, train_loss: 0.0985, step time: 0.0997\n",
      "223/223, train_loss: 0.1040, step time: 0.1003\n",
      "epoch 234 average loss: 0.1006\n",
      "time consuming of epoch 234 is: 92.9124\n",
      "----------\n",
      "epoch 235/300\n",
      "1/223, train_loss: 0.0952, step time: 0.1091\n",
      "2/223, train_loss: 0.0943, step time: 0.1003\n",
      "3/223, train_loss: 0.0991, step time: 0.0995\n",
      "4/223, train_loss: 0.0968, step time: 0.1006\n",
      "5/223, train_loss: 0.0939, step time: 0.1092\n",
      "6/223, train_loss: 0.0979, step time: 0.1326\n",
      "7/223, train_loss: 0.0876, step time: 0.1078\n",
      "8/223, train_loss: 0.0994, step time: 0.1179\n",
      "9/223, train_loss: 0.0897, step time: 0.1157\n",
      "10/223, train_loss: 0.0930, step time: 0.1098\n",
      "11/223, train_loss: 0.1037, step time: 0.1069\n",
      "12/223, train_loss: 0.1000, step time: 0.1001\n",
      "13/223, train_loss: 0.0968, step time: 0.1100\n",
      "14/223, train_loss: 0.1049, step time: 0.1121\n",
      "15/223, train_loss: 0.1015, step time: 0.1054\n",
      "16/223, train_loss: 0.0998, step time: 0.1039\n",
      "17/223, train_loss: 0.0900, step time: 0.1203\n",
      "18/223, train_loss: 0.1016, step time: 0.1069\n",
      "19/223, train_loss: 0.1019, step time: 0.1201\n",
      "20/223, train_loss: 0.1107, step time: 0.1056\n",
      "21/223, train_loss: 0.0973, step time: 0.1111\n",
      "22/223, train_loss: 0.0994, step time: 0.1075\n",
      "23/223, train_loss: 0.0961, step time: 0.1068\n",
      "24/223, train_loss: 0.0953, step time: 0.1075\n",
      "25/223, train_loss: 0.0990, step time: 0.1180\n",
      "26/223, train_loss: 0.0989, step time: 0.1156\n",
      "27/223, train_loss: 0.1093, step time: 0.1027\n",
      "28/223, train_loss: 0.0924, step time: 0.1148\n",
      "29/223, train_loss: 0.1059, step time: 0.1147\n",
      "30/223, train_loss: 0.0926, step time: 0.1058\n",
      "31/223, train_loss: 0.0940, step time: 0.1002\n",
      "32/223, train_loss: 0.1060, step time: 0.1000\n",
      "33/223, train_loss: 0.1090, step time: 0.1046\n",
      "34/223, train_loss: 0.0947, step time: 0.1203\n",
      "35/223, train_loss: 0.1057, step time: 0.1255\n",
      "36/223, train_loss: 0.0951, step time: 0.1389\n",
      "37/223, train_loss: 0.0997, step time: 0.1001\n",
      "38/223, train_loss: 0.0963, step time: 0.1088\n",
      "39/223, train_loss: 0.0928, step time: 0.1433\n",
      "40/223, train_loss: 0.0924, step time: 0.1386\n",
      "41/223, train_loss: 0.0974, step time: 0.1109\n",
      "42/223, train_loss: 0.1022, step time: 0.1149\n",
      "43/223, train_loss: 0.1011, step time: 0.1074\n",
      "44/223, train_loss: 0.0901, step time: 0.1662\n",
      "45/223, train_loss: 0.1128, step time: 0.1108\n",
      "46/223, train_loss: 0.0933, step time: 0.1110\n",
      "47/223, train_loss: 0.0897, step time: 0.1132\n",
      "48/223, train_loss: 0.1029, step time: 0.1163\n",
      "49/223, train_loss: 0.0987, step time: 0.1067\n",
      "50/223, train_loss: 0.0938, step time: 0.1043\n",
      "51/223, train_loss: 0.1034, step time: 0.1101\n",
      "52/223, train_loss: 0.0961, step time: 0.1123\n",
      "53/223, train_loss: 0.0961, step time: 0.1057\n",
      "54/223, train_loss: 0.0971, step time: 0.0998\n",
      "55/223, train_loss: 0.0941, step time: 0.1082\n",
      "56/223, train_loss: 0.0925, step time: 0.1018\n",
      "57/223, train_loss: 0.1079, step time: 0.1194\n",
      "58/223, train_loss: 0.0939, step time: 0.1050\n",
      "59/223, train_loss: 0.0944, step time: 0.1175\n",
      "60/223, train_loss: 0.0915, step time: 0.1064\n",
      "61/223, train_loss: 0.0870, step time: 0.1036\n",
      "62/223, train_loss: 0.1044, step time: 0.1127\n",
      "63/223, train_loss: 0.0965, step time: 0.1566\n",
      "64/223, train_loss: 0.0950, step time: 0.1019\n",
      "65/223, train_loss: 0.0986, step time: 0.1063\n",
      "66/223, train_loss: 0.1051, step time: 0.1091\n",
      "67/223, train_loss: 0.0926, step time: 0.1355\n",
      "68/223, train_loss: 0.1030, step time: 0.0998\n",
      "69/223, train_loss: 0.1015, step time: 0.1146\n",
      "70/223, train_loss: 0.0942, step time: 0.1091\n",
      "71/223, train_loss: 0.1071, step time: 0.1297\n",
      "72/223, train_loss: 0.0993, step time: 0.1116\n",
      "73/223, train_loss: 0.1179, step time: 0.1136\n",
      "74/223, train_loss: 0.1005, step time: 0.1060\n",
      "75/223, train_loss: 0.1063, step time: 0.1153\n",
      "76/223, train_loss: 0.1029, step time: 0.1071\n",
      "77/223, train_loss: 0.0947, step time: 0.1095\n",
      "78/223, train_loss: 0.0951, step time: 0.1165\n",
      "79/223, train_loss: 0.1161, step time: 0.1180\n",
      "80/223, train_loss: 0.1028, step time: 0.1150\n",
      "81/223, train_loss: 0.0955, step time: 0.1163\n",
      "82/223, train_loss: 0.1027, step time: 0.0998\n",
      "83/223, train_loss: 0.1047, step time: 0.1363\n",
      "84/223, train_loss: 0.0935, step time: 0.1100\n",
      "85/223, train_loss: 0.1000, step time: 0.1218\n",
      "86/223, train_loss: 0.1085, step time: 0.1228\n",
      "87/223, train_loss: 0.1150, step time: 0.1142\n",
      "88/223, train_loss: 0.0975, step time: 0.1208\n",
      "89/223, train_loss: 0.1098, step time: 0.1163\n",
      "90/223, train_loss: 0.1026, step time: 0.1075\n",
      "91/223, train_loss: 0.1031, step time: 0.1137\n",
      "92/223, train_loss: 0.0977, step time: 0.1065\n",
      "93/223, train_loss: 0.1037, step time: 0.1002\n",
      "94/223, train_loss: 0.1085, step time: 0.1116\n",
      "95/223, train_loss: 0.0961, step time: 0.1416\n",
      "96/223, train_loss: 0.0987, step time: 0.1002\n",
      "97/223, train_loss: 0.1079, step time: 0.1086\n",
      "98/223, train_loss: 0.1007, step time: 0.1007\n",
      "99/223, train_loss: 0.0944, step time: 0.1212\n",
      "100/223, train_loss: 0.0922, step time: 0.1010\n",
      "101/223, train_loss: 0.1058, step time: 0.1134\n",
      "102/223, train_loss: 0.0999, step time: 0.1127\n",
      "103/223, train_loss: 0.1053, step time: 0.1185\n",
      "104/223, train_loss: 0.0948, step time: 0.1028\n",
      "105/223, train_loss: 0.0925, step time: 0.1075\n",
      "106/223, train_loss: 0.0926, step time: 0.1127\n",
      "107/223, train_loss: 0.1058, step time: 0.1022\n",
      "108/223, train_loss: 0.0868, step time: 0.1307\n",
      "109/223, train_loss: 0.1041, step time: 0.1165\n",
      "110/223, train_loss: 0.1032, step time: 0.1237\n",
      "111/223, train_loss: 0.1010, step time: 0.1192\n",
      "112/223, train_loss: 0.1055, step time: 0.1003\n",
      "113/223, train_loss: 0.1037, step time: 0.1252\n",
      "114/223, train_loss: 0.0957, step time: 0.1190\n",
      "115/223, train_loss: 0.1053, step time: 0.1008\n",
      "116/223, train_loss: 0.1051, step time: 0.1310\n",
      "117/223, train_loss: 0.0842, step time: 0.1051\n",
      "118/223, train_loss: 0.1043, step time: 0.1004\n",
      "119/223, train_loss: 0.1051, step time: 0.1200\n",
      "120/223, train_loss: 0.0981, step time: 0.1005\n",
      "121/223, train_loss: 0.1003, step time: 0.1020\n",
      "122/223, train_loss: 0.1050, step time: 0.1079\n",
      "123/223, train_loss: 0.1044, step time: 0.1276\n",
      "124/223, train_loss: 0.0972, step time: 0.1222\n",
      "125/223, train_loss: 0.0975, step time: 0.1173\n",
      "126/223, train_loss: 0.0981, step time: 0.1371\n",
      "127/223, train_loss: 0.0924, step time: 0.1074\n",
      "128/223, train_loss: 0.1000, step time: 0.1055\n",
      "129/223, train_loss: 0.0929, step time: 0.1029\n",
      "130/223, train_loss: 0.0901, step time: 0.1001\n",
      "131/223, train_loss: 0.0968, step time: 0.0994\n",
      "132/223, train_loss: 0.1002, step time: 0.1294\n",
      "133/223, train_loss: 0.0987, step time: 0.1094\n",
      "134/223, train_loss: 0.1080, step time: 0.1110\n",
      "135/223, train_loss: 0.1000, step time: 0.1244\n",
      "136/223, train_loss: 0.1014, step time: 0.1015\n",
      "137/223, train_loss: 0.1020, step time: 0.1074\n",
      "138/223, train_loss: 0.0927, step time: 0.1221\n",
      "139/223, train_loss: 0.0984, step time: 0.1068\n",
      "140/223, train_loss: 0.0964, step time: 0.1204\n",
      "141/223, train_loss: 0.3011, step time: 0.1132\n",
      "142/223, train_loss: 0.0945, step time: 0.1321\n",
      "143/223, train_loss: 0.1084, step time: 0.1078\n",
      "144/223, train_loss: 0.0941, step time: 0.1066\n",
      "145/223, train_loss: 0.0951, step time: 0.1126\n",
      "146/223, train_loss: 0.0975, step time: 0.1103\n",
      "147/223, train_loss: 0.1142, step time: 0.1380\n",
      "148/223, train_loss: 0.0985, step time: 0.1032\n",
      "149/223, train_loss: 0.0966, step time: 0.1098\n",
      "150/223, train_loss: 0.0975, step time: 0.1018\n",
      "151/223, train_loss: 0.0884, step time: 0.1245\n",
      "152/223, train_loss: 0.0963, step time: 0.1109\n",
      "153/223, train_loss: 0.0992, step time: 0.1021\n",
      "154/223, train_loss: 0.1074, step time: 0.1121\n",
      "155/223, train_loss: 0.1007, step time: 0.1169\n",
      "156/223, train_loss: 0.0868, step time: 0.1012\n",
      "157/223, train_loss: 0.1079, step time: 0.0994\n",
      "158/223, train_loss: 0.1097, step time: 0.0998\n",
      "159/223, train_loss: 0.0898, step time: 0.1001\n",
      "160/223, train_loss: 0.1040, step time: 0.1013\n",
      "161/223, train_loss: 0.0973, step time: 0.1003\n",
      "162/223, train_loss: 0.1160, step time: 0.1003\n",
      "163/223, train_loss: 0.0931, step time: 0.1005\n",
      "164/223, train_loss: 0.0927, step time: 0.1156\n",
      "165/223, train_loss: 0.1068, step time: 0.0994\n",
      "166/223, train_loss: 0.0961, step time: 0.1200\n",
      "167/223, train_loss: 0.0920, step time: 0.1121\n",
      "168/223, train_loss: 0.1144, step time: 0.1111\n",
      "169/223, train_loss: 0.0985, step time: 0.1268\n",
      "170/223, train_loss: 0.0983, step time: 0.1259\n",
      "171/223, train_loss: 0.1040, step time: 0.1277\n",
      "172/223, train_loss: 0.0990, step time: 0.1082\n",
      "173/223, train_loss: 0.0942, step time: 0.0993\n",
      "174/223, train_loss: 0.1036, step time: 0.0992\n",
      "175/223, train_loss: 0.1130, step time: 0.1007\n",
      "176/223, train_loss: 0.1012, step time: 0.0994\n",
      "177/223, train_loss: 0.1014, step time: 0.1113\n",
      "178/223, train_loss: 0.1031, step time: 0.1074\n",
      "179/223, train_loss: 0.1096, step time: 0.1251\n",
      "180/223, train_loss: 0.1059, step time: 0.1071\n",
      "181/223, train_loss: 0.1043, step time: 0.1006\n",
      "182/223, train_loss: 0.0968, step time: 0.1008\n",
      "183/223, train_loss: 0.1013, step time: 0.1006\n",
      "184/223, train_loss: 0.0944, step time: 0.1014\n",
      "185/223, train_loss: 0.0906, step time: 0.0997\n",
      "186/223, train_loss: 0.0889, step time: 0.1003\n",
      "187/223, train_loss: 0.0989, step time: 0.1003\n",
      "188/223, train_loss: 0.1101, step time: 0.1069\n",
      "189/223, train_loss: 0.0921, step time: 0.1003\n",
      "190/223, train_loss: 0.1085, step time: 0.1034\n",
      "191/223, train_loss: 0.1033, step time: 0.1230\n",
      "192/223, train_loss: 0.1074, step time: 0.1218\n",
      "193/223, train_loss: 0.0999, step time: 0.1178\n",
      "194/223, train_loss: 0.0987, step time: 0.1006\n",
      "195/223, train_loss: 0.1007, step time: 0.1010\n",
      "196/223, train_loss: 0.0954, step time: 0.1089\n",
      "197/223, train_loss: 0.1022, step time: 0.1325\n",
      "198/223, train_loss: 0.0985, step time: 0.1056\n",
      "199/223, train_loss: 0.0925, step time: 0.1045\n",
      "200/223, train_loss: 0.0956, step time: 0.1074\n",
      "201/223, train_loss: 0.0937, step time: 0.1037\n",
      "202/223, train_loss: 0.0989, step time: 0.1106\n",
      "203/223, train_loss: 0.0900, step time: 0.1082\n",
      "204/223, train_loss: 0.1080, step time: 0.1041\n",
      "205/223, train_loss: 0.1002, step time: 0.1070\n",
      "206/223, train_loss: 0.1022, step time: 0.1232\n",
      "207/223, train_loss: 0.1021, step time: 0.1100\n",
      "208/223, train_loss: 0.0918, step time: 0.1004\n",
      "209/223, train_loss: 0.0982, step time: 0.1172\n",
      "210/223, train_loss: 0.0958, step time: 0.1096\n",
      "211/223, train_loss: 0.0926, step time: 0.1133\n",
      "212/223, train_loss: 0.1023, step time: 0.1062\n",
      "213/223, train_loss: 0.0997, step time: 0.1096\n",
      "214/223, train_loss: 0.1065, step time: 0.1088\n",
      "215/223, train_loss: 0.0899, step time: 0.1145\n",
      "216/223, train_loss: 0.1057, step time: 0.1067\n",
      "217/223, train_loss: 0.1066, step time: 0.1148\n",
      "218/223, train_loss: 0.0961, step time: 0.1018\n",
      "219/223, train_loss: 0.0990, step time: 0.1237\n",
      "220/223, train_loss: 0.1102, step time: 0.0996\n",
      "221/223, train_loss: 0.1037, step time: 0.0994\n",
      "222/223, train_loss: 0.0997, step time: 0.0992\n",
      "223/223, train_loss: 0.1020, step time: 0.0999\n",
      "epoch 235 average loss: 0.1005\n",
      "saved new best metric model\n",
      "current epoch: 235 current mean dice: 0.8620 tc: 0.9226 wt: 0.8717 et: 0.7916\n",
      "best mean dice: 0.8620 at epoch: 235\n",
      "time consuming of epoch 235 is: 92.4339\n",
      "----------\n",
      "epoch 236/300\n",
      "1/223, train_loss: 0.1137, step time: 0.1011\n",
      "2/223, train_loss: 0.1002, step time: 0.0993\n",
      "3/223, train_loss: 0.1015, step time: 0.0997\n",
      "4/223, train_loss: 0.0884, step time: 0.0996\n",
      "5/223, train_loss: 0.0973, step time: 0.1003\n",
      "6/223, train_loss: 0.1044, step time: 0.1000\n",
      "7/223, train_loss: 0.0863, step time: 0.0998\n",
      "8/223, train_loss: 0.1050, step time: 0.0996\n",
      "9/223, train_loss: 0.1037, step time: 0.1094\n",
      "10/223, train_loss: 0.0953, step time: 0.1099\n",
      "11/223, train_loss: 0.1101, step time: 0.1027\n",
      "12/223, train_loss: 0.1023, step time: 0.1117\n",
      "13/223, train_loss: 0.1124, step time: 0.1308\n",
      "14/223, train_loss: 0.1097, step time: 0.1578\n",
      "15/223, train_loss: 0.0960, step time: 0.1170\n",
      "16/223, train_loss: 0.0965, step time: 0.1175\n",
      "17/223, train_loss: 0.0979, step time: 0.1126\n",
      "18/223, train_loss: 0.0906, step time: 0.1040\n",
      "19/223, train_loss: 0.0982, step time: 0.1048\n",
      "20/223, train_loss: 0.1052, step time: 0.1052\n",
      "21/223, train_loss: 0.0923, step time: 0.1369\n",
      "22/223, train_loss: 0.1018, step time: 0.1235\n",
      "23/223, train_loss: 0.1112, step time: 0.1105\n",
      "24/223, train_loss: 0.1050, step time: 0.1197\n",
      "25/223, train_loss: 0.1058, step time: 0.0998\n",
      "26/223, train_loss: 0.1003, step time: 0.0995\n",
      "27/223, train_loss: 0.0998, step time: 0.0988\n",
      "28/223, train_loss: 0.1036, step time: 0.1061\n",
      "29/223, train_loss: 0.0945, step time: 0.1094\n",
      "30/223, train_loss: 0.0961, step time: 0.1095\n",
      "31/223, train_loss: 0.0967, step time: 0.1143\n",
      "32/223, train_loss: 0.1016, step time: 0.1048\n",
      "33/223, train_loss: 0.0914, step time: 0.1217\n",
      "34/223, train_loss: 0.0976, step time: 0.1115\n",
      "35/223, train_loss: 0.0898, step time: 0.1262\n",
      "36/223, train_loss: 0.1182, step time: 0.1006\n",
      "37/223, train_loss: 0.1077, step time: 0.1271\n",
      "38/223, train_loss: 0.0884, step time: 0.1185\n",
      "39/223, train_loss: 0.1041, step time: 0.0994\n",
      "40/223, train_loss: 0.0982, step time: 0.0990\n",
      "41/223, train_loss: 0.1050, step time: 0.1022\n",
      "42/223, train_loss: 0.0969, step time: 0.1159\n",
      "43/223, train_loss: 0.1077, step time: 0.1110\n",
      "44/223, train_loss: 0.0995, step time: 0.1127\n",
      "45/223, train_loss: 0.0993, step time: 0.1270\n",
      "46/223, train_loss: 0.0973, step time: 0.1114\n",
      "47/223, train_loss: 0.1159, step time: 0.1106\n",
      "48/223, train_loss: 0.0866, step time: 0.1210\n",
      "49/223, train_loss: 0.1076, step time: 0.1216\n",
      "50/223, train_loss: 0.0971, step time: 0.1013\n",
      "51/223, train_loss: 0.0962, step time: 0.1205\n",
      "52/223, train_loss: 0.0997, step time: 0.1178\n",
      "53/223, train_loss: 0.0948, step time: 0.1096\n",
      "54/223, train_loss: 0.1061, step time: 0.1464\n",
      "55/223, train_loss: 0.1056, step time: 0.1246\n",
      "56/223, train_loss: 0.0961, step time: 0.1094\n",
      "57/223, train_loss: 0.1083, step time: 0.1396\n",
      "58/223, train_loss: 0.1063, step time: 0.1286\n",
      "59/223, train_loss: 0.1002, step time: 0.1015\n",
      "60/223, train_loss: 0.0914, step time: 0.1096\n",
      "61/223, train_loss: 0.0981, step time: 0.1197\n",
      "62/223, train_loss: 0.0927, step time: 0.1100\n",
      "63/223, train_loss: 0.1008, step time: 0.1135\n",
      "64/223, train_loss: 0.1105, step time: 0.1139\n",
      "65/223, train_loss: 0.1042, step time: 0.1052\n",
      "66/223, train_loss: 0.1026, step time: 0.1160\n",
      "67/223, train_loss: 0.0987, step time: 0.1012\n",
      "68/223, train_loss: 0.1052, step time: 0.1013\n",
      "69/223, train_loss: 0.0933, step time: 0.1058\n",
      "70/223, train_loss: 0.0864, step time: 0.1018\n",
      "71/223, train_loss: 0.0959, step time: 0.1013\n",
      "72/223, train_loss: 0.0986, step time: 0.1268\n",
      "73/223, train_loss: 0.1102, step time: 0.1038\n",
      "74/223, train_loss: 0.0977, step time: 0.1155\n",
      "75/223, train_loss: 0.0938, step time: 0.1192\n",
      "76/223, train_loss: 0.1051, step time: 0.1011\n",
      "77/223, train_loss: 0.0859, step time: 0.1193\n",
      "78/223, train_loss: 0.1009, step time: 0.1018\n",
      "79/223, train_loss: 0.0990, step time: 0.1003\n",
      "80/223, train_loss: 0.0987, step time: 0.1251\n",
      "81/223, train_loss: 0.2968, step time: 0.1012\n",
      "82/223, train_loss: 0.0945, step time: 0.1028\n",
      "83/223, train_loss: 0.1004, step time: 0.1158\n",
      "84/223, train_loss: 0.0964, step time: 0.1035\n",
      "85/223, train_loss: 0.0958, step time: 0.1244\n",
      "86/223, train_loss: 0.0905, step time: 0.1109\n",
      "87/223, train_loss: 0.1018, step time: 0.1421\n",
      "88/223, train_loss: 0.1139, step time: 0.1281\n",
      "89/223, train_loss: 0.0983, step time: 0.1148\n",
      "90/223, train_loss: 0.0962, step time: 0.1078\n",
      "91/223, train_loss: 0.0929, step time: 0.1109\n",
      "92/223, train_loss: 0.0995, step time: 0.1004\n",
      "93/223, train_loss: 0.1026, step time: 0.1003\n",
      "94/223, train_loss: 0.0932, step time: 0.1003\n",
      "95/223, train_loss: 0.0936, step time: 0.0995\n",
      "96/223, train_loss: 0.1020, step time: 0.1004\n",
      "97/223, train_loss: 0.1018, step time: 0.1198\n",
      "98/223, train_loss: 0.1189, step time: 0.1243\n",
      "99/223, train_loss: 0.0980, step time: 0.1183\n",
      "100/223, train_loss: 0.0901, step time: 0.1182\n",
      "101/223, train_loss: 0.1016, step time: 0.1151\n",
      "102/223, train_loss: 0.1115, step time: 0.1092\n",
      "103/223, train_loss: 0.0978, step time: 0.1192\n",
      "104/223, train_loss: 0.0936, step time: 0.1004\n",
      "105/223, train_loss: 0.1117, step time: 0.1052\n",
      "106/223, train_loss: 0.0962, step time: 0.1087\n",
      "107/223, train_loss: 0.0998, step time: 0.1110\n",
      "108/223, train_loss: 0.0895, step time: 0.1002\n",
      "109/223, train_loss: 0.1039, step time: 0.1007\n",
      "110/223, train_loss: 0.0945, step time: 0.1043\n",
      "111/223, train_loss: 0.1113, step time: 0.0993\n",
      "112/223, train_loss: 0.0980, step time: 0.1104\n",
      "113/223, train_loss: 0.0939, step time: 0.1027\n",
      "114/223, train_loss: 0.1056, step time: 0.0999\n",
      "115/223, train_loss: 0.1039, step time: 0.1057\n",
      "116/223, train_loss: 0.1051, step time: 0.1435\n",
      "117/223, train_loss: 0.1111, step time: 0.1095\n",
      "118/223, train_loss: 0.0990, step time: 0.1115\n",
      "119/223, train_loss: 0.0934, step time: 0.1415\n",
      "120/223, train_loss: 0.1046, step time: 0.1004\n",
      "121/223, train_loss: 0.0930, step time: 0.1578\n",
      "122/223, train_loss: 0.0980, step time: 0.1005\n",
      "123/223, train_loss: 0.0999, step time: 0.1115\n",
      "124/223, train_loss: 0.0947, step time: 0.1058\n",
      "125/223, train_loss: 0.0856, step time: 0.1065\n",
      "126/223, train_loss: 0.0909, step time: 0.1276\n",
      "127/223, train_loss: 0.1036, step time: 0.1107\n",
      "128/223, train_loss: 0.0924, step time: 0.1153\n",
      "129/223, train_loss: 0.1035, step time: 0.1143\n",
      "130/223, train_loss: 0.0988, step time: 0.1152\n",
      "131/223, train_loss: 0.0923, step time: 0.1342\n",
      "132/223, train_loss: 0.0908, step time: 0.1091\n",
      "133/223, train_loss: 0.1110, step time: 0.1072\n",
      "134/223, train_loss: 0.1078, step time: 0.1315\n",
      "135/223, train_loss: 0.1007, step time: 0.1002\n",
      "136/223, train_loss: 0.1116, step time: 0.1014\n",
      "137/223, train_loss: 0.0991, step time: 0.1215\n",
      "138/223, train_loss: 0.1034, step time: 0.1090\n",
      "139/223, train_loss: 0.0904, step time: 0.1520\n",
      "140/223, train_loss: 0.1062, step time: 0.1368\n",
      "141/223, train_loss: 0.1157, step time: 0.0999\n",
      "142/223, train_loss: 0.1088, step time: 0.1014\n",
      "143/223, train_loss: 0.0994, step time: 0.1022\n",
      "144/223, train_loss: 0.0941, step time: 0.1153\n",
      "145/223, train_loss: 0.1101, step time: 0.1054\n",
      "146/223, train_loss: 0.1061, step time: 0.1265\n",
      "147/223, train_loss: 0.0989, step time: 0.1434\n",
      "148/223, train_loss: 0.1077, step time: 0.1066\n",
      "149/223, train_loss: 0.0977, step time: 0.1120\n",
      "150/223, train_loss: 0.1079, step time: 0.1294\n",
      "151/223, train_loss: 0.1104, step time: 0.1157\n",
      "152/223, train_loss: 0.1042, step time: 0.1015\n",
      "153/223, train_loss: 0.1004, step time: 0.1095\n",
      "154/223, train_loss: 0.0913, step time: 0.1328\n",
      "155/223, train_loss: 0.0971, step time: 0.1187\n",
      "156/223, train_loss: 0.1120, step time: 0.1041\n",
      "157/223, train_loss: 0.1067, step time: 0.1154\n",
      "158/223, train_loss: 0.1055, step time: 0.0997\n",
      "159/223, train_loss: 0.1154, step time: 0.1005\n",
      "160/223, train_loss: 0.0955, step time: 0.1021\n",
      "161/223, train_loss: 0.0952, step time: 0.1056\n",
      "162/223, train_loss: 0.0956, step time: 0.1072\n",
      "163/223, train_loss: 0.0935, step time: 0.1296\n",
      "164/223, train_loss: 0.0958, step time: 0.1082\n",
      "165/223, train_loss: 0.1015, step time: 0.1146\n",
      "166/223, train_loss: 0.0968, step time: 0.1146\n",
      "167/223, train_loss: 0.0961, step time: 0.1174\n",
      "168/223, train_loss: 0.1070, step time: 0.1251\n",
      "169/223, train_loss: 0.1032, step time: 0.1083\n",
      "170/223, train_loss: 0.0967, step time: 0.0998\n",
      "171/223, train_loss: 0.0936, step time: 0.1051\n",
      "172/223, train_loss: 0.0942, step time: 0.1093\n",
      "173/223, train_loss: 0.1040, step time: 0.0994\n",
      "174/223, train_loss: 0.0914, step time: 0.1006\n",
      "175/223, train_loss: 0.0947, step time: 0.1022\n",
      "176/223, train_loss: 0.0952, step time: 0.1055\n",
      "177/223, train_loss: 0.1005, step time: 0.1078\n",
      "178/223, train_loss: 0.0979, step time: 0.1293\n",
      "179/223, train_loss: 0.0974, step time: 0.1284\n",
      "180/223, train_loss: 0.1038, step time: 0.1240\n",
      "181/223, train_loss: 0.0954, step time: 0.1155\n",
      "182/223, train_loss: 0.1018, step time: 0.1199\n",
      "183/223, train_loss: 0.0946, step time: 0.1272\n",
      "184/223, train_loss: 0.1009, step time: 0.1120\n",
      "185/223, train_loss: 0.0881, step time: 0.1326\n",
      "186/223, train_loss: 0.0958, step time: 0.1164\n",
      "187/223, train_loss: 0.0984, step time: 0.1003\n",
      "188/223, train_loss: 0.0972, step time: 0.1317\n",
      "189/223, train_loss: 0.0992, step time: 0.1155\n",
      "190/223, train_loss: 0.0951, step time: 0.1009\n",
      "191/223, train_loss: 0.1092, step time: 0.1002\n",
      "192/223, train_loss: 0.0884, step time: 0.1342\n",
      "193/223, train_loss: 0.0912, step time: 0.1154\n",
      "194/223, train_loss: 0.0993, step time: 0.1069\n",
      "195/223, train_loss: 0.1106, step time: 0.1007\n",
      "196/223, train_loss: 0.0961, step time: 0.1010\n",
      "197/223, train_loss: 0.0917, step time: 0.1009\n",
      "198/223, train_loss: 0.0926, step time: 0.1198\n",
      "199/223, train_loss: 0.0960, step time: 0.1008\n",
      "200/223, train_loss: 0.1070, step time: 0.1005\n",
      "201/223, train_loss: 0.0954, step time: 0.1095\n",
      "202/223, train_loss: 0.0975, step time: 0.1001\n",
      "203/223, train_loss: 0.0974, step time: 0.1045\n",
      "204/223, train_loss: 0.0992, step time: 0.1118\n",
      "205/223, train_loss: 0.0946, step time: 0.1100\n",
      "206/223, train_loss: 0.0925, step time: 0.1360\n",
      "207/223, train_loss: 0.0936, step time: 0.1058\n",
      "208/223, train_loss: 0.0911, step time: 0.1003\n",
      "209/223, train_loss: 0.0951, step time: 0.1084\n",
      "210/223, train_loss: 0.1030, step time: 0.0998\n",
      "211/223, train_loss: 0.0993, step time: 0.1011\n",
      "212/223, train_loss: 0.1049, step time: 0.1135\n",
      "213/223, train_loss: 0.0929, step time: 0.1151\n",
      "214/223, train_loss: 0.0995, step time: 0.1009\n",
      "215/223, train_loss: 0.0920, step time: 0.1312\n",
      "216/223, train_loss: 0.0967, step time: 0.1004\n",
      "217/223, train_loss: 0.0945, step time: 0.1050\n",
      "218/223, train_loss: 0.0920, step time: 0.1034\n",
      "219/223, train_loss: 0.1091, step time: 0.1288\n",
      "220/223, train_loss: 0.1038, step time: 0.1006\n",
      "221/223, train_loss: 0.0965, step time: 0.0992\n",
      "222/223, train_loss: 0.0873, step time: 0.1004\n",
      "223/223, train_loss: 0.0999, step time: 0.1005\n",
      "epoch 236 average loss: 0.1005\n",
      "time consuming of epoch 236 is: 90.3182\n",
      "----------\n",
      "epoch 237/300\n",
      "1/223, train_loss: 0.0999, step time: 0.1139\n",
      "2/223, train_loss: 0.0896, step time: 0.1202\n",
      "3/223, train_loss: 0.0924, step time: 0.1198\n",
      "4/223, train_loss: 0.1007, step time: 0.1106\n",
      "5/223, train_loss: 0.1023, step time: 0.1002\n",
      "6/223, train_loss: 0.0990, step time: 0.1010\n",
      "7/223, train_loss: 0.0830, step time: 0.1001\n",
      "8/223, train_loss: 0.1070, step time: 0.1001\n",
      "9/223, train_loss: 0.0951, step time: 0.1082\n",
      "10/223, train_loss: 0.1081, step time: 0.1303\n",
      "11/223, train_loss: 0.0986, step time: 0.1273\n",
      "12/223, train_loss: 0.0872, step time: 0.1294\n",
      "13/223, train_loss: 0.1016, step time: 0.1003\n",
      "14/223, train_loss: 0.0986, step time: 0.1000\n",
      "15/223, train_loss: 0.1040, step time: 0.0995\n",
      "16/223, train_loss: 0.1095, step time: 0.1002\n",
      "17/223, train_loss: 0.1152, step time: 0.1686\n",
      "18/223, train_loss: 0.0995, step time: 0.1099\n",
      "19/223, train_loss: 0.1005, step time: 0.1016\n",
      "20/223, train_loss: 0.0949, step time: 0.1152\n",
      "21/223, train_loss: 0.0889, step time: 0.0999\n",
      "22/223, train_loss: 0.0955, step time: 0.1218\n",
      "23/223, train_loss: 0.0969, step time: 0.1004\n",
      "24/223, train_loss: 0.1031, step time: 0.1087\n",
      "25/223, train_loss: 0.0991, step time: 0.1009\n",
      "26/223, train_loss: 0.1098, step time: 0.1104\n",
      "27/223, train_loss: 0.0995, step time: 0.1233\n",
      "28/223, train_loss: 0.0905, step time: 0.1006\n",
      "29/223, train_loss: 0.0912, step time: 0.1094\n",
      "30/223, train_loss: 0.1082, step time: 0.1164\n",
      "31/223, train_loss: 0.0951, step time: 0.1007\n",
      "32/223, train_loss: 0.0943, step time: 0.1274\n",
      "33/223, train_loss: 0.0952, step time: 0.1066\n",
      "34/223, train_loss: 0.1015, step time: 0.1223\n",
      "35/223, train_loss: 0.1006, step time: 0.1047\n",
      "36/223, train_loss: 0.1014, step time: 0.1005\n",
      "37/223, train_loss: 0.1066, step time: 0.1131\n",
      "38/223, train_loss: 0.0944, step time: 0.1135\n",
      "39/223, train_loss: 0.0940, step time: 0.1003\n",
      "40/223, train_loss: 0.0926, step time: 0.1151\n",
      "41/223, train_loss: 0.1082, step time: 0.1060\n",
      "42/223, train_loss: 0.0962, step time: 0.1123\n",
      "43/223, train_loss: 0.0884, step time: 0.1003\n",
      "44/223, train_loss: 0.1007, step time: 0.1004\n",
      "45/223, train_loss: 0.0960, step time: 0.1034\n",
      "46/223, train_loss: 0.1040, step time: 0.1002\n",
      "47/223, train_loss: 0.0998, step time: 0.1014\n",
      "48/223, train_loss: 0.0953, step time: 0.1097\n",
      "49/223, train_loss: 0.1032, step time: 0.1067\n",
      "50/223, train_loss: 0.1051, step time: 0.1106\n",
      "51/223, train_loss: 0.0922, step time: 0.1005\n",
      "52/223, train_loss: 0.0932, step time: 0.1129\n",
      "53/223, train_loss: 0.0950, step time: 0.1138\n",
      "54/223, train_loss: 0.1072, step time: 0.1168\n",
      "55/223, train_loss: 0.1030, step time: 0.1113\n",
      "56/223, train_loss: 0.1140, step time: 0.1013\n",
      "57/223, train_loss: 0.1082, step time: 0.1046\n",
      "58/223, train_loss: 0.1004, step time: 0.1106\n",
      "59/223, train_loss: 0.1046, step time: 0.0992\n",
      "60/223, train_loss: 0.0911, step time: 0.1050\n",
      "61/223, train_loss: 0.1182, step time: 0.0997\n",
      "62/223, train_loss: 0.0998, step time: 0.0997\n",
      "63/223, train_loss: 0.1010, step time: 0.1137\n",
      "64/223, train_loss: 0.0923, step time: 0.1346\n",
      "65/223, train_loss: 0.1058, step time: 0.1082\n",
      "66/223, train_loss: 0.0959, step time: 0.1159\n",
      "67/223, train_loss: 0.0912, step time: 0.1138\n",
      "68/223, train_loss: 0.1071, step time: 0.1109\n",
      "69/223, train_loss: 0.1019, step time: 0.1036\n",
      "70/223, train_loss: 0.0951, step time: 0.1176\n",
      "71/223, train_loss: 0.0936, step time: 0.1164\n",
      "72/223, train_loss: 0.1045, step time: 0.1038\n",
      "73/223, train_loss: 0.0953, step time: 0.1154\n",
      "74/223, train_loss: 0.0978, step time: 0.1067\n",
      "75/223, train_loss: 0.0916, step time: 0.1174\n",
      "76/223, train_loss: 0.1070, step time: 0.1004\n",
      "77/223, train_loss: 0.0951, step time: 0.1179\n",
      "78/223, train_loss: 0.1072, step time: 0.1001\n",
      "79/223, train_loss: 0.0947, step time: 0.1100\n",
      "80/223, train_loss: 0.1049, step time: 0.1014\n",
      "81/223, train_loss: 0.0982, step time: 0.1105\n",
      "82/223, train_loss: 0.0921, step time: 0.1042\n",
      "83/223, train_loss: 0.0980, step time: 0.1076\n",
      "84/223, train_loss: 0.1038, step time: 0.1083\n",
      "85/223, train_loss: 0.1021, step time: 0.1156\n",
      "86/223, train_loss: 0.1050, step time: 0.1163\n",
      "87/223, train_loss: 0.0980, step time: 0.1003\n",
      "88/223, train_loss: 0.1057, step time: 0.1062\n",
      "89/223, train_loss: 0.1033, step time: 0.1153\n",
      "90/223, train_loss: 0.1133, step time: 0.1006\n",
      "91/223, train_loss: 0.0920, step time: 0.0998\n",
      "92/223, train_loss: 0.1156, step time: 0.0998\n",
      "93/223, train_loss: 0.0947, step time: 0.1062\n",
      "94/223, train_loss: 0.1024, step time: 0.1006\n",
      "95/223, train_loss: 0.0985, step time: 0.1062\n",
      "96/223, train_loss: 0.0951, step time: 0.1007\n",
      "97/223, train_loss: 0.0987, step time: 0.1091\n",
      "98/223, train_loss: 0.0972, step time: 0.1004\n",
      "99/223, train_loss: 0.0986, step time: 0.1128\n",
      "100/223, train_loss: 0.0991, step time: 0.1044\n",
      "101/223, train_loss: 0.1032, step time: 0.1001\n",
      "102/223, train_loss: 0.1091, step time: 0.1025\n",
      "103/223, train_loss: 0.1003, step time: 0.1090\n",
      "104/223, train_loss: 0.0900, step time: 0.1008\n",
      "105/223, train_loss: 0.0973, step time: 0.1019\n",
      "106/223, train_loss: 0.1113, step time: 0.1000\n",
      "107/223, train_loss: 0.0899, step time: 0.1001\n",
      "108/223, train_loss: 0.0965, step time: 0.1038\n",
      "109/223, train_loss: 0.0935, step time: 0.1039\n",
      "110/223, train_loss: 0.0916, step time: 0.1010\n",
      "111/223, train_loss: 0.0978, step time: 0.1065\n",
      "112/223, train_loss: 0.1126, step time: 0.1008\n",
      "113/223, train_loss: 0.1038, step time: 0.1007\n",
      "114/223, train_loss: 0.0961, step time: 0.1001\n",
      "115/223, train_loss: 0.0983, step time: 0.1044\n",
      "116/223, train_loss: 0.0978, step time: 0.1092\n",
      "117/223, train_loss: 0.1044, step time: 0.1003\n",
      "118/223, train_loss: 0.0956, step time: 0.1134\n",
      "119/223, train_loss: 0.0970, step time: 0.1039\n",
      "120/223, train_loss: 0.1064, step time: 0.1099\n",
      "121/223, train_loss: 0.0942, step time: 0.1112\n",
      "122/223, train_loss: 0.1044, step time: 0.1115\n",
      "123/223, train_loss: 0.0923, step time: 0.1227\n",
      "124/223, train_loss: 0.0904, step time: 0.1205\n",
      "125/223, train_loss: 0.1038, step time: 0.0998\n",
      "126/223, train_loss: 0.1109, step time: 0.1008\n",
      "127/223, train_loss: 0.1047, step time: 0.1007\n",
      "128/223, train_loss: 0.0886, step time: 0.1004\n",
      "129/223, train_loss: 0.0891, step time: 0.1135\n",
      "130/223, train_loss: 0.0878, step time: 0.0999\n",
      "131/223, train_loss: 0.1077, step time: 0.1115\n",
      "132/223, train_loss: 0.0919, step time: 0.1119\n",
      "133/223, train_loss: 0.0987, step time: 0.1135\n",
      "134/223, train_loss: 0.1067, step time: 0.1017\n",
      "135/223, train_loss: 0.0896, step time: 0.1157\n",
      "136/223, train_loss: 0.0991, step time: 0.1436\n",
      "137/223, train_loss: 0.1119, step time: 0.1015\n",
      "138/223, train_loss: 0.0937, step time: 0.1139\n",
      "139/223, train_loss: 0.1145, step time: 0.1117\n",
      "140/223, train_loss: 0.1067, step time: 0.1031\n",
      "141/223, train_loss: 0.1179, step time: 0.1298\n",
      "142/223, train_loss: 0.1128, step time: 0.1312\n",
      "143/223, train_loss: 0.1019, step time: 0.1047\n",
      "144/223, train_loss: 0.0925, step time: 0.0997\n",
      "145/223, train_loss: 0.0897, step time: 0.1257\n",
      "146/223, train_loss: 0.0950, step time: 0.1023\n",
      "147/223, train_loss: 0.0900, step time: 0.0995\n",
      "148/223, train_loss: 0.0939, step time: 0.1100\n",
      "149/223, train_loss: 0.0999, step time: 0.1049\n",
      "150/223, train_loss: 0.0935, step time: 0.1006\n",
      "151/223, train_loss: 0.1082, step time: 0.1239\n",
      "152/223, train_loss: 0.1001, step time: 0.1039\n",
      "153/223, train_loss: 0.1018, step time: 0.1273\n",
      "154/223, train_loss: 0.1006, step time: 0.1102\n",
      "155/223, train_loss: 0.0979, step time: 0.1137\n",
      "156/223, train_loss: 0.1016, step time: 0.1047\n",
      "157/223, train_loss: 0.1002, step time: 0.1152\n",
      "158/223, train_loss: 0.1012, step time: 0.1054\n",
      "159/223, train_loss: 0.1070, step time: 0.1106\n",
      "160/223, train_loss: 0.1080, step time: 0.1395\n",
      "161/223, train_loss: 0.0940, step time: 0.1143\n",
      "162/223, train_loss: 0.0892, step time: 0.1001\n",
      "163/223, train_loss: 0.1161, step time: 0.1123\n",
      "164/223, train_loss: 0.1105, step time: 0.1142\n",
      "165/223, train_loss: 0.0980, step time: 0.1205\n",
      "166/223, train_loss: 0.0934, step time: 0.1100\n",
      "167/223, train_loss: 0.0885, step time: 0.1003\n",
      "168/223, train_loss: 0.1055, step time: 0.1002\n",
      "169/223, train_loss: 0.1009, step time: 0.1106\n",
      "170/223, train_loss: 0.1011, step time: 0.1363\n",
      "171/223, train_loss: 0.0979, step time: 0.1287\n",
      "172/223, train_loss: 0.0962, step time: 0.0991\n",
      "173/223, train_loss: 0.0929, step time: 0.1097\n",
      "174/223, train_loss: 0.0906, step time: 0.1189\n",
      "175/223, train_loss: 0.1106, step time: 0.1196\n",
      "176/223, train_loss: 0.1016, step time: 0.1109\n",
      "177/223, train_loss: 0.1029, step time: 0.1049\n",
      "178/223, train_loss: 0.0958, step time: 0.1299\n",
      "179/223, train_loss: 0.1093, step time: 0.1187\n",
      "180/223, train_loss: 0.0962, step time: 0.1098\n",
      "181/223, train_loss: 0.0874, step time: 0.1050\n",
      "182/223, train_loss: 0.0941, step time: 0.0997\n",
      "183/223, train_loss: 0.1054, step time: 0.0997\n",
      "184/223, train_loss: 0.1015, step time: 0.1037\n",
      "185/223, train_loss: 0.1040, step time: 0.1101\n",
      "186/223, train_loss: 0.1094, step time: 0.1005\n",
      "187/223, train_loss: 0.1012, step time: 0.1015\n",
      "188/223, train_loss: 0.0991, step time: 0.1065\n",
      "189/223, train_loss: 0.0944, step time: 0.1009\n",
      "190/223, train_loss: 0.0916, step time: 0.1005\n",
      "191/223, train_loss: 0.1027, step time: 0.0997\n",
      "192/223, train_loss: 0.1072, step time: 0.1065\n",
      "193/223, train_loss: 0.0957, step time: 0.1109\n",
      "194/223, train_loss: 0.1132, step time: 0.1246\n",
      "195/223, train_loss: 0.0938, step time: 0.1001\n",
      "196/223, train_loss: 0.0962, step time: 0.1103\n",
      "197/223, train_loss: 0.1010, step time: 0.1311\n",
      "198/223, train_loss: 0.1035, step time: 0.1008\n",
      "199/223, train_loss: 0.0929, step time: 0.1071\n",
      "200/223, train_loss: 0.1007, step time: 0.1030\n",
      "201/223, train_loss: 0.0910, step time: 0.1115\n",
      "202/223, train_loss: 0.1057, step time: 0.1226\n",
      "203/223, train_loss: 0.0943, step time: 0.1217\n",
      "204/223, train_loss: 0.0978, step time: 0.1068\n",
      "205/223, train_loss: 0.1125, step time: 0.1098\n",
      "206/223, train_loss: 0.0978, step time: 0.1048\n",
      "207/223, train_loss: 0.1081, step time: 0.1094\n",
      "208/223, train_loss: 0.0999, step time: 0.1000\n",
      "209/223, train_loss: 0.1002, step time: 0.1089\n",
      "210/223, train_loss: 0.1000, step time: 0.1020\n",
      "211/223, train_loss: 0.1001, step time: 0.1240\n",
      "212/223, train_loss: 0.1014, step time: 0.1270\n",
      "213/223, train_loss: 0.1053, step time: 0.1090\n",
      "214/223, train_loss: 0.0915, step time: 0.1213\n",
      "215/223, train_loss: 0.0958, step time: 0.1279\n",
      "216/223, train_loss: 0.1035, step time: 0.1072\n",
      "217/223, train_loss: 0.2949, step time: 0.1054\n",
      "218/223, train_loss: 0.1005, step time: 0.1142\n",
      "219/223, train_loss: 0.0933, step time: 0.1090\n",
      "220/223, train_loss: 0.0930, step time: 0.1154\n",
      "221/223, train_loss: 0.1017, step time: 0.0998\n",
      "222/223, train_loss: 0.1021, step time: 0.1011\n",
      "223/223, train_loss: 0.0950, step time: 0.1080\n",
      "epoch 237 average loss: 0.1006\n",
      "time consuming of epoch 237 is: 89.2641\n",
      "----------\n",
      "epoch 238/300\n",
      "1/223, train_loss: 0.1128, step time: 0.1154\n",
      "2/223, train_loss: 0.0946, step time: 0.1175\n",
      "3/223, train_loss: 0.1145, step time: 0.1148\n",
      "4/223, train_loss: 0.0945, step time: 0.1155\n",
      "5/223, train_loss: 0.0931, step time: 0.1121\n",
      "6/223, train_loss: 0.1093, step time: 0.1164\n",
      "7/223, train_loss: 0.1062, step time: 0.1226\n",
      "8/223, train_loss: 0.1052, step time: 0.1428\n",
      "9/223, train_loss: 0.1064, step time: 0.1046\n",
      "10/223, train_loss: 0.1037, step time: 0.1141\n",
      "11/223, train_loss: 0.1082, step time: 0.1164\n",
      "12/223, train_loss: 0.0898, step time: 0.1126\n",
      "13/223, train_loss: 0.0980, step time: 0.1154\n",
      "14/223, train_loss: 0.0910, step time: 0.1125\n",
      "15/223, train_loss: 0.1088, step time: 0.1140\n",
      "16/223, train_loss: 0.0942, step time: 0.1040\n",
      "17/223, train_loss: 0.1039, step time: 0.1096\n",
      "18/223, train_loss: 0.0928, step time: 0.1131\n",
      "19/223, train_loss: 0.0889, step time: 0.1252\n",
      "20/223, train_loss: 0.0973, step time: 0.0996\n",
      "21/223, train_loss: 0.1013, step time: 0.1107\n",
      "22/223, train_loss: 0.1010, step time: 0.1240\n",
      "23/223, train_loss: 0.0953, step time: 0.1076\n",
      "24/223, train_loss: 0.0959, step time: 0.1038\n",
      "25/223, train_loss: 0.1067, step time: 0.1124\n",
      "26/223, train_loss: 0.1028, step time: 0.1130\n",
      "27/223, train_loss: 0.1055, step time: 0.1080\n",
      "28/223, train_loss: 0.1070, step time: 0.1004\n",
      "29/223, train_loss: 0.1057, step time: 0.1010\n",
      "30/223, train_loss: 0.1092, step time: 0.1151\n",
      "31/223, train_loss: 0.0921, step time: 0.1215\n",
      "32/223, train_loss: 0.0934, step time: 0.1135\n",
      "33/223, train_loss: 0.1080, step time: 0.1041\n",
      "34/223, train_loss: 0.0997, step time: 0.1118\n",
      "35/223, train_loss: 0.0946, step time: 0.1172\n",
      "36/223, train_loss: 0.0984, step time: 0.1056\n",
      "37/223, train_loss: 0.1029, step time: 0.1022\n",
      "38/223, train_loss: 0.1221, step time: 0.0996\n",
      "39/223, train_loss: 0.0916, step time: 0.1109\n",
      "40/223, train_loss: 0.1031, step time: 0.1022\n",
      "41/223, train_loss: 0.0923, step time: 0.1190\n",
      "42/223, train_loss: 0.0909, step time: 0.1002\n",
      "43/223, train_loss: 0.1068, step time: 0.1138\n",
      "44/223, train_loss: 0.1018, step time: 0.1177\n",
      "45/223, train_loss: 0.0945, step time: 0.1130\n",
      "46/223, train_loss: 0.1055, step time: 0.1136\n",
      "47/223, train_loss: 0.1098, step time: 0.1257\n",
      "48/223, train_loss: 0.1028, step time: 0.1001\n",
      "49/223, train_loss: 0.0942, step time: 0.0996\n",
      "50/223, train_loss: 0.1052, step time: 0.1000\n",
      "51/223, train_loss: 0.0945, step time: 0.1102\n",
      "52/223, train_loss: 0.1002, step time: 0.1001\n",
      "53/223, train_loss: 0.1039, step time: 0.1006\n",
      "54/223, train_loss: 0.1044, step time: 0.1183\n",
      "55/223, train_loss: 0.0950, step time: 0.1058\n",
      "56/223, train_loss: 0.1008, step time: 0.1130\n",
      "57/223, train_loss: 0.0949, step time: 0.1004\n",
      "58/223, train_loss: 0.0956, step time: 0.1071\n",
      "59/223, train_loss: 0.0925, step time: 0.1355\n",
      "60/223, train_loss: 0.0968, step time: 0.1060\n",
      "61/223, train_loss: 0.0936, step time: 0.1193\n",
      "62/223, train_loss: 0.0912, step time: 0.1529\n",
      "63/223, train_loss: 0.0970, step time: 0.1242\n",
      "64/223, train_loss: 0.0992, step time: 0.1449\n",
      "65/223, train_loss: 0.1012, step time: 0.1213\n",
      "66/223, train_loss: 0.0986, step time: 0.1130\n",
      "67/223, train_loss: 0.1155, step time: 0.0995\n",
      "68/223, train_loss: 0.0905, step time: 0.1078\n",
      "69/223, train_loss: 0.0919, step time: 0.1116\n",
      "70/223, train_loss: 0.1055, step time: 0.1093\n",
      "71/223, train_loss: 0.0967, step time: 0.1102\n",
      "72/223, train_loss: 0.0955, step time: 0.1006\n",
      "73/223, train_loss: 0.0994, step time: 0.1012\n",
      "74/223, train_loss: 0.0981, step time: 0.1004\n",
      "75/223, train_loss: 0.0993, step time: 0.1076\n",
      "76/223, train_loss: 0.0936, step time: 0.1059\n",
      "77/223, train_loss: 0.0993, step time: 0.1002\n",
      "78/223, train_loss: 0.0962, step time: 0.1082\n",
      "79/223, train_loss: 0.0946, step time: 0.0998\n",
      "80/223, train_loss: 0.0971, step time: 0.0999\n",
      "81/223, train_loss: 0.0983, step time: 0.1003\n",
      "82/223, train_loss: 0.0931, step time: 0.1054\n",
      "83/223, train_loss: 0.3050, step time: 0.1004\n",
      "84/223, train_loss: 0.0928, step time: 0.1007\n",
      "85/223, train_loss: 0.1069, step time: 0.1097\n",
      "86/223, train_loss: 0.0922, step time: 0.1074\n",
      "87/223, train_loss: 0.0965, step time: 0.1004\n",
      "88/223, train_loss: 0.0918, step time: 0.1008\n",
      "89/223, train_loss: 0.0951, step time: 0.1010\n",
      "90/223, train_loss: 0.1056, step time: 0.1071\n",
      "91/223, train_loss: 0.1111, step time: 0.1067\n",
      "92/223, train_loss: 0.1007, step time: 0.1005\n",
      "93/223, train_loss: 0.0945, step time: 0.1008\n",
      "94/223, train_loss: 0.0973, step time: 0.1008\n",
      "95/223, train_loss: 0.0928, step time: 0.1002\n",
      "96/223, train_loss: 0.0976, step time: 0.1000\n",
      "97/223, train_loss: 0.0970, step time: 0.0998\n",
      "98/223, train_loss: 0.0920, step time: 0.1170\n",
      "99/223, train_loss: 0.1007, step time: 0.1094\n",
      "100/223, train_loss: 0.1062, step time: 0.1001\n",
      "101/223, train_loss: 0.0961, step time: 0.0998\n",
      "102/223, train_loss: 0.0995, step time: 0.1215\n",
      "103/223, train_loss: 0.0936, step time: 0.1094\n",
      "104/223, train_loss: 0.0990, step time: 0.1234\n",
      "105/223, train_loss: 0.0945, step time: 0.1004\n",
      "106/223, train_loss: 0.0935, step time: 0.1106\n",
      "107/223, train_loss: 0.0940, step time: 0.1089\n",
      "108/223, train_loss: 0.0896, step time: 0.1008\n",
      "109/223, train_loss: 0.0951, step time: 0.1003\n",
      "110/223, train_loss: 0.1016, step time: 0.1059\n",
      "111/223, train_loss: 0.1034, step time: 0.1011\n",
      "112/223, train_loss: 0.1044, step time: 0.1011\n",
      "113/223, train_loss: 0.0989, step time: 0.1011\n",
      "114/223, train_loss: 0.0879, step time: 0.1416\n",
      "115/223, train_loss: 0.1009, step time: 0.1113\n",
      "116/223, train_loss: 0.0912, step time: 0.1051\n",
      "117/223, train_loss: 0.0984, step time: 0.1012\n",
      "118/223, train_loss: 0.0979, step time: 0.1038\n",
      "119/223, train_loss: 0.0956, step time: 0.1162\n",
      "120/223, train_loss: 0.0999, step time: 0.1068\n",
      "121/223, train_loss: 0.1053, step time: 0.1226\n",
      "122/223, train_loss: 0.0919, step time: 0.1145\n",
      "123/223, train_loss: 0.1004, step time: 0.1150\n",
      "124/223, train_loss: 0.1033, step time: 0.1045\n",
      "125/223, train_loss: 0.0906, step time: 0.1363\n",
      "126/223, train_loss: 0.0934, step time: 0.1008\n",
      "127/223, train_loss: 0.0903, step time: 0.1130\n",
      "128/223, train_loss: 0.0964, step time: 0.1091\n",
      "129/223, train_loss: 0.0910, step time: 0.1221\n",
      "130/223, train_loss: 0.1032, step time: 0.1112\n",
      "131/223, train_loss: 0.1015, step time: 0.1243\n",
      "132/223, train_loss: 0.0977, step time: 0.0998\n",
      "133/223, train_loss: 0.0993, step time: 0.1274\n",
      "134/223, train_loss: 0.1051, step time: 0.1006\n",
      "135/223, train_loss: 0.1023, step time: 0.1107\n",
      "136/223, train_loss: 0.1045, step time: 0.1222\n",
      "137/223, train_loss: 0.1169, step time: 0.1008\n",
      "138/223, train_loss: 0.0974, step time: 0.1051\n",
      "139/223, train_loss: 0.1071, step time: 0.1191\n",
      "140/223, train_loss: 0.0934, step time: 0.1178\n",
      "141/223, train_loss: 0.1111, step time: 0.1006\n",
      "142/223, train_loss: 0.0916, step time: 0.1080\n",
      "143/223, train_loss: 0.1038, step time: 0.1083\n",
      "144/223, train_loss: 0.0878, step time: 0.1085\n",
      "145/223, train_loss: 0.0939, step time: 0.1079\n",
      "146/223, train_loss: 0.1083, step time: 0.1002\n",
      "147/223, train_loss: 0.0923, step time: 0.0998\n",
      "148/223, train_loss: 0.0968, step time: 0.1124\n",
      "149/223, train_loss: 0.0951, step time: 0.1124\n",
      "150/223, train_loss: 0.0897, step time: 0.1088\n",
      "151/223, train_loss: 0.1083, step time: 0.1009\n",
      "152/223, train_loss: 0.1016, step time: 0.1002\n",
      "153/223, train_loss: 0.0965, step time: 0.1183\n",
      "154/223, train_loss: 0.0915, step time: 0.0999\n",
      "155/223, train_loss: 0.0917, step time: 0.1097\n",
      "156/223, train_loss: 0.0974, step time: 0.1074\n",
      "157/223, train_loss: 0.1040, step time: 0.1076\n",
      "158/223, train_loss: 0.1001, step time: 0.1212\n",
      "159/223, train_loss: 0.1095, step time: 0.1215\n",
      "160/223, train_loss: 0.1025, step time: 0.1108\n",
      "161/223, train_loss: 0.1084, step time: 0.1229\n",
      "162/223, train_loss: 0.0925, step time: 0.1063\n",
      "163/223, train_loss: 0.1029, step time: 0.1001\n",
      "164/223, train_loss: 0.0911, step time: 0.1003\n",
      "165/223, train_loss: 0.1124, step time: 0.1000\n",
      "166/223, train_loss: 0.0937, step time: 0.1199\n",
      "167/223, train_loss: 0.1059, step time: 0.1162\n",
      "168/223, train_loss: 0.0963, step time: 0.1033\n",
      "169/223, train_loss: 0.0979, step time: 0.1286\n",
      "170/223, train_loss: 0.1048, step time: 0.1005\n",
      "171/223, train_loss: 0.0986, step time: 0.1005\n",
      "172/223, train_loss: 0.0981, step time: 0.1003\n",
      "173/223, train_loss: 0.0941, step time: 0.1058\n",
      "174/223, train_loss: 0.1002, step time: 0.1355\n",
      "175/223, train_loss: 0.1009, step time: 0.1106\n",
      "176/223, train_loss: 0.1192, step time: 0.1007\n",
      "177/223, train_loss: 0.1072, step time: 0.1008\n",
      "178/223, train_loss: 0.1098, step time: 0.1058\n",
      "179/223, train_loss: 0.0978, step time: 0.1233\n",
      "180/223, train_loss: 0.0981, step time: 0.1003\n",
      "181/223, train_loss: 0.0944, step time: 0.1001\n",
      "182/223, train_loss: 0.0996, step time: 0.1120\n",
      "183/223, train_loss: 0.1029, step time: 0.1008\n",
      "184/223, train_loss: 0.0981, step time: 0.1004\n",
      "185/223, train_loss: 0.1037, step time: 0.1001\n",
      "186/223, train_loss: 0.1126, step time: 0.1074\n",
      "187/223, train_loss: 0.0956, step time: 0.1102\n",
      "188/223, train_loss: 0.0904, step time: 0.1000\n",
      "189/223, train_loss: 0.1019, step time: 0.0996\n",
      "190/223, train_loss: 0.0961, step time: 0.1104\n",
      "191/223, train_loss: 0.1020, step time: 0.1002\n",
      "192/223, train_loss: 0.1072, step time: 0.1287\n",
      "193/223, train_loss: 0.0974, step time: 0.1157\n",
      "194/223, train_loss: 0.0992, step time: 0.1003\n",
      "195/223, train_loss: 0.1028, step time: 0.1146\n",
      "196/223, train_loss: 0.1075, step time: 0.0995\n",
      "197/223, train_loss: 0.1042, step time: 0.1201\n",
      "198/223, train_loss: 0.1039, step time: 0.1244\n",
      "199/223, train_loss: 0.0905, step time: 0.0997\n",
      "200/223, train_loss: 0.0945, step time: 0.1095\n",
      "201/223, train_loss: 0.1125, step time: 0.1090\n",
      "202/223, train_loss: 0.1013, step time: 0.1179\n",
      "203/223, train_loss: 0.1109, step time: 0.1021\n",
      "204/223, train_loss: 0.1091, step time: 0.1307\n",
      "205/223, train_loss: 0.0900, step time: 0.1229\n",
      "206/223, train_loss: 0.0917, step time: 0.1183\n",
      "207/223, train_loss: 0.0934, step time: 0.1107\n",
      "208/223, train_loss: 0.0963, step time: 0.1151\n",
      "209/223, train_loss: 0.1048, step time: 0.1154\n",
      "210/223, train_loss: 0.1096, step time: 0.1315\n",
      "211/223, train_loss: 0.1007, step time: 0.0999\n",
      "212/223, train_loss: 0.1087, step time: 0.1012\n",
      "213/223, train_loss: 0.0959, step time: 0.1006\n",
      "214/223, train_loss: 0.0952, step time: 0.1164\n",
      "215/223, train_loss: 0.0921, step time: 0.1088\n",
      "216/223, train_loss: 0.0948, step time: 0.1043\n",
      "217/223, train_loss: 0.0999, step time: 0.1011\n",
      "218/223, train_loss: 0.1049, step time: 0.1007\n",
      "219/223, train_loss: 0.0943, step time: 0.1005\n",
      "220/223, train_loss: 0.1041, step time: 0.0997\n",
      "221/223, train_loss: 0.1051, step time: 0.1005\n",
      "222/223, train_loss: 0.0955, step time: 0.1021\n",
      "223/223, train_loss: 0.0956, step time: 0.0998\n",
      "epoch 238 average loss: 0.1004\n",
      "time consuming of epoch 238 is: 90.3228\n",
      "----------\n",
      "epoch 239/300\n",
      "1/223, train_loss: 0.0984, step time: 0.1015\n",
      "2/223, train_loss: 0.1015, step time: 0.1036\n",
      "3/223, train_loss: 0.0911, step time: 0.1042\n",
      "4/223, train_loss: 0.1009, step time: 0.1192\n",
      "5/223, train_loss: 0.0996, step time: 0.1187\n",
      "6/223, train_loss: 0.1018, step time: 0.0999\n",
      "7/223, train_loss: 0.1014, step time: 0.1091\n",
      "8/223, train_loss: 0.0902, step time: 0.1005\n",
      "9/223, train_loss: 0.0996, step time: 0.1062\n",
      "10/223, train_loss: 0.0932, step time: 0.1005\n",
      "11/223, train_loss: 0.0941, step time: 0.1139\n",
      "12/223, train_loss: 0.0961, step time: 0.1148\n",
      "13/223, train_loss: 0.1095, step time: 0.1090\n",
      "14/223, train_loss: 0.0921, step time: 0.1127\n",
      "15/223, train_loss: 0.1042, step time: 0.1246\n",
      "16/223, train_loss: 0.0987, step time: 0.1523\n",
      "17/223, train_loss: 0.0971, step time: 0.1062\n",
      "18/223, train_loss: 0.1135, step time: 0.1037\n",
      "19/223, train_loss: 0.0951, step time: 0.1554\n",
      "20/223, train_loss: 0.1010, step time: 0.1046\n",
      "21/223, train_loss: 0.1000, step time: 0.1092\n",
      "22/223, train_loss: 0.1015, step time: 0.1072\n",
      "23/223, train_loss: 0.1067, step time: 0.1145\n",
      "24/223, train_loss: 0.1103, step time: 0.1610\n",
      "25/223, train_loss: 0.1028, step time: 0.1036\n",
      "26/223, train_loss: 0.0967, step time: 0.1342\n",
      "27/223, train_loss: 0.1002, step time: 0.1142\n",
      "28/223, train_loss: 0.0894, step time: 0.1236\n",
      "29/223, train_loss: 0.0982, step time: 0.0998\n",
      "30/223, train_loss: 0.0963, step time: 0.1061\n",
      "31/223, train_loss: 0.1073, step time: 0.1217\n",
      "32/223, train_loss: 0.0949, step time: 0.1287\n",
      "33/223, train_loss: 0.0931, step time: 0.1038\n",
      "34/223, train_loss: 0.1010, step time: 0.1150\n",
      "35/223, train_loss: 0.0979, step time: 0.1117\n",
      "36/223, train_loss: 0.0929, step time: 0.1133\n",
      "37/223, train_loss: 0.1070, step time: 0.0992\n",
      "38/223, train_loss: 0.0957, step time: 0.1220\n",
      "39/223, train_loss: 0.0863, step time: 0.1111\n",
      "40/223, train_loss: 0.0882, step time: 0.1012\n",
      "41/223, train_loss: 0.0943, step time: 0.1084\n",
      "42/223, train_loss: 0.0970, step time: 0.1084\n",
      "43/223, train_loss: 0.1078, step time: 0.1095\n",
      "44/223, train_loss: 0.1012, step time: 0.1235\n",
      "45/223, train_loss: 0.1011, step time: 0.1013\n",
      "46/223, train_loss: 0.0948, step time: 0.1067\n",
      "47/223, train_loss: 0.1008, step time: 0.1030\n",
      "48/223, train_loss: 0.0908, step time: 0.1139\n",
      "49/223, train_loss: 0.0973, step time: 0.1113\n",
      "50/223, train_loss: 0.1163, step time: 0.1145\n",
      "51/223, train_loss: 0.1017, step time: 0.1087\n",
      "52/223, train_loss: 0.1080, step time: 0.1229\n",
      "53/223, train_loss: 0.1122, step time: 0.1137\n",
      "54/223, train_loss: 0.1008, step time: 0.1127\n",
      "55/223, train_loss: 0.1033, step time: 0.1268\n",
      "56/223, train_loss: 0.0953, step time: 0.1145\n",
      "57/223, train_loss: 0.1022, step time: 0.1270\n",
      "58/223, train_loss: 0.0948, step time: 0.1101\n",
      "59/223, train_loss: 0.1037, step time: 0.1176\n",
      "60/223, train_loss: 0.1018, step time: 0.1146\n",
      "61/223, train_loss: 0.0929, step time: 0.1009\n",
      "62/223, train_loss: 0.1000, step time: 0.1053\n",
      "63/223, train_loss: 0.1092, step time: 0.1076\n",
      "64/223, train_loss: 0.0984, step time: 0.1005\n",
      "65/223, train_loss: 0.1041, step time: 0.1031\n",
      "66/223, train_loss: 0.0991, step time: 0.1008\n",
      "67/223, train_loss: 0.0922, step time: 0.1173\n",
      "68/223, train_loss: 0.1025, step time: 0.1123\n",
      "69/223, train_loss: 0.0998, step time: 0.1049\n",
      "70/223, train_loss: 0.1004, step time: 0.1702\n",
      "71/223, train_loss: 0.0901, step time: 0.1183\n",
      "72/223, train_loss: 0.0944, step time: 0.1001\n",
      "73/223, train_loss: 0.0979, step time: 0.1018\n",
      "74/223, train_loss: 0.1051, step time: 0.1444\n",
      "75/223, train_loss: 0.0951, step time: 0.1133\n",
      "76/223, train_loss: 0.1020, step time: 0.1064\n",
      "77/223, train_loss: 0.1039, step time: 0.1048\n",
      "78/223, train_loss: 0.0914, step time: 0.1127\n",
      "79/223, train_loss: 0.0972, step time: 0.1094\n",
      "80/223, train_loss: 0.0998, step time: 0.1211\n",
      "81/223, train_loss: 0.0993, step time: 0.1038\n",
      "82/223, train_loss: 0.0895, step time: 0.1124\n",
      "83/223, train_loss: 0.1000, step time: 0.1368\n",
      "84/223, train_loss: 0.0919, step time: 0.1189\n",
      "85/223, train_loss: 0.0908, step time: 0.1132\n",
      "86/223, train_loss: 0.1015, step time: 0.1481\n",
      "87/223, train_loss: 0.0959, step time: 0.1212\n",
      "88/223, train_loss: 0.1125, step time: 0.1007\n",
      "89/223, train_loss: 0.1018, step time: 0.0999\n",
      "90/223, train_loss: 0.1104, step time: 0.0999\n",
      "91/223, train_loss: 0.1192, step time: 0.1056\n",
      "92/223, train_loss: 0.1037, step time: 0.1035\n",
      "93/223, train_loss: 0.0899, step time: 0.1030\n",
      "94/223, train_loss: 0.1015, step time: 0.1062\n",
      "95/223, train_loss: 0.1044, step time: 0.1063\n",
      "96/223, train_loss: 0.1025, step time: 0.1011\n",
      "97/223, train_loss: 0.1003, step time: 0.1417\n",
      "98/223, train_loss: 0.1221, step time: 0.1146\n",
      "99/223, train_loss: 0.0953, step time: 0.1016\n",
      "100/223, train_loss: 0.1213, step time: 0.1358\n",
      "101/223, train_loss: 0.0963, step time: 0.1155\n",
      "102/223, train_loss: 0.0985, step time: 0.1129\n",
      "103/223, train_loss: 0.1060, step time: 0.1016\n",
      "104/223, train_loss: 0.1036, step time: 0.1272\n",
      "105/223, train_loss: 0.1031, step time: 0.1110\n",
      "106/223, train_loss: 0.1012, step time: 0.1149\n",
      "107/223, train_loss: 0.0972, step time: 0.1001\n",
      "108/223, train_loss: 0.0988, step time: 0.1362\n",
      "109/223, train_loss: 0.0944, step time: 0.1004\n",
      "110/223, train_loss: 0.1115, step time: 0.1262\n",
      "111/223, train_loss: 0.0970, step time: 0.0990\n",
      "112/223, train_loss: 0.0897, step time: 0.1246\n",
      "113/223, train_loss: 0.0925, step time: 0.1117\n",
      "114/223, train_loss: 0.0961, step time: 0.1086\n",
      "115/223, train_loss: 0.0987, step time: 0.1015\n",
      "116/223, train_loss: 0.0949, step time: 0.1300\n",
      "117/223, train_loss: 0.1056, step time: 0.1212\n",
      "118/223, train_loss: 0.0886, step time: 0.1118\n",
      "119/223, train_loss: 0.0953, step time: 0.1005\n",
      "120/223, train_loss: 0.1044, step time: 0.1055\n",
      "121/223, train_loss: 0.0996, step time: 0.1109\n",
      "122/223, train_loss: 0.0932, step time: 0.1133\n",
      "123/223, train_loss: 0.1017, step time: 0.1256\n",
      "124/223, train_loss: 0.1038, step time: 0.1082\n",
      "125/223, train_loss: 0.0998, step time: 0.1071\n",
      "126/223, train_loss: 0.0867, step time: 0.1058\n",
      "127/223, train_loss: 0.1015, step time: 0.1083\n",
      "128/223, train_loss: 0.1027, step time: 0.1307\n",
      "129/223, train_loss: 0.0931, step time: 0.1130\n",
      "130/223, train_loss: 0.0958, step time: 0.1070\n",
      "131/223, train_loss: 0.0915, step time: 0.1130\n",
      "132/223, train_loss: 0.0912, step time: 0.1132\n",
      "133/223, train_loss: 0.1017, step time: 0.1147\n",
      "134/223, train_loss: 0.1068, step time: 0.1017\n",
      "135/223, train_loss: 0.1054, step time: 0.1191\n",
      "136/223, train_loss: 0.0929, step time: 0.1250\n",
      "137/223, train_loss: 0.1133, step time: 0.1013\n",
      "138/223, train_loss: 0.1055, step time: 0.1103\n",
      "139/223, train_loss: 0.0990, step time: 0.1152\n",
      "140/223, train_loss: 0.0991, step time: 0.1000\n",
      "141/223, train_loss: 0.1040, step time: 0.1069\n",
      "142/223, train_loss: 0.0991, step time: 0.1131\n",
      "143/223, train_loss: 0.1008, step time: 0.1227\n",
      "144/223, train_loss: 0.1013, step time: 0.1305\n",
      "145/223, train_loss: 0.0886, step time: 0.1368\n",
      "146/223, train_loss: 0.0936, step time: 0.1005\n",
      "147/223, train_loss: 0.1023, step time: 0.1005\n",
      "148/223, train_loss: 0.0952, step time: 0.1006\n",
      "149/223, train_loss: 0.1001, step time: 0.1379\n",
      "150/223, train_loss: 0.0929, step time: 0.0994\n",
      "151/223, train_loss: 0.1116, step time: 0.0997\n",
      "152/223, train_loss: 0.1022, step time: 0.1004\n",
      "153/223, train_loss: 0.0997, step time: 0.1178\n",
      "154/223, train_loss: 0.0918, step time: 0.0995\n",
      "155/223, train_loss: 0.1028, step time: 0.1002\n",
      "156/223, train_loss: 0.0902, step time: 0.1006\n",
      "157/223, train_loss: 0.0941, step time: 0.0999\n",
      "158/223, train_loss: 0.1018, step time: 0.1007\n",
      "159/223, train_loss: 0.0941, step time: 0.1006\n",
      "160/223, train_loss: 0.0951, step time: 0.1368\n",
      "161/223, train_loss: 0.1010, step time: 0.1038\n",
      "162/223, train_loss: 0.0858, step time: 0.1008\n",
      "163/223, train_loss: 0.1052, step time: 0.1024\n",
      "164/223, train_loss: 0.1014, step time: 0.1003\n",
      "165/223, train_loss: 0.1049, step time: 0.1147\n",
      "166/223, train_loss: 0.1158, step time: 0.1099\n",
      "167/223, train_loss: 0.0921, step time: 0.1314\n",
      "168/223, train_loss: 0.1001, step time: 0.1396\n",
      "169/223, train_loss: 0.1100, step time: 0.1205\n",
      "170/223, train_loss: 0.1081, step time: 0.1187\n",
      "171/223, train_loss: 0.1043, step time: 0.1061\n",
      "172/223, train_loss: 0.1172, step time: 0.1014\n",
      "173/223, train_loss: 0.1022, step time: 0.1167\n",
      "174/223, train_loss: 0.0912, step time: 0.1141\n",
      "175/223, train_loss: 0.0921, step time: 0.1099\n",
      "176/223, train_loss: 0.0973, step time: 0.1069\n",
      "177/223, train_loss: 0.0984, step time: 0.1171\n",
      "178/223, train_loss: 0.1057, step time: 0.1073\n",
      "179/223, train_loss: 0.0904, step time: 0.1070\n",
      "180/223, train_loss: 0.0959, step time: 0.1131\n",
      "181/223, train_loss: 0.0979, step time: 0.1231\n",
      "182/223, train_loss: 0.1061, step time: 0.1099\n",
      "183/223, train_loss: 0.0975, step time: 0.1202\n",
      "184/223, train_loss: 0.0918, step time: 0.1155\n",
      "185/223, train_loss: 0.0991, step time: 0.0998\n",
      "186/223, train_loss: 0.1060, step time: 0.1092\n",
      "187/223, train_loss: 0.1035, step time: 0.1052\n",
      "188/223, train_loss: 0.0933, step time: 0.1121\n",
      "189/223, train_loss: 0.1045, step time: 0.1248\n",
      "190/223, train_loss: 0.0968, step time: 0.1091\n",
      "191/223, train_loss: 0.0966, step time: 0.0994\n",
      "192/223, train_loss: 0.0978, step time: 0.1364\n",
      "193/223, train_loss: 0.1102, step time: 0.0998\n",
      "194/223, train_loss: 0.1051, step time: 0.1121\n",
      "195/223, train_loss: 0.0946, step time: 0.1045\n",
      "196/223, train_loss: 0.0876, step time: 0.1051\n",
      "197/223, train_loss: 0.0905, step time: 0.1071\n",
      "198/223, train_loss: 0.0911, step time: 0.1044\n",
      "199/223, train_loss: 0.1087, step time: 0.1226\n",
      "200/223, train_loss: 0.0986, step time: 0.1071\n",
      "201/223, train_loss: 0.0996, step time: 0.1091\n",
      "202/223, train_loss: 0.0994, step time: 0.1171\n",
      "203/223, train_loss: 0.0945, step time: 0.1031\n",
      "204/223, train_loss: 0.0930, step time: 0.1043\n",
      "205/223, train_loss: 0.1025, step time: 0.1152\n",
      "206/223, train_loss: 0.1035, step time: 0.1155\n",
      "207/223, train_loss: 0.0914, step time: 0.1038\n",
      "208/223, train_loss: 0.1005, step time: 0.1132\n",
      "209/223, train_loss: 0.0844, step time: 0.1018\n",
      "210/223, train_loss: 0.0993, step time: 0.1113\n",
      "211/223, train_loss: 0.1032, step time: 0.1148\n",
      "212/223, train_loss: 0.1121, step time: 0.1141\n",
      "213/223, train_loss: 0.0971, step time: 0.1178\n",
      "214/223, train_loss: 0.0950, step time: 0.1226\n",
      "215/223, train_loss: 0.3079, step time: 0.1010\n",
      "216/223, train_loss: 0.1000, step time: 0.1049\n",
      "217/223, train_loss: 0.0997, step time: 0.1047\n",
      "218/223, train_loss: 0.1012, step time: 0.1003\n",
      "219/223, train_loss: 0.1165, step time: 0.1013\n",
      "220/223, train_loss: 0.0949, step time: 0.1002\n",
      "221/223, train_loss: 0.1009, step time: 0.1011\n",
      "222/223, train_loss: 0.1010, step time: 0.0999\n",
      "223/223, train_loss: 0.0995, step time: 0.1000\n",
      "epoch 239 average loss: 0.1005\n",
      "time consuming of epoch 239 is: 92.3139\n",
      "----------\n",
      "epoch 240/300\n",
      "1/223, train_loss: 0.1126, step time: 0.1198\n",
      "2/223, train_loss: 0.0914, step time: 0.1020\n",
      "3/223, train_loss: 0.1074, step time: 0.1155\n",
      "4/223, train_loss: 0.1040, step time: 0.1161\n",
      "5/223, train_loss: 0.0934, step time: 0.1172\n",
      "6/223, train_loss: 0.0978, step time: 0.1100\n",
      "7/223, train_loss: 0.1042, step time: 0.0998\n",
      "8/223, train_loss: 0.0955, step time: 0.1164\n",
      "9/223, train_loss: 0.0886, step time: 0.1168\n",
      "10/223, train_loss: 0.1024, step time: 0.1028\n",
      "11/223, train_loss: 0.1001, step time: 0.1187\n",
      "12/223, train_loss: 0.0938, step time: 0.1008\n",
      "13/223, train_loss: 0.0915, step time: 0.1167\n",
      "14/223, train_loss: 0.0889, step time: 0.1483\n",
      "15/223, train_loss: 0.0950, step time: 0.1213\n",
      "16/223, train_loss: 0.1085, step time: 0.1206\n",
      "17/223, train_loss: 0.0920, step time: 0.1026\n",
      "18/223, train_loss: 0.0986, step time: 0.0999\n",
      "19/223, train_loss: 0.0987, step time: 0.1340\n",
      "20/223, train_loss: 0.1105, step time: 0.1108\n",
      "21/223, train_loss: 0.1069, step time: 0.1046\n",
      "22/223, train_loss: 0.0951, step time: 0.1114\n",
      "23/223, train_loss: 0.1038, step time: 0.1097\n",
      "24/223, train_loss: 0.1013, step time: 0.1273\n",
      "25/223, train_loss: 0.0959, step time: 0.1108\n",
      "26/223, train_loss: 0.3040, step time: 0.1061\n",
      "27/223, train_loss: 0.0988, step time: 0.1038\n",
      "28/223, train_loss: 0.1034, step time: 0.1155\n",
      "29/223, train_loss: 0.0988, step time: 0.1081\n",
      "30/223, train_loss: 0.1002, step time: 0.0998\n",
      "31/223, train_loss: 0.1073, step time: 0.1057\n",
      "32/223, train_loss: 0.0929, step time: 0.0996\n",
      "33/223, train_loss: 0.0951, step time: 0.1089\n",
      "34/223, train_loss: 0.0981, step time: 0.1090\n",
      "35/223, train_loss: 0.1045, step time: 0.1183\n",
      "36/223, train_loss: 0.0965, step time: 0.1231\n",
      "37/223, train_loss: 0.1049, step time: 0.1158\n",
      "38/223, train_loss: 0.0953, step time: 0.1125\n",
      "39/223, train_loss: 0.0898, step time: 0.1045\n",
      "40/223, train_loss: 0.1084, step time: 0.1031\n",
      "41/223, train_loss: 0.0989, step time: 0.1156\n",
      "42/223, train_loss: 0.0988, step time: 0.1043\n",
      "43/223, train_loss: 0.0962, step time: 0.1100\n",
      "44/223, train_loss: 0.1031, step time: 0.1052\n",
      "45/223, train_loss: 0.0937, step time: 0.1004\n",
      "46/223, train_loss: 0.1095, step time: 0.1013\n",
      "47/223, train_loss: 0.1064, step time: 0.1029\n",
      "48/223, train_loss: 0.1037, step time: 0.0998\n",
      "49/223, train_loss: 0.1029, step time: 0.1071\n",
      "50/223, train_loss: 0.0883, step time: 0.1001\n",
      "51/223, train_loss: 0.0954, step time: 0.1042\n",
      "52/223, train_loss: 0.1026, step time: 0.1075\n",
      "53/223, train_loss: 0.0976, step time: 0.1074\n",
      "54/223, train_loss: 0.0994, step time: 0.1079\n",
      "55/223, train_loss: 0.1041, step time: 0.1082\n",
      "56/223, train_loss: 0.1061, step time: 0.1016\n",
      "57/223, train_loss: 0.0902, step time: 0.1006\n",
      "58/223, train_loss: 0.0980, step time: 0.1151\n",
      "59/223, train_loss: 0.0911, step time: 0.1008\n",
      "60/223, train_loss: 0.1071, step time: 0.1011\n",
      "61/223, train_loss: 0.0891, step time: 0.1049\n",
      "62/223, train_loss: 0.1038, step time: 0.1091\n",
      "63/223, train_loss: 0.0975, step time: 0.1105\n",
      "64/223, train_loss: 0.0987, step time: 0.1106\n",
      "65/223, train_loss: 0.0984, step time: 0.1200\n",
      "66/223, train_loss: 0.1141, step time: 0.1063\n",
      "67/223, train_loss: 0.1012, step time: 0.1084\n",
      "68/223, train_loss: 0.1040, step time: 0.1176\n",
      "69/223, train_loss: 0.0956, step time: 0.1131\n",
      "70/223, train_loss: 0.1074, step time: 0.1140\n",
      "71/223, train_loss: 0.1031, step time: 0.1263\n",
      "72/223, train_loss: 0.1012, step time: 0.1589\n",
      "73/223, train_loss: 0.0952, step time: 0.1111\n",
      "74/223, train_loss: 0.0950, step time: 0.1001\n",
      "75/223, train_loss: 0.0961, step time: 0.1155\n",
      "76/223, train_loss: 0.0983, step time: 0.1294\n",
      "77/223, train_loss: 0.1067, step time: 0.1005\n",
      "78/223, train_loss: 0.0970, step time: 0.1000\n",
      "79/223, train_loss: 0.1046, step time: 0.1007\n",
      "80/223, train_loss: 0.1046, step time: 0.1007\n",
      "81/223, train_loss: 0.0923, step time: 0.1002\n",
      "82/223, train_loss: 0.0987, step time: 0.0998\n",
      "83/223, train_loss: 0.0961, step time: 0.1004\n",
      "84/223, train_loss: 0.0889, step time: 0.1007\n",
      "85/223, train_loss: 0.0988, step time: 0.0997\n",
      "86/223, train_loss: 0.0945, step time: 0.0994\n",
      "87/223, train_loss: 0.0894, step time: 0.1022\n",
      "88/223, train_loss: 0.1002, step time: 0.1011\n",
      "89/223, train_loss: 0.1027, step time: 0.1016\n",
      "90/223, train_loss: 0.1085, step time: 0.1221\n",
      "91/223, train_loss: 0.0992, step time: 0.1236\n",
      "92/223, train_loss: 0.0987, step time: 0.1126\n",
      "93/223, train_loss: 0.0943, step time: 0.1142\n",
      "94/223, train_loss: 0.1028, step time: 0.1046\n",
      "95/223, train_loss: 0.0988, step time: 0.1133\n",
      "96/223, train_loss: 0.0884, step time: 0.1176\n",
      "97/223, train_loss: 0.1120, step time: 0.1140\n",
      "98/223, train_loss: 0.1067, step time: 0.1551\n",
      "99/223, train_loss: 0.0983, step time: 0.1365\n",
      "100/223, train_loss: 0.1031, step time: 0.1009\n",
      "101/223, train_loss: 0.0980, step time: 0.1207\n",
      "102/223, train_loss: 0.1083, step time: 0.1095\n",
      "103/223, train_loss: 0.0940, step time: 0.1164\n",
      "104/223, train_loss: 0.1013, step time: 0.1232\n",
      "105/223, train_loss: 0.1088, step time: 0.1127\n",
      "106/223, train_loss: 0.1047, step time: 0.1008\n",
      "107/223, train_loss: 0.0945, step time: 0.1164\n",
      "108/223, train_loss: 0.1234, step time: 0.1044\n",
      "109/223, train_loss: 0.1059, step time: 0.1079\n",
      "110/223, train_loss: 0.1005, step time: 0.1187\n",
      "111/223, train_loss: 0.1045, step time: 0.1224\n",
      "112/223, train_loss: 0.0931, step time: 0.1043\n",
      "113/223, train_loss: 0.1038, step time: 0.1190\n",
      "114/223, train_loss: 0.1084, step time: 0.0982\n",
      "115/223, train_loss: 0.1013, step time: 0.0995\n",
      "116/223, train_loss: 0.1131, step time: 0.1229\n",
      "117/223, train_loss: 0.0881, step time: 0.1053\n",
      "118/223, train_loss: 0.1111, step time: 0.1011\n",
      "119/223, train_loss: 0.1083, step time: 0.1008\n",
      "120/223, train_loss: 0.0936, step time: 0.1278\n",
      "121/223, train_loss: 0.1030, step time: 0.1027\n",
      "122/223, train_loss: 0.1066, step time: 0.1118\n",
      "123/223, train_loss: 0.0934, step time: 0.1174\n",
      "124/223, train_loss: 0.1057, step time: 0.1044\n",
      "125/223, train_loss: 0.0912, step time: 0.1127\n",
      "126/223, train_loss: 0.0924, step time: 0.1233\n",
      "127/223, train_loss: 0.0911, step time: 0.1018\n",
      "128/223, train_loss: 0.0914, step time: 0.1047\n",
      "129/223, train_loss: 0.1051, step time: 0.1237\n",
      "130/223, train_loss: 0.0972, step time: 0.1048\n",
      "131/223, train_loss: 0.0939, step time: 0.1044\n",
      "132/223, train_loss: 0.0976, step time: 0.1475\n",
      "133/223, train_loss: 0.0957, step time: 0.1154\n",
      "134/223, train_loss: 0.0998, step time: 0.1266\n",
      "135/223, train_loss: 0.0970, step time: 0.1671\n",
      "136/223, train_loss: 0.0927, step time: 0.1277\n",
      "137/223, train_loss: 0.1020, step time: 0.1056\n",
      "138/223, train_loss: 0.0940, step time: 0.1101\n",
      "139/223, train_loss: 0.0934, step time: 0.1056\n",
      "140/223, train_loss: 0.0999, step time: 0.1235\n",
      "141/223, train_loss: 0.0917, step time: 0.1144\n",
      "142/223, train_loss: 0.1145, step time: 0.1352\n",
      "143/223, train_loss: 0.0990, step time: 0.1146\n",
      "144/223, train_loss: 0.1026, step time: 0.1124\n",
      "145/223, train_loss: 0.1107, step time: 0.1008\n",
      "146/223, train_loss: 0.0848, step time: 0.0998\n",
      "147/223, train_loss: 0.0941, step time: 0.1076\n",
      "148/223, train_loss: 0.1003, step time: 0.1109\n",
      "149/223, train_loss: 0.0981, step time: 0.1254\n",
      "150/223, train_loss: 0.0900, step time: 0.1138\n",
      "151/223, train_loss: 0.0956, step time: 0.1061\n",
      "152/223, train_loss: 0.0931, step time: 0.1078\n",
      "153/223, train_loss: 0.1054, step time: 0.1222\n",
      "154/223, train_loss: 0.0924, step time: 0.1109\n",
      "155/223, train_loss: 0.1043, step time: 0.1298\n",
      "156/223, train_loss: 0.0942, step time: 0.1009\n",
      "157/223, train_loss: 0.0844, step time: 0.1123\n",
      "158/223, train_loss: 0.1087, step time: 0.1147\n",
      "159/223, train_loss: 0.1038, step time: 0.1490\n",
      "160/223, train_loss: 0.1025, step time: 0.1078\n",
      "161/223, train_loss: 0.0908, step time: 0.1166\n",
      "162/223, train_loss: 0.0968, step time: 0.1347\n",
      "163/223, train_loss: 0.1058, step time: 0.1486\n",
      "164/223, train_loss: 0.1006, step time: 0.1012\n",
      "165/223, train_loss: 0.1030, step time: 0.1033\n",
      "166/223, train_loss: 0.0929, step time: 0.1089\n",
      "167/223, train_loss: 0.0924, step time: 0.1001\n",
      "168/223, train_loss: 0.1067, step time: 0.1005\n",
      "169/223, train_loss: 0.1003, step time: 0.1071\n",
      "170/223, train_loss: 0.0994, step time: 0.1087\n",
      "171/223, train_loss: 0.0914, step time: 0.1046\n",
      "172/223, train_loss: 0.1137, step time: 0.1080\n",
      "173/223, train_loss: 0.1042, step time: 0.1234\n",
      "174/223, train_loss: 0.0971, step time: 0.1104\n",
      "175/223, train_loss: 0.1015, step time: 0.1006\n",
      "176/223, train_loss: 0.0975, step time: 0.1098\n",
      "177/223, train_loss: 0.1074, step time: 0.1109\n",
      "178/223, train_loss: 0.1017, step time: 0.1066\n",
      "179/223, train_loss: 0.0969, step time: 0.1039\n",
      "180/223, train_loss: 0.0969, step time: 0.1382\n",
      "181/223, train_loss: 0.0968, step time: 0.1012\n",
      "182/223, train_loss: 0.0981, step time: 0.1048\n",
      "183/223, train_loss: 0.1003, step time: 0.1055\n",
      "184/223, train_loss: 0.0941, step time: 0.1065\n",
      "185/223, train_loss: 0.0996, step time: 0.1029\n",
      "186/223, train_loss: 0.0935, step time: 0.1048\n",
      "187/223, train_loss: 0.0998, step time: 0.1266\n",
      "188/223, train_loss: 0.0839, step time: 0.1277\n",
      "189/223, train_loss: 0.0981, step time: 0.1056\n",
      "190/223, train_loss: 0.0999, step time: 0.1042\n",
      "191/223, train_loss: 0.0982, step time: 0.1373\n",
      "192/223, train_loss: 0.0924, step time: 0.1007\n",
      "193/223, train_loss: 0.0923, step time: 0.1378\n",
      "194/223, train_loss: 0.0925, step time: 0.1068\n",
      "195/223, train_loss: 0.0936, step time: 0.1005\n",
      "196/223, train_loss: 0.0986, step time: 0.1213\n",
      "197/223, train_loss: 0.0861, step time: 0.0995\n",
      "198/223, train_loss: 0.0945, step time: 0.1004\n",
      "199/223, train_loss: 0.1066, step time: 0.1000\n",
      "200/223, train_loss: 0.0904, step time: 0.1008\n",
      "201/223, train_loss: 0.1085, step time: 0.1007\n",
      "202/223, train_loss: 0.1084, step time: 0.1175\n",
      "203/223, train_loss: 0.1128, step time: 0.1266\n",
      "204/223, train_loss: 0.0959, step time: 0.1173\n",
      "205/223, train_loss: 0.1127, step time: 0.1097\n",
      "206/223, train_loss: 0.1098, step time: 0.1014\n",
      "207/223, train_loss: 0.0982, step time: 0.1128\n",
      "208/223, train_loss: 0.0907, step time: 0.1324\n",
      "209/223, train_loss: 0.1069, step time: 0.1055\n",
      "210/223, train_loss: 0.0896, step time: 0.1137\n",
      "211/223, train_loss: 0.0968, step time: 0.1211\n",
      "212/223, train_loss: 0.1068, step time: 0.1286\n",
      "213/223, train_loss: 0.0947, step time: 0.1159\n",
      "214/223, train_loss: 0.0940, step time: 0.1090\n",
      "215/223, train_loss: 0.0989, step time: 0.1162\n",
      "216/223, train_loss: 0.1006, step time: 0.1131\n",
      "217/223, train_loss: 0.1006, step time: 0.1000\n",
      "218/223, train_loss: 0.0895, step time: 0.1211\n",
      "219/223, train_loss: 0.0949, step time: 0.1181\n",
      "220/223, train_loss: 0.1137, step time: 0.1240\n",
      "221/223, train_loss: 0.1094, step time: 0.0995\n",
      "222/223, train_loss: 0.1095, step time: 0.0990\n",
      "223/223, train_loss: 0.0968, step time: 0.0995\n",
      "epoch 240 average loss: 0.1003\n",
      "current epoch: 240 current mean dice: 0.8619 tc: 0.9224 wt: 0.8715 et: 0.7918\n",
      "best mean dice: 0.8620 at epoch: 235\n",
      "time consuming of epoch 240 is: 91.5399\n",
      "----------\n",
      "epoch 241/300\n",
      "1/223, train_loss: 0.0998, step time: 0.1187\n",
      "2/223, train_loss: 0.1024, step time: 0.1005\n",
      "3/223, train_loss: 0.0928, step time: 0.1010\n",
      "4/223, train_loss: 0.0968, step time: 0.1204\n",
      "5/223, train_loss: 0.0992, step time: 0.1064\n",
      "6/223, train_loss: 0.1077, step time: 0.1099\n",
      "7/223, train_loss: 0.0931, step time: 0.1120\n",
      "8/223, train_loss: 0.0952, step time: 0.1141\n",
      "9/223, train_loss: 0.0963, step time: 0.1030\n",
      "10/223, train_loss: 0.1046, step time: 0.0994\n",
      "11/223, train_loss: 0.1060, step time: 0.1113\n",
      "12/223, train_loss: 0.1048, step time: 0.1144\n",
      "13/223, train_loss: 0.1083, step time: 0.1028\n",
      "14/223, train_loss: 0.1049, step time: 0.1103\n",
      "15/223, train_loss: 0.1000, step time: 0.1161\n",
      "16/223, train_loss: 0.0995, step time: 0.1052\n",
      "17/223, train_loss: 0.0912, step time: 0.1049\n",
      "18/223, train_loss: 0.1017, step time: 0.1051\n",
      "19/223, train_loss: 0.0981, step time: 0.0992\n",
      "20/223, train_loss: 0.0913, step time: 0.1190\n",
      "21/223, train_loss: 0.1019, step time: 0.1061\n",
      "22/223, train_loss: 0.1064, step time: 0.1032\n",
      "23/223, train_loss: 0.1003, step time: 0.1069\n",
      "24/223, train_loss: 0.1002, step time: 0.1088\n",
      "25/223, train_loss: 0.0933, step time: 0.1128\n",
      "26/223, train_loss: 0.0977, step time: 0.1063\n",
      "27/223, train_loss: 0.0939, step time: 0.1048\n",
      "28/223, train_loss: 0.0988, step time: 0.0990\n",
      "29/223, train_loss: 0.0949, step time: 0.1212\n",
      "30/223, train_loss: 0.0991, step time: 0.1036\n",
      "31/223, train_loss: 0.1025, step time: 0.1153\n",
      "32/223, train_loss: 0.1041, step time: 0.1214\n",
      "33/223, train_loss: 0.0939, step time: 0.1047\n",
      "34/223, train_loss: 0.0986, step time: 0.1144\n",
      "35/223, train_loss: 0.0995, step time: 0.1138\n",
      "36/223, train_loss: 0.1085, step time: 0.1093\n",
      "37/223, train_loss: 0.0959, step time: 0.0997\n",
      "38/223, train_loss: 0.0991, step time: 0.1072\n",
      "39/223, train_loss: 0.1000, step time: 0.1135\n",
      "40/223, train_loss: 0.1094, step time: 0.1217\n",
      "41/223, train_loss: 0.1177, step time: 0.1022\n",
      "42/223, train_loss: 0.0886, step time: 0.1084\n",
      "43/223, train_loss: 0.0999, step time: 0.1156\n",
      "44/223, train_loss: 0.0982, step time: 0.1109\n",
      "45/223, train_loss: 0.0976, step time: 0.1051\n",
      "46/223, train_loss: 0.0841, step time: 0.1170\n",
      "47/223, train_loss: 0.0936, step time: 0.1080\n",
      "48/223, train_loss: 0.1049, step time: 0.1092\n",
      "49/223, train_loss: 0.0975, step time: 0.1000\n",
      "50/223, train_loss: 0.0966, step time: 0.1093\n",
      "51/223, train_loss: 0.1072, step time: 0.1139\n",
      "52/223, train_loss: 0.1032, step time: 0.1299\n",
      "53/223, train_loss: 0.1006, step time: 0.1180\n",
      "54/223, train_loss: 0.1010, step time: 0.1048\n",
      "55/223, train_loss: 0.0910, step time: 0.1066\n",
      "56/223, train_loss: 0.1043, step time: 0.1111\n",
      "57/223, train_loss: 0.0922, step time: 0.1151\n",
      "58/223, train_loss: 0.1052, step time: 0.1413\n",
      "59/223, train_loss: 0.1031, step time: 0.1215\n",
      "60/223, train_loss: 0.1001, step time: 0.1231\n",
      "61/223, train_loss: 0.0939, step time: 0.1193\n",
      "62/223, train_loss: 0.0955, step time: 0.1272\n",
      "63/223, train_loss: 0.0979, step time: 0.1121\n",
      "64/223, train_loss: 0.1061, step time: 0.1154\n",
      "65/223, train_loss: 0.0914, step time: 0.1133\n",
      "66/223, train_loss: 0.0961, step time: 0.1166\n",
      "67/223, train_loss: 0.0966, step time: 0.1205\n",
      "68/223, train_loss: 0.0930, step time: 0.1126\n",
      "69/223, train_loss: 0.0987, step time: 0.1155\n",
      "70/223, train_loss: 0.1096, step time: 0.1203\n",
      "71/223, train_loss: 0.1004, step time: 0.1207\n",
      "72/223, train_loss: 0.1020, step time: 0.1068\n",
      "73/223, train_loss: 0.1182, step time: 0.1027\n",
      "74/223, train_loss: 0.1020, step time: 0.1051\n",
      "75/223, train_loss: 0.0952, step time: 0.1333\n",
      "76/223, train_loss: 0.1079, step time: 0.1181\n",
      "77/223, train_loss: 0.0883, step time: 0.1066\n",
      "78/223, train_loss: 0.1059, step time: 0.1598\n",
      "79/223, train_loss: 0.0945, step time: 0.0988\n",
      "80/223, train_loss: 0.0950, step time: 0.1024\n",
      "81/223, train_loss: 0.1028, step time: 0.0999\n",
      "82/223, train_loss: 0.1104, step time: 0.0986\n",
      "83/223, train_loss: 0.0985, step time: 0.1080\n",
      "84/223, train_loss: 0.1116, step time: 0.1022\n",
      "85/223, train_loss: 0.0931, step time: 0.1110\n",
      "86/223, train_loss: 0.0926, step time: 0.0992\n",
      "87/223, train_loss: 0.1144, step time: 0.1240\n",
      "88/223, train_loss: 0.1018, step time: 0.1179\n",
      "89/223, train_loss: 0.0976, step time: 0.1124\n",
      "90/223, train_loss: 0.1081, step time: 0.0990\n",
      "91/223, train_loss: 0.1072, step time: 0.0983\n",
      "92/223, train_loss: 0.0950, step time: 0.1029\n",
      "93/223, train_loss: 0.0935, step time: 0.1020\n",
      "94/223, train_loss: 0.0842, step time: 0.1243\n",
      "95/223, train_loss: 0.1059, step time: 0.1166\n",
      "96/223, train_loss: 0.0997, step time: 0.1271\n",
      "97/223, train_loss: 0.1033, step time: 0.1064\n",
      "98/223, train_loss: 0.0888, step time: 0.1070\n",
      "99/223, train_loss: 0.0983, step time: 0.1466\n",
      "100/223, train_loss: 0.0949, step time: 0.1168\n",
      "101/223, train_loss: 0.1056, step time: 0.1022\n",
      "102/223, train_loss: 0.0901, step time: 0.0995\n",
      "103/223, train_loss: 0.0938, step time: 0.0991\n",
      "104/223, train_loss: 0.1017, step time: 0.1023\n",
      "105/223, train_loss: 0.0975, step time: 0.1130\n",
      "106/223, train_loss: 0.1018, step time: 0.1309\n",
      "107/223, train_loss: 0.0967, step time: 0.1263\n",
      "108/223, train_loss: 0.1019, step time: 0.1056\n",
      "109/223, train_loss: 0.0999, step time: 0.1135\n",
      "110/223, train_loss: 0.0976, step time: 0.1053\n",
      "111/223, train_loss: 0.0931, step time: 0.1316\n",
      "112/223, train_loss: 0.1063, step time: 0.1231\n",
      "113/223, train_loss: 0.0912, step time: 0.1102\n",
      "114/223, train_loss: 0.0995, step time: 0.1092\n",
      "115/223, train_loss: 0.0977, step time: 0.1260\n",
      "116/223, train_loss: 0.0979, step time: 0.0994\n",
      "117/223, train_loss: 0.0976, step time: 0.1055\n",
      "118/223, train_loss: 0.2945, step time: 0.1292\n",
      "119/223, train_loss: 0.0961, step time: 0.1009\n",
      "120/223, train_loss: 0.0991, step time: 0.1005\n",
      "121/223, train_loss: 0.1007, step time: 0.1003\n",
      "122/223, train_loss: 0.1004, step time: 0.0993\n",
      "123/223, train_loss: 0.0956, step time: 0.1003\n",
      "124/223, train_loss: 0.1015, step time: 0.1008\n",
      "125/223, train_loss: 0.0892, step time: 0.0997\n",
      "126/223, train_loss: 0.1055, step time: 0.0999\n",
      "127/223, train_loss: 0.1050, step time: 0.1002\n",
      "128/223, train_loss: 0.0978, step time: 0.1079\n",
      "129/223, train_loss: 0.1134, step time: 0.0994\n",
      "130/223, train_loss: 0.0974, step time: 0.0997\n",
      "131/223, train_loss: 0.1030, step time: 0.1000\n",
      "132/223, train_loss: 0.0982, step time: 0.1176\n",
      "133/223, train_loss: 0.1064, step time: 0.0987\n",
      "134/223, train_loss: 0.0957, step time: 0.0994\n",
      "135/223, train_loss: 0.1026, step time: 0.0996\n",
      "136/223, train_loss: 0.1059, step time: 0.1002\n",
      "137/223, train_loss: 0.0918, step time: 0.0994\n",
      "138/223, train_loss: 0.0880, step time: 0.0983\n",
      "139/223, train_loss: 0.1129, step time: 0.1078\n",
      "140/223, train_loss: 0.1213, step time: 0.1007\n",
      "141/223, train_loss: 0.1008, step time: 0.1057\n",
      "142/223, train_loss: 0.1012, step time: 0.1129\n",
      "143/223, train_loss: 0.1083, step time: 0.1079\n",
      "144/223, train_loss: 0.0993, step time: 0.1444\n",
      "145/223, train_loss: 0.1038, step time: 0.1221\n",
      "146/223, train_loss: 0.1024, step time: 0.1137\n",
      "147/223, train_loss: 0.0944, step time: 0.1149\n",
      "148/223, train_loss: 0.1012, step time: 0.1247\n",
      "149/223, train_loss: 0.0933, step time: 0.1176\n",
      "150/223, train_loss: 0.1030, step time: 0.1048\n",
      "151/223, train_loss: 0.0936, step time: 0.1013\n",
      "152/223, train_loss: 0.1082, step time: 0.1113\n",
      "153/223, train_loss: 0.1005, step time: 0.1106\n",
      "154/223, train_loss: 0.1083, step time: 0.1177\n",
      "155/223, train_loss: 0.1102, step time: 0.1208\n",
      "156/223, train_loss: 0.1033, step time: 0.0998\n",
      "157/223, train_loss: 0.1057, step time: 0.1021\n",
      "158/223, train_loss: 0.0980, step time: 0.1182\n",
      "159/223, train_loss: 0.0898, step time: 0.1040\n",
      "160/223, train_loss: 0.1000, step time: 0.0998\n",
      "161/223, train_loss: 0.1005, step time: 0.0996\n",
      "162/223, train_loss: 0.1080, step time: 0.1086\n",
      "163/223, train_loss: 0.1014, step time: 0.1268\n",
      "164/223, train_loss: 0.0946, step time: 0.1036\n",
      "165/223, train_loss: 0.1010, step time: 0.1155\n",
      "166/223, train_loss: 0.1010, step time: 0.1009\n",
      "167/223, train_loss: 0.0986, step time: 0.1130\n",
      "168/223, train_loss: 0.0925, step time: 0.1038\n",
      "169/223, train_loss: 0.0937, step time: 0.1220\n",
      "170/223, train_loss: 0.0867, step time: 0.1039\n",
      "171/223, train_loss: 0.0936, step time: 0.1085\n",
      "172/223, train_loss: 0.0952, step time: 0.1131\n",
      "173/223, train_loss: 0.0983, step time: 0.1119\n",
      "174/223, train_loss: 0.0955, step time: 0.1007\n",
      "175/223, train_loss: 0.0988, step time: 0.1232\n",
      "176/223, train_loss: 0.0961, step time: 0.1088\n",
      "177/223, train_loss: 0.0951, step time: 0.1204\n",
      "178/223, train_loss: 0.0945, step time: 0.1059\n",
      "179/223, train_loss: 0.1036, step time: 0.1105\n",
      "180/223, train_loss: 0.0965, step time: 0.1066\n",
      "181/223, train_loss: 0.1066, step time: 0.1097\n",
      "182/223, train_loss: 0.1190, step time: 0.1005\n",
      "183/223, train_loss: 0.0992, step time: 0.1212\n",
      "184/223, train_loss: 0.0977, step time: 0.1128\n",
      "185/223, train_loss: 0.0955, step time: 0.1100\n",
      "186/223, train_loss: 0.0907, step time: 0.1227\n",
      "187/223, train_loss: 0.1090, step time: 0.1397\n",
      "188/223, train_loss: 0.0985, step time: 0.1264\n",
      "189/223, train_loss: 0.0937, step time: 0.0998\n",
      "190/223, train_loss: 0.0974, step time: 0.1010\n",
      "191/223, train_loss: 0.0944, step time: 0.1006\n",
      "192/223, train_loss: 0.1076, step time: 0.1030\n",
      "193/223, train_loss: 0.0988, step time: 0.1013\n",
      "194/223, train_loss: 0.1044, step time: 0.1103\n",
      "195/223, train_loss: 0.1002, step time: 0.1024\n",
      "196/223, train_loss: 0.0879, step time: 0.1015\n",
      "197/223, train_loss: 0.1080, step time: 0.1102\n",
      "198/223, train_loss: 0.0980, step time: 0.1006\n",
      "199/223, train_loss: 0.1075, step time: 0.0998\n",
      "200/223, train_loss: 0.0962, step time: 0.1006\n",
      "201/223, train_loss: 0.0983, step time: 0.1003\n",
      "202/223, train_loss: 0.0965, step time: 0.1000\n",
      "203/223, train_loss: 0.0965, step time: 0.1007\n",
      "204/223, train_loss: 0.0963, step time: 0.0999\n",
      "205/223, train_loss: 0.0880, step time: 0.1001\n",
      "206/223, train_loss: 0.1016, step time: 0.1081\n",
      "207/223, train_loss: 0.1016, step time: 0.1774\n",
      "208/223, train_loss: 0.0930, step time: 0.1012\n",
      "209/223, train_loss: 0.0922, step time: 0.1004\n",
      "210/223, train_loss: 0.0891, step time: 0.1151\n",
      "211/223, train_loss: 0.1082, step time: 0.1007\n",
      "212/223, train_loss: 0.0941, step time: 0.1005\n",
      "213/223, train_loss: 0.1000, step time: 0.1143\n",
      "214/223, train_loss: 0.0961, step time: 0.1004\n",
      "215/223, train_loss: 0.0946, step time: 0.0997\n",
      "216/223, train_loss: 0.0968, step time: 0.1003\n",
      "217/223, train_loss: 0.1007, step time: 0.1015\n",
      "218/223, train_loss: 0.0996, step time: 0.0994\n",
      "219/223, train_loss: 0.1000, step time: 0.0989\n",
      "220/223, train_loss: 0.0891, step time: 0.0996\n",
      "221/223, train_loss: 0.1119, step time: 0.1049\n",
      "222/223, train_loss: 0.0964, step time: 0.1004\n",
      "223/223, train_loss: 0.0989, step time: 0.1003\n",
      "epoch 241 average loss: 0.1004\n",
      "time consuming of epoch 241 is: 94.5946\n",
      "----------\n",
      "epoch 242/300\n",
      "1/223, train_loss: 0.1109, step time: 0.1072\n",
      "2/223, train_loss: 0.1071, step time: 0.0993\n",
      "3/223, train_loss: 0.1000, step time: 0.1011\n",
      "4/223, train_loss: 0.0887, step time: 0.1054\n",
      "5/223, train_loss: 0.2901, step time: 0.1175\n",
      "6/223, train_loss: 0.0922, step time: 0.1121\n",
      "7/223, train_loss: 0.1047, step time: 0.1007\n",
      "8/223, train_loss: 0.0988, step time: 0.1007\n",
      "9/223, train_loss: 0.0956, step time: 0.1223\n",
      "10/223, train_loss: 0.0907, step time: 0.1064\n",
      "11/223, train_loss: 0.1012, step time: 0.1207\n",
      "12/223, train_loss: 0.1071, step time: 0.1230\n",
      "13/223, train_loss: 0.1016, step time: 0.1286\n",
      "14/223, train_loss: 0.1028, step time: 0.1332\n",
      "15/223, train_loss: 0.0988, step time: 0.1161\n",
      "16/223, train_loss: 0.1110, step time: 0.1161\n",
      "17/223, train_loss: 0.0873, step time: 0.1150\n",
      "18/223, train_loss: 0.1066, step time: 0.1141\n",
      "19/223, train_loss: 0.0982, step time: 0.1188\n",
      "20/223, train_loss: 0.0862, step time: 0.1062\n",
      "21/223, train_loss: 0.0948, step time: 0.1080\n",
      "22/223, train_loss: 0.1006, step time: 0.1043\n",
      "23/223, train_loss: 0.1017, step time: 0.1206\n",
      "24/223, train_loss: 0.0992, step time: 0.1151\n",
      "25/223, train_loss: 0.0922, step time: 0.1178\n",
      "26/223, train_loss: 0.1026, step time: 0.1121\n",
      "27/223, train_loss: 0.1036, step time: 0.1041\n",
      "28/223, train_loss: 0.0972, step time: 0.1043\n",
      "29/223, train_loss: 0.1016, step time: 0.1172\n",
      "30/223, train_loss: 0.1110, step time: 0.1358\n",
      "31/223, train_loss: 0.0965, step time: 0.1482\n",
      "32/223, train_loss: 0.0895, step time: 0.1006\n",
      "33/223, train_loss: 0.1018, step time: 0.1303\n",
      "34/223, train_loss: 0.1052, step time: 0.1044\n",
      "35/223, train_loss: 0.0969, step time: 0.1291\n",
      "36/223, train_loss: 0.0996, step time: 0.1059\n",
      "37/223, train_loss: 0.1066, step time: 0.1228\n",
      "38/223, train_loss: 0.0964, step time: 0.1056\n",
      "39/223, train_loss: 0.0948, step time: 0.1144\n",
      "40/223, train_loss: 0.0904, step time: 0.1098\n",
      "41/223, train_loss: 0.0964, step time: 0.1309\n",
      "42/223, train_loss: 0.1022, step time: 0.1102\n",
      "43/223, train_loss: 0.1003, step time: 0.1036\n",
      "44/223, train_loss: 0.0975, step time: 0.1096\n",
      "45/223, train_loss: 0.1004, step time: 0.1115\n",
      "46/223, train_loss: 0.0999, step time: 0.1106\n",
      "47/223, train_loss: 0.0932, step time: 0.1325\n",
      "48/223, train_loss: 0.1008, step time: 0.1244\n",
      "49/223, train_loss: 0.0848, step time: 0.1052\n",
      "50/223, train_loss: 0.0871, step time: 0.1015\n",
      "51/223, train_loss: 0.1066, step time: 0.1060\n",
      "52/223, train_loss: 0.1127, step time: 0.1402\n",
      "53/223, train_loss: 0.1085, step time: 0.1006\n",
      "54/223, train_loss: 0.1063, step time: 0.1064\n",
      "55/223, train_loss: 0.1054, step time: 0.1042\n",
      "56/223, train_loss: 0.0975, step time: 0.1001\n",
      "57/223, train_loss: 0.1024, step time: 0.1113\n",
      "58/223, train_loss: 0.1047, step time: 0.1010\n",
      "59/223, train_loss: 0.1069, step time: 0.1010\n",
      "60/223, train_loss: 0.1020, step time: 0.1332\n",
      "61/223, train_loss: 0.0932, step time: 0.1005\n",
      "62/223, train_loss: 0.1088, step time: 0.1009\n",
      "63/223, train_loss: 0.0942, step time: 0.0996\n",
      "64/223, train_loss: 0.0951, step time: 0.1181\n",
      "65/223, train_loss: 0.0974, step time: 0.1189\n",
      "66/223, train_loss: 0.0966, step time: 0.1034\n",
      "67/223, train_loss: 0.0923, step time: 0.1006\n",
      "68/223, train_loss: 0.0981, step time: 0.1074\n",
      "69/223, train_loss: 0.1008, step time: 0.1141\n",
      "70/223, train_loss: 0.1072, step time: 0.1051\n",
      "71/223, train_loss: 0.0972, step time: 0.1059\n",
      "72/223, train_loss: 0.1047, step time: 0.1071\n",
      "73/223, train_loss: 0.1008, step time: 0.1151\n",
      "74/223, train_loss: 0.1081, step time: 0.1157\n",
      "75/223, train_loss: 0.1006, step time: 0.1024\n",
      "76/223, train_loss: 0.0972, step time: 0.1018\n",
      "77/223, train_loss: 0.0966, step time: 0.1376\n",
      "78/223, train_loss: 0.1100, step time: 0.1073\n",
      "79/223, train_loss: 0.0957, step time: 0.0998\n",
      "80/223, train_loss: 0.1009, step time: 0.1001\n",
      "81/223, train_loss: 0.0941, step time: 0.1118\n",
      "82/223, train_loss: 0.1084, step time: 0.1021\n",
      "83/223, train_loss: 0.0961, step time: 0.1381\n",
      "84/223, train_loss: 0.1015, step time: 0.1039\n",
      "85/223, train_loss: 0.1082, step time: 0.1002\n",
      "86/223, train_loss: 0.1107, step time: 0.1184\n",
      "87/223, train_loss: 0.1027, step time: 0.1255\n",
      "88/223, train_loss: 0.1015, step time: 0.1575\n",
      "89/223, train_loss: 0.1075, step time: 0.1057\n",
      "90/223, train_loss: 0.0908, step time: 0.1144\n",
      "91/223, train_loss: 0.1051, step time: 0.1496\n",
      "92/223, train_loss: 0.1036, step time: 0.1062\n",
      "93/223, train_loss: 0.0975, step time: 0.1091\n",
      "94/223, train_loss: 0.0967, step time: 0.1018\n",
      "95/223, train_loss: 0.0859, step time: 0.1303\n",
      "96/223, train_loss: 0.0941, step time: 0.1057\n",
      "97/223, train_loss: 0.1018, step time: 0.1003\n",
      "98/223, train_loss: 0.0936, step time: 0.1108\n",
      "99/223, train_loss: 0.1131, step time: 0.1014\n",
      "100/223, train_loss: 0.0999, step time: 0.1016\n",
      "101/223, train_loss: 0.1008, step time: 0.0994\n",
      "102/223, train_loss: 0.0963, step time: 0.0991\n",
      "103/223, train_loss: 0.0935, step time: 0.0997\n",
      "104/223, train_loss: 0.0921, step time: 0.1001\n",
      "105/223, train_loss: 0.0989, step time: 0.0997\n",
      "106/223, train_loss: 0.1004, step time: 0.0995\n",
      "107/223, train_loss: 0.0959, step time: 0.1003\n",
      "108/223, train_loss: 0.0995, step time: 0.1018\n",
      "109/223, train_loss: 0.0994, step time: 0.0993\n",
      "110/223, train_loss: 0.0992, step time: 0.0993\n",
      "111/223, train_loss: 0.0896, step time: 0.0996\n",
      "112/223, train_loss: 0.0978, step time: 0.1126\n",
      "113/223, train_loss: 0.1040, step time: 0.1001\n",
      "114/223, train_loss: 0.0910, step time: 0.0994\n",
      "115/223, train_loss: 0.0897, step time: 0.0994\n",
      "116/223, train_loss: 0.0982, step time: 0.0999\n",
      "117/223, train_loss: 0.0908, step time: 0.0992\n",
      "118/223, train_loss: 0.0972, step time: 0.0998\n",
      "119/223, train_loss: 0.0935, step time: 0.0994\n",
      "120/223, train_loss: 0.1003, step time: 0.1116\n",
      "121/223, train_loss: 0.1023, step time: 0.0988\n",
      "122/223, train_loss: 0.0963, step time: 0.0991\n",
      "123/223, train_loss: 0.1062, step time: 0.0988\n",
      "124/223, train_loss: 0.0965, step time: 0.1018\n",
      "125/223, train_loss: 0.0933, step time: 0.1153\n",
      "126/223, train_loss: 0.0997, step time: 0.1065\n",
      "127/223, train_loss: 0.1195, step time: 0.1131\n",
      "128/223, train_loss: 0.0943, step time: 0.1130\n",
      "129/223, train_loss: 0.1009, step time: 0.1088\n",
      "130/223, train_loss: 0.0957, step time: 0.1202\n",
      "131/223, train_loss: 0.0962, step time: 0.1074\n",
      "132/223, train_loss: 0.0974, step time: 0.0996\n",
      "133/223, train_loss: 0.1013, step time: 0.1064\n",
      "134/223, train_loss: 0.0891, step time: 0.1065\n",
      "135/223, train_loss: 0.1011, step time: 0.1072\n",
      "136/223, train_loss: 0.1077, step time: 0.1121\n",
      "137/223, train_loss: 0.1032, step time: 0.1076\n",
      "138/223, train_loss: 0.1063, step time: 0.1164\n",
      "139/223, train_loss: 0.1092, step time: 0.1188\n",
      "140/223, train_loss: 0.1049, step time: 0.1285\n",
      "141/223, train_loss: 0.1073, step time: 0.1064\n",
      "142/223, train_loss: 0.0978, step time: 0.1047\n",
      "143/223, train_loss: 0.1069, step time: 0.1124\n",
      "144/223, train_loss: 0.1125, step time: 0.1023\n",
      "145/223, train_loss: 0.0993, step time: 0.1220\n",
      "146/223, train_loss: 0.1108, step time: 0.1352\n",
      "147/223, train_loss: 0.1141, step time: 0.1565\n",
      "148/223, train_loss: 0.1108, step time: 0.1307\n",
      "149/223, train_loss: 0.0915, step time: 0.1089\n",
      "150/223, train_loss: 0.1001, step time: 0.1027\n",
      "151/223, train_loss: 0.0948, step time: 0.1033\n",
      "152/223, train_loss: 0.0983, step time: 0.1140\n",
      "153/223, train_loss: 0.0900, step time: 0.1009\n",
      "154/223, train_loss: 0.0924, step time: 0.1012\n",
      "155/223, train_loss: 0.1020, step time: 0.1006\n",
      "156/223, train_loss: 0.1034, step time: 0.1398\n",
      "157/223, train_loss: 0.1027, step time: 0.1244\n",
      "158/223, train_loss: 0.0929, step time: 0.0999\n",
      "159/223, train_loss: 0.0874, step time: 0.1005\n",
      "160/223, train_loss: 0.0980, step time: 0.1021\n",
      "161/223, train_loss: 0.0850, step time: 0.1152\n",
      "162/223, train_loss: 0.0945, step time: 0.1327\n",
      "163/223, train_loss: 0.1056, step time: 0.1288\n",
      "164/223, train_loss: 0.1044, step time: 0.1070\n",
      "165/223, train_loss: 0.0988, step time: 0.1193\n",
      "166/223, train_loss: 0.0982, step time: 0.1210\n",
      "167/223, train_loss: 0.0933, step time: 0.1142\n",
      "168/223, train_loss: 0.1019, step time: 0.1257\n",
      "169/223, train_loss: 0.0913, step time: 0.1164\n",
      "170/223, train_loss: 0.0929, step time: 0.1236\n",
      "171/223, train_loss: 0.0975, step time: 0.1224\n",
      "172/223, train_loss: 0.1054, step time: 0.1028\n",
      "173/223, train_loss: 0.1023, step time: 0.1006\n",
      "174/223, train_loss: 0.0968, step time: 0.1008\n",
      "175/223, train_loss: 0.1001, step time: 0.1054\n",
      "176/223, train_loss: 0.1018, step time: 0.1009\n",
      "177/223, train_loss: 0.1042, step time: 0.1103\n",
      "178/223, train_loss: 0.0945, step time: 0.1065\n",
      "179/223, train_loss: 0.0967, step time: 0.1006\n",
      "180/223, train_loss: 0.1100, step time: 0.1016\n",
      "181/223, train_loss: 0.1041, step time: 0.1138\n",
      "182/223, train_loss: 0.1082, step time: 0.1121\n",
      "183/223, train_loss: 0.1087, step time: 0.1055\n",
      "184/223, train_loss: 0.0979, step time: 0.1078\n",
      "185/223, train_loss: 0.1076, step time: 0.1056\n",
      "186/223, train_loss: 0.0968, step time: 0.0998\n",
      "187/223, train_loss: 0.0927, step time: 0.1070\n",
      "188/223, train_loss: 0.0944, step time: 0.1064\n",
      "189/223, train_loss: 0.0973, step time: 0.1176\n",
      "190/223, train_loss: 0.1005, step time: 0.1115\n",
      "191/223, train_loss: 0.0862, step time: 0.1822\n",
      "192/223, train_loss: 0.0953, step time: 0.1014\n",
      "193/223, train_loss: 0.0951, step time: 0.1239\n",
      "194/223, train_loss: 0.0968, step time: 0.1512\n",
      "195/223, train_loss: 0.1014, step time: 0.1385\n",
      "196/223, train_loss: 0.0925, step time: 0.1170\n",
      "197/223, train_loss: 0.1108, step time: 0.1238\n",
      "198/223, train_loss: 0.0981, step time: 0.1005\n",
      "199/223, train_loss: 0.1049, step time: 0.1000\n",
      "200/223, train_loss: 0.1020, step time: 0.1240\n",
      "201/223, train_loss: 0.1029, step time: 0.1190\n",
      "202/223, train_loss: 0.1064, step time: 0.1173\n",
      "203/223, train_loss: 0.0942, step time: 0.1172\n",
      "204/223, train_loss: 0.0961, step time: 0.1164\n",
      "205/223, train_loss: 0.1022, step time: 0.0999\n",
      "206/223, train_loss: 0.1071, step time: 0.1073\n",
      "207/223, train_loss: 0.0957, step time: 0.1173\n",
      "208/223, train_loss: 0.0933, step time: 0.1226\n",
      "209/223, train_loss: 0.0980, step time: 0.1045\n",
      "210/223, train_loss: 0.1004, step time: 0.1016\n",
      "211/223, train_loss: 0.0972, step time: 0.1204\n",
      "212/223, train_loss: 0.0970, step time: 0.1074\n",
      "213/223, train_loss: 0.1072, step time: 0.1092\n",
      "214/223, train_loss: 0.1018, step time: 0.1074\n",
      "215/223, train_loss: 0.0921, step time: 0.1098\n",
      "216/223, train_loss: 0.0986, step time: 0.1375\n",
      "217/223, train_loss: 0.0988, step time: 0.1154\n",
      "218/223, train_loss: 0.0945, step time: 0.1252\n",
      "219/223, train_loss: 0.1014, step time: 0.1118\n",
      "220/223, train_loss: 0.0910, step time: 0.0998\n",
      "221/223, train_loss: 0.0965, step time: 0.0986\n",
      "222/223, train_loss: 0.1130, step time: 0.0993\n",
      "223/223, train_loss: 0.0953, step time: 0.0994\n",
      "epoch 242 average loss: 0.1004\n",
      "time consuming of epoch 242 is: 93.8314\n",
      "----------\n",
      "epoch 243/300\n",
      "1/223, train_loss: 0.1046, step time: 0.1019\n",
      "2/223, train_loss: 0.0963, step time: 0.1016\n",
      "3/223, train_loss: 0.0920, step time: 0.1003\n",
      "4/223, train_loss: 0.1073, step time: 0.1032\n",
      "5/223, train_loss: 0.0963, step time: 0.1058\n",
      "6/223, train_loss: 0.0968, step time: 0.1132\n",
      "7/223, train_loss: 0.0940, step time: 0.1181\n",
      "8/223, train_loss: 0.0983, step time: 0.1007\n",
      "9/223, train_loss: 0.0997, step time: 0.1258\n",
      "10/223, train_loss: 0.1047, step time: 0.1031\n",
      "11/223, train_loss: 0.0985, step time: 0.1133\n",
      "12/223, train_loss: 0.1034, step time: 0.1252\n",
      "13/223, train_loss: 0.0966, step time: 0.1186\n",
      "14/223, train_loss: 0.1115, step time: 0.1286\n",
      "15/223, train_loss: 0.1071, step time: 0.1149\n",
      "16/223, train_loss: 0.0905, step time: 0.1076\n",
      "17/223, train_loss: 0.1021, step time: 0.1061\n",
      "18/223, train_loss: 0.1083, step time: 0.1189\n",
      "19/223, train_loss: 0.1069, step time: 0.1054\n",
      "20/223, train_loss: 0.0980, step time: 0.1051\n",
      "21/223, train_loss: 0.1124, step time: 0.1096\n",
      "22/223, train_loss: 0.1073, step time: 0.1060\n",
      "23/223, train_loss: 0.1006, step time: 0.1265\n",
      "24/223, train_loss: 0.0874, step time: 0.1152\n",
      "25/223, train_loss: 0.1003, step time: 0.1103\n",
      "26/223, train_loss: 0.1010, step time: 0.1194\n",
      "27/223, train_loss: 0.1058, step time: 0.1172\n",
      "28/223, train_loss: 0.1027, step time: 0.1195\n",
      "29/223, train_loss: 0.1065, step time: 0.1072\n",
      "30/223, train_loss: 0.0936, step time: 0.1129\n",
      "31/223, train_loss: 0.0931, step time: 0.1125\n",
      "32/223, train_loss: 0.1130, step time: 0.1006\n",
      "33/223, train_loss: 0.1007, step time: 0.1085\n",
      "34/223, train_loss: 0.0985, step time: 0.1026\n",
      "35/223, train_loss: 0.1075, step time: 0.1043\n",
      "36/223, train_loss: 0.0915, step time: 0.1029\n",
      "37/223, train_loss: 0.0910, step time: 0.1272\n",
      "38/223, train_loss: 0.1017, step time: 0.1057\n",
      "39/223, train_loss: 0.1039, step time: 0.1072\n",
      "40/223, train_loss: 0.0954, step time: 0.1272\n",
      "41/223, train_loss: 0.1099, step time: 0.1090\n",
      "42/223, train_loss: 0.1006, step time: 0.1124\n",
      "43/223, train_loss: 0.0917, step time: 0.1202\n",
      "44/223, train_loss: 0.1032, step time: 0.1049\n",
      "45/223, train_loss: 0.0906, step time: 0.1160\n",
      "46/223, train_loss: 0.0920, step time: 0.1106\n",
      "47/223, train_loss: 0.0893, step time: 0.1009\n",
      "48/223, train_loss: 0.0978, step time: 0.1056\n",
      "49/223, train_loss: 0.0992, step time: 0.1045\n",
      "50/223, train_loss: 0.0977, step time: 0.1102\n",
      "51/223, train_loss: 0.0942, step time: 0.1001\n",
      "52/223, train_loss: 0.0901, step time: 0.1017\n",
      "53/223, train_loss: 0.0957, step time: 0.1292\n",
      "54/223, train_loss: 0.0872, step time: 0.1152\n",
      "55/223, train_loss: 0.0950, step time: 0.1093\n",
      "56/223, train_loss: 0.1014, step time: 0.1023\n",
      "57/223, train_loss: 0.0946, step time: 0.1494\n",
      "58/223, train_loss: 0.0994, step time: 0.1160\n",
      "59/223, train_loss: 0.0928, step time: 0.1009\n",
      "60/223, train_loss: 0.1013, step time: 0.0996\n",
      "61/223, train_loss: 0.1012, step time: 0.0984\n",
      "62/223, train_loss: 0.0961, step time: 0.1053\n",
      "63/223, train_loss: 0.1042, step time: 0.1192\n",
      "64/223, train_loss: 0.0919, step time: 0.1022\n",
      "65/223, train_loss: 0.1032, step time: 0.0998\n",
      "66/223, train_loss: 0.0897, step time: 0.1029\n",
      "67/223, train_loss: 0.0999, step time: 0.1005\n",
      "68/223, train_loss: 0.0993, step time: 0.1004\n",
      "69/223, train_loss: 0.1023, step time: 0.1161\n",
      "70/223, train_loss: 0.1099, step time: 0.1111\n",
      "71/223, train_loss: 0.1031, step time: 0.1108\n",
      "72/223, train_loss: 0.0953, step time: 0.1027\n",
      "73/223, train_loss: 0.3053, step time: 0.1126\n",
      "74/223, train_loss: 0.0879, step time: 0.1021\n",
      "75/223, train_loss: 0.0935, step time: 0.1244\n",
      "76/223, train_loss: 0.1008, step time: 0.1137\n",
      "77/223, train_loss: 0.0982, step time: 0.1143\n",
      "78/223, train_loss: 0.1048, step time: 0.1185\n",
      "79/223, train_loss: 0.0989, step time: 0.1230\n",
      "80/223, train_loss: 0.1066, step time: 0.1223\n",
      "81/223, train_loss: 0.0903, step time: 0.1227\n",
      "82/223, train_loss: 0.0951, step time: 0.1017\n",
      "83/223, train_loss: 0.1069, step time: 0.1014\n",
      "84/223, train_loss: 0.0971, step time: 0.1161\n",
      "85/223, train_loss: 0.0915, step time: 0.1002\n",
      "86/223, train_loss: 0.0877, step time: 0.1157\n",
      "87/223, train_loss: 0.0972, step time: 0.1159\n",
      "88/223, train_loss: 0.0952, step time: 0.1162\n",
      "89/223, train_loss: 0.0993, step time: 0.1125\n",
      "90/223, train_loss: 0.0938, step time: 0.1105\n",
      "91/223, train_loss: 0.0930, step time: 0.1219\n",
      "92/223, train_loss: 0.0926, step time: 0.1246\n",
      "93/223, train_loss: 0.1068, step time: 0.1001\n",
      "94/223, train_loss: 0.1044, step time: 0.1021\n",
      "95/223, train_loss: 0.0992, step time: 0.1163\n",
      "96/223, train_loss: 0.1077, step time: 0.1080\n",
      "97/223, train_loss: 0.0878, step time: 0.1001\n",
      "98/223, train_loss: 0.1035, step time: 0.1154\n",
      "99/223, train_loss: 0.1005, step time: 0.1057\n",
      "100/223, train_loss: 0.1002, step time: 0.1048\n",
      "101/223, train_loss: 0.1033, step time: 0.1076\n",
      "102/223, train_loss: 0.1023, step time: 0.1191\n",
      "103/223, train_loss: 0.0930, step time: 0.1023\n",
      "104/223, train_loss: 0.0970, step time: 0.0995\n",
      "105/223, train_loss: 0.0943, step time: 0.1002\n",
      "106/223, train_loss: 0.1012, step time: 0.1043\n",
      "107/223, train_loss: 0.0983, step time: 0.1001\n",
      "108/223, train_loss: 0.0987, step time: 0.1118\n",
      "109/223, train_loss: 0.1019, step time: 0.1147\n",
      "110/223, train_loss: 0.1071, step time: 0.1137\n",
      "111/223, train_loss: 0.1035, step time: 0.1039\n",
      "112/223, train_loss: 0.0913, step time: 0.1171\n",
      "113/223, train_loss: 0.1072, step time: 0.1140\n",
      "114/223, train_loss: 0.0910, step time: 0.1426\n",
      "115/223, train_loss: 0.1056, step time: 0.1130\n",
      "116/223, train_loss: 0.0968, step time: 0.1068\n",
      "117/223, train_loss: 0.0895, step time: 0.1068\n",
      "118/223, train_loss: 0.0983, step time: 0.1013\n",
      "119/223, train_loss: 0.1103, step time: 0.1104\n",
      "120/223, train_loss: 0.1028, step time: 0.1106\n",
      "121/223, train_loss: 0.1017, step time: 0.1056\n",
      "122/223, train_loss: 0.1146, step time: 0.1078\n",
      "123/223, train_loss: 0.1001, step time: 0.1027\n",
      "124/223, train_loss: 0.1090, step time: 0.1074\n",
      "125/223, train_loss: 0.0914, step time: 0.1004\n",
      "126/223, train_loss: 0.0976, step time: 0.1017\n",
      "127/223, train_loss: 0.0934, step time: 0.1093\n",
      "128/223, train_loss: 0.1075, step time: 0.1163\n",
      "129/223, train_loss: 0.0870, step time: 0.0994\n",
      "130/223, train_loss: 0.0968, step time: 0.1207\n",
      "131/223, train_loss: 0.1035, step time: 0.1067\n",
      "132/223, train_loss: 0.1053, step time: 0.1041\n",
      "133/223, train_loss: 0.1073, step time: 0.0988\n",
      "134/223, train_loss: 0.1091, step time: 0.0987\n",
      "135/223, train_loss: 0.0954, step time: 0.1167\n",
      "136/223, train_loss: 0.1040, step time: 0.1080\n",
      "137/223, train_loss: 0.1015, step time: 0.0996\n",
      "138/223, train_loss: 0.1052, step time: 0.1088\n",
      "139/223, train_loss: 0.1073, step time: 0.1037\n",
      "140/223, train_loss: 0.1117, step time: 0.1025\n",
      "141/223, train_loss: 0.0891, step time: 0.1139\n",
      "142/223, train_loss: 0.1034, step time: 0.1032\n",
      "143/223, train_loss: 0.0962, step time: 0.1084\n",
      "144/223, train_loss: 0.0956, step time: 0.1088\n",
      "145/223, train_loss: 0.1161, step time: 0.1000\n",
      "146/223, train_loss: 0.1064, step time: 0.1004\n",
      "147/223, train_loss: 0.0884, step time: 0.1112\n",
      "148/223, train_loss: 0.1007, step time: 0.1159\n",
      "149/223, train_loss: 0.0870, step time: 0.1009\n",
      "150/223, train_loss: 0.1064, step time: 0.1014\n",
      "151/223, train_loss: 0.1131, step time: 0.1063\n",
      "152/223, train_loss: 0.0976, step time: 0.1051\n",
      "153/223, train_loss: 0.1081, step time: 0.1001\n",
      "154/223, train_loss: 0.0926, step time: 0.1004\n",
      "155/223, train_loss: 0.0956, step time: 0.1162\n",
      "156/223, train_loss: 0.0982, step time: 0.1053\n",
      "157/223, train_loss: 0.0953, step time: 0.1142\n",
      "158/223, train_loss: 0.0972, step time: 0.1008\n",
      "159/223, train_loss: 0.0901, step time: 0.1004\n",
      "160/223, train_loss: 0.0867, step time: 0.1088\n",
      "161/223, train_loss: 0.0991, step time: 0.1225\n",
      "162/223, train_loss: 0.0886, step time: 0.0993\n",
      "163/223, train_loss: 0.0997, step time: 0.1056\n",
      "164/223, train_loss: 0.0911, step time: 0.1005\n",
      "165/223, train_loss: 0.1073, step time: 0.1059\n",
      "166/223, train_loss: 0.1028, step time: 0.1078\n",
      "167/223, train_loss: 0.0958, step time: 0.1058\n",
      "168/223, train_loss: 0.1016, step time: 0.1004\n",
      "169/223, train_loss: 0.1015, step time: 0.1002\n",
      "170/223, train_loss: 0.0901, step time: 0.1006\n",
      "171/223, train_loss: 0.1156, step time: 0.1166\n",
      "172/223, train_loss: 0.0965, step time: 0.1108\n",
      "173/223, train_loss: 0.0957, step time: 0.1120\n",
      "174/223, train_loss: 0.0963, step time: 0.1083\n",
      "175/223, train_loss: 0.1107, step time: 0.1240\n",
      "176/223, train_loss: 0.0929, step time: 0.1149\n",
      "177/223, train_loss: 0.1041, step time: 0.1006\n",
      "178/223, train_loss: 0.1081, step time: 0.1027\n",
      "179/223, train_loss: 0.0981, step time: 0.1049\n",
      "180/223, train_loss: 0.0998, step time: 0.1002\n",
      "181/223, train_loss: 0.0995, step time: 0.1008\n",
      "182/223, train_loss: 0.1011, step time: 0.1018\n",
      "183/223, train_loss: 0.1007, step time: 0.1071\n",
      "184/223, train_loss: 0.1020, step time: 0.1022\n",
      "185/223, train_loss: 0.0970, step time: 0.1151\n",
      "186/223, train_loss: 0.0961, step time: 0.1197\n",
      "187/223, train_loss: 0.1019, step time: 0.1126\n",
      "188/223, train_loss: 0.0970, step time: 0.1094\n",
      "189/223, train_loss: 0.1026, step time: 0.1189\n",
      "190/223, train_loss: 0.1045, step time: 0.1067\n",
      "191/223, train_loss: 0.1031, step time: 0.1046\n",
      "192/223, train_loss: 0.1127, step time: 0.0997\n",
      "193/223, train_loss: 0.1002, step time: 0.1054\n",
      "194/223, train_loss: 0.1019, step time: 0.1296\n",
      "195/223, train_loss: 0.1000, step time: 0.1002\n",
      "196/223, train_loss: 0.0952, step time: 0.1002\n",
      "197/223, train_loss: 0.0973, step time: 0.1089\n",
      "198/223, train_loss: 0.0947, step time: 0.1059\n",
      "199/223, train_loss: 0.1185, step time: 0.1219\n",
      "200/223, train_loss: 0.0919, step time: 0.1025\n",
      "201/223, train_loss: 0.1028, step time: 0.1039\n",
      "202/223, train_loss: 0.0908, step time: 0.1254\n",
      "203/223, train_loss: 0.1065, step time: 0.1004\n",
      "204/223, train_loss: 0.0996, step time: 0.1071\n",
      "205/223, train_loss: 0.0889, step time: 0.1147\n",
      "206/223, train_loss: 0.0894, step time: 0.1133\n",
      "207/223, train_loss: 0.0986, step time: 0.1111\n",
      "208/223, train_loss: 0.0978, step time: 0.1158\n",
      "209/223, train_loss: 0.1015, step time: 0.1110\n",
      "210/223, train_loss: 0.0898, step time: 0.1218\n",
      "211/223, train_loss: 0.0901, step time: 0.1002\n",
      "212/223, train_loss: 0.0964, step time: 0.1013\n",
      "213/223, train_loss: 0.1088, step time: 0.1089\n",
      "214/223, train_loss: 0.0991, step time: 0.1169\n",
      "215/223, train_loss: 0.0977, step time: 0.1454\n",
      "216/223, train_loss: 0.0953, step time: 0.1705\n",
      "217/223, train_loss: 0.1007, step time: 0.1286\n",
      "218/223, train_loss: 0.0985, step time: 0.1223\n",
      "219/223, train_loss: 0.0869, step time: 0.1119\n",
      "220/223, train_loss: 0.0996, step time: 0.1005\n",
      "221/223, train_loss: 0.1029, step time: 0.1002\n",
      "222/223, train_loss: 0.1029, step time: 0.0995\n",
      "223/223, train_loss: 0.1134, step time: 0.0981\n",
      "epoch 243 average loss: 0.1003\n",
      "time consuming of epoch 243 is: 87.7135\n",
      "----------\n",
      "epoch 244/300\n",
      "1/223, train_loss: 0.0942, step time: 0.1006\n",
      "2/223, train_loss: 0.1075, step time: 0.1069\n",
      "3/223, train_loss: 0.1020, step time: 0.1012\n",
      "4/223, train_loss: 0.1000, step time: 0.1001\n",
      "5/223, train_loss: 0.0998, step time: 0.1006\n",
      "6/223, train_loss: 0.1124, step time: 0.1004\n",
      "7/223, train_loss: 0.0975, step time: 0.1218\n",
      "8/223, train_loss: 0.1136, step time: 0.1004\n",
      "9/223, train_loss: 0.0933, step time: 0.1226\n",
      "10/223, train_loss: 0.1045, step time: 0.1519\n",
      "11/223, train_loss: 0.0969, step time: 0.1255\n",
      "12/223, train_loss: 0.1042, step time: 0.1036\n",
      "13/223, train_loss: 0.0970, step time: 0.1137\n",
      "14/223, train_loss: 0.0943, step time: 0.0999\n",
      "15/223, train_loss: 0.0941, step time: 0.1105\n",
      "16/223, train_loss: 0.0859, step time: 0.1076\n",
      "17/223, train_loss: 0.1028, step time: 0.1010\n",
      "18/223, train_loss: 0.1100, step time: 0.1263\n",
      "19/223, train_loss: 0.1069, step time: 0.1312\n",
      "20/223, train_loss: 0.1025, step time: 0.1184\n",
      "21/223, train_loss: 0.1048, step time: 0.1144\n",
      "22/223, train_loss: 0.0891, step time: 0.1006\n",
      "23/223, train_loss: 0.0943, step time: 0.1077\n",
      "24/223, train_loss: 0.1034, step time: 0.1095\n",
      "25/223, train_loss: 0.0897, step time: 0.1200\n",
      "26/223, train_loss: 0.0963, step time: 0.1149\n",
      "27/223, train_loss: 0.1069, step time: 0.1127\n",
      "28/223, train_loss: 0.0853, step time: 0.1039\n",
      "29/223, train_loss: 0.1107, step time: 0.1082\n",
      "30/223, train_loss: 0.1024, step time: 0.1276\n",
      "31/223, train_loss: 0.0969, step time: 0.1506\n",
      "32/223, train_loss: 0.1026, step time: 0.1004\n",
      "33/223, train_loss: 0.1144, step time: 0.1100\n",
      "34/223, train_loss: 0.0998, step time: 0.1227\n",
      "35/223, train_loss: 0.0983, step time: 0.1220\n",
      "36/223, train_loss: 0.0932, step time: 0.1059\n",
      "37/223, train_loss: 0.0988, step time: 0.1638\n",
      "38/223, train_loss: 0.0962, step time: 0.1052\n",
      "39/223, train_loss: 0.0987, step time: 0.1109\n",
      "40/223, train_loss: 0.1047, step time: 0.1000\n",
      "41/223, train_loss: 0.0935, step time: 0.1061\n",
      "42/223, train_loss: 0.1000, step time: 0.1262\n",
      "43/223, train_loss: 0.1051, step time: 0.0998\n",
      "44/223, train_loss: 0.1007, step time: 0.1695\n",
      "45/223, train_loss: 0.1009, step time: 0.1067\n",
      "46/223, train_loss: 0.1042, step time: 0.1001\n",
      "47/223, train_loss: 0.0936, step time: 0.1052\n",
      "48/223, train_loss: 0.0933, step time: 0.1152\n",
      "49/223, train_loss: 0.1089, step time: 0.1377\n",
      "50/223, train_loss: 0.1085, step time: 0.1000\n",
      "51/223, train_loss: 0.1023, step time: 0.1001\n",
      "52/223, train_loss: 0.1035, step time: 0.1167\n",
      "53/223, train_loss: 0.0993, step time: 0.1188\n",
      "54/223, train_loss: 0.1042, step time: 0.1252\n",
      "55/223, train_loss: 0.0964, step time: 0.1148\n",
      "56/223, train_loss: 0.1065, step time: 0.1202\n",
      "57/223, train_loss: 0.1015, step time: 0.1210\n",
      "58/223, train_loss: 0.1031, step time: 0.1074\n",
      "59/223, train_loss: 0.1031, step time: 0.1028\n",
      "60/223, train_loss: 0.1034, step time: 0.1216\n",
      "61/223, train_loss: 0.0894, step time: 0.1013\n",
      "62/223, train_loss: 0.1018, step time: 0.1003\n",
      "63/223, train_loss: 0.1002, step time: 0.1598\n",
      "64/223, train_loss: 0.0943, step time: 0.1084\n",
      "65/223, train_loss: 0.0971, step time: 0.1134\n",
      "66/223, train_loss: 0.1006, step time: 0.1000\n",
      "67/223, train_loss: 0.0862, step time: 0.1426\n",
      "68/223, train_loss: 0.0926, step time: 0.1143\n",
      "69/223, train_loss: 0.0960, step time: 0.1081\n",
      "70/223, train_loss: 0.1078, step time: 0.1005\n",
      "71/223, train_loss: 0.1030, step time: 0.1258\n",
      "72/223, train_loss: 0.1129, step time: 0.1050\n",
      "73/223, train_loss: 0.1081, step time: 0.1119\n",
      "74/223, train_loss: 0.1086, step time: 0.1001\n",
      "75/223, train_loss: 0.0954, step time: 0.1170\n",
      "76/223, train_loss: 0.0970, step time: 0.0994\n",
      "77/223, train_loss: 0.0942, step time: 0.1037\n",
      "78/223, train_loss: 0.1047, step time: 0.1106\n",
      "79/223, train_loss: 0.0963, step time: 0.1224\n",
      "80/223, train_loss: 0.0956, step time: 0.1781\n",
      "81/223, train_loss: 0.1035, step time: 0.1022\n",
      "82/223, train_loss: 0.1037, step time: 0.1017\n",
      "83/223, train_loss: 0.0972, step time: 0.1121\n",
      "84/223, train_loss: 0.1052, step time: 0.1181\n",
      "85/223, train_loss: 0.0998, step time: 0.1147\n",
      "86/223, train_loss: 0.1124, step time: 0.1125\n",
      "87/223, train_loss: 0.0958, step time: 0.1270\n",
      "88/223, train_loss: 0.0945, step time: 0.1124\n",
      "89/223, train_loss: 0.1107, step time: 0.1214\n",
      "90/223, train_loss: 0.0886, step time: 0.1224\n",
      "91/223, train_loss: 0.1038, step time: 0.1267\n",
      "92/223, train_loss: 0.0951, step time: 0.1144\n",
      "93/223, train_loss: 0.0980, step time: 0.1057\n",
      "94/223, train_loss: 0.1074, step time: 0.1168\n",
      "95/223, train_loss: 0.0951, step time: 0.1505\n",
      "96/223, train_loss: 0.1022, step time: 0.1157\n",
      "97/223, train_loss: 0.1067, step time: 0.1045\n",
      "98/223, train_loss: 0.0968, step time: 0.1063\n",
      "99/223, train_loss: 0.1032, step time: 0.1317\n",
      "100/223, train_loss: 0.0984, step time: 0.1151\n",
      "101/223, train_loss: 0.0927, step time: 0.1042\n",
      "102/223, train_loss: 0.0961, step time: 0.1161\n",
      "103/223, train_loss: 0.0999, step time: 0.1134\n",
      "104/223, train_loss: 0.1049, step time: 0.1906\n",
      "105/223, train_loss: 0.1085, step time: 0.1003\n",
      "106/223, train_loss: 0.0999, step time: 0.1229\n",
      "107/223, train_loss: 0.0925, step time: 0.1061\n",
      "108/223, train_loss: 0.0940, step time: 0.1480\n",
      "109/223, train_loss: 0.0969, step time: 0.0993\n",
      "110/223, train_loss: 0.0892, step time: 0.1152\n",
      "111/223, train_loss: 0.1072, step time: 0.1110\n",
      "112/223, train_loss: 0.1086, step time: 0.1162\n",
      "113/223, train_loss: 0.1039, step time: 0.1093\n",
      "114/223, train_loss: 0.0964, step time: 0.1141\n",
      "115/223, train_loss: 0.1044, step time: 0.0994\n",
      "116/223, train_loss: 0.0911, step time: 0.1096\n",
      "117/223, train_loss: 0.0953, step time: 0.1299\n",
      "118/223, train_loss: 0.1048, step time: 0.1047\n",
      "119/223, train_loss: 0.1070, step time: 0.1532\n",
      "120/223, train_loss: 0.0920, step time: 0.1020\n",
      "121/223, train_loss: 0.0917, step time: 0.1088\n",
      "122/223, train_loss: 0.0928, step time: 0.1040\n",
      "123/223, train_loss: 0.0972, step time: 0.1204\n",
      "124/223, train_loss: 0.1035, step time: 0.1170\n",
      "125/223, train_loss: 0.0951, step time: 0.1175\n",
      "126/223, train_loss: 0.1023, step time: 0.1141\n",
      "127/223, train_loss: 0.1112, step time: 0.1010\n",
      "128/223, train_loss: 0.1007, step time: 0.1156\n",
      "129/223, train_loss: 0.1019, step time: 0.1164\n",
      "130/223, train_loss: 0.1021, step time: 0.1152\n",
      "131/223, train_loss: 0.1022, step time: 0.1189\n",
      "132/223, train_loss: 0.1018, step time: 0.1396\n",
      "133/223, train_loss: 0.1084, step time: 0.1104\n",
      "134/223, train_loss: 0.1035, step time: 0.1220\n",
      "135/223, train_loss: 0.1030, step time: 0.1241\n",
      "136/223, train_loss: 0.1046, step time: 0.1102\n",
      "137/223, train_loss: 0.0981, step time: 0.1073\n",
      "138/223, train_loss: 0.1007, step time: 0.1020\n",
      "139/223, train_loss: 0.1025, step time: 0.1146\n",
      "140/223, train_loss: 0.0963, step time: 0.1125\n",
      "141/223, train_loss: 0.1007, step time: 0.1191\n",
      "142/223, train_loss: 0.1015, step time: 0.1271\n",
      "143/223, train_loss: 0.0892, step time: 0.1004\n",
      "144/223, train_loss: 0.1061, step time: 0.1160\n",
      "145/223, train_loss: 0.0984, step time: 0.1118\n",
      "146/223, train_loss: 0.1037, step time: 0.1157\n",
      "147/223, train_loss: 0.1089, step time: 0.1287\n",
      "148/223, train_loss: 0.0972, step time: 0.1088\n",
      "149/223, train_loss: 0.0932, step time: 0.1166\n",
      "150/223, train_loss: 0.1005, step time: 0.1275\n",
      "151/223, train_loss: 0.0998, step time: 0.1182\n",
      "152/223, train_loss: 0.0927, step time: 0.1001\n",
      "153/223, train_loss: 0.1014, step time: 0.1135\n",
      "154/223, train_loss: 0.0911, step time: 0.1141\n",
      "155/223, train_loss: 0.0999, step time: 0.1111\n",
      "156/223, train_loss: 0.1096, step time: 0.1003\n",
      "157/223, train_loss: 0.0912, step time: 0.1120\n",
      "158/223, train_loss: 0.0944, step time: 0.1058\n",
      "159/223, train_loss: 0.0920, step time: 0.1355\n",
      "160/223, train_loss: 0.0976, step time: 0.1120\n",
      "161/223, train_loss: 0.0925, step time: 0.1090\n",
      "162/223, train_loss: 0.0965, step time: 0.1136\n",
      "163/223, train_loss: 0.0950, step time: 0.1507\n",
      "164/223, train_loss: 0.1008, step time: 0.1222\n",
      "165/223, train_loss: 0.0978, step time: 0.1127\n",
      "166/223, train_loss: 0.0898, step time: 0.1224\n",
      "167/223, train_loss: 0.0965, step time: 0.1000\n",
      "168/223, train_loss: 0.1103, step time: 0.1014\n",
      "169/223, train_loss: 0.1061, step time: 0.1141\n",
      "170/223, train_loss: 0.0927, step time: 0.1038\n",
      "171/223, train_loss: 0.1036, step time: 0.1352\n",
      "172/223, train_loss: 0.0956, step time: 0.1146\n",
      "173/223, train_loss: 0.1081, step time: 0.1216\n",
      "174/223, train_loss: 0.1007, step time: 0.1067\n",
      "175/223, train_loss: 0.0950, step time: 0.1085\n",
      "176/223, train_loss: 0.0883, step time: 0.1140\n",
      "177/223, train_loss: 0.0863, step time: 0.1085\n",
      "178/223, train_loss: 0.1128, step time: 0.1069\n",
      "179/223, train_loss: 0.1006, step time: 0.1167\n",
      "180/223, train_loss: 0.1083, step time: 0.1372\n",
      "181/223, train_loss: 0.0943, step time: 0.1018\n",
      "182/223, train_loss: 0.0944, step time: 0.1225\n",
      "183/223, train_loss: 0.3005, step time: 0.1210\n",
      "184/223, train_loss: 0.0890, step time: 0.1008\n",
      "185/223, train_loss: 0.1012, step time: 0.1270\n",
      "186/223, train_loss: 0.0889, step time: 0.1148\n",
      "187/223, train_loss: 0.0844, step time: 0.1016\n",
      "188/223, train_loss: 0.1031, step time: 0.1353\n",
      "189/223, train_loss: 0.0928, step time: 0.1207\n",
      "190/223, train_loss: 0.0894, step time: 0.1125\n",
      "191/223, train_loss: 0.0873, step time: 0.1006\n",
      "192/223, train_loss: 0.1003, step time: 0.1108\n",
      "193/223, train_loss: 0.1014, step time: 0.1119\n",
      "194/223, train_loss: 0.0888, step time: 0.1194\n",
      "195/223, train_loss: 0.1061, step time: 0.1555\n",
      "196/223, train_loss: 0.1000, step time: 0.1006\n",
      "197/223, train_loss: 0.0961, step time: 0.1292\n",
      "198/223, train_loss: 0.0885, step time: 0.1096\n",
      "199/223, train_loss: 0.1189, step time: 0.1183\n",
      "200/223, train_loss: 0.0929, step time: 0.1234\n",
      "201/223, train_loss: 0.0855, step time: 0.1117\n",
      "202/223, train_loss: 0.0895, step time: 0.1061\n",
      "203/223, train_loss: 0.1051, step time: 0.1226\n",
      "204/223, train_loss: 0.0849, step time: 0.1290\n",
      "205/223, train_loss: 0.1003, step time: 0.0997\n",
      "206/223, train_loss: 0.0878, step time: 0.1127\n",
      "207/223, train_loss: 0.0933, step time: 0.1050\n",
      "208/223, train_loss: 0.1056, step time: 0.1008\n",
      "209/223, train_loss: 0.1010, step time: 0.1181\n",
      "210/223, train_loss: 0.1023, step time: 0.1493\n",
      "211/223, train_loss: 0.0977, step time: 0.1108\n",
      "212/223, train_loss: 0.0930, step time: 0.1135\n",
      "213/223, train_loss: 0.0990, step time: 0.1145\n",
      "214/223, train_loss: 0.0984, step time: 0.1008\n",
      "215/223, train_loss: 0.1002, step time: 0.1483\n",
      "216/223, train_loss: 0.0953, step time: 0.1001\n",
      "217/223, train_loss: 0.0989, step time: 0.1000\n",
      "218/223, train_loss: 0.0946, step time: 0.0996\n",
      "219/223, train_loss: 0.0959, step time: 0.0993\n",
      "220/223, train_loss: 0.1003, step time: 0.1010\n",
      "221/223, train_loss: 0.1040, step time: 0.1002\n",
      "222/223, train_loss: 0.1103, step time: 0.0995\n",
      "223/223, train_loss: 0.0924, step time: 0.1005\n",
      "epoch 244 average loss: 0.1002\n",
      "time consuming of epoch 244 is: 90.4433\n",
      "----------\n",
      "epoch 245/300\n",
      "1/223, train_loss: 0.1054, step time: 0.1016\n",
      "2/223, train_loss: 0.1073, step time: 0.1000\n",
      "3/223, train_loss: 0.1089, step time: 0.1111\n",
      "4/223, train_loss: 0.1026, step time: 0.1101\n",
      "5/223, train_loss: 0.0933, step time: 0.1133\n",
      "6/223, train_loss: 0.0953, step time: 0.1124\n",
      "7/223, train_loss: 0.0897, step time: 0.1088\n",
      "8/223, train_loss: 0.1016, step time: 0.1027\n",
      "9/223, train_loss: 0.0990, step time: 0.1036\n",
      "10/223, train_loss: 0.0997, step time: 0.1023\n",
      "11/223, train_loss: 0.0926, step time: 0.1092\n",
      "12/223, train_loss: 0.1032, step time: 0.1096\n",
      "13/223, train_loss: 0.1069, step time: 0.1088\n",
      "14/223, train_loss: 0.0999, step time: 0.1041\n",
      "15/223, train_loss: 0.0993, step time: 0.1083\n",
      "16/223, train_loss: 0.0931, step time: 0.1495\n",
      "17/223, train_loss: 0.0987, step time: 0.1070\n",
      "18/223, train_loss: 0.1017, step time: 0.1009\n",
      "19/223, train_loss: 0.1044, step time: 0.1057\n",
      "20/223, train_loss: 0.0925, step time: 0.1312\n",
      "21/223, train_loss: 0.1057, step time: 0.1173\n",
      "22/223, train_loss: 0.0998, step time: 0.1043\n",
      "23/223, train_loss: 0.0950, step time: 0.1683\n",
      "24/223, train_loss: 0.0945, step time: 0.1142\n",
      "25/223, train_loss: 0.0988, step time: 0.1157\n",
      "26/223, train_loss: 0.0927, step time: 0.1265\n",
      "27/223, train_loss: 0.1034, step time: 0.1138\n",
      "28/223, train_loss: 0.0955, step time: 0.1010\n",
      "29/223, train_loss: 0.1129, step time: 0.1068\n",
      "30/223, train_loss: 0.1008, step time: 0.1072\n",
      "31/223, train_loss: 0.0937, step time: 0.1348\n",
      "32/223, train_loss: 0.1081, step time: 0.1240\n",
      "33/223, train_loss: 0.0983, step time: 0.1021\n",
      "34/223, train_loss: 0.0900, step time: 0.1211\n",
      "35/223, train_loss: 0.0916, step time: 0.1196\n",
      "36/223, train_loss: 0.0936, step time: 0.1075\n",
      "37/223, train_loss: 0.1090, step time: 0.1062\n",
      "38/223, train_loss: 0.1079, step time: 0.1200\n",
      "39/223, train_loss: 0.1027, step time: 0.1138\n",
      "40/223, train_loss: 0.0963, step time: 0.1406\n",
      "41/223, train_loss: 0.1110, step time: 0.1043\n",
      "42/223, train_loss: 0.0938, step time: 0.1067\n",
      "43/223, train_loss: 0.1117, step time: 0.1139\n",
      "44/223, train_loss: 0.0959, step time: 0.1051\n",
      "45/223, train_loss: 0.1011, step time: 0.1031\n",
      "46/223, train_loss: 0.0910, step time: 0.1035\n",
      "47/223, train_loss: 0.0910, step time: 0.1073\n",
      "48/223, train_loss: 0.0933, step time: 0.1166\n",
      "49/223, train_loss: 0.1088, step time: 0.0997\n",
      "50/223, train_loss: 0.1013, step time: 0.1109\n",
      "51/223, train_loss: 0.1020, step time: 0.1126\n",
      "52/223, train_loss: 0.0922, step time: 0.1194\n",
      "53/223, train_loss: 0.0898, step time: 0.1163\n",
      "54/223, train_loss: 0.0991, step time: 0.1037\n",
      "55/223, train_loss: 0.0897, step time: 0.1144\n",
      "56/223, train_loss: 0.0973, step time: 0.1195\n",
      "57/223, train_loss: 0.0925, step time: 0.1068\n",
      "58/223, train_loss: 0.1067, step time: 0.1136\n",
      "59/223, train_loss: 0.0989, step time: 0.1141\n",
      "60/223, train_loss: 0.1008, step time: 0.1109\n",
      "61/223, train_loss: 0.0905, step time: 0.1045\n",
      "62/223, train_loss: 0.0946, step time: 0.1150\n",
      "63/223, train_loss: 0.0993, step time: 0.1302\n",
      "64/223, train_loss: 0.0949, step time: 0.1305\n",
      "65/223, train_loss: 0.0978, step time: 0.0997\n",
      "66/223, train_loss: 0.0958, step time: 0.1064\n",
      "67/223, train_loss: 0.0870, step time: 0.1008\n",
      "68/223, train_loss: 0.0965, step time: 0.1011\n",
      "69/223, train_loss: 0.0940, step time: 0.1012\n",
      "70/223, train_loss: 0.0868, step time: 0.1050\n",
      "71/223, train_loss: 0.1044, step time: 0.1005\n",
      "72/223, train_loss: 0.0988, step time: 0.1008\n",
      "73/223, train_loss: 0.0989, step time: 0.1169\n",
      "74/223, train_loss: 0.0945, step time: 0.1006\n",
      "75/223, train_loss: 0.2985, step time: 0.1001\n",
      "76/223, train_loss: 0.1019, step time: 0.1008\n",
      "77/223, train_loss: 0.0887, step time: 0.1026\n",
      "78/223, train_loss: 0.0906, step time: 0.1040\n",
      "79/223, train_loss: 0.1046, step time: 0.1102\n",
      "80/223, train_loss: 0.1036, step time: 0.1121\n",
      "81/223, train_loss: 0.0974, step time: 0.1002\n",
      "82/223, train_loss: 0.0990, step time: 0.1238\n",
      "83/223, train_loss: 0.1004, step time: 0.1043\n",
      "84/223, train_loss: 0.0991, step time: 0.1137\n",
      "85/223, train_loss: 0.1012, step time: 0.1042\n",
      "86/223, train_loss: 0.1073, step time: 0.1006\n",
      "87/223, train_loss: 0.0973, step time: 0.1072\n",
      "88/223, train_loss: 0.0940, step time: 0.1001\n",
      "89/223, train_loss: 0.0979, step time: 0.1160\n",
      "90/223, train_loss: 0.0987, step time: 0.1050\n",
      "91/223, train_loss: 0.1000, step time: 0.1084\n",
      "92/223, train_loss: 0.0933, step time: 0.1141\n",
      "93/223, train_loss: 0.0956, step time: 0.1316\n",
      "94/223, train_loss: 0.0952, step time: 0.1134\n",
      "95/223, train_loss: 0.1214, step time: 0.1079\n",
      "96/223, train_loss: 0.1050, step time: 0.1242\n",
      "97/223, train_loss: 0.1002, step time: 0.1008\n",
      "98/223, train_loss: 0.1026, step time: 0.1006\n",
      "99/223, train_loss: 0.0959, step time: 0.1160\n",
      "100/223, train_loss: 0.1018, step time: 0.1276\n",
      "101/223, train_loss: 0.0930, step time: 0.1078\n",
      "102/223, train_loss: 0.0879, step time: 0.1015\n",
      "103/223, train_loss: 0.0892, step time: 0.1216\n",
      "104/223, train_loss: 0.0982, step time: 0.1134\n",
      "105/223, train_loss: 0.0956, step time: 0.1097\n",
      "106/223, train_loss: 0.1026, step time: 0.1133\n",
      "107/223, train_loss: 0.1042, step time: 0.1149\n",
      "108/223, train_loss: 0.1031, step time: 0.1005\n",
      "109/223, train_loss: 0.0978, step time: 0.1195\n",
      "110/223, train_loss: 0.0910, step time: 0.1055\n",
      "111/223, train_loss: 0.1194, step time: 0.1108\n",
      "112/223, train_loss: 0.0952, step time: 0.1009\n",
      "113/223, train_loss: 0.1014, step time: 0.1155\n",
      "114/223, train_loss: 0.1054, step time: 0.1268\n",
      "115/223, train_loss: 0.1169, step time: 0.1004\n",
      "116/223, train_loss: 0.1011, step time: 0.0995\n",
      "117/223, train_loss: 0.1026, step time: 0.1019\n",
      "118/223, train_loss: 0.1019, step time: 0.0999\n",
      "119/223, train_loss: 0.0924, step time: 0.1002\n",
      "120/223, train_loss: 0.0996, step time: 0.1035\n",
      "121/223, train_loss: 0.1062, step time: 0.1010\n",
      "122/223, train_loss: 0.0969, step time: 0.0998\n",
      "123/223, train_loss: 0.0955, step time: 0.1001\n",
      "124/223, train_loss: 0.0975, step time: 0.1041\n",
      "125/223, train_loss: 0.1041, step time: 0.1116\n",
      "126/223, train_loss: 0.0972, step time: 0.1126\n",
      "127/223, train_loss: 0.1054, step time: 0.1281\n",
      "128/223, train_loss: 0.1033, step time: 0.1020\n",
      "129/223, train_loss: 0.1022, step time: 0.1078\n",
      "130/223, train_loss: 0.1074, step time: 0.1004\n",
      "131/223, train_loss: 0.1089, step time: 0.1014\n",
      "132/223, train_loss: 0.0970, step time: 0.1043\n",
      "133/223, train_loss: 0.1256, step time: 0.1279\n",
      "134/223, train_loss: 0.1065, step time: 0.1335\n",
      "135/223, train_loss: 0.0982, step time: 0.1135\n",
      "136/223, train_loss: 0.1062, step time: 0.1127\n",
      "137/223, train_loss: 0.1003, step time: 0.0999\n",
      "138/223, train_loss: 0.1041, step time: 0.1050\n",
      "139/223, train_loss: 0.1032, step time: 0.1056\n",
      "140/223, train_loss: 0.1042, step time: 0.1453\n",
      "141/223, train_loss: 0.1076, step time: 0.1047\n",
      "142/223, train_loss: 0.0880, step time: 0.1013\n",
      "143/223, train_loss: 0.1102, step time: 0.1006\n",
      "144/223, train_loss: 0.0980, step time: 0.1154\n",
      "145/223, train_loss: 0.1055, step time: 0.1007\n",
      "146/223, train_loss: 0.0872, step time: 0.0998\n",
      "147/223, train_loss: 0.0984, step time: 0.1007\n",
      "148/223, train_loss: 0.1088, step time: 0.1028\n",
      "149/223, train_loss: 0.1080, step time: 0.1067\n",
      "150/223, train_loss: 0.1008, step time: 0.1188\n",
      "151/223, train_loss: 0.0910, step time: 0.1197\n",
      "152/223, train_loss: 0.0956, step time: 0.1156\n",
      "153/223, train_loss: 0.0945, step time: 0.1192\n",
      "154/223, train_loss: 0.0919, step time: 0.1084\n",
      "155/223, train_loss: 0.1030, step time: 0.1222\n",
      "156/223, train_loss: 0.0910, step time: 0.1277\n",
      "157/223, train_loss: 0.0974, step time: 0.1059\n",
      "158/223, train_loss: 0.0951, step time: 0.1150\n",
      "159/223, train_loss: 0.0995, step time: 0.1416\n",
      "160/223, train_loss: 0.0998, step time: 0.1160\n",
      "161/223, train_loss: 0.1010, step time: 0.1061\n",
      "162/223, train_loss: 0.1100, step time: 0.1079\n",
      "163/223, train_loss: 0.1078, step time: 0.1102\n",
      "164/223, train_loss: 0.1059, step time: 0.1200\n",
      "165/223, train_loss: 0.0952, step time: 0.1113\n",
      "166/223, train_loss: 0.0959, step time: 0.1135\n",
      "167/223, train_loss: 0.0972, step time: 0.1151\n",
      "168/223, train_loss: 0.1113, step time: 0.1116\n",
      "169/223, train_loss: 0.0911, step time: 0.1104\n",
      "170/223, train_loss: 0.0963, step time: 0.1076\n",
      "171/223, train_loss: 0.1039, step time: 0.1503\n",
      "172/223, train_loss: 0.1011, step time: 0.1069\n",
      "173/223, train_loss: 0.1021, step time: 0.1173\n",
      "174/223, train_loss: 0.0903, step time: 0.1159\n",
      "175/223, train_loss: 0.0972, step time: 0.1734\n",
      "176/223, train_loss: 0.1013, step time: 0.1166\n",
      "177/223, train_loss: 0.1016, step time: 0.1009\n",
      "178/223, train_loss: 0.1046, step time: 0.1276\n",
      "179/223, train_loss: 0.0999, step time: 0.1085\n",
      "180/223, train_loss: 0.0898, step time: 0.1158\n",
      "181/223, train_loss: 0.0877, step time: 0.1065\n",
      "182/223, train_loss: 0.1000, step time: 0.1093\n",
      "183/223, train_loss: 0.0932, step time: 0.1130\n",
      "184/223, train_loss: 0.1005, step time: 0.1280\n",
      "185/223, train_loss: 0.0906, step time: 0.1006\n",
      "186/223, train_loss: 0.0980, step time: 0.1321\n",
      "187/223, train_loss: 0.0990, step time: 0.1337\n",
      "188/223, train_loss: 0.1097, step time: 0.1126\n",
      "189/223, train_loss: 0.1080, step time: 0.1057\n",
      "190/223, train_loss: 0.0979, step time: 0.1237\n",
      "191/223, train_loss: 0.0847, step time: 0.1158\n",
      "192/223, train_loss: 0.0927, step time: 0.1352\n",
      "193/223, train_loss: 0.1012, step time: 0.1202\n",
      "194/223, train_loss: 0.0998, step time: 0.1321\n",
      "195/223, train_loss: 0.0942, step time: 0.1389\n",
      "196/223, train_loss: 0.0992, step time: 0.1325\n",
      "197/223, train_loss: 0.0931, step time: 0.1251\n",
      "198/223, train_loss: 0.1002, step time: 0.1107\n",
      "199/223, train_loss: 0.1088, step time: 0.1227\n",
      "200/223, train_loss: 0.1003, step time: 0.1000\n",
      "201/223, train_loss: 0.0957, step time: 0.1142\n",
      "202/223, train_loss: 0.0905, step time: 0.1160\n",
      "203/223, train_loss: 0.1128, step time: 0.1586\n",
      "204/223, train_loss: 0.0922, step time: 0.0999\n",
      "205/223, train_loss: 0.1000, step time: 0.1228\n",
      "206/223, train_loss: 0.0990, step time: 0.1052\n",
      "207/223, train_loss: 0.0845, step time: 0.1297\n",
      "208/223, train_loss: 0.0994, step time: 0.1246\n",
      "209/223, train_loss: 0.1046, step time: 0.1307\n",
      "210/223, train_loss: 0.0944, step time: 0.1153\n",
      "211/223, train_loss: 0.1038, step time: 0.1141\n",
      "212/223, train_loss: 0.0983, step time: 0.1002\n",
      "213/223, train_loss: 0.0922, step time: 0.1158\n",
      "214/223, train_loss: 0.1035, step time: 0.1184\n",
      "215/223, train_loss: 0.0935, step time: 0.1069\n",
      "216/223, train_loss: 0.0995, step time: 0.1012\n",
      "217/223, train_loss: 0.0928, step time: 0.1093\n",
      "218/223, train_loss: 0.0931, step time: 0.1353\n",
      "219/223, train_loss: 0.1018, step time: 0.1116\n",
      "220/223, train_loss: 0.0945, step time: 0.1008\n",
      "221/223, train_loss: 0.1100, step time: 0.0995\n",
      "222/223, train_loss: 0.1127, step time: 0.0995\n",
      "223/223, train_loss: 0.0989, step time: 0.1055\n",
      "epoch 245 average loss: 0.1002\n",
      "current epoch: 245 current mean dice: 0.8618 tc: 0.9224 wt: 0.8715 et: 0.7915\n",
      "best mean dice: 0.8620 at epoch: 235\n",
      "time consuming of epoch 245 is: 92.5508\n",
      "----------\n",
      "epoch 246/300\n",
      "1/223, train_loss: 0.1020, step time: 0.1005\n",
      "2/223, train_loss: 0.0881, step time: 0.1002\n",
      "3/223, train_loss: 0.1046, step time: 0.0999\n",
      "4/223, train_loss: 0.0899, step time: 0.1010\n",
      "5/223, train_loss: 0.1044, step time: 0.0996\n",
      "6/223, train_loss: 0.0876, step time: 0.0990\n",
      "7/223, train_loss: 0.0958, step time: 0.0999\n",
      "8/223, train_loss: 0.1021, step time: 0.1019\n",
      "9/223, train_loss: 0.0884, step time: 0.0999\n",
      "10/223, train_loss: 0.0960, step time: 0.1004\n",
      "11/223, train_loss: 0.0943, step time: 0.1003\n",
      "12/223, train_loss: 0.1055, step time: 0.1003\n",
      "13/223, train_loss: 0.0908, step time: 0.0994\n",
      "14/223, train_loss: 0.1063, step time: 0.0998\n",
      "15/223, train_loss: 0.0975, step time: 0.0990\n",
      "16/223, train_loss: 0.0984, step time: 0.1003\n",
      "17/223, train_loss: 0.1066, step time: 0.1056\n",
      "18/223, train_loss: 0.0865, step time: 0.1007\n",
      "19/223, train_loss: 0.0970, step time: 0.1010\n",
      "20/223, train_loss: 0.0974, step time: 0.1009\n",
      "21/223, train_loss: 0.0839, step time: 0.1131\n",
      "22/223, train_loss: 0.1025, step time: 0.1431\n",
      "23/223, train_loss: 0.0994, step time: 0.1173\n",
      "24/223, train_loss: 0.0981, step time: 0.1660\n",
      "25/223, train_loss: 0.0982, step time: 0.1225\n",
      "26/223, train_loss: 0.1028, step time: 0.1158\n",
      "27/223, train_loss: 0.0940, step time: 0.1211\n",
      "28/223, train_loss: 0.0879, step time: 0.1023\n",
      "29/223, train_loss: 0.1038, step time: 0.1164\n",
      "30/223, train_loss: 0.1097, step time: 0.1148\n",
      "31/223, train_loss: 0.0945, step time: 0.1141\n",
      "32/223, train_loss: 0.1013, step time: 0.1211\n",
      "33/223, train_loss: 0.0987, step time: 0.1184\n",
      "34/223, train_loss: 0.1077, step time: 0.1143\n",
      "35/223, train_loss: 0.0997, step time: 0.1010\n",
      "36/223, train_loss: 0.1103, step time: 0.1118\n",
      "37/223, train_loss: 0.0947, step time: 0.1080\n",
      "38/223, train_loss: 0.0947, step time: 0.1073\n",
      "39/223, train_loss: 0.0989, step time: 0.1000\n",
      "40/223, train_loss: 0.0965, step time: 0.1012\n",
      "41/223, train_loss: 0.1021, step time: 0.1089\n",
      "42/223, train_loss: 0.1014, step time: 0.1169\n",
      "43/223, train_loss: 0.1007, step time: 0.1214\n",
      "44/223, train_loss: 0.1050, step time: 0.1445\n",
      "45/223, train_loss: 0.0934, step time: 0.1095\n",
      "46/223, train_loss: 0.1123, step time: 0.1128\n",
      "47/223, train_loss: 0.1014, step time: 0.1119\n",
      "48/223, train_loss: 0.0983, step time: 0.1004\n",
      "49/223, train_loss: 0.1043, step time: 0.1196\n",
      "50/223, train_loss: 0.1153, step time: 0.1068\n",
      "51/223, train_loss: 0.1082, step time: 0.1005\n",
      "52/223, train_loss: 0.0955, step time: 0.1010\n",
      "53/223, train_loss: 0.1027, step time: 0.1203\n",
      "54/223, train_loss: 0.0957, step time: 0.1002\n",
      "55/223, train_loss: 0.1081, step time: 0.1205\n",
      "56/223, train_loss: 0.0969, step time: 0.1335\n",
      "57/223, train_loss: 0.0917, step time: 0.1047\n",
      "58/223, train_loss: 0.1054, step time: 0.1237\n",
      "59/223, train_loss: 0.0978, step time: 0.1034\n",
      "60/223, train_loss: 0.1128, step time: 0.1007\n",
      "61/223, train_loss: 0.0920, step time: 0.1170\n",
      "62/223, train_loss: 0.1016, step time: 0.1046\n",
      "63/223, train_loss: 0.1043, step time: 0.1123\n",
      "64/223, train_loss: 0.1092, step time: 0.1096\n",
      "65/223, train_loss: 0.0985, step time: 0.1091\n",
      "66/223, train_loss: 0.0953, step time: 0.1266\n",
      "67/223, train_loss: 0.0994, step time: 0.1161\n",
      "68/223, train_loss: 0.1041, step time: 0.1178\n",
      "69/223, train_loss: 0.0933, step time: 0.1128\n",
      "70/223, train_loss: 0.0927, step time: 0.1004\n",
      "71/223, train_loss: 0.1028, step time: 0.1052\n",
      "72/223, train_loss: 0.1112, step time: 0.1142\n",
      "73/223, train_loss: 0.1091, step time: 0.1206\n",
      "74/223, train_loss: 0.1033, step time: 0.1164\n",
      "75/223, train_loss: 0.0974, step time: 0.1125\n",
      "76/223, train_loss: 0.0933, step time: 0.1246\n",
      "77/223, train_loss: 0.0938, step time: 0.1124\n",
      "78/223, train_loss: 0.0990, step time: 0.1092\n",
      "79/223, train_loss: 0.0925, step time: 0.1120\n",
      "80/223, train_loss: 0.0886, step time: 0.1086\n",
      "81/223, train_loss: 0.0994, step time: 0.1158\n",
      "82/223, train_loss: 0.0989, step time: 0.1102\n",
      "83/223, train_loss: 0.1057, step time: 0.1098\n",
      "84/223, train_loss: 0.0945, step time: 0.1272\n",
      "85/223, train_loss: 0.0994, step time: 0.1140\n",
      "86/223, train_loss: 0.1055, step time: 0.1141\n",
      "87/223, train_loss: 0.0966, step time: 0.1101\n",
      "88/223, train_loss: 0.0959, step time: 0.1093\n",
      "89/223, train_loss: 0.0965, step time: 0.1091\n",
      "90/223, train_loss: 0.0978, step time: 0.1005\n",
      "91/223, train_loss: 0.0962, step time: 0.1413\n",
      "92/223, train_loss: 0.1000, step time: 0.1355\n",
      "93/223, train_loss: 0.1104, step time: 0.2062\n",
      "94/223, train_loss: 0.0941, step time: 0.1087\n",
      "95/223, train_loss: 0.0984, step time: 0.1151\n",
      "96/223, train_loss: 0.1059, step time: 0.1086\n",
      "97/223, train_loss: 0.0960, step time: 0.1076\n",
      "98/223, train_loss: 0.0923, step time: 0.1265\n",
      "99/223, train_loss: 0.1090, step time: 0.1154\n",
      "100/223, train_loss: 0.1131, step time: 0.1193\n",
      "101/223, train_loss: 0.0875, step time: 0.1127\n",
      "102/223, train_loss: 0.0948, step time: 0.1208\n",
      "103/223, train_loss: 0.1043, step time: 0.0988\n",
      "104/223, train_loss: 0.0995, step time: 0.1088\n",
      "105/223, train_loss: 0.1116, step time: 0.1224\n",
      "106/223, train_loss: 0.0976, step time: 0.1345\n",
      "107/223, train_loss: 0.1064, step time: 0.1002\n",
      "108/223, train_loss: 0.1076, step time: 0.1069\n",
      "109/223, train_loss: 0.0875, step time: 0.1002\n",
      "110/223, train_loss: 0.1053, step time: 0.1205\n",
      "111/223, train_loss: 0.0965, step time: 0.0998\n",
      "112/223, train_loss: 0.0952, step time: 0.0987\n",
      "113/223, train_loss: 0.0943, step time: 0.1121\n",
      "114/223, train_loss: 0.0992, step time: 0.1018\n",
      "115/223, train_loss: 0.0918, step time: 0.1112\n",
      "116/223, train_loss: 0.0985, step time: 0.1153\n",
      "117/223, train_loss: 0.0946, step time: 0.1039\n",
      "118/223, train_loss: 0.0999, step time: 0.1235\n",
      "119/223, train_loss: 0.0892, step time: 0.1292\n",
      "120/223, train_loss: 0.0974, step time: 0.1403\n",
      "121/223, train_loss: 0.0904, step time: 0.1284\n",
      "122/223, train_loss: 0.0972, step time: 0.1039\n",
      "123/223, train_loss: 0.1029, step time: 0.1142\n",
      "124/223, train_loss: 0.1055, step time: 0.1070\n",
      "125/223, train_loss: 0.3010, step time: 0.0995\n",
      "126/223, train_loss: 0.1030, step time: 0.1051\n",
      "127/223, train_loss: 0.1046, step time: 0.1116\n",
      "128/223, train_loss: 0.1006, step time: 0.1251\n",
      "129/223, train_loss: 0.1053, step time: 0.1166\n",
      "130/223, train_loss: 0.0963, step time: 0.1000\n",
      "131/223, train_loss: 0.1065, step time: 0.1005\n",
      "132/223, train_loss: 0.0923, step time: 0.1018\n",
      "133/223, train_loss: 0.1032, step time: 0.1070\n",
      "134/223, train_loss: 0.0980, step time: 0.1046\n",
      "135/223, train_loss: 0.1090, step time: 0.1005\n",
      "136/223, train_loss: 0.0925, step time: 0.1004\n",
      "137/223, train_loss: 0.0948, step time: 0.1172\n",
      "138/223, train_loss: 0.1020, step time: 0.1063\n",
      "139/223, train_loss: 0.0913, step time: 0.1249\n",
      "140/223, train_loss: 0.1092, step time: 0.1053\n",
      "141/223, train_loss: 0.0903, step time: 0.1540\n",
      "142/223, train_loss: 0.0954, step time: 0.1372\n",
      "143/223, train_loss: 0.0977, step time: 0.1076\n",
      "144/223, train_loss: 0.0890, step time: 0.1004\n",
      "145/223, train_loss: 0.1080, step time: 0.1062\n",
      "146/223, train_loss: 0.1036, step time: 0.1005\n",
      "147/223, train_loss: 0.0978, step time: 0.1053\n",
      "148/223, train_loss: 0.1002, step time: 0.1405\n",
      "149/223, train_loss: 0.0971, step time: 0.1056\n",
      "150/223, train_loss: 0.1036, step time: 0.1002\n",
      "151/223, train_loss: 0.0934, step time: 0.1403\n",
      "152/223, train_loss: 0.0922, step time: 0.1053\n",
      "153/223, train_loss: 0.1059, step time: 0.1076\n",
      "154/223, train_loss: 0.1044, step time: 0.1283\n",
      "155/223, train_loss: 0.0994, step time: 0.1081\n",
      "156/223, train_loss: 0.0915, step time: 0.1380\n",
      "157/223, train_loss: 0.0974, step time: 0.1173\n",
      "158/223, train_loss: 0.0883, step time: 0.1007\n",
      "159/223, train_loss: 0.1075, step time: 0.1347\n",
      "160/223, train_loss: 0.1019, step time: 0.1304\n",
      "161/223, train_loss: 0.0995, step time: 0.1094\n",
      "162/223, train_loss: 0.0925, step time: 0.1157\n",
      "163/223, train_loss: 0.0841, step time: 0.1196\n",
      "164/223, train_loss: 0.0945, step time: 0.1163\n",
      "165/223, train_loss: 0.1009, step time: 0.1151\n",
      "166/223, train_loss: 0.1018, step time: 0.1422\n",
      "167/223, train_loss: 0.0909, step time: 0.1406\n",
      "168/223, train_loss: 0.1046, step time: 0.1146\n",
      "169/223, train_loss: 0.1008, step time: 0.1081\n",
      "170/223, train_loss: 0.1014, step time: 0.1237\n",
      "171/223, train_loss: 0.0950, step time: 0.1252\n",
      "172/223, train_loss: 0.1128, step time: 0.1210\n",
      "173/223, train_loss: 0.1050, step time: 0.1154\n",
      "174/223, train_loss: 0.1016, step time: 0.1072\n",
      "175/223, train_loss: 0.1094, step time: 0.1122\n",
      "176/223, train_loss: 0.1130, step time: 0.1383\n",
      "177/223, train_loss: 0.1030, step time: 0.1097\n",
      "178/223, train_loss: 0.1043, step time: 0.1282\n",
      "179/223, train_loss: 0.1006, step time: 0.1199\n",
      "180/223, train_loss: 0.0967, step time: 0.1137\n",
      "181/223, train_loss: 0.0870, step time: 0.1021\n",
      "182/223, train_loss: 0.0994, step time: 0.1212\n",
      "183/223, train_loss: 0.0924, step time: 0.1065\n",
      "184/223, train_loss: 0.1057, step time: 0.1135\n",
      "185/223, train_loss: 0.1039, step time: 0.1335\n",
      "186/223, train_loss: 0.0891, step time: 0.1040\n",
      "187/223, train_loss: 0.0974, step time: 0.1013\n",
      "188/223, train_loss: 0.1049, step time: 0.1466\n",
      "189/223, train_loss: 0.0914, step time: 0.1197\n",
      "190/223, train_loss: 0.1007, step time: 0.1045\n",
      "191/223, train_loss: 0.1066, step time: 0.1194\n",
      "192/223, train_loss: 0.1000, step time: 0.1069\n",
      "193/223, train_loss: 0.0941, step time: 0.1049\n",
      "194/223, train_loss: 0.0948, step time: 0.0987\n",
      "195/223, train_loss: 0.1016, step time: 0.0995\n",
      "196/223, train_loss: 0.0984, step time: 0.1003\n",
      "197/223, train_loss: 0.1010, step time: 0.1224\n",
      "198/223, train_loss: 0.0970, step time: 0.1188\n",
      "199/223, train_loss: 0.1117, step time: 0.1175\n",
      "200/223, train_loss: 0.1063, step time: 0.1001\n",
      "201/223, train_loss: 0.0934, step time: 0.1278\n",
      "202/223, train_loss: 0.0991, step time: 0.1425\n",
      "203/223, train_loss: 0.0949, step time: 0.1008\n",
      "204/223, train_loss: 0.0963, step time: 0.1009\n",
      "205/223, train_loss: 0.1029, step time: 0.1411\n",
      "206/223, train_loss: 0.1070, step time: 0.1127\n",
      "207/223, train_loss: 0.0949, step time: 0.1007\n",
      "208/223, train_loss: 0.0995, step time: 0.1264\n",
      "209/223, train_loss: 0.1056, step time: 0.1176\n",
      "210/223, train_loss: 0.0964, step time: 0.1054\n",
      "211/223, train_loss: 0.0966, step time: 0.1001\n",
      "212/223, train_loss: 0.1106, step time: 0.1245\n",
      "213/223, train_loss: 0.0975, step time: 0.1306\n",
      "214/223, train_loss: 0.0949, step time: 0.1175\n",
      "215/223, train_loss: 0.0937, step time: 0.1191\n",
      "216/223, train_loss: 0.0904, step time: 0.1088\n",
      "217/223, train_loss: 0.0927, step time: 0.1007\n",
      "218/223, train_loss: 0.0972, step time: 0.1003\n",
      "219/223, train_loss: 0.0900, step time: 0.0997\n",
      "220/223, train_loss: 0.0863, step time: 0.1008\n",
      "221/223, train_loss: 0.1024, step time: 0.0998\n",
      "222/223, train_loss: 0.1117, step time: 0.1003\n",
      "223/223, train_loss: 0.1003, step time: 0.1013\n",
      "epoch 246 average loss: 0.1001\n",
      "time consuming of epoch 246 is: 94.4073\n",
      "----------\n",
      "epoch 247/300\n",
      "1/223, train_loss: 0.0994, step time: 0.1116\n",
      "2/223, train_loss: 0.0991, step time: 0.1161\n",
      "3/223, train_loss: 0.1034, step time: 0.1111\n",
      "4/223, train_loss: 0.0942, step time: 0.1004\n",
      "5/223, train_loss: 0.1041, step time: 0.1625\n",
      "6/223, train_loss: 0.1234, step time: 0.1086\n",
      "7/223, train_loss: 0.0978, step time: 0.1126\n",
      "8/223, train_loss: 0.1028, step time: 0.1041\n",
      "9/223, train_loss: 0.0893, step time: 0.1086\n",
      "10/223, train_loss: 0.1048, step time: 0.1097\n",
      "11/223, train_loss: 0.0941, step time: 0.1012\n",
      "12/223, train_loss: 0.1122, step time: 0.1102\n",
      "13/223, train_loss: 0.1079, step time: 0.1115\n",
      "14/223, train_loss: 0.0942, step time: 0.1001\n",
      "15/223, train_loss: 0.0987, step time: 0.1008\n",
      "16/223, train_loss: 0.1178, step time: 0.0999\n",
      "17/223, train_loss: 0.0918, step time: 0.1018\n",
      "18/223, train_loss: 0.1032, step time: 0.1057\n",
      "19/223, train_loss: 0.1006, step time: 0.1171\n",
      "20/223, train_loss: 0.1125, step time: 0.1126\n",
      "21/223, train_loss: 0.0908, step time: 0.1099\n",
      "22/223, train_loss: 0.0965, step time: 0.1001\n",
      "23/223, train_loss: 0.0938, step time: 0.1005\n",
      "24/223, train_loss: 0.0940, step time: 0.1008\n",
      "25/223, train_loss: 0.1080, step time: 0.1096\n",
      "26/223, train_loss: 0.0932, step time: 0.1019\n",
      "27/223, train_loss: 0.1150, step time: 0.0999\n",
      "28/223, train_loss: 0.0938, step time: 0.1241\n",
      "29/223, train_loss: 0.3009, step time: 0.1511\n",
      "30/223, train_loss: 0.0906, step time: 0.1240\n",
      "31/223, train_loss: 0.0894, step time: 0.1186\n",
      "32/223, train_loss: 0.1014, step time: 0.1164\n",
      "33/223, train_loss: 0.0860, step time: 0.1526\n",
      "34/223, train_loss: 0.0977, step time: 0.1329\n",
      "35/223, train_loss: 0.0984, step time: 0.1378\n",
      "36/223, train_loss: 0.0868, step time: 0.1413\n",
      "37/223, train_loss: 0.1083, step time: 0.1000\n",
      "38/223, train_loss: 0.0970, step time: 0.1187\n",
      "39/223, train_loss: 0.0927, step time: 0.1011\n",
      "40/223, train_loss: 0.0961, step time: 0.1001\n",
      "41/223, train_loss: 0.1016, step time: 0.0999\n",
      "42/223, train_loss: 0.0871, step time: 0.1149\n",
      "43/223, train_loss: 0.1015, step time: 0.1105\n",
      "44/223, train_loss: 0.1080, step time: 0.1107\n",
      "45/223, train_loss: 0.0985, step time: 0.1295\n",
      "46/223, train_loss: 0.0856, step time: 0.1098\n",
      "47/223, train_loss: 0.0954, step time: 0.1033\n",
      "48/223, train_loss: 0.0952, step time: 0.1015\n",
      "49/223, train_loss: 0.0981, step time: 0.1002\n",
      "50/223, train_loss: 0.0919, step time: 0.1634\n",
      "51/223, train_loss: 0.1117, step time: 0.1258\n",
      "52/223, train_loss: 0.1045, step time: 0.1043\n",
      "53/223, train_loss: 0.0932, step time: 0.1036\n",
      "54/223, train_loss: 0.0896, step time: 0.1134\n",
      "55/223, train_loss: 0.1010, step time: 0.1036\n",
      "56/223, train_loss: 0.0931, step time: 0.1004\n",
      "57/223, train_loss: 0.0978, step time: 0.1131\n",
      "58/223, train_loss: 0.0978, step time: 0.1198\n",
      "59/223, train_loss: 0.1024, step time: 0.1264\n",
      "60/223, train_loss: 0.1014, step time: 0.1144\n",
      "61/223, train_loss: 0.1040, step time: 0.1195\n",
      "62/223, train_loss: 0.0973, step time: 0.1364\n",
      "63/223, train_loss: 0.0953, step time: 0.0999\n",
      "64/223, train_loss: 0.1058, step time: 0.1094\n",
      "65/223, train_loss: 0.0984, step time: 0.1030\n",
      "66/223, train_loss: 0.1090, step time: 0.1145\n",
      "67/223, train_loss: 0.0936, step time: 0.1094\n",
      "68/223, train_loss: 0.0982, step time: 0.1410\n",
      "69/223, train_loss: 0.1023, step time: 0.1118\n",
      "70/223, train_loss: 0.1144, step time: 0.1225\n",
      "71/223, train_loss: 0.1004, step time: 0.1024\n",
      "72/223, train_loss: 0.1039, step time: 0.1297\n",
      "73/223, train_loss: 0.0978, step time: 0.1088\n",
      "74/223, train_loss: 0.0995, step time: 0.1003\n",
      "75/223, train_loss: 0.1074, step time: 0.1010\n",
      "76/223, train_loss: 0.1019, step time: 0.1213\n",
      "77/223, train_loss: 0.0994, step time: 0.1007\n",
      "78/223, train_loss: 0.0946, step time: 0.1214\n",
      "79/223, train_loss: 0.1101, step time: 0.1009\n",
      "80/223, train_loss: 0.1029, step time: 0.0997\n",
      "81/223, train_loss: 0.0937, step time: 0.1229\n",
      "82/223, train_loss: 0.1023, step time: 0.1409\n",
      "83/223, train_loss: 0.0877, step time: 0.1075\n",
      "84/223, train_loss: 0.1064, step time: 0.1075\n",
      "85/223, train_loss: 0.0972, step time: 0.1007\n",
      "86/223, train_loss: 0.1087, step time: 0.1061\n",
      "87/223, train_loss: 0.0982, step time: 0.1004\n",
      "88/223, train_loss: 0.1107, step time: 0.1000\n",
      "89/223, train_loss: 0.1022, step time: 0.1041\n",
      "90/223, train_loss: 0.1028, step time: 0.1017\n",
      "91/223, train_loss: 0.0963, step time: 0.1084\n",
      "92/223, train_loss: 0.1004, step time: 0.1105\n",
      "93/223, train_loss: 0.1013, step time: 0.1009\n",
      "94/223, train_loss: 0.1050, step time: 0.0998\n",
      "95/223, train_loss: 0.0987, step time: 0.1007\n",
      "96/223, train_loss: 0.1058, step time: 0.1002\n",
      "97/223, train_loss: 0.0954, step time: 0.1211\n",
      "98/223, train_loss: 0.0984, step time: 0.1000\n",
      "99/223, train_loss: 0.0926, step time: 0.1005\n",
      "100/223, train_loss: 0.1063, step time: 0.1009\n",
      "101/223, train_loss: 0.1038, step time: 0.1001\n",
      "102/223, train_loss: 0.0912, step time: 0.1007\n",
      "103/223, train_loss: 0.0940, step time: 0.1131\n",
      "104/223, train_loss: 0.0936, step time: 0.1109\n",
      "105/223, train_loss: 0.0892, step time: 0.1031\n",
      "106/223, train_loss: 0.0914, step time: 0.1382\n",
      "107/223, train_loss: 0.0992, step time: 0.1251\n",
      "108/223, train_loss: 0.1042, step time: 0.1026\n",
      "109/223, train_loss: 0.0887, step time: 0.1009\n",
      "110/223, train_loss: 0.0984, step time: 0.1054\n",
      "111/223, train_loss: 0.1026, step time: 0.1015\n",
      "112/223, train_loss: 0.0982, step time: 0.1141\n",
      "113/223, train_loss: 0.0928, step time: 0.1002\n",
      "114/223, train_loss: 0.0909, step time: 0.1138\n",
      "115/223, train_loss: 0.1059, step time: 0.1145\n",
      "116/223, train_loss: 0.0946, step time: 0.1004\n",
      "117/223, train_loss: 0.0994, step time: 0.1006\n",
      "118/223, train_loss: 0.1114, step time: 0.1112\n",
      "119/223, train_loss: 0.0965, step time: 0.1000\n",
      "120/223, train_loss: 0.1023, step time: 0.1006\n",
      "121/223, train_loss: 0.1052, step time: 0.1152\n",
      "122/223, train_loss: 0.1055, step time: 0.1187\n",
      "123/223, train_loss: 0.0883, step time: 0.1126\n",
      "124/223, train_loss: 0.1117, step time: 0.1059\n",
      "125/223, train_loss: 0.0886, step time: 0.1132\n",
      "126/223, train_loss: 0.0885, step time: 0.1017\n",
      "127/223, train_loss: 0.0891, step time: 0.1268\n",
      "128/223, train_loss: 0.0969, step time: 0.1307\n",
      "129/223, train_loss: 0.0902, step time: 0.1439\n",
      "130/223, train_loss: 0.1099, step time: 0.1410\n",
      "131/223, train_loss: 0.0969, step time: 0.1088\n",
      "132/223, train_loss: 0.1054, step time: 0.1030\n",
      "133/223, train_loss: 0.1018, step time: 0.1200\n",
      "134/223, train_loss: 0.0997, step time: 0.1183\n",
      "135/223, train_loss: 0.0944, step time: 0.1146\n",
      "136/223, train_loss: 0.1033, step time: 0.1181\n",
      "137/223, train_loss: 0.0954, step time: 0.1053\n",
      "138/223, train_loss: 0.1002, step time: 0.1211\n",
      "139/223, train_loss: 0.0931, step time: 0.1119\n",
      "140/223, train_loss: 0.1049, step time: 0.1071\n",
      "141/223, train_loss: 0.1128, step time: 0.1086\n",
      "142/223, train_loss: 0.1017, step time: 0.1167\n",
      "143/223, train_loss: 0.0933, step time: 0.1360\n",
      "144/223, train_loss: 0.0907, step time: 0.1031\n",
      "145/223, train_loss: 0.0927, step time: 0.1147\n",
      "146/223, train_loss: 0.0922, step time: 0.1086\n",
      "147/223, train_loss: 0.1023, step time: 0.1234\n",
      "148/223, train_loss: 0.0878, step time: 0.1002\n",
      "149/223, train_loss: 0.1071, step time: 0.1003\n",
      "150/223, train_loss: 0.0896, step time: 0.1661\n",
      "151/223, train_loss: 0.0999, step time: 0.1402\n",
      "152/223, train_loss: 0.1057, step time: 0.1175\n",
      "153/223, train_loss: 0.1133, step time: 0.1062\n",
      "154/223, train_loss: 0.1012, step time: 0.1099\n",
      "155/223, train_loss: 0.0970, step time: 0.1013\n",
      "156/223, train_loss: 0.0962, step time: 0.1300\n",
      "157/223, train_loss: 0.1006, step time: 0.1337\n",
      "158/223, train_loss: 0.0901, step time: 0.1180\n",
      "159/223, train_loss: 0.1108, step time: 0.1005\n",
      "160/223, train_loss: 0.0955, step time: 0.1056\n",
      "161/223, train_loss: 0.0882, step time: 0.1254\n",
      "162/223, train_loss: 0.0910, step time: 0.1007\n",
      "163/223, train_loss: 0.0951, step time: 0.1156\n",
      "164/223, train_loss: 0.0972, step time: 0.1161\n",
      "165/223, train_loss: 0.0985, step time: 0.1079\n",
      "166/223, train_loss: 0.0981, step time: 0.1000\n",
      "167/223, train_loss: 0.0982, step time: 0.1066\n",
      "168/223, train_loss: 0.1040, step time: 0.1157\n",
      "169/223, train_loss: 0.0945, step time: 0.1227\n",
      "170/223, train_loss: 0.0994, step time: 0.1339\n",
      "171/223, train_loss: 0.1089, step time: 0.1006\n",
      "172/223, train_loss: 0.0973, step time: 0.1176\n",
      "173/223, train_loss: 0.0934, step time: 0.1045\n",
      "174/223, train_loss: 0.0931, step time: 0.1135\n",
      "175/223, train_loss: 0.1022, step time: 0.1002\n",
      "176/223, train_loss: 0.1034, step time: 0.1104\n",
      "177/223, train_loss: 0.0950, step time: 0.1242\n",
      "178/223, train_loss: 0.1035, step time: 0.1147\n",
      "179/223, train_loss: 0.1153, step time: 0.1028\n",
      "180/223, train_loss: 0.0896, step time: 0.1225\n",
      "181/223, train_loss: 0.0952, step time: 0.1108\n",
      "182/223, train_loss: 0.1061, step time: 0.1058\n",
      "183/223, train_loss: 0.1182, step time: 0.1012\n",
      "184/223, train_loss: 0.1099, step time: 0.1221\n",
      "185/223, train_loss: 0.0901, step time: 0.1247\n",
      "186/223, train_loss: 0.0976, step time: 0.0997\n",
      "187/223, train_loss: 0.0958, step time: 0.1159\n",
      "188/223, train_loss: 0.0935, step time: 0.1141\n",
      "189/223, train_loss: 0.1170, step time: 0.1168\n",
      "190/223, train_loss: 0.1065, step time: 0.1007\n",
      "191/223, train_loss: 0.0925, step time: 0.1006\n",
      "192/223, train_loss: 0.0947, step time: 0.1001\n",
      "193/223, train_loss: 0.1120, step time: 0.1014\n",
      "194/223, train_loss: 0.0992, step time: 0.1002\n",
      "195/223, train_loss: 0.0914, step time: 0.1011\n",
      "196/223, train_loss: 0.0890, step time: 0.0998\n",
      "197/223, train_loss: 0.1043, step time: 0.1014\n",
      "198/223, train_loss: 0.1020, step time: 0.1012\n",
      "199/223, train_loss: 0.0984, step time: 0.1193\n",
      "200/223, train_loss: 0.1020, step time: 0.0994\n",
      "201/223, train_loss: 0.0970, step time: 0.1003\n",
      "202/223, train_loss: 0.0993, step time: 0.0998\n",
      "203/223, train_loss: 0.0906, step time: 0.0998\n",
      "204/223, train_loss: 0.0895, step time: 0.0994\n",
      "205/223, train_loss: 0.0984, step time: 0.1001\n",
      "206/223, train_loss: 0.1059, step time: 0.1006\n",
      "207/223, train_loss: 0.0934, step time: 0.1141\n",
      "208/223, train_loss: 0.0921, step time: 0.1004\n",
      "209/223, train_loss: 0.1031, step time: 0.1000\n",
      "210/223, train_loss: 0.1015, step time: 0.1010\n",
      "211/223, train_loss: 0.0984, step time: 0.1124\n",
      "212/223, train_loss: 0.1010, step time: 0.1131\n",
      "213/223, train_loss: 0.1013, step time: 0.1261\n",
      "214/223, train_loss: 0.1024, step time: 0.1046\n",
      "215/223, train_loss: 0.0948, step time: 0.1365\n",
      "216/223, train_loss: 0.1003, step time: 0.1312\n",
      "217/223, train_loss: 0.0950, step time: 0.1220\n",
      "218/223, train_loss: 0.1027, step time: 0.1046\n",
      "219/223, train_loss: 0.1064, step time: 0.1035\n",
      "220/223, train_loss: 0.1049, step time: 0.1020\n",
      "221/223, train_loss: 0.1070, step time: 0.0999\n",
      "222/223, train_loss: 0.0956, step time: 0.1010\n",
      "223/223, train_loss: 0.0968, step time: 0.1000\n",
      "epoch 247 average loss: 0.1002\n",
      "time consuming of epoch 247 is: 95.0505\n",
      "----------\n",
      "epoch 248/300\n",
      "1/223, train_loss: 0.0834, step time: 0.1015\n",
      "2/223, train_loss: 0.1016, step time: 0.1004\n",
      "3/223, train_loss: 0.0919, step time: 0.1094\n",
      "4/223, train_loss: 0.1011, step time: 0.1109\n",
      "5/223, train_loss: 0.0950, step time: 0.1130\n",
      "6/223, train_loss: 0.1042, step time: 0.1433\n",
      "7/223, train_loss: 0.1085, step time: 0.1146\n",
      "8/223, train_loss: 0.0944, step time: 0.1129\n",
      "9/223, train_loss: 0.1068, step time: 0.1006\n",
      "10/223, train_loss: 0.0958, step time: 0.1004\n",
      "11/223, train_loss: 0.1030, step time: 0.1081\n",
      "12/223, train_loss: 0.0981, step time: 0.1206\n",
      "13/223, train_loss: 0.0978, step time: 0.1083\n",
      "14/223, train_loss: 0.0976, step time: 0.1583\n",
      "15/223, train_loss: 0.0960, step time: 0.1013\n",
      "16/223, train_loss: 0.0985, step time: 0.1150\n",
      "17/223, train_loss: 0.0989, step time: 0.1014\n",
      "18/223, train_loss: 0.0982, step time: 0.1022\n",
      "19/223, train_loss: 0.1103, step time: 0.1053\n",
      "20/223, train_loss: 0.1144, step time: 0.1449\n",
      "21/223, train_loss: 0.0849, step time: 0.1089\n",
      "22/223, train_loss: 0.0948, step time: 0.1166\n",
      "23/223, train_loss: 0.0950, step time: 0.1083\n",
      "24/223, train_loss: 0.1003, step time: 0.1083\n",
      "25/223, train_loss: 0.0912, step time: 0.1056\n",
      "26/223, train_loss: 0.1042, step time: 0.1006\n",
      "27/223, train_loss: 0.1051, step time: 0.1112\n",
      "28/223, train_loss: 0.1144, step time: 0.1127\n",
      "29/223, train_loss: 0.1105, step time: 0.1010\n",
      "30/223, train_loss: 0.1096, step time: 0.1092\n",
      "31/223, train_loss: 0.0921, step time: 0.1014\n",
      "32/223, train_loss: 0.0994, step time: 0.1156\n",
      "33/223, train_loss: 0.1046, step time: 0.1007\n",
      "34/223, train_loss: 0.0948, step time: 0.1000\n",
      "35/223, train_loss: 0.0922, step time: 0.1456\n",
      "36/223, train_loss: 0.0904, step time: 0.1247\n",
      "37/223, train_loss: 0.1035, step time: 0.1016\n",
      "38/223, train_loss: 0.0950, step time: 0.1011\n",
      "39/223, train_loss: 0.0911, step time: 0.1002\n",
      "40/223, train_loss: 0.0899, step time: 0.1348\n",
      "41/223, train_loss: 0.1061, step time: 0.1225\n",
      "42/223, train_loss: 0.0970, step time: 0.1103\n",
      "43/223, train_loss: 0.0998, step time: 0.1102\n",
      "44/223, train_loss: 0.0937, step time: 0.1039\n",
      "45/223, train_loss: 0.0924, step time: 0.1115\n",
      "46/223, train_loss: 0.0881, step time: 0.1070\n",
      "47/223, train_loss: 0.1050, step time: 0.1032\n",
      "48/223, train_loss: 0.0985, step time: 0.1029\n",
      "49/223, train_loss: 0.0946, step time: 0.1005\n",
      "50/223, train_loss: 0.0993, step time: 0.1295\n",
      "51/223, train_loss: 0.1032, step time: 0.0988\n",
      "52/223, train_loss: 0.0974, step time: 0.1321\n",
      "53/223, train_loss: 0.1023, step time: 0.1031\n",
      "54/223, train_loss: 0.0937, step time: 0.1100\n",
      "55/223, train_loss: 0.1132, step time: 0.1134\n",
      "56/223, train_loss: 0.0960, step time: 0.1161\n",
      "57/223, train_loss: 0.0978, step time: 0.1061\n",
      "58/223, train_loss: 0.0865, step time: 0.1110\n",
      "59/223, train_loss: 0.1004, step time: 0.1138\n",
      "60/223, train_loss: 0.0991, step time: 0.1049\n",
      "61/223, train_loss: 0.0948, step time: 0.1010\n",
      "62/223, train_loss: 0.1023, step time: 0.1086\n",
      "63/223, train_loss: 0.1005, step time: 0.1094\n",
      "64/223, train_loss: 0.1010, step time: 0.1181\n",
      "65/223, train_loss: 0.1008, step time: 0.1167\n",
      "66/223, train_loss: 0.0927, step time: 0.1883\n",
      "67/223, train_loss: 0.0992, step time: 0.1484\n",
      "68/223, train_loss: 0.1102, step time: 0.1162\n",
      "69/223, train_loss: 0.1007, step time: 0.1232\n",
      "70/223, train_loss: 0.1055, step time: 0.1035\n",
      "71/223, train_loss: 0.0991, step time: 0.1006\n",
      "72/223, train_loss: 0.1008, step time: 0.1122\n",
      "73/223, train_loss: 0.0941, step time: 0.1012\n",
      "74/223, train_loss: 0.0963, step time: 0.1152\n",
      "75/223, train_loss: 0.0990, step time: 0.1170\n",
      "76/223, train_loss: 0.0917, step time: 0.1012\n",
      "77/223, train_loss: 0.1090, step time: 0.1055\n",
      "78/223, train_loss: 0.0967, step time: 0.1007\n",
      "79/223, train_loss: 0.1110, step time: 0.1009\n",
      "80/223, train_loss: 0.1038, step time: 0.1023\n",
      "81/223, train_loss: 0.1051, step time: 0.1183\n",
      "82/223, train_loss: 0.1028, step time: 0.1180\n",
      "83/223, train_loss: 0.0897, step time: 0.1391\n",
      "84/223, train_loss: 0.0995, step time: 0.1062\n",
      "85/223, train_loss: 0.1009, step time: 0.1228\n",
      "86/223, train_loss: 0.0959, step time: 0.1152\n",
      "87/223, train_loss: 0.1020, step time: 0.1247\n",
      "88/223, train_loss: 0.0944, step time: 0.1153\n",
      "89/223, train_loss: 0.0987, step time: 0.1105\n",
      "90/223, train_loss: 0.0882, step time: 0.1043\n",
      "91/223, train_loss: 0.0935, step time: 0.1251\n",
      "92/223, train_loss: 0.1106, step time: 0.1294\n",
      "93/223, train_loss: 0.0990, step time: 0.1006\n",
      "94/223, train_loss: 0.0914, step time: 0.1004\n",
      "95/223, train_loss: 0.1057, step time: 0.1006\n",
      "96/223, train_loss: 0.0906, step time: 0.1346\n",
      "97/223, train_loss: 0.1081, step time: 0.1007\n",
      "98/223, train_loss: 0.1020, step time: 0.1120\n",
      "99/223, train_loss: 0.1097, step time: 0.1091\n",
      "100/223, train_loss: 0.0957, step time: 0.1084\n",
      "101/223, train_loss: 0.0934, step time: 0.1084\n",
      "102/223, train_loss: 0.0881, step time: 0.1000\n",
      "103/223, train_loss: 0.0961, step time: 0.1008\n",
      "104/223, train_loss: 0.1089, step time: 0.1021\n",
      "105/223, train_loss: 0.0883, step time: 0.1345\n",
      "106/223, train_loss: 0.0962, step time: 0.1335\n",
      "107/223, train_loss: 0.0944, step time: 0.1005\n",
      "108/223, train_loss: 0.1020, step time: 0.1034\n",
      "109/223, train_loss: 0.1012, step time: 0.1166\n",
      "110/223, train_loss: 0.1093, step time: 0.1077\n",
      "111/223, train_loss: 0.1160, step time: 0.1440\n",
      "112/223, train_loss: 0.0877, step time: 0.1116\n",
      "113/223, train_loss: 0.1036, step time: 0.1097\n",
      "114/223, train_loss: 0.0975, step time: 0.1026\n",
      "115/223, train_loss: 0.0969, step time: 0.1276\n",
      "116/223, train_loss: 0.0931, step time: 0.1129\n",
      "117/223, train_loss: 0.1084, step time: 0.1076\n",
      "118/223, train_loss: 0.0980, step time: 0.1413\n",
      "119/223, train_loss: 0.1153, step time: 0.1429\n",
      "120/223, train_loss: 0.1062, step time: 0.1025\n",
      "121/223, train_loss: 0.1001, step time: 0.1010\n",
      "122/223, train_loss: 0.1026, step time: 0.1121\n",
      "123/223, train_loss: 0.0991, step time: 0.1260\n",
      "124/223, train_loss: 0.0956, step time: 0.1050\n",
      "125/223, train_loss: 0.1013, step time: 0.1150\n",
      "126/223, train_loss: 0.0923, step time: 0.1352\n",
      "127/223, train_loss: 0.1007, step time: 0.1255\n",
      "128/223, train_loss: 0.1023, step time: 0.1507\n",
      "129/223, train_loss: 0.1019, step time: 0.1040\n",
      "130/223, train_loss: 0.0966, step time: 0.1222\n",
      "131/223, train_loss: 0.1060, step time: 0.1014\n",
      "132/223, train_loss: 0.3061, step time: 0.1052\n",
      "133/223, train_loss: 0.1062, step time: 0.1047\n",
      "134/223, train_loss: 0.0956, step time: 0.1725\n",
      "135/223, train_loss: 0.0989, step time: 0.1598\n",
      "136/223, train_loss: 0.1036, step time: 0.1269\n",
      "137/223, train_loss: 0.0873, step time: 0.0999\n",
      "138/223, train_loss: 0.0989, step time: 0.0997\n",
      "139/223, train_loss: 0.0979, step time: 0.0994\n",
      "140/223, train_loss: 0.0868, step time: 0.1003\n",
      "141/223, train_loss: 0.0939, step time: 0.0991\n",
      "142/223, train_loss: 0.0940, step time: 0.1002\n",
      "143/223, train_loss: 0.1000, step time: 0.1002\n",
      "144/223, train_loss: 0.0920, step time: 0.1108\n",
      "145/223, train_loss: 0.0973, step time: 0.1106\n",
      "146/223, train_loss: 0.1059, step time: 0.1071\n",
      "147/223, train_loss: 0.1013, step time: 0.1136\n",
      "148/223, train_loss: 0.1016, step time: 0.1069\n",
      "149/223, train_loss: 0.1044, step time: 0.1122\n",
      "150/223, train_loss: 0.1009, step time: 0.1285\n",
      "151/223, train_loss: 0.1091, step time: 0.1257\n",
      "152/223, train_loss: 0.1027, step time: 0.1120\n",
      "153/223, train_loss: 0.1032, step time: 0.1234\n",
      "154/223, train_loss: 0.1108, step time: 0.1100\n",
      "155/223, train_loss: 0.1101, step time: 0.1362\n",
      "156/223, train_loss: 0.0917, step time: 0.1235\n",
      "157/223, train_loss: 0.0905, step time: 0.1165\n",
      "158/223, train_loss: 0.0968, step time: 0.1009\n",
      "159/223, train_loss: 0.1032, step time: 0.1178\n",
      "160/223, train_loss: 0.0950, step time: 0.1227\n",
      "161/223, train_loss: 0.0927, step time: 0.1062\n",
      "162/223, train_loss: 0.1123, step time: 0.1054\n",
      "163/223, train_loss: 0.1087, step time: 0.1539\n",
      "164/223, train_loss: 0.0977, step time: 0.1073\n",
      "165/223, train_loss: 0.0927, step time: 0.1049\n",
      "166/223, train_loss: 0.0932, step time: 0.1199\n",
      "167/223, train_loss: 0.0928, step time: 0.1354\n",
      "168/223, train_loss: 0.0947, step time: 0.1084\n",
      "169/223, train_loss: 0.1105, step time: 0.1188\n",
      "170/223, train_loss: 0.0961, step time: 0.1128\n",
      "171/223, train_loss: 0.0932, step time: 0.1167\n",
      "172/223, train_loss: 0.1002, step time: 0.1203\n",
      "173/223, train_loss: 0.1094, step time: 0.1141\n",
      "174/223, train_loss: 0.1017, step time: 0.1014\n",
      "175/223, train_loss: 0.0983, step time: 0.1031\n",
      "176/223, train_loss: 0.1069, step time: 0.1166\n",
      "177/223, train_loss: 0.0993, step time: 0.1196\n",
      "178/223, train_loss: 0.1043, step time: 0.0990\n",
      "179/223, train_loss: 0.0863, step time: 0.1277\n",
      "180/223, train_loss: 0.0916, step time: 0.0992\n",
      "181/223, train_loss: 0.0931, step time: 0.1152\n",
      "182/223, train_loss: 0.1059, step time: 0.1156\n",
      "183/223, train_loss: 0.1142, step time: 0.1108\n",
      "184/223, train_loss: 0.0943, step time: 0.1058\n",
      "185/223, train_loss: 0.0999, step time: 0.1287\n",
      "186/223, train_loss: 0.0962, step time: 0.1169\n",
      "187/223, train_loss: 0.1008, step time: 0.1137\n",
      "188/223, train_loss: 0.1102, step time: 0.1017\n",
      "189/223, train_loss: 0.1033, step time: 0.1306\n",
      "190/223, train_loss: 0.0941, step time: 0.1087\n",
      "191/223, train_loss: 0.1040, step time: 0.1012\n",
      "192/223, train_loss: 0.1016, step time: 0.1003\n",
      "193/223, train_loss: 0.0895, step time: 0.1432\n",
      "194/223, train_loss: 0.1033, step time: 0.1333\n",
      "195/223, train_loss: 0.1035, step time: 0.1007\n",
      "196/223, train_loss: 0.0978, step time: 0.1123\n",
      "197/223, train_loss: 0.0903, step time: 0.1050\n",
      "198/223, train_loss: 0.1155, step time: 0.1116\n",
      "199/223, train_loss: 0.0942, step time: 0.1184\n",
      "200/223, train_loss: 0.1009, step time: 0.1148\n",
      "201/223, train_loss: 0.0960, step time: 0.0988\n",
      "202/223, train_loss: 0.0955, step time: 0.1085\n",
      "203/223, train_loss: 0.1028, step time: 0.1236\n",
      "204/223, train_loss: 0.0975, step time: 0.1004\n",
      "205/223, train_loss: 0.0976, step time: 0.1141\n",
      "206/223, train_loss: 0.1145, step time: 0.1214\n",
      "207/223, train_loss: 0.1042, step time: 0.1107\n",
      "208/223, train_loss: 0.0828, step time: 0.1003\n",
      "209/223, train_loss: 0.0987, step time: 0.1185\n",
      "210/223, train_loss: 0.1026, step time: 0.1144\n",
      "211/223, train_loss: 0.0922, step time: 0.1453\n",
      "212/223, train_loss: 0.0927, step time: 0.0998\n",
      "213/223, train_loss: 0.0869, step time: 0.1502\n",
      "214/223, train_loss: 0.0876, step time: 0.1292\n",
      "215/223, train_loss: 0.1143, step time: 0.1443\n",
      "216/223, train_loss: 0.0979, step time: 0.1234\n",
      "217/223, train_loss: 0.1001, step time: 0.1186\n",
      "218/223, train_loss: 0.1126, step time: 0.1097\n",
      "219/223, train_loss: 0.0926, step time: 0.1269\n",
      "220/223, train_loss: 0.0954, step time: 0.1090\n",
      "221/223, train_loss: 0.0893, step time: 0.1010\n",
      "222/223, train_loss: 0.0901, step time: 0.1042\n",
      "223/223, train_loss: 0.0949, step time: 0.1007\n",
      "epoch 248 average loss: 0.1002\n",
      "time consuming of epoch 248 is: 91.6865\n",
      "----------\n",
      "epoch 249/300\n",
      "1/223, train_loss: 0.1043, step time: 0.1243\n",
      "2/223, train_loss: 0.0893, step time: 0.1222\n",
      "3/223, train_loss: 0.1009, step time: 0.1079\n",
      "4/223, train_loss: 0.1095, step time: 0.1219\n",
      "5/223, train_loss: 0.0939, step time: 0.1137\n",
      "6/223, train_loss: 0.0992, step time: 0.1140\n",
      "7/223, train_loss: 0.0908, step time: 0.1230\n",
      "8/223, train_loss: 0.0885, step time: 0.1014\n",
      "9/223, train_loss: 0.0931, step time: 0.1092\n",
      "10/223, train_loss: 0.1135, step time: 0.1011\n",
      "11/223, train_loss: 0.0960, step time: 0.1097\n",
      "12/223, train_loss: 0.1049, step time: 0.1368\n",
      "13/223, train_loss: 0.0858, step time: 0.1263\n",
      "14/223, train_loss: 0.0961, step time: 0.1194\n",
      "15/223, train_loss: 0.0851, step time: 0.1095\n",
      "16/223, train_loss: 0.0897, step time: 0.1227\n",
      "17/223, train_loss: 0.0941, step time: 0.1050\n",
      "18/223, train_loss: 0.0928, step time: 0.1002\n",
      "19/223, train_loss: 0.0911, step time: 0.1203\n",
      "20/223, train_loss: 0.1009, step time: 0.1006\n",
      "21/223, train_loss: 0.0948, step time: 0.1021\n",
      "22/223, train_loss: 0.0950, step time: 0.1010\n",
      "23/223, train_loss: 0.1010, step time: 0.1036\n",
      "24/223, train_loss: 0.1068, step time: 0.1198\n",
      "25/223, train_loss: 0.0976, step time: 0.1082\n",
      "26/223, train_loss: 0.1117, step time: 0.1141\n",
      "27/223, train_loss: 0.0995, step time: 0.1022\n",
      "28/223, train_loss: 0.0908, step time: 0.1298\n",
      "29/223, train_loss: 0.0970, step time: 0.1089\n",
      "30/223, train_loss: 0.0920, step time: 0.1335\n",
      "31/223, train_loss: 0.0950, step time: 0.1043\n",
      "32/223, train_loss: 0.0973, step time: 0.1014\n",
      "33/223, train_loss: 0.1063, step time: 0.1012\n",
      "34/223, train_loss: 0.0941, step time: 0.1052\n",
      "35/223, train_loss: 0.0880, step time: 0.1009\n",
      "36/223, train_loss: 0.0967, step time: 0.1166\n",
      "37/223, train_loss: 0.0926, step time: 0.1095\n",
      "38/223, train_loss: 0.0918, step time: 0.1082\n",
      "39/223, train_loss: 0.1016, step time: 0.1091\n",
      "40/223, train_loss: 0.1013, step time: 0.1065\n",
      "41/223, train_loss: 0.1002, step time: 0.1324\n",
      "42/223, train_loss: 0.0924, step time: 0.1241\n",
      "43/223, train_loss: 0.0947, step time: 0.1230\n",
      "44/223, train_loss: 0.1094, step time: 0.1012\n",
      "45/223, train_loss: 0.0937, step time: 0.1172\n",
      "46/223, train_loss: 0.0993, step time: 0.1162\n",
      "47/223, train_loss: 0.1005, step time: 0.1039\n",
      "48/223, train_loss: 0.1035, step time: 0.1251\n",
      "49/223, train_loss: 0.0927, step time: 0.0999\n",
      "50/223, train_loss: 0.1089, step time: 0.1000\n",
      "51/223, train_loss: 0.1063, step time: 0.1008\n",
      "52/223, train_loss: 0.0979, step time: 0.1210\n",
      "53/223, train_loss: 0.1052, step time: 0.1044\n",
      "54/223, train_loss: 0.0939, step time: 0.1107\n",
      "55/223, train_loss: 0.1108, step time: 0.1256\n",
      "56/223, train_loss: 0.1091, step time: 0.1294\n",
      "57/223, train_loss: 0.0908, step time: 0.1382\n",
      "58/223, train_loss: 0.0924, step time: 0.1236\n",
      "59/223, train_loss: 0.0923, step time: 0.1100\n",
      "60/223, train_loss: 0.0896, step time: 0.1042\n",
      "61/223, train_loss: 0.0990, step time: 0.1505\n",
      "62/223, train_loss: 0.1198, step time: 0.1200\n",
      "63/223, train_loss: 0.1009, step time: 0.1081\n",
      "64/223, train_loss: 0.0953, step time: 0.1162\n",
      "65/223, train_loss: 0.0904, step time: 0.1017\n",
      "66/223, train_loss: 0.0973, step time: 0.1155\n",
      "67/223, train_loss: 0.1037, step time: 0.1081\n",
      "68/223, train_loss: 0.1018, step time: 0.1024\n",
      "69/223, train_loss: 0.1078, step time: 0.1014\n",
      "70/223, train_loss: 0.1038, step time: 0.1007\n",
      "71/223, train_loss: 0.0951, step time: 0.1010\n",
      "72/223, train_loss: 0.0952, step time: 0.1129\n",
      "73/223, train_loss: 0.0854, step time: 0.1009\n",
      "74/223, train_loss: 0.0933, step time: 0.1008\n",
      "75/223, train_loss: 0.1111, step time: 0.1007\n",
      "76/223, train_loss: 0.1050, step time: 0.1019\n",
      "77/223, train_loss: 0.0929, step time: 0.1188\n",
      "78/223, train_loss: 0.0957, step time: 0.1001\n",
      "79/223, train_loss: 0.0964, step time: 0.0983\n",
      "80/223, train_loss: 0.1104, step time: 0.0989\n",
      "81/223, train_loss: 0.0954, step time: 0.1003\n",
      "82/223, train_loss: 0.0903, step time: 0.1001\n",
      "83/223, train_loss: 0.0985, step time: 0.0997\n",
      "84/223, train_loss: 0.0930, step time: 0.1005\n",
      "85/223, train_loss: 0.0955, step time: 0.1097\n",
      "86/223, train_loss: 0.0957, step time: 0.1018\n",
      "87/223, train_loss: 0.0948, step time: 0.1003\n",
      "88/223, train_loss: 0.0963, step time: 0.1418\n",
      "89/223, train_loss: 0.0933, step time: 0.1357\n",
      "90/223, train_loss: 0.1038, step time: 0.1067\n",
      "91/223, train_loss: 0.0978, step time: 0.0997\n",
      "92/223, train_loss: 0.0878, step time: 0.1009\n",
      "93/223, train_loss: 0.0983, step time: 0.1024\n",
      "94/223, train_loss: 0.1158, step time: 0.1150\n",
      "95/223, train_loss: 0.0991, step time: 0.1103\n",
      "96/223, train_loss: 0.0928, step time: 0.1010\n",
      "97/223, train_loss: 0.0998, step time: 0.1012\n",
      "98/223, train_loss: 0.1034, step time: 0.1523\n",
      "99/223, train_loss: 0.0997, step time: 0.1424\n",
      "100/223, train_loss: 0.1061, step time: 0.1289\n",
      "101/223, train_loss: 0.0977, step time: 0.1006\n",
      "102/223, train_loss: 0.0970, step time: 0.1159\n",
      "103/223, train_loss: 0.1085, step time: 0.1091\n",
      "104/223, train_loss: 0.0925, step time: 0.1072\n",
      "105/223, train_loss: 0.0952, step time: 0.1007\n",
      "106/223, train_loss: 0.0966, step time: 0.0999\n",
      "107/223, train_loss: 0.0923, step time: 0.1026\n",
      "108/223, train_loss: 0.0988, step time: 0.1001\n",
      "109/223, train_loss: 0.0863, step time: 0.1021\n",
      "110/223, train_loss: 0.0967, step time: 0.1202\n",
      "111/223, train_loss: 0.1038, step time: 0.1267\n",
      "112/223, train_loss: 0.0851, step time: 0.1046\n",
      "113/223, train_loss: 0.0962, step time: 0.1147\n",
      "114/223, train_loss: 0.1032, step time: 0.1056\n",
      "115/223, train_loss: 0.1108, step time: 0.1104\n",
      "116/223, train_loss: 0.0943, step time: 0.1168\n",
      "117/223, train_loss: 0.0980, step time: 0.1138\n",
      "118/223, train_loss: 0.1033, step time: 0.1167\n",
      "119/223, train_loss: 0.1009, step time: 0.1005\n",
      "120/223, train_loss: 0.1153, step time: 0.1287\n",
      "121/223, train_loss: 0.0987, step time: 0.1009\n",
      "122/223, train_loss: 0.0933, step time: 0.1131\n",
      "123/223, train_loss: 0.0997, step time: 0.1096\n",
      "124/223, train_loss: 0.1007, step time: 0.1194\n",
      "125/223, train_loss: 0.0909, step time: 0.1282\n",
      "126/223, train_loss: 0.1068, step time: 0.0992\n",
      "127/223, train_loss: 0.0904, step time: 0.0993\n",
      "128/223, train_loss: 0.0998, step time: 0.1369\n",
      "129/223, train_loss: 0.0942, step time: 0.1201\n",
      "130/223, train_loss: 0.0944, step time: 0.1096\n",
      "131/223, train_loss: 0.1073, step time: 0.1160\n",
      "132/223, train_loss: 0.0957, step time: 0.1156\n",
      "133/223, train_loss: 0.1030, step time: 0.1049\n",
      "134/223, train_loss: 0.0858, step time: 0.1200\n",
      "135/223, train_loss: 0.0943, step time: 0.1107\n",
      "136/223, train_loss: 0.1009, step time: 0.1112\n",
      "137/223, train_loss: 0.1055, step time: 0.1135\n",
      "138/223, train_loss: 0.1001, step time: 0.1041\n",
      "139/223, train_loss: 0.1072, step time: 0.1396\n",
      "140/223, train_loss: 0.1003, step time: 0.1119\n",
      "141/223, train_loss: 0.0980, step time: 0.1159\n",
      "142/223, train_loss: 0.1107, step time: 0.1108\n",
      "143/223, train_loss: 0.0850, step time: 0.1235\n",
      "144/223, train_loss: 0.1079, step time: 0.1130\n",
      "145/223, train_loss: 0.1082, step time: 0.1187\n",
      "146/223, train_loss: 0.1095, step time: 0.1236\n",
      "147/223, train_loss: 0.1064, step time: 0.1129\n",
      "148/223, train_loss: 0.1081, step time: 0.1189\n",
      "149/223, train_loss: 0.1001, step time: 0.1303\n",
      "150/223, train_loss: 0.0989, step time: 0.1209\n",
      "151/223, train_loss: 0.1071, step time: 0.1097\n",
      "152/223, train_loss: 0.1031, step time: 0.1304\n",
      "153/223, train_loss: 0.1045, step time: 0.1230\n",
      "154/223, train_loss: 0.1008, step time: 0.1138\n",
      "155/223, train_loss: 0.0943, step time: 0.1230\n",
      "156/223, train_loss: 0.0984, step time: 0.1021\n",
      "157/223, train_loss: 0.1076, step time: 0.1169\n",
      "158/223, train_loss: 0.0951, step time: 0.1520\n",
      "159/223, train_loss: 0.0960, step time: 0.1273\n",
      "160/223, train_loss: 0.0991, step time: 0.1013\n",
      "161/223, train_loss: 0.1034, step time: 0.1103\n",
      "162/223, train_loss: 0.0999, step time: 0.1267\n",
      "163/223, train_loss: 0.1015, step time: 0.1011\n",
      "164/223, train_loss: 0.1059, step time: 0.1237\n",
      "165/223, train_loss: 0.0972, step time: 0.1012\n",
      "166/223, train_loss: 0.0940, step time: 0.1095\n",
      "167/223, train_loss: 0.0921, step time: 0.1093\n",
      "168/223, train_loss: 0.1077, step time: 0.1089\n",
      "169/223, train_loss: 0.1046, step time: 0.1116\n",
      "170/223, train_loss: 0.1035, step time: 0.1002\n",
      "171/223, train_loss: 0.1089, step time: 0.0994\n",
      "172/223, train_loss: 0.0910, step time: 0.0992\n",
      "173/223, train_loss: 0.1015, step time: 0.0989\n",
      "174/223, train_loss: 0.1031, step time: 0.1009\n",
      "175/223, train_loss: 0.1010, step time: 0.1057\n",
      "176/223, train_loss: 0.1047, step time: 0.1009\n",
      "177/223, train_loss: 0.0981, step time: 0.1125\n",
      "178/223, train_loss: 0.2961, step time: 0.1006\n",
      "179/223, train_loss: 0.0982, step time: 0.1102\n",
      "180/223, train_loss: 0.0987, step time: 0.1206\n",
      "181/223, train_loss: 0.0876, step time: 0.1085\n",
      "182/223, train_loss: 0.0982, step time: 0.1075\n",
      "183/223, train_loss: 0.0960, step time: 0.1153\n",
      "184/223, train_loss: 0.1000, step time: 0.1152\n",
      "185/223, train_loss: 0.1004, step time: 0.1121\n",
      "186/223, train_loss: 0.1054, step time: 0.1109\n",
      "187/223, train_loss: 0.1071, step time: 0.1146\n",
      "188/223, train_loss: 0.1104, step time: 0.1162\n",
      "189/223, train_loss: 0.0914, step time: 0.1178\n",
      "190/223, train_loss: 0.0957, step time: 0.1126\n",
      "191/223, train_loss: 0.1002, step time: 0.1062\n",
      "192/223, train_loss: 0.1010, step time: 0.1030\n",
      "193/223, train_loss: 0.0950, step time: 0.1094\n",
      "194/223, train_loss: 0.1040, step time: 0.1146\n",
      "195/223, train_loss: 0.0979, step time: 0.1076\n",
      "196/223, train_loss: 0.1016, step time: 0.1551\n",
      "197/223, train_loss: 0.0912, step time: 0.1054\n",
      "198/223, train_loss: 0.0944, step time: 0.1198\n",
      "199/223, train_loss: 0.0961, step time: 0.0993\n",
      "200/223, train_loss: 0.0998, step time: 0.1108\n",
      "201/223, train_loss: 0.0933, step time: 0.1011\n",
      "202/223, train_loss: 0.1118, step time: 0.1212\n",
      "203/223, train_loss: 0.0898, step time: 0.1003\n",
      "204/223, train_loss: 0.1085, step time: 0.1003\n",
      "205/223, train_loss: 0.1117, step time: 0.0994\n",
      "206/223, train_loss: 0.0947, step time: 0.1062\n",
      "207/223, train_loss: 0.1088, step time: 0.1057\n",
      "208/223, train_loss: 0.0937, step time: 0.1006\n",
      "209/223, train_loss: 0.0931, step time: 0.1002\n",
      "210/223, train_loss: 0.0963, step time: 0.1087\n",
      "211/223, train_loss: 0.1106, step time: 0.1065\n",
      "212/223, train_loss: 0.1088, step time: 0.1114\n",
      "213/223, train_loss: 0.0959, step time: 0.1072\n",
      "214/223, train_loss: 0.0971, step time: 0.1177\n",
      "215/223, train_loss: 0.0938, step time: 0.1109\n",
      "216/223, train_loss: 0.1003, step time: 0.1214\n",
      "217/223, train_loss: 0.0994, step time: 0.1003\n",
      "218/223, train_loss: 0.1023, step time: 0.1037\n",
      "219/223, train_loss: 0.1133, step time: 0.1041\n",
      "220/223, train_loss: 0.1006, step time: 0.1147\n",
      "221/223, train_loss: 0.1039, step time: 0.1475\n",
      "222/223, train_loss: 0.1071, step time: 0.0998\n",
      "223/223, train_loss: 0.1017, step time: 0.0994\n",
      "epoch 249 average loss: 0.1001\n",
      "time consuming of epoch 249 is: 93.3458\n",
      "----------\n",
      "epoch 250/300\n",
      "1/223, train_loss: 0.1044, step time: 0.1131\n",
      "2/223, train_loss: 0.0937, step time: 0.1243\n",
      "3/223, train_loss: 0.0932, step time: 0.1203\n",
      "4/223, train_loss: 0.1178, step time: 0.1135\n",
      "5/223, train_loss: 0.0982, step time: 0.1012\n",
      "6/223, train_loss: 0.0875, step time: 0.1017\n",
      "7/223, train_loss: 0.0991, step time: 0.1020\n",
      "8/223, train_loss: 0.1033, step time: 0.1295\n",
      "9/223, train_loss: 0.0941, step time: 0.1017\n",
      "10/223, train_loss: 0.0895, step time: 0.1218\n",
      "11/223, train_loss: 0.0883, step time: 0.1182\n",
      "12/223, train_loss: 0.0905, step time: 0.1114\n",
      "13/223, train_loss: 0.0928, step time: 0.1007\n",
      "14/223, train_loss: 0.0872, step time: 0.1009\n",
      "15/223, train_loss: 0.1011, step time: 0.1790\n",
      "16/223, train_loss: 0.1116, step time: 0.1005\n",
      "17/223, train_loss: 0.1004, step time: 0.1159\n",
      "18/223, train_loss: 0.0923, step time: 0.1147\n",
      "19/223, train_loss: 0.0919, step time: 0.1032\n",
      "20/223, train_loss: 0.0940, step time: 0.1017\n",
      "21/223, train_loss: 0.1174, step time: 0.1090\n",
      "22/223, train_loss: 0.1015, step time: 0.1194\n",
      "23/223, train_loss: 0.1105, step time: 0.1200\n",
      "24/223, train_loss: 0.0890, step time: 0.1098\n",
      "25/223, train_loss: 0.1038, step time: 0.1009\n",
      "26/223, train_loss: 0.1121, step time: 0.1005\n",
      "27/223, train_loss: 0.0939, step time: 0.1089\n",
      "28/223, train_loss: 0.1012, step time: 0.1098\n",
      "29/223, train_loss: 0.0979, step time: 0.1137\n",
      "30/223, train_loss: 0.0955, step time: 0.1170\n",
      "31/223, train_loss: 0.0901, step time: 0.1155\n",
      "32/223, train_loss: 0.1044, step time: 0.1001\n",
      "33/223, train_loss: 0.0961, step time: 0.1079\n",
      "34/223, train_loss: 0.0885, step time: 0.1117\n",
      "35/223, train_loss: 0.0947, step time: 0.1030\n",
      "36/223, train_loss: 0.0877, step time: 0.0998\n",
      "37/223, train_loss: 0.0995, step time: 0.1178\n",
      "38/223, train_loss: 0.0956, step time: 0.1180\n",
      "39/223, train_loss: 0.1197, step time: 0.1202\n",
      "40/223, train_loss: 0.0965, step time: 0.1108\n",
      "41/223, train_loss: 0.0971, step time: 0.1007\n",
      "42/223, train_loss: 0.0949, step time: 0.1467\n",
      "43/223, train_loss: 0.0909, step time: 0.1172\n",
      "44/223, train_loss: 0.1133, step time: 0.1142\n",
      "45/223, train_loss: 0.0999, step time: 0.1298\n",
      "46/223, train_loss: 0.0973, step time: 0.1211\n",
      "47/223, train_loss: 0.1047, step time: 0.1488\n",
      "48/223, train_loss: 0.0967, step time: 0.1013\n",
      "49/223, train_loss: 0.0940, step time: 0.1262\n",
      "50/223, train_loss: 0.2951, step time: 0.1122\n",
      "51/223, train_loss: 0.1003, step time: 0.1240\n",
      "52/223, train_loss: 0.1011, step time: 0.1002\n",
      "53/223, train_loss: 0.0992, step time: 0.1274\n",
      "54/223, train_loss: 0.0937, step time: 0.1108\n",
      "55/223, train_loss: 0.0966, step time: 0.1180\n",
      "56/223, train_loss: 0.0992, step time: 0.1502\n",
      "57/223, train_loss: 0.1081, step time: 0.1322\n",
      "58/223, train_loss: 0.0930, step time: 0.1452\n",
      "59/223, train_loss: 0.0881, step time: 0.1234\n",
      "60/223, train_loss: 0.0974, step time: 0.1064\n",
      "61/223, train_loss: 0.0952, step time: 0.1069\n",
      "62/223, train_loss: 0.1062, step time: 0.1163\n",
      "63/223, train_loss: 0.0988, step time: 0.1386\n",
      "64/223, train_loss: 0.0941, step time: 0.1196\n",
      "65/223, train_loss: 0.0969, step time: 0.1095\n",
      "66/223, train_loss: 0.0938, step time: 0.1185\n",
      "67/223, train_loss: 0.0896, step time: 0.1140\n",
      "68/223, train_loss: 0.1004, step time: 0.1210\n",
      "69/223, train_loss: 0.0942, step time: 0.1132\n",
      "70/223, train_loss: 0.1018, step time: 0.1143\n",
      "71/223, train_loss: 0.1044, step time: 0.1198\n",
      "72/223, train_loss: 0.1165, step time: 0.1091\n",
      "73/223, train_loss: 0.1148, step time: 0.1219\n",
      "74/223, train_loss: 0.0983, step time: 0.1087\n",
      "75/223, train_loss: 0.0928, step time: 0.1030\n",
      "76/223, train_loss: 0.0862, step time: 0.1004\n",
      "77/223, train_loss: 0.1072, step time: 0.1117\n",
      "78/223, train_loss: 0.1083, step time: 0.1082\n",
      "79/223, train_loss: 0.1072, step time: 0.1156\n",
      "80/223, train_loss: 0.0991, step time: 0.1005\n",
      "81/223, train_loss: 0.1034, step time: 0.1072\n",
      "82/223, train_loss: 0.0986, step time: 0.1117\n",
      "83/223, train_loss: 0.0979, step time: 0.1012\n",
      "84/223, train_loss: 0.1072, step time: 0.1070\n",
      "85/223, train_loss: 0.0917, step time: 0.1055\n",
      "86/223, train_loss: 0.0975, step time: 0.1114\n",
      "87/223, train_loss: 0.1043, step time: 0.1015\n",
      "88/223, train_loss: 0.1087, step time: 0.1038\n",
      "89/223, train_loss: 0.1064, step time: 0.1124\n",
      "90/223, train_loss: 0.1006, step time: 0.1198\n",
      "91/223, train_loss: 0.1079, step time: 0.1049\n",
      "92/223, train_loss: 0.0953, step time: 0.1004\n",
      "93/223, train_loss: 0.1117, step time: 0.1052\n",
      "94/223, train_loss: 0.0932, step time: 0.1006\n",
      "95/223, train_loss: 0.1008, step time: 0.1038\n",
      "96/223, train_loss: 0.0985, step time: 0.1000\n",
      "97/223, train_loss: 0.1078, step time: 0.1154\n",
      "98/223, train_loss: 0.0994, step time: 0.1170\n",
      "99/223, train_loss: 0.0943, step time: 0.1036\n",
      "100/223, train_loss: 0.0878, step time: 0.1005\n",
      "101/223, train_loss: 0.1105, step time: 0.1258\n",
      "102/223, train_loss: 0.0978, step time: 0.1076\n",
      "103/223, train_loss: 0.1007, step time: 0.1018\n",
      "104/223, train_loss: 0.1049, step time: 0.1176\n",
      "105/223, train_loss: 0.0908, step time: 0.1181\n",
      "106/223, train_loss: 0.1094, step time: 0.1252\n",
      "107/223, train_loss: 0.0979, step time: 0.1107\n",
      "108/223, train_loss: 0.1091, step time: 0.1008\n",
      "109/223, train_loss: 0.0971, step time: 0.1276\n",
      "110/223, train_loss: 0.1014, step time: 0.1040\n",
      "111/223, train_loss: 0.1001, step time: 0.1269\n",
      "112/223, train_loss: 0.0981, step time: 0.1180\n",
      "113/223, train_loss: 0.1049, step time: 0.1005\n",
      "114/223, train_loss: 0.0967, step time: 0.1003\n",
      "115/223, train_loss: 0.1021, step time: 0.1230\n",
      "116/223, train_loss: 0.0958, step time: 0.1009\n",
      "117/223, train_loss: 0.0933, step time: 0.1098\n",
      "118/223, train_loss: 0.1048, step time: 0.1216\n",
      "119/223, train_loss: 0.0941, step time: 0.1132\n",
      "120/223, train_loss: 0.1068, step time: 0.1182\n",
      "121/223, train_loss: 0.0931, step time: 0.1017\n",
      "122/223, train_loss: 0.0961, step time: 0.1057\n",
      "123/223, train_loss: 0.1013, step time: 0.1300\n",
      "124/223, train_loss: 0.0985, step time: 0.1003\n",
      "125/223, train_loss: 0.0912, step time: 0.0998\n",
      "126/223, train_loss: 0.0933, step time: 0.1010\n",
      "127/223, train_loss: 0.0921, step time: 0.1012\n",
      "128/223, train_loss: 0.0949, step time: 0.1002\n",
      "129/223, train_loss: 0.1026, step time: 0.1010\n",
      "130/223, train_loss: 0.1023, step time: 0.1076\n",
      "131/223, train_loss: 0.1087, step time: 0.1005\n",
      "132/223, train_loss: 0.1014, step time: 0.1010\n",
      "133/223, train_loss: 0.0966, step time: 0.1007\n",
      "134/223, train_loss: 0.0951, step time: 0.1054\n",
      "135/223, train_loss: 0.1064, step time: 0.1324\n",
      "136/223, train_loss: 0.0995, step time: 0.1182\n",
      "137/223, train_loss: 0.0965, step time: 0.0994\n",
      "138/223, train_loss: 0.1016, step time: 0.1001\n",
      "139/223, train_loss: 0.0954, step time: 0.1057\n",
      "140/223, train_loss: 0.0961, step time: 0.1005\n",
      "141/223, train_loss: 0.1078, step time: 0.1025\n",
      "142/223, train_loss: 0.1031, step time: 0.0994\n",
      "143/223, train_loss: 0.1049, step time: 0.1364\n",
      "144/223, train_loss: 0.1064, step time: 0.1150\n",
      "145/223, train_loss: 0.0997, step time: 0.1100\n",
      "146/223, train_loss: 0.1081, step time: 0.1154\n",
      "147/223, train_loss: 0.0857, step time: 0.1282\n",
      "148/223, train_loss: 0.0936, step time: 0.1189\n",
      "149/223, train_loss: 0.0983, step time: 0.1007\n",
      "150/223, train_loss: 0.0916, step time: 0.1117\n",
      "151/223, train_loss: 0.0959, step time: 0.1320\n",
      "152/223, train_loss: 0.1001, step time: 0.1325\n",
      "153/223, train_loss: 0.0884, step time: 0.1262\n",
      "154/223, train_loss: 0.1058, step time: 0.1005\n",
      "155/223, train_loss: 0.1045, step time: 0.1385\n",
      "156/223, train_loss: 0.0923, step time: 0.1187\n",
      "157/223, train_loss: 0.1006, step time: 0.1324\n",
      "158/223, train_loss: 0.0933, step time: 0.1211\n",
      "159/223, train_loss: 0.0972, step time: 0.1123\n",
      "160/223, train_loss: 0.0993, step time: 0.1037\n",
      "161/223, train_loss: 0.1062, step time: 0.1180\n",
      "162/223, train_loss: 0.1017, step time: 0.1181\n",
      "163/223, train_loss: 0.0961, step time: 0.1002\n",
      "164/223, train_loss: 0.0931, step time: 0.1006\n",
      "165/223, train_loss: 0.0974, step time: 0.1034\n",
      "166/223, train_loss: 0.1056, step time: 0.1203\n",
      "167/223, train_loss: 0.1088, step time: 0.1335\n",
      "168/223, train_loss: 0.0956, step time: 0.0999\n",
      "169/223, train_loss: 0.1081, step time: 0.1067\n",
      "170/223, train_loss: 0.0928, step time: 0.1048\n",
      "171/223, train_loss: 0.0903, step time: 0.1000\n",
      "172/223, train_loss: 0.0967, step time: 0.0998\n",
      "173/223, train_loss: 0.0971, step time: 0.1021\n",
      "174/223, train_loss: 0.0905, step time: 0.1020\n",
      "175/223, train_loss: 0.1028, step time: 0.1247\n",
      "176/223, train_loss: 0.1007, step time: 0.1006\n",
      "177/223, train_loss: 0.0888, step time: 0.1008\n",
      "178/223, train_loss: 0.1014, step time: 0.1077\n",
      "179/223, train_loss: 0.1023, step time: 0.1115\n",
      "180/223, train_loss: 0.0964, step time: 0.1139\n",
      "181/223, train_loss: 0.0933, step time: 0.1076\n",
      "182/223, train_loss: 0.1046, step time: 0.1155\n",
      "183/223, train_loss: 0.1157, step time: 0.1244\n",
      "184/223, train_loss: 0.1025, step time: 0.1050\n",
      "185/223, train_loss: 0.1007, step time: 0.1057\n",
      "186/223, train_loss: 0.0921, step time: 0.1142\n",
      "187/223, train_loss: 0.0971, step time: 0.1006\n",
      "188/223, train_loss: 0.0895, step time: 0.1017\n",
      "189/223, train_loss: 0.0932, step time: 0.1388\n",
      "190/223, train_loss: 0.0898, step time: 0.1117\n",
      "191/223, train_loss: 0.0994, step time: 0.1247\n",
      "192/223, train_loss: 0.1003, step time: 0.1478\n",
      "193/223, train_loss: 0.0916, step time: 0.1292\n",
      "194/223, train_loss: 0.1028, step time: 0.1001\n",
      "195/223, train_loss: 0.0914, step time: 0.0998\n",
      "196/223, train_loss: 0.1035, step time: 0.0990\n",
      "197/223, train_loss: 0.0869, step time: 0.1089\n",
      "198/223, train_loss: 0.0937, step time: 0.0989\n",
      "199/223, train_loss: 0.0957, step time: 0.1001\n",
      "200/223, train_loss: 0.1071, step time: 0.0997\n",
      "201/223, train_loss: 0.1036, step time: 0.1003\n",
      "202/223, train_loss: 0.1091, step time: 0.0998\n",
      "203/223, train_loss: 0.1096, step time: 0.0996\n",
      "204/223, train_loss: 0.1177, step time: 0.0995\n",
      "205/223, train_loss: 0.0995, step time: 0.1148\n",
      "206/223, train_loss: 0.0908, step time: 0.0989\n",
      "207/223, train_loss: 0.1107, step time: 0.0999\n",
      "208/223, train_loss: 0.1154, step time: 0.0999\n",
      "209/223, train_loss: 0.0983, step time: 0.0999\n",
      "210/223, train_loss: 0.0949, step time: 0.1001\n",
      "211/223, train_loss: 0.1100, step time: 0.0994\n",
      "212/223, train_loss: 0.1007, step time: 0.1003\n",
      "213/223, train_loss: 0.0848, step time: 0.1001\n",
      "214/223, train_loss: 0.1019, step time: 0.1010\n",
      "215/223, train_loss: 0.0922, step time: 0.1100\n",
      "216/223, train_loss: 0.1016, step time: 0.1673\n",
      "217/223, train_loss: 0.0888, step time: 0.1090\n",
      "218/223, train_loss: 0.1020, step time: 0.1012\n",
      "219/223, train_loss: 0.0950, step time: 0.1019\n",
      "220/223, train_loss: 0.1029, step time: 0.0997\n",
      "221/223, train_loss: 0.0948, step time: 0.1004\n",
      "222/223, train_loss: 0.0918, step time: 0.1000\n",
      "223/223, train_loss: 0.0968, step time: 0.0996\n",
      "epoch 250 average loss: 0.1000\n",
      "saved new best metric model\n",
      "current epoch: 250 current mean dice: 0.8620 tc: 0.9225 wt: 0.8716 et: 0.7918\n",
      "best mean dice: 0.8620 at epoch: 250\n",
      "time consuming of epoch 250 is: 99.1599\n",
      "----------\n",
      "epoch 251/300\n",
      "1/223, train_loss: 0.0993, step time: 0.1055\n",
      "2/223, train_loss: 0.1023, step time: 0.0993\n",
      "3/223, train_loss: 0.1037, step time: 0.1001\n",
      "4/223, train_loss: 0.1082, step time: 0.1003\n",
      "5/223, train_loss: 0.0978, step time: 0.1070\n",
      "6/223, train_loss: 0.1070, step time: 0.1002\n",
      "7/223, train_loss: 0.0948, step time: 0.1140\n",
      "8/223, train_loss: 0.0995, step time: 0.1177\n",
      "9/223, train_loss: 0.1010, step time: 0.1191\n",
      "10/223, train_loss: 0.1011, step time: 0.1002\n",
      "11/223, train_loss: 0.0910, step time: 0.1107\n",
      "12/223, train_loss: 0.0937, step time: 0.1200\n",
      "13/223, train_loss: 0.0933, step time: 0.1080\n",
      "14/223, train_loss: 0.0993, step time: 0.1025\n",
      "15/223, train_loss: 0.0865, step time: 0.0996\n",
      "16/223, train_loss: 0.0904, step time: 0.1005\n",
      "17/223, train_loss: 0.0964, step time: 0.1002\n",
      "18/223, train_loss: 0.0979, step time: 0.0997\n",
      "19/223, train_loss: 0.0866, step time: 0.1007\n",
      "20/223, train_loss: 0.0934, step time: 0.1188\n",
      "21/223, train_loss: 0.1136, step time: 0.1095\n",
      "22/223, train_loss: 0.1001, step time: 0.1004\n",
      "23/223, train_loss: 0.0910, step time: 0.0997\n",
      "24/223, train_loss: 0.0995, step time: 0.1184\n",
      "25/223, train_loss: 0.1024, step time: 0.1055\n",
      "26/223, train_loss: 0.1078, step time: 0.1204\n",
      "27/223, train_loss: 0.1096, step time: 0.0997\n",
      "28/223, train_loss: 0.0942, step time: 0.1152\n",
      "29/223, train_loss: 0.1030, step time: 0.1013\n",
      "30/223, train_loss: 0.1026, step time: 0.1200\n",
      "31/223, train_loss: 0.1012, step time: 0.1141\n",
      "32/223, train_loss: 0.1045, step time: 0.1087\n",
      "33/223, train_loss: 0.1026, step time: 0.1257\n",
      "34/223, train_loss: 0.1011, step time: 0.1053\n",
      "35/223, train_loss: 0.0852, step time: 0.1040\n",
      "36/223, train_loss: 0.1045, step time: 0.1100\n",
      "37/223, train_loss: 0.0948, step time: 0.1001\n",
      "38/223, train_loss: 0.0862, step time: 0.1128\n",
      "39/223, train_loss: 0.1039, step time: 0.1136\n",
      "40/223, train_loss: 0.0955, step time: 0.1184\n",
      "41/223, train_loss: 0.1067, step time: 0.1021\n",
      "42/223, train_loss: 0.0972, step time: 0.1003\n",
      "43/223, train_loss: 0.0943, step time: 0.1063\n",
      "44/223, train_loss: 0.0913, step time: 0.1147\n",
      "45/223, train_loss: 0.1072, step time: 0.0994\n",
      "46/223, train_loss: 0.0916, step time: 0.1012\n",
      "47/223, train_loss: 0.1014, step time: 0.1025\n",
      "48/223, train_loss: 0.0870, step time: 0.1007\n",
      "49/223, train_loss: 0.2993, step time: 0.1001\n",
      "50/223, train_loss: 0.0961, step time: 0.1003\n",
      "51/223, train_loss: 0.0988, step time: 0.0995\n",
      "52/223, train_loss: 0.1177, step time: 0.1002\n",
      "53/223, train_loss: 0.1078, step time: 0.1115\n",
      "54/223, train_loss: 0.0936, step time: 0.1007\n",
      "55/223, train_loss: 0.1047, step time: 0.1167\n",
      "56/223, train_loss: 0.1010, step time: 0.1008\n",
      "57/223, train_loss: 0.0917, step time: 0.1008\n",
      "58/223, train_loss: 0.1056, step time: 0.1000\n",
      "59/223, train_loss: 0.0942, step time: 0.1203\n",
      "60/223, train_loss: 0.0944, step time: 0.1137\n",
      "61/223, train_loss: 0.0986, step time: 0.1023\n",
      "62/223, train_loss: 0.0899, step time: 0.1009\n",
      "63/223, train_loss: 0.1110, step time: 0.1213\n",
      "64/223, train_loss: 0.1081, step time: 0.1185\n",
      "65/223, train_loss: 0.1014, step time: 0.1005\n",
      "66/223, train_loss: 0.0907, step time: 0.1010\n",
      "67/223, train_loss: 0.0965, step time: 0.1148\n",
      "68/223, train_loss: 0.1120, step time: 0.1003\n",
      "69/223, train_loss: 0.1004, step time: 0.1071\n",
      "70/223, train_loss: 0.1024, step time: 0.1003\n",
      "71/223, train_loss: 0.0930, step time: 0.1090\n",
      "72/223, train_loss: 0.0904, step time: 0.1179\n",
      "73/223, train_loss: 0.0973, step time: 0.1000\n",
      "74/223, train_loss: 0.0985, step time: 0.1005\n",
      "75/223, train_loss: 0.1004, step time: 0.1092\n",
      "76/223, train_loss: 0.0997, step time: 0.1004\n",
      "77/223, train_loss: 0.0974, step time: 0.1285\n",
      "78/223, train_loss: 0.0959, step time: 0.1230\n",
      "79/223, train_loss: 0.1087, step time: 0.1149\n",
      "80/223, train_loss: 0.1068, step time: 0.1214\n",
      "81/223, train_loss: 0.0903, step time: 0.1053\n",
      "82/223, train_loss: 0.1041, step time: 0.1018\n",
      "83/223, train_loss: 0.0965, step time: 0.1008\n",
      "84/223, train_loss: 0.1029, step time: 0.1004\n",
      "85/223, train_loss: 0.0866, step time: 0.1015\n",
      "86/223, train_loss: 0.1063, step time: 0.1119\n",
      "87/223, train_loss: 0.0944, step time: 0.1180\n",
      "88/223, train_loss: 0.1038, step time: 0.1021\n",
      "89/223, train_loss: 0.0953, step time: 0.1040\n",
      "90/223, train_loss: 0.1060, step time: 0.1090\n",
      "91/223, train_loss: 0.0912, step time: 0.1171\n",
      "92/223, train_loss: 0.0974, step time: 0.1105\n",
      "93/223, train_loss: 0.0973, step time: 0.1233\n",
      "94/223, train_loss: 0.1032, step time: 0.1049\n",
      "95/223, train_loss: 0.1007, step time: 0.1146\n",
      "96/223, train_loss: 0.0999, step time: 0.1008\n",
      "97/223, train_loss: 0.0898, step time: 0.1148\n",
      "98/223, train_loss: 0.0969, step time: 0.1017\n",
      "99/223, train_loss: 0.0977, step time: 0.1122\n",
      "100/223, train_loss: 0.1058, step time: 0.1118\n",
      "101/223, train_loss: 0.0864, step time: 0.1279\n",
      "102/223, train_loss: 0.1131, step time: 0.1259\n",
      "103/223, train_loss: 0.0907, step time: 0.1275\n",
      "104/223, train_loss: 0.1005, step time: 0.1071\n",
      "105/223, train_loss: 0.0994, step time: 0.1122\n",
      "106/223, train_loss: 0.1022, step time: 0.1113\n",
      "107/223, train_loss: 0.0966, step time: 0.1002\n",
      "108/223, train_loss: 0.0886, step time: 0.1079\n",
      "109/223, train_loss: 0.0848, step time: 0.1165\n",
      "110/223, train_loss: 0.0895, step time: 0.1106\n",
      "111/223, train_loss: 0.0979, step time: 0.1094\n",
      "112/223, train_loss: 0.1032, step time: 0.1025\n",
      "113/223, train_loss: 0.1011, step time: 0.1013\n",
      "114/223, train_loss: 0.0885, step time: 0.1179\n",
      "115/223, train_loss: 0.1043, step time: 0.1134\n",
      "116/223, train_loss: 0.0908, step time: 0.1059\n",
      "117/223, train_loss: 0.0984, step time: 0.1134\n",
      "118/223, train_loss: 0.1067, step time: 0.1074\n",
      "119/223, train_loss: 0.1048, step time: 0.1224\n",
      "120/223, train_loss: 0.0989, step time: 0.1003\n",
      "121/223, train_loss: 0.1071, step time: 0.1004\n",
      "122/223, train_loss: 0.0905, step time: 0.1005\n",
      "123/223, train_loss: 0.0961, step time: 0.1060\n",
      "124/223, train_loss: 0.1148, step time: 0.1031\n",
      "125/223, train_loss: 0.0897, step time: 0.1111\n",
      "126/223, train_loss: 0.1024, step time: 0.1125\n",
      "127/223, train_loss: 0.0912, step time: 0.1203\n",
      "128/223, train_loss: 0.1033, step time: 0.1042\n",
      "129/223, train_loss: 0.1045, step time: 0.1048\n",
      "130/223, train_loss: 0.0943, step time: 0.1001\n",
      "131/223, train_loss: 0.0962, step time: 0.1004\n",
      "132/223, train_loss: 0.1048, step time: 0.1009\n",
      "133/223, train_loss: 0.1025, step time: 0.1006\n",
      "134/223, train_loss: 0.1063, step time: 0.1136\n",
      "135/223, train_loss: 0.1012, step time: 0.1304\n",
      "136/223, train_loss: 0.0953, step time: 0.1104\n",
      "137/223, train_loss: 0.1014, step time: 0.1010\n",
      "138/223, train_loss: 0.1010, step time: 0.1311\n",
      "139/223, train_loss: 0.0937, step time: 0.1007\n",
      "140/223, train_loss: 0.0947, step time: 0.1192\n",
      "141/223, train_loss: 0.1090, step time: 0.1526\n",
      "142/223, train_loss: 0.0967, step time: 0.1015\n",
      "143/223, train_loss: 0.0986, step time: 0.0997\n",
      "144/223, train_loss: 0.0918, step time: 0.1004\n",
      "145/223, train_loss: 0.0873, step time: 0.1006\n",
      "146/223, train_loss: 0.0991, step time: 0.1020\n",
      "147/223, train_loss: 0.1043, step time: 0.1046\n",
      "148/223, train_loss: 0.1021, step time: 0.1046\n",
      "149/223, train_loss: 0.1058, step time: 0.1176\n",
      "150/223, train_loss: 0.0920, step time: 0.1067\n",
      "151/223, train_loss: 0.0932, step time: 0.1092\n",
      "152/223, train_loss: 0.1080, step time: 0.1184\n",
      "153/223, train_loss: 0.1110, step time: 0.1262\n",
      "154/223, train_loss: 0.1037, step time: 0.1002\n",
      "155/223, train_loss: 0.0976, step time: 0.1275\n",
      "156/223, train_loss: 0.0985, step time: 0.1004\n",
      "157/223, train_loss: 0.0911, step time: 0.1059\n",
      "158/223, train_loss: 0.0964, step time: 0.1049\n",
      "159/223, train_loss: 0.0966, step time: 0.1084\n",
      "160/223, train_loss: 0.0965, step time: 0.1283\n",
      "161/223, train_loss: 0.1009, step time: 0.1085\n",
      "162/223, train_loss: 0.1060, step time: 0.1272\n",
      "163/223, train_loss: 0.0897, step time: 0.1059\n",
      "164/223, train_loss: 0.1027, step time: 0.1172\n",
      "165/223, train_loss: 0.1042, step time: 0.1169\n",
      "166/223, train_loss: 0.0973, step time: 0.1070\n",
      "167/223, train_loss: 0.1070, step time: 0.1198\n",
      "168/223, train_loss: 0.1062, step time: 0.0998\n",
      "169/223, train_loss: 0.1031, step time: 0.0993\n",
      "170/223, train_loss: 0.1150, step time: 0.0992\n",
      "171/223, train_loss: 0.0829, step time: 0.1010\n",
      "172/223, train_loss: 0.1077, step time: 0.1035\n",
      "173/223, train_loss: 0.0970, step time: 0.1135\n",
      "174/223, train_loss: 0.1120, step time: 0.1005\n",
      "175/223, train_loss: 0.0978, step time: 0.1244\n",
      "176/223, train_loss: 0.0958, step time: 0.1075\n",
      "177/223, train_loss: 0.1035, step time: 0.1045\n",
      "178/223, train_loss: 0.0938, step time: 0.1077\n",
      "179/223, train_loss: 0.0919, step time: 0.1176\n",
      "180/223, train_loss: 0.0932, step time: 0.1150\n",
      "181/223, train_loss: 0.1084, step time: 0.1400\n",
      "182/223, train_loss: 0.0956, step time: 0.1049\n",
      "183/223, train_loss: 0.1052, step time: 0.1123\n",
      "184/223, train_loss: 0.0981, step time: 0.1003\n",
      "185/223, train_loss: 0.0981, step time: 0.1014\n",
      "186/223, train_loss: 0.1033, step time: 0.1277\n",
      "187/223, train_loss: 0.0925, step time: 0.1013\n",
      "188/223, train_loss: 0.0897, step time: 0.1009\n",
      "189/223, train_loss: 0.1136, step time: 0.1242\n",
      "190/223, train_loss: 0.0979, step time: 0.1202\n",
      "191/223, train_loss: 0.0931, step time: 0.1010\n",
      "192/223, train_loss: 0.0964, step time: 0.1083\n",
      "193/223, train_loss: 0.0913, step time: 0.1193\n",
      "194/223, train_loss: 0.0994, step time: 0.1128\n",
      "195/223, train_loss: 0.0978, step time: 0.1154\n",
      "196/223, train_loss: 0.1083, step time: 0.1015\n",
      "197/223, train_loss: 0.0914, step time: 0.1014\n",
      "198/223, train_loss: 0.0935, step time: 0.1295\n",
      "199/223, train_loss: 0.1133, step time: 0.1341\n",
      "200/223, train_loss: 0.0893, step time: 0.1036\n",
      "201/223, train_loss: 0.1085, step time: 0.1126\n",
      "202/223, train_loss: 0.0955, step time: 0.1094\n",
      "203/223, train_loss: 0.0945, step time: 0.1263\n",
      "204/223, train_loss: 0.0947, step time: 0.1000\n",
      "205/223, train_loss: 0.1134, step time: 0.1161\n",
      "206/223, train_loss: 0.0943, step time: 0.1214\n",
      "207/223, train_loss: 0.1036, step time: 0.1070\n",
      "208/223, train_loss: 0.0969, step time: 0.1001\n",
      "209/223, train_loss: 0.0977, step time: 0.1130\n",
      "210/223, train_loss: 0.0934, step time: 0.1233\n",
      "211/223, train_loss: 0.1030, step time: 0.1242\n",
      "212/223, train_loss: 0.0960, step time: 0.1211\n",
      "213/223, train_loss: 0.0991, step time: 0.0999\n",
      "214/223, train_loss: 0.0950, step time: 0.1198\n",
      "215/223, train_loss: 0.1010, step time: 0.1043\n",
      "216/223, train_loss: 0.1010, step time: 0.1006\n",
      "217/223, train_loss: 0.1007, step time: 0.1148\n",
      "218/223, train_loss: 0.1005, step time: 0.1009\n",
      "219/223, train_loss: 0.0918, step time: 0.0993\n",
      "220/223, train_loss: 0.1168, step time: 0.0997\n",
      "221/223, train_loss: 0.1072, step time: 0.1004\n",
      "222/223, train_loss: 0.1035, step time: 0.1003\n",
      "223/223, train_loss: 0.1081, step time: 0.0983\n",
      "epoch 251 average loss: 0.1000\n",
      "time consuming of epoch 251 is: 92.6026\n",
      "----------\n",
      "epoch 252/300\n",
      "1/223, train_loss: 0.1026, step time: 0.1017\n",
      "2/223, train_loss: 0.1093, step time: 0.1006\n",
      "3/223, train_loss: 0.0975, step time: 0.1345\n",
      "4/223, train_loss: 0.0936, step time: 0.1005\n",
      "5/223, train_loss: 0.1047, step time: 0.1109\n",
      "6/223, train_loss: 0.0987, step time: 0.1082\n",
      "7/223, train_loss: 0.0978, step time: 0.1002\n",
      "8/223, train_loss: 0.1002, step time: 0.1011\n",
      "9/223, train_loss: 0.1077, step time: 0.1316\n",
      "10/223, train_loss: 0.0930, step time: 0.1188\n",
      "11/223, train_loss: 0.0923, step time: 0.1131\n",
      "12/223, train_loss: 0.0961, step time: 0.1030\n",
      "13/223, train_loss: 0.0900, step time: 0.1006\n",
      "14/223, train_loss: 0.1001, step time: 0.1448\n",
      "15/223, train_loss: 0.1012, step time: 0.1023\n",
      "16/223, train_loss: 0.1030, step time: 0.1062\n",
      "17/223, train_loss: 0.1041, step time: 0.1009\n",
      "18/223, train_loss: 0.1043, step time: 0.1048\n",
      "19/223, train_loss: 0.1037, step time: 0.1058\n",
      "20/223, train_loss: 0.1013, step time: 0.0997\n",
      "21/223, train_loss: 0.1001, step time: 0.1139\n",
      "22/223, train_loss: 0.1014, step time: 0.1262\n",
      "23/223, train_loss: 0.2985, step time: 0.1128\n",
      "24/223, train_loss: 0.1173, step time: 0.1143\n",
      "25/223, train_loss: 0.0926, step time: 0.1005\n",
      "26/223, train_loss: 0.1063, step time: 0.1060\n",
      "27/223, train_loss: 0.0831, step time: 0.1392\n",
      "28/223, train_loss: 0.1010, step time: 0.1006\n",
      "29/223, train_loss: 0.1002, step time: 0.1027\n",
      "30/223, train_loss: 0.0953, step time: 0.0997\n",
      "31/223, train_loss: 0.1145, step time: 0.1002\n",
      "32/223, train_loss: 0.0924, step time: 0.1316\n",
      "33/223, train_loss: 0.0955, step time: 0.1004\n",
      "34/223, train_loss: 0.0853, step time: 0.1072\n",
      "35/223, train_loss: 0.1095, step time: 0.1002\n",
      "36/223, train_loss: 0.1016, step time: 0.1168\n",
      "37/223, train_loss: 0.1089, step time: 0.1254\n",
      "38/223, train_loss: 0.1011, step time: 0.1007\n",
      "39/223, train_loss: 0.1058, step time: 0.1011\n",
      "40/223, train_loss: 0.1023, step time: 0.1144\n",
      "41/223, train_loss: 0.1132, step time: 0.1127\n",
      "42/223, train_loss: 0.0888, step time: 0.1004\n",
      "43/223, train_loss: 0.0993, step time: 0.1003\n",
      "44/223, train_loss: 0.1057, step time: 0.1166\n",
      "45/223, train_loss: 0.0893, step time: 0.1199\n",
      "46/223, train_loss: 0.0990, step time: 0.1036\n",
      "47/223, train_loss: 0.0905, step time: 0.1028\n",
      "48/223, train_loss: 0.0897, step time: 0.1013\n",
      "49/223, train_loss: 0.0990, step time: 0.1201\n",
      "50/223, train_loss: 0.1087, step time: 0.0998\n",
      "51/223, train_loss: 0.0895, step time: 0.1027\n",
      "52/223, train_loss: 0.1102, step time: 0.1391\n",
      "53/223, train_loss: 0.1125, step time: 0.1108\n",
      "54/223, train_loss: 0.0882, step time: 0.1081\n",
      "55/223, train_loss: 0.0998, step time: 0.1307\n",
      "56/223, train_loss: 0.0996, step time: 0.1316\n",
      "57/223, train_loss: 0.1112, step time: 0.1119\n",
      "58/223, train_loss: 0.0900, step time: 0.1076\n",
      "59/223, train_loss: 0.0892, step time: 0.1769\n",
      "60/223, train_loss: 0.0883, step time: 0.1144\n",
      "61/223, train_loss: 0.0956, step time: 0.1216\n",
      "62/223, train_loss: 0.0993, step time: 0.1376\n",
      "63/223, train_loss: 0.0963, step time: 0.1008\n",
      "64/223, train_loss: 0.1023, step time: 0.1007\n",
      "65/223, train_loss: 0.0985, step time: 0.1001\n",
      "66/223, train_loss: 0.0954, step time: 0.0996\n",
      "67/223, train_loss: 0.0945, step time: 0.0995\n",
      "68/223, train_loss: 0.1017, step time: 0.1061\n",
      "69/223, train_loss: 0.1170, step time: 0.0999\n",
      "70/223, train_loss: 0.1065, step time: 0.1000\n",
      "71/223, train_loss: 0.0979, step time: 0.1012\n",
      "72/223, train_loss: 0.1005, step time: 0.1007\n",
      "73/223, train_loss: 0.0960, step time: 0.0993\n",
      "74/223, train_loss: 0.0982, step time: 0.0992\n",
      "75/223, train_loss: 0.1077, step time: 0.1003\n",
      "76/223, train_loss: 0.0965, step time: 0.1045\n",
      "77/223, train_loss: 0.1008, step time: 0.1021\n",
      "78/223, train_loss: 0.1094, step time: 0.1059\n",
      "79/223, train_loss: 0.0992, step time: 0.1147\n",
      "80/223, train_loss: 0.0992, step time: 0.1067\n",
      "81/223, train_loss: 0.1011, step time: 0.1105\n",
      "82/223, train_loss: 0.0938, step time: 0.1187\n",
      "83/223, train_loss: 0.0911, step time: 0.1069\n",
      "84/223, train_loss: 0.1023, step time: 0.1164\n",
      "85/223, train_loss: 0.1006, step time: 0.1132\n",
      "86/223, train_loss: 0.0931, step time: 0.1181\n",
      "87/223, train_loss: 0.0953, step time: 0.1184\n",
      "88/223, train_loss: 0.0993, step time: 0.1460\n",
      "89/223, train_loss: 0.0939, step time: 0.1030\n",
      "90/223, train_loss: 0.1154, step time: 0.1063\n",
      "91/223, train_loss: 0.1029, step time: 0.1058\n",
      "92/223, train_loss: 0.0891, step time: 0.1022\n",
      "93/223, train_loss: 0.0955, step time: 0.1424\n",
      "94/223, train_loss: 0.1037, step time: 0.1015\n",
      "95/223, train_loss: 0.0914, step time: 0.1142\n",
      "96/223, train_loss: 0.1034, step time: 0.1069\n",
      "97/223, train_loss: 0.0923, step time: 0.1020\n",
      "98/223, train_loss: 0.1019, step time: 0.1000\n",
      "99/223, train_loss: 0.1023, step time: 0.1303\n",
      "100/223, train_loss: 0.1072, step time: 0.1114\n",
      "101/223, train_loss: 0.0884, step time: 0.1077\n",
      "102/223, train_loss: 0.0969, step time: 0.1099\n",
      "103/223, train_loss: 0.0917, step time: 0.1005\n",
      "104/223, train_loss: 0.0973, step time: 0.1026\n",
      "105/223, train_loss: 0.1002, step time: 0.1394\n",
      "106/223, train_loss: 0.0977, step time: 0.1221\n",
      "107/223, train_loss: 0.1031, step time: 0.1207\n",
      "108/223, train_loss: 0.0911, step time: 0.1088\n",
      "109/223, train_loss: 0.0967, step time: 0.1091\n",
      "110/223, train_loss: 0.1027, step time: 0.0991\n",
      "111/223, train_loss: 0.0969, step time: 0.1000\n",
      "112/223, train_loss: 0.0955, step time: 0.1198\n",
      "113/223, train_loss: 0.0950, step time: 0.1154\n",
      "114/223, train_loss: 0.0924, step time: 0.1056\n",
      "115/223, train_loss: 0.0894, step time: 0.1139\n",
      "116/223, train_loss: 0.0945, step time: 0.1079\n",
      "117/223, train_loss: 0.0908, step time: 0.1162\n",
      "118/223, train_loss: 0.0961, step time: 0.1179\n",
      "119/223, train_loss: 0.0963, step time: 0.1203\n",
      "120/223, train_loss: 0.1126, step time: 0.1118\n",
      "121/223, train_loss: 0.1110, step time: 0.1028\n",
      "122/223, train_loss: 0.1035, step time: 0.1013\n",
      "123/223, train_loss: 0.0971, step time: 0.1193\n",
      "124/223, train_loss: 0.1003, step time: 0.1096\n",
      "125/223, train_loss: 0.1023, step time: 0.0993\n",
      "126/223, train_loss: 0.0946, step time: 0.1096\n",
      "127/223, train_loss: 0.1050, step time: 0.1167\n",
      "128/223, train_loss: 0.1054, step time: 0.1140\n",
      "129/223, train_loss: 0.0887, step time: 0.1163\n",
      "130/223, train_loss: 0.0922, step time: 0.0993\n",
      "131/223, train_loss: 0.0924, step time: 0.1288\n",
      "132/223, train_loss: 0.0877, step time: 0.0998\n",
      "133/223, train_loss: 0.1060, step time: 0.1052\n",
      "134/223, train_loss: 0.0978, step time: 0.1145\n",
      "135/223, train_loss: 0.0961, step time: 0.1103\n",
      "136/223, train_loss: 0.0886, step time: 0.1046\n",
      "137/223, train_loss: 0.1015, step time: 0.1142\n",
      "138/223, train_loss: 0.1038, step time: 0.1053\n",
      "139/223, train_loss: 0.0985, step time: 0.1200\n",
      "140/223, train_loss: 0.0974, step time: 0.1293\n",
      "141/223, train_loss: 0.0981, step time: 0.1145\n",
      "142/223, train_loss: 0.1118, step time: 0.1285\n",
      "143/223, train_loss: 0.1004, step time: 0.1258\n",
      "144/223, train_loss: 0.1025, step time: 0.1079\n",
      "145/223, train_loss: 0.1226, step time: 0.0989\n",
      "146/223, train_loss: 0.1136, step time: 0.1105\n",
      "147/223, train_loss: 0.1025, step time: 0.1123\n",
      "148/223, train_loss: 0.1026, step time: 0.1166\n",
      "149/223, train_loss: 0.0941, step time: 0.1182\n",
      "150/223, train_loss: 0.0973, step time: 0.1012\n",
      "151/223, train_loss: 0.0928, step time: 0.1012\n",
      "152/223, train_loss: 0.0922, step time: 0.1159\n",
      "153/223, train_loss: 0.1085, step time: 0.1102\n",
      "154/223, train_loss: 0.0922, step time: 0.1094\n",
      "155/223, train_loss: 0.1069, step time: 0.1106\n",
      "156/223, train_loss: 0.0984, step time: 0.1237\n",
      "157/223, train_loss: 0.0975, step time: 0.1036\n",
      "158/223, train_loss: 0.1020, step time: 0.1004\n",
      "159/223, train_loss: 0.0955, step time: 0.1171\n",
      "160/223, train_loss: 0.1070, step time: 0.1000\n",
      "161/223, train_loss: 0.1014, step time: 0.1044\n",
      "162/223, train_loss: 0.0961, step time: 0.1094\n",
      "163/223, train_loss: 0.1019, step time: 0.1077\n",
      "164/223, train_loss: 0.0941, step time: 0.0998\n",
      "165/223, train_loss: 0.1034, step time: 0.1460\n",
      "166/223, train_loss: 0.1055, step time: 0.1197\n",
      "167/223, train_loss: 0.1038, step time: 0.1088\n",
      "168/223, train_loss: 0.0968, step time: 0.1001\n",
      "169/223, train_loss: 0.0982, step time: 0.1003\n",
      "170/223, train_loss: 0.0902, step time: 0.1007\n",
      "171/223, train_loss: 0.0929, step time: 0.1018\n",
      "172/223, train_loss: 0.1008, step time: 0.1030\n",
      "173/223, train_loss: 0.0881, step time: 0.1032\n",
      "174/223, train_loss: 0.1008, step time: 0.0998\n",
      "175/223, train_loss: 0.1002, step time: 0.1083\n",
      "176/223, train_loss: 0.0980, step time: 0.1009\n",
      "177/223, train_loss: 0.0896, step time: 0.1094\n",
      "178/223, train_loss: 0.0940, step time: 0.1267\n",
      "179/223, train_loss: 0.1021, step time: 0.1004\n",
      "180/223, train_loss: 0.1061, step time: 0.1001\n",
      "181/223, train_loss: 0.1016, step time: 0.1148\n",
      "182/223, train_loss: 0.1097, step time: 0.1001\n",
      "183/223, train_loss: 0.1057, step time: 0.1001\n",
      "184/223, train_loss: 0.0869, step time: 0.1084\n",
      "185/223, train_loss: 0.0939, step time: 0.0997\n",
      "186/223, train_loss: 0.0919, step time: 0.0994\n",
      "187/223, train_loss: 0.0996, step time: 0.1007\n",
      "188/223, train_loss: 0.0997, step time: 0.1119\n",
      "189/223, train_loss: 0.1071, step time: 0.1122\n",
      "190/223, train_loss: 0.0929, step time: 0.1086\n",
      "191/223, train_loss: 0.1040, step time: 0.1520\n",
      "192/223, train_loss: 0.1078, step time: 0.1252\n",
      "193/223, train_loss: 0.1050, step time: 0.1006\n",
      "194/223, train_loss: 0.0939, step time: 0.1162\n",
      "195/223, train_loss: 0.1026, step time: 0.1151\n",
      "196/223, train_loss: 0.1034, step time: 0.1137\n",
      "197/223, train_loss: 0.0993, step time: 0.1025\n",
      "198/223, train_loss: 0.1034, step time: 0.0998\n",
      "199/223, train_loss: 0.0872, step time: 0.1009\n",
      "200/223, train_loss: 0.0998, step time: 0.1082\n",
      "201/223, train_loss: 0.0987, step time: 0.1339\n",
      "202/223, train_loss: 0.0932, step time: 0.1027\n",
      "203/223, train_loss: 0.1042, step time: 0.1465\n",
      "204/223, train_loss: 0.1139, step time: 0.1156\n",
      "205/223, train_loss: 0.0995, step time: 0.1079\n",
      "206/223, train_loss: 0.0876, step time: 0.1070\n",
      "207/223, train_loss: 0.1010, step time: 0.1133\n",
      "208/223, train_loss: 0.0942, step time: 0.1037\n",
      "209/223, train_loss: 0.0970, step time: 0.1004\n",
      "210/223, train_loss: 0.0943, step time: 0.1036\n",
      "211/223, train_loss: 0.0983, step time: 0.1209\n",
      "212/223, train_loss: 0.0961, step time: 0.1267\n",
      "213/223, train_loss: 0.0952, step time: 0.1001\n",
      "214/223, train_loss: 0.0874, step time: 0.1004\n",
      "215/223, train_loss: 0.0926, step time: 0.1023\n",
      "216/223, train_loss: 0.1080, step time: 0.1046\n",
      "217/223, train_loss: 0.0935, step time: 0.1005\n",
      "218/223, train_loss: 0.0987, step time: 0.1007\n",
      "219/223, train_loss: 0.0980, step time: 0.1072\n",
      "220/223, train_loss: 0.0997, step time: 0.1042\n",
      "221/223, train_loss: 0.0920, step time: 0.0995\n",
      "222/223, train_loss: 0.0976, step time: 0.1003\n",
      "223/223, train_loss: 0.0912, step time: 0.1007\n",
      "epoch 252 average loss: 0.1000\n",
      "time consuming of epoch 252 is: 93.7682\n",
      "----------\n",
      "epoch 253/300\n",
      "1/223, train_loss: 0.0977, step time: 0.1017\n",
      "2/223, train_loss: 0.1010, step time: 0.1004\n",
      "3/223, train_loss: 0.1008, step time: 0.0998\n",
      "4/223, train_loss: 0.0998, step time: 0.1019\n",
      "5/223, train_loss: 0.0894, step time: 0.1363\n",
      "6/223, train_loss: 0.0936, step time: 0.1165\n",
      "7/223, train_loss: 0.1130, step time: 0.1070\n",
      "8/223, train_loss: 0.1041, step time: 0.0999\n",
      "9/223, train_loss: 0.1031, step time: 0.1047\n",
      "10/223, train_loss: 0.0853, step time: 0.1114\n",
      "11/223, train_loss: 0.0993, step time: 0.1031\n",
      "12/223, train_loss: 0.0989, step time: 0.1170\n",
      "13/223, train_loss: 0.0992, step time: 0.1023\n",
      "14/223, train_loss: 0.0915, step time: 0.1108\n",
      "15/223, train_loss: 0.0990, step time: 0.1100\n",
      "16/223, train_loss: 0.1086, step time: 0.1052\n",
      "17/223, train_loss: 0.1155, step time: 0.1053\n",
      "18/223, train_loss: 0.1139, step time: 0.1010\n",
      "19/223, train_loss: 0.0969, step time: 0.1102\n",
      "20/223, train_loss: 0.1010, step time: 0.1003\n",
      "21/223, train_loss: 0.1119, step time: 0.1081\n",
      "22/223, train_loss: 0.0855, step time: 0.1277\n",
      "23/223, train_loss: 0.0921, step time: 0.1142\n",
      "24/223, train_loss: 0.0997, step time: 0.1015\n",
      "25/223, train_loss: 0.0978, step time: 0.1019\n",
      "26/223, train_loss: 0.0943, step time: 0.1013\n",
      "27/223, train_loss: 0.1056, step time: 0.1018\n",
      "28/223, train_loss: 0.1035, step time: 0.1115\n",
      "29/223, train_loss: 0.0861, step time: 0.0998\n",
      "30/223, train_loss: 0.1116, step time: 0.1003\n",
      "31/223, train_loss: 0.0996, step time: 0.1272\n",
      "32/223, train_loss: 0.0945, step time: 0.1000\n",
      "33/223, train_loss: 0.1069, step time: 0.1003\n",
      "34/223, train_loss: 0.0897, step time: 0.1000\n",
      "35/223, train_loss: 0.1039, step time: 0.1191\n",
      "36/223, train_loss: 0.0911, step time: 0.1010\n",
      "37/223, train_loss: 0.1008, step time: 0.1003\n",
      "38/223, train_loss: 0.1027, step time: 0.0996\n",
      "39/223, train_loss: 0.0986, step time: 0.1205\n",
      "40/223, train_loss: 0.1101, step time: 0.1013\n",
      "41/223, train_loss: 0.0983, step time: 0.0994\n",
      "42/223, train_loss: 0.1032, step time: 0.1000\n",
      "43/223, train_loss: 0.0948, step time: 0.1006\n",
      "44/223, train_loss: 0.0965, step time: 0.0997\n",
      "45/223, train_loss: 0.0960, step time: 0.1025\n",
      "46/223, train_loss: 0.1115, step time: 0.1129\n",
      "47/223, train_loss: 0.1051, step time: 0.1285\n",
      "48/223, train_loss: 0.0979, step time: 0.1255\n",
      "49/223, train_loss: 0.0895, step time: 0.1118\n",
      "50/223, train_loss: 0.0904, step time: 0.1091\n",
      "51/223, train_loss: 0.0973, step time: 0.1114\n",
      "52/223, train_loss: 0.0922, step time: 0.1336\n",
      "53/223, train_loss: 0.0986, step time: 0.1123\n",
      "54/223, train_loss: 0.1065, step time: 0.1027\n",
      "55/223, train_loss: 0.1079, step time: 0.1285\n",
      "56/223, train_loss: 0.1011, step time: 0.1104\n",
      "57/223, train_loss: 0.1026, step time: 0.1117\n",
      "58/223, train_loss: 0.0885, step time: 0.1128\n",
      "59/223, train_loss: 0.1006, step time: 0.1115\n",
      "60/223, train_loss: 0.1062, step time: 0.1018\n",
      "61/223, train_loss: 0.1068, step time: 0.1165\n",
      "62/223, train_loss: 0.0947, step time: 0.1251\n",
      "63/223, train_loss: 0.1002, step time: 0.1118\n",
      "64/223, train_loss: 0.1151, step time: 0.1195\n",
      "65/223, train_loss: 0.1033, step time: 0.1114\n",
      "66/223, train_loss: 0.0973, step time: 0.1009\n",
      "67/223, train_loss: 0.0944, step time: 0.1003\n",
      "68/223, train_loss: 0.0923, step time: 0.1097\n",
      "69/223, train_loss: 0.1056, step time: 0.1239\n",
      "70/223, train_loss: 0.1005, step time: 0.1217\n",
      "71/223, train_loss: 0.1062, step time: 0.1372\n",
      "72/223, train_loss: 0.1121, step time: 0.1128\n",
      "73/223, train_loss: 0.0990, step time: 0.1276\n",
      "74/223, train_loss: 0.0937, step time: 0.1068\n",
      "75/223, train_loss: 0.0964, step time: 0.1174\n",
      "76/223, train_loss: 0.1021, step time: 0.1378\n",
      "77/223, train_loss: 0.0925, step time: 0.1009\n",
      "78/223, train_loss: 0.1032, step time: 0.1066\n",
      "79/223, train_loss: 0.1010, step time: 0.1236\n",
      "80/223, train_loss: 0.1021, step time: 0.1071\n",
      "81/223, train_loss: 0.1049, step time: 0.1342\n",
      "82/223, train_loss: 0.1102, step time: 0.1296\n",
      "83/223, train_loss: 0.0904, step time: 0.1456\n",
      "84/223, train_loss: 0.1076, step time: 0.1001\n",
      "85/223, train_loss: 0.0933, step time: 0.1102\n",
      "86/223, train_loss: 0.0935, step time: 0.1125\n",
      "87/223, train_loss: 0.0939, step time: 0.1244\n",
      "88/223, train_loss: 0.0938, step time: 0.1049\n",
      "89/223, train_loss: 0.0900, step time: 0.1173\n",
      "90/223, train_loss: 0.0909, step time: 0.1303\n",
      "91/223, train_loss: 0.1106, step time: 0.1125\n",
      "92/223, train_loss: 0.0958, step time: 0.0999\n",
      "93/223, train_loss: 0.0912, step time: 0.1196\n",
      "94/223, train_loss: 0.0880, step time: 0.1248\n",
      "95/223, train_loss: 0.0888, step time: 0.1004\n",
      "96/223, train_loss: 0.0937, step time: 0.1238\n",
      "97/223, train_loss: 0.0968, step time: 0.1057\n",
      "98/223, train_loss: 0.1009, step time: 0.1196\n",
      "99/223, train_loss: 0.0837, step time: 0.1022\n",
      "100/223, train_loss: 0.0908, step time: 0.1134\n",
      "101/223, train_loss: 0.0983, step time: 0.1001\n",
      "102/223, train_loss: 0.1028, step time: 0.0998\n",
      "103/223, train_loss: 0.1077, step time: 0.0993\n",
      "104/223, train_loss: 0.0888, step time: 0.1228\n",
      "105/223, train_loss: 0.1061, step time: 0.0989\n",
      "106/223, train_loss: 0.1060, step time: 0.0999\n",
      "107/223, train_loss: 0.0945, step time: 0.0998\n",
      "108/223, train_loss: 0.0981, step time: 0.1008\n",
      "109/223, train_loss: 0.0931, step time: 0.0995\n",
      "110/223, train_loss: 0.1137, step time: 0.0997\n",
      "111/223, train_loss: 0.1094, step time: 0.0996\n",
      "112/223, train_loss: 0.1018, step time: 0.1043\n",
      "113/223, train_loss: 0.1104, step time: 0.0997\n",
      "114/223, train_loss: 0.1099, step time: 0.0993\n",
      "115/223, train_loss: 0.1006, step time: 0.0993\n",
      "116/223, train_loss: 0.1028, step time: 0.1013\n",
      "117/223, train_loss: 0.0982, step time: 0.1001\n",
      "118/223, train_loss: 0.1065, step time: 0.1006\n",
      "119/223, train_loss: 0.0839, step time: 0.0994\n",
      "120/223, train_loss: 0.1004, step time: 0.0994\n",
      "121/223, train_loss: 0.0873, step time: 0.1099\n",
      "122/223, train_loss: 0.0923, step time: 0.1007\n",
      "123/223, train_loss: 0.0927, step time: 0.0997\n",
      "124/223, train_loss: 0.1020, step time: 0.1009\n",
      "125/223, train_loss: 0.0967, step time: 0.1217\n",
      "126/223, train_loss: 0.1038, step time: 0.1196\n",
      "127/223, train_loss: 0.0942, step time: 0.1468\n",
      "128/223, train_loss: 0.1029, step time: 0.1092\n",
      "129/223, train_loss: 0.1009, step time: 0.1110\n",
      "130/223, train_loss: 0.1014, step time: 0.1008\n",
      "131/223, train_loss: 0.1049, step time: 0.1003\n",
      "132/223, train_loss: 0.1102, step time: 0.1019\n",
      "133/223, train_loss: 0.1141, step time: 0.1121\n",
      "134/223, train_loss: 0.1029, step time: 0.1199\n",
      "135/223, train_loss: 0.0920, step time: 0.1200\n",
      "136/223, train_loss: 0.0976, step time: 0.1056\n",
      "137/223, train_loss: 0.0994, step time: 0.1296\n",
      "138/223, train_loss: 0.1022, step time: 0.1032\n",
      "139/223, train_loss: 0.1046, step time: 0.1246\n",
      "140/223, train_loss: 0.1005, step time: 0.1117\n",
      "141/223, train_loss: 0.1006, step time: 0.1195\n",
      "142/223, train_loss: 0.0977, step time: 0.1067\n",
      "143/223, train_loss: 0.0925, step time: 0.1124\n",
      "144/223, train_loss: 0.0993, step time: 0.1203\n",
      "145/223, train_loss: 0.0924, step time: 0.1133\n",
      "146/223, train_loss: 0.1089, step time: 0.1105\n",
      "147/223, train_loss: 0.1008, step time: 0.1229\n",
      "148/223, train_loss: 0.0875, step time: 0.1095\n",
      "149/223, train_loss: 0.1011, step time: 0.1186\n",
      "150/223, train_loss: 0.0980, step time: 0.1158\n",
      "151/223, train_loss: 0.0980, step time: 0.0994\n",
      "152/223, train_loss: 0.0951, step time: 0.1003\n",
      "153/223, train_loss: 0.1120, step time: 0.1178\n",
      "154/223, train_loss: 0.0882, step time: 0.1169\n",
      "155/223, train_loss: 0.1059, step time: 0.1151\n",
      "156/223, train_loss: 0.0899, step time: 0.1082\n",
      "157/223, train_loss: 0.0981, step time: 0.1149\n",
      "158/223, train_loss: 0.1017, step time: 0.1114\n",
      "159/223, train_loss: 0.0931, step time: 0.1196\n",
      "160/223, train_loss: 0.0870, step time: 0.1078\n",
      "161/223, train_loss: 0.0884, step time: 0.1167\n",
      "162/223, train_loss: 0.0974, step time: 0.1086\n",
      "163/223, train_loss: 0.0951, step time: 0.1068\n",
      "164/223, train_loss: 0.1114, step time: 0.1427\n",
      "165/223, train_loss: 0.3009, step time: 0.1019\n",
      "166/223, train_loss: 0.0968, step time: 0.0991\n",
      "167/223, train_loss: 0.1025, step time: 0.1015\n",
      "168/223, train_loss: 0.0874, step time: 0.1006\n",
      "169/223, train_loss: 0.0994, step time: 0.0997\n",
      "170/223, train_loss: 0.1044, step time: 0.0992\n",
      "171/223, train_loss: 0.1013, step time: 0.1008\n",
      "172/223, train_loss: 0.0948, step time: 0.1083\n",
      "173/223, train_loss: 0.0978, step time: 0.1007\n",
      "174/223, train_loss: 0.1060, step time: 0.0998\n",
      "175/223, train_loss: 0.0923, step time: 0.1006\n",
      "176/223, train_loss: 0.1014, step time: 0.1002\n",
      "177/223, train_loss: 0.0913, step time: 0.0997\n",
      "178/223, train_loss: 0.1084, step time: 0.0993\n",
      "179/223, train_loss: 0.0924, step time: 0.1009\n",
      "180/223, train_loss: 0.0932, step time: 0.1034\n",
      "181/223, train_loss: 0.1003, step time: 0.1001\n",
      "182/223, train_loss: 0.0943, step time: 0.1003\n",
      "183/223, train_loss: 0.0943, step time: 0.0995\n",
      "184/223, train_loss: 0.1031, step time: 0.1002\n",
      "185/223, train_loss: 0.1108, step time: 0.1134\n",
      "186/223, train_loss: 0.1089, step time: 0.1124\n",
      "187/223, train_loss: 0.0973, step time: 0.1590\n",
      "188/223, train_loss: 0.0893, step time: 0.1005\n",
      "189/223, train_loss: 0.1026, step time: 0.1121\n",
      "190/223, train_loss: 0.0930, step time: 0.1165\n",
      "191/223, train_loss: 0.0983, step time: 0.1000\n",
      "192/223, train_loss: 0.0957, step time: 0.1012\n",
      "193/223, train_loss: 0.1177, step time: 0.1022\n",
      "194/223, train_loss: 0.0981, step time: 0.1045\n",
      "195/223, train_loss: 0.1007, step time: 0.1189\n",
      "196/223, train_loss: 0.0987, step time: 0.1147\n",
      "197/223, train_loss: 0.0955, step time: 0.1119\n",
      "198/223, train_loss: 0.0870, step time: 0.1025\n",
      "199/223, train_loss: 0.0985, step time: 0.1007\n",
      "200/223, train_loss: 0.0979, step time: 0.1075\n",
      "201/223, train_loss: 0.0996, step time: 0.1190\n",
      "202/223, train_loss: 0.0983, step time: 0.1210\n",
      "203/223, train_loss: 0.0889, step time: 0.1109\n",
      "204/223, train_loss: 0.1000, step time: 0.1131\n",
      "205/223, train_loss: 0.0938, step time: 0.1189\n",
      "206/223, train_loss: 0.0914, step time: 0.1188\n",
      "207/223, train_loss: 0.0953, step time: 0.1140\n",
      "208/223, train_loss: 0.0885, step time: 0.1151\n",
      "209/223, train_loss: 0.1033, step time: 0.1190\n",
      "210/223, train_loss: 0.1022, step time: 0.1165\n",
      "211/223, train_loss: 0.1016, step time: 0.1232\n",
      "212/223, train_loss: 0.0933, step time: 0.1056\n",
      "213/223, train_loss: 0.0934, step time: 0.1127\n",
      "214/223, train_loss: 0.0945, step time: 0.1064\n",
      "215/223, train_loss: 0.0976, step time: 0.1141\n",
      "216/223, train_loss: 0.0950, step time: 0.1030\n",
      "217/223, train_loss: 0.0985, step time: 0.1028\n",
      "218/223, train_loss: 0.0937, step time: 0.1001\n",
      "219/223, train_loss: 0.1048, step time: 0.0986\n",
      "220/223, train_loss: 0.0952, step time: 0.1071\n",
      "221/223, train_loss: 0.0986, step time: 0.1000\n",
      "222/223, train_loss: 0.1068, step time: 0.0997\n",
      "223/223, train_loss: 0.1051, step time: 0.0995\n",
      "epoch 253 average loss: 0.0999\n",
      "time consuming of epoch 253 is: 101.9031\n",
      "----------\n",
      "epoch 254/300\n",
      "1/223, train_loss: 0.0902, step time: 0.1022\n",
      "2/223, train_loss: 0.0919, step time: 0.1031\n",
      "3/223, train_loss: 0.0979, step time: 0.1281\n",
      "4/223, train_loss: 0.1138, step time: 0.1210\n",
      "5/223, train_loss: 0.1041, step time: 0.1012\n",
      "6/223, train_loss: 0.0986, step time: 0.0998\n",
      "7/223, train_loss: 0.0897, step time: 0.1143\n",
      "8/223, train_loss: 0.1080, step time: 0.1074\n",
      "9/223, train_loss: 0.0958, step time: 0.1053\n",
      "10/223, train_loss: 0.1023, step time: 0.1293\n",
      "11/223, train_loss: 0.0995, step time: 0.1111\n",
      "12/223, train_loss: 0.1003, step time: 0.1142\n",
      "13/223, train_loss: 0.0960, step time: 0.1136\n",
      "14/223, train_loss: 0.0966, step time: 0.1257\n",
      "15/223, train_loss: 0.1103, step time: 0.1053\n",
      "16/223, train_loss: 0.1101, step time: 0.1053\n",
      "17/223, train_loss: 0.1039, step time: 0.1049\n",
      "18/223, train_loss: 0.0905, step time: 0.1002\n",
      "19/223, train_loss: 0.1018, step time: 0.1069\n",
      "20/223, train_loss: 0.0886, step time: 0.1139\n",
      "21/223, train_loss: 0.0930, step time: 0.1042\n",
      "22/223, train_loss: 0.1019, step time: 0.1000\n",
      "23/223, train_loss: 0.1078, step time: 0.1229\n",
      "24/223, train_loss: 0.0984, step time: 0.1013\n",
      "25/223, train_loss: 0.0874, step time: 0.1014\n",
      "26/223, train_loss: 0.0918, step time: 0.1002\n",
      "27/223, train_loss: 0.0976, step time: 0.1114\n",
      "28/223, train_loss: 0.1041, step time: 0.1000\n",
      "29/223, train_loss: 0.1105, step time: 0.1314\n",
      "30/223, train_loss: 0.0912, step time: 0.1225\n",
      "31/223, train_loss: 0.0908, step time: 0.1185\n",
      "32/223, train_loss: 0.0964, step time: 0.1071\n",
      "33/223, train_loss: 0.1047, step time: 0.1057\n",
      "34/223, train_loss: 0.0961, step time: 0.1111\n",
      "35/223, train_loss: 0.1001, step time: 0.1144\n",
      "36/223, train_loss: 0.0996, step time: 0.1058\n",
      "37/223, train_loss: 0.0928, step time: 0.1222\n",
      "38/223, train_loss: 0.1051, step time: 0.1054\n",
      "39/223, train_loss: 0.0938, step time: 0.1121\n",
      "40/223, train_loss: 0.0924, step time: 0.1013\n",
      "41/223, train_loss: 0.1017, step time: 0.1111\n",
      "42/223, train_loss: 0.0929, step time: 0.1006\n",
      "43/223, train_loss: 0.0989, step time: 0.1005\n",
      "44/223, train_loss: 0.0940, step time: 0.1355\n",
      "45/223, train_loss: 0.1068, step time: 0.1064\n",
      "46/223, train_loss: 0.1054, step time: 0.1003\n",
      "47/223, train_loss: 0.0866, step time: 0.1005\n",
      "48/223, train_loss: 0.0888, step time: 0.1010\n",
      "49/223, train_loss: 0.0908, step time: 0.1147\n",
      "50/223, train_loss: 0.0953, step time: 0.1076\n",
      "51/223, train_loss: 0.1067, step time: 0.1018\n",
      "52/223, train_loss: 0.1083, step time: 0.1664\n",
      "53/223, train_loss: 0.1060, step time: 0.1212\n",
      "54/223, train_loss: 0.1072, step time: 0.1144\n",
      "55/223, train_loss: 0.1002, step time: 0.1112\n",
      "56/223, train_loss: 0.1045, step time: 0.1103\n",
      "57/223, train_loss: 0.0983, step time: 0.1186\n",
      "58/223, train_loss: 0.0975, step time: 0.1158\n",
      "59/223, train_loss: 0.0968, step time: 0.1230\n",
      "60/223, train_loss: 0.0897, step time: 0.1020\n",
      "61/223, train_loss: 0.0868, step time: 0.1646\n",
      "62/223, train_loss: 0.0953, step time: 0.1287\n",
      "63/223, train_loss: 0.0984, step time: 0.1139\n",
      "64/223, train_loss: 0.0968, step time: 0.1092\n",
      "65/223, train_loss: 0.2971, step time: 0.1024\n",
      "66/223, train_loss: 0.0898, step time: 0.1069\n",
      "67/223, train_loss: 0.0941, step time: 0.1102\n",
      "68/223, train_loss: 0.0998, step time: 0.1072\n",
      "69/223, train_loss: 0.1085, step time: 0.1055\n",
      "70/223, train_loss: 0.0981, step time: 0.1025\n",
      "71/223, train_loss: 0.1065, step time: 0.1049\n",
      "72/223, train_loss: 0.1048, step time: 0.0998\n",
      "73/223, train_loss: 0.1050, step time: 0.1090\n",
      "74/223, train_loss: 0.1156, step time: 0.1264\n",
      "75/223, train_loss: 0.0916, step time: 0.0996\n",
      "76/223, train_loss: 0.1042, step time: 0.1091\n",
      "77/223, train_loss: 0.1099, step time: 0.1124\n",
      "78/223, train_loss: 0.0956, step time: 0.1183\n",
      "79/223, train_loss: 0.0972, step time: 0.1191\n",
      "80/223, train_loss: 0.1039, step time: 0.1127\n",
      "81/223, train_loss: 0.0987, step time: 0.1143\n",
      "82/223, train_loss: 0.1040, step time: 0.1140\n",
      "83/223, train_loss: 0.1030, step time: 0.1195\n",
      "84/223, train_loss: 0.0966, step time: 0.1224\n",
      "85/223, train_loss: 0.1043, step time: 0.1087\n",
      "86/223, train_loss: 0.0989, step time: 0.1115\n",
      "87/223, train_loss: 0.1019, step time: 0.1180\n",
      "88/223, train_loss: 0.0992, step time: 0.1288\n",
      "89/223, train_loss: 0.0955, step time: 0.1100\n",
      "90/223, train_loss: 0.1020, step time: 0.1341\n",
      "91/223, train_loss: 0.0987, step time: 0.1329\n",
      "92/223, train_loss: 0.1073, step time: 0.1432\n",
      "93/223, train_loss: 0.0938, step time: 0.1104\n",
      "94/223, train_loss: 0.1111, step time: 0.1208\n",
      "95/223, train_loss: 0.1082, step time: 0.1127\n",
      "96/223, train_loss: 0.0872, step time: 0.1152\n",
      "97/223, train_loss: 0.1031, step time: 0.1141\n",
      "98/223, train_loss: 0.1023, step time: 0.1022\n",
      "99/223, train_loss: 0.0986, step time: 0.1280\n",
      "100/223, train_loss: 0.1089, step time: 0.1099\n",
      "101/223, train_loss: 0.1028, step time: 0.1188\n",
      "102/223, train_loss: 0.1027, step time: 0.1036\n",
      "103/223, train_loss: 0.0919, step time: 0.1114\n",
      "104/223, train_loss: 0.1049, step time: 0.1138\n",
      "105/223, train_loss: 0.1144, step time: 0.1330\n",
      "106/223, train_loss: 0.1063, step time: 0.1364\n",
      "107/223, train_loss: 0.0961, step time: 0.1551\n",
      "108/223, train_loss: 0.0884, step time: 0.1076\n",
      "109/223, train_loss: 0.0979, step time: 0.1052\n",
      "110/223, train_loss: 0.1008, step time: 0.1083\n",
      "111/223, train_loss: 0.1024, step time: 0.1055\n",
      "112/223, train_loss: 0.1051, step time: 0.1500\n",
      "113/223, train_loss: 0.0958, step time: 0.0997\n",
      "114/223, train_loss: 0.0951, step time: 0.0996\n",
      "115/223, train_loss: 0.0883, step time: 0.1004\n",
      "116/223, train_loss: 0.0926, step time: 0.1030\n",
      "117/223, train_loss: 0.1017, step time: 0.0998\n",
      "118/223, train_loss: 0.1033, step time: 0.0991\n",
      "119/223, train_loss: 0.0951, step time: 0.0995\n",
      "120/223, train_loss: 0.0842, step time: 0.1074\n",
      "121/223, train_loss: 0.0990, step time: 0.1127\n",
      "122/223, train_loss: 0.1007, step time: 0.1129\n",
      "123/223, train_loss: 0.0946, step time: 0.0992\n",
      "124/223, train_loss: 0.1019, step time: 0.1073\n",
      "125/223, train_loss: 0.1042, step time: 0.1039\n",
      "126/223, train_loss: 0.1113, step time: 0.1080\n",
      "127/223, train_loss: 0.0903, step time: 0.1336\n",
      "128/223, train_loss: 0.0869, step time: 0.1046\n",
      "129/223, train_loss: 0.1030, step time: 0.1000\n",
      "130/223, train_loss: 0.0983, step time: 0.1008\n",
      "131/223, train_loss: 0.1052, step time: 0.1092\n",
      "132/223, train_loss: 0.0953, step time: 0.1258\n",
      "133/223, train_loss: 0.0907, step time: 0.1510\n",
      "134/223, train_loss: 0.1047, step time: 0.1256\n",
      "135/223, train_loss: 0.0965, step time: 0.1042\n",
      "136/223, train_loss: 0.0980, step time: 0.1034\n",
      "137/223, train_loss: 0.1049, step time: 0.1172\n",
      "138/223, train_loss: 0.0966, step time: 0.1105\n",
      "139/223, train_loss: 0.0969, step time: 0.1310\n",
      "140/223, train_loss: 0.1067, step time: 0.1150\n",
      "141/223, train_loss: 0.0955, step time: 0.1038\n",
      "142/223, train_loss: 0.1084, step time: 0.1004\n",
      "143/223, train_loss: 0.1173, step time: 0.1468\n",
      "144/223, train_loss: 0.1046, step time: 0.1309\n",
      "145/223, train_loss: 0.0937, step time: 0.0992\n",
      "146/223, train_loss: 0.0922, step time: 0.1007\n",
      "147/223, train_loss: 0.0969, step time: 0.1174\n",
      "148/223, train_loss: 0.0974, step time: 0.1076\n",
      "149/223, train_loss: 0.1070, step time: 0.1047\n",
      "150/223, train_loss: 0.1017, step time: 0.1034\n",
      "151/223, train_loss: 0.0914, step time: 0.1215\n",
      "152/223, train_loss: 0.1052, step time: 0.1305\n",
      "153/223, train_loss: 0.0977, step time: 0.1080\n",
      "154/223, train_loss: 0.0992, step time: 0.1216\n",
      "155/223, train_loss: 0.1024, step time: 0.1089\n",
      "156/223, train_loss: 0.0912, step time: 0.1395\n",
      "157/223, train_loss: 0.1076, step time: 0.1023\n",
      "158/223, train_loss: 0.0932, step time: 0.1134\n",
      "159/223, train_loss: 0.0954, step time: 0.1258\n",
      "160/223, train_loss: 0.0931, step time: 0.1105\n",
      "161/223, train_loss: 0.1036, step time: 0.1076\n",
      "162/223, train_loss: 0.0925, step time: 0.1260\n",
      "163/223, train_loss: 0.0925, step time: 0.1190\n",
      "164/223, train_loss: 0.1164, step time: 0.1064\n",
      "165/223, train_loss: 0.0972, step time: 0.1029\n",
      "166/223, train_loss: 0.0978, step time: 0.1004\n",
      "167/223, train_loss: 0.0914, step time: 0.1189\n",
      "168/223, train_loss: 0.1020, step time: 0.1109\n",
      "169/223, train_loss: 0.1078, step time: 0.1294\n",
      "170/223, train_loss: 0.1011, step time: 0.1176\n",
      "171/223, train_loss: 0.0912, step time: 0.1113\n",
      "172/223, train_loss: 0.1007, step time: 0.1070\n",
      "173/223, train_loss: 0.1042, step time: 0.1407\n",
      "174/223, train_loss: 0.0987, step time: 0.1000\n",
      "175/223, train_loss: 0.0866, step time: 0.1133\n",
      "176/223, train_loss: 0.1012, step time: 0.1002\n",
      "177/223, train_loss: 0.0987, step time: 0.1480\n",
      "178/223, train_loss: 0.0930, step time: 0.1167\n",
      "179/223, train_loss: 0.0964, step time: 0.0997\n",
      "180/223, train_loss: 0.0908, step time: 0.1098\n",
      "181/223, train_loss: 0.1025, step time: 0.1217\n",
      "182/223, train_loss: 0.0911, step time: 0.1008\n",
      "183/223, train_loss: 0.1002, step time: 0.1187\n",
      "184/223, train_loss: 0.1004, step time: 0.1097\n",
      "185/223, train_loss: 0.1016, step time: 0.1235\n",
      "186/223, train_loss: 0.1017, step time: 0.1025\n",
      "187/223, train_loss: 0.0973, step time: 0.1004\n",
      "188/223, train_loss: 0.0909, step time: 0.1040\n",
      "189/223, train_loss: 0.1027, step time: 0.1137\n",
      "190/223, train_loss: 0.1105, step time: 0.1128\n",
      "191/223, train_loss: 0.1075, step time: 0.1239\n",
      "192/223, train_loss: 0.0876, step time: 0.1128\n",
      "193/223, train_loss: 0.0993, step time: 0.1359\n",
      "194/223, train_loss: 0.0965, step time: 0.1323\n",
      "195/223, train_loss: 0.0982, step time: 0.1000\n",
      "196/223, train_loss: 0.0953, step time: 0.1077\n",
      "197/223, train_loss: 0.0943, step time: 0.1090\n",
      "198/223, train_loss: 0.0910, step time: 0.1001\n",
      "199/223, train_loss: 0.1029, step time: 0.1009\n",
      "200/223, train_loss: 0.1105, step time: 0.1080\n",
      "201/223, train_loss: 0.1065, step time: 0.1006\n",
      "202/223, train_loss: 0.0907, step time: 0.1009\n",
      "203/223, train_loss: 0.1012, step time: 0.1091\n",
      "204/223, train_loss: 0.1052, step time: 0.1012\n",
      "205/223, train_loss: 0.0925, step time: 0.1214\n",
      "206/223, train_loss: 0.1002, step time: 0.1168\n",
      "207/223, train_loss: 0.1053, step time: 0.1404\n",
      "208/223, train_loss: 0.0906, step time: 0.1298\n",
      "209/223, train_loss: 0.0908, step time: 0.1250\n",
      "210/223, train_loss: 0.0912, step time: 0.1105\n",
      "211/223, train_loss: 0.0987, step time: 0.1004\n",
      "212/223, train_loss: 0.0926, step time: 0.1192\n",
      "213/223, train_loss: 0.0985, step time: 0.1186\n",
      "214/223, train_loss: 0.0850, step time: 0.1000\n",
      "215/223, train_loss: 0.1032, step time: 0.1184\n",
      "216/223, train_loss: 0.0960, step time: 0.1331\n",
      "217/223, train_loss: 0.0979, step time: 0.1007\n",
      "218/223, train_loss: 0.1033, step time: 0.1001\n",
      "219/223, train_loss: 0.0911, step time: 0.1006\n",
      "220/223, train_loss: 0.1036, step time: 0.1003\n",
      "221/223, train_loss: 0.0969, step time: 0.1002\n",
      "222/223, train_loss: 0.1042, step time: 0.1001\n",
      "223/223, train_loss: 0.0913, step time: 0.0992\n",
      "epoch 254 average loss: 0.0999\n",
      "time consuming of epoch 254 is: 90.2435\n",
      "----------\n",
      "epoch 255/300\n",
      "1/223, train_loss: 0.0892, step time: 0.1076\n",
      "2/223, train_loss: 0.0872, step time: 0.1005\n",
      "3/223, train_loss: 0.0939, step time: 0.1014\n",
      "4/223, train_loss: 0.1050, step time: 0.1080\n",
      "5/223, train_loss: 0.1073, step time: 0.1208\n",
      "6/223, train_loss: 0.1049, step time: 0.0999\n",
      "7/223, train_loss: 0.0935, step time: 0.1194\n",
      "8/223, train_loss: 0.0918, step time: 0.1428\n",
      "9/223, train_loss: 0.0973, step time: 0.1503\n",
      "10/223, train_loss: 0.1037, step time: 0.1434\n",
      "11/223, train_loss: 0.0890, step time: 0.1192\n",
      "12/223, train_loss: 0.0956, step time: 0.0999\n",
      "13/223, train_loss: 0.1041, step time: 0.1439\n",
      "14/223, train_loss: 0.1100, step time: 0.1006\n",
      "15/223, train_loss: 0.0927, step time: 0.1497\n",
      "16/223, train_loss: 0.1036, step time: 0.1009\n",
      "17/223, train_loss: 0.0963, step time: 0.1037\n",
      "18/223, train_loss: 0.0975, step time: 0.1347\n",
      "19/223, train_loss: 0.0976, step time: 0.1764\n",
      "20/223, train_loss: 0.1041, step time: 0.1230\n",
      "21/223, train_loss: 0.1031, step time: 0.1100\n",
      "22/223, train_loss: 0.0903, step time: 0.1006\n",
      "23/223, train_loss: 0.0898, step time: 0.1024\n",
      "24/223, train_loss: 0.0975, step time: 0.1159\n",
      "25/223, train_loss: 0.0933, step time: 0.1390\n",
      "26/223, train_loss: 0.0918, step time: 0.1003\n",
      "27/223, train_loss: 0.0953, step time: 0.1090\n",
      "28/223, train_loss: 0.1115, step time: 0.1008\n",
      "29/223, train_loss: 0.1044, step time: 0.1137\n",
      "30/223, train_loss: 0.1036, step time: 0.1059\n",
      "31/223, train_loss: 0.0969, step time: 0.1028\n",
      "32/223, train_loss: 0.0926, step time: 0.1107\n",
      "33/223, train_loss: 0.0960, step time: 0.1153\n",
      "34/223, train_loss: 0.0916, step time: 0.1100\n",
      "35/223, train_loss: 0.0984, step time: 0.1083\n",
      "36/223, train_loss: 0.0954, step time: 0.1143\n",
      "37/223, train_loss: 0.0933, step time: 0.1172\n",
      "38/223, train_loss: 0.1015, step time: 0.1077\n",
      "39/223, train_loss: 0.1062, step time: 0.1191\n",
      "40/223, train_loss: 0.1007, step time: 0.1007\n",
      "41/223, train_loss: 0.0980, step time: 0.1220\n",
      "42/223, train_loss: 0.0996, step time: 0.0996\n",
      "43/223, train_loss: 0.1052, step time: 0.1017\n",
      "44/223, train_loss: 0.0933, step time: 0.1001\n",
      "45/223, train_loss: 0.1023, step time: 0.1038\n",
      "46/223, train_loss: 0.0979, step time: 0.0998\n",
      "47/223, train_loss: 0.0888, step time: 0.0996\n",
      "48/223, train_loss: 0.1048, step time: 0.1040\n",
      "49/223, train_loss: 0.0915, step time: 0.1001\n",
      "50/223, train_loss: 0.0964, step time: 0.1393\n",
      "51/223, train_loss: 0.0931, step time: 0.1008\n",
      "52/223, train_loss: 0.0967, step time: 0.1258\n",
      "53/223, train_loss: 0.0911, step time: 0.1119\n",
      "54/223, train_loss: 0.0996, step time: 0.1214\n",
      "55/223, train_loss: 0.0988, step time: 0.1058\n",
      "56/223, train_loss: 0.0879, step time: 0.1057\n",
      "57/223, train_loss: 0.0947, step time: 0.1231\n",
      "58/223, train_loss: 0.1054, step time: 0.1101\n",
      "59/223, train_loss: 0.0938, step time: 0.1572\n",
      "60/223, train_loss: 0.0970, step time: 0.1376\n",
      "61/223, train_loss: 0.1048, step time: 0.1087\n",
      "62/223, train_loss: 0.1000, step time: 0.1081\n",
      "63/223, train_loss: 0.1004, step time: 0.1517\n",
      "64/223, train_loss: 0.0907, step time: 0.1142\n",
      "65/223, train_loss: 0.1027, step time: 0.1146\n",
      "66/223, train_loss: 0.0885, step time: 0.1258\n",
      "67/223, train_loss: 0.1052, step time: 0.1243\n",
      "68/223, train_loss: 0.0883, step time: 0.1305\n",
      "69/223, train_loss: 0.1225, step time: 0.1067\n",
      "70/223, train_loss: 0.1075, step time: 0.1005\n",
      "71/223, train_loss: 0.0969, step time: 0.1053\n",
      "72/223, train_loss: 0.0929, step time: 0.1043\n",
      "73/223, train_loss: 0.1011, step time: 0.1107\n",
      "74/223, train_loss: 0.1101, step time: 0.1004\n",
      "75/223, train_loss: 0.1117, step time: 0.1009\n",
      "76/223, train_loss: 0.1086, step time: 0.1027\n",
      "77/223, train_loss: 0.1091, step time: 0.1062\n",
      "78/223, train_loss: 0.1026, step time: 0.1140\n",
      "79/223, train_loss: 0.1070, step time: 0.1363\n",
      "80/223, train_loss: 0.0920, step time: 0.1402\n",
      "81/223, train_loss: 0.0999, step time: 0.1002\n",
      "82/223, train_loss: 0.0918, step time: 0.1085\n",
      "83/223, train_loss: 0.1219, step time: 0.1118\n",
      "84/223, train_loss: 0.1086, step time: 0.1070\n",
      "85/223, train_loss: 0.1016, step time: 0.1510\n",
      "86/223, train_loss: 0.0882, step time: 0.1229\n",
      "87/223, train_loss: 0.1097, step time: 0.1168\n",
      "88/223, train_loss: 0.0889, step time: 0.1169\n",
      "89/223, train_loss: 0.0998, step time: 0.1258\n",
      "90/223, train_loss: 0.0916, step time: 0.1042\n",
      "91/223, train_loss: 0.0927, step time: 0.1311\n",
      "92/223, train_loss: 0.0937, step time: 0.1101\n",
      "93/223, train_loss: 0.1048, step time: 0.1222\n",
      "94/223, train_loss: 0.0969, step time: 0.1286\n",
      "95/223, train_loss: 0.0958, step time: 0.1037\n",
      "96/223, train_loss: 0.1004, step time: 0.1057\n",
      "97/223, train_loss: 0.1098, step time: 0.1211\n",
      "98/223, train_loss: 0.1008, step time: 0.1407\n",
      "99/223, train_loss: 0.1102, step time: 0.1210\n",
      "100/223, train_loss: 0.0917, step time: 0.1006\n",
      "101/223, train_loss: 0.0931, step time: 0.1131\n",
      "102/223, train_loss: 0.1031, step time: 0.1161\n",
      "103/223, train_loss: 0.0895, step time: 0.1034\n",
      "104/223, train_loss: 0.0832, step time: 0.1759\n",
      "105/223, train_loss: 0.0973, step time: 0.1061\n",
      "106/223, train_loss: 0.0830, step time: 0.1108\n",
      "107/223, train_loss: 0.1004, step time: 0.1694\n",
      "108/223, train_loss: 0.0900, step time: 0.1118\n",
      "109/223, train_loss: 0.0949, step time: 0.1222\n",
      "110/223, train_loss: 0.0953, step time: 0.1235\n",
      "111/223, train_loss: 0.1034, step time: 0.1773\n",
      "112/223, train_loss: 0.0926, step time: 0.1452\n",
      "113/223, train_loss: 0.0922, step time: 0.1144\n",
      "114/223, train_loss: 0.1153, step time: 0.1278\n",
      "115/223, train_loss: 0.0916, step time: 0.1077\n",
      "116/223, train_loss: 0.1012, step time: 0.1044\n",
      "117/223, train_loss: 0.0959, step time: 0.1001\n",
      "118/223, train_loss: 0.0993, step time: 0.1003\n",
      "119/223, train_loss: 0.1039, step time: 0.1002\n",
      "120/223, train_loss: 0.1045, step time: 0.1022\n",
      "121/223, train_loss: 0.0949, step time: 0.1007\n",
      "122/223, train_loss: 0.0996, step time: 0.1024\n",
      "123/223, train_loss: 0.1050, step time: 0.1010\n",
      "124/223, train_loss: 0.1065, step time: 0.1168\n",
      "125/223, train_loss: 0.1029, step time: 0.1207\n",
      "126/223, train_loss: 0.0906, step time: 0.1268\n",
      "127/223, train_loss: 0.1061, step time: 0.1409\n",
      "128/223, train_loss: 0.0899, step time: 0.1306\n",
      "129/223, train_loss: 0.1056, step time: 0.1001\n",
      "130/223, train_loss: 0.0980, step time: 0.1110\n",
      "131/223, train_loss: 0.1056, step time: 0.1060\n",
      "132/223, train_loss: 0.1018, step time: 0.1207\n",
      "133/223, train_loss: 0.0973, step time: 0.1298\n",
      "134/223, train_loss: 0.0981, step time: 0.1243\n",
      "135/223, train_loss: 0.1040, step time: 0.1058\n",
      "136/223, train_loss: 0.0989, step time: 0.1048\n",
      "137/223, train_loss: 0.1077, step time: 0.1194\n",
      "138/223, train_loss: 0.1076, step time: 0.1145\n",
      "139/223, train_loss: 0.0952, step time: 0.1036\n",
      "140/223, train_loss: 0.0963, step time: 0.1025\n",
      "141/223, train_loss: 0.0942, step time: 0.0997\n",
      "142/223, train_loss: 0.0964, step time: 0.1001\n",
      "143/223, train_loss: 0.1017, step time: 0.1002\n",
      "144/223, train_loss: 0.0975, step time: 0.1367\n",
      "145/223, train_loss: 0.0877, step time: 0.1157\n",
      "146/223, train_loss: 0.0940, step time: 0.1210\n",
      "147/223, train_loss: 0.1038, step time: 0.1235\n",
      "148/223, train_loss: 0.1013, step time: 0.1109\n",
      "149/223, train_loss: 0.0990, step time: 0.1309\n",
      "150/223, train_loss: 0.0983, step time: 0.1012\n",
      "151/223, train_loss: 0.0913, step time: 0.1057\n",
      "152/223, train_loss: 0.0900, step time: 0.1174\n",
      "153/223, train_loss: 0.1052, step time: 0.1017\n",
      "154/223, train_loss: 0.0944, step time: 0.1226\n",
      "155/223, train_loss: 0.1052, step time: 0.1053\n",
      "156/223, train_loss: 0.0951, step time: 0.1008\n",
      "157/223, train_loss: 0.0856, step time: 0.0999\n",
      "158/223, train_loss: 0.1016, step time: 0.1087\n",
      "159/223, train_loss: 0.0931, step time: 0.0995\n",
      "160/223, train_loss: 0.0948, step time: 0.1094\n",
      "161/223, train_loss: 0.1003, step time: 0.1112\n",
      "162/223, train_loss: 0.0951, step time: 0.1078\n",
      "163/223, train_loss: 0.0890, step time: 0.1109\n",
      "164/223, train_loss: 0.0948, step time: 0.1192\n",
      "165/223, train_loss: 0.1028, step time: 0.1014\n",
      "166/223, train_loss: 0.1060, step time: 0.0992\n",
      "167/223, train_loss: 0.1021, step time: 0.1002\n",
      "168/223, train_loss: 0.0950, step time: 0.1030\n",
      "169/223, train_loss: 0.0990, step time: 0.0995\n",
      "170/223, train_loss: 0.1133, step time: 0.1061\n",
      "171/223, train_loss: 0.0957, step time: 0.1042\n",
      "172/223, train_loss: 0.0959, step time: 0.1321\n",
      "173/223, train_loss: 0.1037, step time: 0.1126\n",
      "174/223, train_loss: 0.0945, step time: 0.1196\n",
      "175/223, train_loss: 0.1024, step time: 0.1121\n",
      "176/223, train_loss: 0.0918, step time: 0.1307\n",
      "177/223, train_loss: 0.0996, step time: 0.1102\n",
      "178/223, train_loss: 0.1055, step time: 0.0998\n",
      "179/223, train_loss: 0.0935, step time: 0.1005\n",
      "180/223, train_loss: 0.1025, step time: 0.1264\n",
      "181/223, train_loss: 0.1118, step time: 0.1204\n",
      "182/223, train_loss: 0.1039, step time: 0.1061\n",
      "183/223, train_loss: 0.0973, step time: 0.1180\n",
      "184/223, train_loss: 0.0858, step time: 0.1013\n",
      "185/223, train_loss: 0.0848, step time: 0.1087\n",
      "186/223, train_loss: 0.1019, step time: 0.1035\n",
      "187/223, train_loss: 0.1195, step time: 0.1234\n",
      "188/223, train_loss: 0.1027, step time: 0.1107\n",
      "189/223, train_loss: 0.0954, step time: 0.1168\n",
      "190/223, train_loss: 0.1055, step time: 0.0999\n",
      "191/223, train_loss: 0.1005, step time: 0.1006\n",
      "192/223, train_loss: 0.1070, step time: 0.1045\n",
      "193/223, train_loss: 0.0997, step time: 0.1061\n",
      "194/223, train_loss: 0.0890, step time: 0.1007\n",
      "195/223, train_loss: 0.0884, step time: 0.1007\n",
      "196/223, train_loss: 0.1067, step time: 0.1092\n",
      "197/223, train_loss: 0.0959, step time: 0.0999\n",
      "198/223, train_loss: 0.0963, step time: 0.1007\n",
      "199/223, train_loss: 0.1105, step time: 0.1017\n",
      "200/223, train_loss: 0.1045, step time: 0.1013\n",
      "201/223, train_loss: 0.0958, step time: 0.1127\n",
      "202/223, train_loss: 0.0964, step time: 0.1004\n",
      "203/223, train_loss: 0.1043, step time: 0.1002\n",
      "204/223, train_loss: 0.1087, step time: 0.1346\n",
      "205/223, train_loss: 0.0955, step time: 0.1047\n",
      "206/223, train_loss: 0.1104, step time: 0.1218\n",
      "207/223, train_loss: 0.1050, step time: 0.1030\n",
      "208/223, train_loss: 0.0989, step time: 0.1216\n",
      "209/223, train_loss: 0.0999, step time: 0.1073\n",
      "210/223, train_loss: 0.1045, step time: 0.1096\n",
      "211/223, train_loss: 0.0935, step time: 0.1008\n",
      "212/223, train_loss: 0.0980, step time: 0.1006\n",
      "213/223, train_loss: 0.3024, step time: 0.1169\n",
      "214/223, train_loss: 0.1124, step time: 0.1074\n",
      "215/223, train_loss: 0.1015, step time: 0.1036\n",
      "216/223, train_loss: 0.1021, step time: 0.1230\n",
      "217/223, train_loss: 0.1070, step time: 0.1003\n",
      "218/223, train_loss: 0.0942, step time: 0.1000\n",
      "219/223, train_loss: 0.0993, step time: 0.1003\n",
      "220/223, train_loss: 0.0947, step time: 0.1072\n",
      "221/223, train_loss: 0.0998, step time: 0.0986\n",
      "222/223, train_loss: 0.0976, step time: 0.0993\n",
      "223/223, train_loss: 0.0979, step time: 0.1000\n",
      "epoch 255 average loss: 0.0999\n",
      "saved new best metric model\n",
      "current epoch: 255 current mean dice: 0.8622 tc: 0.9225 wt: 0.8717 et: 0.7923\n",
      "best mean dice: 0.8622 at epoch: 255\n",
      "time consuming of epoch 255 is: 91.5646\n",
      "----------\n",
      "epoch 256/300\n",
      "1/223, train_loss: 0.1026, step time: 0.1016\n",
      "2/223, train_loss: 0.0989, step time: 0.1057\n",
      "3/223, train_loss: 0.1089, step time: 0.1055\n",
      "4/223, train_loss: 0.1130, step time: 0.1000\n",
      "5/223, train_loss: 0.0949, step time: 0.1068\n",
      "6/223, train_loss: 0.0932, step time: 0.1101\n",
      "7/223, train_loss: 0.1035, step time: 0.1013\n",
      "8/223, train_loss: 0.0987, step time: 0.1483\n",
      "9/223, train_loss: 0.1098, step time: 0.1242\n",
      "10/223, train_loss: 0.0967, step time: 0.0994\n",
      "11/223, train_loss: 0.0942, step time: 0.1275\n",
      "12/223, train_loss: 0.1037, step time: 0.1072\n",
      "13/223, train_loss: 0.1009, step time: 0.1000\n",
      "14/223, train_loss: 0.0999, step time: 0.1107\n",
      "15/223, train_loss: 0.1006, step time: 0.1030\n",
      "16/223, train_loss: 0.0995, step time: 0.1003\n",
      "17/223, train_loss: 0.1051, step time: 0.1046\n",
      "18/223, train_loss: 0.0969, step time: 0.1054\n",
      "19/223, train_loss: 0.1008, step time: 0.1170\n",
      "20/223, train_loss: 0.0920, step time: 0.1115\n",
      "21/223, train_loss: 0.0974, step time: 0.1184\n",
      "22/223, train_loss: 0.1103, step time: 0.1276\n",
      "23/223, train_loss: 0.1152, step time: 0.1304\n",
      "24/223, train_loss: 0.1011, step time: 0.1236\n",
      "25/223, train_loss: 0.1109, step time: 0.1232\n",
      "26/223, train_loss: 0.1020, step time: 0.1155\n",
      "27/223, train_loss: 0.1044, step time: 0.1458\n",
      "28/223, train_loss: 0.1092, step time: 0.1124\n",
      "29/223, train_loss: 0.1075, step time: 0.1278\n",
      "30/223, train_loss: 0.0926, step time: 0.1597\n",
      "31/223, train_loss: 0.0982, step time: 0.1140\n",
      "32/223, train_loss: 0.0978, step time: 0.1010\n",
      "33/223, train_loss: 0.0915, step time: 0.1257\n",
      "34/223, train_loss: 0.0940, step time: 0.1128\n",
      "35/223, train_loss: 0.1099, step time: 0.1092\n",
      "36/223, train_loss: 0.0993, step time: 0.1001\n",
      "37/223, train_loss: 0.1125, step time: 0.1072\n",
      "38/223, train_loss: 0.0889, step time: 0.1000\n",
      "39/223, train_loss: 0.1079, step time: 0.1001\n",
      "40/223, train_loss: 0.0935, step time: 0.1010\n",
      "41/223, train_loss: 0.1068, step time: 0.1412\n",
      "42/223, train_loss: 0.2954, step time: 0.1162\n",
      "43/223, train_loss: 0.0998, step time: 0.1406\n",
      "44/223, train_loss: 0.0905, step time: 0.1360\n",
      "45/223, train_loss: 0.0989, step time: 0.1079\n",
      "46/223, train_loss: 0.0956, step time: 0.1008\n",
      "47/223, train_loss: 0.0950, step time: 0.1001\n",
      "48/223, train_loss: 0.1070, step time: 0.1083\n",
      "49/223, train_loss: 0.0913, step time: 0.1115\n",
      "50/223, train_loss: 0.1010, step time: 0.0998\n",
      "51/223, train_loss: 0.0961, step time: 0.0997\n",
      "52/223, train_loss: 0.0971, step time: 0.1122\n",
      "53/223, train_loss: 0.1070, step time: 0.1048\n",
      "54/223, train_loss: 0.0908, step time: 0.1000\n",
      "55/223, train_loss: 0.0898, step time: 0.1020\n",
      "56/223, train_loss: 0.0945, step time: 0.1001\n",
      "57/223, train_loss: 0.0860, step time: 0.1392\n",
      "58/223, train_loss: 0.0918, step time: 0.1025\n",
      "59/223, train_loss: 0.1138, step time: 0.1012\n",
      "60/223, train_loss: 0.0885, step time: 0.1283\n",
      "61/223, train_loss: 0.0972, step time: 0.1150\n",
      "62/223, train_loss: 0.0998, step time: 0.1140\n",
      "63/223, train_loss: 0.0958, step time: 0.1173\n",
      "64/223, train_loss: 0.1011, step time: 0.1162\n",
      "65/223, train_loss: 0.0943, step time: 0.0999\n",
      "66/223, train_loss: 0.0947, step time: 0.1056\n",
      "67/223, train_loss: 0.1001, step time: 0.1046\n",
      "68/223, train_loss: 0.1069, step time: 0.1105\n",
      "69/223, train_loss: 0.0994, step time: 0.1231\n",
      "70/223, train_loss: 0.1029, step time: 0.1048\n",
      "71/223, train_loss: 0.1014, step time: 0.1114\n",
      "72/223, train_loss: 0.0898, step time: 0.1166\n",
      "73/223, train_loss: 0.1012, step time: 0.1179\n",
      "74/223, train_loss: 0.0980, step time: 0.1003\n",
      "75/223, train_loss: 0.0949, step time: 0.1195\n",
      "76/223, train_loss: 0.0946, step time: 0.1107\n",
      "77/223, train_loss: 0.1064, step time: 0.1106\n",
      "78/223, train_loss: 0.0993, step time: 0.1130\n",
      "79/223, train_loss: 0.0967, step time: 0.1428\n",
      "80/223, train_loss: 0.0998, step time: 0.1256\n",
      "81/223, train_loss: 0.1055, step time: 0.1080\n",
      "82/223, train_loss: 0.0924, step time: 0.1046\n",
      "83/223, train_loss: 0.0959, step time: 0.1215\n",
      "84/223, train_loss: 0.1055, step time: 0.1001\n",
      "85/223, train_loss: 0.0896, step time: 0.1101\n",
      "86/223, train_loss: 0.1002, step time: 0.1038\n",
      "87/223, train_loss: 0.1028, step time: 0.1519\n",
      "88/223, train_loss: 0.1100, step time: 0.1145\n",
      "89/223, train_loss: 0.0987, step time: 0.1114\n",
      "90/223, train_loss: 0.1039, step time: 0.1043\n",
      "91/223, train_loss: 0.1086, step time: 0.1113\n",
      "92/223, train_loss: 0.1039, step time: 0.1620\n",
      "93/223, train_loss: 0.1003, step time: 0.1137\n",
      "94/223, train_loss: 0.1034, step time: 0.1159\n",
      "95/223, train_loss: 0.0982, step time: 0.1618\n",
      "96/223, train_loss: 0.1079, step time: 0.1006\n",
      "97/223, train_loss: 0.0907, step time: 0.0993\n",
      "98/223, train_loss: 0.1075, step time: 0.1093\n",
      "99/223, train_loss: 0.0930, step time: 0.1441\n",
      "100/223, train_loss: 0.1078, step time: 0.1489\n",
      "101/223, train_loss: 0.0868, step time: 0.1144\n",
      "102/223, train_loss: 0.0998, step time: 0.1013\n",
      "103/223, train_loss: 0.1082, step time: 0.1249\n",
      "104/223, train_loss: 0.1078, step time: 0.1604\n",
      "105/223, train_loss: 0.1096, step time: 0.1181\n",
      "106/223, train_loss: 0.0960, step time: 0.1311\n",
      "107/223, train_loss: 0.1061, step time: 0.1554\n",
      "108/223, train_loss: 0.0996, step time: 0.1048\n",
      "109/223, train_loss: 0.1033, step time: 0.1248\n",
      "110/223, train_loss: 0.0948, step time: 0.1486\n",
      "111/223, train_loss: 0.0974, step time: 0.1287\n",
      "112/223, train_loss: 0.1184, step time: 0.1067\n",
      "113/223, train_loss: 0.0939, step time: 0.1026\n",
      "114/223, train_loss: 0.1035, step time: 0.0996\n",
      "115/223, train_loss: 0.0943, step time: 0.1004\n",
      "116/223, train_loss: 0.1047, step time: 0.1127\n",
      "117/223, train_loss: 0.0964, step time: 0.0993\n",
      "118/223, train_loss: 0.0960, step time: 0.0995\n",
      "119/223, train_loss: 0.1054, step time: 0.1013\n",
      "120/223, train_loss: 0.1074, step time: 0.1000\n",
      "121/223, train_loss: 0.0929, step time: 0.0990\n",
      "122/223, train_loss: 0.1141, step time: 0.0992\n",
      "123/223, train_loss: 0.1030, step time: 0.1003\n",
      "124/223, train_loss: 0.1004, step time: 0.1024\n",
      "125/223, train_loss: 0.0957, step time: 0.1025\n",
      "126/223, train_loss: 0.0911, step time: 0.0999\n",
      "127/223, train_loss: 0.0975, step time: 0.1037\n",
      "128/223, train_loss: 0.0905, step time: 0.1138\n",
      "129/223, train_loss: 0.1054, step time: 0.1204\n",
      "130/223, train_loss: 0.0956, step time: 0.1092\n",
      "131/223, train_loss: 0.0941, step time: 0.1163\n",
      "132/223, train_loss: 0.0982, step time: 0.1130\n",
      "133/223, train_loss: 0.0918, step time: 0.1194\n",
      "134/223, train_loss: 0.0976, step time: 0.1006\n",
      "135/223, train_loss: 0.0884, step time: 0.1058\n",
      "136/223, train_loss: 0.1015, step time: 0.1019\n",
      "137/223, train_loss: 0.0991, step time: 0.1162\n",
      "138/223, train_loss: 0.1080, step time: 0.1044\n",
      "139/223, train_loss: 0.1124, step time: 0.1208\n",
      "140/223, train_loss: 0.0950, step time: 0.1061\n",
      "141/223, train_loss: 0.1036, step time: 0.1029\n",
      "142/223, train_loss: 0.0997, step time: 0.1317\n",
      "143/223, train_loss: 0.0990, step time: 0.1064\n",
      "144/223, train_loss: 0.0909, step time: 0.0994\n",
      "145/223, train_loss: 0.0960, step time: 0.1136\n",
      "146/223, train_loss: 0.0974, step time: 0.1109\n",
      "147/223, train_loss: 0.0896, step time: 0.1412\n",
      "148/223, train_loss: 0.0980, step time: 0.1232\n",
      "149/223, train_loss: 0.0904, step time: 0.1074\n",
      "150/223, train_loss: 0.0878, step time: 0.1229\n",
      "151/223, train_loss: 0.1118, step time: 0.0999\n",
      "152/223, train_loss: 0.0936, step time: 0.0995\n",
      "153/223, train_loss: 0.1066, step time: 0.1125\n",
      "154/223, train_loss: 0.0970, step time: 0.1177\n",
      "155/223, train_loss: 0.0890, step time: 0.1012\n",
      "156/223, train_loss: 0.1053, step time: 0.1099\n",
      "157/223, train_loss: 0.1028, step time: 0.1149\n",
      "158/223, train_loss: 0.0934, step time: 0.1011\n",
      "159/223, train_loss: 0.0932, step time: 0.1101\n",
      "160/223, train_loss: 0.1020, step time: 0.1003\n",
      "161/223, train_loss: 0.1061, step time: 0.1002\n",
      "162/223, train_loss: 0.0961, step time: 0.1014\n",
      "163/223, train_loss: 0.1040, step time: 0.1023\n",
      "164/223, train_loss: 0.1117, step time: 0.1101\n",
      "165/223, train_loss: 0.0938, step time: 0.1109\n",
      "166/223, train_loss: 0.1021, step time: 0.1151\n",
      "167/223, train_loss: 0.0964, step time: 0.1033\n",
      "168/223, train_loss: 0.0966, step time: 0.1286\n",
      "169/223, train_loss: 0.1079, step time: 0.1144\n",
      "170/223, train_loss: 0.0986, step time: 0.1026\n",
      "171/223, train_loss: 0.0913, step time: 0.1019\n",
      "172/223, train_loss: 0.0897, step time: 0.1101\n",
      "173/223, train_loss: 0.1058, step time: 0.1139\n",
      "174/223, train_loss: 0.0909, step time: 0.1177\n",
      "175/223, train_loss: 0.0932, step time: 0.1077\n",
      "176/223, train_loss: 0.0926, step time: 0.1004\n",
      "177/223, train_loss: 0.1026, step time: 0.1089\n",
      "178/223, train_loss: 0.1038, step time: 0.1005\n",
      "179/223, train_loss: 0.0996, step time: 0.1005\n",
      "180/223, train_loss: 0.0968, step time: 0.1195\n",
      "181/223, train_loss: 0.1021, step time: 0.1141\n",
      "182/223, train_loss: 0.0955, step time: 0.1263\n",
      "183/223, train_loss: 0.1026, step time: 0.1009\n",
      "184/223, train_loss: 0.0902, step time: 0.1163\n",
      "185/223, train_loss: 0.1189, step time: 0.1411\n",
      "186/223, train_loss: 0.0937, step time: 0.1000\n",
      "187/223, train_loss: 0.0885, step time: 0.1011\n",
      "188/223, train_loss: 0.0966, step time: 0.1245\n",
      "189/223, train_loss: 0.0970, step time: 0.1114\n",
      "190/223, train_loss: 0.0943, step time: 0.1114\n",
      "191/223, train_loss: 0.0917, step time: 0.1288\n",
      "192/223, train_loss: 0.0879, step time: 0.1010\n",
      "193/223, train_loss: 0.0912, step time: 0.1150\n",
      "194/223, train_loss: 0.0946, step time: 0.1135\n",
      "195/223, train_loss: 0.0838, step time: 0.1190\n",
      "196/223, train_loss: 0.1026, step time: 0.1311\n",
      "197/223, train_loss: 0.0985, step time: 0.1011\n",
      "198/223, train_loss: 0.0953, step time: 0.1091\n",
      "199/223, train_loss: 0.0917, step time: 0.1095\n",
      "200/223, train_loss: 0.0973, step time: 0.1015\n",
      "201/223, train_loss: 0.0901, step time: 0.1154\n",
      "202/223, train_loss: 0.1046, step time: 0.1209\n",
      "203/223, train_loss: 0.1041, step time: 0.1055\n",
      "204/223, train_loss: 0.0997, step time: 0.1105\n",
      "205/223, train_loss: 0.1024, step time: 0.1120\n",
      "206/223, train_loss: 0.0935, step time: 0.1168\n",
      "207/223, train_loss: 0.1040, step time: 0.1050\n",
      "208/223, train_loss: 0.0841, step time: 0.1150\n",
      "209/223, train_loss: 0.1081, step time: 0.1006\n",
      "210/223, train_loss: 0.1012, step time: 0.1080\n",
      "211/223, train_loss: 0.0926, step time: 0.0998\n",
      "212/223, train_loss: 0.0943, step time: 0.1089\n",
      "213/223, train_loss: 0.1089, step time: 0.1194\n",
      "214/223, train_loss: 0.1034, step time: 0.1307\n",
      "215/223, train_loss: 0.0969, step time: 0.1047\n",
      "216/223, train_loss: 0.1022, step time: 0.1303\n",
      "217/223, train_loss: 0.0936, step time: 0.1014\n",
      "218/223, train_loss: 0.1015, step time: 0.1001\n",
      "219/223, train_loss: 0.0913, step time: 0.1002\n",
      "220/223, train_loss: 0.0888, step time: 0.1014\n",
      "221/223, train_loss: 0.0910, step time: 0.1005\n",
      "222/223, train_loss: 0.0928, step time: 0.0995\n",
      "223/223, train_loss: 0.0943, step time: 0.1003\n",
      "epoch 256 average loss: 0.0999\n",
      "time consuming of epoch 256 is: 90.7836\n",
      "----------\n",
      "epoch 257/300\n",
      "1/223, train_loss: 0.1058, step time: 0.1085\n",
      "2/223, train_loss: 0.1047, step time: 0.1128\n",
      "3/223, train_loss: 0.0862, step time: 0.1302\n",
      "4/223, train_loss: 0.0956, step time: 0.1232\n",
      "5/223, train_loss: 0.0981, step time: 0.1291\n",
      "6/223, train_loss: 0.0976, step time: 0.1048\n",
      "7/223, train_loss: 0.1100, step time: 0.1060\n",
      "8/223, train_loss: 0.0952, step time: 0.1033\n",
      "9/223, train_loss: 0.1032, step time: 0.1120\n",
      "10/223, train_loss: 0.1022, step time: 0.1279\n",
      "11/223, train_loss: 0.0867, step time: 0.1077\n",
      "12/223, train_loss: 0.1138, step time: 0.1083\n",
      "13/223, train_loss: 0.0985, step time: 0.1306\n",
      "14/223, train_loss: 0.1079, step time: 0.1360\n",
      "15/223, train_loss: 0.0969, step time: 0.1129\n",
      "16/223, train_loss: 0.0961, step time: 0.1007\n",
      "17/223, train_loss: 0.0976, step time: 0.1053\n",
      "18/223, train_loss: 0.1018, step time: 0.1132\n",
      "19/223, train_loss: 0.0948, step time: 0.1173\n",
      "20/223, train_loss: 0.0874, step time: 0.1060\n",
      "21/223, train_loss: 0.0983, step time: 0.0996\n",
      "22/223, train_loss: 0.0962, step time: 0.1403\n",
      "23/223, train_loss: 0.1047, step time: 0.1179\n",
      "24/223, train_loss: 0.1115, step time: 0.1083\n",
      "25/223, train_loss: 0.0924, step time: 0.1128\n",
      "26/223, train_loss: 0.1059, step time: 0.1349\n",
      "27/223, train_loss: 0.0982, step time: 0.1233\n",
      "28/223, train_loss: 0.0944, step time: 0.1234\n",
      "29/223, train_loss: 0.0869, step time: 0.1316\n",
      "30/223, train_loss: 0.0975, step time: 0.1051\n",
      "31/223, train_loss: 0.0966, step time: 0.1073\n",
      "32/223, train_loss: 0.1013, step time: 0.1149\n",
      "33/223, train_loss: 0.0850, step time: 0.1093\n",
      "34/223, train_loss: 0.1031, step time: 0.1194\n",
      "35/223, train_loss: 0.0928, step time: 0.1181\n",
      "36/223, train_loss: 0.0990, step time: 0.1153\n",
      "37/223, train_loss: 0.0962, step time: 0.1112\n",
      "38/223, train_loss: 0.0930, step time: 0.1005\n",
      "39/223, train_loss: 0.1183, step time: 0.1081\n",
      "40/223, train_loss: 0.0898, step time: 0.1324\n",
      "41/223, train_loss: 0.0938, step time: 0.1110\n",
      "42/223, train_loss: 0.1042, step time: 0.1145\n",
      "43/223, train_loss: 0.0898, step time: 0.1016\n",
      "44/223, train_loss: 0.1073, step time: 0.1008\n",
      "45/223, train_loss: 0.1116, step time: 0.1119\n",
      "46/223, train_loss: 0.1036, step time: 0.1035\n",
      "47/223, train_loss: 0.1044, step time: 0.1170\n",
      "48/223, train_loss: 0.1034, step time: 0.1002\n",
      "49/223, train_loss: 0.2959, step time: 0.1014\n",
      "50/223, train_loss: 0.1041, step time: 0.1193\n",
      "51/223, train_loss: 0.0854, step time: 0.1154\n",
      "52/223, train_loss: 0.0974, step time: 0.1111\n",
      "53/223, train_loss: 0.0967, step time: 0.1199\n",
      "54/223, train_loss: 0.0901, step time: 0.1264\n",
      "55/223, train_loss: 0.0954, step time: 0.1291\n",
      "56/223, train_loss: 0.0981, step time: 0.1010\n",
      "57/223, train_loss: 0.0949, step time: 0.1123\n",
      "58/223, train_loss: 0.1009, step time: 0.1088\n",
      "59/223, train_loss: 0.1021, step time: 0.1108\n",
      "60/223, train_loss: 0.1012, step time: 0.1099\n",
      "61/223, train_loss: 0.1065, step time: 0.0997\n",
      "62/223, train_loss: 0.0982, step time: 0.1091\n",
      "63/223, train_loss: 0.1049, step time: 0.1243\n",
      "64/223, train_loss: 0.1047, step time: 0.1045\n",
      "65/223, train_loss: 0.1028, step time: 0.1160\n",
      "66/223, train_loss: 0.0962, step time: 0.1186\n",
      "67/223, train_loss: 0.1042, step time: 0.1367\n",
      "68/223, train_loss: 0.0914, step time: 0.1129\n",
      "69/223, train_loss: 0.0956, step time: 0.1119\n",
      "70/223, train_loss: 0.0893, step time: 0.0998\n",
      "71/223, train_loss: 0.1011, step time: 0.1112\n",
      "72/223, train_loss: 0.0872, step time: 0.1073\n",
      "73/223, train_loss: 0.0996, step time: 0.1011\n",
      "74/223, train_loss: 0.0951, step time: 0.1160\n",
      "75/223, train_loss: 0.1075, step time: 0.1105\n",
      "76/223, train_loss: 0.1077, step time: 0.1124\n",
      "77/223, train_loss: 0.1048, step time: 0.1201\n",
      "78/223, train_loss: 0.0934, step time: 0.1183\n",
      "79/223, train_loss: 0.0973, step time: 0.1035\n",
      "80/223, train_loss: 0.1076, step time: 0.1102\n",
      "81/223, train_loss: 0.1186, step time: 0.1158\n",
      "82/223, train_loss: 0.0889, step time: 0.1221\n",
      "83/223, train_loss: 0.0970, step time: 0.1220\n",
      "84/223, train_loss: 0.0920, step time: 0.0987\n",
      "85/223, train_loss: 0.1057, step time: 0.1121\n",
      "86/223, train_loss: 0.1007, step time: 0.1161\n",
      "87/223, train_loss: 0.0971, step time: 0.1095\n",
      "88/223, train_loss: 0.1059, step time: 0.1012\n",
      "89/223, train_loss: 0.0946, step time: 0.1271\n",
      "90/223, train_loss: 0.0958, step time: 0.1069\n",
      "91/223, train_loss: 0.1009, step time: 0.1084\n",
      "92/223, train_loss: 0.0914, step time: 0.1116\n",
      "93/223, train_loss: 0.1106, step time: 0.1137\n",
      "94/223, train_loss: 0.1070, step time: 0.1370\n",
      "95/223, train_loss: 0.0886, step time: 0.1146\n",
      "96/223, train_loss: 0.0904, step time: 0.1235\n",
      "97/223, train_loss: 0.0949, step time: 0.1337\n",
      "98/223, train_loss: 0.0887, step time: 0.1113\n",
      "99/223, train_loss: 0.0934, step time: 0.1182\n",
      "100/223, train_loss: 0.1016, step time: 0.1168\n",
      "101/223, train_loss: 0.1005, step time: 0.1047\n",
      "102/223, train_loss: 0.1021, step time: 0.1299\n",
      "103/223, train_loss: 0.1121, step time: 0.1002\n",
      "104/223, train_loss: 0.0997, step time: 0.1526\n",
      "105/223, train_loss: 0.0969, step time: 0.1207\n",
      "106/223, train_loss: 0.0937, step time: 0.1334\n",
      "107/223, train_loss: 0.1027, step time: 0.1376\n",
      "108/223, train_loss: 0.1021, step time: 0.1286\n",
      "109/223, train_loss: 0.0892, step time: 0.1173\n",
      "110/223, train_loss: 0.1123, step time: 0.1191\n",
      "111/223, train_loss: 0.1023, step time: 0.1073\n",
      "112/223, train_loss: 0.0911, step time: 0.1007\n",
      "113/223, train_loss: 0.1008, step time: 0.1162\n",
      "114/223, train_loss: 0.0972, step time: 0.1002\n",
      "115/223, train_loss: 0.0955, step time: 0.1253\n",
      "116/223, train_loss: 0.0948, step time: 0.1196\n",
      "117/223, train_loss: 0.1083, step time: 0.1073\n",
      "118/223, train_loss: 0.0979, step time: 0.1151\n",
      "119/223, train_loss: 0.0968, step time: 0.1138\n",
      "120/223, train_loss: 0.0941, step time: 0.1091\n",
      "121/223, train_loss: 0.0945, step time: 0.1101\n",
      "122/223, train_loss: 0.1025, step time: 0.1187\n",
      "123/223, train_loss: 0.0957, step time: 0.1288\n",
      "124/223, train_loss: 0.0937, step time: 0.1122\n",
      "125/223, train_loss: 0.1072, step time: 0.1051\n",
      "126/223, train_loss: 0.0891, step time: 0.1211\n",
      "127/223, train_loss: 0.0943, step time: 0.1196\n",
      "128/223, train_loss: 0.1011, step time: 0.1121\n",
      "129/223, train_loss: 0.0922, step time: 0.1164\n",
      "130/223, train_loss: 0.0934, step time: 0.1292\n",
      "131/223, train_loss: 0.0996, step time: 0.1245\n",
      "132/223, train_loss: 0.0999, step time: 0.1209\n",
      "133/223, train_loss: 0.1107, step time: 0.1232\n",
      "134/223, train_loss: 0.0950, step time: 0.1113\n",
      "135/223, train_loss: 0.1008, step time: 0.1129\n",
      "136/223, train_loss: 0.0989, step time: 0.1055\n",
      "137/223, train_loss: 0.1105, step time: 0.1127\n",
      "138/223, train_loss: 0.1047, step time: 0.1119\n",
      "139/223, train_loss: 0.1015, step time: 0.1015\n",
      "140/223, train_loss: 0.0942, step time: 0.1003\n",
      "141/223, train_loss: 0.1038, step time: 0.1006\n",
      "142/223, train_loss: 0.1018, step time: 0.1177\n",
      "143/223, train_loss: 0.1066, step time: 0.1355\n",
      "144/223, train_loss: 0.1009, step time: 0.1233\n",
      "145/223, train_loss: 0.0946, step time: 0.1133\n",
      "146/223, train_loss: 0.1004, step time: 0.1219\n",
      "147/223, train_loss: 0.0917, step time: 0.1171\n",
      "148/223, train_loss: 0.0995, step time: 0.1367\n",
      "149/223, train_loss: 0.1044, step time: 0.1101\n",
      "150/223, train_loss: 0.0955, step time: 0.1098\n",
      "151/223, train_loss: 0.1099, step time: 0.1132\n",
      "152/223, train_loss: 0.0960, step time: 0.1132\n",
      "153/223, train_loss: 0.0921, step time: 0.1341\n",
      "154/223, train_loss: 0.0979, step time: 0.1072\n",
      "155/223, train_loss: 0.0906, step time: 0.1168\n",
      "156/223, train_loss: 0.0875, step time: 0.1058\n",
      "157/223, train_loss: 0.0988, step time: 0.1002\n",
      "158/223, train_loss: 0.1017, step time: 0.0998\n",
      "159/223, train_loss: 0.0900, step time: 0.0998\n",
      "160/223, train_loss: 0.1075, step time: 0.1132\n",
      "161/223, train_loss: 0.1033, step time: 0.1098\n",
      "162/223, train_loss: 0.0912, step time: 0.1048\n",
      "163/223, train_loss: 0.0944, step time: 0.1007\n",
      "164/223, train_loss: 0.0998, step time: 0.1034\n",
      "165/223, train_loss: 0.0986, step time: 0.1216\n",
      "166/223, train_loss: 0.1030, step time: 0.1082\n",
      "167/223, train_loss: 0.0919, step time: 0.1120\n",
      "168/223, train_loss: 0.1108, step time: 0.1057\n",
      "169/223, train_loss: 0.0983, step time: 0.1170\n",
      "170/223, train_loss: 0.0835, step time: 0.1088\n",
      "171/223, train_loss: 0.0998, step time: 0.1061\n",
      "172/223, train_loss: 0.0844, step time: 0.1013\n",
      "173/223, train_loss: 0.1079, step time: 0.1089\n",
      "174/223, train_loss: 0.0922, step time: 0.1151\n",
      "175/223, train_loss: 0.1120, step time: 0.1030\n",
      "176/223, train_loss: 0.1157, step time: 0.1062\n",
      "177/223, train_loss: 0.0893, step time: 0.1226\n",
      "178/223, train_loss: 0.1013, step time: 0.1123\n",
      "179/223, train_loss: 0.1011, step time: 0.1190\n",
      "180/223, train_loss: 0.0948, step time: 0.1239\n",
      "181/223, train_loss: 0.1039, step time: 0.1228\n",
      "182/223, train_loss: 0.0969, step time: 0.1266\n",
      "183/223, train_loss: 0.1009, step time: 0.1007\n",
      "184/223, train_loss: 0.1105, step time: 0.1002\n",
      "185/223, train_loss: 0.0886, step time: 0.1058\n",
      "186/223, train_loss: 0.1090, step time: 0.0999\n",
      "187/223, train_loss: 0.1052, step time: 0.1158\n",
      "188/223, train_loss: 0.0937, step time: 0.1070\n",
      "189/223, train_loss: 0.1045, step time: 0.0996\n",
      "190/223, train_loss: 0.0993, step time: 0.1002\n",
      "191/223, train_loss: 0.0907, step time: 0.1079\n",
      "192/223, train_loss: 0.0872, step time: 0.1102\n",
      "193/223, train_loss: 0.0987, step time: 0.1161\n",
      "194/223, train_loss: 0.0968, step time: 0.1301\n",
      "195/223, train_loss: 0.0965, step time: 0.1332\n",
      "196/223, train_loss: 0.0996, step time: 0.1005\n",
      "197/223, train_loss: 0.0959, step time: 0.0988\n",
      "198/223, train_loss: 0.0987, step time: 0.0987\n",
      "199/223, train_loss: 0.0901, step time: 0.1003\n",
      "200/223, train_loss: 0.0878, step time: 0.1007\n",
      "201/223, train_loss: 0.0978, step time: 0.1000\n",
      "202/223, train_loss: 0.0880, step time: 0.0998\n",
      "203/223, train_loss: 0.0885, step time: 0.1004\n",
      "204/223, train_loss: 0.0961, step time: 0.1130\n",
      "205/223, train_loss: 0.1045, step time: 0.1002\n",
      "206/223, train_loss: 0.1042, step time: 0.0993\n",
      "207/223, train_loss: 0.1063, step time: 0.1000\n",
      "208/223, train_loss: 0.1021, step time: 0.1012\n",
      "209/223, train_loss: 0.1005, step time: 0.0989\n",
      "210/223, train_loss: 0.1131, step time: 0.1001\n",
      "211/223, train_loss: 0.0880, step time: 0.1006\n",
      "212/223, train_loss: 0.0941, step time: 0.1107\n",
      "213/223, train_loss: 0.1228, step time: 0.1123\n",
      "214/223, train_loss: 0.1031, step time: 0.1189\n",
      "215/223, train_loss: 0.0950, step time: 0.1203\n",
      "216/223, train_loss: 0.1014, step time: 0.1053\n",
      "217/223, train_loss: 0.0985, step time: 0.1009\n",
      "218/223, train_loss: 0.0964, step time: 0.1009\n",
      "219/223, train_loss: 0.0901, step time: 0.1005\n",
      "220/223, train_loss: 0.1089, step time: 0.0999\n",
      "221/223, train_loss: 0.0939, step time: 0.1003\n",
      "222/223, train_loss: 0.1083, step time: 0.0994\n",
      "223/223, train_loss: 0.1061, step time: 0.1003\n",
      "epoch 257 average loss: 0.0998\n",
      "time consuming of epoch 257 is: 94.2742\n",
      "----------\n",
      "epoch 258/300\n",
      "1/223, train_loss: 0.1019, step time: 0.1087\n",
      "2/223, train_loss: 0.0880, step time: 0.1009\n",
      "3/223, train_loss: 0.0967, step time: 0.1035\n",
      "4/223, train_loss: 0.0910, step time: 0.1008\n",
      "5/223, train_loss: 0.0929, step time: 0.1128\n",
      "6/223, train_loss: 0.0951, step time: 0.1060\n",
      "7/223, train_loss: 0.1026, step time: 0.1069\n",
      "8/223, train_loss: 0.0990, step time: 0.1403\n",
      "9/223, train_loss: 0.0912, step time: 0.1110\n",
      "10/223, train_loss: 0.0921, step time: 0.1139\n",
      "11/223, train_loss: 0.1204, step time: 0.1196\n",
      "12/223, train_loss: 0.1011, step time: 0.1049\n",
      "13/223, train_loss: 0.1065, step time: 0.1046\n",
      "14/223, train_loss: 0.1002, step time: 0.1099\n",
      "15/223, train_loss: 0.1010, step time: 0.1181\n",
      "16/223, train_loss: 0.1116, step time: 0.1320\n",
      "17/223, train_loss: 0.0899, step time: 0.1002\n",
      "18/223, train_loss: 0.0939, step time: 0.0994\n",
      "19/223, train_loss: 0.0901, step time: 0.1012\n",
      "20/223, train_loss: 0.1019, step time: 0.1316\n",
      "21/223, train_loss: 0.0968, step time: 0.1110\n",
      "22/223, train_loss: 0.1072, step time: 0.1045\n",
      "23/223, train_loss: 0.1004, step time: 0.1229\n",
      "24/223, train_loss: 0.1032, step time: 0.1111\n",
      "25/223, train_loss: 0.1041, step time: 0.1143\n",
      "26/223, train_loss: 0.0945, step time: 0.1000\n",
      "27/223, train_loss: 0.0898, step time: 0.1002\n",
      "28/223, train_loss: 0.1058, step time: 0.1296\n",
      "29/223, train_loss: 0.0968, step time: 0.1049\n",
      "30/223, train_loss: 0.0980, step time: 0.1005\n",
      "31/223, train_loss: 0.0979, step time: 0.1103\n",
      "32/223, train_loss: 0.0930, step time: 0.1083\n",
      "33/223, train_loss: 0.0978, step time: 0.1111\n",
      "34/223, train_loss: 0.1050, step time: 0.1006\n",
      "35/223, train_loss: 0.1097, step time: 0.1185\n",
      "36/223, train_loss: 0.0928, step time: 0.1059\n",
      "37/223, train_loss: 0.0956, step time: 0.1185\n",
      "38/223, train_loss: 0.1022, step time: 0.1151\n",
      "39/223, train_loss: 0.0886, step time: 0.1101\n",
      "40/223, train_loss: 0.1021, step time: 0.1002\n",
      "41/223, train_loss: 0.1051, step time: 0.1114\n",
      "42/223, train_loss: 0.0915, step time: 0.1102\n",
      "43/223, train_loss: 0.1049, step time: 0.1416\n",
      "44/223, train_loss: 0.1022, step time: 0.1039\n",
      "45/223, train_loss: 0.1000, step time: 0.1073\n",
      "46/223, train_loss: 0.0932, step time: 0.1067\n",
      "47/223, train_loss: 0.1136, step time: 0.1082\n",
      "48/223, train_loss: 0.0972, step time: 0.1151\n",
      "49/223, train_loss: 0.1035, step time: 0.0999\n",
      "50/223, train_loss: 0.0985, step time: 0.1155\n",
      "51/223, train_loss: 0.0927, step time: 0.1298\n",
      "52/223, train_loss: 0.0899, step time: 0.1001\n",
      "53/223, train_loss: 0.0995, step time: 0.1283\n",
      "54/223, train_loss: 0.1067, step time: 0.1543\n",
      "55/223, train_loss: 0.0941, step time: 0.1213\n",
      "56/223, train_loss: 0.0953, step time: 0.1098\n",
      "57/223, train_loss: 0.1003, step time: 0.1035\n",
      "58/223, train_loss: 0.1140, step time: 0.1252\n",
      "59/223, train_loss: 0.0963, step time: 0.1307\n",
      "60/223, train_loss: 0.0950, step time: 0.1281\n",
      "61/223, train_loss: 0.0899, step time: 0.1038\n",
      "62/223, train_loss: 0.0856, step time: 0.0994\n",
      "63/223, train_loss: 0.0919, step time: 0.1550\n",
      "64/223, train_loss: 0.0942, step time: 0.1086\n",
      "65/223, train_loss: 0.0889, step time: 0.0998\n",
      "66/223, train_loss: 0.0992, step time: 0.1333\n",
      "67/223, train_loss: 0.0910, step time: 0.1512\n",
      "68/223, train_loss: 0.0890, step time: 0.1201\n",
      "69/223, train_loss: 0.1077, step time: 0.1296\n",
      "70/223, train_loss: 0.1015, step time: 0.1176\n",
      "71/223, train_loss: 0.1007, step time: 0.1007\n",
      "72/223, train_loss: 0.0974, step time: 0.1007\n",
      "73/223, train_loss: 0.1038, step time: 0.0999\n",
      "74/223, train_loss: 0.1021, step time: 0.1146\n",
      "75/223, train_loss: 0.0986, step time: 0.1064\n",
      "76/223, train_loss: 0.0973, step time: 0.1252\n",
      "77/223, train_loss: 0.0947, step time: 0.1000\n",
      "78/223, train_loss: 0.0890, step time: 0.1010\n",
      "79/223, train_loss: 0.1120, step time: 0.1107\n",
      "80/223, train_loss: 0.0947, step time: 0.1059\n",
      "81/223, train_loss: 0.0895, step time: 0.1034\n",
      "82/223, train_loss: 0.0893, step time: 0.1159\n",
      "83/223, train_loss: 0.0994, step time: 0.1052\n",
      "84/223, train_loss: 0.0956, step time: 0.1230\n",
      "85/223, train_loss: 0.1008, step time: 0.1159\n",
      "86/223, train_loss: 0.0864, step time: 0.1097\n",
      "87/223, train_loss: 0.1075, step time: 0.1121\n",
      "88/223, train_loss: 0.0904, step time: 0.1007\n",
      "89/223, train_loss: 0.0958, step time: 0.1006\n",
      "90/223, train_loss: 0.0974, step time: 0.0999\n",
      "91/223, train_loss: 0.0898, step time: 0.1011\n",
      "92/223, train_loss: 0.1007, step time: 0.1007\n",
      "93/223, train_loss: 0.0930, step time: 0.1184\n",
      "94/223, train_loss: 0.0965, step time: 0.0998\n",
      "95/223, train_loss: 0.1062, step time: 0.0997\n",
      "96/223, train_loss: 0.1011, step time: 0.1050\n",
      "97/223, train_loss: 0.0935, step time: 0.1036\n",
      "98/223, train_loss: 0.0924, step time: 0.1098\n",
      "99/223, train_loss: 0.0958, step time: 0.1059\n",
      "100/223, train_loss: 0.0882, step time: 0.1166\n",
      "101/223, train_loss: 0.1113, step time: 0.1007\n",
      "102/223, train_loss: 0.1011, step time: 0.1167\n",
      "103/223, train_loss: 0.0932, step time: 0.1109\n",
      "104/223, train_loss: 0.0986, step time: 0.1052\n",
      "105/223, train_loss: 0.1084, step time: 0.1179\n",
      "106/223, train_loss: 0.0974, step time: 0.1252\n",
      "107/223, train_loss: 0.0997, step time: 0.1033\n",
      "108/223, train_loss: 0.1072, step time: 0.1146\n",
      "109/223, train_loss: 0.0985, step time: 0.1118\n",
      "110/223, train_loss: 0.0951, step time: 0.1139\n",
      "111/223, train_loss: 0.0959, step time: 0.1140\n",
      "112/223, train_loss: 0.0999, step time: 0.1252\n",
      "113/223, train_loss: 0.0942, step time: 0.0994\n",
      "114/223, train_loss: 0.1070, step time: 0.1015\n",
      "115/223, train_loss: 0.1020, step time: 0.0997\n",
      "116/223, train_loss: 0.1047, step time: 0.1523\n",
      "117/223, train_loss: 0.0994, step time: 0.0999\n",
      "118/223, train_loss: 0.0901, step time: 0.1116\n",
      "119/223, train_loss: 0.1061, step time: 0.1069\n",
      "120/223, train_loss: 0.0923, step time: 0.1155\n",
      "121/223, train_loss: 0.1046, step time: 0.1038\n",
      "122/223, train_loss: 0.1106, step time: 0.1045\n",
      "123/223, train_loss: 0.0909, step time: 0.0998\n",
      "124/223, train_loss: 0.0984, step time: 0.1002\n",
      "125/223, train_loss: 0.0988, step time: 0.1057\n",
      "126/223, train_loss: 0.1047, step time: 0.1164\n",
      "127/223, train_loss: 0.0954, step time: 0.1146\n",
      "128/223, train_loss: 0.1007, step time: 0.1087\n",
      "129/223, train_loss: 0.0886, step time: 0.1095\n",
      "130/223, train_loss: 0.1014, step time: 0.1108\n",
      "131/223, train_loss: 0.1044, step time: 0.1175\n",
      "132/223, train_loss: 0.1033, step time: 0.1211\n",
      "133/223, train_loss: 0.1009, step time: 0.1137\n",
      "134/223, train_loss: 0.0998, step time: 0.1007\n",
      "135/223, train_loss: 0.0902, step time: 0.1037\n",
      "136/223, train_loss: 0.1168, step time: 0.1181\n",
      "137/223, train_loss: 0.1000, step time: 0.1114\n",
      "138/223, train_loss: 0.0960, step time: 0.1004\n",
      "139/223, train_loss: 0.0982, step time: 0.1009\n",
      "140/223, train_loss: 0.0988, step time: 0.1008\n",
      "141/223, train_loss: 0.0956, step time: 0.1117\n",
      "142/223, train_loss: 0.1054, step time: 0.1223\n",
      "143/223, train_loss: 0.0868, step time: 0.1014\n",
      "144/223, train_loss: 0.1013, step time: 0.1150\n",
      "145/223, train_loss: 0.0976, step time: 0.1380\n",
      "146/223, train_loss: 0.0901, step time: 0.1085\n",
      "147/223, train_loss: 0.1065, step time: 0.1000\n",
      "148/223, train_loss: 0.0948, step time: 0.1007\n",
      "149/223, train_loss: 0.1017, step time: 0.1223\n",
      "150/223, train_loss: 0.0962, step time: 0.1094\n",
      "151/223, train_loss: 0.1004, step time: 0.1123\n",
      "152/223, train_loss: 0.0972, step time: 0.1048\n",
      "153/223, train_loss: 0.1050, step time: 0.1136\n",
      "154/223, train_loss: 0.1059, step time: 0.1125\n",
      "155/223, train_loss: 0.0927, step time: 0.1214\n",
      "156/223, train_loss: 0.1032, step time: 0.1006\n",
      "157/223, train_loss: 0.1076, step time: 0.1156\n",
      "158/223, train_loss: 0.0904, step time: 0.1031\n",
      "159/223, train_loss: 0.1050, step time: 0.1161\n",
      "160/223, train_loss: 0.0947, step time: 0.1150\n",
      "161/223, train_loss: 0.2973, step time: 0.1534\n",
      "162/223, train_loss: 0.0928, step time: 0.1057\n",
      "163/223, train_loss: 0.1002, step time: 0.1175\n",
      "164/223, train_loss: 0.1025, step time: 0.1007\n",
      "165/223, train_loss: 0.0983, step time: 0.1002\n",
      "166/223, train_loss: 0.1037, step time: 0.1046\n",
      "167/223, train_loss: 0.0995, step time: 0.1353\n",
      "168/223, train_loss: 0.1014, step time: 0.1004\n",
      "169/223, train_loss: 0.1111, step time: 0.1018\n",
      "170/223, train_loss: 0.0887, step time: 0.1141\n",
      "171/223, train_loss: 0.1074, step time: 0.1004\n",
      "172/223, train_loss: 0.0965, step time: 0.1113\n",
      "173/223, train_loss: 0.1033, step time: 0.1143\n",
      "174/223, train_loss: 0.0947, step time: 0.1116\n",
      "175/223, train_loss: 0.1023, step time: 0.1057\n",
      "176/223, train_loss: 0.0993, step time: 0.1067\n",
      "177/223, train_loss: 0.1040, step time: 0.1039\n",
      "178/223, train_loss: 0.1104, step time: 0.1007\n",
      "179/223, train_loss: 0.1121, step time: 0.1405\n",
      "180/223, train_loss: 0.1049, step time: 0.1164\n",
      "181/223, train_loss: 0.0949, step time: 0.1129\n",
      "182/223, train_loss: 0.0982, step time: 0.1177\n",
      "183/223, train_loss: 0.1073, step time: 0.1058\n",
      "184/223, train_loss: 0.1117, step time: 0.1020\n",
      "185/223, train_loss: 0.0949, step time: 0.1164\n",
      "186/223, train_loss: 0.0998, step time: 0.1051\n",
      "187/223, train_loss: 0.0953, step time: 0.1030\n",
      "188/223, train_loss: 0.1062, step time: 0.1232\n",
      "189/223, train_loss: 0.0969, step time: 0.1258\n",
      "190/223, train_loss: 0.0955, step time: 0.1148\n",
      "191/223, train_loss: 0.0923, step time: 0.1009\n",
      "192/223, train_loss: 0.1013, step time: 0.1018\n",
      "193/223, train_loss: 0.0975, step time: 0.1102\n",
      "194/223, train_loss: 0.1045, step time: 0.1054\n",
      "195/223, train_loss: 0.1045, step time: 0.1208\n",
      "196/223, train_loss: 0.0984, step time: 0.1066\n",
      "197/223, train_loss: 0.0964, step time: 0.1007\n",
      "198/223, train_loss: 0.1002, step time: 0.1022\n",
      "199/223, train_loss: 0.0936, step time: 0.1135\n",
      "200/223, train_loss: 0.0979, step time: 0.1471\n",
      "201/223, train_loss: 0.1082, step time: 0.1302\n",
      "202/223, train_loss: 0.1039, step time: 0.1106\n",
      "203/223, train_loss: 0.1018, step time: 0.1119\n",
      "204/223, train_loss: 0.0862, step time: 0.1106\n",
      "205/223, train_loss: 0.0937, step time: 0.1512\n",
      "206/223, train_loss: 0.0907, step time: 0.1111\n",
      "207/223, train_loss: 0.1077, step time: 0.1224\n",
      "208/223, train_loss: 0.0963, step time: 0.1123\n",
      "209/223, train_loss: 0.1050, step time: 0.1211\n",
      "210/223, train_loss: 0.1065, step time: 0.1082\n",
      "211/223, train_loss: 0.0936, step time: 0.1200\n",
      "212/223, train_loss: 0.1018, step time: 0.1105\n",
      "213/223, train_loss: 0.0924, step time: 0.1115\n",
      "214/223, train_loss: 0.0982, step time: 0.1245\n",
      "215/223, train_loss: 0.1073, step time: 0.1030\n",
      "216/223, train_loss: 0.0897, step time: 0.1078\n",
      "217/223, train_loss: 0.0970, step time: 0.1094\n",
      "218/223, train_loss: 0.1019, step time: 0.1206\n",
      "219/223, train_loss: 0.0951, step time: 0.1010\n",
      "220/223, train_loss: 0.0923, step time: 0.1177\n",
      "221/223, train_loss: 0.1008, step time: 0.0999\n",
      "222/223, train_loss: 0.1043, step time: 0.0996\n",
      "223/223, train_loss: 0.1216, step time: 0.0993\n",
      "epoch 258 average loss: 0.0999\n",
      "time consuming of epoch 258 is: 91.4375\n",
      "----------\n",
      "epoch 259/300\n",
      "1/223, train_loss: 0.1047, step time: 0.1022\n",
      "2/223, train_loss: 0.0908, step time: 0.1009\n",
      "3/223, train_loss: 0.1023, step time: 0.1161\n",
      "4/223, train_loss: 0.0953, step time: 0.1005\n",
      "5/223, train_loss: 0.0974, step time: 0.1009\n",
      "6/223, train_loss: 0.1052, step time: 0.1008\n",
      "7/223, train_loss: 0.0892, step time: 0.1003\n",
      "8/223, train_loss: 0.0903, step time: 0.1014\n",
      "9/223, train_loss: 0.1071, step time: 0.1360\n",
      "10/223, train_loss: 0.0950, step time: 0.1262\n",
      "11/223, train_loss: 0.1014, step time: 0.1127\n",
      "12/223, train_loss: 0.1033, step time: 0.1157\n",
      "13/223, train_loss: 0.1198, step time: 0.1204\n",
      "14/223, train_loss: 0.0928, step time: 0.1001\n",
      "15/223, train_loss: 0.1000, step time: 0.1021\n",
      "16/223, train_loss: 0.1065, step time: 0.0996\n",
      "17/223, train_loss: 0.1030, step time: 0.1049\n",
      "18/223, train_loss: 0.1081, step time: 0.1187\n",
      "19/223, train_loss: 0.1041, step time: 0.1000\n",
      "20/223, train_loss: 0.0984, step time: 0.1072\n",
      "21/223, train_loss: 0.1045, step time: 0.1147\n",
      "22/223, train_loss: 0.0929, step time: 0.1147\n",
      "23/223, train_loss: 0.0904, step time: 0.1366\n",
      "24/223, train_loss: 0.1032, step time: 0.1169\n",
      "25/223, train_loss: 0.1045, step time: 0.1055\n",
      "26/223, train_loss: 0.0983, step time: 0.1083\n",
      "27/223, train_loss: 0.0940, step time: 0.1165\n",
      "28/223, train_loss: 0.0985, step time: 0.1160\n",
      "29/223, train_loss: 0.1023, step time: 0.1042\n",
      "30/223, train_loss: 0.0970, step time: 0.1229\n",
      "31/223, train_loss: 0.1086, step time: 0.1003\n",
      "32/223, train_loss: 0.1015, step time: 0.1018\n",
      "33/223, train_loss: 0.0998, step time: 0.1164\n",
      "34/223, train_loss: 0.1016, step time: 0.1133\n",
      "35/223, train_loss: 0.0935, step time: 0.0998\n",
      "36/223, train_loss: 0.0898, step time: 0.0995\n",
      "37/223, train_loss: 0.0938, step time: 0.1046\n",
      "38/223, train_loss: 0.0977, step time: 0.1076\n",
      "39/223, train_loss: 0.1048, step time: 0.1006\n",
      "40/223, train_loss: 0.0921, step time: 0.1008\n",
      "41/223, train_loss: 0.0972, step time: 0.1135\n",
      "42/223, train_loss: 0.0937, step time: 0.1009\n",
      "43/223, train_loss: 0.0897, step time: 0.1008\n",
      "44/223, train_loss: 0.1006, step time: 0.1430\n",
      "45/223, train_loss: 0.0917, step time: 0.1069\n",
      "46/223, train_loss: 0.0943, step time: 0.1005\n",
      "47/223, train_loss: 0.0842, step time: 0.1006\n",
      "48/223, train_loss: 0.1077, step time: 0.0999\n",
      "49/223, train_loss: 0.0988, step time: 0.1404\n",
      "50/223, train_loss: 0.0882, step time: 0.1522\n",
      "51/223, train_loss: 0.1030, step time: 0.1018\n",
      "52/223, train_loss: 0.0980, step time: 0.1212\n",
      "53/223, train_loss: 0.1005, step time: 0.1282\n",
      "54/223, train_loss: 0.0961, step time: 0.1149\n",
      "55/223, train_loss: 0.0906, step time: 0.1041\n",
      "56/223, train_loss: 0.1033, step time: 0.1097\n",
      "57/223, train_loss: 0.0990, step time: 0.1124\n",
      "58/223, train_loss: 0.0999, step time: 0.1213\n",
      "59/223, train_loss: 0.1077, step time: 0.1201\n",
      "60/223, train_loss: 0.0903, step time: 0.1010\n",
      "61/223, train_loss: 0.0967, step time: 0.1187\n",
      "62/223, train_loss: 0.0928, step time: 0.1388\n",
      "63/223, train_loss: 0.0842, step time: 0.1212\n",
      "64/223, train_loss: 0.1017, step time: 0.1124\n",
      "65/223, train_loss: 0.1015, step time: 0.1181\n",
      "66/223, train_loss: 0.1065, step time: 0.1144\n",
      "67/223, train_loss: 0.1003, step time: 0.1007\n",
      "68/223, train_loss: 0.1068, step time: 0.1130\n",
      "69/223, train_loss: 0.0976, step time: 0.1104\n",
      "70/223, train_loss: 0.0955, step time: 0.1135\n",
      "71/223, train_loss: 0.0949, step time: 0.1061\n",
      "72/223, train_loss: 0.1075, step time: 0.1239\n",
      "73/223, train_loss: 0.0997, step time: 0.1070\n",
      "74/223, train_loss: 0.1031, step time: 0.1162\n",
      "75/223, train_loss: 0.1056, step time: 0.1295\n",
      "76/223, train_loss: 0.0929, step time: 0.1081\n",
      "77/223, train_loss: 0.0948, step time: 0.0989\n",
      "78/223, train_loss: 0.1017, step time: 0.1272\n",
      "79/223, train_loss: 0.0978, step time: 0.1025\n",
      "80/223, train_loss: 0.0985, step time: 0.1005\n",
      "81/223, train_loss: 0.1008, step time: 0.1105\n",
      "82/223, train_loss: 0.0978, step time: 0.1325\n",
      "83/223, train_loss: 0.0959, step time: 0.1039\n",
      "84/223, train_loss: 0.0947, step time: 0.0992\n",
      "85/223, train_loss: 0.1061, step time: 0.1216\n",
      "86/223, train_loss: 0.1150, step time: 0.1141\n",
      "87/223, train_loss: 0.0927, step time: 0.0992\n",
      "88/223, train_loss: 0.0877, step time: 0.1159\n",
      "89/223, train_loss: 0.0995, step time: 0.0991\n",
      "90/223, train_loss: 0.1026, step time: 0.1100\n",
      "91/223, train_loss: 0.0965, step time: 0.1126\n",
      "92/223, train_loss: 0.1024, step time: 0.1167\n",
      "93/223, train_loss: 0.0984, step time: 0.1248\n",
      "94/223, train_loss: 0.0953, step time: 0.1020\n",
      "95/223, train_loss: 0.0947, step time: 0.1134\n",
      "96/223, train_loss: 0.1050, step time: 0.1008\n",
      "97/223, train_loss: 0.1065, step time: 0.1076\n",
      "98/223, train_loss: 0.0927, step time: 0.1034\n",
      "99/223, train_loss: 0.0983, step time: 0.1055\n",
      "100/223, train_loss: 0.0989, step time: 0.1151\n",
      "101/223, train_loss: 0.0975, step time: 0.1135\n",
      "102/223, train_loss: 0.0947, step time: 0.1120\n",
      "103/223, train_loss: 0.0888, step time: 0.1131\n",
      "104/223, train_loss: 0.0964, step time: 0.1611\n",
      "105/223, train_loss: 0.0935, step time: 0.1325\n",
      "106/223, train_loss: 0.0962, step time: 0.1015\n",
      "107/223, train_loss: 0.0953, step time: 0.1271\n",
      "108/223, train_loss: 0.1146, step time: 0.1113\n",
      "109/223, train_loss: 0.1064, step time: 0.1362\n",
      "110/223, train_loss: 0.1093, step time: 0.1215\n",
      "111/223, train_loss: 0.0974, step time: 0.1302\n",
      "112/223, train_loss: 0.0975, step time: 0.1023\n",
      "113/223, train_loss: 0.0934, step time: 0.1142\n",
      "114/223, train_loss: 0.0941, step time: 0.1283\n",
      "115/223, train_loss: 0.0966, step time: 0.1009\n",
      "116/223, train_loss: 0.2935, step time: 0.1139\n",
      "117/223, train_loss: 0.0911, step time: 0.1343\n",
      "118/223, train_loss: 0.1003, step time: 0.1044\n",
      "119/223, train_loss: 0.1093, step time: 0.1311\n",
      "120/223, train_loss: 0.1013, step time: 0.1064\n",
      "121/223, train_loss: 0.1036, step time: 0.1004\n",
      "122/223, train_loss: 0.1012, step time: 0.1136\n",
      "123/223, train_loss: 0.1107, step time: 0.1001\n",
      "124/223, train_loss: 0.0859, step time: 0.1048\n",
      "125/223, train_loss: 0.0988, step time: 0.1158\n",
      "126/223, train_loss: 0.0952, step time: 0.1124\n",
      "127/223, train_loss: 0.0990, step time: 0.1071\n",
      "128/223, train_loss: 0.0897, step time: 0.1089\n",
      "129/223, train_loss: 0.0961, step time: 0.1162\n",
      "130/223, train_loss: 0.1057, step time: 0.1058\n",
      "131/223, train_loss: 0.0871, step time: 0.1003\n",
      "132/223, train_loss: 0.1078, step time: 0.1606\n",
      "133/223, train_loss: 0.0953, step time: 0.1591\n",
      "134/223, train_loss: 0.0969, step time: 0.1096\n",
      "135/223, train_loss: 0.0899, step time: 0.1032\n",
      "136/223, train_loss: 0.1011, step time: 0.1154\n",
      "137/223, train_loss: 0.0991, step time: 0.1043\n",
      "138/223, train_loss: 0.0921, step time: 0.1123\n",
      "139/223, train_loss: 0.0936, step time: 0.1019\n",
      "140/223, train_loss: 0.1046, step time: 0.1063\n",
      "141/223, train_loss: 0.0954, step time: 0.1119\n",
      "142/223, train_loss: 0.0983, step time: 0.1005\n",
      "143/223, train_loss: 0.0995, step time: 0.1004\n",
      "144/223, train_loss: 0.0996, step time: 0.1043\n",
      "145/223, train_loss: 0.1056, step time: 0.1055\n",
      "146/223, train_loss: 0.1029, step time: 0.1135\n",
      "147/223, train_loss: 0.0951, step time: 0.1041\n",
      "148/223, train_loss: 0.0984, step time: 0.1260\n",
      "149/223, train_loss: 0.1010, step time: 0.1157\n",
      "150/223, train_loss: 0.0981, step time: 0.1187\n",
      "151/223, train_loss: 0.0972, step time: 0.1006\n",
      "152/223, train_loss: 0.1001, step time: 0.1007\n",
      "153/223, train_loss: 0.1097, step time: 0.1032\n",
      "154/223, train_loss: 0.1064, step time: 0.1002\n",
      "155/223, train_loss: 0.0984, step time: 0.1078\n",
      "156/223, train_loss: 0.0953, step time: 0.1023\n",
      "157/223, train_loss: 0.1063, step time: 0.1005\n",
      "158/223, train_loss: 0.0909, step time: 0.0994\n",
      "159/223, train_loss: 0.0878, step time: 0.1173\n",
      "160/223, train_loss: 0.0946, step time: 0.1144\n",
      "161/223, train_loss: 0.0967, step time: 0.1034\n",
      "162/223, train_loss: 0.0906, step time: 0.1106\n",
      "163/223, train_loss: 0.1012, step time: 0.1071\n",
      "164/223, train_loss: 0.0968, step time: 0.1187\n",
      "165/223, train_loss: 0.0992, step time: 0.1025\n",
      "166/223, train_loss: 0.1023, step time: 0.1202\n",
      "167/223, train_loss: 0.0893, step time: 0.1001\n",
      "168/223, train_loss: 0.0995, step time: 0.1241\n",
      "169/223, train_loss: 0.1044, step time: 0.1238\n",
      "170/223, train_loss: 0.0944, step time: 0.1146\n",
      "171/223, train_loss: 0.1064, step time: 0.1094\n",
      "172/223, train_loss: 0.1000, step time: 0.1081\n",
      "173/223, train_loss: 0.0947, step time: 0.1190\n",
      "174/223, train_loss: 0.0944, step time: 0.1375\n",
      "175/223, train_loss: 0.1094, step time: 0.1131\n",
      "176/223, train_loss: 0.0926, step time: 0.1169\n",
      "177/223, train_loss: 0.0921, step time: 0.1666\n",
      "178/223, train_loss: 0.0911, step time: 0.1080\n",
      "179/223, train_loss: 0.1090, step time: 0.1330\n",
      "180/223, train_loss: 0.1039, step time: 0.1234\n",
      "181/223, train_loss: 0.1031, step time: 0.1005\n",
      "182/223, train_loss: 0.1130, step time: 0.0994\n",
      "183/223, train_loss: 0.0862, step time: 0.0984\n",
      "184/223, train_loss: 0.1012, step time: 0.1052\n",
      "185/223, train_loss: 0.0967, step time: 0.1300\n",
      "186/223, train_loss: 0.1055, step time: 0.1004\n",
      "187/223, train_loss: 0.0948, step time: 0.1013\n",
      "188/223, train_loss: 0.0947, step time: 0.1126\n",
      "189/223, train_loss: 0.0988, step time: 0.1228\n",
      "190/223, train_loss: 0.0943, step time: 0.0999\n",
      "191/223, train_loss: 0.0998, step time: 0.0995\n",
      "192/223, train_loss: 0.1009, step time: 0.1004\n",
      "193/223, train_loss: 0.0954, step time: 0.1101\n",
      "194/223, train_loss: 0.1000, step time: 0.0997\n",
      "195/223, train_loss: 0.0985, step time: 0.1000\n",
      "196/223, train_loss: 0.1004, step time: 0.1075\n",
      "197/223, train_loss: 0.0979, step time: 0.1132\n",
      "198/223, train_loss: 0.0979, step time: 0.1053\n",
      "199/223, train_loss: 0.1029, step time: 0.1084\n",
      "200/223, train_loss: 0.1020, step time: 0.1002\n",
      "201/223, train_loss: 0.0942, step time: 0.1004\n",
      "202/223, train_loss: 0.1084, step time: 0.1002\n",
      "203/223, train_loss: 0.1019, step time: 0.1008\n",
      "204/223, train_loss: 0.1165, step time: 0.0992\n",
      "205/223, train_loss: 0.1103, step time: 0.0992\n",
      "206/223, train_loss: 0.1009, step time: 0.1002\n",
      "207/223, train_loss: 0.1066, step time: 0.1319\n",
      "208/223, train_loss: 0.1001, step time: 0.1001\n",
      "209/223, train_loss: 0.0882, step time: 0.0998\n",
      "210/223, train_loss: 0.1018, step time: 0.0997\n",
      "211/223, train_loss: 0.0956, step time: 0.1193\n",
      "212/223, train_loss: 0.1060, step time: 0.1000\n",
      "213/223, train_loss: 0.0995, step time: 0.1012\n",
      "214/223, train_loss: 0.0894, step time: 0.1011\n",
      "215/223, train_loss: 0.1028, step time: 0.1046\n",
      "216/223, train_loss: 0.0890, step time: 0.1068\n",
      "217/223, train_loss: 0.1030, step time: 0.1067\n",
      "218/223, train_loss: 0.0911, step time: 0.1143\n",
      "219/223, train_loss: 0.0934, step time: 0.1077\n",
      "220/223, train_loss: 0.0973, step time: 0.0986\n",
      "221/223, train_loss: 0.1033, step time: 0.0990\n",
      "222/223, train_loss: 0.1070, step time: 0.0992\n",
      "223/223, train_loss: 0.1015, step time: 0.0992\n",
      "epoch 259 average loss: 0.0997\n",
      "time consuming of epoch 259 is: 94.6751\n",
      "----------\n",
      "epoch 260/300\n",
      "1/223, train_loss: 0.1182, step time: 0.1131\n",
      "2/223, train_loss: 0.1013, step time: 0.1256\n",
      "3/223, train_loss: 0.1033, step time: 0.1179\n",
      "4/223, train_loss: 0.0976, step time: 0.1270\n",
      "5/223, train_loss: 0.1034, step time: 0.1158\n",
      "6/223, train_loss: 0.1028, step time: 0.1027\n",
      "7/223, train_loss: 0.0867, step time: 0.1141\n",
      "8/223, train_loss: 0.0967, step time: 0.1531\n",
      "9/223, train_loss: 0.0991, step time: 0.1146\n",
      "10/223, train_loss: 0.0999, step time: 0.1395\n",
      "11/223, train_loss: 0.0916, step time: 0.1306\n",
      "12/223, train_loss: 0.1029, step time: 0.1157\n",
      "13/223, train_loss: 0.0979, step time: 0.1192\n",
      "14/223, train_loss: 0.0944, step time: 0.1046\n",
      "15/223, train_loss: 0.0900, step time: 0.1149\n",
      "16/223, train_loss: 0.0950, step time: 0.1280\n",
      "17/223, train_loss: 0.0946, step time: 0.1177\n",
      "18/223, train_loss: 0.1107, step time: 0.1197\n",
      "19/223, train_loss: 0.1009, step time: 0.1065\n",
      "20/223, train_loss: 0.1191, step time: 0.1022\n",
      "21/223, train_loss: 0.1148, step time: 0.1133\n",
      "22/223, train_loss: 0.0965, step time: 0.1015\n",
      "23/223, train_loss: 0.0977, step time: 0.1003\n",
      "24/223, train_loss: 0.1022, step time: 0.0999\n",
      "25/223, train_loss: 0.1206, step time: 0.1143\n",
      "26/223, train_loss: 0.1013, step time: 0.1142\n",
      "27/223, train_loss: 0.1043, step time: 0.1019\n",
      "28/223, train_loss: 0.0957, step time: 0.1133\n",
      "29/223, train_loss: 0.0999, step time: 0.1128\n",
      "30/223, train_loss: 0.1102, step time: 0.1057\n",
      "31/223, train_loss: 0.1069, step time: 0.1009\n",
      "32/223, train_loss: 0.0979, step time: 0.1106\n",
      "33/223, train_loss: 0.0823, step time: 0.1914\n",
      "34/223, train_loss: 0.0985, step time: 0.1015\n",
      "35/223, train_loss: 0.0992, step time: 0.1479\n",
      "36/223, train_loss: 0.0974, step time: 0.1057\n",
      "37/223, train_loss: 0.0953, step time: 0.1133\n",
      "38/223, train_loss: 0.1165, step time: 0.1154\n",
      "39/223, train_loss: 0.0936, step time: 0.1156\n",
      "40/223, train_loss: 0.0978, step time: 0.1138\n",
      "41/223, train_loss: 0.0934, step time: 0.1360\n",
      "42/223, train_loss: 0.1008, step time: 0.1017\n",
      "43/223, train_loss: 0.0834, step time: 0.1128\n",
      "44/223, train_loss: 0.0958, step time: 0.1147\n",
      "45/223, train_loss: 0.0986, step time: 0.1031\n",
      "46/223, train_loss: 0.0974, step time: 0.1171\n",
      "47/223, train_loss: 0.1049, step time: 0.1008\n",
      "48/223, train_loss: 0.1009, step time: 0.0996\n",
      "49/223, train_loss: 0.0982, step time: 0.1054\n",
      "50/223, train_loss: 0.1021, step time: 0.1067\n",
      "51/223, train_loss: 0.0870, step time: 0.1086\n",
      "52/223, train_loss: 0.1018, step time: 0.1050\n",
      "53/223, train_loss: 0.1029, step time: 0.0995\n",
      "54/223, train_loss: 0.1037, step time: 0.1012\n",
      "55/223, train_loss: 0.0953, step time: 0.0991\n",
      "56/223, train_loss: 0.2916, step time: 0.1262\n",
      "57/223, train_loss: 0.1030, step time: 0.1133\n",
      "58/223, train_loss: 0.1014, step time: 0.1141\n",
      "59/223, train_loss: 0.0963, step time: 0.1100\n",
      "60/223, train_loss: 0.0988, step time: 0.1288\n",
      "61/223, train_loss: 0.0958, step time: 0.1260\n",
      "62/223, train_loss: 0.1055, step time: 0.1147\n",
      "63/223, train_loss: 0.0940, step time: 0.1120\n",
      "64/223, train_loss: 0.0899, step time: 0.1275\n",
      "65/223, train_loss: 0.0976, step time: 0.1045\n",
      "66/223, train_loss: 0.1066, step time: 0.1254\n",
      "67/223, train_loss: 0.0916, step time: 0.1061\n",
      "68/223, train_loss: 0.0982, step time: 0.1040\n",
      "69/223, train_loss: 0.1061, step time: 0.1064\n",
      "70/223, train_loss: 0.1014, step time: 0.1135\n",
      "71/223, train_loss: 0.1068, step time: 0.1069\n",
      "72/223, train_loss: 0.1031, step time: 0.1319\n",
      "73/223, train_loss: 0.1024, step time: 0.1180\n",
      "74/223, train_loss: 0.0973, step time: 0.1094\n",
      "75/223, train_loss: 0.1005, step time: 0.1004\n",
      "76/223, train_loss: 0.0978, step time: 0.1131\n",
      "77/223, train_loss: 0.0890, step time: 0.1072\n",
      "78/223, train_loss: 0.0873, step time: 0.1154\n",
      "79/223, train_loss: 0.1068, step time: 0.1175\n",
      "80/223, train_loss: 0.0981, step time: 0.1259\n",
      "81/223, train_loss: 0.0986, step time: 0.1202\n",
      "82/223, train_loss: 0.1010, step time: 0.1234\n",
      "83/223, train_loss: 0.1054, step time: 0.1112\n",
      "84/223, train_loss: 0.0986, step time: 0.1099\n",
      "85/223, train_loss: 0.0919, step time: 0.1264\n",
      "86/223, train_loss: 0.0950, step time: 0.1008\n",
      "87/223, train_loss: 0.1068, step time: 0.1020\n",
      "88/223, train_loss: 0.1091, step time: 0.1127\n",
      "89/223, train_loss: 0.0925, step time: 0.0992\n",
      "90/223, train_loss: 0.0984, step time: 0.1092\n",
      "91/223, train_loss: 0.1030, step time: 0.1158\n",
      "92/223, train_loss: 0.0951, step time: 0.1224\n",
      "93/223, train_loss: 0.1096, step time: 0.1110\n",
      "94/223, train_loss: 0.1023, step time: 0.1015\n",
      "95/223, train_loss: 0.1022, step time: 0.1635\n",
      "96/223, train_loss: 0.0982, step time: 0.1225\n",
      "97/223, train_loss: 0.0998, step time: 0.1006\n",
      "98/223, train_loss: 0.1029, step time: 0.1176\n",
      "99/223, train_loss: 0.1026, step time: 0.1101\n",
      "100/223, train_loss: 0.1045, step time: 0.1084\n",
      "101/223, train_loss: 0.1062, step time: 0.1132\n",
      "102/223, train_loss: 0.0932, step time: 0.1017\n",
      "103/223, train_loss: 0.1010, step time: 0.1127\n",
      "104/223, train_loss: 0.0992, step time: 0.1021\n",
      "105/223, train_loss: 0.0940, step time: 0.1201\n",
      "106/223, train_loss: 0.0922, step time: 0.1217\n",
      "107/223, train_loss: 0.1108, step time: 0.1174\n",
      "108/223, train_loss: 0.0956, step time: 0.1001\n",
      "109/223, train_loss: 0.0921, step time: 0.0996\n",
      "110/223, train_loss: 0.0838, step time: 0.1128\n",
      "111/223, train_loss: 0.0993, step time: 0.1031\n",
      "112/223, train_loss: 0.1016, step time: 0.1003\n",
      "113/223, train_loss: 0.1086, step time: 0.1015\n",
      "114/223, train_loss: 0.1079, step time: 0.1186\n",
      "115/223, train_loss: 0.1021, step time: 0.1011\n",
      "116/223, train_loss: 0.0980, step time: 0.1007\n",
      "117/223, train_loss: 0.0963, step time: 0.1248\n",
      "118/223, train_loss: 0.0885, step time: 0.1278\n",
      "119/223, train_loss: 0.0963, step time: 0.1215\n",
      "120/223, train_loss: 0.1103, step time: 0.0999\n",
      "121/223, train_loss: 0.0896, step time: 0.1048\n",
      "122/223, train_loss: 0.1047, step time: 0.1150\n",
      "123/223, train_loss: 0.0975, step time: 0.1189\n",
      "124/223, train_loss: 0.0982, step time: 0.0998\n",
      "125/223, train_loss: 0.1048, step time: 0.1140\n",
      "126/223, train_loss: 0.0893, step time: 0.1134\n",
      "127/223, train_loss: 0.1021, step time: 0.1058\n",
      "128/223, train_loss: 0.0984, step time: 0.1357\n",
      "129/223, train_loss: 0.1014, step time: 0.1009\n",
      "130/223, train_loss: 0.1122, step time: 0.1190\n",
      "131/223, train_loss: 0.1003, step time: 0.1006\n",
      "132/223, train_loss: 0.0947, step time: 0.1155\n",
      "133/223, train_loss: 0.0941, step time: 0.1113\n",
      "134/223, train_loss: 0.0960, step time: 0.1261\n",
      "135/223, train_loss: 0.0866, step time: 0.1005\n",
      "136/223, train_loss: 0.1042, step time: 0.1006\n",
      "137/223, train_loss: 0.0949, step time: 0.1236\n",
      "138/223, train_loss: 0.0943, step time: 0.1267\n",
      "139/223, train_loss: 0.0966, step time: 0.1261\n",
      "140/223, train_loss: 0.0951, step time: 0.1056\n",
      "141/223, train_loss: 0.1059, step time: 0.1193\n",
      "142/223, train_loss: 0.1057, step time: 0.1014\n",
      "143/223, train_loss: 0.0897, step time: 0.0999\n",
      "144/223, train_loss: 0.1026, step time: 0.1014\n",
      "145/223, train_loss: 0.0910, step time: 0.1069\n",
      "146/223, train_loss: 0.0982, step time: 0.1176\n",
      "147/223, train_loss: 0.0960, step time: 0.1126\n",
      "148/223, train_loss: 0.0899, step time: 0.1211\n",
      "149/223, train_loss: 0.0915, step time: 0.1209\n",
      "150/223, train_loss: 0.1029, step time: 0.1103\n",
      "151/223, train_loss: 0.0924, step time: 0.1286\n",
      "152/223, train_loss: 0.0978, step time: 0.1101\n",
      "153/223, train_loss: 0.0973, step time: 0.1008\n",
      "154/223, train_loss: 0.0999, step time: 0.1147\n",
      "155/223, train_loss: 0.1014, step time: 0.0999\n",
      "156/223, train_loss: 0.0994, step time: 0.1004\n",
      "157/223, train_loss: 0.1051, step time: 0.1005\n",
      "158/223, train_loss: 0.0920, step time: 0.1276\n",
      "159/223, train_loss: 0.1046, step time: 0.1275\n",
      "160/223, train_loss: 0.0927, step time: 0.1005\n",
      "161/223, train_loss: 0.1057, step time: 0.1166\n",
      "162/223, train_loss: 0.1020, step time: 0.1193\n",
      "163/223, train_loss: 0.0985, step time: 0.1136\n",
      "164/223, train_loss: 0.0933, step time: 0.1132\n",
      "165/223, train_loss: 0.1021, step time: 0.1155\n",
      "166/223, train_loss: 0.0933, step time: 0.1138\n",
      "167/223, train_loss: 0.1039, step time: 0.1128\n",
      "168/223, train_loss: 0.0927, step time: 0.1087\n",
      "169/223, train_loss: 0.0950, step time: 0.1131\n",
      "170/223, train_loss: 0.0957, step time: 0.1126\n",
      "171/223, train_loss: 0.1043, step time: 0.1108\n",
      "172/223, train_loss: 0.1060, step time: 0.1007\n",
      "173/223, train_loss: 0.1020, step time: 0.1126\n",
      "174/223, train_loss: 0.0908, step time: 0.1075\n",
      "175/223, train_loss: 0.1060, step time: 0.1001\n",
      "176/223, train_loss: 0.0834, step time: 0.0998\n",
      "177/223, train_loss: 0.0925, step time: 0.1115\n",
      "178/223, train_loss: 0.0997, step time: 0.1117\n",
      "179/223, train_loss: 0.0902, step time: 0.1204\n",
      "180/223, train_loss: 0.0979, step time: 0.1197\n",
      "181/223, train_loss: 0.0960, step time: 0.1217\n",
      "182/223, train_loss: 0.1023, step time: 0.1252\n",
      "183/223, train_loss: 0.1025, step time: 0.1224\n",
      "184/223, train_loss: 0.0974, step time: 0.1009\n",
      "185/223, train_loss: 0.0929, step time: 0.1141\n",
      "186/223, train_loss: 0.1024, step time: 0.1003\n",
      "187/223, train_loss: 0.0937, step time: 0.1006\n",
      "188/223, train_loss: 0.1022, step time: 0.0998\n",
      "189/223, train_loss: 0.0917, step time: 0.1126\n",
      "190/223, train_loss: 0.1006, step time: 0.1100\n",
      "191/223, train_loss: 0.0880, step time: 0.1268\n",
      "192/223, train_loss: 0.1084, step time: 0.1321\n",
      "193/223, train_loss: 0.0953, step time: 0.1003\n",
      "194/223, train_loss: 0.1007, step time: 0.1089\n",
      "195/223, train_loss: 0.0983, step time: 0.1261\n",
      "196/223, train_loss: 0.0922, step time: 0.1437\n",
      "197/223, train_loss: 0.0994, step time: 0.1372\n",
      "198/223, train_loss: 0.1024, step time: 0.1012\n",
      "199/223, train_loss: 0.0966, step time: 0.1311\n",
      "200/223, train_loss: 0.0995, step time: 0.1032\n",
      "201/223, train_loss: 0.1020, step time: 0.1049\n",
      "202/223, train_loss: 0.1099, step time: 0.1048\n",
      "203/223, train_loss: 0.1054, step time: 0.1078\n",
      "204/223, train_loss: 0.1015, step time: 0.1132\n",
      "205/223, train_loss: 0.1084, step time: 0.1203\n",
      "206/223, train_loss: 0.0918, step time: 0.1184\n",
      "207/223, train_loss: 0.0936, step time: 0.1238\n",
      "208/223, train_loss: 0.1039, step time: 0.1493\n",
      "209/223, train_loss: 0.0933, step time: 0.1087\n",
      "210/223, train_loss: 0.0982, step time: 0.1051\n",
      "211/223, train_loss: 0.0935, step time: 0.1129\n",
      "212/223, train_loss: 0.1065, step time: 0.1284\n",
      "213/223, train_loss: 0.1079, step time: 0.1174\n",
      "214/223, train_loss: 0.0904, step time: 0.1380\n",
      "215/223, train_loss: 0.0842, step time: 0.1290\n",
      "216/223, train_loss: 0.0987, step time: 0.1016\n",
      "217/223, train_loss: 0.0998, step time: 0.1013\n",
      "218/223, train_loss: 0.0863, step time: 0.1004\n",
      "219/223, train_loss: 0.0904, step time: 0.0992\n",
      "220/223, train_loss: 0.1010, step time: 0.0996\n",
      "221/223, train_loss: 0.0923, step time: 0.1001\n",
      "222/223, train_loss: 0.0941, step time: 0.0986\n",
      "223/223, train_loss: 0.0945, step time: 0.0993\n",
      "epoch 260 average loss: 0.0997\n",
      "saved new best metric model\n",
      "current epoch: 260 current mean dice: 0.8622 tc: 0.9225 wt: 0.8718 et: 0.7923\n",
      "best mean dice: 0.8622 at epoch: 260\n",
      "time consuming of epoch 260 is: 92.3472\n",
      "----------\n",
      "epoch 261/300\n",
      "1/223, train_loss: 0.0933, step time: 0.1015\n",
      "2/223, train_loss: 0.0921, step time: 0.1008\n",
      "3/223, train_loss: 0.0887, step time: 0.1005\n",
      "4/223, train_loss: 0.0921, step time: 0.1013\n",
      "5/223, train_loss: 0.1058, step time: 0.1185\n",
      "6/223, train_loss: 0.0956, step time: 0.1216\n",
      "7/223, train_loss: 0.0958, step time: 0.1116\n",
      "8/223, train_loss: 0.1039, step time: 0.1290\n",
      "9/223, train_loss: 0.0993, step time: 0.1024\n",
      "10/223, train_loss: 0.0914, step time: 0.1033\n",
      "11/223, train_loss: 0.0986, step time: 0.1412\n",
      "12/223, train_loss: 0.1083, step time: 0.1238\n",
      "13/223, train_loss: 0.0922, step time: 0.1202\n",
      "14/223, train_loss: 0.0994, step time: 0.1069\n",
      "15/223, train_loss: 0.0974, step time: 0.1002\n",
      "16/223, train_loss: 0.0907, step time: 0.1375\n",
      "17/223, train_loss: 0.0949, step time: 0.1125\n",
      "18/223, train_loss: 0.1076, step time: 0.1000\n",
      "19/223, train_loss: 0.1007, step time: 0.1160\n",
      "20/223, train_loss: 0.0928, step time: 0.1008\n",
      "21/223, train_loss: 0.0928, step time: 0.1140\n",
      "22/223, train_loss: 0.0951, step time: 0.1147\n",
      "23/223, train_loss: 0.0971, step time: 0.1185\n",
      "24/223, train_loss: 0.1174, step time: 0.1048\n",
      "25/223, train_loss: 0.1117, step time: 0.1449\n",
      "26/223, train_loss: 0.0993, step time: 0.1105\n",
      "27/223, train_loss: 0.0901, step time: 0.1337\n",
      "28/223, train_loss: 0.0960, step time: 0.1009\n",
      "29/223, train_loss: 0.0938, step time: 0.1111\n",
      "30/223, train_loss: 0.1158, step time: 0.0998\n",
      "31/223, train_loss: 0.1060, step time: 0.1007\n",
      "32/223, train_loss: 0.1027, step time: 0.1016\n",
      "33/223, train_loss: 0.1060, step time: 0.1042\n",
      "34/223, train_loss: 0.0994, step time: 0.1201\n",
      "35/223, train_loss: 0.0891, step time: 0.1322\n",
      "36/223, train_loss: 0.1010, step time: 0.1297\n",
      "37/223, train_loss: 0.1028, step time: 0.1086\n",
      "38/223, train_loss: 0.1118, step time: 0.1203\n",
      "39/223, train_loss: 0.1097, step time: 0.1240\n",
      "40/223, train_loss: 0.1031, step time: 0.1038\n",
      "41/223, train_loss: 0.1047, step time: 0.1119\n",
      "42/223, train_loss: 0.0931, step time: 0.1014\n",
      "43/223, train_loss: 0.1034, step time: 0.1322\n",
      "44/223, train_loss: 0.1051, step time: 0.1046\n",
      "45/223, train_loss: 0.0926, step time: 0.1161\n",
      "46/223, train_loss: 0.0965, step time: 0.1159\n",
      "47/223, train_loss: 0.0924, step time: 0.1270\n",
      "48/223, train_loss: 0.0969, step time: 0.1062\n",
      "49/223, train_loss: 0.1049, step time: 0.1199\n",
      "50/223, train_loss: 0.0917, step time: 0.1131\n",
      "51/223, train_loss: 0.0984, step time: 0.1448\n",
      "52/223, train_loss: 0.1130, step time: 0.1066\n",
      "53/223, train_loss: 0.1030, step time: 0.1212\n",
      "54/223, train_loss: 0.0943, step time: 0.1008\n",
      "55/223, train_loss: 0.0911, step time: 0.1172\n",
      "56/223, train_loss: 0.0990, step time: 0.0995\n",
      "57/223, train_loss: 0.0941, step time: 0.1033\n",
      "58/223, train_loss: 0.1082, step time: 0.1089\n",
      "59/223, train_loss: 0.1070, step time: 0.1180\n",
      "60/223, train_loss: 0.0936, step time: 0.1201\n",
      "61/223, train_loss: 0.0916, step time: 0.1006\n",
      "62/223, train_loss: 0.0984, step time: 0.1141\n",
      "63/223, train_loss: 0.1096, step time: 0.1387\n",
      "64/223, train_loss: 0.1078, step time: 0.1185\n",
      "65/223, train_loss: 0.0994, step time: 0.1101\n",
      "66/223, train_loss: 0.0981, step time: 0.1199\n",
      "67/223, train_loss: 0.1016, step time: 0.1284\n",
      "68/223, train_loss: 0.0895, step time: 0.1475\n",
      "69/223, train_loss: 0.0944, step time: 0.1102\n",
      "70/223, train_loss: 0.0982, step time: 0.1005\n",
      "71/223, train_loss: 0.1001, step time: 0.1061\n",
      "72/223, train_loss: 0.1037, step time: 0.1008\n",
      "73/223, train_loss: 0.0912, step time: 0.1166\n",
      "74/223, train_loss: 0.1022, step time: 0.1260\n",
      "75/223, train_loss: 0.0996, step time: 0.1241\n",
      "76/223, train_loss: 0.1028, step time: 0.1166\n",
      "77/223, train_loss: 0.1036, step time: 0.1048\n",
      "78/223, train_loss: 0.0859, step time: 0.1052\n",
      "79/223, train_loss: 0.1020, step time: 0.1327\n",
      "80/223, train_loss: 0.1149, step time: 0.1204\n",
      "81/223, train_loss: 0.0909, step time: 0.1235\n",
      "82/223, train_loss: 0.0967, step time: 0.1217\n",
      "83/223, train_loss: 0.1023, step time: 0.1172\n",
      "84/223, train_loss: 0.0994, step time: 0.1260\n",
      "85/223, train_loss: 0.1005, step time: 0.1097\n",
      "86/223, train_loss: 0.1016, step time: 0.1050\n",
      "87/223, train_loss: 0.0861, step time: 0.1201\n",
      "88/223, train_loss: 0.0912, step time: 0.0997\n",
      "89/223, train_loss: 0.0968, step time: 0.1044\n",
      "90/223, train_loss: 0.1030, step time: 0.1119\n",
      "91/223, train_loss: 0.0910, step time: 0.1105\n",
      "92/223, train_loss: 0.0995, step time: 0.1148\n",
      "93/223, train_loss: 0.0932, step time: 0.1501\n",
      "94/223, train_loss: 0.1045, step time: 0.1316\n",
      "95/223, train_loss: 0.0952, step time: 0.1042\n",
      "96/223, train_loss: 0.1004, step time: 0.1188\n",
      "97/223, train_loss: 0.0986, step time: 0.1072\n",
      "98/223, train_loss: 0.0994, step time: 0.0989\n",
      "99/223, train_loss: 0.0938, step time: 0.1152\n",
      "100/223, train_loss: 0.1034, step time: 0.1169\n",
      "101/223, train_loss: 0.0988, step time: 0.1190\n",
      "102/223, train_loss: 0.0915, step time: 0.1061\n",
      "103/223, train_loss: 0.0980, step time: 0.1243\n",
      "104/223, train_loss: 0.0991, step time: 0.1193\n",
      "105/223, train_loss: 0.1007, step time: 0.1250\n",
      "106/223, train_loss: 0.0983, step time: 0.1003\n",
      "107/223, train_loss: 0.1052, step time: 0.1414\n",
      "108/223, train_loss: 0.0912, step time: 0.1333\n",
      "109/223, train_loss: 0.1050, step time: 0.1171\n",
      "110/223, train_loss: 0.0967, step time: 0.1123\n",
      "111/223, train_loss: 0.0986, step time: 0.1106\n",
      "112/223, train_loss: 0.0892, step time: 0.1135\n",
      "113/223, train_loss: 0.0923, step time: 0.1207\n",
      "114/223, train_loss: 0.0941, step time: 0.1248\n",
      "115/223, train_loss: 0.0961, step time: 0.1107\n",
      "116/223, train_loss: 0.1024, step time: 0.1004\n",
      "117/223, train_loss: 0.1031, step time: 0.1101\n",
      "118/223, train_loss: 0.1011, step time: 0.1243\n",
      "119/223, train_loss: 0.0871, step time: 0.1110\n",
      "120/223, train_loss: 0.1010, step time: 0.1300\n",
      "121/223, train_loss: 0.0904, step time: 0.1481\n",
      "122/223, train_loss: 0.1007, step time: 0.1268\n",
      "123/223, train_loss: 0.1014, step time: 0.1268\n",
      "124/223, train_loss: 0.1020, step time: 0.1052\n",
      "125/223, train_loss: 0.1079, step time: 0.0988\n",
      "126/223, train_loss: 0.1050, step time: 0.0996\n",
      "127/223, train_loss: 0.1157, step time: 0.0995\n",
      "128/223, train_loss: 0.0905, step time: 0.0999\n",
      "129/223, train_loss: 0.1068, step time: 0.1097\n",
      "130/223, train_loss: 0.0947, step time: 0.1092\n",
      "131/223, train_loss: 0.0994, step time: 0.1303\n",
      "132/223, train_loss: 0.1034, step time: 0.0995\n",
      "133/223, train_loss: 0.0842, step time: 0.1105\n",
      "134/223, train_loss: 0.1088, step time: 0.1008\n",
      "135/223, train_loss: 0.0947, step time: 0.1375\n",
      "136/223, train_loss: 0.1024, step time: 0.1121\n",
      "137/223, train_loss: 0.1010, step time: 0.1144\n",
      "138/223, train_loss: 0.1023, step time: 0.1057\n",
      "139/223, train_loss: 0.1042, step time: 0.1250\n",
      "140/223, train_loss: 0.0875, step time: 0.1069\n",
      "141/223, train_loss: 0.0987, step time: 0.1083\n",
      "142/223, train_loss: 0.1000, step time: 0.1138\n",
      "143/223, train_loss: 0.0961, step time: 0.1315\n",
      "144/223, train_loss: 0.0968, step time: 0.1082\n",
      "145/223, train_loss: 0.1078, step time: 0.1118\n",
      "146/223, train_loss: 0.1043, step time: 0.1283\n",
      "147/223, train_loss: 0.0951, step time: 0.1155\n",
      "148/223, train_loss: 0.0911, step time: 0.1182\n",
      "149/223, train_loss: 0.0964, step time: 0.1151\n",
      "150/223, train_loss: 0.0910, step time: 0.1438\n",
      "151/223, train_loss: 0.0940, step time: 0.1247\n",
      "152/223, train_loss: 0.0944, step time: 0.1000\n",
      "153/223, train_loss: 0.1041, step time: 0.1127\n",
      "154/223, train_loss: 0.1020, step time: 0.1238\n",
      "155/223, train_loss: 0.2903, step time: 0.1557\n",
      "156/223, train_loss: 0.1184, step time: 0.1036\n",
      "157/223, train_loss: 0.0979, step time: 0.1004\n",
      "158/223, train_loss: 0.0960, step time: 0.1002\n",
      "159/223, train_loss: 0.1019, step time: 0.1101\n",
      "160/223, train_loss: 0.0929, step time: 0.1021\n",
      "161/223, train_loss: 0.0974, step time: 0.1077\n",
      "162/223, train_loss: 0.1001, step time: 0.1071\n",
      "163/223, train_loss: 0.1085, step time: 0.1003\n",
      "164/223, train_loss: 0.1017, step time: 0.1083\n",
      "165/223, train_loss: 0.0897, step time: 0.1127\n",
      "166/223, train_loss: 0.1034, step time: 0.1195\n",
      "167/223, train_loss: 0.0910, step time: 0.1138\n",
      "168/223, train_loss: 0.0982, step time: 0.1290\n",
      "169/223, train_loss: 0.1023, step time: 0.1196\n",
      "170/223, train_loss: 0.1144, step time: 0.1023\n",
      "171/223, train_loss: 0.0948, step time: 0.1260\n",
      "172/223, train_loss: 0.1009, step time: 0.1268\n",
      "173/223, train_loss: 0.0861, step time: 0.1070\n",
      "174/223, train_loss: 0.0933, step time: 0.1283\n",
      "175/223, train_loss: 0.1007, step time: 0.1142\n",
      "176/223, train_loss: 0.1034, step time: 0.1395\n",
      "177/223, train_loss: 0.0924, step time: 0.1272\n",
      "178/223, train_loss: 0.0939, step time: 0.1062\n",
      "179/223, train_loss: 0.1081, step time: 0.1180\n",
      "180/223, train_loss: 0.1029, step time: 0.1183\n",
      "181/223, train_loss: 0.0895, step time: 0.1017\n",
      "182/223, train_loss: 0.0930, step time: 0.1127\n",
      "183/223, train_loss: 0.0981, step time: 0.1370\n",
      "184/223, train_loss: 0.1010, step time: 0.1145\n",
      "185/223, train_loss: 0.1010, step time: 0.1008\n",
      "186/223, train_loss: 0.0984, step time: 0.1061\n",
      "187/223, train_loss: 0.0969, step time: 0.1067\n",
      "188/223, train_loss: 0.0885, step time: 0.1002\n",
      "189/223, train_loss: 0.0935, step time: 0.1097\n",
      "190/223, train_loss: 0.1051, step time: 0.1002\n",
      "191/223, train_loss: 0.1034, step time: 0.0996\n",
      "192/223, train_loss: 0.0938, step time: 0.1133\n",
      "193/223, train_loss: 0.0949, step time: 0.1124\n",
      "194/223, train_loss: 0.1038, step time: 0.1111\n",
      "195/223, train_loss: 0.0973, step time: 0.1276\n",
      "196/223, train_loss: 0.1016, step time: 0.1249\n",
      "197/223, train_loss: 0.1022, step time: 0.1150\n",
      "198/223, train_loss: 0.1013, step time: 0.1169\n",
      "199/223, train_loss: 0.1023, step time: 0.1005\n",
      "200/223, train_loss: 0.0975, step time: 0.1173\n",
      "201/223, train_loss: 0.0912, step time: 0.1232\n",
      "202/223, train_loss: 0.0975, step time: 0.1002\n",
      "203/223, train_loss: 0.0905, step time: 0.1187\n",
      "204/223, train_loss: 0.0955, step time: 0.1059\n",
      "205/223, train_loss: 0.0970, step time: 0.1229\n",
      "206/223, train_loss: 0.0980, step time: 0.1143\n",
      "207/223, train_loss: 0.1063, step time: 0.1199\n",
      "208/223, train_loss: 0.1047, step time: 0.1330\n",
      "209/223, train_loss: 0.0992, step time: 0.1128\n",
      "210/223, train_loss: 0.0943, step time: 0.1108\n",
      "211/223, train_loss: 0.1007, step time: 0.1168\n",
      "212/223, train_loss: 0.1073, step time: 0.1048\n",
      "213/223, train_loss: 0.0994, step time: 0.1059\n",
      "214/223, train_loss: 0.0993, step time: 0.1102\n",
      "215/223, train_loss: 0.1013, step time: 0.1119\n",
      "216/223, train_loss: 0.1097, step time: 0.1201\n",
      "217/223, train_loss: 0.0882, step time: 0.1180\n",
      "218/223, train_loss: 0.1093, step time: 0.1110\n",
      "219/223, train_loss: 0.0962, step time: 0.1158\n",
      "220/223, train_loss: 0.1074, step time: 0.0992\n",
      "221/223, train_loss: 0.0975, step time: 0.0998\n",
      "222/223, train_loss: 0.1019, step time: 0.1001\n",
      "223/223, train_loss: 0.0946, step time: 0.1002\n",
      "epoch 261 average loss: 0.0998\n",
      "time consuming of epoch 261 is: 89.3386\n",
      "----------\n",
      "epoch 262/300\n",
      "1/223, train_loss: 0.1165, step time: 0.1162\n",
      "2/223, train_loss: 0.0955, step time: 0.1002\n",
      "3/223, train_loss: 0.1102, step time: 0.0986\n",
      "4/223, train_loss: 0.0993, step time: 0.1389\n",
      "5/223, train_loss: 0.0917, step time: 0.1404\n",
      "6/223, train_loss: 0.1058, step time: 0.1518\n",
      "7/223, train_loss: 0.0996, step time: 0.1298\n",
      "8/223, train_loss: 0.0978, step time: 0.0996\n",
      "9/223, train_loss: 0.0917, step time: 0.1260\n",
      "10/223, train_loss: 0.1055, step time: 0.1099\n",
      "11/223, train_loss: 0.0971, step time: 0.1081\n",
      "12/223, train_loss: 0.0966, step time: 0.1009\n",
      "13/223, train_loss: 0.0910, step time: 0.0996\n",
      "14/223, train_loss: 0.0947, step time: 0.1070\n",
      "15/223, train_loss: 0.0951, step time: 0.1155\n",
      "16/223, train_loss: 0.1049, step time: 0.1330\n",
      "17/223, train_loss: 0.0983, step time: 0.1194\n",
      "18/223, train_loss: 0.1109, step time: 0.1108\n",
      "19/223, train_loss: 0.0909, step time: 0.1274\n",
      "20/223, train_loss: 0.1043, step time: 0.1359\n",
      "21/223, train_loss: 0.0984, step time: 0.1223\n",
      "22/223, train_loss: 0.1009, step time: 0.1087\n",
      "23/223, train_loss: 0.0991, step time: 0.1148\n",
      "24/223, train_loss: 0.0938, step time: 0.1034\n",
      "25/223, train_loss: 0.1038, step time: 0.1157\n",
      "26/223, train_loss: 0.0879, step time: 0.1006\n",
      "27/223, train_loss: 0.0867, step time: 0.1007\n",
      "28/223, train_loss: 0.1009, step time: 0.1161\n",
      "29/223, train_loss: 0.0913, step time: 0.1135\n",
      "30/223, train_loss: 0.1012, step time: 0.1079\n",
      "31/223, train_loss: 0.1064, step time: 0.1266\n",
      "32/223, train_loss: 0.0996, step time: 0.1444\n",
      "33/223, train_loss: 0.1062, step time: 0.1047\n",
      "34/223, train_loss: 0.0946, step time: 0.1006\n",
      "35/223, train_loss: 0.1045, step time: 0.1008\n",
      "36/223, train_loss: 0.0877, step time: 0.1126\n",
      "37/223, train_loss: 0.0909, step time: 0.1398\n",
      "38/223, train_loss: 0.1024, step time: 0.1118\n",
      "39/223, train_loss: 0.0951, step time: 0.1040\n",
      "40/223, train_loss: 0.0945, step time: 0.1017\n",
      "41/223, train_loss: 0.0936, step time: 0.1010\n",
      "42/223, train_loss: 0.0962, step time: 0.1003\n",
      "43/223, train_loss: 0.0879, step time: 0.1057\n",
      "44/223, train_loss: 0.0912, step time: 0.1004\n",
      "45/223, train_loss: 0.0909, step time: 0.1076\n",
      "46/223, train_loss: 0.0935, step time: 0.1263\n",
      "47/223, train_loss: 0.1036, step time: 0.1045\n",
      "48/223, train_loss: 0.0914, step time: 0.1119\n",
      "49/223, train_loss: 0.0856, step time: 0.1136\n",
      "50/223, train_loss: 0.1034, step time: 0.1142\n",
      "51/223, train_loss: 0.1164, step time: 0.1007\n",
      "52/223, train_loss: 0.0869, step time: 0.0995\n",
      "53/223, train_loss: 0.0901, step time: 0.1222\n",
      "54/223, train_loss: 0.1051, step time: 0.1170\n",
      "55/223, train_loss: 0.0968, step time: 0.1177\n",
      "56/223, train_loss: 0.1073, step time: 0.1222\n",
      "57/223, train_loss: 0.1049, step time: 0.1076\n",
      "58/223, train_loss: 0.1042, step time: 0.1008\n",
      "59/223, train_loss: 0.0963, step time: 0.1011\n",
      "60/223, train_loss: 0.0974, step time: 0.1018\n",
      "61/223, train_loss: 0.0962, step time: 0.1120\n",
      "62/223, train_loss: 0.0964, step time: 0.1003\n",
      "63/223, train_loss: 0.1021, step time: 0.1008\n",
      "64/223, train_loss: 0.1043, step time: 0.1011\n",
      "65/223, train_loss: 0.0950, step time: 0.1169\n",
      "66/223, train_loss: 0.1018, step time: 0.1254\n",
      "67/223, train_loss: 0.0899, step time: 0.1040\n",
      "68/223, train_loss: 0.1043, step time: 0.1092\n",
      "69/223, train_loss: 0.0916, step time: 0.1184\n",
      "70/223, train_loss: 0.1005, step time: 0.1091\n",
      "71/223, train_loss: 0.0944, step time: 0.1209\n",
      "72/223, train_loss: 0.0936, step time: 0.1197\n",
      "73/223, train_loss: 0.0987, step time: 0.1157\n",
      "74/223, train_loss: 0.0915, step time: 0.1267\n",
      "75/223, train_loss: 0.1076, step time: 0.1004\n",
      "76/223, train_loss: 0.0920, step time: 0.1001\n",
      "77/223, train_loss: 0.0935, step time: 0.1127\n",
      "78/223, train_loss: 0.0954, step time: 0.1279\n",
      "79/223, train_loss: 0.0984, step time: 0.1213\n",
      "80/223, train_loss: 0.0925, step time: 0.1309\n",
      "81/223, train_loss: 0.0961, step time: 0.1064\n",
      "82/223, train_loss: 0.2985, step time: 0.1020\n",
      "83/223, train_loss: 0.1059, step time: 0.1005\n",
      "84/223, train_loss: 0.1028, step time: 0.1013\n",
      "85/223, train_loss: 0.0977, step time: 0.1002\n",
      "86/223, train_loss: 0.0938, step time: 0.1139\n",
      "87/223, train_loss: 0.1067, step time: 0.1147\n",
      "88/223, train_loss: 0.1059, step time: 0.1107\n",
      "89/223, train_loss: 0.0993, step time: 0.1145\n",
      "90/223, train_loss: 0.0929, step time: 0.1162\n",
      "91/223, train_loss: 0.0974, step time: 0.1157\n",
      "92/223, train_loss: 0.1064, step time: 0.1483\n",
      "93/223, train_loss: 0.0950, step time: 0.1466\n",
      "94/223, train_loss: 0.0995, step time: 0.1160\n",
      "95/223, train_loss: 0.1132, step time: 0.1163\n",
      "96/223, train_loss: 0.1078, step time: 0.1177\n",
      "97/223, train_loss: 0.1062, step time: 0.1067\n",
      "98/223, train_loss: 0.1023, step time: 0.1263\n",
      "99/223, train_loss: 0.0942, step time: 0.1176\n",
      "100/223, train_loss: 0.0929, step time: 0.1004\n",
      "101/223, train_loss: 0.1218, step time: 0.1120\n",
      "102/223, train_loss: 0.0970, step time: 0.1141\n",
      "103/223, train_loss: 0.0984, step time: 0.1181\n",
      "104/223, train_loss: 0.0943, step time: 0.1049\n",
      "105/223, train_loss: 0.0962, step time: 0.1144\n",
      "106/223, train_loss: 0.0890, step time: 0.1065\n",
      "107/223, train_loss: 0.1019, step time: 0.1540\n",
      "108/223, train_loss: 0.0958, step time: 0.1576\n",
      "109/223, train_loss: 0.0924, step time: 0.1130\n",
      "110/223, train_loss: 0.0961, step time: 0.1151\n",
      "111/223, train_loss: 0.1058, step time: 0.1030\n",
      "112/223, train_loss: 0.1057, step time: 0.1177\n",
      "113/223, train_loss: 0.1003, step time: 0.1050\n",
      "114/223, train_loss: 0.1161, step time: 0.1169\n",
      "115/223, train_loss: 0.1029, step time: 0.1361\n",
      "116/223, train_loss: 0.1111, step time: 0.1470\n",
      "117/223, train_loss: 0.0994, step time: 0.1095\n",
      "118/223, train_loss: 0.0863, step time: 0.1170\n",
      "119/223, train_loss: 0.1001, step time: 0.1310\n",
      "120/223, train_loss: 0.1059, step time: 0.1178\n",
      "121/223, train_loss: 0.0972, step time: 0.1166\n",
      "122/223, train_loss: 0.1117, step time: 0.1254\n",
      "123/223, train_loss: 0.1014, step time: 0.1282\n",
      "124/223, train_loss: 0.0891, step time: 0.1004\n",
      "125/223, train_loss: 0.0977, step time: 0.1100\n",
      "126/223, train_loss: 0.1042, step time: 0.1064\n",
      "127/223, train_loss: 0.0936, step time: 0.1394\n",
      "128/223, train_loss: 0.0921, step time: 0.1132\n",
      "129/223, train_loss: 0.1026, step time: 0.1048\n",
      "130/223, train_loss: 0.1030, step time: 0.1292\n",
      "131/223, train_loss: 0.0956, step time: 0.1165\n",
      "132/223, train_loss: 0.1006, step time: 0.0997\n",
      "133/223, train_loss: 0.0968, step time: 0.0996\n",
      "134/223, train_loss: 0.1018, step time: 0.0999\n",
      "135/223, train_loss: 0.0998, step time: 0.0998\n",
      "136/223, train_loss: 0.0875, step time: 0.1000\n",
      "137/223, train_loss: 0.1153, step time: 0.1000\n",
      "138/223, train_loss: 0.0905, step time: 0.1002\n",
      "139/223, train_loss: 0.0975, step time: 0.0997\n",
      "140/223, train_loss: 0.0926, step time: 0.1169\n",
      "141/223, train_loss: 0.1008, step time: 0.1111\n",
      "142/223, train_loss: 0.0959, step time: 0.1011\n",
      "143/223, train_loss: 0.0972, step time: 0.1265\n",
      "144/223, train_loss: 0.0988, step time: 0.1204\n",
      "145/223, train_loss: 0.0978, step time: 0.1006\n",
      "146/223, train_loss: 0.0954, step time: 0.1005\n",
      "147/223, train_loss: 0.0946, step time: 0.1004\n",
      "148/223, train_loss: 0.0925, step time: 0.1230\n",
      "149/223, train_loss: 0.0984, step time: 0.1003\n",
      "150/223, train_loss: 0.1080, step time: 0.1025\n",
      "151/223, train_loss: 0.1054, step time: 0.1058\n",
      "152/223, train_loss: 0.0935, step time: 0.1159\n",
      "153/223, train_loss: 0.0975, step time: 0.1148\n",
      "154/223, train_loss: 0.0956, step time: 0.1416\n",
      "155/223, train_loss: 0.1038, step time: 0.1355\n",
      "156/223, train_loss: 0.0875, step time: 0.1184\n",
      "157/223, train_loss: 0.0999, step time: 0.1080\n",
      "158/223, train_loss: 0.1112, step time: 0.1016\n",
      "159/223, train_loss: 0.0957, step time: 0.1026\n",
      "160/223, train_loss: 0.0993, step time: 0.1188\n",
      "161/223, train_loss: 0.1050, step time: 0.1158\n",
      "162/223, train_loss: 0.1049, step time: 0.1011\n",
      "163/223, train_loss: 0.0943, step time: 0.1121\n",
      "164/223, train_loss: 0.1066, step time: 0.1218\n",
      "165/223, train_loss: 0.0900, step time: 0.1095\n",
      "166/223, train_loss: 0.1083, step time: 0.1002\n",
      "167/223, train_loss: 0.0903, step time: 0.1365\n",
      "168/223, train_loss: 0.0953, step time: 0.1203\n",
      "169/223, train_loss: 0.0929, step time: 0.1124\n",
      "170/223, train_loss: 0.1048, step time: 0.1165\n",
      "171/223, train_loss: 0.1012, step time: 0.1019\n",
      "172/223, train_loss: 0.0883, step time: 0.1093\n",
      "173/223, train_loss: 0.1102, step time: 0.1081\n",
      "174/223, train_loss: 0.0932, step time: 0.1145\n",
      "175/223, train_loss: 0.0902, step time: 0.1094\n",
      "176/223, train_loss: 0.0928, step time: 0.1006\n",
      "177/223, train_loss: 0.0984, step time: 0.1119\n",
      "178/223, train_loss: 0.0892, step time: 0.1196\n",
      "179/223, train_loss: 0.1013, step time: 0.1067\n",
      "180/223, train_loss: 0.1138, step time: 0.1212\n",
      "181/223, train_loss: 0.0979, step time: 0.1127\n",
      "182/223, train_loss: 0.0938, step time: 0.1116\n",
      "183/223, train_loss: 0.1021, step time: 0.1164\n",
      "184/223, train_loss: 0.0938, step time: 0.1556\n",
      "185/223, train_loss: 0.0964, step time: 0.1107\n",
      "186/223, train_loss: 0.0959, step time: 0.1172\n",
      "187/223, train_loss: 0.0863, step time: 0.1230\n",
      "188/223, train_loss: 0.0965, step time: 0.1220\n",
      "189/223, train_loss: 0.1040, step time: 0.1158\n",
      "190/223, train_loss: 0.0977, step time: 0.1047\n",
      "191/223, train_loss: 0.1070, step time: 0.1051\n",
      "192/223, train_loss: 0.1095, step time: 0.1167\n",
      "193/223, train_loss: 0.1004, step time: 0.1050\n",
      "194/223, train_loss: 0.1040, step time: 0.1189\n",
      "195/223, train_loss: 0.0955, step time: 0.1315\n",
      "196/223, train_loss: 0.0867, step time: 0.1461\n",
      "197/223, train_loss: 0.0942, step time: 0.1302\n",
      "198/223, train_loss: 0.1081, step time: 0.1004\n",
      "199/223, train_loss: 0.1068, step time: 0.1015\n",
      "200/223, train_loss: 0.1100, step time: 0.1180\n",
      "201/223, train_loss: 0.0938, step time: 0.1112\n",
      "202/223, train_loss: 0.1009, step time: 0.1012\n",
      "203/223, train_loss: 0.0968, step time: 0.1203\n",
      "204/223, train_loss: 0.0952, step time: 0.1131\n",
      "205/223, train_loss: 0.0990, step time: 0.1172\n",
      "206/223, train_loss: 0.1173, step time: 0.1109\n",
      "207/223, train_loss: 0.0981, step time: 0.1277\n",
      "208/223, train_loss: 0.1084, step time: 0.1081\n",
      "209/223, train_loss: 0.0924, step time: 0.1235\n",
      "210/223, train_loss: 0.1013, step time: 0.1096\n",
      "211/223, train_loss: 0.1076, step time: 0.1003\n",
      "212/223, train_loss: 0.1021, step time: 0.1285\n",
      "213/223, train_loss: 0.1048, step time: 0.1299\n",
      "214/223, train_loss: 0.1033, step time: 0.1078\n",
      "215/223, train_loss: 0.1006, step time: 0.1121\n",
      "216/223, train_loss: 0.0982, step time: 0.1511\n",
      "217/223, train_loss: 0.1024, step time: 0.1012\n",
      "218/223, train_loss: 0.0989, step time: 0.1002\n",
      "219/223, train_loss: 0.1046, step time: 0.1003\n",
      "220/223, train_loss: 0.0985, step time: 0.1000\n",
      "221/223, train_loss: 0.0915, step time: 0.0997\n",
      "222/223, train_loss: 0.1138, step time: 0.0993\n",
      "223/223, train_loss: 0.0995, step time: 0.0996\n",
      "epoch 262 average loss: 0.0999\n",
      "time consuming of epoch 262 is: 90.8570\n",
      "----------\n",
      "epoch 263/300\n",
      "1/223, train_loss: 0.1023, step time: 0.1042\n",
      "2/223, train_loss: 0.1038, step time: 0.1095\n",
      "3/223, train_loss: 0.0910, step time: 0.1049\n",
      "4/223, train_loss: 0.0937, step time: 0.1100\n",
      "5/223, train_loss: 0.0850, step time: 0.1141\n",
      "6/223, train_loss: 0.0966, step time: 0.1102\n",
      "7/223, train_loss: 0.0966, step time: 0.0992\n",
      "8/223, train_loss: 0.0946, step time: 0.0992\n",
      "9/223, train_loss: 0.0881, step time: 0.1173\n",
      "10/223, train_loss: 0.1041, step time: 0.1035\n",
      "11/223, train_loss: 0.0976, step time: 0.1139\n",
      "12/223, train_loss: 0.0953, step time: 0.0991\n",
      "13/223, train_loss: 0.0919, step time: 0.1222\n",
      "14/223, train_loss: 0.1066, step time: 0.1080\n",
      "15/223, train_loss: 0.0887, step time: 0.1196\n",
      "16/223, train_loss: 0.0950, step time: 0.1071\n",
      "17/223, train_loss: 0.0903, step time: 0.1057\n",
      "18/223, train_loss: 0.0927, step time: 0.1147\n",
      "19/223, train_loss: 0.0984, step time: 0.1082\n",
      "20/223, train_loss: 0.1036, step time: 0.1114\n",
      "21/223, train_loss: 0.1176, step time: 0.1158\n",
      "22/223, train_loss: 0.1017, step time: 0.0995\n",
      "23/223, train_loss: 0.1021, step time: 0.1451\n",
      "24/223, train_loss: 0.1003, step time: 0.1282\n",
      "25/223, train_loss: 0.1132, step time: 0.1180\n",
      "26/223, train_loss: 0.0976, step time: 0.1533\n",
      "27/223, train_loss: 0.0870, step time: 0.1556\n",
      "28/223, train_loss: 0.0977, step time: 0.1048\n",
      "29/223, train_loss: 0.0950, step time: 0.1145\n",
      "30/223, train_loss: 0.0993, step time: 0.1099\n",
      "31/223, train_loss: 0.1081, step time: 0.1143\n",
      "32/223, train_loss: 0.0913, step time: 0.1715\n",
      "33/223, train_loss: 0.0933, step time: 0.1065\n",
      "34/223, train_loss: 0.1101, step time: 0.1502\n",
      "35/223, train_loss: 0.1176, step time: 0.1516\n",
      "36/223, train_loss: 0.1027, step time: 0.1078\n",
      "37/223, train_loss: 0.1040, step time: 0.1073\n",
      "38/223, train_loss: 0.0923, step time: 0.1082\n",
      "39/223, train_loss: 0.0929, step time: 0.1645\n",
      "40/223, train_loss: 0.0966, step time: 0.1038\n",
      "41/223, train_loss: 0.0959, step time: 0.0994\n",
      "42/223, train_loss: 0.0982, step time: 0.1000\n",
      "43/223, train_loss: 0.0967, step time: 0.1006\n",
      "44/223, train_loss: 0.0917, step time: 0.1028\n",
      "45/223, train_loss: 0.0976, step time: 0.0994\n",
      "46/223, train_loss: 0.1055, step time: 0.0996\n",
      "47/223, train_loss: 0.0951, step time: 0.1001\n",
      "48/223, train_loss: 0.0883, step time: 0.1011\n",
      "49/223, train_loss: 0.0985, step time: 0.0997\n",
      "50/223, train_loss: 0.0942, step time: 0.1005\n",
      "51/223, train_loss: 0.1061, step time: 0.1007\n",
      "52/223, train_loss: 0.0971, step time: 0.1347\n",
      "53/223, train_loss: 0.0911, step time: 0.1008\n",
      "54/223, train_loss: 0.1033, step time: 0.0999\n",
      "55/223, train_loss: 0.0966, step time: 0.1077\n",
      "56/223, train_loss: 0.1092, step time: 0.1100\n",
      "57/223, train_loss: 0.0902, step time: 0.1260\n",
      "58/223, train_loss: 0.1019, step time: 0.1006\n",
      "59/223, train_loss: 0.1076, step time: 0.1260\n",
      "60/223, train_loss: 0.0954, step time: 0.1113\n",
      "61/223, train_loss: 0.1121, step time: 0.1057\n",
      "62/223, train_loss: 0.1020, step time: 0.1126\n",
      "63/223, train_loss: 0.1013, step time: 0.1154\n",
      "64/223, train_loss: 0.0940, step time: 0.1137\n",
      "65/223, train_loss: 0.0990, step time: 0.1522\n",
      "66/223, train_loss: 0.0902, step time: 0.1113\n",
      "67/223, train_loss: 0.0938, step time: 0.1337\n",
      "68/223, train_loss: 0.0913, step time: 0.1147\n",
      "69/223, train_loss: 0.1089, step time: 0.1117\n",
      "70/223, train_loss: 0.1033, step time: 0.1201\n",
      "71/223, train_loss: 0.0904, step time: 0.1053\n",
      "72/223, train_loss: 0.0950, step time: 0.1040\n",
      "73/223, train_loss: 0.1052, step time: 0.1053\n",
      "74/223, train_loss: 0.1055, step time: 0.1119\n",
      "75/223, train_loss: 0.0938, step time: 0.1240\n",
      "76/223, train_loss: 0.0873, step time: 0.1162\n",
      "77/223, train_loss: 0.0992, step time: 0.1072\n",
      "78/223, train_loss: 0.0957, step time: 0.0999\n",
      "79/223, train_loss: 0.0981, step time: 0.1309\n",
      "80/223, train_loss: 0.0984, step time: 0.1286\n",
      "81/223, train_loss: 0.1075, step time: 0.1069\n",
      "82/223, train_loss: 0.1047, step time: 0.1094\n",
      "83/223, train_loss: 0.0905, step time: 0.1262\n",
      "84/223, train_loss: 0.0941, step time: 0.1162\n",
      "85/223, train_loss: 0.1006, step time: 0.1200\n",
      "86/223, train_loss: 0.1033, step time: 0.0989\n",
      "87/223, train_loss: 0.1090, step time: 0.1205\n",
      "88/223, train_loss: 0.1029, step time: 0.1034\n",
      "89/223, train_loss: 0.0877, step time: 0.1094\n",
      "90/223, train_loss: 0.1043, step time: 0.1157\n",
      "91/223, train_loss: 0.2990, step time: 0.1229\n",
      "92/223, train_loss: 0.1076, step time: 0.1248\n",
      "93/223, train_loss: 0.1034, step time: 0.1131\n",
      "94/223, train_loss: 0.0985, step time: 0.1132\n",
      "95/223, train_loss: 0.0967, step time: 0.1264\n",
      "96/223, train_loss: 0.0907, step time: 0.1211\n",
      "97/223, train_loss: 0.0959, step time: 0.1090\n",
      "98/223, train_loss: 0.0919, step time: 0.1095\n",
      "99/223, train_loss: 0.1099, step time: 0.1241\n",
      "100/223, train_loss: 0.1009, step time: 0.0995\n",
      "101/223, train_loss: 0.0988, step time: 0.1146\n",
      "102/223, train_loss: 0.1098, step time: 0.1071\n",
      "103/223, train_loss: 0.0979, step time: 0.1061\n",
      "104/223, train_loss: 0.1055, step time: 0.1080\n",
      "105/223, train_loss: 0.0993, step time: 0.1213\n",
      "106/223, train_loss: 0.0895, step time: 0.1103\n",
      "107/223, train_loss: 0.0912, step time: 0.1113\n",
      "108/223, train_loss: 0.0974, step time: 0.1044\n",
      "109/223, train_loss: 0.1026, step time: 0.1024\n",
      "110/223, train_loss: 0.1080, step time: 0.1402\n",
      "111/223, train_loss: 0.0954, step time: 0.1150\n",
      "112/223, train_loss: 0.0998, step time: 0.1124\n",
      "113/223, train_loss: 0.1162, step time: 0.1107\n",
      "114/223, train_loss: 0.0978, step time: 0.1507\n",
      "115/223, train_loss: 0.0986, step time: 0.1215\n",
      "116/223, train_loss: 0.0995, step time: 0.1146\n",
      "117/223, train_loss: 0.1018, step time: 0.1100\n",
      "118/223, train_loss: 0.1011, step time: 0.1034\n",
      "119/223, train_loss: 0.0914, step time: 0.1214\n",
      "120/223, train_loss: 0.0943, step time: 0.1304\n",
      "121/223, train_loss: 0.1161, step time: 0.1205\n",
      "122/223, train_loss: 0.0927, step time: 0.1146\n",
      "123/223, train_loss: 0.0951, step time: 0.1077\n",
      "124/223, train_loss: 0.0996, step time: 0.1280\n",
      "125/223, train_loss: 0.0885, step time: 0.0998\n",
      "126/223, train_loss: 0.0897, step time: 0.1093\n",
      "127/223, train_loss: 0.0973, step time: 0.1011\n",
      "128/223, train_loss: 0.1053, step time: 0.1139\n",
      "129/223, train_loss: 0.1117, step time: 0.1082\n",
      "130/223, train_loss: 0.1040, step time: 0.1163\n",
      "131/223, train_loss: 0.1018, step time: 0.1509\n",
      "132/223, train_loss: 0.0995, step time: 0.1209\n",
      "133/223, train_loss: 0.1095, step time: 0.0997\n",
      "134/223, train_loss: 0.0939, step time: 0.1008\n",
      "135/223, train_loss: 0.0899, step time: 0.1209\n",
      "136/223, train_loss: 0.0992, step time: 0.1086\n",
      "137/223, train_loss: 0.1006, step time: 0.1131\n",
      "138/223, train_loss: 0.1021, step time: 0.0997\n",
      "139/223, train_loss: 0.0987, step time: 0.1229\n",
      "140/223, train_loss: 0.1021, step time: 0.1390\n",
      "141/223, train_loss: 0.1104, step time: 0.1120\n",
      "142/223, train_loss: 0.0889, step time: 0.1234\n",
      "143/223, train_loss: 0.0922, step time: 0.1122\n",
      "144/223, train_loss: 0.1018, step time: 0.1174\n",
      "145/223, train_loss: 0.1020, step time: 0.1230\n",
      "146/223, train_loss: 0.1005, step time: 0.1007\n",
      "147/223, train_loss: 0.0946, step time: 0.1726\n",
      "148/223, train_loss: 0.0980, step time: 0.1067\n",
      "149/223, train_loss: 0.1069, step time: 0.1132\n",
      "150/223, train_loss: 0.1023, step time: 0.1119\n",
      "151/223, train_loss: 0.0931, step time: 0.1328\n",
      "152/223, train_loss: 0.1023, step time: 0.1008\n",
      "153/223, train_loss: 0.0917, step time: 0.1103\n",
      "154/223, train_loss: 0.0959, step time: 0.1122\n",
      "155/223, train_loss: 0.0997, step time: 0.1293\n",
      "156/223, train_loss: 0.0951, step time: 0.1003\n",
      "157/223, train_loss: 0.1064, step time: 0.1004\n",
      "158/223, train_loss: 0.1045, step time: 0.1002\n",
      "159/223, train_loss: 0.1012, step time: 0.1026\n",
      "160/223, train_loss: 0.0863, step time: 0.1146\n",
      "161/223, train_loss: 0.1070, step time: 0.1217\n",
      "162/223, train_loss: 0.0896, step time: 0.1119\n",
      "163/223, train_loss: 0.0986, step time: 0.1272\n",
      "164/223, train_loss: 0.0850, step time: 0.1005\n",
      "165/223, train_loss: 0.0940, step time: 0.1008\n",
      "166/223, train_loss: 0.0946, step time: 0.1011\n",
      "167/223, train_loss: 0.0983, step time: 0.1021\n",
      "168/223, train_loss: 0.1080, step time: 0.1009\n",
      "169/223, train_loss: 0.0967, step time: 0.1006\n",
      "170/223, train_loss: 0.0975, step time: 0.1211\n",
      "171/223, train_loss: 0.0990, step time: 0.1264\n",
      "172/223, train_loss: 0.0960, step time: 0.1000\n",
      "173/223, train_loss: 0.0937, step time: 0.1133\n",
      "174/223, train_loss: 0.0918, step time: 0.1110\n",
      "175/223, train_loss: 0.1024, step time: 0.1067\n",
      "176/223, train_loss: 0.0997, step time: 0.1583\n",
      "177/223, train_loss: 0.0941, step time: 0.1205\n",
      "178/223, train_loss: 0.1171, step time: 0.1122\n",
      "179/223, train_loss: 0.0968, step time: 0.1060\n",
      "180/223, train_loss: 0.0940, step time: 0.1151\n",
      "181/223, train_loss: 0.0976, step time: 0.1048\n",
      "182/223, train_loss: 0.1064, step time: 0.1060\n",
      "183/223, train_loss: 0.0951, step time: 0.1390\n",
      "184/223, train_loss: 0.1028, step time: 0.1138\n",
      "185/223, train_loss: 0.1043, step time: 0.1043\n",
      "186/223, train_loss: 0.0936, step time: 0.1056\n",
      "187/223, train_loss: 0.1016, step time: 0.1123\n",
      "188/223, train_loss: 0.0994, step time: 0.1008\n",
      "189/223, train_loss: 0.0938, step time: 0.1067\n",
      "190/223, train_loss: 0.0951, step time: 0.1168\n",
      "191/223, train_loss: 0.0995, step time: 0.1158\n",
      "192/223, train_loss: 0.0963, step time: 0.1221\n",
      "193/223, train_loss: 0.1090, step time: 0.1186\n",
      "194/223, train_loss: 0.1024, step time: 0.1054\n",
      "195/223, train_loss: 0.0939, step time: 0.1149\n",
      "196/223, train_loss: 0.0879, step time: 0.1084\n",
      "197/223, train_loss: 0.0971, step time: 0.1152\n",
      "198/223, train_loss: 0.1040, step time: 0.1155\n",
      "199/223, train_loss: 0.0967, step time: 0.1001\n",
      "200/223, train_loss: 0.1048, step time: 0.1000\n",
      "201/223, train_loss: 0.0948, step time: 0.0998\n",
      "202/223, train_loss: 0.0844, step time: 0.1108\n",
      "203/223, train_loss: 0.1039, step time: 0.1001\n",
      "204/223, train_loss: 0.1096, step time: 0.0997\n",
      "205/223, train_loss: 0.0953, step time: 0.1007\n",
      "206/223, train_loss: 0.0974, step time: 0.1154\n",
      "207/223, train_loss: 0.1032, step time: 0.1099\n",
      "208/223, train_loss: 0.0941, step time: 0.1006\n",
      "209/223, train_loss: 0.0949, step time: 0.1000\n",
      "210/223, train_loss: 0.0963, step time: 0.1007\n",
      "211/223, train_loss: 0.0924, step time: 0.1046\n",
      "212/223, train_loss: 0.0970, step time: 0.1006\n",
      "213/223, train_loss: 0.0958, step time: 0.1042\n",
      "214/223, train_loss: 0.1083, step time: 0.1137\n",
      "215/223, train_loss: 0.0978, step time: 0.1207\n",
      "216/223, train_loss: 0.1071, step time: 0.1064\n",
      "217/223, train_loss: 0.0900, step time: 0.1322\n",
      "218/223, train_loss: 0.0952, step time: 0.1106\n",
      "219/223, train_loss: 0.1062, step time: 0.1001\n",
      "220/223, train_loss: 0.0915, step time: 0.1633\n",
      "221/223, train_loss: 0.0946, step time: 0.1001\n",
      "222/223, train_loss: 0.1008, step time: 0.1071\n",
      "223/223, train_loss: 0.1089, step time: 0.1006\n",
      "epoch 263 average loss: 0.0997\n",
      "time consuming of epoch 263 is: 91.1314\n",
      "----------\n",
      "epoch 264/300\n",
      "1/223, train_loss: 0.0965, step time: 0.1056\n",
      "2/223, train_loss: 0.0929, step time: 0.1004\n",
      "3/223, train_loss: 0.0846, step time: 0.1005\n",
      "4/223, train_loss: 0.1025, step time: 0.1014\n",
      "5/223, train_loss: 0.0969, step time: 0.1117\n",
      "6/223, train_loss: 0.0937, step time: 0.1116\n",
      "7/223, train_loss: 0.0952, step time: 0.1196\n",
      "8/223, train_loss: 0.1038, step time: 0.1004\n",
      "9/223, train_loss: 0.1017, step time: 0.1118\n",
      "10/223, train_loss: 0.0998, step time: 0.1088\n",
      "11/223, train_loss: 0.1116, step time: 0.1013\n",
      "12/223, train_loss: 0.1005, step time: 0.1001\n",
      "13/223, train_loss: 0.0936, step time: 0.1092\n",
      "14/223, train_loss: 0.1011, step time: 0.1030\n",
      "15/223, train_loss: 0.0999, step time: 0.1219\n",
      "16/223, train_loss: 0.0929, step time: 0.1264\n",
      "17/223, train_loss: 0.0989, step time: 0.1206\n",
      "18/223, train_loss: 0.1073, step time: 0.1087\n",
      "19/223, train_loss: 0.1018, step time: 0.1128\n",
      "20/223, train_loss: 0.1130, step time: 0.1003\n",
      "21/223, train_loss: 0.0951, step time: 0.1132\n",
      "22/223, train_loss: 0.0938, step time: 0.1192\n",
      "23/223, train_loss: 0.0939, step time: 0.1003\n",
      "24/223, train_loss: 0.0901, step time: 0.1006\n",
      "25/223, train_loss: 0.0886, step time: 0.1243\n",
      "26/223, train_loss: 0.0939, step time: 0.1078\n",
      "27/223, train_loss: 0.0934, step time: 0.1107\n",
      "28/223, train_loss: 0.1013, step time: 0.1106\n",
      "29/223, train_loss: 0.1060, step time: 0.1052\n",
      "30/223, train_loss: 0.0919, step time: 0.1093\n",
      "31/223, train_loss: 0.0949, step time: 0.1138\n",
      "32/223, train_loss: 0.1016, step time: 0.1172\n",
      "33/223, train_loss: 0.0920, step time: 0.1225\n",
      "34/223, train_loss: 0.0995, step time: 0.1025\n",
      "35/223, train_loss: 0.0923, step time: 0.1082\n",
      "36/223, train_loss: 0.1121, step time: 0.1429\n",
      "37/223, train_loss: 0.0881, step time: 0.1188\n",
      "38/223, train_loss: 0.1023, step time: 0.1099\n",
      "39/223, train_loss: 0.1005, step time: 0.1190\n",
      "40/223, train_loss: 0.1027, step time: 0.1205\n",
      "41/223, train_loss: 0.1004, step time: 0.1025\n",
      "42/223, train_loss: 0.0917, step time: 0.1010\n",
      "43/223, train_loss: 0.1092, step time: 0.1006\n",
      "44/223, train_loss: 0.0933, step time: 0.1388\n",
      "45/223, train_loss: 0.0979, step time: 0.1432\n",
      "46/223, train_loss: 0.0986, step time: 0.1063\n",
      "47/223, train_loss: 0.1059, step time: 0.1010\n",
      "48/223, train_loss: 0.0876, step time: 0.1465\n",
      "49/223, train_loss: 0.0998, step time: 0.1221\n",
      "50/223, train_loss: 0.0952, step time: 0.1205\n",
      "51/223, train_loss: 0.1016, step time: 0.1095\n",
      "52/223, train_loss: 0.0992, step time: 0.1352\n",
      "53/223, train_loss: 0.1085, step time: 0.1278\n",
      "54/223, train_loss: 0.0949, step time: 0.1146\n",
      "55/223, train_loss: 0.1018, step time: 0.1052\n",
      "56/223, train_loss: 0.1022, step time: 0.1052\n",
      "57/223, train_loss: 0.1006, step time: 0.1437\n",
      "58/223, train_loss: 0.0914, step time: 0.1023\n",
      "59/223, train_loss: 0.1101, step time: 0.1091\n",
      "60/223, train_loss: 0.1035, step time: 0.1404\n",
      "61/223, train_loss: 0.1032, step time: 0.0998\n",
      "62/223, train_loss: 0.0952, step time: 0.1000\n",
      "63/223, train_loss: 0.0946, step time: 0.1005\n",
      "64/223, train_loss: 0.0936, step time: 0.1625\n",
      "65/223, train_loss: 0.1007, step time: 0.1287\n",
      "66/223, train_loss: 0.0977, step time: 0.1179\n",
      "67/223, train_loss: 0.0996, step time: 0.1253\n",
      "68/223, train_loss: 0.0960, step time: 0.1217\n",
      "69/223, train_loss: 0.1009, step time: 0.1156\n",
      "70/223, train_loss: 0.1102, step time: 0.1118\n",
      "71/223, train_loss: 0.1045, step time: 0.1019\n",
      "72/223, train_loss: 0.0860, step time: 0.1208\n",
      "73/223, train_loss: 0.1068, step time: 0.1277\n",
      "74/223, train_loss: 0.0996, step time: 0.1221\n",
      "75/223, train_loss: 0.0902, step time: 0.1310\n",
      "76/223, train_loss: 0.1029, step time: 0.1307\n",
      "77/223, train_loss: 0.0921, step time: 0.1006\n",
      "78/223, train_loss: 0.0976, step time: 0.1103\n",
      "79/223, train_loss: 0.1113, step time: 0.1465\n",
      "80/223, train_loss: 0.1119, step time: 0.1205\n",
      "81/223, train_loss: 0.1013, step time: 0.1048\n",
      "82/223, train_loss: 0.0914, step time: 0.1363\n",
      "83/223, train_loss: 0.1042, step time: 0.1176\n",
      "84/223, train_loss: 0.0988, step time: 0.1066\n",
      "85/223, train_loss: 0.0873, step time: 0.1069\n",
      "86/223, train_loss: 0.0907, step time: 0.1159\n",
      "87/223, train_loss: 0.0974, step time: 0.0993\n",
      "88/223, train_loss: 0.0962, step time: 0.1023\n",
      "89/223, train_loss: 0.1134, step time: 0.1031\n",
      "90/223, train_loss: 0.1033, step time: 0.1114\n",
      "91/223, train_loss: 0.0964, step time: 0.1001\n",
      "92/223, train_loss: 0.0966, step time: 0.1008\n",
      "93/223, train_loss: 0.0929, step time: 0.1042\n",
      "94/223, train_loss: 0.0905, step time: 0.1210\n",
      "95/223, train_loss: 0.0905, step time: 0.1121\n",
      "96/223, train_loss: 0.1090, step time: 0.1000\n",
      "97/223, train_loss: 0.0861, step time: 0.1004\n",
      "98/223, train_loss: 0.0892, step time: 0.1249\n",
      "99/223, train_loss: 0.0951, step time: 0.1235\n",
      "100/223, train_loss: 0.0998, step time: 0.1124\n",
      "101/223, train_loss: 0.1003, step time: 0.1235\n",
      "102/223, train_loss: 0.0939, step time: 0.1095\n",
      "103/223, train_loss: 0.1105, step time: 0.0998\n",
      "104/223, train_loss: 0.1015, step time: 0.1004\n",
      "105/223, train_loss: 0.1057, step time: 0.1436\n",
      "106/223, train_loss: 0.0915, step time: 0.1088\n",
      "107/223, train_loss: 0.1037, step time: 0.1119\n",
      "108/223, train_loss: 0.0831, step time: 0.1248\n",
      "109/223, train_loss: 0.0991, step time: 0.1054\n",
      "110/223, train_loss: 0.0943, step time: 0.1068\n",
      "111/223, train_loss: 0.0907, step time: 0.1271\n",
      "112/223, train_loss: 0.0962, step time: 0.1444\n",
      "113/223, train_loss: 0.0969, step time: 0.1229\n",
      "114/223, train_loss: 0.0925, step time: 0.1121\n",
      "115/223, train_loss: 0.1079, step time: 0.1136\n",
      "116/223, train_loss: 0.0914, step time: 0.1064\n",
      "117/223, train_loss: 0.1091, step time: 0.1090\n",
      "118/223, train_loss: 0.0893, step time: 0.1160\n",
      "119/223, train_loss: 0.0994, step time: 0.1101\n",
      "120/223, train_loss: 0.1010, step time: 0.1336\n",
      "121/223, train_loss: 0.0904, step time: 0.1227\n",
      "122/223, train_loss: 0.0965, step time: 0.1444\n",
      "123/223, train_loss: 0.0929, step time: 0.0995\n",
      "124/223, train_loss: 0.1070, step time: 0.1051\n",
      "125/223, train_loss: 0.1049, step time: 0.1142\n",
      "126/223, train_loss: 0.0946, step time: 0.2029\n",
      "127/223, train_loss: 0.0987, step time: 0.1961\n",
      "128/223, train_loss: 0.0916, step time: 0.1160\n",
      "129/223, train_loss: 0.0866, step time: 0.1022\n",
      "130/223, train_loss: 0.0955, step time: 0.1141\n",
      "131/223, train_loss: 0.1121, step time: 0.1119\n",
      "132/223, train_loss: 0.0932, step time: 0.1253\n",
      "133/223, train_loss: 0.0919, step time: 0.1177\n",
      "134/223, train_loss: 0.0975, step time: 0.1239\n",
      "135/223, train_loss: 0.0920, step time: 0.1110\n",
      "136/223, train_loss: 0.0946, step time: 0.1074\n",
      "137/223, train_loss: 0.1020, step time: 0.1111\n",
      "138/223, train_loss: 0.1034, step time: 0.1108\n",
      "139/223, train_loss: 0.1019, step time: 0.1013\n",
      "140/223, train_loss: 0.1088, step time: 0.1073\n",
      "141/223, train_loss: 0.1003, step time: 0.1367\n",
      "142/223, train_loss: 0.1003, step time: 0.1168\n",
      "143/223, train_loss: 0.1131, step time: 0.1063\n",
      "144/223, train_loss: 0.1054, step time: 0.1105\n",
      "145/223, train_loss: 0.0961, step time: 0.1053\n",
      "146/223, train_loss: 0.0898, step time: 0.1222\n",
      "147/223, train_loss: 0.0982, step time: 0.1129\n",
      "148/223, train_loss: 0.0976, step time: 0.1098\n",
      "149/223, train_loss: 0.1039, step time: 0.1157\n",
      "150/223, train_loss: 0.1066, step time: 0.1083\n",
      "151/223, train_loss: 0.0946, step time: 0.1262\n",
      "152/223, train_loss: 0.0966, step time: 0.1258\n",
      "153/223, train_loss: 0.1029, step time: 0.1179\n",
      "154/223, train_loss: 0.1052, step time: 0.1240\n",
      "155/223, train_loss: 0.0950, step time: 0.1108\n",
      "156/223, train_loss: 0.0981, step time: 0.1156\n",
      "157/223, train_loss: 0.0894, step time: 0.1112\n",
      "158/223, train_loss: 0.1009, step time: 0.1202\n",
      "159/223, train_loss: 0.0913, step time: 0.1217\n",
      "160/223, train_loss: 0.1186, step time: 0.1052\n",
      "161/223, train_loss: 0.0923, step time: 0.1055\n",
      "162/223, train_loss: 0.0906, step time: 0.1091\n",
      "163/223, train_loss: 0.0994, step time: 0.1055\n",
      "164/223, train_loss: 0.0968, step time: 0.1136\n",
      "165/223, train_loss: 0.0982, step time: 0.1109\n",
      "166/223, train_loss: 0.0924, step time: 0.1065\n",
      "167/223, train_loss: 0.0979, step time: 0.1007\n",
      "168/223, train_loss: 0.0856, step time: 0.1179\n",
      "169/223, train_loss: 0.1143, step time: 0.1162\n",
      "170/223, train_loss: 0.1026, step time: 0.1078\n",
      "171/223, train_loss: 0.1048, step time: 0.1184\n",
      "172/223, train_loss: 0.0893, step time: 0.1139\n",
      "173/223, train_loss: 0.1094, step time: 0.1157\n",
      "174/223, train_loss: 0.1095, step time: 0.1236\n",
      "175/223, train_loss: 0.0984, step time: 0.1223\n",
      "176/223, train_loss: 0.0982, step time: 0.1038\n",
      "177/223, train_loss: 0.1085, step time: 0.1236\n",
      "178/223, train_loss: 0.0964, step time: 0.1465\n",
      "179/223, train_loss: 0.0960, step time: 0.1109\n",
      "180/223, train_loss: 0.1000, step time: 0.1126\n",
      "181/223, train_loss: 0.0975, step time: 0.1270\n",
      "182/223, train_loss: 0.1018, step time: 0.1134\n",
      "183/223, train_loss: 0.1020, step time: 0.1107\n",
      "184/223, train_loss: 0.0973, step time: 0.1104\n",
      "185/223, train_loss: 0.1038, step time: 0.1036\n",
      "186/223, train_loss: 0.0943, step time: 0.1054\n",
      "187/223, train_loss: 0.0925, step time: 0.1213\n",
      "188/223, train_loss: 0.0895, step time: 0.1287\n",
      "189/223, train_loss: 0.0961, step time: 0.1123\n",
      "190/223, train_loss: 0.0897, step time: 0.1143\n",
      "191/223, train_loss: 0.0880, step time: 0.1233\n",
      "192/223, train_loss: 0.0961, step time: 0.1112\n",
      "193/223, train_loss: 0.0980, step time: 0.1101\n",
      "194/223, train_loss: 0.1092, step time: 0.1131\n",
      "195/223, train_loss: 0.1144, step time: 0.1103\n",
      "196/223, train_loss: 0.0924, step time: 0.1104\n",
      "197/223, train_loss: 0.3013, step time: 0.1262\n",
      "198/223, train_loss: 0.1009, step time: 0.1135\n",
      "199/223, train_loss: 0.1120, step time: 0.1176\n",
      "200/223, train_loss: 0.0924, step time: 0.1215\n",
      "201/223, train_loss: 0.1139, step time: 0.1304\n",
      "202/223, train_loss: 0.0986, step time: 0.0991\n",
      "203/223, train_loss: 0.1005, step time: 0.0997\n",
      "204/223, train_loss: 0.0942, step time: 0.0994\n",
      "205/223, train_loss: 0.1016, step time: 0.1002\n",
      "206/223, train_loss: 0.0970, step time: 0.1002\n",
      "207/223, train_loss: 0.1124, step time: 0.1005\n",
      "208/223, train_loss: 0.0878, step time: 0.1001\n",
      "209/223, train_loss: 0.1099, step time: 0.1140\n",
      "210/223, train_loss: 0.1051, step time: 0.0997\n",
      "211/223, train_loss: 0.0961, step time: 0.0994\n",
      "212/223, train_loss: 0.0921, step time: 0.1000\n",
      "213/223, train_loss: 0.1003, step time: 0.0997\n",
      "214/223, train_loss: 0.1216, step time: 0.0992\n",
      "215/223, train_loss: 0.0962, step time: 0.0995\n",
      "216/223, train_loss: 0.1014, step time: 0.0999\n",
      "217/223, train_loss: 0.1121, step time: 0.1030\n",
      "218/223, train_loss: 0.0906, step time: 0.0997\n",
      "219/223, train_loss: 0.0989, step time: 0.0993\n",
      "220/223, train_loss: 0.0910, step time: 0.1009\n",
      "221/223, train_loss: 0.1031, step time: 0.1003\n",
      "222/223, train_loss: 0.0974, step time: 0.1003\n",
      "223/223, train_loss: 0.0991, step time: 0.0998\n",
      "epoch 264 average loss: 0.0997\n",
      "time consuming of epoch 264 is: 93.0106\n",
      "----------\n",
      "epoch 265/300\n",
      "1/223, train_loss: 0.0903, step time: 0.1016\n",
      "2/223, train_loss: 0.0906, step time: 0.1004\n",
      "3/223, train_loss: 0.0960, step time: 0.1298\n",
      "4/223, train_loss: 0.1066, step time: 0.1011\n",
      "5/223, train_loss: 0.0991, step time: 0.1062\n",
      "6/223, train_loss: 0.0913, step time: 0.1275\n",
      "7/223, train_loss: 0.1056, step time: 0.1190\n",
      "8/223, train_loss: 0.1006, step time: 0.1114\n",
      "9/223, train_loss: 0.1096, step time: 0.1236\n",
      "10/223, train_loss: 0.0939, step time: 0.1027\n",
      "11/223, train_loss: 0.0947, step time: 0.1107\n",
      "12/223, train_loss: 0.0912, step time: 0.1081\n",
      "13/223, train_loss: 0.1067, step time: 0.1007\n",
      "14/223, train_loss: 0.0971, step time: 0.1008\n",
      "15/223, train_loss: 0.1026, step time: 0.1000\n",
      "16/223, train_loss: 0.1064, step time: 0.1175\n",
      "17/223, train_loss: 0.0982, step time: 0.1170\n",
      "18/223, train_loss: 0.1002, step time: 0.1762\n",
      "19/223, train_loss: 0.1014, step time: 0.1043\n",
      "20/223, train_loss: 0.0954, step time: 0.1011\n",
      "21/223, train_loss: 0.0896, step time: 0.1292\n",
      "22/223, train_loss: 0.0953, step time: 0.1056\n",
      "23/223, train_loss: 0.0852, step time: 0.1162\n",
      "24/223, train_loss: 0.0936, step time: 0.1386\n",
      "25/223, train_loss: 0.1144, step time: 0.1092\n",
      "26/223, train_loss: 0.0966, step time: 0.1103\n",
      "27/223, train_loss: 0.0944, step time: 0.1075\n",
      "28/223, train_loss: 0.0999, step time: 0.1039\n",
      "29/223, train_loss: 0.1038, step time: 0.1004\n",
      "30/223, train_loss: 0.0979, step time: 0.0997\n",
      "31/223, train_loss: 0.0930, step time: 0.1000\n",
      "32/223, train_loss: 0.0974, step time: 0.1010\n",
      "33/223, train_loss: 0.0940, step time: 0.1353\n",
      "34/223, train_loss: 0.0973, step time: 0.1060\n",
      "35/223, train_loss: 0.0923, step time: 0.1021\n",
      "36/223, train_loss: 0.0965, step time: 0.1143\n",
      "37/223, train_loss: 0.0945, step time: 0.1101\n",
      "38/223, train_loss: 0.1084, step time: 0.1000\n",
      "39/223, train_loss: 0.0999, step time: 0.1007\n",
      "40/223, train_loss: 0.0965, step time: 0.1023\n",
      "41/223, train_loss: 0.0943, step time: 0.1069\n",
      "42/223, train_loss: 0.0896, step time: 0.1023\n",
      "43/223, train_loss: 0.0881, step time: 0.1026\n",
      "44/223, train_loss: 0.1068, step time: 0.1104\n",
      "45/223, train_loss: 0.0981, step time: 0.1194\n",
      "46/223, train_loss: 0.0953, step time: 0.1000\n",
      "47/223, train_loss: 0.1054, step time: 0.1006\n",
      "48/223, train_loss: 0.1006, step time: 0.1172\n",
      "49/223, train_loss: 0.1003, step time: 0.1096\n",
      "50/223, train_loss: 0.0941, step time: 0.1120\n",
      "51/223, train_loss: 0.0884, step time: 0.1088\n",
      "52/223, train_loss: 0.1007, step time: 0.1092\n",
      "53/223, train_loss: 0.1023, step time: 0.1410\n",
      "54/223, train_loss: 0.0980, step time: 0.1001\n",
      "55/223, train_loss: 0.0930, step time: 0.1000\n",
      "56/223, train_loss: 0.1022, step time: 0.1076\n",
      "57/223, train_loss: 0.1061, step time: 0.1258\n",
      "58/223, train_loss: 0.0950, step time: 0.1051\n",
      "59/223, train_loss: 0.0997, step time: 0.1001\n",
      "60/223, train_loss: 0.1044, step time: 0.1245\n",
      "61/223, train_loss: 0.1038, step time: 0.1246\n",
      "62/223, train_loss: 0.0983, step time: 0.1362\n",
      "63/223, train_loss: 0.0899, step time: 0.1090\n",
      "64/223, train_loss: 0.0957, step time: 0.1003\n",
      "65/223, train_loss: 0.1111, step time: 0.1014\n",
      "66/223, train_loss: 0.0887, step time: 0.1010\n",
      "67/223, train_loss: 0.0956, step time: 0.1011\n",
      "68/223, train_loss: 0.0924, step time: 0.1043\n",
      "69/223, train_loss: 0.0923, step time: 0.1005\n",
      "70/223, train_loss: 0.0909, step time: 0.1106\n",
      "71/223, train_loss: 0.0943, step time: 0.1093\n",
      "72/223, train_loss: 0.0921, step time: 0.1232\n",
      "73/223, train_loss: 0.0974, step time: 0.1048\n",
      "74/223, train_loss: 0.0918, step time: 0.1134\n",
      "75/223, train_loss: 0.0915, step time: 0.1187\n",
      "76/223, train_loss: 0.0953, step time: 0.1007\n",
      "77/223, train_loss: 0.0970, step time: 0.1094\n",
      "78/223, train_loss: 0.1039, step time: 0.1157\n",
      "79/223, train_loss: 0.0951, step time: 0.1165\n",
      "80/223, train_loss: 0.1085, step time: 0.1088\n",
      "81/223, train_loss: 0.0924, step time: 0.1056\n",
      "82/223, train_loss: 0.0940, step time: 0.1067\n",
      "83/223, train_loss: 0.0920, step time: 0.1352\n",
      "84/223, train_loss: 0.1064, step time: 0.1164\n",
      "85/223, train_loss: 0.0955, step time: 0.1118\n",
      "86/223, train_loss: 0.1043, step time: 0.1144\n",
      "87/223, train_loss: 0.1037, step time: 0.1347\n",
      "88/223, train_loss: 0.0918, step time: 0.1149\n",
      "89/223, train_loss: 0.0965, step time: 0.1016\n",
      "90/223, train_loss: 0.1086, step time: 0.1191\n",
      "91/223, train_loss: 0.0993, step time: 0.1166\n",
      "92/223, train_loss: 0.0915, step time: 0.1228\n",
      "93/223, train_loss: 0.0996, step time: 0.1145\n",
      "94/223, train_loss: 0.0965, step time: 0.1125\n",
      "95/223, train_loss: 0.0957, step time: 0.1233\n",
      "96/223, train_loss: 0.0991, step time: 0.1000\n",
      "97/223, train_loss: 0.1052, step time: 0.1153\n",
      "98/223, train_loss: 0.1017, step time: 0.1097\n",
      "99/223, train_loss: 0.0949, step time: 0.1174\n",
      "100/223, train_loss: 0.1012, step time: 0.1420\n",
      "101/223, train_loss: 0.0963, step time: 0.0998\n",
      "102/223, train_loss: 0.0929, step time: 0.1009\n",
      "103/223, train_loss: 0.1077, step time: 0.1016\n",
      "104/223, train_loss: 0.1068, step time: 0.1291\n",
      "105/223, train_loss: 0.1070, step time: 0.1151\n",
      "106/223, train_loss: 0.1016, step time: 0.1149\n",
      "107/223, train_loss: 0.1032, step time: 0.1949\n",
      "108/223, train_loss: 0.0905, step time: 0.0997\n",
      "109/223, train_loss: 0.1083, step time: 0.1061\n",
      "110/223, train_loss: 0.1073, step time: 0.1280\n",
      "111/223, train_loss: 0.0968, step time: 0.1173\n",
      "112/223, train_loss: 0.0920, step time: 0.1536\n",
      "113/223, train_loss: 0.1095, step time: 0.1009\n",
      "114/223, train_loss: 0.1135, step time: 0.1075\n",
      "115/223, train_loss: 0.0871, step time: 0.1320\n",
      "116/223, train_loss: 0.0971, step time: 0.1137\n",
      "117/223, train_loss: 0.0937, step time: 0.1081\n",
      "118/223, train_loss: 0.0904, step time: 0.1288\n",
      "119/223, train_loss: 0.0963, step time: 0.1809\n",
      "120/223, train_loss: 0.0980, step time: 0.1090\n",
      "121/223, train_loss: 0.0990, step time: 0.1032\n",
      "122/223, train_loss: 0.0877, step time: 0.1078\n",
      "123/223, train_loss: 0.0911, step time: 0.1205\n",
      "124/223, train_loss: 0.0884, step time: 0.1424\n",
      "125/223, train_loss: 0.1050, step time: 0.1853\n",
      "126/223, train_loss: 0.0950, step time: 0.1550\n",
      "127/223, train_loss: 0.0998, step time: 0.1243\n",
      "128/223, train_loss: 0.0976, step time: 0.1098\n",
      "129/223, train_loss: 0.1134, step time: 0.1049\n",
      "130/223, train_loss: 0.0997, step time: 0.1108\n",
      "131/223, train_loss: 0.0932, step time: 0.1030\n",
      "132/223, train_loss: 0.0956, step time: 0.1216\n",
      "133/223, train_loss: 0.1167, step time: 0.1111\n",
      "134/223, train_loss: 0.1052, step time: 0.1549\n",
      "135/223, train_loss: 0.1053, step time: 0.1299\n",
      "136/223, train_loss: 0.0968, step time: 0.1109\n",
      "137/223, train_loss: 0.0972, step time: 0.1091\n",
      "138/223, train_loss: 0.0982, step time: 0.1174\n",
      "139/223, train_loss: 0.1235, step time: 0.1129\n",
      "140/223, train_loss: 0.1057, step time: 0.1122\n",
      "141/223, train_loss: 0.1022, step time: 0.1036\n",
      "142/223, train_loss: 0.0923, step time: 0.1018\n",
      "143/223, train_loss: 0.1001, step time: 0.0993\n",
      "144/223, train_loss: 0.1032, step time: 0.1168\n",
      "145/223, train_loss: 0.1017, step time: 0.1121\n",
      "146/223, train_loss: 0.1026, step time: 0.1493\n",
      "147/223, train_loss: 0.0871, step time: 0.1118\n",
      "148/223, train_loss: 0.0944, step time: 0.1086\n",
      "149/223, train_loss: 0.0958, step time: 0.1248\n",
      "150/223, train_loss: 0.0998, step time: 0.0999\n",
      "151/223, train_loss: 0.0918, step time: 0.1200\n",
      "152/223, train_loss: 0.1079, step time: 0.1048\n",
      "153/223, train_loss: 0.0892, step time: 0.1017\n",
      "154/223, train_loss: 0.0920, step time: 0.1258\n",
      "155/223, train_loss: 0.1071, step time: 0.1007\n",
      "156/223, train_loss: 0.1069, step time: 0.1001\n",
      "157/223, train_loss: 0.1085, step time: 0.1145\n",
      "158/223, train_loss: 0.0945, step time: 0.1181\n",
      "159/223, train_loss: 0.1024, step time: 0.1222\n",
      "160/223, train_loss: 0.0942, step time: 0.1248\n",
      "161/223, train_loss: 0.0934, step time: 0.1447\n",
      "162/223, train_loss: 0.0991, step time: 0.1269\n",
      "163/223, train_loss: 0.0929, step time: 0.1083\n",
      "164/223, train_loss: 0.1060, step time: 0.1208\n",
      "165/223, train_loss: 0.1030, step time: 0.1114\n",
      "166/223, train_loss: 0.1000, step time: 0.1013\n",
      "167/223, train_loss: 0.1006, step time: 0.1072\n",
      "168/223, train_loss: 0.1059, step time: 0.1000\n",
      "169/223, train_loss: 0.1042, step time: 0.1069\n",
      "170/223, train_loss: 0.0989, step time: 0.1012\n",
      "171/223, train_loss: 0.1072, step time: 0.1007\n",
      "172/223, train_loss: 0.0947, step time: 0.1432\n",
      "173/223, train_loss: 0.2926, step time: 0.1053\n",
      "174/223, train_loss: 0.0978, step time: 0.1263\n",
      "175/223, train_loss: 0.1030, step time: 0.1088\n",
      "176/223, train_loss: 0.0934, step time: 0.1328\n",
      "177/223, train_loss: 0.1061, step time: 0.1066\n",
      "178/223, train_loss: 0.0904, step time: 0.1378\n",
      "179/223, train_loss: 0.1033, step time: 0.1007\n",
      "180/223, train_loss: 0.1048, step time: 0.0996\n",
      "181/223, train_loss: 0.0870, step time: 0.1116\n",
      "182/223, train_loss: 0.0996, step time: 0.1163\n",
      "183/223, train_loss: 0.1005, step time: 0.1192\n",
      "184/223, train_loss: 0.1067, step time: 0.1287\n",
      "185/223, train_loss: 0.1042, step time: 0.1133\n",
      "186/223, train_loss: 0.0982, step time: 0.1021\n",
      "187/223, train_loss: 0.0982, step time: 0.1323\n",
      "188/223, train_loss: 0.0896, step time: 0.1006\n",
      "189/223, train_loss: 0.0968, step time: 0.1028\n",
      "190/223, train_loss: 0.1063, step time: 0.1071\n",
      "191/223, train_loss: 0.0970, step time: 0.1068\n",
      "192/223, train_loss: 0.1126, step time: 0.1149\n",
      "193/223, train_loss: 0.1049, step time: 0.1119\n",
      "194/223, train_loss: 0.1135, step time: 0.1082\n",
      "195/223, train_loss: 0.1193, step time: 0.1078\n",
      "196/223, train_loss: 0.1055, step time: 0.1009\n",
      "197/223, train_loss: 0.0987, step time: 0.1044\n",
      "198/223, train_loss: 0.0991, step time: 0.1085\n",
      "199/223, train_loss: 0.0971, step time: 0.1010\n",
      "200/223, train_loss: 0.0926, step time: 0.1118\n",
      "201/223, train_loss: 0.0982, step time: 0.0994\n",
      "202/223, train_loss: 0.1004, step time: 0.1198\n",
      "203/223, train_loss: 0.0828, step time: 0.1050\n",
      "204/223, train_loss: 0.1016, step time: 0.1054\n",
      "205/223, train_loss: 0.1098, step time: 0.1139\n",
      "206/223, train_loss: 0.1018, step time: 0.1009\n",
      "207/223, train_loss: 0.1078, step time: 0.1047\n",
      "208/223, train_loss: 0.1026, step time: 0.1135\n",
      "209/223, train_loss: 0.0874, step time: 0.1406\n",
      "210/223, train_loss: 0.0938, step time: 0.1022\n",
      "211/223, train_loss: 0.0967, step time: 0.1001\n",
      "212/223, train_loss: 0.1024, step time: 0.1318\n",
      "213/223, train_loss: 0.1053, step time: 0.1096\n",
      "214/223, train_loss: 0.1135, step time: 0.1085\n",
      "215/223, train_loss: 0.0997, step time: 0.1360\n",
      "216/223, train_loss: 0.0951, step time: 0.1008\n",
      "217/223, train_loss: 0.1042, step time: 0.1057\n",
      "218/223, train_loss: 0.0979, step time: 0.1002\n",
      "219/223, train_loss: 0.0933, step time: 0.1454\n",
      "220/223, train_loss: 0.0927, step time: 0.1422\n",
      "221/223, train_loss: 0.0954, step time: 0.0990\n",
      "222/223, train_loss: 0.0910, step time: 0.0992\n",
      "223/223, train_loss: 0.0889, step time: 0.0999\n",
      "epoch 265 average loss: 0.0998\n",
      "current epoch: 265 current mean dice: 0.8621 tc: 0.9225 wt: 0.8717 et: 0.7923\n",
      "best mean dice: 0.8622 at epoch: 260\n",
      "time consuming of epoch 265 is: 91.7802\n",
      "----------\n",
      "epoch 266/300\n",
      "1/223, train_loss: 0.0987, step time: 0.1052\n",
      "2/223, train_loss: 0.0974, step time: 0.1003\n",
      "3/223, train_loss: 0.1044, step time: 0.1009\n",
      "4/223, train_loss: 0.0998, step time: 0.1016\n",
      "5/223, train_loss: 0.1149, step time: 0.1139\n",
      "6/223, train_loss: 0.1059, step time: 0.1206\n",
      "7/223, train_loss: 0.1064, step time: 0.1080\n",
      "8/223, train_loss: 0.0880, step time: 0.1199\n",
      "9/223, train_loss: 0.1057, step time: 0.1056\n",
      "10/223, train_loss: 0.1070, step time: 0.1128\n",
      "11/223, train_loss: 0.0990, step time: 0.1058\n",
      "12/223, train_loss: 0.0947, step time: 0.1004\n",
      "13/223, train_loss: 0.1064, step time: 0.1054\n",
      "14/223, train_loss: 0.1115, step time: 0.1000\n",
      "15/223, train_loss: 0.0975, step time: 0.1010\n",
      "16/223, train_loss: 0.1145, step time: 0.1006\n",
      "17/223, train_loss: 0.1004, step time: 0.1129\n",
      "18/223, train_loss: 0.1022, step time: 0.1071\n",
      "19/223, train_loss: 0.0994, step time: 0.1003\n",
      "20/223, train_loss: 0.0958, step time: 0.1003\n",
      "21/223, train_loss: 0.0958, step time: 0.1162\n",
      "22/223, train_loss: 0.0942, step time: 0.1086\n",
      "23/223, train_loss: 0.0830, step time: 0.1059\n",
      "24/223, train_loss: 0.0997, step time: 0.1193\n",
      "25/223, train_loss: 0.0906, step time: 0.1250\n",
      "26/223, train_loss: 0.0974, step time: 0.1169\n",
      "27/223, train_loss: 0.1081, step time: 0.1132\n",
      "28/223, train_loss: 0.0991, step time: 0.1281\n",
      "29/223, train_loss: 0.0878, step time: 0.1204\n",
      "30/223, train_loss: 0.0904, step time: 0.1173\n",
      "31/223, train_loss: 0.0942, step time: 0.1151\n",
      "32/223, train_loss: 0.0959, step time: 0.1028\n",
      "33/223, train_loss: 0.0996, step time: 0.1146\n",
      "34/223, train_loss: 0.1038, step time: 0.1073\n",
      "35/223, train_loss: 0.0876, step time: 0.1061\n",
      "36/223, train_loss: 0.1010, step time: 0.1009\n",
      "37/223, train_loss: 0.1046, step time: 0.1056\n",
      "38/223, train_loss: 0.1014, step time: 0.1081\n",
      "39/223, train_loss: 0.1127, step time: 0.1038\n",
      "40/223, train_loss: 0.1081, step time: 0.1076\n",
      "41/223, train_loss: 0.1022, step time: 0.1114\n",
      "42/223, train_loss: 0.1037, step time: 0.1156\n",
      "43/223, train_loss: 0.0998, step time: 0.1045\n",
      "44/223, train_loss: 0.0939, step time: 0.1125\n",
      "45/223, train_loss: 0.0943, step time: 0.1464\n",
      "46/223, train_loss: 0.1077, step time: 0.1042\n",
      "47/223, train_loss: 0.1038, step time: 0.1205\n",
      "48/223, train_loss: 0.0931, step time: 0.1207\n",
      "49/223, train_loss: 0.0953, step time: 0.1124\n",
      "50/223, train_loss: 0.1061, step time: 0.1142\n",
      "51/223, train_loss: 0.0987, step time: 0.1131\n",
      "52/223, train_loss: 0.1095, step time: 0.1004\n",
      "53/223, train_loss: 0.0880, step time: 0.1008\n",
      "54/223, train_loss: 0.0918, step time: 0.1066\n",
      "55/223, train_loss: 0.1000, step time: 0.1094\n",
      "56/223, train_loss: 0.0874, step time: 0.1003\n",
      "57/223, train_loss: 0.2937, step time: 0.1016\n",
      "58/223, train_loss: 0.0962, step time: 0.1059\n",
      "59/223, train_loss: 0.1126, step time: 0.1140\n",
      "60/223, train_loss: 0.0983, step time: 0.1008\n",
      "61/223, train_loss: 0.1179, step time: 0.1004\n",
      "62/223, train_loss: 0.0941, step time: 0.1196\n",
      "63/223, train_loss: 0.1026, step time: 0.1115\n",
      "64/223, train_loss: 0.0932, step time: 0.1007\n",
      "65/223, train_loss: 0.0940, step time: 0.1008\n",
      "66/223, train_loss: 0.0961, step time: 0.1067\n",
      "67/223, train_loss: 0.0879, step time: 0.0998\n",
      "68/223, train_loss: 0.1058, step time: 0.1004\n",
      "69/223, train_loss: 0.1097, step time: 0.1012\n",
      "70/223, train_loss: 0.1239, step time: 0.1001\n",
      "71/223, train_loss: 0.0918, step time: 0.0995\n",
      "72/223, train_loss: 0.0974, step time: 0.0994\n",
      "73/223, train_loss: 0.0946, step time: 0.1010\n",
      "74/223, train_loss: 0.1004, step time: 0.1019\n",
      "75/223, train_loss: 0.0890, step time: 0.1005\n",
      "76/223, train_loss: 0.0921, step time: 0.1007\n",
      "77/223, train_loss: 0.1032, step time: 0.1002\n",
      "78/223, train_loss: 0.0997, step time: 0.1003\n",
      "79/223, train_loss: 0.1102, step time: 0.1108\n",
      "80/223, train_loss: 0.1062, step time: 0.1007\n",
      "81/223, train_loss: 0.1050, step time: 0.1011\n",
      "82/223, train_loss: 0.0971, step time: 0.1143\n",
      "83/223, train_loss: 0.0932, step time: 0.1041\n",
      "84/223, train_loss: 0.0945, step time: 0.0999\n",
      "85/223, train_loss: 0.0989, step time: 0.1098\n",
      "86/223, train_loss: 0.1050, step time: 0.1462\n",
      "87/223, train_loss: 0.0949, step time: 0.1179\n",
      "88/223, train_loss: 0.0944, step time: 0.1104\n",
      "89/223, train_loss: 0.0981, step time: 0.1010\n",
      "90/223, train_loss: 0.0912, step time: 0.1004\n",
      "91/223, train_loss: 0.1014, step time: 0.1104\n",
      "92/223, train_loss: 0.0940, step time: 0.1005\n",
      "93/223, train_loss: 0.0963, step time: 0.1122\n",
      "94/223, train_loss: 0.0929, step time: 0.1179\n",
      "95/223, train_loss: 0.0973, step time: 0.1211\n",
      "96/223, train_loss: 0.1077, step time: 0.1008\n",
      "97/223, train_loss: 0.0956, step time: 0.1365\n",
      "98/223, train_loss: 0.1051, step time: 0.1012\n",
      "99/223, train_loss: 0.0996, step time: 0.1113\n",
      "100/223, train_loss: 0.0964, step time: 0.1852\n",
      "101/223, train_loss: 0.1073, step time: 0.1647\n",
      "102/223, train_loss: 0.1076, step time: 0.1008\n",
      "103/223, train_loss: 0.1014, step time: 0.1158\n",
      "104/223, train_loss: 0.0910, step time: 0.1004\n",
      "105/223, train_loss: 0.0930, step time: 0.1145\n",
      "106/223, train_loss: 0.1045, step time: 0.1079\n",
      "107/223, train_loss: 0.0942, step time: 0.1219\n",
      "108/223, train_loss: 0.1008, step time: 0.1140\n",
      "109/223, train_loss: 0.0867, step time: 0.1204\n",
      "110/223, train_loss: 0.0941, step time: 0.1253\n",
      "111/223, train_loss: 0.0981, step time: 0.1199\n",
      "112/223, train_loss: 0.0921, step time: 0.1030\n",
      "113/223, train_loss: 0.1079, step time: 0.1013\n",
      "114/223, train_loss: 0.0895, step time: 0.1277\n",
      "115/223, train_loss: 0.0988, step time: 0.1163\n",
      "116/223, train_loss: 0.1057, step time: 0.1171\n",
      "117/223, train_loss: 0.1009, step time: 0.1013\n",
      "118/223, train_loss: 0.0881, step time: 0.1039\n",
      "119/223, train_loss: 0.1072, step time: 0.1050\n",
      "120/223, train_loss: 0.1044, step time: 0.1079\n",
      "121/223, train_loss: 0.0905, step time: 0.1206\n",
      "122/223, train_loss: 0.0872, step time: 0.1055\n",
      "123/223, train_loss: 0.0933, step time: 0.1222\n",
      "124/223, train_loss: 0.1018, step time: 0.1125\n",
      "125/223, train_loss: 0.1050, step time: 0.1176\n",
      "126/223, train_loss: 0.0867, step time: 0.1279\n",
      "127/223, train_loss: 0.0938, step time: 0.1135\n",
      "128/223, train_loss: 0.0917, step time: 0.0995\n",
      "129/223, train_loss: 0.0944, step time: 0.1176\n",
      "130/223, train_loss: 0.0841, step time: 0.1101\n",
      "131/223, train_loss: 0.1047, step time: 0.1010\n",
      "132/223, train_loss: 0.0963, step time: 0.0999\n",
      "133/223, train_loss: 0.1043, step time: 0.0997\n",
      "134/223, train_loss: 0.0982, step time: 0.1286\n",
      "135/223, train_loss: 0.0928, step time: 0.1365\n",
      "136/223, train_loss: 0.0912, step time: 0.1007\n",
      "137/223, train_loss: 0.0944, step time: 0.1350\n",
      "138/223, train_loss: 0.0911, step time: 0.1145\n",
      "139/223, train_loss: 0.0947, step time: 0.1053\n",
      "140/223, train_loss: 0.0992, step time: 0.1026\n",
      "141/223, train_loss: 0.0993, step time: 0.1313\n",
      "142/223, train_loss: 0.0982, step time: 0.1119\n",
      "143/223, train_loss: 0.0994, step time: 0.1050\n",
      "144/223, train_loss: 0.1060, step time: 0.0998\n",
      "145/223, train_loss: 0.0969, step time: 0.1251\n",
      "146/223, train_loss: 0.0954, step time: 0.1526\n",
      "147/223, train_loss: 0.0958, step time: 0.1054\n",
      "148/223, train_loss: 0.0938, step time: 0.1078\n",
      "149/223, train_loss: 0.0975, step time: 0.1005\n",
      "150/223, train_loss: 0.1076, step time: 0.1427\n",
      "151/223, train_loss: 0.1052, step time: 0.1359\n",
      "152/223, train_loss: 0.1036, step time: 0.0999\n",
      "153/223, train_loss: 0.1110, step time: 0.1140\n",
      "154/223, train_loss: 0.0894, step time: 0.1202\n",
      "155/223, train_loss: 0.0876, step time: 0.1368\n",
      "156/223, train_loss: 0.0967, step time: 0.1413\n",
      "157/223, train_loss: 0.1015, step time: 0.1085\n",
      "158/223, train_loss: 0.0959, step time: 0.1076\n",
      "159/223, train_loss: 0.0902, step time: 0.1134\n",
      "160/223, train_loss: 0.0988, step time: 0.1105\n",
      "161/223, train_loss: 0.0967, step time: 0.1141\n",
      "162/223, train_loss: 0.0911, step time: 0.1002\n",
      "163/223, train_loss: 0.1118, step time: 0.1054\n",
      "164/223, train_loss: 0.1104, step time: 0.1266\n",
      "165/223, train_loss: 0.0916, step time: 0.1003\n",
      "166/223, train_loss: 0.0947, step time: 0.1266\n",
      "167/223, train_loss: 0.0924, step time: 0.1323\n",
      "168/223, train_loss: 0.0984, step time: 0.1179\n",
      "169/223, train_loss: 0.0945, step time: 0.1007\n",
      "170/223, train_loss: 0.0902, step time: 0.1114\n",
      "171/223, train_loss: 0.0991, step time: 0.1056\n",
      "172/223, train_loss: 0.0933, step time: 0.1007\n",
      "173/223, train_loss: 0.0957, step time: 0.1228\n",
      "174/223, train_loss: 0.1127, step time: 0.1143\n",
      "175/223, train_loss: 0.0987, step time: 0.1128\n",
      "176/223, train_loss: 0.0999, step time: 0.1243\n",
      "177/223, train_loss: 0.1035, step time: 0.1004\n",
      "178/223, train_loss: 0.0970, step time: 0.1211\n",
      "179/223, train_loss: 0.0950, step time: 0.1058\n",
      "180/223, train_loss: 0.1038, step time: 0.1158\n",
      "181/223, train_loss: 0.0970, step time: 0.1184\n",
      "182/223, train_loss: 0.0964, step time: 0.1048\n",
      "183/223, train_loss: 0.1014, step time: 0.1103\n",
      "184/223, train_loss: 0.1017, step time: 0.1009\n",
      "185/223, train_loss: 0.0963, step time: 0.1149\n",
      "186/223, train_loss: 0.0984, step time: 0.1068\n",
      "187/223, train_loss: 0.0967, step time: 0.1348\n",
      "188/223, train_loss: 0.0943, step time: 0.1321\n",
      "189/223, train_loss: 0.1077, step time: 0.1102\n",
      "190/223, train_loss: 0.0927, step time: 0.1006\n",
      "191/223, train_loss: 0.0995, step time: 0.1154\n",
      "192/223, train_loss: 0.1029, step time: 0.1232\n",
      "193/223, train_loss: 0.0976, step time: 0.1032\n",
      "194/223, train_loss: 0.1065, step time: 0.1159\n",
      "195/223, train_loss: 0.0987, step time: 0.1003\n",
      "196/223, train_loss: 0.0935, step time: 0.1163\n",
      "197/223, train_loss: 0.1064, step time: 0.1157\n",
      "198/223, train_loss: 0.1109, step time: 0.1144\n",
      "199/223, train_loss: 0.1102, step time: 0.1151\n",
      "200/223, train_loss: 0.0855, step time: 0.1278\n",
      "201/223, train_loss: 0.1027, step time: 0.1071\n",
      "202/223, train_loss: 0.0952, step time: 0.1002\n",
      "203/223, train_loss: 0.0977, step time: 0.1008\n",
      "204/223, train_loss: 0.0984, step time: 0.0997\n",
      "205/223, train_loss: 0.0903, step time: 0.1076\n",
      "206/223, train_loss: 0.0951, step time: 0.1000\n",
      "207/223, train_loss: 0.0996, step time: 0.0999\n",
      "208/223, train_loss: 0.1014, step time: 0.1006\n",
      "209/223, train_loss: 0.1005, step time: 0.1002\n",
      "210/223, train_loss: 0.1024, step time: 0.1138\n",
      "211/223, train_loss: 0.1105, step time: 0.1126\n",
      "212/223, train_loss: 0.0880, step time: 0.1334\n",
      "213/223, train_loss: 0.0983, step time: 0.1035\n",
      "214/223, train_loss: 0.0915, step time: 0.1230\n",
      "215/223, train_loss: 0.0931, step time: 0.1098\n",
      "216/223, train_loss: 0.1004, step time: 0.1260\n",
      "217/223, train_loss: 0.1031, step time: 0.1145\n",
      "218/223, train_loss: 0.0936, step time: 0.1001\n",
      "219/223, train_loss: 0.0931, step time: 0.1001\n",
      "220/223, train_loss: 0.1005, step time: 0.1004\n",
      "221/223, train_loss: 0.1040, step time: 0.1005\n",
      "222/223, train_loss: 0.1004, step time: 0.0990\n",
      "223/223, train_loss: 0.1001, step time: 0.0992\n",
      "epoch 266 average loss: 0.0996\n",
      "time consuming of epoch 266 is: 89.6180\n",
      "----------\n",
      "epoch 267/300\n",
      "1/223, train_loss: 0.0922, step time: 0.1031\n",
      "2/223, train_loss: 0.0933, step time: 0.1101\n",
      "3/223, train_loss: 0.0933, step time: 0.1116\n",
      "4/223, train_loss: 0.0845, step time: 0.1075\n",
      "5/223, train_loss: 0.0927, step time: 0.1475\n",
      "6/223, train_loss: 0.1018, step time: 0.1545\n",
      "7/223, train_loss: 0.0998, step time: 0.1012\n",
      "8/223, train_loss: 0.0977, step time: 0.1228\n",
      "9/223, train_loss: 0.0957, step time: 0.1204\n",
      "10/223, train_loss: 0.0978, step time: 0.1005\n",
      "11/223, train_loss: 0.0903, step time: 0.1019\n",
      "12/223, train_loss: 0.1071, step time: 0.1007\n",
      "13/223, train_loss: 0.1102, step time: 0.1379\n",
      "14/223, train_loss: 0.0886, step time: 0.1001\n",
      "15/223, train_loss: 0.0932, step time: 0.1230\n",
      "16/223, train_loss: 0.0996, step time: 0.1062\n",
      "17/223, train_loss: 0.1030, step time: 0.1118\n",
      "18/223, train_loss: 0.1027, step time: 0.1002\n",
      "19/223, train_loss: 0.0914, step time: 0.1162\n",
      "20/223, train_loss: 0.1088, step time: 0.1207\n",
      "21/223, train_loss: 0.0868, step time: 0.1005\n",
      "22/223, train_loss: 0.1125, step time: 0.1147\n",
      "23/223, train_loss: 0.1026, step time: 0.1138\n",
      "24/223, train_loss: 0.0985, step time: 0.1179\n",
      "25/223, train_loss: 0.1093, step time: 0.1020\n",
      "26/223, train_loss: 0.0968, step time: 0.1139\n",
      "27/223, train_loss: 0.0882, step time: 0.1061\n",
      "28/223, train_loss: 0.0862, step time: 0.1088\n",
      "29/223, train_loss: 0.1002, step time: 0.1003\n",
      "30/223, train_loss: 0.1039, step time: 0.1034\n",
      "31/223, train_loss: 0.0994, step time: 0.0996\n",
      "32/223, train_loss: 0.1102, step time: 0.1234\n",
      "33/223, train_loss: 0.1132, step time: 0.1081\n",
      "34/223, train_loss: 0.0934, step time: 0.1211\n",
      "35/223, train_loss: 0.0920, step time: 0.1256\n",
      "36/223, train_loss: 0.1023, step time: 0.1159\n",
      "37/223, train_loss: 0.0935, step time: 0.1005\n",
      "38/223, train_loss: 0.0938, step time: 0.1344\n",
      "39/223, train_loss: 0.1014, step time: 0.1261\n",
      "40/223, train_loss: 0.0972, step time: 0.1012\n",
      "41/223, train_loss: 0.1060, step time: 0.0999\n",
      "42/223, train_loss: 0.0989, step time: 0.1057\n",
      "43/223, train_loss: 0.0950, step time: 0.1002\n",
      "44/223, train_loss: 0.1049, step time: 0.1019\n",
      "45/223, train_loss: 0.1044, step time: 0.1092\n",
      "46/223, train_loss: 0.0931, step time: 0.1013\n",
      "47/223, train_loss: 0.0980, step time: 0.1140\n",
      "48/223, train_loss: 0.0942, step time: 0.1010\n",
      "49/223, train_loss: 0.0983, step time: 0.1168\n",
      "50/223, train_loss: 0.0971, step time: 0.1046\n",
      "51/223, train_loss: 0.1018, step time: 0.1596\n",
      "52/223, train_loss: 0.1016, step time: 0.1121\n",
      "53/223, train_loss: 0.1001, step time: 0.0998\n",
      "54/223, train_loss: 0.0965, step time: 0.0997\n",
      "55/223, train_loss: 0.0929, step time: 0.0997\n",
      "56/223, train_loss: 0.1004, step time: 0.1005\n",
      "57/223, train_loss: 0.0992, step time: 0.1058\n",
      "58/223, train_loss: 0.1045, step time: 0.1016\n",
      "59/223, train_loss: 0.0897, step time: 0.1032\n",
      "60/223, train_loss: 0.1018, step time: 0.1081\n",
      "61/223, train_loss: 0.0963, step time: 0.1055\n",
      "62/223, train_loss: 0.0959, step time: 0.1107\n",
      "63/223, train_loss: 0.1082, step time: 0.1006\n",
      "64/223, train_loss: 0.1011, step time: 0.1017\n",
      "65/223, train_loss: 0.0924, step time: 0.1170\n",
      "66/223, train_loss: 0.1064, step time: 0.1108\n",
      "67/223, train_loss: 0.1052, step time: 0.1002\n",
      "68/223, train_loss: 0.1066, step time: 0.1003\n",
      "69/223, train_loss: 0.1013, step time: 0.1223\n",
      "70/223, train_loss: 0.1015, step time: 0.1137\n",
      "71/223, train_loss: 0.1056, step time: 0.1011\n",
      "72/223, train_loss: 0.1000, step time: 0.1005\n",
      "73/223, train_loss: 0.0959, step time: 0.1128\n",
      "74/223, train_loss: 0.0957, step time: 0.1009\n",
      "75/223, train_loss: 0.0879, step time: 0.1181\n",
      "76/223, train_loss: 0.0994, step time: 0.1027\n",
      "77/223, train_loss: 0.0857, step time: 0.1245\n",
      "78/223, train_loss: 0.1057, step time: 0.1110\n",
      "79/223, train_loss: 0.0904, step time: 0.1001\n",
      "80/223, train_loss: 0.1068, step time: 0.1016\n",
      "81/223, train_loss: 0.1045, step time: 0.1011\n",
      "82/223, train_loss: 0.0875, step time: 0.1160\n",
      "83/223, train_loss: 0.0985, step time: 0.1005\n",
      "84/223, train_loss: 0.0975, step time: 0.1094\n",
      "85/223, train_loss: 0.1009, step time: 0.1148\n",
      "86/223, train_loss: 0.0962, step time: 0.1349\n",
      "87/223, train_loss: 0.0922, step time: 0.1241\n",
      "88/223, train_loss: 0.1012, step time: 0.1000\n",
      "89/223, train_loss: 0.1041, step time: 0.1162\n",
      "90/223, train_loss: 0.1219, step time: 0.1292\n",
      "91/223, train_loss: 0.0909, step time: 0.1254\n",
      "92/223, train_loss: 0.0928, step time: 0.1007\n",
      "93/223, train_loss: 0.0976, step time: 0.1003\n",
      "94/223, train_loss: 0.1019, step time: 0.1022\n",
      "95/223, train_loss: 0.1045, step time: 0.1055\n",
      "96/223, train_loss: 0.0985, step time: 0.1405\n",
      "97/223, train_loss: 0.1050, step time: 0.1194\n",
      "98/223, train_loss: 0.1006, step time: 0.1368\n",
      "99/223, train_loss: 0.0979, step time: 0.1408\n",
      "100/223, train_loss: 0.0885, step time: 0.1191\n",
      "101/223, train_loss: 0.0984, step time: 0.1123\n",
      "102/223, train_loss: 0.1025, step time: 0.1172\n",
      "103/223, train_loss: 0.0918, step time: 0.1131\n",
      "104/223, train_loss: 0.0941, step time: 0.1108\n",
      "105/223, train_loss: 0.0951, step time: 0.1094\n",
      "106/223, train_loss: 0.0961, step time: 0.1097\n",
      "107/223, train_loss: 0.0890, step time: 0.1230\n",
      "108/223, train_loss: 0.0984, step time: 0.1229\n",
      "109/223, train_loss: 0.0973, step time: 0.1094\n",
      "110/223, train_loss: 0.0941, step time: 0.1317\n",
      "111/223, train_loss: 0.1020, step time: 0.1200\n",
      "112/223, train_loss: 0.1099, step time: 0.1213\n",
      "113/223, train_loss: 0.1055, step time: 0.1230\n",
      "114/223, train_loss: 0.0954, step time: 0.1014\n",
      "115/223, train_loss: 0.0965, step time: 0.0995\n",
      "116/223, train_loss: 0.0959, step time: 0.1221\n",
      "117/223, train_loss: 0.0992, step time: 0.1009\n",
      "118/223, train_loss: 0.1145, step time: 0.1007\n",
      "119/223, train_loss: 0.1050, step time: 0.1026\n",
      "120/223, train_loss: 0.0924, step time: 0.1010\n",
      "121/223, train_loss: 0.1097, step time: 0.1137\n",
      "122/223, train_loss: 0.0926, step time: 0.1278\n",
      "123/223, train_loss: 0.0989, step time: 0.1012\n",
      "124/223, train_loss: 0.0975, step time: 0.1006\n",
      "125/223, train_loss: 0.0951, step time: 0.1159\n",
      "126/223, train_loss: 0.1011, step time: 0.1029\n",
      "127/223, train_loss: 0.1052, step time: 0.1002\n",
      "128/223, train_loss: 0.0975, step time: 0.1220\n",
      "129/223, train_loss: 0.0954, step time: 0.1158\n",
      "130/223, train_loss: 0.1106, step time: 0.1015\n",
      "131/223, train_loss: 0.1005, step time: 0.1019\n",
      "132/223, train_loss: 0.1045, step time: 0.1154\n",
      "133/223, train_loss: 0.0940, step time: 0.1108\n",
      "134/223, train_loss: 0.0994, step time: 0.1157\n",
      "135/223, train_loss: 0.0935, step time: 0.1098\n",
      "136/223, train_loss: 0.1020, step time: 0.1007\n",
      "137/223, train_loss: 0.0964, step time: 0.1117\n",
      "138/223, train_loss: 0.1076, step time: 0.1032\n",
      "139/223, train_loss: 0.0976, step time: 0.1865\n",
      "140/223, train_loss: 0.0985, step time: 0.1007\n",
      "141/223, train_loss: 0.1062, step time: 0.1323\n",
      "142/223, train_loss: 0.0897, step time: 0.1001\n",
      "143/223, train_loss: 0.0983, step time: 0.1134\n",
      "144/223, train_loss: 0.1040, step time: 0.1148\n",
      "145/223, train_loss: 0.1030, step time: 0.1044\n",
      "146/223, train_loss: 0.0956, step time: 0.1201\n",
      "147/223, train_loss: 0.1006, step time: 0.1201\n",
      "148/223, train_loss: 0.0959, step time: 0.1201\n",
      "149/223, train_loss: 0.1036, step time: 0.1093\n",
      "150/223, train_loss: 0.1043, step time: 0.1173\n",
      "151/223, train_loss: 0.1022, step time: 0.1312\n",
      "152/223, train_loss: 0.1081, step time: 0.1224\n",
      "153/223, train_loss: 0.1003, step time: 0.1275\n",
      "154/223, train_loss: 0.0872, step time: 0.1200\n",
      "155/223, train_loss: 0.0978, step time: 0.1086\n",
      "156/223, train_loss: 0.0989, step time: 0.1151\n",
      "157/223, train_loss: 0.0996, step time: 0.1196\n",
      "158/223, train_loss: 0.0944, step time: 0.1091\n",
      "159/223, train_loss: 0.0992, step time: 0.1168\n",
      "160/223, train_loss: 0.0949, step time: 0.1144\n",
      "161/223, train_loss: 0.0951, step time: 0.1128\n",
      "162/223, train_loss: 0.1023, step time: 0.1440\n",
      "163/223, train_loss: 0.0952, step time: 0.1357\n",
      "164/223, train_loss: 0.0897, step time: 0.1242\n",
      "165/223, train_loss: 0.1113, step time: 0.1126\n",
      "166/223, train_loss: 0.1018, step time: 0.1007\n",
      "167/223, train_loss: 0.0929, step time: 0.1073\n",
      "168/223, train_loss: 0.1005, step time: 0.1079\n",
      "169/223, train_loss: 0.0933, step time: 0.1142\n",
      "170/223, train_loss: 0.1004, step time: 0.1193\n",
      "171/223, train_loss: 0.0935, step time: 0.1235\n",
      "172/223, train_loss: 0.1027, step time: 0.1140\n",
      "173/223, train_loss: 0.1070, step time: 0.1094\n",
      "174/223, train_loss: 0.0898, step time: 0.1019\n",
      "175/223, train_loss: 0.0992, step time: 0.1352\n",
      "176/223, train_loss: 0.1141, step time: 0.1140\n",
      "177/223, train_loss: 0.1148, step time: 0.1091\n",
      "178/223, train_loss: 0.0926, step time: 0.1219\n",
      "179/223, train_loss: 0.1022, step time: 0.1138\n",
      "180/223, train_loss: 0.0925, step time: 0.1095\n",
      "181/223, train_loss: 0.1006, step time: 0.1185\n",
      "182/223, train_loss: 0.0978, step time: 0.1102\n",
      "183/223, train_loss: 0.1042, step time: 0.1313\n",
      "184/223, train_loss: 0.0874, step time: 0.1177\n",
      "185/223, train_loss: 0.0988, step time: 0.1045\n",
      "186/223, train_loss: 0.0961, step time: 0.1221\n",
      "187/223, train_loss: 0.0987, step time: 0.1087\n",
      "188/223, train_loss: 0.0953, step time: 0.1214\n",
      "189/223, train_loss: 0.0914, step time: 0.1118\n",
      "190/223, train_loss: 0.1052, step time: 0.1169\n",
      "191/223, train_loss: 0.2905, step time: 0.1275\n",
      "192/223, train_loss: 0.0902, step time: 0.1028\n",
      "193/223, train_loss: 0.0988, step time: 0.1167\n",
      "194/223, train_loss: 0.0888, step time: 0.1065\n",
      "195/223, train_loss: 0.0979, step time: 0.1303\n",
      "196/223, train_loss: 0.1068, step time: 0.1161\n",
      "197/223, train_loss: 0.0983, step time: 0.1117\n",
      "198/223, train_loss: 0.0887, step time: 0.1231\n",
      "199/223, train_loss: 0.0877, step time: 0.1130\n",
      "200/223, train_loss: 0.0988, step time: 0.0994\n",
      "201/223, train_loss: 0.0994, step time: 0.1075\n",
      "202/223, train_loss: 0.1194, step time: 0.1088\n",
      "203/223, train_loss: 0.1109, step time: 0.0998\n",
      "204/223, train_loss: 0.0994, step time: 0.1048\n",
      "205/223, train_loss: 0.1023, step time: 0.1098\n",
      "206/223, train_loss: 0.1176, step time: 0.1446\n",
      "207/223, train_loss: 0.1033, step time: 0.1007\n",
      "208/223, train_loss: 0.0926, step time: 0.1023\n",
      "209/223, train_loss: 0.0946, step time: 0.1185\n",
      "210/223, train_loss: 0.1062, step time: 0.1150\n",
      "211/223, train_loss: 0.0989, step time: 0.1000\n",
      "212/223, train_loss: 0.0915, step time: 0.1405\n",
      "213/223, train_loss: 0.0936, step time: 0.1103\n",
      "214/223, train_loss: 0.1009, step time: 0.1194\n",
      "215/223, train_loss: 0.0929, step time: 0.1348\n",
      "216/223, train_loss: 0.0952, step time: 0.1024\n",
      "217/223, train_loss: 0.0968, step time: 0.0995\n",
      "218/223, train_loss: 0.1151, step time: 0.1071\n",
      "219/223, train_loss: 0.0936, step time: 0.1001\n",
      "220/223, train_loss: 0.0973, step time: 0.1005\n",
      "221/223, train_loss: 0.0979, step time: 0.1001\n",
      "222/223, train_loss: 0.0845, step time: 0.0989\n",
      "223/223, train_loss: 0.1013, step time: 0.1001\n",
      "epoch 267 average loss: 0.0998\n",
      "time consuming of epoch 267 is: 89.2676\n",
      "----------\n",
      "epoch 268/300\n",
      "1/223, train_loss: 0.0965, step time: 0.1020\n",
      "2/223, train_loss: 0.1076, step time: 0.1058\n",
      "3/223, train_loss: 0.0953, step time: 0.1143\n",
      "4/223, train_loss: 0.0950, step time: 0.1002\n",
      "5/223, train_loss: 0.0915, step time: 0.1126\n",
      "6/223, train_loss: 0.0982, step time: 0.1047\n",
      "7/223, train_loss: 0.0938, step time: 0.1233\n",
      "8/223, train_loss: 0.0968, step time: 0.1174\n",
      "9/223, train_loss: 0.1054, step time: 0.1266\n",
      "10/223, train_loss: 0.1001, step time: 0.1190\n",
      "11/223, train_loss: 0.1023, step time: 0.1369\n",
      "12/223, train_loss: 0.0959, step time: 0.1195\n",
      "13/223, train_loss: 0.0923, step time: 0.1300\n",
      "14/223, train_loss: 0.0916, step time: 0.1007\n",
      "15/223, train_loss: 0.0929, step time: 0.1001\n",
      "16/223, train_loss: 0.1022, step time: 0.1018\n",
      "17/223, train_loss: 0.0977, step time: 0.1119\n",
      "18/223, train_loss: 0.1048, step time: 0.1008\n",
      "19/223, train_loss: 0.1042, step time: 0.1070\n",
      "20/223, train_loss: 0.1010, step time: 0.1232\n",
      "21/223, train_loss: 0.0970, step time: 0.1160\n",
      "22/223, train_loss: 0.1001, step time: 0.1009\n",
      "23/223, train_loss: 0.0912, step time: 0.1006\n",
      "24/223, train_loss: 0.0969, step time: 0.1049\n",
      "25/223, train_loss: 0.1036, step time: 0.1122\n",
      "26/223, train_loss: 0.1063, step time: 0.1221\n",
      "27/223, train_loss: 0.1088, step time: 0.0999\n",
      "28/223, train_loss: 0.1060, step time: 0.0994\n",
      "29/223, train_loss: 0.1022, step time: 0.1144\n",
      "30/223, train_loss: 0.0985, step time: 0.1083\n",
      "31/223, train_loss: 0.0945, step time: 0.1276\n",
      "32/223, train_loss: 0.1046, step time: 0.1182\n",
      "33/223, train_loss: 0.0986, step time: 0.1037\n",
      "34/223, train_loss: 0.0935, step time: 0.1117\n",
      "35/223, train_loss: 0.0856, step time: 0.1296\n",
      "36/223, train_loss: 0.1003, step time: 0.1004\n",
      "37/223, train_loss: 0.0892, step time: 0.1186\n",
      "38/223, train_loss: 0.0977, step time: 0.1229\n",
      "39/223, train_loss: 0.0977, step time: 0.1138\n",
      "40/223, train_loss: 0.1052, step time: 0.1186\n",
      "41/223, train_loss: 0.0995, step time: 0.1004\n",
      "42/223, train_loss: 0.1100, step time: 0.1002\n",
      "43/223, train_loss: 0.1056, step time: 0.1155\n",
      "44/223, train_loss: 0.1066, step time: 0.1069\n",
      "45/223, train_loss: 0.1064, step time: 0.1145\n",
      "46/223, train_loss: 0.0965, step time: 0.1211\n",
      "47/223, train_loss: 0.0939, step time: 0.1085\n",
      "48/223, train_loss: 0.0984, step time: 0.1198\n",
      "49/223, train_loss: 0.0872, step time: 0.1145\n",
      "50/223, train_loss: 0.1036, step time: 0.1150\n",
      "51/223, train_loss: 0.1035, step time: 0.1365\n",
      "52/223, train_loss: 0.0938, step time: 0.1279\n",
      "53/223, train_loss: 0.0922, step time: 0.1233\n",
      "54/223, train_loss: 0.0991, step time: 0.1100\n",
      "55/223, train_loss: 0.0976, step time: 0.1205\n",
      "56/223, train_loss: 0.1083, step time: 0.1085\n",
      "57/223, train_loss: 0.1021, step time: 0.1196\n",
      "58/223, train_loss: 0.1034, step time: 0.1261\n",
      "59/223, train_loss: 0.0985, step time: 0.1259\n",
      "60/223, train_loss: 0.1017, step time: 0.1326\n",
      "61/223, train_loss: 0.1117, step time: 0.1204\n",
      "62/223, train_loss: 0.1015, step time: 0.1173\n",
      "63/223, train_loss: 0.1060, step time: 0.1202\n",
      "64/223, train_loss: 0.0881, step time: 0.1062\n",
      "65/223, train_loss: 0.0987, step time: 0.1044\n",
      "66/223, train_loss: 0.1152, step time: 0.1062\n",
      "67/223, train_loss: 0.0962, step time: 0.1072\n",
      "68/223, train_loss: 0.0992, step time: 0.1149\n",
      "69/223, train_loss: 0.1067, step time: 0.1036\n",
      "70/223, train_loss: 0.1010, step time: 0.1192\n",
      "71/223, train_loss: 0.0972, step time: 0.1003\n",
      "72/223, train_loss: 0.1106, step time: 0.1057\n",
      "73/223, train_loss: 0.1152, step time: 0.1065\n",
      "74/223, train_loss: 0.0984, step time: 0.1001\n",
      "75/223, train_loss: 0.0985, step time: 0.1021\n",
      "76/223, train_loss: 0.0970, step time: 0.1048\n",
      "77/223, train_loss: 0.0844, step time: 0.1005\n",
      "78/223, train_loss: 0.0914, step time: 0.1125\n",
      "79/223, train_loss: 0.0969, step time: 0.1008\n",
      "80/223, train_loss: 0.1048, step time: 0.1104\n",
      "81/223, train_loss: 0.0864, step time: 0.1268\n",
      "82/223, train_loss: 0.0925, step time: 0.1029\n",
      "83/223, train_loss: 0.1036, step time: 0.1166\n",
      "84/223, train_loss: 0.0861, step time: 0.1533\n",
      "85/223, train_loss: 0.1053, step time: 0.1138\n",
      "86/223, train_loss: 0.1079, step time: 0.1201\n",
      "87/223, train_loss: 0.1102, step time: 0.1073\n",
      "88/223, train_loss: 0.0997, step time: 0.1211\n",
      "89/223, train_loss: 0.0960, step time: 0.1074\n",
      "90/223, train_loss: 0.0973, step time: 0.1146\n",
      "91/223, train_loss: 0.1005, step time: 0.1137\n",
      "92/223, train_loss: 0.0902, step time: 0.1062\n",
      "93/223, train_loss: 0.1035, step time: 0.1191\n",
      "94/223, train_loss: 0.1002, step time: 0.1582\n",
      "95/223, train_loss: 0.0917, step time: 0.1274\n",
      "96/223, train_loss: 0.1116, step time: 0.1306\n",
      "97/223, train_loss: 0.1078, step time: 0.1200\n",
      "98/223, train_loss: 0.1037, step time: 0.1077\n",
      "99/223, train_loss: 0.0973, step time: 0.1152\n",
      "100/223, train_loss: 0.0871, step time: 0.1186\n",
      "101/223, train_loss: 0.0957, step time: 0.1078\n",
      "102/223, train_loss: 0.1011, step time: 0.0999\n",
      "103/223, train_loss: 0.1006, step time: 0.1069\n",
      "104/223, train_loss: 0.0918, step time: 0.1037\n",
      "105/223, train_loss: 0.0922, step time: 0.1156\n",
      "106/223, train_loss: 0.1059, step time: 0.1066\n",
      "107/223, train_loss: 0.0979, step time: 0.1136\n",
      "108/223, train_loss: 0.0901, step time: 0.1224\n",
      "109/223, train_loss: 0.1103, step time: 0.1172\n",
      "110/223, train_loss: 0.1017, step time: 0.1282\n",
      "111/223, train_loss: 0.1019, step time: 0.1116\n",
      "112/223, train_loss: 0.0947, step time: 0.1021\n",
      "113/223, train_loss: 0.0967, step time: 0.1067\n",
      "114/223, train_loss: 0.1162, step time: 0.1104\n",
      "115/223, train_loss: 0.0922, step time: 0.1099\n",
      "116/223, train_loss: 0.0975, step time: 0.1270\n",
      "117/223, train_loss: 0.1031, step time: 0.1096\n",
      "118/223, train_loss: 0.1012, step time: 0.1386\n",
      "119/223, train_loss: 0.1115, step time: 0.1229\n",
      "120/223, train_loss: 0.0972, step time: 0.1123\n",
      "121/223, train_loss: 0.0985, step time: 0.1060\n",
      "122/223, train_loss: 0.1045, step time: 0.0998\n",
      "123/223, train_loss: 0.0989, step time: 0.1005\n",
      "124/223, train_loss: 0.0913, step time: 0.1011\n",
      "125/223, train_loss: 0.0937, step time: 0.0999\n",
      "126/223, train_loss: 0.0884, step time: 0.1186\n",
      "127/223, train_loss: 0.0964, step time: 0.1131\n",
      "128/223, train_loss: 0.0983, step time: 0.1061\n",
      "129/223, train_loss: 0.1060, step time: 0.1221\n",
      "130/223, train_loss: 0.0889, step time: 0.1143\n",
      "131/223, train_loss: 0.0870, step time: 0.1233\n",
      "132/223, train_loss: 0.0892, step time: 0.1238\n",
      "133/223, train_loss: 0.1127, step time: 0.1095\n",
      "134/223, train_loss: 0.0966, step time: 0.1053\n",
      "135/223, train_loss: 0.0894, step time: 0.1318\n",
      "136/223, train_loss: 0.0970, step time: 0.1002\n",
      "137/223, train_loss: 0.1124, step time: 0.1019\n",
      "138/223, train_loss: 0.1009, step time: 0.1014\n",
      "139/223, train_loss: 0.1119, step time: 0.1019\n",
      "140/223, train_loss: 0.0976, step time: 0.1242\n",
      "141/223, train_loss: 0.0924, step time: 0.1164\n",
      "142/223, train_loss: 0.0938, step time: 0.1192\n",
      "143/223, train_loss: 0.1139, step time: 0.1174\n",
      "144/223, train_loss: 0.1053, step time: 0.1253\n",
      "145/223, train_loss: 0.0995, step time: 0.1213\n",
      "146/223, train_loss: 0.1009, step time: 0.1048\n",
      "147/223, train_loss: 0.1033, step time: 0.1143\n",
      "148/223, train_loss: 0.0961, step time: 0.1006\n",
      "149/223, train_loss: 0.0874, step time: 0.1231\n",
      "150/223, train_loss: 0.1006, step time: 0.1057\n",
      "151/223, train_loss: 0.0909, step time: 0.1003\n",
      "152/223, train_loss: 0.0977, step time: 0.1346\n",
      "153/223, train_loss: 0.1019, step time: 0.1176\n",
      "154/223, train_loss: 0.0981, step time: 0.1178\n",
      "155/223, train_loss: 0.0928, step time: 0.1058\n",
      "156/223, train_loss: 0.0933, step time: 0.1107\n",
      "157/223, train_loss: 0.1224, step time: 0.1662\n",
      "158/223, train_loss: 0.2914, step time: 0.1004\n",
      "159/223, train_loss: 0.1068, step time: 0.1009\n",
      "160/223, train_loss: 0.0922, step time: 0.1008\n",
      "161/223, train_loss: 0.0928, step time: 0.1225\n",
      "162/223, train_loss: 0.0990, step time: 0.1065\n",
      "163/223, train_loss: 0.1091, step time: 0.1404\n",
      "164/223, train_loss: 0.0974, step time: 0.1006\n",
      "165/223, train_loss: 0.1040, step time: 0.1302\n",
      "166/223, train_loss: 0.0894, step time: 0.1027\n",
      "167/223, train_loss: 0.0934, step time: 0.1128\n",
      "168/223, train_loss: 0.0974, step time: 0.1154\n",
      "169/223, train_loss: 0.0981, step time: 0.1129\n",
      "170/223, train_loss: 0.0944, step time: 0.1051\n",
      "171/223, train_loss: 0.1103, step time: 0.1014\n",
      "172/223, train_loss: 0.1068, step time: 0.1129\n",
      "173/223, train_loss: 0.0991, step time: 0.1201\n",
      "174/223, train_loss: 0.0856, step time: 0.1137\n",
      "175/223, train_loss: 0.1065, step time: 0.1219\n",
      "176/223, train_loss: 0.0981, step time: 0.1106\n",
      "177/223, train_loss: 0.0954, step time: 0.1164\n",
      "178/223, train_loss: 0.0912, step time: 0.1489\n",
      "179/223, train_loss: 0.1014, step time: 0.1159\n",
      "180/223, train_loss: 0.0926, step time: 0.1124\n",
      "181/223, train_loss: 0.1080, step time: 0.0995\n",
      "182/223, train_loss: 0.0958, step time: 0.1119\n",
      "183/223, train_loss: 0.0967, step time: 0.1098\n",
      "184/223, train_loss: 0.1004, step time: 0.1197\n",
      "185/223, train_loss: 0.0955, step time: 0.1138\n",
      "186/223, train_loss: 0.0961, step time: 0.1079\n",
      "187/223, train_loss: 0.1028, step time: 0.0999\n",
      "188/223, train_loss: 0.0906, step time: 0.0996\n",
      "189/223, train_loss: 0.1027, step time: 0.1024\n",
      "190/223, train_loss: 0.0925, step time: 0.1016\n",
      "191/223, train_loss: 0.0951, step time: 0.1008\n",
      "192/223, train_loss: 0.1021, step time: 0.1424\n",
      "193/223, train_loss: 0.0914, step time: 0.1145\n",
      "194/223, train_loss: 0.0894, step time: 0.1158\n",
      "195/223, train_loss: 0.0967, step time: 0.1179\n",
      "196/223, train_loss: 0.1075, step time: 0.1235\n",
      "197/223, train_loss: 0.1051, step time: 0.1019\n",
      "198/223, train_loss: 0.0936, step time: 0.1003\n",
      "199/223, train_loss: 0.0992, step time: 0.1067\n",
      "200/223, train_loss: 0.0923, step time: 0.1109\n",
      "201/223, train_loss: 0.1043, step time: 0.1010\n",
      "202/223, train_loss: 0.1031, step time: 0.0999\n",
      "203/223, train_loss: 0.0919, step time: 0.1202\n",
      "204/223, train_loss: 0.0969, step time: 0.1009\n",
      "205/223, train_loss: 0.0950, step time: 0.1087\n",
      "206/223, train_loss: 0.1012, step time: 0.1286\n",
      "207/223, train_loss: 0.0897, step time: 0.1236\n",
      "208/223, train_loss: 0.0898, step time: 0.1238\n",
      "209/223, train_loss: 0.0994, step time: 0.1074\n",
      "210/223, train_loss: 0.0980, step time: 0.1042\n",
      "211/223, train_loss: 0.1080, step time: 0.1245\n",
      "212/223, train_loss: 0.0882, step time: 0.1174\n",
      "213/223, train_loss: 0.0948, step time: 0.1013\n",
      "214/223, train_loss: 0.0973, step time: 0.1010\n",
      "215/223, train_loss: 0.0957, step time: 0.1222\n",
      "216/223, train_loss: 0.1005, step time: 0.1172\n",
      "217/223, train_loss: 0.1060, step time: 0.1042\n",
      "218/223, train_loss: 0.0976, step time: 0.0991\n",
      "219/223, train_loss: 0.1039, step time: 0.0994\n",
      "220/223, train_loss: 0.0979, step time: 0.0990\n",
      "221/223, train_loss: 0.0888, step time: 0.0997\n",
      "222/223, train_loss: 0.0972, step time: 0.0994\n",
      "223/223, train_loss: 0.1022, step time: 0.0995\n",
      "epoch 268 average loss: 0.0998\n",
      "time consuming of epoch 268 is: 90.6721\n",
      "----------\n",
      "epoch 269/300\n",
      "1/223, train_loss: 0.0931, step time: 0.1064\n",
      "2/223, train_loss: 0.0919, step time: 0.1084\n",
      "3/223, train_loss: 0.0936, step time: 0.0994\n",
      "4/223, train_loss: 0.0982, step time: 0.0998\n",
      "5/223, train_loss: 0.1097, step time: 0.1095\n",
      "6/223, train_loss: 0.0864, step time: 0.1030\n",
      "7/223, train_loss: 0.1121, step time: 0.1265\n",
      "8/223, train_loss: 0.0971, step time: 0.1222\n",
      "9/223, train_loss: 0.1091, step time: 0.1000\n",
      "10/223, train_loss: 0.1053, step time: 0.1200\n",
      "11/223, train_loss: 0.1036, step time: 0.1039\n",
      "12/223, train_loss: 0.1058, step time: 0.1086\n",
      "13/223, train_loss: 0.0902, step time: 0.1266\n",
      "14/223, train_loss: 0.0960, step time: 0.1286\n",
      "15/223, train_loss: 0.0993, step time: 0.1128\n",
      "16/223, train_loss: 0.0943, step time: 0.1111\n",
      "17/223, train_loss: 0.0973, step time: 0.1093\n",
      "18/223, train_loss: 0.0933, step time: 0.1271\n",
      "19/223, train_loss: 0.0913, step time: 0.1207\n",
      "20/223, train_loss: 0.0895, step time: 0.1051\n",
      "21/223, train_loss: 0.0956, step time: 0.1238\n",
      "22/223, train_loss: 0.1086, step time: 0.1283\n",
      "23/223, train_loss: 0.0965, step time: 0.1159\n",
      "24/223, train_loss: 0.1011, step time: 0.1009\n",
      "25/223, train_loss: 0.1009, step time: 0.1267\n",
      "26/223, train_loss: 0.0927, step time: 0.1169\n",
      "27/223, train_loss: 0.0966, step time: 0.1152\n",
      "28/223, train_loss: 0.0981, step time: 0.1092\n",
      "29/223, train_loss: 0.0956, step time: 0.1146\n",
      "30/223, train_loss: 0.0948, step time: 0.1294\n",
      "31/223, train_loss: 0.0967, step time: 0.1256\n",
      "32/223, train_loss: 0.1007, step time: 0.1212\n",
      "33/223, train_loss: 0.1106, step time: 0.1211\n",
      "34/223, train_loss: 0.0918, step time: 0.1335\n",
      "35/223, train_loss: 0.0998, step time: 0.0994\n",
      "36/223, train_loss: 0.1065, step time: 0.1181\n",
      "37/223, train_loss: 0.1037, step time: 0.1014\n",
      "38/223, train_loss: 0.0971, step time: 0.1174\n",
      "39/223, train_loss: 0.1014, step time: 0.1139\n",
      "40/223, train_loss: 0.1030, step time: 0.1013\n",
      "41/223, train_loss: 0.1007, step time: 0.1284\n",
      "42/223, train_loss: 0.1105, step time: 0.1002\n",
      "43/223, train_loss: 0.0955, step time: 0.1006\n",
      "44/223, train_loss: 0.0948, step time: 0.1031\n",
      "45/223, train_loss: 0.0878, step time: 0.1007\n",
      "46/223, train_loss: 0.0907, step time: 0.1004\n",
      "47/223, train_loss: 0.1081, step time: 0.1007\n",
      "48/223, train_loss: 0.1105, step time: 0.1047\n",
      "49/223, train_loss: 0.0859, step time: 0.1097\n",
      "50/223, train_loss: 0.0974, step time: 0.1117\n",
      "51/223, train_loss: 0.1059, step time: 0.1164\n",
      "52/223, train_loss: 0.0888, step time: 0.1235\n",
      "53/223, train_loss: 0.0982, step time: 0.1024\n",
      "54/223, train_loss: 0.0978, step time: 0.0994\n",
      "55/223, train_loss: 0.0972, step time: 0.1005\n",
      "56/223, train_loss: 0.1044, step time: 0.1115\n",
      "57/223, train_loss: 0.1011, step time: 0.1098\n",
      "58/223, train_loss: 0.1055, step time: 0.1021\n",
      "59/223, train_loss: 0.0999, step time: 0.1029\n",
      "60/223, train_loss: 0.0954, step time: 0.1008\n",
      "61/223, train_loss: 0.0995, step time: 0.1145\n",
      "62/223, train_loss: 0.1120, step time: 0.1332\n",
      "63/223, train_loss: 0.1052, step time: 0.1257\n",
      "64/223, train_loss: 0.1007, step time: 0.1144\n",
      "65/223, train_loss: 0.0935, step time: 0.1285\n",
      "66/223, train_loss: 0.1099, step time: 0.1195\n",
      "67/223, train_loss: 0.0916, step time: 0.1169\n",
      "68/223, train_loss: 0.0927, step time: 0.1088\n",
      "69/223, train_loss: 0.1063, step time: 0.1164\n",
      "70/223, train_loss: 0.1015, step time: 0.1099\n",
      "71/223, train_loss: 0.1073, step time: 0.1064\n",
      "72/223, train_loss: 0.0902, step time: 0.1061\n",
      "73/223, train_loss: 0.1092, step time: 0.1139\n",
      "74/223, train_loss: 0.1054, step time: 0.1246\n",
      "75/223, train_loss: 0.0969, step time: 0.1007\n",
      "76/223, train_loss: 0.0904, step time: 0.1146\n",
      "77/223, train_loss: 0.1012, step time: 0.1223\n",
      "78/223, train_loss: 0.0907, step time: 0.0996\n",
      "79/223, train_loss: 0.0870, step time: 0.1069\n",
      "80/223, train_loss: 0.1098, step time: 0.1267\n",
      "81/223, train_loss: 0.1016, step time: 0.0998\n",
      "82/223, train_loss: 0.0976, step time: 0.0997\n",
      "83/223, train_loss: 0.0977, step time: 0.1007\n",
      "84/223, train_loss: 0.0963, step time: 0.1023\n",
      "85/223, train_loss: 0.0999, step time: 0.0996\n",
      "86/223, train_loss: 0.1037, step time: 0.0989\n",
      "87/223, train_loss: 0.0937, step time: 0.1003\n",
      "88/223, train_loss: 0.1080, step time: 0.1025\n",
      "89/223, train_loss: 0.0954, step time: 0.1000\n",
      "90/223, train_loss: 0.0964, step time: 0.0996\n",
      "91/223, train_loss: 0.1025, step time: 0.1011\n",
      "92/223, train_loss: 0.1039, step time: 0.1010\n",
      "93/223, train_loss: 0.0927, step time: 0.0993\n",
      "94/223, train_loss: 0.0934, step time: 0.0987\n",
      "95/223, train_loss: 0.0921, step time: 0.0997\n",
      "96/223, train_loss: 0.0884, step time: 0.1013\n",
      "97/223, train_loss: 0.1083, step time: 0.1003\n",
      "98/223, train_loss: 0.1113, step time: 0.1001\n",
      "99/223, train_loss: 0.0962, step time: 0.1002\n",
      "100/223, train_loss: 0.0989, step time: 0.1026\n",
      "101/223, train_loss: 0.1007, step time: 0.1109\n",
      "102/223, train_loss: 0.1054, step time: 0.1167\n",
      "103/223, train_loss: 0.1006, step time: 0.1588\n",
      "104/223, train_loss: 0.1019, step time: 0.1505\n",
      "105/223, train_loss: 0.0923, step time: 0.1091\n",
      "106/223, train_loss: 0.1030, step time: 0.1066\n",
      "107/223, train_loss: 0.1034, step time: 0.1031\n",
      "108/223, train_loss: 0.0859, step time: 0.1065\n",
      "109/223, train_loss: 0.1028, step time: 0.1113\n",
      "110/223, train_loss: 0.1065, step time: 0.1138\n",
      "111/223, train_loss: 0.0971, step time: 0.1044\n",
      "112/223, train_loss: 0.0956, step time: 0.1049\n",
      "113/223, train_loss: 0.0920, step time: 0.1105\n",
      "114/223, train_loss: 0.1080, step time: 0.1096\n",
      "115/223, train_loss: 0.0919, step time: 0.1150\n",
      "116/223, train_loss: 0.1014, step time: 0.1196\n",
      "117/223, train_loss: 0.0931, step time: 0.1147\n",
      "118/223, train_loss: 0.0936, step time: 0.1069\n",
      "119/223, train_loss: 0.1018, step time: 0.1176\n",
      "120/223, train_loss: 0.0941, step time: 0.1108\n",
      "121/223, train_loss: 0.0965, step time: 0.1066\n",
      "122/223, train_loss: 0.0982, step time: 0.1162\n",
      "123/223, train_loss: 0.0920, step time: 0.1130\n",
      "124/223, train_loss: 0.1049, step time: 0.1145\n",
      "125/223, train_loss: 0.1014, step time: 0.1093\n",
      "126/223, train_loss: 0.1074, step time: 0.1064\n",
      "127/223, train_loss: 0.0983, step time: 0.1216\n",
      "128/223, train_loss: 0.0967, step time: 0.1067\n",
      "129/223, train_loss: 0.0993, step time: 0.1175\n",
      "130/223, train_loss: 0.1023, step time: 0.1026\n",
      "131/223, train_loss: 0.1061, step time: 0.1094\n",
      "132/223, train_loss: 0.0978, step time: 0.1099\n",
      "133/223, train_loss: 0.0981, step time: 0.1204\n",
      "134/223, train_loss: 0.1051, step time: 0.1100\n",
      "135/223, train_loss: 0.0882, step time: 0.1152\n",
      "136/223, train_loss: 0.1063, step time: 0.1202\n",
      "137/223, train_loss: 0.0970, step time: 0.1149\n",
      "138/223, train_loss: 0.1050, step time: 0.1041\n",
      "139/223, train_loss: 0.0914, step time: 0.1125\n",
      "140/223, train_loss: 0.1030, step time: 0.1135\n",
      "141/223, train_loss: 0.0950, step time: 0.1172\n",
      "142/223, train_loss: 0.1042, step time: 0.1087\n",
      "143/223, train_loss: 0.0867, step time: 0.1135\n",
      "144/223, train_loss: 0.0998, step time: 0.1057\n",
      "145/223, train_loss: 0.0895, step time: 0.1138\n",
      "146/223, train_loss: 0.1058, step time: 0.1066\n",
      "147/223, train_loss: 0.1013, step time: 0.1248\n",
      "148/223, train_loss: 0.1015, step time: 0.1310\n",
      "149/223, train_loss: 0.0965, step time: 0.1114\n",
      "150/223, train_loss: 0.0924, step time: 0.1031\n",
      "151/223, train_loss: 0.0925, step time: 0.1142\n",
      "152/223, train_loss: 0.0985, step time: 0.1069\n",
      "153/223, train_loss: 0.0901, step time: 0.1089\n",
      "154/223, train_loss: 0.0952, step time: 0.1014\n",
      "155/223, train_loss: 0.0886, step time: 0.1298\n",
      "156/223, train_loss: 0.0877, step time: 0.1216\n",
      "157/223, train_loss: 0.0976, step time: 0.1108\n",
      "158/223, train_loss: 0.0900, step time: 0.1018\n",
      "159/223, train_loss: 0.1092, step time: 0.1111\n",
      "160/223, train_loss: 0.0929, step time: 0.1077\n",
      "161/223, train_loss: 0.0975, step time: 0.1135\n",
      "162/223, train_loss: 0.1011, step time: 0.1092\n",
      "163/223, train_loss: 0.1030, step time: 0.1138\n",
      "164/223, train_loss: 0.0852, step time: 0.1125\n",
      "165/223, train_loss: 0.0883, step time: 0.1092\n",
      "166/223, train_loss: 0.1049, step time: 0.1067\n",
      "167/223, train_loss: 0.1014, step time: 0.1121\n",
      "168/223, train_loss: 0.0952, step time: 0.1003\n",
      "169/223, train_loss: 0.0995, step time: 0.1152\n",
      "170/223, train_loss: 0.1049, step time: 0.1096\n",
      "171/223, train_loss: 0.0970, step time: 0.1111\n",
      "172/223, train_loss: 0.1083, step time: 0.1326\n",
      "173/223, train_loss: 0.0998, step time: 0.1359\n",
      "174/223, train_loss: 0.0949, step time: 0.1146\n",
      "175/223, train_loss: 0.1028, step time: 0.1115\n",
      "176/223, train_loss: 0.0943, step time: 0.1011\n",
      "177/223, train_loss: 0.0998, step time: 0.1518\n",
      "178/223, train_loss: 0.1068, step time: 0.1095\n",
      "179/223, train_loss: 0.0961, step time: 0.1246\n",
      "180/223, train_loss: 0.0992, step time: 0.1192\n",
      "181/223, train_loss: 0.1168, step time: 0.1038\n",
      "182/223, train_loss: 0.3044, step time: 0.1007\n",
      "183/223, train_loss: 0.0905, step time: 0.1003\n",
      "184/223, train_loss: 0.0872, step time: 0.1147\n",
      "185/223, train_loss: 0.0944, step time: 0.1139\n",
      "186/223, train_loss: 0.0951, step time: 0.1162\n",
      "187/223, train_loss: 0.1021, step time: 0.1170\n",
      "188/223, train_loss: 0.0973, step time: 0.1016\n",
      "189/223, train_loss: 0.0939, step time: 0.1291\n",
      "190/223, train_loss: 0.1030, step time: 0.0997\n",
      "191/223, train_loss: 0.0960, step time: 0.0999\n",
      "192/223, train_loss: 0.1099, step time: 0.1281\n",
      "193/223, train_loss: 0.0932, step time: 0.1150\n",
      "194/223, train_loss: 0.1068, step time: 0.1117\n",
      "195/223, train_loss: 0.0919, step time: 0.1322\n",
      "196/223, train_loss: 0.0972, step time: 0.1255\n",
      "197/223, train_loss: 0.0924, step time: 0.1007\n",
      "198/223, train_loss: 0.0861, step time: 0.1082\n",
      "199/223, train_loss: 0.0963, step time: 0.1092\n",
      "200/223, train_loss: 0.1114, step time: 0.1024\n",
      "201/223, train_loss: 0.1014, step time: 0.1109\n",
      "202/223, train_loss: 0.1030, step time: 0.1002\n",
      "203/223, train_loss: 0.0891, step time: 0.1102\n",
      "204/223, train_loss: 0.1083, step time: 0.1026\n",
      "205/223, train_loss: 0.1054, step time: 0.1406\n",
      "206/223, train_loss: 0.0952, step time: 0.1008\n",
      "207/223, train_loss: 0.1055, step time: 0.1292\n",
      "208/223, train_loss: 0.1010, step time: 0.1012\n",
      "209/223, train_loss: 0.0933, step time: 0.1063\n",
      "210/223, train_loss: 0.0958, step time: 0.1006\n",
      "211/223, train_loss: 0.1042, step time: 0.1006\n",
      "212/223, train_loss: 0.0899, step time: 0.1040\n",
      "213/223, train_loss: 0.0950, step time: 0.1059\n",
      "214/223, train_loss: 0.1072, step time: 0.1249\n",
      "215/223, train_loss: 0.0978, step time: 0.1132\n",
      "216/223, train_loss: 0.1166, step time: 0.1076\n",
      "217/223, train_loss: 0.0923, step time: 0.0998\n",
      "218/223, train_loss: 0.1077, step time: 0.1165\n",
      "219/223, train_loss: 0.0954, step time: 0.1346\n",
      "220/223, train_loss: 0.0939, step time: 0.1025\n",
      "221/223, train_loss: 0.0910, step time: 0.0988\n",
      "222/223, train_loss: 0.1050, step time: 0.0998\n",
      "223/223, train_loss: 0.1053, step time: 0.1000\n",
      "epoch 269 average loss: 0.0997\n",
      "time consuming of epoch 269 is: 93.5898\n",
      "----------\n",
      "epoch 270/300\n",
      "1/223, train_loss: 0.0879, step time: 0.1026\n",
      "2/223, train_loss: 0.1122, step time: 0.1002\n",
      "3/223, train_loss: 0.0908, step time: 0.1006\n",
      "4/223, train_loss: 0.1119, step time: 0.1041\n",
      "5/223, train_loss: 0.1030, step time: 0.1175\n",
      "6/223, train_loss: 0.0965, step time: 0.1123\n",
      "7/223, train_loss: 0.0983, step time: 0.1082\n",
      "8/223, train_loss: 0.0949, step time: 0.1217\n",
      "9/223, train_loss: 0.0914, step time: 0.0999\n",
      "10/223, train_loss: 0.0892, step time: 0.1097\n",
      "11/223, train_loss: 0.1027, step time: 0.1078\n",
      "12/223, train_loss: 0.1051, step time: 0.1446\n",
      "13/223, train_loss: 0.1031, step time: 0.1002\n",
      "14/223, train_loss: 0.0920, step time: 0.1072\n",
      "15/223, train_loss: 0.1131, step time: 0.1007\n",
      "16/223, train_loss: 0.0979, step time: 0.1007\n",
      "17/223, train_loss: 0.0922, step time: 0.1038\n",
      "18/223, train_loss: 0.0964, step time: 0.1090\n",
      "19/223, train_loss: 0.0925, step time: 0.1221\n",
      "20/223, train_loss: 0.1114, step time: 0.1206\n",
      "21/223, train_loss: 0.0993, step time: 0.1047\n",
      "22/223, train_loss: 0.1021, step time: 0.1169\n",
      "23/223, train_loss: 0.1041, step time: 0.1098\n",
      "24/223, train_loss: 0.1040, step time: 0.1112\n",
      "25/223, train_loss: 0.1014, step time: 0.1059\n",
      "26/223, train_loss: 0.0938, step time: 0.1042\n",
      "27/223, train_loss: 0.0986, step time: 0.1320\n",
      "28/223, train_loss: 0.0908, step time: 0.1138\n",
      "29/223, train_loss: 0.1041, step time: 0.1093\n",
      "30/223, train_loss: 0.0929, step time: 0.1060\n",
      "31/223, train_loss: 0.0976, step time: 0.1161\n",
      "32/223, train_loss: 0.1051, step time: 0.1162\n",
      "33/223, train_loss: 0.0931, step time: 0.0994\n",
      "34/223, train_loss: 0.1028, step time: 0.1094\n",
      "35/223, train_loss: 0.0918, step time: 0.1523\n",
      "36/223, train_loss: 0.1037, step time: 0.1155\n",
      "37/223, train_loss: 0.0961, step time: 0.1152\n",
      "38/223, train_loss: 0.0901, step time: 0.1129\n",
      "39/223, train_loss: 0.1054, step time: 0.1238\n",
      "40/223, train_loss: 0.0898, step time: 0.1122\n",
      "41/223, train_loss: 0.0982, step time: 0.1145\n",
      "42/223, train_loss: 0.1033, step time: 0.1090\n",
      "43/223, train_loss: 0.1004, step time: 0.1169\n",
      "44/223, train_loss: 0.1027, step time: 0.1017\n",
      "45/223, train_loss: 0.0920, step time: 0.1043\n",
      "46/223, train_loss: 0.1013, step time: 0.1065\n",
      "47/223, train_loss: 0.1026, step time: 0.1259\n",
      "48/223, train_loss: 0.0958, step time: 0.1263\n",
      "49/223, train_loss: 0.0940, step time: 0.1133\n",
      "50/223, train_loss: 0.0945, step time: 0.1013\n",
      "51/223, train_loss: 0.0961, step time: 0.1019\n",
      "52/223, train_loss: 0.0930, step time: 0.1043\n",
      "53/223, train_loss: 0.0994, step time: 0.1095\n",
      "54/223, train_loss: 0.1067, step time: 0.1168\n",
      "55/223, train_loss: 0.0944, step time: 0.1002\n",
      "56/223, train_loss: 0.0999, step time: 0.1002\n",
      "57/223, train_loss: 0.0951, step time: 0.1204\n",
      "58/223, train_loss: 0.0890, step time: 0.1167\n",
      "59/223, train_loss: 0.1002, step time: 0.1162\n",
      "60/223, train_loss: 0.0899, step time: 0.1256\n",
      "61/223, train_loss: 0.0901, step time: 0.1082\n",
      "62/223, train_loss: 0.1130, step time: 0.1006\n",
      "63/223, train_loss: 0.0963, step time: 0.1239\n",
      "64/223, train_loss: 0.1038, step time: 0.1008\n",
      "65/223, train_loss: 0.0926, step time: 0.1144\n",
      "66/223, train_loss: 0.1121, step time: 0.1143\n",
      "67/223, train_loss: 0.0896, step time: 0.1175\n",
      "68/223, train_loss: 0.0964, step time: 0.1022\n",
      "69/223, train_loss: 0.1014, step time: 0.1101\n",
      "70/223, train_loss: 0.1037, step time: 0.1005\n",
      "71/223, train_loss: 0.0876, step time: 0.1087\n",
      "72/223, train_loss: 0.0912, step time: 0.1002\n",
      "73/223, train_loss: 0.1032, step time: 0.1051\n",
      "74/223, train_loss: 0.1007, step time: 0.1002\n",
      "75/223, train_loss: 0.0897, step time: 0.1025\n",
      "76/223, train_loss: 0.0924, step time: 0.1010\n",
      "77/223, train_loss: 0.1046, step time: 0.1183\n",
      "78/223, train_loss: 0.0983, step time: 0.1073\n",
      "79/223, train_loss: 0.1042, step time: 0.1321\n",
      "80/223, train_loss: 0.0873, step time: 0.1070\n",
      "81/223, train_loss: 0.1053, step time: 0.1035\n",
      "82/223, train_loss: 0.0997, step time: 0.1090\n",
      "83/223, train_loss: 0.1028, step time: 0.1181\n",
      "84/223, train_loss: 0.1065, step time: 0.1071\n",
      "85/223, train_loss: 0.1030, step time: 0.1116\n",
      "86/223, train_loss: 0.1277, step time: 0.1127\n",
      "87/223, train_loss: 0.0913, step time: 0.1009\n",
      "88/223, train_loss: 0.0949, step time: 0.1015\n",
      "89/223, train_loss: 0.0964, step time: 0.1122\n",
      "90/223, train_loss: 0.0991, step time: 0.1164\n",
      "91/223, train_loss: 0.0974, step time: 0.0999\n",
      "92/223, train_loss: 0.1009, step time: 0.1000\n",
      "93/223, train_loss: 0.1106, step time: 0.1097\n",
      "94/223, train_loss: 0.0939, step time: 0.1395\n",
      "95/223, train_loss: 0.0937, step time: 0.1439\n",
      "96/223, train_loss: 0.0947, step time: 0.1761\n",
      "97/223, train_loss: 0.0989, step time: 0.1003\n",
      "98/223, train_loss: 0.0974, step time: 0.1018\n",
      "99/223, train_loss: 0.0961, step time: 0.1006\n",
      "100/223, train_loss: 0.1035, step time: 0.1012\n",
      "101/223, train_loss: 0.1148, step time: 0.1012\n",
      "102/223, train_loss: 0.0939, step time: 0.1083\n",
      "103/223, train_loss: 0.0879, step time: 0.1159\n",
      "104/223, train_loss: 0.0966, step time: 0.1011\n",
      "105/223, train_loss: 0.0983, step time: 0.1078\n",
      "106/223, train_loss: 0.1009, step time: 0.1111\n",
      "107/223, train_loss: 0.1183, step time: 0.1016\n",
      "108/223, train_loss: 0.0918, step time: 0.1020\n",
      "109/223, train_loss: 0.0929, step time: 0.1124\n",
      "110/223, train_loss: 0.1053, step time: 0.1154\n",
      "111/223, train_loss: 0.0985, step time: 0.1028\n",
      "112/223, train_loss: 0.1041, step time: 0.1032\n",
      "113/223, train_loss: 0.0875, step time: 0.1122\n",
      "114/223, train_loss: 0.1006, step time: 0.1007\n",
      "115/223, train_loss: 0.0893, step time: 0.1008\n",
      "116/223, train_loss: 0.0949, step time: 0.1009\n",
      "117/223, train_loss: 0.1029, step time: 0.1122\n",
      "118/223, train_loss: 0.1020, step time: 0.1011\n",
      "119/223, train_loss: 0.0866, step time: 0.1002\n",
      "120/223, train_loss: 0.1091, step time: 0.2077\n",
      "121/223, train_loss: 0.0902, step time: 0.1175\n",
      "122/223, train_loss: 0.0927, step time: 0.1028\n",
      "123/223, train_loss: 0.0879, step time: 0.1000\n",
      "124/223, train_loss: 0.0996, step time: 0.1009\n",
      "125/223, train_loss: 0.1031, step time: 0.1091\n",
      "126/223, train_loss: 0.0936, step time: 0.1236\n",
      "127/223, train_loss: 0.0990, step time: 0.1048\n",
      "128/223, train_loss: 0.0993, step time: 0.1072\n",
      "129/223, train_loss: 0.0967, step time: 0.1251\n",
      "130/223, train_loss: 0.0913, step time: 0.1229\n",
      "131/223, train_loss: 0.0920, step time: 0.1113\n",
      "132/223, train_loss: 0.0918, step time: 0.1099\n",
      "133/223, train_loss: 0.0948, step time: 0.1114\n",
      "134/223, train_loss: 0.1055, step time: 0.0990\n",
      "135/223, train_loss: 0.0925, step time: 0.1009\n",
      "136/223, train_loss: 0.1054, step time: 0.1005\n",
      "137/223, train_loss: 0.1056, step time: 0.1069\n",
      "138/223, train_loss: 0.0932, step time: 0.1036\n",
      "139/223, train_loss: 0.0947, step time: 0.1022\n",
      "140/223, train_loss: 0.1120, step time: 0.1097\n",
      "141/223, train_loss: 0.0983, step time: 0.1000\n",
      "142/223, train_loss: 0.0994, step time: 0.1146\n",
      "143/223, train_loss: 0.0883, step time: 0.1097\n",
      "144/223, train_loss: 0.1160, step time: 0.0995\n",
      "145/223, train_loss: 0.1119, step time: 0.1114\n",
      "146/223, train_loss: 0.1048, step time: 0.1167\n",
      "147/223, train_loss: 0.0902, step time: 0.1130\n",
      "148/223, train_loss: 0.1102, step time: 0.1191\n",
      "149/223, train_loss: 0.1056, step time: 0.1082\n",
      "150/223, train_loss: 0.1070, step time: 0.1131\n",
      "151/223, train_loss: 0.0970, step time: 0.1004\n",
      "152/223, train_loss: 0.0939, step time: 0.1012\n",
      "153/223, train_loss: 0.0911, step time: 0.1115\n",
      "154/223, train_loss: 0.1045, step time: 0.1014\n",
      "155/223, train_loss: 0.0926, step time: 0.1002\n",
      "156/223, train_loss: 0.0905, step time: 0.1007\n",
      "157/223, train_loss: 0.1029, step time: 0.1068\n",
      "158/223, train_loss: 0.0959, step time: 0.1115\n",
      "159/223, train_loss: 0.1028, step time: 0.1171\n",
      "160/223, train_loss: 0.1044, step time: 0.1227\n",
      "161/223, train_loss: 0.1095, step time: 0.1132\n",
      "162/223, train_loss: 0.0881, step time: 0.1167\n",
      "163/223, train_loss: 0.0998, step time: 0.1221\n",
      "164/223, train_loss: 0.1101, step time: 0.1051\n",
      "165/223, train_loss: 0.1051, step time: 0.1001\n",
      "166/223, train_loss: 0.0933, step time: 0.1059\n",
      "167/223, train_loss: 0.0936, step time: 0.1120\n",
      "168/223, train_loss: 0.0902, step time: 0.1197\n",
      "169/223, train_loss: 0.0955, step time: 0.1095\n",
      "170/223, train_loss: 0.1002, step time: 0.1053\n",
      "171/223, train_loss: 0.0899, step time: 0.1135\n",
      "172/223, train_loss: 0.0820, step time: 0.1346\n",
      "173/223, train_loss: 0.0983, step time: 0.1081\n",
      "174/223, train_loss: 0.0958, step time: 0.1009\n",
      "175/223, train_loss: 0.0954, step time: 0.1047\n",
      "176/223, train_loss: 0.1076, step time: 0.1223\n",
      "177/223, train_loss: 0.1038, step time: 0.1020\n",
      "178/223, train_loss: 0.1005, step time: 0.1005\n",
      "179/223, train_loss: 0.1056, step time: 0.1008\n",
      "180/223, train_loss: 0.1127, step time: 0.1086\n",
      "181/223, train_loss: 0.0922, step time: 0.1019\n",
      "182/223, train_loss: 0.1090, step time: 0.1308\n",
      "183/223, train_loss: 0.1114, step time: 0.1068\n",
      "184/223, train_loss: 0.0959, step time: 0.1203\n",
      "185/223, train_loss: 0.1037, step time: 0.1011\n",
      "186/223, train_loss: 0.0936, step time: 0.1139\n",
      "187/223, train_loss: 0.1040, step time: 0.1020\n",
      "188/223, train_loss: 0.0942, step time: 0.1164\n",
      "189/223, train_loss: 0.0974, step time: 0.1038\n",
      "190/223, train_loss: 0.0903, step time: 0.1109\n",
      "191/223, train_loss: 0.0945, step time: 0.1209\n",
      "192/223, train_loss: 0.0944, step time: 0.1088\n",
      "193/223, train_loss: 0.1081, step time: 0.1117\n",
      "194/223, train_loss: 0.1012, step time: 0.1076\n",
      "195/223, train_loss: 0.0941, step time: 0.1131\n",
      "196/223, train_loss: 0.0945, step time: 0.1686\n",
      "197/223, train_loss: 0.1052, step time: 0.1187\n",
      "198/223, train_loss: 0.0898, step time: 0.1030\n",
      "199/223, train_loss: 0.1050, step time: 0.0999\n",
      "200/223, train_loss: 0.0920, step time: 0.1451\n",
      "201/223, train_loss: 0.1153, step time: 0.1123\n",
      "202/223, train_loss: 0.0929, step time: 0.1004\n",
      "203/223, train_loss: 0.1050, step time: 0.1012\n",
      "204/223, train_loss: 0.0861, step time: 0.1527\n",
      "205/223, train_loss: 0.1096, step time: 0.1018\n",
      "206/223, train_loss: 0.0923, step time: 0.1077\n",
      "207/223, train_loss: 0.0895, step time: 0.1139\n",
      "208/223, train_loss: 0.0990, step time: 0.1050\n",
      "209/223, train_loss: 0.1000, step time: 0.1081\n",
      "210/223, train_loss: 0.1039, step time: 0.1046\n",
      "211/223, train_loss: 0.0992, step time: 0.1108\n",
      "212/223, train_loss: 0.1025, step time: 0.1016\n",
      "213/223, train_loss: 0.1110, step time: 0.1001\n",
      "214/223, train_loss: 0.0899, step time: 0.0999\n",
      "215/223, train_loss: 0.0937, step time: 0.1002\n",
      "216/223, train_loss: 0.1146, step time: 0.1344\n",
      "217/223, train_loss: 0.1047, step time: 0.1068\n",
      "218/223, train_loss: 0.1000, step time: 0.1140\n",
      "219/223, train_loss: 0.2961, step time: 0.1194\n",
      "220/223, train_loss: 0.0954, step time: 0.0993\n",
      "221/223, train_loss: 0.1021, step time: 0.1003\n",
      "222/223, train_loss: 0.0968, step time: 0.0988\n",
      "223/223, train_loss: 0.0909, step time: 0.0996\n",
      "epoch 270 average loss: 0.0997\n",
      "current epoch: 270 current mean dice: 0.8621 tc: 0.9226 wt: 0.8718 et: 0.7917\n",
      "best mean dice: 0.8622 at epoch: 260\n",
      "time consuming of epoch 270 is: 90.5778\n",
      "----------\n",
      "epoch 271/300\n",
      "1/223, train_loss: 0.0838, step time: 0.1060\n",
      "2/223, train_loss: 0.2941, step time: 0.1009\n",
      "3/223, train_loss: 0.0958, step time: 0.1158\n",
      "4/223, train_loss: 0.0964, step time: 0.1205\n",
      "5/223, train_loss: 0.0876, step time: 0.1249\n",
      "6/223, train_loss: 0.0971, step time: 0.1429\n",
      "7/223, train_loss: 0.0989, step time: 0.1140\n",
      "8/223, train_loss: 0.0943, step time: 0.1143\n",
      "9/223, train_loss: 0.1056, step time: 0.1098\n",
      "10/223, train_loss: 0.1003, step time: 0.1003\n",
      "11/223, train_loss: 0.0940, step time: 0.1001\n",
      "12/223, train_loss: 0.1070, step time: 0.1004\n",
      "13/223, train_loss: 0.0933, step time: 0.1165\n",
      "14/223, train_loss: 0.0970, step time: 0.1024\n",
      "15/223, train_loss: 0.0960, step time: 0.1110\n",
      "16/223, train_loss: 0.1008, step time: 0.1325\n",
      "17/223, train_loss: 0.0973, step time: 0.1262\n",
      "18/223, train_loss: 0.1093, step time: 0.1064\n",
      "19/223, train_loss: 0.0982, step time: 0.1378\n",
      "20/223, train_loss: 0.0899, step time: 0.1008\n",
      "21/223, train_loss: 0.0882, step time: 0.1000\n",
      "22/223, train_loss: 0.1062, step time: 0.1005\n",
      "23/223, train_loss: 0.0869, step time: 0.1259\n",
      "24/223, train_loss: 0.1073, step time: 0.1188\n",
      "25/223, train_loss: 0.0951, step time: 0.1136\n",
      "26/223, train_loss: 0.0925, step time: 0.1194\n",
      "27/223, train_loss: 0.0948, step time: 0.1147\n",
      "28/223, train_loss: 0.0979, step time: 0.1061\n",
      "29/223, train_loss: 0.1169, step time: 0.1018\n",
      "30/223, train_loss: 0.0865, step time: 0.1037\n",
      "31/223, train_loss: 0.1081, step time: 0.1373\n",
      "32/223, train_loss: 0.0990, step time: 0.1000\n",
      "33/223, train_loss: 0.0987, step time: 0.1090\n",
      "34/223, train_loss: 0.0964, step time: 0.1094\n",
      "35/223, train_loss: 0.1008, step time: 0.1188\n",
      "36/223, train_loss: 0.0954, step time: 0.1049\n",
      "37/223, train_loss: 0.1051, step time: 0.0999\n",
      "38/223, train_loss: 0.0961, step time: 0.1008\n",
      "39/223, train_loss: 0.0998, step time: 0.1011\n",
      "40/223, train_loss: 0.1037, step time: 0.1040\n",
      "41/223, train_loss: 0.0964, step time: 0.1157\n",
      "42/223, train_loss: 0.0924, step time: 0.1041\n",
      "43/223, train_loss: 0.0891, step time: 0.1006\n",
      "44/223, train_loss: 0.0930, step time: 0.1002\n",
      "45/223, train_loss: 0.0958, step time: 0.0999\n",
      "46/223, train_loss: 0.0919, step time: 0.1370\n",
      "47/223, train_loss: 0.0990, step time: 0.1568\n",
      "48/223, train_loss: 0.0971, step time: 0.1005\n",
      "49/223, train_loss: 0.0979, step time: 0.1158\n",
      "50/223, train_loss: 0.0982, step time: 0.1003\n",
      "51/223, train_loss: 0.0991, step time: 0.1103\n",
      "52/223, train_loss: 0.1051, step time: 0.1189\n",
      "53/223, train_loss: 0.1012, step time: 0.1000\n",
      "54/223, train_loss: 0.1114, step time: 0.0998\n",
      "55/223, train_loss: 0.0975, step time: 0.1011\n",
      "56/223, train_loss: 0.0967, step time: 0.1219\n",
      "57/223, train_loss: 0.0972, step time: 0.1124\n",
      "58/223, train_loss: 0.0937, step time: 0.1056\n",
      "59/223, train_loss: 0.0933, step time: 0.0999\n",
      "60/223, train_loss: 0.0875, step time: 0.1008\n",
      "61/223, train_loss: 0.0915, step time: 0.1524\n",
      "62/223, train_loss: 0.0983, step time: 0.1213\n",
      "63/223, train_loss: 0.1028, step time: 0.1182\n",
      "64/223, train_loss: 0.1027, step time: 0.1159\n",
      "65/223, train_loss: 0.1111, step time: 0.1125\n",
      "66/223, train_loss: 0.1000, step time: 0.1090\n",
      "67/223, train_loss: 0.0938, step time: 0.1139\n",
      "68/223, train_loss: 0.1009, step time: 0.1035\n",
      "69/223, train_loss: 0.0980, step time: 0.1307\n",
      "70/223, train_loss: 0.1060, step time: 0.1104\n",
      "71/223, train_loss: 0.1114, step time: 0.1089\n",
      "72/223, train_loss: 0.1024, step time: 0.1069\n",
      "73/223, train_loss: 0.0954, step time: 0.1059\n",
      "74/223, train_loss: 0.0895, step time: 0.1008\n",
      "75/223, train_loss: 0.0907, step time: 0.1093\n",
      "76/223, train_loss: 0.0900, step time: 0.1086\n",
      "77/223, train_loss: 0.0975, step time: 0.1178\n",
      "78/223, train_loss: 0.0996, step time: 0.1080\n",
      "79/223, train_loss: 0.0902, step time: 0.1304\n",
      "80/223, train_loss: 0.0995, step time: 0.1471\n",
      "81/223, train_loss: 0.0948, step time: 0.1273\n",
      "82/223, train_loss: 0.1026, step time: 0.1038\n",
      "83/223, train_loss: 0.0968, step time: 0.1002\n",
      "84/223, train_loss: 0.0932, step time: 0.1009\n",
      "85/223, train_loss: 0.0957, step time: 0.1145\n",
      "86/223, train_loss: 0.1121, step time: 0.1207\n",
      "87/223, train_loss: 0.0990, step time: 0.1161\n",
      "88/223, train_loss: 0.0986, step time: 0.1104\n",
      "89/223, train_loss: 0.0901, step time: 0.1056\n",
      "90/223, train_loss: 0.0906, step time: 0.1009\n",
      "91/223, train_loss: 0.1004, step time: 0.1117\n",
      "92/223, train_loss: 0.1147, step time: 0.1167\n",
      "93/223, train_loss: 0.0934, step time: 0.1263\n",
      "94/223, train_loss: 0.1056, step time: 0.1233\n",
      "95/223, train_loss: 0.0961, step time: 0.1258\n",
      "96/223, train_loss: 0.1077, step time: 0.1002\n",
      "97/223, train_loss: 0.0980, step time: 0.1197\n",
      "98/223, train_loss: 0.0954, step time: 0.1076\n",
      "99/223, train_loss: 0.0896, step time: 0.1319\n",
      "100/223, train_loss: 0.1079, step time: 0.1069\n",
      "101/223, train_loss: 0.1106, step time: 0.1017\n",
      "102/223, train_loss: 0.0953, step time: 0.1031\n",
      "103/223, train_loss: 0.1008, step time: 0.1105\n",
      "104/223, train_loss: 0.0936, step time: 0.1138\n",
      "105/223, train_loss: 0.1112, step time: 0.1082\n",
      "106/223, train_loss: 0.0943, step time: 0.1152\n",
      "107/223, train_loss: 0.1094, step time: 0.1079\n",
      "108/223, train_loss: 0.0936, step time: 0.1015\n",
      "109/223, train_loss: 0.0944, step time: 0.1220\n",
      "110/223, train_loss: 0.0918, step time: 0.1321\n",
      "111/223, train_loss: 0.1033, step time: 0.1281\n",
      "112/223, train_loss: 0.1017, step time: 0.1383\n",
      "113/223, train_loss: 0.1069, step time: 0.0997\n",
      "114/223, train_loss: 0.1043, step time: 0.1152\n",
      "115/223, train_loss: 0.1112, step time: 0.1162\n",
      "116/223, train_loss: 0.0941, step time: 0.1217\n",
      "117/223, train_loss: 0.1047, step time: 0.1027\n",
      "118/223, train_loss: 0.0948, step time: 0.1147\n",
      "119/223, train_loss: 0.1065, step time: 0.1278\n",
      "120/223, train_loss: 0.0931, step time: 0.1069\n",
      "121/223, train_loss: 0.1049, step time: 0.1149\n",
      "122/223, train_loss: 0.0969, step time: 0.1005\n",
      "123/223, train_loss: 0.1002, step time: 0.1191\n",
      "124/223, train_loss: 0.1080, step time: 0.1095\n",
      "125/223, train_loss: 0.0913, step time: 0.1060\n",
      "126/223, train_loss: 0.0918, step time: 0.1166\n",
      "127/223, train_loss: 0.0898, step time: 0.0990\n",
      "128/223, train_loss: 0.0939, step time: 0.1273\n",
      "129/223, train_loss: 0.0992, step time: 0.1020\n",
      "130/223, train_loss: 0.0910, step time: 0.0986\n",
      "131/223, train_loss: 0.0933, step time: 0.0990\n",
      "132/223, train_loss: 0.0981, step time: 0.1027\n",
      "133/223, train_loss: 0.1076, step time: 0.1178\n",
      "134/223, train_loss: 0.1042, step time: 0.1043\n",
      "135/223, train_loss: 0.1082, step time: 0.1060\n",
      "136/223, train_loss: 0.0918, step time: 0.1121\n",
      "137/223, train_loss: 0.0967, step time: 0.1163\n",
      "138/223, train_loss: 0.0996, step time: 0.1426\n",
      "139/223, train_loss: 0.0905, step time: 0.1098\n",
      "140/223, train_loss: 0.1096, step time: 0.1096\n",
      "141/223, train_loss: 0.1008, step time: 0.1171\n",
      "142/223, train_loss: 0.1068, step time: 0.1082\n",
      "143/223, train_loss: 0.1039, step time: 0.1230\n",
      "144/223, train_loss: 0.0983, step time: 0.1264\n",
      "145/223, train_loss: 0.1045, step time: 0.1001\n",
      "146/223, train_loss: 0.0951, step time: 0.1000\n",
      "147/223, train_loss: 0.0945, step time: 0.1009\n",
      "148/223, train_loss: 0.1021, step time: 0.1244\n",
      "149/223, train_loss: 0.1038, step time: 0.0996\n",
      "150/223, train_loss: 0.1162, step time: 0.0992\n",
      "151/223, train_loss: 0.1021, step time: 0.1005\n",
      "152/223, train_loss: 0.1023, step time: 0.1127\n",
      "153/223, train_loss: 0.0972, step time: 0.0993\n",
      "154/223, train_loss: 0.0981, step time: 0.0995\n",
      "155/223, train_loss: 0.1064, step time: 0.1003\n",
      "156/223, train_loss: 0.0978, step time: 0.1040\n",
      "157/223, train_loss: 0.0933, step time: 0.0997\n",
      "158/223, train_loss: 0.0897, step time: 0.0995\n",
      "159/223, train_loss: 0.1058, step time: 0.0995\n",
      "160/223, train_loss: 0.0967, step time: 0.1195\n",
      "161/223, train_loss: 0.1102, step time: 0.0988\n",
      "162/223, train_loss: 0.0895, step time: 0.0990\n",
      "163/223, train_loss: 0.0951, step time: 0.1001\n",
      "164/223, train_loss: 0.1093, step time: 0.1150\n",
      "165/223, train_loss: 0.0857, step time: 0.1005\n",
      "166/223, train_loss: 0.1147, step time: 0.1001\n",
      "167/223, train_loss: 0.0927, step time: 0.1013\n",
      "168/223, train_loss: 0.0984, step time: 0.1125\n",
      "169/223, train_loss: 0.1104, step time: 0.1072\n",
      "170/223, train_loss: 0.0892, step time: 0.1428\n",
      "171/223, train_loss: 0.0866, step time: 0.1348\n",
      "172/223, train_loss: 0.0969, step time: 0.1343\n",
      "173/223, train_loss: 0.0985, step time: 0.1009\n",
      "174/223, train_loss: 0.1027, step time: 0.0991\n",
      "175/223, train_loss: 0.0945, step time: 0.1042\n",
      "176/223, train_loss: 0.0862, step time: 0.1008\n",
      "177/223, train_loss: 0.0895, step time: 0.1034\n",
      "178/223, train_loss: 0.0967, step time: 0.1109\n",
      "179/223, train_loss: 0.1014, step time: 0.1281\n",
      "180/223, train_loss: 0.0904, step time: 0.1007\n",
      "181/223, train_loss: 0.0834, step time: 0.1121\n",
      "182/223, train_loss: 0.0918, step time: 0.1144\n",
      "183/223, train_loss: 0.0991, step time: 0.1254\n",
      "184/223, train_loss: 0.1070, step time: 0.1053\n",
      "185/223, train_loss: 0.1070, step time: 0.1257\n",
      "186/223, train_loss: 0.1138, step time: 0.0992\n",
      "187/223, train_loss: 0.0985, step time: 0.1016\n",
      "188/223, train_loss: 0.0969, step time: 0.1285\n",
      "189/223, train_loss: 0.1135, step time: 0.1034\n",
      "190/223, train_loss: 0.0917, step time: 0.1052\n",
      "191/223, train_loss: 0.1009, step time: 0.1094\n",
      "192/223, train_loss: 0.0975, step time: 0.1031\n",
      "193/223, train_loss: 0.1066, step time: 0.1023\n",
      "194/223, train_loss: 0.0891, step time: 0.1222\n",
      "195/223, train_loss: 0.0927, step time: 0.1006\n",
      "196/223, train_loss: 0.1107, step time: 0.0996\n",
      "197/223, train_loss: 0.1038, step time: 0.1220\n",
      "198/223, train_loss: 0.0987, step time: 0.1309\n",
      "199/223, train_loss: 0.0983, step time: 0.1033\n",
      "200/223, train_loss: 0.1024, step time: 0.1039\n",
      "201/223, train_loss: 0.0921, step time: 0.1181\n",
      "202/223, train_loss: 0.1103, step time: 0.1127\n",
      "203/223, train_loss: 0.1002, step time: 0.1004\n",
      "204/223, train_loss: 0.1013, step time: 0.1005\n",
      "205/223, train_loss: 0.0887, step time: 0.1280\n",
      "206/223, train_loss: 0.1010, step time: 0.1158\n",
      "207/223, train_loss: 0.1034, step time: 0.1110\n",
      "208/223, train_loss: 0.0990, step time: 0.1215\n",
      "209/223, train_loss: 0.0959, step time: 0.1253\n",
      "210/223, train_loss: 0.0963, step time: 0.1134\n",
      "211/223, train_loss: 0.1044, step time: 0.1349\n",
      "212/223, train_loss: 0.0903, step time: 0.1064\n",
      "213/223, train_loss: 0.0920, step time: 0.1009\n",
      "214/223, train_loss: 0.0932, step time: 0.1101\n",
      "215/223, train_loss: 0.1083, step time: 0.1230\n",
      "216/223, train_loss: 0.1204, step time: 0.1022\n",
      "217/223, train_loss: 0.0984, step time: 0.1094\n",
      "218/223, train_loss: 0.0980, step time: 0.1026\n",
      "219/223, train_loss: 0.0991, step time: 0.0998\n",
      "220/223, train_loss: 0.0930, step time: 0.1010\n",
      "221/223, train_loss: 0.1070, step time: 0.0994\n",
      "222/223, train_loss: 0.1013, step time: 0.0999\n",
      "223/223, train_loss: 0.0907, step time: 0.0996\n",
      "epoch 271 average loss: 0.0997\n",
      "time consuming of epoch 271 is: 97.1957\n",
      "----------\n",
      "epoch 272/300\n",
      "1/223, train_loss: 0.0929, step time: 0.1013\n",
      "2/223, train_loss: 0.0968, step time: 0.1065\n",
      "3/223, train_loss: 0.1072, step time: 0.1302\n",
      "4/223, train_loss: 0.0962, step time: 0.1084\n",
      "5/223, train_loss: 0.0910, step time: 0.1043\n",
      "6/223, train_loss: 0.1074, step time: 0.1263\n",
      "7/223, train_loss: 0.1024, step time: 0.1266\n",
      "8/223, train_loss: 0.0959, step time: 0.1120\n",
      "9/223, train_loss: 0.0965, step time: 0.1141\n",
      "10/223, train_loss: 0.1075, step time: 0.1044\n",
      "11/223, train_loss: 0.1072, step time: 0.1038\n",
      "12/223, train_loss: 0.0993, step time: 0.1217\n",
      "13/223, train_loss: 0.1097, step time: 0.1137\n",
      "14/223, train_loss: 0.0901, step time: 0.1168\n",
      "15/223, train_loss: 0.1134, step time: 0.1116\n",
      "16/223, train_loss: 0.0946, step time: 0.1548\n",
      "17/223, train_loss: 0.0981, step time: 0.1156\n",
      "18/223, train_loss: 0.0991, step time: 0.1236\n",
      "19/223, train_loss: 0.1017, step time: 0.1081\n",
      "20/223, train_loss: 0.0969, step time: 0.1502\n",
      "21/223, train_loss: 0.1047, step time: 0.1021\n",
      "22/223, train_loss: 0.0986, step time: 0.1091\n",
      "23/223, train_loss: 0.0993, step time: 0.1007\n",
      "24/223, train_loss: 0.1009, step time: 0.1007\n",
      "25/223, train_loss: 0.0969, step time: 0.1066\n",
      "26/223, train_loss: 0.1130, step time: 0.1176\n",
      "27/223, train_loss: 0.0960, step time: 0.0998\n",
      "28/223, train_loss: 0.0951, step time: 0.1010\n",
      "29/223, train_loss: 0.0993, step time: 0.1075\n",
      "30/223, train_loss: 0.1014, step time: 0.1409\n",
      "31/223, train_loss: 0.1068, step time: 0.1284\n",
      "32/223, train_loss: 0.0924, step time: 0.1007\n",
      "33/223, train_loss: 0.1051, step time: 0.1101\n",
      "34/223, train_loss: 0.0974, step time: 0.1312\n",
      "35/223, train_loss: 0.1044, step time: 0.1113\n",
      "36/223, train_loss: 0.1013, step time: 0.1088\n",
      "37/223, train_loss: 0.0908, step time: 0.1158\n",
      "38/223, train_loss: 0.0948, step time: 0.1049\n",
      "39/223, train_loss: 0.0967, step time: 0.1114\n",
      "40/223, train_loss: 0.1048, step time: 0.1097\n",
      "41/223, train_loss: 0.0928, step time: 0.1161\n",
      "42/223, train_loss: 0.0970, step time: 0.1176\n",
      "43/223, train_loss: 0.0994, step time: 0.1025\n",
      "44/223, train_loss: 0.1032, step time: 0.1094\n",
      "45/223, train_loss: 0.1085, step time: 0.1171\n",
      "46/223, train_loss: 0.0901, step time: 0.0995\n",
      "47/223, train_loss: 0.0988, step time: 0.0985\n",
      "48/223, train_loss: 0.0967, step time: 0.1053\n",
      "49/223, train_loss: 0.1007, step time: 0.1071\n",
      "50/223, train_loss: 0.0970, step time: 0.1106\n",
      "51/223, train_loss: 0.0918, step time: 0.1339\n",
      "52/223, train_loss: 0.1048, step time: 0.1439\n",
      "53/223, train_loss: 0.0954, step time: 0.1092\n",
      "54/223, train_loss: 0.0895, step time: 0.1133\n",
      "55/223, train_loss: 0.0938, step time: 0.1071\n",
      "56/223, train_loss: 0.1106, step time: 0.1498\n",
      "57/223, train_loss: 0.1062, step time: 0.1167\n",
      "58/223, train_loss: 0.0982, step time: 0.1007\n",
      "59/223, train_loss: 0.0985, step time: 0.1244\n",
      "60/223, train_loss: 0.0954, step time: 0.1358\n",
      "61/223, train_loss: 0.0890, step time: 0.1117\n",
      "62/223, train_loss: 0.0951, step time: 0.1068\n",
      "63/223, train_loss: 0.1062, step time: 0.1095\n",
      "64/223, train_loss: 0.1060, step time: 0.1096\n",
      "65/223, train_loss: 0.0966, step time: 0.1211\n",
      "66/223, train_loss: 0.1059, step time: 0.0994\n",
      "67/223, train_loss: 0.0929, step time: 0.1108\n",
      "68/223, train_loss: 0.0961, step time: 0.1042\n",
      "69/223, train_loss: 0.0865, step time: 0.1002\n",
      "70/223, train_loss: 0.0938, step time: 0.0998\n",
      "71/223, train_loss: 0.0900, step time: 0.1077\n",
      "72/223, train_loss: 0.0983, step time: 0.1075\n",
      "73/223, train_loss: 0.1059, step time: 0.0997\n",
      "74/223, train_loss: 0.0993, step time: 0.1260\n",
      "75/223, train_loss: 0.1144, step time: 0.1148\n",
      "76/223, train_loss: 0.0919, step time: 0.1259\n",
      "77/223, train_loss: 0.0955, step time: 0.1002\n",
      "78/223, train_loss: 0.0994, step time: 0.1000\n",
      "79/223, train_loss: 0.0964, step time: 0.1016\n",
      "80/223, train_loss: 0.0865, step time: 0.1010\n",
      "81/223, train_loss: 0.1000, step time: 0.1133\n",
      "82/223, train_loss: 0.1010, step time: 0.1207\n",
      "83/223, train_loss: 0.0996, step time: 0.1186\n",
      "84/223, train_loss: 0.0891, step time: 0.1195\n",
      "85/223, train_loss: 0.0942, step time: 0.1058\n",
      "86/223, train_loss: 0.0944, step time: 0.1091\n",
      "87/223, train_loss: 0.0954, step time: 0.1112\n",
      "88/223, train_loss: 0.1026, step time: 0.1035\n",
      "89/223, train_loss: 0.1076, step time: 0.1028\n",
      "90/223, train_loss: 0.1007, step time: 0.1008\n",
      "91/223, train_loss: 0.0935, step time: 0.1003\n",
      "92/223, train_loss: 0.0932, step time: 0.1215\n",
      "93/223, train_loss: 0.0975, step time: 0.1033\n",
      "94/223, train_loss: 0.0963, step time: 0.1367\n",
      "95/223, train_loss: 0.1008, step time: 0.1461\n",
      "96/223, train_loss: 0.1070, step time: 0.1442\n",
      "97/223, train_loss: 0.0981, step time: 0.1031\n",
      "98/223, train_loss: 0.1100, step time: 0.1217\n",
      "99/223, train_loss: 0.1019, step time: 0.1106\n",
      "100/223, train_loss: 0.0917, step time: 0.1146\n",
      "101/223, train_loss: 0.0992, step time: 0.1099\n",
      "102/223, train_loss: 0.1054, step time: 0.1005\n",
      "103/223, train_loss: 0.0952, step time: 0.1200\n",
      "104/223, train_loss: 0.0975, step time: 0.1270\n",
      "105/223, train_loss: 0.0888, step time: 0.1153\n",
      "106/223, train_loss: 0.0875, step time: 0.1115\n",
      "107/223, train_loss: 0.0885, step time: 0.1001\n",
      "108/223, train_loss: 0.1004, step time: 0.1001\n",
      "109/223, train_loss: 0.1002, step time: 0.1182\n",
      "110/223, train_loss: 0.1029, step time: 0.1112\n",
      "111/223, train_loss: 0.0992, step time: 0.1008\n",
      "112/223, train_loss: 0.0978, step time: 0.1004\n",
      "113/223, train_loss: 0.1058, step time: 0.1072\n",
      "114/223, train_loss: 0.0857, step time: 0.1002\n",
      "115/223, train_loss: 0.1015, step time: 0.1008\n",
      "116/223, train_loss: 0.1131, step time: 0.1009\n",
      "117/223, train_loss: 0.0919, step time: 0.1105\n",
      "118/223, train_loss: 0.0868, step time: 0.1220\n",
      "119/223, train_loss: 0.0976, step time: 0.1165\n",
      "120/223, train_loss: 0.1067, step time: 0.1017\n",
      "121/223, train_loss: 0.0967, step time: 0.1148\n",
      "122/223, train_loss: 0.1067, step time: 0.1626\n",
      "123/223, train_loss: 0.0925, step time: 0.1346\n",
      "124/223, train_loss: 0.0908, step time: 0.1446\n",
      "125/223, train_loss: 0.1120, step time: 0.1085\n",
      "126/223, train_loss: 0.1025, step time: 0.1004\n",
      "127/223, train_loss: 0.1112, step time: 0.1004\n",
      "128/223, train_loss: 0.1075, step time: 0.1010\n",
      "129/223, train_loss: 0.0905, step time: 0.1048\n",
      "130/223, train_loss: 0.0966, step time: 0.1078\n",
      "131/223, train_loss: 0.0895, step time: 0.1021\n",
      "132/223, train_loss: 0.0971, step time: 0.1172\n",
      "133/223, train_loss: 0.1038, step time: 0.1007\n",
      "134/223, train_loss: 0.1028, step time: 0.1119\n",
      "135/223, train_loss: 0.0975, step time: 0.1240\n",
      "136/223, train_loss: 0.1053, step time: 0.1090\n",
      "137/223, train_loss: 0.1076, step time: 0.1008\n",
      "138/223, train_loss: 0.0954, step time: 0.1263\n",
      "139/223, train_loss: 0.1054, step time: 0.1207\n",
      "140/223, train_loss: 0.1031, step time: 0.1048\n",
      "141/223, train_loss: 0.0900, step time: 0.1087\n",
      "142/223, train_loss: 0.1174, step time: 0.1014\n",
      "143/223, train_loss: 0.1083, step time: 0.1108\n",
      "144/223, train_loss: 0.1010, step time: 0.1012\n",
      "145/223, train_loss: 0.0914, step time: 0.1037\n",
      "146/223, train_loss: 0.0946, step time: 0.1069\n",
      "147/223, train_loss: 0.1109, step time: 0.1006\n",
      "148/223, train_loss: 0.0939, step time: 0.1383\n",
      "149/223, train_loss: 0.1012, step time: 0.1382\n",
      "150/223, train_loss: 0.1007, step time: 0.1265\n",
      "151/223, train_loss: 0.0899, step time: 0.1006\n",
      "152/223, train_loss: 0.0961, step time: 0.0998\n",
      "153/223, train_loss: 0.0943, step time: 0.1001\n",
      "154/223, train_loss: 0.1078, step time: 0.1007\n",
      "155/223, train_loss: 0.0944, step time: 0.0993\n",
      "156/223, train_loss: 0.0908, step time: 0.1003\n",
      "157/223, train_loss: 0.0983, step time: 0.1005\n",
      "158/223, train_loss: 0.0987, step time: 0.1109\n",
      "159/223, train_loss: 0.0924, step time: 0.0994\n",
      "160/223, train_loss: 0.1036, step time: 0.0997\n",
      "161/223, train_loss: 0.1056, step time: 0.1004\n",
      "162/223, train_loss: 0.1071, step time: 0.1004\n",
      "163/223, train_loss: 0.1054, step time: 0.1004\n",
      "164/223, train_loss: 0.0934, step time: 0.0998\n",
      "165/223, train_loss: 0.1016, step time: 0.0994\n",
      "166/223, train_loss: 0.0949, step time: 0.1000\n",
      "167/223, train_loss: 0.1187, step time: 0.1077\n",
      "168/223, train_loss: 0.1005, step time: 0.1141\n",
      "169/223, train_loss: 0.1144, step time: 0.0998\n",
      "170/223, train_loss: 0.0984, step time: 0.1048\n",
      "171/223, train_loss: 0.0994, step time: 0.1092\n",
      "172/223, train_loss: 0.0952, step time: 0.1129\n",
      "173/223, train_loss: 0.1020, step time: 0.1006\n",
      "174/223, train_loss: 0.3032, step time: 0.1123\n",
      "175/223, train_loss: 0.0909, step time: 0.1072\n",
      "176/223, train_loss: 0.0946, step time: 0.1001\n",
      "177/223, train_loss: 0.0944, step time: 0.1014\n",
      "178/223, train_loss: 0.1005, step time: 0.1010\n",
      "179/223, train_loss: 0.0934, step time: 0.1041\n",
      "180/223, train_loss: 0.0977, step time: 0.1151\n",
      "181/223, train_loss: 0.0950, step time: 0.1062\n",
      "182/223, train_loss: 0.1097, step time: 0.1007\n",
      "183/223, train_loss: 0.0893, step time: 0.1119\n",
      "184/223, train_loss: 0.0995, step time: 0.1144\n",
      "185/223, train_loss: 0.1011, step time: 0.1211\n",
      "186/223, train_loss: 0.0957, step time: 0.1005\n",
      "187/223, train_loss: 0.0900, step time: 0.1083\n",
      "188/223, train_loss: 0.0988, step time: 0.1326\n",
      "189/223, train_loss: 0.0997, step time: 0.1073\n",
      "190/223, train_loss: 0.0879, step time: 0.1006\n",
      "191/223, train_loss: 0.0924, step time: 0.1107\n",
      "192/223, train_loss: 0.0999, step time: 0.1153\n",
      "193/223, train_loss: 0.1090, step time: 0.1451\n",
      "194/223, train_loss: 0.0931, step time: 0.1224\n",
      "195/223, train_loss: 0.1002, step time: 0.1003\n",
      "196/223, train_loss: 0.0875, step time: 0.1001\n",
      "197/223, train_loss: 0.0875, step time: 0.0998\n",
      "198/223, train_loss: 0.0981, step time: 0.1003\n",
      "199/223, train_loss: 0.0947, step time: 0.1500\n",
      "200/223, train_loss: 0.0978, step time: 0.1403\n",
      "201/223, train_loss: 0.0941, step time: 0.1266\n",
      "202/223, train_loss: 0.0934, step time: 0.1131\n",
      "203/223, train_loss: 0.0967, step time: 0.1057\n",
      "204/223, train_loss: 0.0996, step time: 0.1005\n",
      "205/223, train_loss: 0.0923, step time: 0.1090\n",
      "206/223, train_loss: 0.0943, step time: 0.1377\n",
      "207/223, train_loss: 0.1143, step time: 0.1004\n",
      "208/223, train_loss: 0.1050, step time: 0.1109\n",
      "209/223, train_loss: 0.1066, step time: 0.1003\n",
      "210/223, train_loss: 0.1093, step time: 0.1026\n",
      "211/223, train_loss: 0.1039, step time: 0.1049\n",
      "212/223, train_loss: 0.0949, step time: 0.1194\n",
      "213/223, train_loss: 0.0923, step time: 0.1644\n",
      "214/223, train_loss: 0.0949, step time: 0.1387\n",
      "215/223, train_loss: 0.1086, step time: 0.1301\n",
      "216/223, train_loss: 0.0944, step time: 0.1071\n",
      "217/223, train_loss: 0.0891, step time: 0.1285\n",
      "218/223, train_loss: 0.0971, step time: 0.1095\n",
      "219/223, train_loss: 0.0894, step time: 0.1026\n",
      "220/223, train_loss: 0.0930, step time: 0.0994\n",
      "221/223, train_loss: 0.1001, step time: 0.0996\n",
      "222/223, train_loss: 0.1006, step time: 0.0995\n",
      "223/223, train_loss: 0.0968, step time: 0.1001\n",
      "epoch 272 average loss: 0.0997\n",
      "time consuming of epoch 272 is: 92.4665\n",
      "----------\n",
      "epoch 273/300\n",
      "1/223, train_loss: 0.0919, step time: 0.1127\n",
      "2/223, train_loss: 0.0970, step time: 0.1137\n",
      "3/223, train_loss: 0.0966, step time: 0.1196\n",
      "4/223, train_loss: 0.0966, step time: 0.0998\n",
      "5/223, train_loss: 0.0967, step time: 0.1188\n",
      "6/223, train_loss: 0.1064, step time: 0.1099\n",
      "7/223, train_loss: 0.0937, step time: 0.1143\n",
      "8/223, train_loss: 0.1001, step time: 0.1098\n",
      "9/223, train_loss: 0.0958, step time: 0.1161\n",
      "10/223, train_loss: 0.1219, step time: 0.1183\n",
      "11/223, train_loss: 0.1024, step time: 0.1107\n",
      "12/223, train_loss: 0.2927, step time: 0.1006\n",
      "13/223, train_loss: 0.1111, step time: 0.1176\n",
      "14/223, train_loss: 0.0917, step time: 0.1137\n",
      "15/223, train_loss: 0.1090, step time: 0.1055\n",
      "16/223, train_loss: 0.0900, step time: 0.1035\n",
      "17/223, train_loss: 0.0871, step time: 0.1589\n",
      "18/223, train_loss: 0.1065, step time: 0.1172\n",
      "19/223, train_loss: 0.1000, step time: 0.1191\n",
      "20/223, train_loss: 0.0893, step time: 0.1029\n",
      "21/223, train_loss: 0.0941, step time: 0.1200\n",
      "22/223, train_loss: 0.0875, step time: 0.1019\n",
      "23/223, train_loss: 0.0943, step time: 0.1001\n",
      "24/223, train_loss: 0.1038, step time: 0.1273\n",
      "25/223, train_loss: 0.0961, step time: 0.1082\n",
      "26/223, train_loss: 0.0946, step time: 0.1229\n",
      "27/223, train_loss: 0.1028, step time: 0.1407\n",
      "28/223, train_loss: 0.0867, step time: 0.1400\n",
      "29/223, train_loss: 0.1071, step time: 0.1127\n",
      "30/223, train_loss: 0.1081, step time: 0.1096\n",
      "31/223, train_loss: 0.0918, step time: 0.0996\n",
      "32/223, train_loss: 0.0945, step time: 0.1116\n",
      "33/223, train_loss: 0.1006, step time: 0.1107\n",
      "34/223, train_loss: 0.1009, step time: 0.1118\n",
      "35/223, train_loss: 0.0880, step time: 0.2095\n",
      "36/223, train_loss: 0.0959, step time: 0.1145\n",
      "37/223, train_loss: 0.0884, step time: 0.1106\n",
      "38/223, train_loss: 0.0948, step time: 0.1047\n",
      "39/223, train_loss: 0.0970, step time: 0.1109\n",
      "40/223, train_loss: 0.1036, step time: 0.1227\n",
      "41/223, train_loss: 0.1102, step time: 0.1146\n",
      "42/223, train_loss: 0.1000, step time: 0.1191\n",
      "43/223, train_loss: 0.1041, step time: 0.1049\n",
      "44/223, train_loss: 0.1030, step time: 0.1117\n",
      "45/223, train_loss: 0.0899, step time: 0.1149\n",
      "46/223, train_loss: 0.0925, step time: 0.1173\n",
      "47/223, train_loss: 0.0959, step time: 0.1016\n",
      "48/223, train_loss: 0.1007, step time: 0.1091\n",
      "49/223, train_loss: 0.0911, step time: 0.1128\n",
      "50/223, train_loss: 0.1066, step time: 0.1153\n",
      "51/223, train_loss: 0.0973, step time: 0.1219\n",
      "52/223, train_loss: 0.0922, step time: 0.1177\n",
      "53/223, train_loss: 0.0980, step time: 0.1003\n",
      "54/223, train_loss: 0.0869, step time: 0.1024\n",
      "55/223, train_loss: 0.1113, step time: 0.1035\n",
      "56/223, train_loss: 0.1237, step time: 0.1015\n",
      "57/223, train_loss: 0.1013, step time: 0.1159\n",
      "58/223, train_loss: 0.0878, step time: 0.1340\n",
      "59/223, train_loss: 0.0968, step time: 0.1194\n",
      "60/223, train_loss: 0.0967, step time: 0.1330\n",
      "61/223, train_loss: 0.0918, step time: 0.1075\n",
      "62/223, train_loss: 0.1109, step time: 0.1121\n",
      "63/223, train_loss: 0.1069, step time: 0.1102\n",
      "64/223, train_loss: 0.0927, step time: 0.1039\n",
      "65/223, train_loss: 0.1119, step time: 0.1208\n",
      "66/223, train_loss: 0.0910, step time: 0.1100\n",
      "67/223, train_loss: 0.0984, step time: 0.1048\n",
      "68/223, train_loss: 0.0947, step time: 0.1366\n",
      "69/223, train_loss: 0.0987, step time: 0.1156\n",
      "70/223, train_loss: 0.0917, step time: 0.1072\n",
      "71/223, train_loss: 0.0966, step time: 0.1306\n",
      "72/223, train_loss: 0.0925, step time: 0.1170\n",
      "73/223, train_loss: 0.0940, step time: 0.1063\n",
      "74/223, train_loss: 0.1035, step time: 0.1013\n",
      "75/223, train_loss: 0.0974, step time: 0.1169\n",
      "76/223, train_loss: 0.0972, step time: 0.1073\n",
      "77/223, train_loss: 0.1040, step time: 0.1144\n",
      "78/223, train_loss: 0.0964, step time: 0.1085\n",
      "79/223, train_loss: 0.0919, step time: 0.1231\n",
      "80/223, train_loss: 0.0958, step time: 0.1048\n",
      "81/223, train_loss: 0.1000, step time: 0.1110\n",
      "82/223, train_loss: 0.1011, step time: 0.1348\n",
      "83/223, train_loss: 0.0924, step time: 0.1053\n",
      "84/223, train_loss: 0.1092, step time: 0.1021\n",
      "85/223, train_loss: 0.0928, step time: 0.1119\n",
      "86/223, train_loss: 0.0953, step time: 0.1005\n",
      "87/223, train_loss: 0.0971, step time: 0.0988\n",
      "88/223, train_loss: 0.0951, step time: 0.1029\n",
      "89/223, train_loss: 0.0938, step time: 0.1115\n",
      "90/223, train_loss: 0.0980, step time: 0.1600\n",
      "91/223, train_loss: 0.0908, step time: 0.0998\n",
      "92/223, train_loss: 0.0941, step time: 0.1003\n",
      "93/223, train_loss: 0.0996, step time: 0.1099\n",
      "94/223, train_loss: 0.0931, step time: 0.1091\n",
      "95/223, train_loss: 0.1061, step time: 0.1076\n",
      "96/223, train_loss: 0.1029, step time: 0.1291\n",
      "97/223, train_loss: 0.0988, step time: 0.1156\n",
      "98/223, train_loss: 0.1024, step time: 0.1057\n",
      "99/223, train_loss: 0.0941, step time: 0.1174\n",
      "100/223, train_loss: 0.0972, step time: 0.1411\n",
      "101/223, train_loss: 0.1023, step time: 0.1117\n",
      "102/223, train_loss: 0.1051, step time: 0.1169\n",
      "103/223, train_loss: 0.1057, step time: 0.1305\n",
      "104/223, train_loss: 0.0937, step time: 0.1131\n",
      "105/223, train_loss: 0.0974, step time: 0.1009\n",
      "106/223, train_loss: 0.0926, step time: 0.1112\n",
      "107/223, train_loss: 0.1041, step time: 0.0999\n",
      "108/223, train_loss: 0.0899, step time: 0.1125\n",
      "109/223, train_loss: 0.0948, step time: 0.1148\n",
      "110/223, train_loss: 0.0918, step time: 0.1417\n",
      "111/223, train_loss: 0.1049, step time: 0.1213\n",
      "112/223, train_loss: 0.0935, step time: 0.1160\n",
      "113/223, train_loss: 0.0997, step time: 0.1087\n",
      "114/223, train_loss: 0.0975, step time: 0.1148\n",
      "115/223, train_loss: 0.1041, step time: 0.1508\n",
      "116/223, train_loss: 0.1023, step time: 0.1348\n",
      "117/223, train_loss: 0.0974, step time: 0.1095\n",
      "118/223, train_loss: 0.0950, step time: 0.1015\n",
      "119/223, train_loss: 0.1006, step time: 0.1054\n",
      "120/223, train_loss: 0.0988, step time: 0.1295\n",
      "121/223, train_loss: 0.1119, step time: 0.1104\n",
      "122/223, train_loss: 0.1049, step time: 0.1238\n",
      "123/223, train_loss: 0.1023, step time: 0.1009\n",
      "124/223, train_loss: 0.0935, step time: 0.1150\n",
      "125/223, train_loss: 0.0928, step time: 0.1088\n",
      "126/223, train_loss: 0.0869, step time: 0.1089\n",
      "127/223, train_loss: 0.1057, step time: 0.1316\n",
      "128/223, train_loss: 0.1028, step time: 0.1011\n",
      "129/223, train_loss: 0.0945, step time: 0.1019\n",
      "130/223, train_loss: 0.1023, step time: 0.1291\n",
      "131/223, train_loss: 0.0853, step time: 0.1004\n",
      "132/223, train_loss: 0.0967, step time: 0.1117\n",
      "133/223, train_loss: 0.1060, step time: 0.1084\n",
      "134/223, train_loss: 0.0979, step time: 0.1144\n",
      "135/223, train_loss: 0.1027, step time: 0.1594\n",
      "136/223, train_loss: 0.0969, step time: 0.1338\n",
      "137/223, train_loss: 0.1033, step time: 0.1152\n",
      "138/223, train_loss: 0.1060, step time: 0.1200\n",
      "139/223, train_loss: 0.1071, step time: 0.1450\n",
      "140/223, train_loss: 0.1019, step time: 0.1132\n",
      "141/223, train_loss: 0.1015, step time: 0.1008\n",
      "142/223, train_loss: 0.0923, step time: 0.1271\n",
      "143/223, train_loss: 0.1019, step time: 0.1229\n",
      "144/223, train_loss: 0.1036, step time: 0.1193\n",
      "145/223, train_loss: 0.0945, step time: 0.1062\n",
      "146/223, train_loss: 0.0960, step time: 0.1145\n",
      "147/223, train_loss: 0.0982, step time: 0.1005\n",
      "148/223, train_loss: 0.1101, step time: 0.1000\n",
      "149/223, train_loss: 0.0941, step time: 0.1405\n",
      "150/223, train_loss: 0.0941, step time: 0.1179\n",
      "151/223, train_loss: 0.1125, step time: 0.1131\n",
      "152/223, train_loss: 0.1023, step time: 0.1004\n",
      "153/223, train_loss: 0.1066, step time: 0.1070\n",
      "154/223, train_loss: 0.0959, step time: 0.1228\n",
      "155/223, train_loss: 0.0938, step time: 0.1219\n",
      "156/223, train_loss: 0.0940, step time: 0.1344\n",
      "157/223, train_loss: 0.1032, step time: 0.1250\n",
      "158/223, train_loss: 0.0968, step time: 0.1143\n",
      "159/223, train_loss: 0.1024, step time: 0.1310\n",
      "160/223, train_loss: 0.1026, step time: 0.1135\n",
      "161/223, train_loss: 0.0852, step time: 0.1181\n",
      "162/223, train_loss: 0.1021, step time: 0.1180\n",
      "163/223, train_loss: 0.0941, step time: 0.1222\n",
      "164/223, train_loss: 0.0974, step time: 0.1035\n",
      "165/223, train_loss: 0.0990, step time: 0.1357\n",
      "166/223, train_loss: 0.1008, step time: 0.1181\n",
      "167/223, train_loss: 0.1013, step time: 0.1056\n",
      "168/223, train_loss: 0.1013, step time: 0.1062\n",
      "169/223, train_loss: 0.1001, step time: 0.1142\n",
      "170/223, train_loss: 0.1058, step time: 0.0999\n",
      "171/223, train_loss: 0.1005, step time: 0.1139\n",
      "172/223, train_loss: 0.1072, step time: 0.1107\n",
      "173/223, train_loss: 0.1136, step time: 0.1137\n",
      "174/223, train_loss: 0.1013, step time: 0.1123\n",
      "175/223, train_loss: 0.0955, step time: 0.1321\n",
      "176/223, train_loss: 0.0866, step time: 0.1006\n",
      "177/223, train_loss: 0.1005, step time: 0.1338\n",
      "178/223, train_loss: 0.0918, step time: 0.1195\n",
      "179/223, train_loss: 0.1086, step time: 0.1061\n",
      "180/223, train_loss: 0.0931, step time: 0.1117\n",
      "181/223, train_loss: 0.0970, step time: 0.0998\n",
      "182/223, train_loss: 0.1060, step time: 0.1008\n",
      "183/223, train_loss: 0.0875, step time: 0.1258\n",
      "184/223, train_loss: 0.0993, step time: 0.0999\n",
      "185/223, train_loss: 0.1086, step time: 0.1146\n",
      "186/223, train_loss: 0.1096, step time: 0.1142\n",
      "187/223, train_loss: 0.0932, step time: 0.1004\n",
      "188/223, train_loss: 0.0973, step time: 0.1100\n",
      "189/223, train_loss: 0.0948, step time: 0.1005\n",
      "190/223, train_loss: 0.0966, step time: 0.1011\n",
      "191/223, train_loss: 0.0872, step time: 0.1007\n",
      "192/223, train_loss: 0.0959, step time: 0.1002\n",
      "193/223, train_loss: 0.0931, step time: 0.1201\n",
      "194/223, train_loss: 0.1031, step time: 0.1081\n",
      "195/223, train_loss: 0.1002, step time: 0.1002\n",
      "196/223, train_loss: 0.0990, step time: 0.1198\n",
      "197/223, train_loss: 0.1093, step time: 0.1001\n",
      "198/223, train_loss: 0.0922, step time: 0.1008\n",
      "199/223, train_loss: 0.0906, step time: 0.1005\n",
      "200/223, train_loss: 0.0941, step time: 0.1047\n",
      "201/223, train_loss: 0.1007, step time: 0.1131\n",
      "202/223, train_loss: 0.1023, step time: 0.1384\n",
      "203/223, train_loss: 0.1070, step time: 0.1140\n",
      "204/223, train_loss: 0.1111, step time: 0.1091\n",
      "205/223, train_loss: 0.1152, step time: 0.1246\n",
      "206/223, train_loss: 0.1010, step time: 0.1248\n",
      "207/223, train_loss: 0.0957, step time: 0.1002\n",
      "208/223, train_loss: 0.0891, step time: 0.1014\n",
      "209/223, train_loss: 0.0937, step time: 0.1109\n",
      "210/223, train_loss: 0.1039, step time: 0.1069\n",
      "211/223, train_loss: 0.1021, step time: 0.1159\n",
      "212/223, train_loss: 0.0923, step time: 0.1192\n",
      "213/223, train_loss: 0.0968, step time: 0.1136\n",
      "214/223, train_loss: 0.1138, step time: 0.1116\n",
      "215/223, train_loss: 0.0955, step time: 0.1215\n",
      "216/223, train_loss: 0.1082, step time: 0.1111\n",
      "217/223, train_loss: 0.1054, step time: 0.1119\n",
      "218/223, train_loss: 0.1040, step time: 0.1191\n",
      "219/223, train_loss: 0.0982, step time: 0.1176\n",
      "220/223, train_loss: 0.1043, step time: 0.1104\n",
      "221/223, train_loss: 0.0962, step time: 0.0994\n",
      "222/223, train_loss: 0.0868, step time: 0.1000\n",
      "223/223, train_loss: 0.0913, step time: 0.1000\n",
      "epoch 273 average loss: 0.0997\n",
      "time consuming of epoch 273 is: 90.7629\n",
      "----------\n",
      "epoch 274/300\n",
      "1/223, train_loss: 0.0898, step time: 0.1178\n",
      "2/223, train_loss: 0.0874, step time: 0.1357\n",
      "3/223, train_loss: 0.1006, step time: 0.1179\n",
      "4/223, train_loss: 0.0985, step time: 0.1136\n",
      "5/223, train_loss: 0.0923, step time: 0.1099\n",
      "6/223, train_loss: 0.0966, step time: 0.1119\n",
      "7/223, train_loss: 0.0928, step time: 0.1063\n",
      "8/223, train_loss: 0.0913, step time: 0.1112\n",
      "9/223, train_loss: 0.0945, step time: 0.1235\n",
      "10/223, train_loss: 0.0984, step time: 0.1100\n",
      "11/223, train_loss: 0.0916, step time: 0.1071\n",
      "12/223, train_loss: 0.2999, step time: 0.1003\n",
      "13/223, train_loss: 0.0981, step time: 0.1145\n",
      "14/223, train_loss: 0.0916, step time: 0.1086\n",
      "15/223, train_loss: 0.0883, step time: 0.1001\n",
      "16/223, train_loss: 0.0910, step time: 0.1005\n",
      "17/223, train_loss: 0.0967, step time: 0.1072\n",
      "18/223, train_loss: 0.0982, step time: 0.1002\n",
      "19/223, train_loss: 0.0930, step time: 0.1230\n",
      "20/223, train_loss: 0.0961, step time: 0.1080\n",
      "21/223, train_loss: 0.0916, step time: 0.1153\n",
      "22/223, train_loss: 0.1013, step time: 0.1025\n",
      "23/223, train_loss: 0.0955, step time: 0.1002\n",
      "24/223, train_loss: 0.0896, step time: 0.1004\n",
      "25/223, train_loss: 0.0890, step time: 0.1131\n",
      "26/223, train_loss: 0.1236, step time: 0.0997\n",
      "27/223, train_loss: 0.1012, step time: 0.1032\n",
      "28/223, train_loss: 0.0929, step time: 0.1059\n",
      "29/223, train_loss: 0.0910, step time: 0.1128\n",
      "30/223, train_loss: 0.0942, step time: 0.1057\n",
      "31/223, train_loss: 0.0996, step time: 0.1115\n",
      "32/223, train_loss: 0.0906, step time: 0.1170\n",
      "33/223, train_loss: 0.1062, step time: 0.1025\n",
      "34/223, train_loss: 0.0992, step time: 0.1059\n",
      "35/223, train_loss: 0.1045, step time: 0.1055\n",
      "36/223, train_loss: 0.1000, step time: 0.1488\n",
      "37/223, train_loss: 0.0983, step time: 0.1004\n",
      "38/223, train_loss: 0.1003, step time: 0.1045\n",
      "39/223, train_loss: 0.1059, step time: 0.1233\n",
      "40/223, train_loss: 0.0884, step time: 0.1273\n",
      "41/223, train_loss: 0.1050, step time: 0.1047\n",
      "42/223, train_loss: 0.1078, step time: 0.1006\n",
      "43/223, train_loss: 0.1046, step time: 0.1003\n",
      "44/223, train_loss: 0.1041, step time: 0.1192\n",
      "45/223, train_loss: 0.1039, step time: 0.1068\n",
      "46/223, train_loss: 0.1023, step time: 0.1021\n",
      "47/223, train_loss: 0.0974, step time: 0.1110\n",
      "48/223, train_loss: 0.0996, step time: 0.1008\n",
      "49/223, train_loss: 0.1111, step time: 0.1003\n",
      "50/223, train_loss: 0.0981, step time: 0.1002\n",
      "51/223, train_loss: 0.0956, step time: 0.1008\n",
      "52/223, train_loss: 0.1045, step time: 0.1023\n",
      "53/223, train_loss: 0.1093, step time: 0.1085\n",
      "54/223, train_loss: 0.0912, step time: 0.1064\n",
      "55/223, train_loss: 0.1063, step time: 0.1071\n",
      "56/223, train_loss: 0.1064, step time: 0.1071\n",
      "57/223, train_loss: 0.0951, step time: 0.1005\n",
      "58/223, train_loss: 0.0990, step time: 0.1231\n",
      "59/223, train_loss: 0.0929, step time: 0.1206\n",
      "60/223, train_loss: 0.1029, step time: 0.1097\n",
      "61/223, train_loss: 0.1097, step time: 0.1181\n",
      "62/223, train_loss: 0.1026, step time: 0.1092\n",
      "63/223, train_loss: 0.1113, step time: 0.1052\n",
      "64/223, train_loss: 0.0973, step time: 0.1172\n",
      "65/223, train_loss: 0.0935, step time: 0.1219\n",
      "66/223, train_loss: 0.1006, step time: 0.1114\n",
      "67/223, train_loss: 0.1097, step time: 0.1059\n",
      "68/223, train_loss: 0.1047, step time: 0.1073\n",
      "69/223, train_loss: 0.0980, step time: 0.1042\n",
      "70/223, train_loss: 0.0931, step time: 0.0998\n",
      "71/223, train_loss: 0.0971, step time: 0.1007\n",
      "72/223, train_loss: 0.0947, step time: 0.1000\n",
      "73/223, train_loss: 0.1003, step time: 0.1011\n",
      "74/223, train_loss: 0.1072, step time: 0.0995\n",
      "75/223, train_loss: 0.1036, step time: 0.0998\n",
      "76/223, train_loss: 0.1145, step time: 0.1010\n",
      "77/223, train_loss: 0.0857, step time: 0.1023\n",
      "78/223, train_loss: 0.1021, step time: 0.0990\n",
      "79/223, train_loss: 0.0836, step time: 0.1001\n",
      "80/223, train_loss: 0.1044, step time: 0.1007\n",
      "81/223, train_loss: 0.1048, step time: 0.1000\n",
      "82/223, train_loss: 0.1031, step time: 0.0987\n",
      "83/223, train_loss: 0.1021, step time: 0.1001\n",
      "84/223, train_loss: 0.1108, step time: 0.1014\n",
      "85/223, train_loss: 0.0979, step time: 0.1010\n",
      "86/223, train_loss: 0.0924, step time: 0.1014\n",
      "87/223, train_loss: 0.0904, step time: 0.0991\n",
      "88/223, train_loss: 0.0991, step time: 0.1048\n",
      "89/223, train_loss: 0.0974, step time: 0.1065\n",
      "90/223, train_loss: 0.1047, step time: 0.1191\n",
      "91/223, train_loss: 0.0961, step time: 0.1005\n",
      "92/223, train_loss: 0.0893, step time: 0.1064\n",
      "93/223, train_loss: 0.1077, step time: 0.1233\n",
      "94/223, train_loss: 0.1046, step time: 0.1056\n",
      "95/223, train_loss: 0.0976, step time: 0.1007\n",
      "96/223, train_loss: 0.0976, step time: 0.1008\n",
      "97/223, train_loss: 0.0906, step time: 0.1317\n",
      "98/223, train_loss: 0.1003, step time: 0.1186\n",
      "99/223, train_loss: 0.0939, step time: 0.1123\n",
      "100/223, train_loss: 0.1082, step time: 0.0996\n",
      "101/223, train_loss: 0.0993, step time: 0.1126\n",
      "102/223, train_loss: 0.1017, step time: 0.1067\n",
      "103/223, train_loss: 0.1022, step time: 0.1010\n",
      "104/223, train_loss: 0.1029, step time: 0.1007\n",
      "105/223, train_loss: 0.0975, step time: 0.1051\n",
      "106/223, train_loss: 0.0959, step time: 0.1137\n",
      "107/223, train_loss: 0.0874, step time: 0.1023\n",
      "108/223, train_loss: 0.0994, step time: 0.1173\n",
      "109/223, train_loss: 0.1068, step time: 0.1112\n",
      "110/223, train_loss: 0.0946, step time: 0.1036\n",
      "111/223, train_loss: 0.1050, step time: 0.1059\n",
      "112/223, train_loss: 0.0978, step time: 0.1006\n",
      "113/223, train_loss: 0.1110, step time: 0.1063\n",
      "114/223, train_loss: 0.1068, step time: 0.1160\n",
      "115/223, train_loss: 0.1039, step time: 0.1005\n",
      "116/223, train_loss: 0.1012, step time: 0.1005\n",
      "117/223, train_loss: 0.0950, step time: 0.1002\n",
      "118/223, train_loss: 0.0924, step time: 0.1075\n",
      "119/223, train_loss: 0.0954, step time: 0.1083\n",
      "120/223, train_loss: 0.0985, step time: 0.1009\n",
      "121/223, train_loss: 0.1022, step time: 0.1058\n",
      "122/223, train_loss: 0.0931, step time: 0.1043\n",
      "123/223, train_loss: 0.0986, step time: 0.1603\n",
      "124/223, train_loss: 0.0861, step time: 0.1182\n",
      "125/223, train_loss: 0.1041, step time: 0.1048\n",
      "126/223, train_loss: 0.1000, step time: 0.1071\n",
      "127/223, train_loss: 0.1118, step time: 0.1107\n",
      "128/223, train_loss: 0.1131, step time: 0.1401\n",
      "129/223, train_loss: 0.1010, step time: 0.1156\n",
      "130/223, train_loss: 0.1045, step time: 0.1144\n",
      "131/223, train_loss: 0.1210, step time: 0.1141\n",
      "132/223, train_loss: 0.0903, step time: 0.1206\n",
      "133/223, train_loss: 0.0948, step time: 0.1052\n",
      "134/223, train_loss: 0.1183, step time: 0.1156\n",
      "135/223, train_loss: 0.1017, step time: 0.1117\n",
      "136/223, train_loss: 0.0931, step time: 0.1133\n",
      "137/223, train_loss: 0.0949, step time: 0.1239\n",
      "138/223, train_loss: 0.0902, step time: 0.1174\n",
      "139/223, train_loss: 0.0966, step time: 0.1123\n",
      "140/223, train_loss: 0.0965, step time: 0.1167\n",
      "141/223, train_loss: 0.0927, step time: 0.1111\n",
      "142/223, train_loss: 0.0888, step time: 0.0998\n",
      "143/223, train_loss: 0.1076, step time: 0.1046\n",
      "144/223, train_loss: 0.1025, step time: 0.1213\n",
      "145/223, train_loss: 0.0896, step time: 0.1177\n",
      "146/223, train_loss: 0.1011, step time: 0.1176\n",
      "147/223, train_loss: 0.1062, step time: 0.1210\n",
      "148/223, train_loss: 0.0892, step time: 0.1225\n",
      "149/223, train_loss: 0.1037, step time: 0.1088\n",
      "150/223, train_loss: 0.1052, step time: 0.1215\n",
      "151/223, train_loss: 0.0978, step time: 0.1128\n",
      "152/223, train_loss: 0.1005, step time: 0.1071\n",
      "153/223, train_loss: 0.0948, step time: 0.1056\n",
      "154/223, train_loss: 0.1038, step time: 0.0994\n",
      "155/223, train_loss: 0.1022, step time: 0.1136\n",
      "156/223, train_loss: 0.1009, step time: 0.1081\n",
      "157/223, train_loss: 0.0919, step time: 0.1113\n",
      "158/223, train_loss: 0.0905, step time: 0.1080\n",
      "159/223, train_loss: 0.0956, step time: 0.1025\n",
      "160/223, train_loss: 0.0919, step time: 0.1061\n",
      "161/223, train_loss: 0.0921, step time: 0.1096\n",
      "162/223, train_loss: 0.0993, step time: 0.1005\n",
      "163/223, train_loss: 0.0969, step time: 0.1011\n",
      "164/223, train_loss: 0.1083, step time: 0.1144\n",
      "165/223, train_loss: 0.0909, step time: 0.0999\n",
      "166/223, train_loss: 0.1106, step time: 0.1001\n",
      "167/223, train_loss: 0.0918, step time: 0.1003\n",
      "168/223, train_loss: 0.0906, step time: 0.1005\n",
      "169/223, train_loss: 0.0947, step time: 0.1065\n",
      "170/223, train_loss: 0.0886, step time: 0.1000\n",
      "171/223, train_loss: 0.0974, step time: 0.1095\n",
      "172/223, train_loss: 0.1060, step time: 0.1153\n",
      "173/223, train_loss: 0.0930, step time: 0.1005\n",
      "174/223, train_loss: 0.1040, step time: 0.1136\n",
      "175/223, train_loss: 0.1051, step time: 0.1089\n",
      "176/223, train_loss: 0.0961, step time: 0.1147\n",
      "177/223, train_loss: 0.0869, step time: 0.1183\n",
      "178/223, train_loss: 0.0986, step time: 0.1030\n",
      "179/223, train_loss: 0.1055, step time: 0.1136\n",
      "180/223, train_loss: 0.0980, step time: 0.1211\n",
      "181/223, train_loss: 0.0948, step time: 0.1086\n",
      "182/223, train_loss: 0.1024, step time: 0.1105\n",
      "183/223, train_loss: 0.1141, step time: 0.1127\n",
      "184/223, train_loss: 0.0984, step time: 0.1616\n",
      "185/223, train_loss: 0.0905, step time: 0.1284\n",
      "186/223, train_loss: 0.1001, step time: 0.1183\n",
      "187/223, train_loss: 0.0985, step time: 0.1050\n",
      "188/223, train_loss: 0.0903, step time: 0.1005\n",
      "189/223, train_loss: 0.0944, step time: 0.1059\n",
      "190/223, train_loss: 0.0933, step time: 0.1001\n",
      "191/223, train_loss: 0.1052, step time: 0.1000\n",
      "192/223, train_loss: 0.1009, step time: 0.1057\n",
      "193/223, train_loss: 0.1060, step time: 0.1103\n",
      "194/223, train_loss: 0.1022, step time: 0.1002\n",
      "195/223, train_loss: 0.1022, step time: 0.1001\n",
      "196/223, train_loss: 0.0956, step time: 0.1014\n",
      "197/223, train_loss: 0.1013, step time: 0.1013\n",
      "198/223, train_loss: 0.1006, step time: 0.1042\n",
      "199/223, train_loss: 0.1012, step time: 0.1001\n",
      "200/223, train_loss: 0.0964, step time: 0.1070\n",
      "201/223, train_loss: 0.0988, step time: 0.1052\n",
      "202/223, train_loss: 0.0904, step time: 0.1095\n",
      "203/223, train_loss: 0.1011, step time: 0.1015\n",
      "204/223, train_loss: 0.0995, step time: 0.1283\n",
      "205/223, train_loss: 0.1009, step time: 0.1317\n",
      "206/223, train_loss: 0.0880, step time: 0.1169\n",
      "207/223, train_loss: 0.0932, step time: 0.1127\n",
      "208/223, train_loss: 0.0944, step time: 0.1013\n",
      "209/223, train_loss: 0.0969, step time: 0.1040\n",
      "210/223, train_loss: 0.0961, step time: 0.1160\n",
      "211/223, train_loss: 0.1028, step time: 0.1005\n",
      "212/223, train_loss: 0.0955, step time: 0.1005\n",
      "213/223, train_loss: 0.0912, step time: 0.1328\n",
      "214/223, train_loss: 0.0924, step time: 0.1008\n",
      "215/223, train_loss: 0.1020, step time: 0.1282\n",
      "216/223, train_loss: 0.1051, step time: 0.1025\n",
      "217/223, train_loss: 0.1015, step time: 0.1031\n",
      "218/223, train_loss: 0.0926, step time: 0.1000\n",
      "219/223, train_loss: 0.0919, step time: 0.1002\n",
      "220/223, train_loss: 0.0956, step time: 0.1007\n",
      "221/223, train_loss: 0.1075, step time: 0.1030\n",
      "222/223, train_loss: 0.0948, step time: 0.0987\n",
      "223/223, train_loss: 0.0872, step time: 0.0997\n",
      "epoch 274 average loss: 0.0996\n",
      "time consuming of epoch 274 is: 92.0905\n",
      "----------\n",
      "epoch 275/300\n",
      "1/223, train_loss: 0.1021, step time: 0.1014\n",
      "2/223, train_loss: 0.0974, step time: 0.1126\n",
      "3/223, train_loss: 0.2959, step time: 0.1226\n",
      "4/223, train_loss: 0.1013, step time: 0.1137\n",
      "5/223, train_loss: 0.0900, step time: 0.1013\n",
      "6/223, train_loss: 0.0930, step time: 0.1007\n",
      "7/223, train_loss: 0.0972, step time: 0.1003\n",
      "8/223, train_loss: 0.0963, step time: 0.1243\n",
      "9/223, train_loss: 0.1073, step time: 0.1092\n",
      "10/223, train_loss: 0.0986, step time: 0.1217\n",
      "11/223, train_loss: 0.1091, step time: 0.1001\n",
      "12/223, train_loss: 0.1016, step time: 0.1087\n",
      "13/223, train_loss: 0.0917, step time: 0.1036\n",
      "14/223, train_loss: 0.1057, step time: 0.1079\n",
      "15/223, train_loss: 0.0978, step time: 0.1478\n",
      "16/223, train_loss: 0.1157, step time: 0.1068\n",
      "17/223, train_loss: 0.0957, step time: 0.0994\n",
      "18/223, train_loss: 0.0950, step time: 0.1071\n",
      "19/223, train_loss: 0.0902, step time: 0.1214\n",
      "20/223, train_loss: 0.0999, step time: 0.1130\n",
      "21/223, train_loss: 0.1049, step time: 0.1027\n",
      "22/223, train_loss: 0.1063, step time: 0.1109\n",
      "23/223, train_loss: 0.0940, step time: 0.1157\n",
      "24/223, train_loss: 0.1098, step time: 0.1374\n",
      "25/223, train_loss: 0.0963, step time: 0.1042\n",
      "26/223, train_loss: 0.1020, step time: 0.1002\n",
      "27/223, train_loss: 0.1050, step time: 0.1023\n",
      "28/223, train_loss: 0.0986, step time: 0.1047\n",
      "29/223, train_loss: 0.1023, step time: 0.1060\n",
      "30/223, train_loss: 0.1065, step time: 0.1105\n",
      "31/223, train_loss: 0.1127, step time: 0.1178\n",
      "32/223, train_loss: 0.0985, step time: 0.1071\n",
      "33/223, train_loss: 0.0901, step time: 0.1051\n",
      "34/223, train_loss: 0.1063, step time: 0.1002\n",
      "35/223, train_loss: 0.0960, step time: 0.1200\n",
      "36/223, train_loss: 0.0886, step time: 0.1293\n",
      "37/223, train_loss: 0.1034, step time: 0.1093\n",
      "38/223, train_loss: 0.0903, step time: 0.1130\n",
      "39/223, train_loss: 0.1016, step time: 0.1170\n",
      "40/223, train_loss: 0.0965, step time: 0.1121\n",
      "41/223, train_loss: 0.0813, step time: 0.1043\n",
      "42/223, train_loss: 0.1008, step time: 0.1146\n",
      "43/223, train_loss: 0.0973, step time: 0.1231\n",
      "44/223, train_loss: 0.0967, step time: 0.1113\n",
      "45/223, train_loss: 0.1095, step time: 0.1041\n",
      "46/223, train_loss: 0.0989, step time: 0.1003\n",
      "47/223, train_loss: 0.0931, step time: 0.1106\n",
      "48/223, train_loss: 0.0894, step time: 0.1011\n",
      "49/223, train_loss: 0.0920, step time: 0.1177\n",
      "50/223, train_loss: 0.1040, step time: 0.1143\n",
      "51/223, train_loss: 0.0898, step time: 0.1088\n",
      "52/223, train_loss: 0.0983, step time: 0.1008\n",
      "53/223, train_loss: 0.0978, step time: 0.1133\n",
      "54/223, train_loss: 0.0991, step time: 0.1182\n",
      "55/223, train_loss: 0.1024, step time: 0.1140\n",
      "56/223, train_loss: 0.1074, step time: 0.1113\n",
      "57/223, train_loss: 0.1023, step time: 0.1018\n",
      "58/223, train_loss: 0.0991, step time: 0.1013\n",
      "59/223, train_loss: 0.1066, step time: 0.1084\n",
      "60/223, train_loss: 0.0956, step time: 0.0999\n",
      "61/223, train_loss: 0.1067, step time: 0.1036\n",
      "62/223, train_loss: 0.1008, step time: 0.1017\n",
      "63/223, train_loss: 0.0971, step time: 0.1011\n",
      "64/223, train_loss: 0.0936, step time: 0.1003\n",
      "65/223, train_loss: 0.1045, step time: 0.1040\n",
      "66/223, train_loss: 0.0900, step time: 0.1097\n",
      "67/223, train_loss: 0.1014, step time: 0.1006\n",
      "68/223, train_loss: 0.0900, step time: 0.1006\n",
      "69/223, train_loss: 0.1022, step time: 0.1072\n",
      "70/223, train_loss: 0.0909, step time: 0.1004\n",
      "71/223, train_loss: 0.1093, step time: 0.1031\n",
      "72/223, train_loss: 0.1101, step time: 0.1010\n",
      "73/223, train_loss: 0.1207, step time: 0.1120\n",
      "74/223, train_loss: 0.1058, step time: 0.1006\n",
      "75/223, train_loss: 0.1012, step time: 0.1079\n",
      "76/223, train_loss: 0.0956, step time: 0.1011\n",
      "77/223, train_loss: 0.1011, step time: 0.1192\n",
      "78/223, train_loss: 0.0958, step time: 0.1011\n",
      "79/223, train_loss: 0.0990, step time: 0.1137\n",
      "80/223, train_loss: 0.0988, step time: 0.1089\n",
      "81/223, train_loss: 0.0910, step time: 0.1069\n",
      "82/223, train_loss: 0.1089, step time: 0.1053\n",
      "83/223, train_loss: 0.0941, step time: 0.1140\n",
      "84/223, train_loss: 0.0996, step time: 0.1002\n",
      "85/223, train_loss: 0.0998, step time: 0.1144\n",
      "86/223, train_loss: 0.0995, step time: 0.1227\n",
      "87/223, train_loss: 0.0910, step time: 0.1063\n",
      "88/223, train_loss: 0.0964, step time: 0.1105\n",
      "89/223, train_loss: 0.1028, step time: 0.1069\n",
      "90/223, train_loss: 0.0909, step time: 0.1005\n",
      "91/223, train_loss: 0.1101, step time: 0.1194\n",
      "92/223, train_loss: 0.0938, step time: 0.1162\n",
      "93/223, train_loss: 0.0925, step time: 0.1289\n",
      "94/223, train_loss: 0.1016, step time: 0.1606\n",
      "95/223, train_loss: 0.0911, step time: 0.1188\n",
      "96/223, train_loss: 0.1047, step time: 0.1442\n",
      "97/223, train_loss: 0.0869, step time: 0.1099\n",
      "98/223, train_loss: 0.0985, step time: 0.1297\n",
      "99/223, train_loss: 0.0874, step time: 0.1018\n",
      "100/223, train_loss: 0.0980, step time: 0.1002\n",
      "101/223, train_loss: 0.1008, step time: 0.1400\n",
      "102/223, train_loss: 0.0970, step time: 0.1327\n",
      "103/223, train_loss: 0.0885, step time: 0.1002\n",
      "104/223, train_loss: 0.0965, step time: 0.1389\n",
      "105/223, train_loss: 0.1021, step time: 0.1074\n",
      "106/223, train_loss: 0.0994, step time: 0.1329\n",
      "107/223, train_loss: 0.1030, step time: 0.1136\n",
      "108/223, train_loss: 0.1015, step time: 0.1018\n",
      "109/223, train_loss: 0.1045, step time: 0.1117\n",
      "110/223, train_loss: 0.0924, step time: 0.1006\n",
      "111/223, train_loss: 0.1037, step time: 0.1116\n",
      "112/223, train_loss: 0.0915, step time: 0.1150\n",
      "113/223, train_loss: 0.0978, step time: 0.1098\n",
      "114/223, train_loss: 0.0943, step time: 0.1006\n",
      "115/223, train_loss: 0.1023, step time: 0.1015\n",
      "116/223, train_loss: 0.1001, step time: 0.1011\n",
      "117/223, train_loss: 0.0858, step time: 0.0995\n",
      "118/223, train_loss: 0.1049, step time: 0.1162\n",
      "119/223, train_loss: 0.0985, step time: 0.1200\n",
      "120/223, train_loss: 0.0845, step time: 0.1214\n",
      "121/223, train_loss: 0.1090, step time: 0.1295\n",
      "122/223, train_loss: 0.0978, step time: 0.1010\n",
      "123/223, train_loss: 0.1013, step time: 0.1127\n",
      "124/223, train_loss: 0.1003, step time: 0.1007\n",
      "125/223, train_loss: 0.0970, step time: 0.1208\n",
      "126/223, train_loss: 0.1017, step time: 0.1147\n",
      "127/223, train_loss: 0.0987, step time: 0.1106\n",
      "128/223, train_loss: 0.1137, step time: 0.1010\n",
      "129/223, train_loss: 0.0908, step time: 0.1183\n",
      "130/223, train_loss: 0.1120, step time: 0.1032\n",
      "131/223, train_loss: 0.0954, step time: 0.1066\n",
      "132/223, train_loss: 0.1134, step time: 0.1144\n",
      "133/223, train_loss: 0.0995, step time: 0.1017\n",
      "134/223, train_loss: 0.1025, step time: 0.1070\n",
      "135/223, train_loss: 0.0967, step time: 0.1443\n",
      "136/223, train_loss: 0.1097, step time: 0.1186\n",
      "137/223, train_loss: 0.1010, step time: 0.1117\n",
      "138/223, train_loss: 0.1015, step time: 0.1248\n",
      "139/223, train_loss: 0.0921, step time: 0.1109\n",
      "140/223, train_loss: 0.1044, step time: 0.1024\n",
      "141/223, train_loss: 0.1077, step time: 0.1169\n",
      "142/223, train_loss: 0.1128, step time: 0.1172\n",
      "143/223, train_loss: 0.0912, step time: 0.1120\n",
      "144/223, train_loss: 0.0914, step time: 0.1194\n",
      "145/223, train_loss: 0.1043, step time: 0.1122\n",
      "146/223, train_loss: 0.0933, step time: 0.1227\n",
      "147/223, train_loss: 0.1064, step time: 0.1294\n",
      "148/223, train_loss: 0.0982, step time: 0.1043\n",
      "149/223, train_loss: 0.0918, step time: 0.1057\n",
      "150/223, train_loss: 0.0997, step time: 0.1038\n",
      "151/223, train_loss: 0.1082, step time: 0.1156\n",
      "152/223, train_loss: 0.0989, step time: 0.1007\n",
      "153/223, train_loss: 0.0976, step time: 0.0999\n",
      "154/223, train_loss: 0.0954, step time: 0.1008\n",
      "155/223, train_loss: 0.0981, step time: 0.1120\n",
      "156/223, train_loss: 0.0982, step time: 0.1051\n",
      "157/223, train_loss: 0.0957, step time: 0.1302\n",
      "158/223, train_loss: 0.0860, step time: 0.1021\n",
      "159/223, train_loss: 0.0999, step time: 0.1153\n",
      "160/223, train_loss: 0.1048, step time: 0.1086\n",
      "161/223, train_loss: 0.0927, step time: 0.1733\n",
      "162/223, train_loss: 0.0940, step time: 0.1093\n",
      "163/223, train_loss: 0.1132, step time: 0.0993\n",
      "164/223, train_loss: 0.1074, step time: 0.0994\n",
      "165/223, train_loss: 0.1030, step time: 0.1006\n",
      "166/223, train_loss: 0.0922, step time: 0.1008\n",
      "167/223, train_loss: 0.0976, step time: 0.1069\n",
      "168/223, train_loss: 0.0917, step time: 0.1092\n",
      "169/223, train_loss: 0.0941, step time: 0.0999\n",
      "170/223, train_loss: 0.0981, step time: 0.1073\n",
      "171/223, train_loss: 0.1027, step time: 0.1157\n",
      "172/223, train_loss: 0.0938, step time: 0.1219\n",
      "173/223, train_loss: 0.0918, step time: 0.0997\n",
      "174/223, train_loss: 0.0923, step time: 0.1118\n",
      "175/223, train_loss: 0.0940, step time: 0.1364\n",
      "176/223, train_loss: 0.0960, step time: 0.0997\n",
      "177/223, train_loss: 0.0901, step time: 0.0987\n",
      "178/223, train_loss: 0.0894, step time: 0.1127\n",
      "179/223, train_loss: 0.1146, step time: 0.0998\n",
      "180/223, train_loss: 0.1061, step time: 0.1117\n",
      "181/223, train_loss: 0.0981, step time: 0.1109\n",
      "182/223, train_loss: 0.0861, step time: 0.1104\n",
      "183/223, train_loss: 0.0948, step time: 0.1041\n",
      "184/223, train_loss: 0.1053, step time: 0.1545\n",
      "185/223, train_loss: 0.0921, step time: 0.1053\n",
      "186/223, train_loss: 0.0981, step time: 0.1140\n",
      "187/223, train_loss: 0.0986, step time: 0.1295\n",
      "188/223, train_loss: 0.0927, step time: 0.1442\n",
      "189/223, train_loss: 0.0990, step time: 0.0997\n",
      "190/223, train_loss: 0.0967, step time: 0.1061\n",
      "191/223, train_loss: 0.1110, step time: 0.1277\n",
      "192/223, train_loss: 0.0910, step time: 0.1103\n",
      "193/223, train_loss: 0.1070, step time: 0.1563\n",
      "194/223, train_loss: 0.1058, step time: 0.1059\n",
      "195/223, train_loss: 0.1075, step time: 0.1167\n",
      "196/223, train_loss: 0.0990, step time: 0.1385\n",
      "197/223, train_loss: 0.0878, step time: 0.1232\n",
      "198/223, train_loss: 0.0870, step time: 0.1355\n",
      "199/223, train_loss: 0.1037, step time: 0.1271\n",
      "200/223, train_loss: 0.0908, step time: 0.1347\n",
      "201/223, train_loss: 0.0933, step time: 0.1013\n",
      "202/223, train_loss: 0.0977, step time: 0.1049\n",
      "203/223, train_loss: 0.0916, step time: 0.1262\n",
      "204/223, train_loss: 0.1037, step time: 0.1131\n",
      "205/223, train_loss: 0.0879, step time: 0.1276\n",
      "206/223, train_loss: 0.0938, step time: 0.0996\n",
      "207/223, train_loss: 0.0945, step time: 0.1104\n",
      "208/223, train_loss: 0.0917, step time: 0.1188\n",
      "209/223, train_loss: 0.1001, step time: 0.1228\n",
      "210/223, train_loss: 0.0846, step time: 0.1062\n",
      "211/223, train_loss: 0.0966, step time: 0.1076\n",
      "212/223, train_loss: 0.0996, step time: 0.1064\n",
      "213/223, train_loss: 0.1017, step time: 0.1154\n",
      "214/223, train_loss: 0.0946, step time: 0.1021\n",
      "215/223, train_loss: 0.0986, step time: 0.1299\n",
      "216/223, train_loss: 0.0906, step time: 0.1170\n",
      "217/223, train_loss: 0.0918, step time: 0.1263\n",
      "218/223, train_loss: 0.1013, step time: 0.1046\n",
      "219/223, train_loss: 0.1009, step time: 0.1048\n",
      "220/223, train_loss: 0.0961, step time: 0.1006\n",
      "221/223, train_loss: 0.0942, step time: 0.1004\n",
      "222/223, train_loss: 0.0942, step time: 0.1043\n",
      "223/223, train_loss: 0.1025, step time: 0.0997\n",
      "epoch 275 average loss: 0.0995\n",
      "current epoch: 275 current mean dice: 0.8621 tc: 0.9226 wt: 0.8717 et: 0.7921\n",
      "best mean dice: 0.8622 at epoch: 260\n",
      "time consuming of epoch 275 is: 91.3536\n",
      "----------\n",
      "epoch 276/300\n",
      "1/223, train_loss: 0.0880, step time: 0.1006\n",
      "2/223, train_loss: 0.1047, step time: 0.0997\n",
      "3/223, train_loss: 0.0887, step time: 0.1002\n",
      "4/223, train_loss: 0.1002, step time: 0.1010\n",
      "5/223, train_loss: 0.0964, step time: 0.1178\n",
      "6/223, train_loss: 0.1055, step time: 0.1073\n",
      "7/223, train_loss: 0.1017, step time: 0.1087\n",
      "8/223, train_loss: 0.1065, step time: 0.1008\n",
      "9/223, train_loss: 0.1049, step time: 0.1398\n",
      "10/223, train_loss: 0.0939, step time: 0.1024\n",
      "11/223, train_loss: 0.0967, step time: 0.1113\n",
      "12/223, train_loss: 0.0989, step time: 0.1010\n",
      "13/223, train_loss: 0.1073, step time: 0.1396\n",
      "14/223, train_loss: 0.0889, step time: 0.1090\n",
      "15/223, train_loss: 0.0983, step time: 0.1163\n",
      "16/223, train_loss: 0.0917, step time: 0.1003\n",
      "17/223, train_loss: 0.0979, step time: 0.1321\n",
      "18/223, train_loss: 0.0971, step time: 0.1009\n",
      "19/223, train_loss: 0.0866, step time: 0.1055\n",
      "20/223, train_loss: 0.1016, step time: 0.1093\n",
      "21/223, train_loss: 0.0906, step time: 0.1084\n",
      "22/223, train_loss: 0.1042, step time: 0.1148\n",
      "23/223, train_loss: 0.0977, step time: 0.1148\n",
      "24/223, train_loss: 0.0971, step time: 0.1002\n",
      "25/223, train_loss: 0.0990, step time: 0.1075\n",
      "26/223, train_loss: 0.0997, step time: 0.1155\n",
      "27/223, train_loss: 0.0942, step time: 0.0993\n",
      "28/223, train_loss: 0.0949, step time: 0.1077\n",
      "29/223, train_loss: 0.0873, step time: 0.0995\n",
      "30/223, train_loss: 0.1129, step time: 0.1174\n",
      "31/223, train_loss: 0.1085, step time: 0.1306\n",
      "32/223, train_loss: 0.0971, step time: 0.1147\n",
      "33/223, train_loss: 0.0982, step time: 0.1206\n",
      "34/223, train_loss: 0.0917, step time: 0.1020\n",
      "35/223, train_loss: 0.0886, step time: 0.1003\n",
      "36/223, train_loss: 0.0966, step time: 0.1089\n",
      "37/223, train_loss: 0.0906, step time: 0.1167\n",
      "38/223, train_loss: 0.0915, step time: 0.1062\n",
      "39/223, train_loss: 0.0999, step time: 0.1005\n",
      "40/223, train_loss: 0.0921, step time: 0.1167\n",
      "41/223, train_loss: 0.1061, step time: 0.1040\n",
      "42/223, train_loss: 0.0994, step time: 0.1007\n",
      "43/223, train_loss: 0.0958, step time: 0.1010\n",
      "44/223, train_loss: 0.1024, step time: 0.1076\n",
      "45/223, train_loss: 0.1087, step time: 0.1007\n",
      "46/223, train_loss: 0.1003, step time: 0.1004\n",
      "47/223, train_loss: 0.1030, step time: 0.1318\n",
      "48/223, train_loss: 0.0900, step time: 0.1143\n",
      "49/223, train_loss: 0.1004, step time: 0.1324\n",
      "50/223, train_loss: 0.0978, step time: 0.1110\n",
      "51/223, train_loss: 0.1046, step time: 0.1021\n",
      "52/223, train_loss: 0.0989, step time: 0.1041\n",
      "53/223, train_loss: 0.0994, step time: 0.1267\n",
      "54/223, train_loss: 0.0927, step time: 0.0997\n",
      "55/223, train_loss: 0.1029, step time: 0.1070\n",
      "56/223, train_loss: 0.1010, step time: 0.1129\n",
      "57/223, train_loss: 0.0895, step time: 0.1096\n",
      "58/223, train_loss: 0.0916, step time: 0.1008\n",
      "59/223, train_loss: 0.1003, step time: 0.1007\n",
      "60/223, train_loss: 0.0973, step time: 0.1134\n",
      "61/223, train_loss: 0.0894, step time: 0.1004\n",
      "62/223, train_loss: 0.0941, step time: 0.1005\n",
      "63/223, train_loss: 0.0966, step time: 0.0999\n",
      "64/223, train_loss: 0.0858, step time: 0.1120\n",
      "65/223, train_loss: 0.0944, step time: 0.1039\n",
      "66/223, train_loss: 0.1007, step time: 0.1007\n",
      "67/223, train_loss: 0.1091, step time: 0.1002\n",
      "68/223, train_loss: 0.0904, step time: 0.1139\n",
      "69/223, train_loss: 0.0962, step time: 0.1007\n",
      "70/223, train_loss: 0.1021, step time: 0.1012\n",
      "71/223, train_loss: 0.1108, step time: 0.1010\n",
      "72/223, train_loss: 0.0918, step time: 0.1103\n",
      "73/223, train_loss: 0.0975, step time: 0.1063\n",
      "74/223, train_loss: 0.0936, step time: 0.1116\n",
      "75/223, train_loss: 0.0928, step time: 0.0996\n",
      "76/223, train_loss: 0.0978, step time: 0.1058\n",
      "77/223, train_loss: 0.1108, step time: 0.1054\n",
      "78/223, train_loss: 0.0897, step time: 0.1003\n",
      "79/223, train_loss: 0.0886, step time: 0.1108\n",
      "80/223, train_loss: 0.0985, step time: 0.1038\n",
      "81/223, train_loss: 0.1146, step time: 0.1044\n",
      "82/223, train_loss: 0.1027, step time: 0.1032\n",
      "83/223, train_loss: 0.0987, step time: 0.1249\n",
      "84/223, train_loss: 0.0984, step time: 0.1031\n",
      "85/223, train_loss: 0.0900, step time: 0.1090\n",
      "86/223, train_loss: 0.0926, step time: 0.1009\n",
      "87/223, train_loss: 0.0989, step time: 0.1007\n",
      "88/223, train_loss: 0.0959, step time: 0.1058\n",
      "89/223, train_loss: 0.0884, step time: 0.1074\n",
      "90/223, train_loss: 0.0999, step time: 0.1011\n",
      "91/223, train_loss: 0.1042, step time: 0.1007\n",
      "92/223, train_loss: 0.1060, step time: 0.1052\n",
      "93/223, train_loss: 0.1073, step time: 0.1136\n",
      "94/223, train_loss: 0.1001, step time: 0.1128\n",
      "95/223, train_loss: 0.1006, step time: 0.1308\n",
      "96/223, train_loss: 0.1038, step time: 0.1006\n",
      "97/223, train_loss: 0.1040, step time: 0.1106\n",
      "98/223, train_loss: 0.0976, step time: 0.1106\n",
      "99/223, train_loss: 0.1109, step time: 0.1169\n",
      "100/223, train_loss: 0.1029, step time: 0.1078\n",
      "101/223, train_loss: 0.0983, step time: 0.1150\n",
      "102/223, train_loss: 0.0948, step time: 0.1010\n",
      "103/223, train_loss: 0.0975, step time: 0.1324\n",
      "104/223, train_loss: 0.1105, step time: 0.1133\n",
      "105/223, train_loss: 0.0938, step time: 0.1183\n",
      "106/223, train_loss: 0.0925, step time: 0.1118\n",
      "107/223, train_loss: 0.1021, step time: 0.1349\n",
      "108/223, train_loss: 0.0911, step time: 0.1068\n",
      "109/223, train_loss: 0.1083, step time: 0.0993\n",
      "110/223, train_loss: 0.1090, step time: 0.0998\n",
      "111/223, train_loss: 0.0955, step time: 0.1164\n",
      "112/223, train_loss: 0.1052, step time: 0.1011\n",
      "113/223, train_loss: 0.1065, step time: 0.1039\n",
      "114/223, train_loss: 0.0897, step time: 0.1068\n",
      "115/223, train_loss: 0.1018, step time: 0.1505\n",
      "116/223, train_loss: 0.1083, step time: 0.1106\n",
      "117/223, train_loss: 0.0970, step time: 0.1052\n",
      "118/223, train_loss: 0.1053, step time: 0.1200\n",
      "119/223, train_loss: 0.0943, step time: 0.1162\n",
      "120/223, train_loss: 0.0894, step time: 0.1022\n",
      "121/223, train_loss: 0.0983, step time: 0.1131\n",
      "122/223, train_loss: 0.0970, step time: 0.1009\n",
      "123/223, train_loss: 0.1012, step time: 0.1005\n",
      "124/223, train_loss: 0.1031, step time: 0.1174\n",
      "125/223, train_loss: 0.0966, step time: 0.1188\n",
      "126/223, train_loss: 0.2999, step time: 0.1139\n",
      "127/223, train_loss: 0.1000, step time: 0.1627\n",
      "128/223, train_loss: 0.0927, step time: 0.0998\n",
      "129/223, train_loss: 0.1081, step time: 0.1011\n",
      "130/223, train_loss: 0.0950, step time: 0.1034\n",
      "131/223, train_loss: 0.1093, step time: 0.1039\n",
      "132/223, train_loss: 0.0927, step time: 0.1013\n",
      "133/223, train_loss: 0.1005, step time: 0.1153\n",
      "134/223, train_loss: 0.1034, step time: 0.1597\n",
      "135/223, train_loss: 0.0917, step time: 0.1005\n",
      "136/223, train_loss: 0.0953, step time: 0.1007\n",
      "137/223, train_loss: 0.0932, step time: 0.1115\n",
      "138/223, train_loss: 0.0990, step time: 0.1009\n",
      "139/223, train_loss: 0.0863, step time: 0.1003\n",
      "140/223, train_loss: 0.1067, step time: 0.1135\n",
      "141/223, train_loss: 0.0932, step time: 0.1115\n",
      "142/223, train_loss: 0.1067, step time: 0.1007\n",
      "143/223, train_loss: 0.0969, step time: 0.1225\n",
      "144/223, train_loss: 0.0946, step time: 0.1079\n",
      "145/223, train_loss: 0.0964, step time: 0.1003\n",
      "146/223, train_loss: 0.0900, step time: 0.1030\n",
      "147/223, train_loss: 0.1103, step time: 0.1000\n",
      "148/223, train_loss: 0.1049, step time: 0.1163\n",
      "149/223, train_loss: 0.0962, step time: 0.1102\n",
      "150/223, train_loss: 0.1010, step time: 0.1078\n",
      "151/223, train_loss: 0.0908, step time: 0.1179\n",
      "152/223, train_loss: 0.1073, step time: 0.1138\n",
      "153/223, train_loss: 0.1020, step time: 0.1125\n",
      "154/223, train_loss: 0.1009, step time: 0.1082\n",
      "155/223, train_loss: 0.0995, step time: 0.1205\n",
      "156/223, train_loss: 0.1061, step time: 0.1293\n",
      "157/223, train_loss: 0.0905, step time: 0.1162\n",
      "158/223, train_loss: 0.1047, step time: 0.1017\n",
      "159/223, train_loss: 0.1086, step time: 0.1044\n",
      "160/223, train_loss: 0.1016, step time: 0.1039\n",
      "161/223, train_loss: 0.1043, step time: 0.1025\n",
      "162/223, train_loss: 0.1070, step time: 0.1001\n",
      "163/223, train_loss: 0.0928, step time: 0.1007\n",
      "164/223, train_loss: 0.0930, step time: 0.0998\n",
      "165/223, train_loss: 0.1138, step time: 0.1346\n",
      "166/223, train_loss: 0.0910, step time: 0.1165\n",
      "167/223, train_loss: 0.1013, step time: 0.1001\n",
      "168/223, train_loss: 0.0867, step time: 0.1093\n",
      "169/223, train_loss: 0.0897, step time: 0.1112\n",
      "170/223, train_loss: 0.0976, step time: 0.1186\n",
      "171/223, train_loss: 0.1125, step time: 0.1174\n",
      "172/223, train_loss: 0.0960, step time: 0.1010\n",
      "173/223, train_loss: 0.0957, step time: 0.1152\n",
      "174/223, train_loss: 0.1043, step time: 0.1135\n",
      "175/223, train_loss: 0.1025, step time: 0.1035\n",
      "176/223, train_loss: 0.0908, step time: 0.1157\n",
      "177/223, train_loss: 0.0947, step time: 0.1076\n",
      "178/223, train_loss: 0.1010, step time: 0.1358\n",
      "179/223, train_loss: 0.1093, step time: 0.1156\n",
      "180/223, train_loss: 0.0925, step time: 0.1028\n",
      "181/223, train_loss: 0.1013, step time: 0.1057\n",
      "182/223, train_loss: 0.1026, step time: 0.1009\n",
      "183/223, train_loss: 0.0999, step time: 0.1004\n",
      "184/223, train_loss: 0.0957, step time: 0.1216\n",
      "185/223, train_loss: 0.1020, step time: 0.1174\n",
      "186/223, train_loss: 0.1035, step time: 0.1042\n",
      "187/223, train_loss: 0.0973, step time: 0.0993\n",
      "188/223, train_loss: 0.1057, step time: 0.1304\n",
      "189/223, train_loss: 0.0984, step time: 0.1156\n",
      "190/223, train_loss: 0.1037, step time: 0.1003\n",
      "191/223, train_loss: 0.0856, step time: 0.1000\n",
      "192/223, train_loss: 0.1003, step time: 0.1009\n",
      "193/223, train_loss: 0.0951, step time: 0.1172\n",
      "194/223, train_loss: 0.1094, step time: 0.1071\n",
      "195/223, train_loss: 0.1023, step time: 0.1216\n",
      "196/223, train_loss: 0.1069, step time: 0.1175\n",
      "197/223, train_loss: 0.1099, step time: 0.1123\n",
      "198/223, train_loss: 0.0889, step time: 0.1208\n",
      "199/223, train_loss: 0.0915, step time: 0.1126\n",
      "200/223, train_loss: 0.1058, step time: 0.1017\n",
      "201/223, train_loss: 0.0902, step time: 0.1055\n",
      "202/223, train_loss: 0.1045, step time: 0.1199\n",
      "203/223, train_loss: 0.0966, step time: 0.1283\n",
      "204/223, train_loss: 0.0938, step time: 0.0998\n",
      "205/223, train_loss: 0.1008, step time: 0.0996\n",
      "206/223, train_loss: 0.0862, step time: 0.1132\n",
      "207/223, train_loss: 0.0947, step time: 0.0989\n",
      "208/223, train_loss: 0.0903, step time: 0.0992\n",
      "209/223, train_loss: 0.1114, step time: 0.1052\n",
      "210/223, train_loss: 0.0999, step time: 0.1157\n",
      "211/223, train_loss: 0.0993, step time: 0.1083\n",
      "212/223, train_loss: 0.1045, step time: 0.1294\n",
      "213/223, train_loss: 0.0870, step time: 0.1138\n",
      "214/223, train_loss: 0.1013, step time: 0.1137\n",
      "215/223, train_loss: 0.0980, step time: 0.1097\n",
      "216/223, train_loss: 0.0894, step time: 0.1049\n",
      "217/223, train_loss: 0.0927, step time: 0.0996\n",
      "218/223, train_loss: 0.0997, step time: 0.1017\n",
      "219/223, train_loss: 0.1016, step time: 0.1002\n",
      "220/223, train_loss: 0.0941, step time: 0.1008\n",
      "221/223, train_loss: 0.0914, step time: 0.0999\n",
      "222/223, train_loss: 0.1139, step time: 0.0994\n",
      "223/223, train_loss: 0.1010, step time: 0.0999\n",
      "epoch 276 average loss: 0.0996\n",
      "time consuming of epoch 276 is: 88.7123\n",
      "----------\n",
      "epoch 277/300\n",
      "1/223, train_loss: 0.0954, step time: 0.1019\n",
      "2/223, train_loss: 0.1009, step time: 0.1177\n",
      "3/223, train_loss: 0.1099, step time: 0.1097\n",
      "4/223, train_loss: 0.0998, step time: 0.1156\n",
      "5/223, train_loss: 0.1089, step time: 0.1146\n",
      "6/223, train_loss: 0.0976, step time: 0.1029\n",
      "7/223, train_loss: 0.0907, step time: 0.1242\n",
      "8/223, train_loss: 0.0962, step time: 0.1145\n",
      "9/223, train_loss: 0.1035, step time: 0.1080\n",
      "10/223, train_loss: 0.1032, step time: 0.1118\n",
      "11/223, train_loss: 0.1063, step time: 0.1240\n",
      "12/223, train_loss: 0.1014, step time: 0.1129\n",
      "13/223, train_loss: 0.1030, step time: 0.1041\n",
      "14/223, train_loss: 0.1044, step time: 0.1002\n",
      "15/223, train_loss: 0.0942, step time: 0.1069\n",
      "16/223, train_loss: 0.1071, step time: 0.1130\n",
      "17/223, train_loss: 0.0990, step time: 0.1164\n",
      "18/223, train_loss: 0.0953, step time: 0.1169\n",
      "19/223, train_loss: 0.0953, step time: 0.1003\n",
      "20/223, train_loss: 0.0853, step time: 0.1009\n",
      "21/223, train_loss: 0.0981, step time: 0.1237\n",
      "22/223, train_loss: 0.1007, step time: 0.1062\n",
      "23/223, train_loss: 0.1055, step time: 0.1186\n",
      "24/223, train_loss: 0.0954, step time: 0.1224\n",
      "25/223, train_loss: 0.1069, step time: 0.1094\n",
      "26/223, train_loss: 0.1030, step time: 0.1004\n",
      "27/223, train_loss: 0.0924, step time: 0.1481\n",
      "28/223, train_loss: 0.1095, step time: 0.1028\n",
      "29/223, train_loss: 0.0971, step time: 0.1162\n",
      "30/223, train_loss: 0.0991, step time: 0.1026\n",
      "31/223, train_loss: 0.1106, step time: 0.1146\n",
      "32/223, train_loss: 0.1003, step time: 0.1114\n",
      "33/223, train_loss: 0.0953, step time: 0.1013\n",
      "34/223, train_loss: 0.0957, step time: 0.1004\n",
      "35/223, train_loss: 0.1043, step time: 0.0994\n",
      "36/223, train_loss: 0.1103, step time: 0.1292\n",
      "37/223, train_loss: 0.0977, step time: 0.1127\n",
      "38/223, train_loss: 0.1073, step time: 0.1048\n",
      "39/223, train_loss: 0.0903, step time: 0.1075\n",
      "40/223, train_loss: 0.1073, step time: 0.1177\n",
      "41/223, train_loss: 0.0968, step time: 0.1005\n",
      "42/223, train_loss: 0.0911, step time: 0.1015\n",
      "43/223, train_loss: 0.1046, step time: 0.1338\n",
      "44/223, train_loss: 0.0961, step time: 0.1181\n",
      "45/223, train_loss: 0.1101, step time: 0.1190\n",
      "46/223, train_loss: 0.1002, step time: 0.1001\n",
      "47/223, train_loss: 0.0909, step time: 0.1178\n",
      "48/223, train_loss: 0.0903, step time: 0.1316\n",
      "49/223, train_loss: 0.0901, step time: 0.1043\n",
      "50/223, train_loss: 0.1002, step time: 0.1087\n",
      "51/223, train_loss: 0.1005, step time: 0.1261\n",
      "52/223, train_loss: 0.0908, step time: 0.1247\n",
      "53/223, train_loss: 0.1065, step time: 0.1087\n",
      "54/223, train_loss: 0.0942, step time: 0.1409\n",
      "55/223, train_loss: 0.0873, step time: 0.1049\n",
      "56/223, train_loss: 0.0923, step time: 0.1149\n",
      "57/223, train_loss: 0.1041, step time: 0.1049\n",
      "58/223, train_loss: 0.1043, step time: 0.1227\n",
      "59/223, train_loss: 0.0904, step time: 0.1102\n",
      "60/223, train_loss: 0.1126, step time: 0.1153\n",
      "61/223, train_loss: 0.0996, step time: 0.1088\n",
      "62/223, train_loss: 0.0964, step time: 0.1099\n",
      "63/223, train_loss: 0.1023, step time: 0.1315\n",
      "64/223, train_loss: 0.0992, step time: 0.1157\n",
      "65/223, train_loss: 0.0892, step time: 0.1033\n",
      "66/223, train_loss: 0.0952, step time: 0.1003\n",
      "67/223, train_loss: 0.0960, step time: 0.0996\n",
      "68/223, train_loss: 0.0966, step time: 0.1595\n",
      "69/223, train_loss: 0.1131, step time: 0.1124\n",
      "70/223, train_loss: 0.1079, step time: 0.0997\n",
      "71/223, train_loss: 0.0977, step time: 0.0996\n",
      "72/223, train_loss: 0.0878, step time: 0.1203\n",
      "73/223, train_loss: 0.1002, step time: 0.1085\n",
      "74/223, train_loss: 0.1068, step time: 0.1152\n",
      "75/223, train_loss: 0.0930, step time: 0.1033\n",
      "76/223, train_loss: 0.0901, step time: 0.0989\n",
      "77/223, train_loss: 0.1091, step time: 0.1059\n",
      "78/223, train_loss: 0.1047, step time: 0.1152\n",
      "79/223, train_loss: 0.0901, step time: 0.1058\n",
      "80/223, train_loss: 0.1050, step time: 0.1542\n",
      "81/223, train_loss: 0.1084, step time: 0.1407\n",
      "82/223, train_loss: 0.0953, step time: 0.1260\n",
      "83/223, train_loss: 0.0911, step time: 0.1014\n",
      "84/223, train_loss: 0.0968, step time: 0.1086\n",
      "85/223, train_loss: 0.1015, step time: 0.1003\n",
      "86/223, train_loss: 0.0868, step time: 0.1110\n",
      "87/223, train_loss: 0.0895, step time: 0.1007\n",
      "88/223, train_loss: 0.0917, step time: 0.1004\n",
      "89/223, train_loss: 0.1035, step time: 0.1015\n",
      "90/223, train_loss: 0.0883, step time: 0.1005\n",
      "91/223, train_loss: 0.1072, step time: 0.1009\n",
      "92/223, train_loss: 0.0941, step time: 0.1190\n",
      "93/223, train_loss: 0.0923, step time: 0.1132\n",
      "94/223, train_loss: 0.2965, step time: 0.1531\n",
      "95/223, train_loss: 0.0975, step time: 0.1023\n",
      "96/223, train_loss: 0.1078, step time: 0.1028\n",
      "97/223, train_loss: 0.0962, step time: 0.1316\n",
      "98/223, train_loss: 0.1079, step time: 0.1311\n",
      "99/223, train_loss: 0.0903, step time: 0.1259\n",
      "100/223, train_loss: 0.0901, step time: 0.1058\n",
      "101/223, train_loss: 0.0870, step time: 0.1232\n",
      "102/223, train_loss: 0.0955, step time: 0.1050\n",
      "103/223, train_loss: 0.1009, step time: 0.1012\n",
      "104/223, train_loss: 0.0902, step time: 0.1150\n",
      "105/223, train_loss: 0.1049, step time: 0.1156\n",
      "106/223, train_loss: 0.1028, step time: 0.1304\n",
      "107/223, train_loss: 0.1012, step time: 0.1373\n",
      "108/223, train_loss: 0.0994, step time: 0.1373\n",
      "109/223, train_loss: 0.1116, step time: 0.1113\n",
      "110/223, train_loss: 0.0955, step time: 0.1250\n",
      "111/223, train_loss: 0.0955, step time: 0.1059\n",
      "112/223, train_loss: 0.1102, step time: 0.1122\n",
      "113/223, train_loss: 0.0949, step time: 0.1429\n",
      "114/223, train_loss: 0.1039, step time: 0.1241\n",
      "115/223, train_loss: 0.0937, step time: 0.1017\n",
      "116/223, train_loss: 0.0965, step time: 0.1037\n",
      "117/223, train_loss: 0.0924, step time: 0.1079\n",
      "118/223, train_loss: 0.0961, step time: 0.1008\n",
      "119/223, train_loss: 0.0859, step time: 0.1019\n",
      "120/223, train_loss: 0.1018, step time: 0.1036\n",
      "121/223, train_loss: 0.0965, step time: 0.1058\n",
      "122/223, train_loss: 0.0968, step time: 0.1011\n",
      "123/223, train_loss: 0.0958, step time: 0.1162\n",
      "124/223, train_loss: 0.0929, step time: 0.1071\n",
      "125/223, train_loss: 0.0960, step time: 0.1035\n",
      "126/223, train_loss: 0.1028, step time: 0.1020\n",
      "127/223, train_loss: 0.0907, step time: 0.1208\n",
      "128/223, train_loss: 0.0931, step time: 0.1187\n",
      "129/223, train_loss: 0.0992, step time: 0.1009\n",
      "130/223, train_loss: 0.0925, step time: 0.0995\n",
      "131/223, train_loss: 0.1091, step time: 0.1008\n",
      "132/223, train_loss: 0.1058, step time: 0.1017\n",
      "133/223, train_loss: 0.1077, step time: 0.1108\n",
      "134/223, train_loss: 0.1024, step time: 0.1017\n",
      "135/223, train_loss: 0.0974, step time: 0.1020\n",
      "136/223, train_loss: 0.0935, step time: 0.1285\n",
      "137/223, train_loss: 0.0984, step time: 0.1190\n",
      "138/223, train_loss: 0.0941, step time: 0.1163\n",
      "139/223, train_loss: 0.1039, step time: 0.1002\n",
      "140/223, train_loss: 0.0954, step time: 0.1003\n",
      "141/223, train_loss: 0.0986, step time: 0.1137\n",
      "142/223, train_loss: 0.0932, step time: 0.1012\n",
      "143/223, train_loss: 0.1028, step time: 0.1023\n",
      "144/223, train_loss: 0.0865, step time: 0.1125\n",
      "145/223, train_loss: 0.1057, step time: 0.1154\n",
      "146/223, train_loss: 0.0958, step time: 0.1453\n",
      "147/223, train_loss: 0.0988, step time: 0.1000\n",
      "148/223, train_loss: 0.0950, step time: 0.1131\n",
      "149/223, train_loss: 0.0883, step time: 0.1104\n",
      "150/223, train_loss: 0.1070, step time: 0.1231\n",
      "151/223, train_loss: 0.1110, step time: 0.1337\n",
      "152/223, train_loss: 0.1020, step time: 0.1238\n",
      "153/223, train_loss: 0.1070, step time: 0.1209\n",
      "154/223, train_loss: 0.1054, step time: 0.1231\n",
      "155/223, train_loss: 0.1054, step time: 0.1237\n",
      "156/223, train_loss: 0.1000, step time: 0.1092\n",
      "157/223, train_loss: 0.0994, step time: 0.1107\n",
      "158/223, train_loss: 0.1047, step time: 0.1091\n",
      "159/223, train_loss: 0.0924, step time: 0.1130\n",
      "160/223, train_loss: 0.0932, step time: 0.1115\n",
      "161/223, train_loss: 0.1016, step time: 0.1209\n",
      "162/223, train_loss: 0.0939, step time: 0.1147\n",
      "163/223, train_loss: 0.0909, step time: 0.1016\n",
      "164/223, train_loss: 0.0942, step time: 0.1132\n",
      "165/223, train_loss: 0.0830, step time: 0.1671\n",
      "166/223, train_loss: 0.1070, step time: 0.1032\n",
      "167/223, train_loss: 0.0923, step time: 0.1091\n",
      "168/223, train_loss: 0.0901, step time: 0.1094\n",
      "169/223, train_loss: 0.0973, step time: 0.1109\n",
      "170/223, train_loss: 0.0869, step time: 0.1115\n",
      "171/223, train_loss: 0.1107, step time: 0.1170\n",
      "172/223, train_loss: 0.0940, step time: 0.1004\n",
      "173/223, train_loss: 0.1064, step time: 0.1123\n",
      "174/223, train_loss: 0.1029, step time: 0.1097\n",
      "175/223, train_loss: 0.1024, step time: 0.1007\n",
      "176/223, train_loss: 0.1027, step time: 0.1118\n",
      "177/223, train_loss: 0.0997, step time: 0.1131\n",
      "178/223, train_loss: 0.1057, step time: 0.1167\n",
      "179/223, train_loss: 0.0935, step time: 0.1097\n",
      "180/223, train_loss: 0.0934, step time: 0.1005\n",
      "181/223, train_loss: 0.1022, step time: 0.1186\n",
      "182/223, train_loss: 0.1021, step time: 0.1007\n",
      "183/223, train_loss: 0.1060, step time: 0.1011\n",
      "184/223, train_loss: 0.1098, step time: 0.1016\n",
      "185/223, train_loss: 0.0908, step time: 0.1182\n",
      "186/223, train_loss: 0.1031, step time: 0.1073\n",
      "187/223, train_loss: 0.0892, step time: 0.1070\n",
      "188/223, train_loss: 0.0920, step time: 0.1017\n",
      "189/223, train_loss: 0.1118, step time: 0.1173\n",
      "190/223, train_loss: 0.0995, step time: 0.1095\n",
      "191/223, train_loss: 0.1014, step time: 0.1211\n",
      "192/223, train_loss: 0.0949, step time: 0.1078\n",
      "193/223, train_loss: 0.0882, step time: 0.1044\n",
      "194/223, train_loss: 0.0893, step time: 0.1098\n",
      "195/223, train_loss: 0.1058, step time: 0.1006\n",
      "196/223, train_loss: 0.0944, step time: 0.1112\n",
      "197/223, train_loss: 0.0917, step time: 0.1212\n",
      "198/223, train_loss: 0.0911, step time: 0.1424\n",
      "199/223, train_loss: 0.0852, step time: 0.1382\n",
      "200/223, train_loss: 0.0934, step time: 0.1185\n",
      "201/223, train_loss: 0.0979, step time: 0.1463\n",
      "202/223, train_loss: 0.0981, step time: 0.1072\n",
      "203/223, train_loss: 0.0977, step time: 0.1003\n",
      "204/223, train_loss: 0.0955, step time: 0.1078\n",
      "205/223, train_loss: 0.1065, step time: 0.1192\n",
      "206/223, train_loss: 0.0945, step time: 0.1028\n",
      "207/223, train_loss: 0.0999, step time: 0.1029\n",
      "208/223, train_loss: 0.1160, step time: 0.1138\n",
      "209/223, train_loss: 0.1095, step time: 0.1126\n",
      "210/223, train_loss: 0.0896, step time: 0.1102\n",
      "211/223, train_loss: 0.1103, step time: 0.1008\n",
      "212/223, train_loss: 0.0974, step time: 0.1010\n",
      "213/223, train_loss: 0.0972, step time: 0.1040\n",
      "214/223, train_loss: 0.1036, step time: 0.1002\n",
      "215/223, train_loss: 0.1014, step time: 0.1072\n",
      "216/223, train_loss: 0.0952, step time: 0.1272\n",
      "217/223, train_loss: 0.0976, step time: 0.1019\n",
      "218/223, train_loss: 0.0952, step time: 0.1020\n",
      "219/223, train_loss: 0.0914, step time: 0.0997\n",
      "220/223, train_loss: 0.1018, step time: 0.1021\n",
      "221/223, train_loss: 0.1073, step time: 0.0991\n",
      "222/223, train_loss: 0.0918, step time: 0.0993\n",
      "223/223, train_loss: 0.1046, step time: 0.0989\n",
      "epoch 277 average loss: 0.0995\n",
      "time consuming of epoch 277 is: 89.6360\n",
      "----------\n",
      "epoch 278/300\n",
      "1/223, train_loss: 0.0947, step time: 0.1012\n",
      "2/223, train_loss: 0.0924, step time: 0.1010\n",
      "3/223, train_loss: 0.0989, step time: 0.1001\n",
      "4/223, train_loss: 0.0973, step time: 0.1012\n",
      "5/223, train_loss: 0.1022, step time: 0.1093\n",
      "6/223, train_loss: 0.0913, step time: 0.1194\n",
      "7/223, train_loss: 0.0917, step time: 0.1220\n",
      "8/223, train_loss: 0.1053, step time: 0.1306\n",
      "9/223, train_loss: 0.0959, step time: 0.1176\n",
      "10/223, train_loss: 0.1069, step time: 0.1079\n",
      "11/223, train_loss: 0.1029, step time: 0.1529\n",
      "12/223, train_loss: 0.0991, step time: 0.1242\n",
      "13/223, train_loss: 0.0928, step time: 0.1064\n",
      "14/223, train_loss: 0.0876, step time: 0.1189\n",
      "15/223, train_loss: 0.0982, step time: 0.1288\n",
      "16/223, train_loss: 0.0916, step time: 0.1092\n",
      "17/223, train_loss: 0.0966, step time: 0.1197\n",
      "18/223, train_loss: 0.0964, step time: 0.1195\n",
      "19/223, train_loss: 0.1026, step time: 0.1345\n",
      "20/223, train_loss: 0.1097, step time: 0.1083\n",
      "21/223, train_loss: 0.1013, step time: 0.1128\n",
      "22/223, train_loss: 0.0859, step time: 0.1280\n",
      "23/223, train_loss: 0.0974, step time: 0.1199\n",
      "24/223, train_loss: 0.0943, step time: 0.1108\n",
      "25/223, train_loss: 0.1029, step time: 0.1200\n",
      "26/223, train_loss: 0.0987, step time: 0.1331\n",
      "27/223, train_loss: 0.0886, step time: 0.1497\n",
      "28/223, train_loss: 0.1012, step time: 0.1005\n",
      "29/223, train_loss: 0.1073, step time: 0.1131\n",
      "30/223, train_loss: 0.0948, step time: 0.1166\n",
      "31/223, train_loss: 0.0936, step time: 0.1222\n",
      "32/223, train_loss: 0.1036, step time: 0.1075\n",
      "33/223, train_loss: 0.0978, step time: 0.1006\n",
      "34/223, train_loss: 0.0874, step time: 0.1003\n",
      "35/223, train_loss: 0.0920, step time: 0.1091\n",
      "36/223, train_loss: 0.0905, step time: 0.1094\n",
      "37/223, train_loss: 0.0970, step time: 0.1021\n",
      "38/223, train_loss: 0.0917, step time: 0.1024\n",
      "39/223, train_loss: 0.0911, step time: 0.1006\n",
      "40/223, train_loss: 0.1069, step time: 0.1029\n",
      "41/223, train_loss: 0.1026, step time: 0.1055\n",
      "42/223, train_loss: 0.0965, step time: 0.1001\n",
      "43/223, train_loss: 0.1004, step time: 0.1015\n",
      "44/223, train_loss: 0.0989, step time: 0.1094\n",
      "45/223, train_loss: 0.1077, step time: 0.1190\n",
      "46/223, train_loss: 0.0955, step time: 0.1013\n",
      "47/223, train_loss: 0.0909, step time: 0.1261\n",
      "48/223, train_loss: 0.1006, step time: 0.1050\n",
      "49/223, train_loss: 0.0932, step time: 0.1000\n",
      "50/223, train_loss: 0.0961, step time: 0.1007\n",
      "51/223, train_loss: 0.0981, step time: 0.1122\n",
      "52/223, train_loss: 0.1067, step time: 0.1138\n",
      "53/223, train_loss: 0.0939, step time: 0.1004\n",
      "54/223, train_loss: 0.0949, step time: 0.1003\n",
      "55/223, train_loss: 0.0973, step time: 0.1135\n",
      "56/223, train_loss: 0.0958, step time: 0.1059\n",
      "57/223, train_loss: 0.0876, step time: 0.1000\n",
      "58/223, train_loss: 0.0964, step time: 0.1140\n",
      "59/223, train_loss: 0.0955, step time: 0.0999\n",
      "60/223, train_loss: 0.1030, step time: 0.1256\n",
      "61/223, train_loss: 0.0872, step time: 0.1104\n",
      "62/223, train_loss: 0.1016, step time: 0.1001\n",
      "63/223, train_loss: 0.1073, step time: 0.1181\n",
      "64/223, train_loss: 0.1273, step time: 0.1008\n",
      "65/223, train_loss: 0.0968, step time: 0.1141\n",
      "66/223, train_loss: 0.0931, step time: 0.1081\n",
      "67/223, train_loss: 0.0988, step time: 0.1050\n",
      "68/223, train_loss: 0.0953, step time: 0.1092\n",
      "69/223, train_loss: 0.0921, step time: 0.1289\n",
      "70/223, train_loss: 0.0920, step time: 0.1149\n",
      "71/223, train_loss: 0.0937, step time: 0.1068\n",
      "72/223, train_loss: 0.1009, step time: 0.1006\n",
      "73/223, train_loss: 0.0894, step time: 0.1067\n",
      "74/223, train_loss: 0.0997, step time: 0.1203\n",
      "75/223, train_loss: 0.0991, step time: 0.0996\n",
      "76/223, train_loss: 0.0973, step time: 0.1005\n",
      "77/223, train_loss: 0.1056, step time: 0.1096\n",
      "78/223, train_loss: 0.2990, step time: 0.1004\n",
      "79/223, train_loss: 0.0994, step time: 0.1026\n",
      "80/223, train_loss: 0.1179, step time: 0.1003\n",
      "81/223, train_loss: 0.1009, step time: 0.1006\n",
      "82/223, train_loss: 0.1013, step time: 0.1010\n",
      "83/223, train_loss: 0.0986, step time: 0.1006\n",
      "84/223, train_loss: 0.1011, step time: 0.1009\n",
      "85/223, train_loss: 0.0916, step time: 0.1137\n",
      "86/223, train_loss: 0.0952, step time: 0.1136\n",
      "87/223, train_loss: 0.0936, step time: 0.1109\n",
      "88/223, train_loss: 0.1058, step time: 0.1004\n",
      "89/223, train_loss: 0.1017, step time: 0.1096\n",
      "90/223, train_loss: 0.1095, step time: 0.1194\n",
      "91/223, train_loss: 0.0917, step time: 0.1149\n",
      "92/223, train_loss: 0.0955, step time: 0.1002\n",
      "93/223, train_loss: 0.1100, step time: 0.1052\n",
      "94/223, train_loss: 0.0999, step time: 0.1269\n",
      "95/223, train_loss: 0.0969, step time: 0.0993\n",
      "96/223, train_loss: 0.0980, step time: 0.1006\n",
      "97/223, train_loss: 0.0959, step time: 0.1232\n",
      "98/223, train_loss: 0.0958, step time: 0.1030\n",
      "99/223, train_loss: 0.0968, step time: 0.1094\n",
      "100/223, train_loss: 0.1049, step time: 0.1014\n",
      "101/223, train_loss: 0.0990, step time: 0.1252\n",
      "102/223, train_loss: 0.1032, step time: 0.1215\n",
      "103/223, train_loss: 0.0907, step time: 0.1216\n",
      "104/223, train_loss: 0.1098, step time: 0.0996\n",
      "105/223, train_loss: 0.1042, step time: 0.1013\n",
      "106/223, train_loss: 0.0993, step time: 0.1008\n",
      "107/223, train_loss: 0.0983, step time: 0.1053\n",
      "108/223, train_loss: 0.0906, step time: 0.1067\n",
      "109/223, train_loss: 0.0949, step time: 0.1032\n",
      "110/223, train_loss: 0.1068, step time: 0.1013\n",
      "111/223, train_loss: 0.0872, step time: 0.1104\n",
      "112/223, train_loss: 0.0886, step time: 0.1246\n",
      "113/223, train_loss: 0.1044, step time: 0.1118\n",
      "114/223, train_loss: 0.1036, step time: 0.1144\n",
      "115/223, train_loss: 0.0906, step time: 0.1067\n",
      "116/223, train_loss: 0.0884, step time: 0.1020\n",
      "117/223, train_loss: 0.0914, step time: 0.1003\n",
      "118/223, train_loss: 0.1106, step time: 0.1198\n",
      "119/223, train_loss: 0.0993, step time: 0.1132\n",
      "120/223, train_loss: 0.0953, step time: 0.1099\n",
      "121/223, train_loss: 0.1004, step time: 0.1021\n",
      "122/223, train_loss: 0.1007, step time: 0.0994\n",
      "123/223, train_loss: 0.1042, step time: 0.1117\n",
      "124/223, train_loss: 0.1083, step time: 0.1104\n",
      "125/223, train_loss: 0.0990, step time: 0.1001\n",
      "126/223, train_loss: 0.0925, step time: 0.1004\n",
      "127/223, train_loss: 0.1089, step time: 0.1281\n",
      "128/223, train_loss: 0.1040, step time: 0.1096\n",
      "129/223, train_loss: 0.0905, step time: 0.1204\n",
      "130/223, train_loss: 0.1016, step time: 0.1016\n",
      "131/223, train_loss: 0.0960, step time: 0.1134\n",
      "132/223, train_loss: 0.1012, step time: 0.1172\n",
      "133/223, train_loss: 0.1100, step time: 0.1123\n",
      "134/223, train_loss: 0.1032, step time: 0.1008\n",
      "135/223, train_loss: 0.0953, step time: 0.1082\n",
      "136/223, train_loss: 0.0964, step time: 0.1151\n",
      "137/223, train_loss: 0.0953, step time: 0.1301\n",
      "138/223, train_loss: 0.0996, step time: 0.1059\n",
      "139/223, train_loss: 0.1019, step time: 0.1061\n",
      "140/223, train_loss: 0.0819, step time: 0.1165\n",
      "141/223, train_loss: 0.0998, step time: 0.1266\n",
      "142/223, train_loss: 0.0963, step time: 0.1009\n",
      "143/223, train_loss: 0.1037, step time: 0.1140\n",
      "144/223, train_loss: 0.1072, step time: 0.1035\n",
      "145/223, train_loss: 0.0978, step time: 0.1004\n",
      "146/223, train_loss: 0.1166, step time: 0.1223\n",
      "147/223, train_loss: 0.0858, step time: 0.0998\n",
      "148/223, train_loss: 0.1003, step time: 0.0997\n",
      "149/223, train_loss: 0.0985, step time: 0.1007\n",
      "150/223, train_loss: 0.0981, step time: 0.1001\n",
      "151/223, train_loss: 0.0966, step time: 0.1386\n",
      "152/223, train_loss: 0.1116, step time: 0.1143\n",
      "153/223, train_loss: 0.0899, step time: 0.1309\n",
      "154/223, train_loss: 0.1092, step time: 0.1009\n",
      "155/223, train_loss: 0.1015, step time: 0.1126\n",
      "156/223, train_loss: 0.1103, step time: 0.1154\n",
      "157/223, train_loss: 0.0912, step time: 0.0998\n",
      "158/223, train_loss: 0.0969, step time: 0.1051\n",
      "159/223, train_loss: 0.0948, step time: 0.1007\n",
      "160/223, train_loss: 0.1029, step time: 0.1004\n",
      "161/223, train_loss: 0.0998, step time: 0.1140\n",
      "162/223, train_loss: 0.0917, step time: 0.1044\n",
      "163/223, train_loss: 0.0955, step time: 0.1152\n",
      "164/223, train_loss: 0.0997, step time: 0.1246\n",
      "165/223, train_loss: 0.0932, step time: 0.1174\n",
      "166/223, train_loss: 0.0995, step time: 0.1292\n",
      "167/223, train_loss: 0.1095, step time: 0.1072\n",
      "168/223, train_loss: 0.0916, step time: 0.1051\n",
      "169/223, train_loss: 0.0925, step time: 0.1028\n",
      "170/223, train_loss: 0.1071, step time: 0.1081\n",
      "171/223, train_loss: 0.0922, step time: 0.1187\n",
      "172/223, train_loss: 0.0980, step time: 0.1001\n",
      "173/223, train_loss: 0.1011, step time: 0.1000\n",
      "174/223, train_loss: 0.0899, step time: 0.1005\n",
      "175/223, train_loss: 0.1049, step time: 0.1173\n",
      "176/223, train_loss: 0.0932, step time: 0.1141\n",
      "177/223, train_loss: 0.0888, step time: 0.1292\n",
      "178/223, train_loss: 0.0982, step time: 0.1025\n",
      "179/223, train_loss: 0.1007, step time: 0.1150\n",
      "180/223, train_loss: 0.0992, step time: 0.1137\n",
      "181/223, train_loss: 0.1160, step time: 0.1061\n",
      "182/223, train_loss: 0.1044, step time: 0.1277\n",
      "183/223, train_loss: 0.0881, step time: 0.1170\n",
      "184/223, train_loss: 0.1055, step time: 0.1276\n",
      "185/223, train_loss: 0.1124, step time: 0.1024\n",
      "186/223, train_loss: 0.1056, step time: 0.1060\n",
      "187/223, train_loss: 0.0866, step time: 0.1101\n",
      "188/223, train_loss: 0.0926, step time: 0.1714\n",
      "189/223, train_loss: 0.0978, step time: 0.1004\n",
      "190/223, train_loss: 0.1158, step time: 0.1084\n",
      "191/223, train_loss: 0.1047, step time: 0.1115\n",
      "192/223, train_loss: 0.0910, step time: 0.1187\n",
      "193/223, train_loss: 0.0934, step time: 0.1263\n",
      "194/223, train_loss: 0.1053, step time: 0.1034\n",
      "195/223, train_loss: 0.0981, step time: 0.1240\n",
      "196/223, train_loss: 0.0973, step time: 0.1189\n",
      "197/223, train_loss: 0.0999, step time: 0.1362\n",
      "198/223, train_loss: 0.1004, step time: 0.1099\n",
      "199/223, train_loss: 0.0993, step time: 0.1205\n",
      "200/223, train_loss: 0.1025, step time: 0.1255\n",
      "201/223, train_loss: 0.0966, step time: 0.1171\n",
      "202/223, train_loss: 0.1050, step time: 0.1150\n",
      "203/223, train_loss: 0.0908, step time: 0.1147\n",
      "204/223, train_loss: 0.0921, step time: 0.1253\n",
      "205/223, train_loss: 0.0916, step time: 0.1121\n",
      "206/223, train_loss: 0.1053, step time: 0.1018\n",
      "207/223, train_loss: 0.0908, step time: 0.1135\n",
      "208/223, train_loss: 0.0999, step time: 0.1083\n",
      "209/223, train_loss: 0.1020, step time: 0.1065\n",
      "210/223, train_loss: 0.0924, step time: 0.1097\n",
      "211/223, train_loss: 0.0889, step time: 0.1181\n",
      "212/223, train_loss: 0.0945, step time: 0.1088\n",
      "213/223, train_loss: 0.0943, step time: 0.1095\n",
      "214/223, train_loss: 0.0950, step time: 0.1155\n",
      "215/223, train_loss: 0.0980, step time: 0.1075\n",
      "216/223, train_loss: 0.1071, step time: 0.1013\n",
      "217/223, train_loss: 0.0974, step time: 0.1209\n",
      "218/223, train_loss: 0.0939, step time: 0.1058\n",
      "219/223, train_loss: 0.0973, step time: 0.0999\n",
      "220/223, train_loss: 0.0959, step time: 0.1003\n",
      "221/223, train_loss: 0.1049, step time: 0.0999\n",
      "222/223, train_loss: 0.1067, step time: 0.0986\n",
      "223/223, train_loss: 0.1066, step time: 0.1002\n",
      "epoch 278 average loss: 0.0994\n",
      "time consuming of epoch 278 is: 88.7837\n",
      "----------\n",
      "epoch 279/300\n",
      "1/223, train_loss: 0.0962, step time: 0.1014\n",
      "2/223, train_loss: 0.1089, step time: 0.1009\n",
      "3/223, train_loss: 0.0955, step time: 0.0995\n",
      "4/223, train_loss: 0.0902, step time: 0.1087\n",
      "5/223, train_loss: 0.0971, step time: 0.1160\n",
      "6/223, train_loss: 0.1169, step time: 0.1197\n",
      "7/223, train_loss: 0.1007, step time: 0.0999\n",
      "8/223, train_loss: 0.1085, step time: 0.1068\n",
      "9/223, train_loss: 0.0978, step time: 0.1106\n",
      "10/223, train_loss: 0.1059, step time: 0.1011\n",
      "11/223, train_loss: 0.1007, step time: 0.1004\n",
      "12/223, train_loss: 0.0854, step time: 0.1269\n",
      "13/223, train_loss: 0.0919, step time: 0.1154\n",
      "14/223, train_loss: 0.0904, step time: 0.0998\n",
      "15/223, train_loss: 0.1008, step time: 0.1044\n",
      "16/223, train_loss: 0.0972, step time: 0.1241\n",
      "17/223, train_loss: 0.0984, step time: 0.1040\n",
      "18/223, train_loss: 0.0943, step time: 0.1071\n",
      "19/223, train_loss: 0.0858, step time: 0.1307\n",
      "20/223, train_loss: 0.0968, step time: 0.1219\n",
      "21/223, train_loss: 0.0937, step time: 0.1102\n",
      "22/223, train_loss: 0.0972, step time: 0.1551\n",
      "23/223, train_loss: 0.1011, step time: 0.1416\n",
      "24/223, train_loss: 0.1052, step time: 0.1192\n",
      "25/223, train_loss: 0.0934, step time: 0.1189\n",
      "26/223, train_loss: 0.1010, step time: 0.1154\n",
      "27/223, train_loss: 0.0959, step time: 0.1042\n",
      "28/223, train_loss: 0.0937, step time: 0.1084\n",
      "29/223, train_loss: 0.0955, step time: 0.1159\n",
      "30/223, train_loss: 0.1126, step time: 0.1362\n",
      "31/223, train_loss: 0.1039, step time: 0.1447\n",
      "32/223, train_loss: 0.1018, step time: 0.1137\n",
      "33/223, train_loss: 0.0929, step time: 0.1067\n",
      "34/223, train_loss: 0.1061, step time: 0.1121\n",
      "35/223, train_loss: 0.0912, step time: 0.1087\n",
      "36/223, train_loss: 0.0932, step time: 0.1190\n",
      "37/223, train_loss: 0.1068, step time: 0.1122\n",
      "38/223, train_loss: 0.0961, step time: 0.1010\n",
      "39/223, train_loss: 0.1053, step time: 0.1190\n",
      "40/223, train_loss: 0.0986, step time: 0.1157\n",
      "41/223, train_loss: 0.1046, step time: 0.1099\n",
      "42/223, train_loss: 0.1156, step time: 0.1250\n",
      "43/223, train_loss: 0.1015, step time: 0.1121\n",
      "44/223, train_loss: 0.0912, step time: 0.1073\n",
      "45/223, train_loss: 0.0902, step time: 0.1120\n",
      "46/223, train_loss: 0.0952, step time: 0.1098\n",
      "47/223, train_loss: 0.1105, step time: 0.1124\n",
      "48/223, train_loss: 0.1042, step time: 0.1061\n",
      "49/223, train_loss: 0.1055, step time: 0.1153\n",
      "50/223, train_loss: 0.0913, step time: 0.1032\n",
      "51/223, train_loss: 0.0976, step time: 0.1217\n",
      "52/223, train_loss: 0.1163, step time: 0.1124\n",
      "53/223, train_loss: 0.0949, step time: 0.1065\n",
      "54/223, train_loss: 0.1036, step time: 0.1071\n",
      "55/223, train_loss: 0.0998, step time: 0.1160\n",
      "56/223, train_loss: 0.0967, step time: 0.1104\n",
      "57/223, train_loss: 0.0894, step time: 0.1079\n",
      "58/223, train_loss: 0.0893, step time: 0.1018\n",
      "59/223, train_loss: 0.0902, step time: 0.1278\n",
      "60/223, train_loss: 0.0949, step time: 0.1056\n",
      "61/223, train_loss: 0.1044, step time: 0.1219\n",
      "62/223, train_loss: 0.1024, step time: 0.1089\n",
      "63/223, train_loss: 0.0995, step time: 0.1152\n",
      "64/223, train_loss: 0.0872, step time: 0.1214\n",
      "65/223, train_loss: 0.0948, step time: 0.1181\n",
      "66/223, train_loss: 0.0991, step time: 0.1061\n",
      "67/223, train_loss: 0.0898, step time: 0.1085\n",
      "68/223, train_loss: 0.0977, step time: 0.1096\n",
      "69/223, train_loss: 0.0944, step time: 0.1024\n",
      "70/223, train_loss: 0.1069, step time: 0.1154\n",
      "71/223, train_loss: 0.1089, step time: 0.1125\n",
      "72/223, train_loss: 0.1015, step time: 0.1185\n",
      "73/223, train_loss: 0.0865, step time: 0.1061\n",
      "74/223, train_loss: 0.0952, step time: 0.1129\n",
      "75/223, train_loss: 0.0980, step time: 0.1006\n",
      "76/223, train_loss: 0.0887, step time: 0.1177\n",
      "77/223, train_loss: 0.0922, step time: 0.1006\n",
      "78/223, train_loss: 0.0964, step time: 0.1072\n",
      "79/223, train_loss: 0.1036, step time: 0.1010\n",
      "80/223, train_loss: 0.1025, step time: 0.1067\n",
      "81/223, train_loss: 0.0997, step time: 0.1165\n",
      "82/223, train_loss: 0.0870, step time: 0.1075\n",
      "83/223, train_loss: 0.1120, step time: 0.1148\n",
      "84/223, train_loss: 0.0990, step time: 0.1047\n",
      "85/223, train_loss: 0.0993, step time: 0.1010\n",
      "86/223, train_loss: 0.0990, step time: 0.1178\n",
      "87/223, train_loss: 0.0994, step time: 0.1000\n",
      "88/223, train_loss: 0.1005, step time: 0.1522\n",
      "89/223, train_loss: 0.0978, step time: 0.1230\n",
      "90/223, train_loss: 0.0907, step time: 0.1019\n",
      "91/223, train_loss: 0.1024, step time: 0.1366\n",
      "92/223, train_loss: 0.1064, step time: 0.1257\n",
      "93/223, train_loss: 0.1012, step time: 0.1141\n",
      "94/223, train_loss: 0.0948, step time: 0.1061\n",
      "95/223, train_loss: 0.0894, step time: 0.1079\n",
      "96/223, train_loss: 0.0925, step time: 0.1217\n",
      "97/223, train_loss: 0.0994, step time: 0.1109\n",
      "98/223, train_loss: 0.0956, step time: 0.1097\n",
      "99/223, train_loss: 0.0910, step time: 0.1337\n",
      "100/223, train_loss: 0.0946, step time: 0.1160\n",
      "101/223, train_loss: 0.0979, step time: 0.1146\n",
      "102/223, train_loss: 0.1096, step time: 0.1176\n",
      "103/223, train_loss: 0.0881, step time: 0.1225\n",
      "104/223, train_loss: 0.1058, step time: 0.1073\n",
      "105/223, train_loss: 0.1020, step time: 0.1078\n",
      "106/223, train_loss: 0.0856, step time: 0.1007\n",
      "107/223, train_loss: 0.1026, step time: 0.1092\n",
      "108/223, train_loss: 0.0939, step time: 0.1097\n",
      "109/223, train_loss: 0.1112, step time: 0.1153\n",
      "110/223, train_loss: 0.0893, step time: 0.1095\n",
      "111/223, train_loss: 0.0968, step time: 0.1175\n",
      "112/223, train_loss: 0.1008, step time: 0.1174\n",
      "113/223, train_loss: 0.0948, step time: 0.1090\n",
      "114/223, train_loss: 0.0878, step time: 0.1248\n",
      "115/223, train_loss: 0.0953, step time: 0.1576\n",
      "116/223, train_loss: 0.1093, step time: 0.1006\n",
      "117/223, train_loss: 0.0895, step time: 0.1130\n",
      "118/223, train_loss: 0.1025, step time: 0.1100\n",
      "119/223, train_loss: 0.1087, step time: 0.1117\n",
      "120/223, train_loss: 0.1094, step time: 0.1039\n",
      "121/223, train_loss: 0.0888, step time: 0.1357\n",
      "122/223, train_loss: 0.0991, step time: 0.1256\n",
      "123/223, train_loss: 0.1112, step time: 0.1076\n",
      "124/223, train_loss: 0.1036, step time: 0.1017\n",
      "125/223, train_loss: 0.0906, step time: 0.1055\n",
      "126/223, train_loss: 0.0991, step time: 0.1067\n",
      "127/223, train_loss: 0.0980, step time: 0.1172\n",
      "128/223, train_loss: 0.1060, step time: 0.1008\n",
      "129/223, train_loss: 0.1007, step time: 0.1136\n",
      "130/223, train_loss: 0.1105, step time: 0.1122\n",
      "131/223, train_loss: 0.0991, step time: 0.1250\n",
      "132/223, train_loss: 0.0869, step time: 0.1123\n",
      "133/223, train_loss: 0.0980, step time: 0.0998\n",
      "134/223, train_loss: 0.1077, step time: 0.0994\n",
      "135/223, train_loss: 0.1054, step time: 0.0989\n",
      "136/223, train_loss: 0.0943, step time: 0.1226\n",
      "137/223, train_loss: 0.0985, step time: 0.1092\n",
      "138/223, train_loss: 0.0967, step time: 0.1235\n",
      "139/223, train_loss: 0.0939, step time: 0.1192\n",
      "140/223, train_loss: 0.1052, step time: 0.1093\n",
      "141/223, train_loss: 0.0930, step time: 0.1050\n",
      "142/223, train_loss: 0.0966, step time: 0.1368\n",
      "143/223, train_loss: 0.2968, step time: 0.1346\n",
      "144/223, train_loss: 0.0974, step time: 0.1068\n",
      "145/223, train_loss: 0.0961, step time: 0.1336\n",
      "146/223, train_loss: 0.0914, step time: 0.1092\n",
      "147/223, train_loss: 0.0951, step time: 0.1372\n",
      "148/223, train_loss: 0.1030, step time: 0.1383\n",
      "149/223, train_loss: 0.1038, step time: 0.1095\n",
      "150/223, train_loss: 0.1052, step time: 0.1304\n",
      "151/223, train_loss: 0.0960, step time: 0.1101\n",
      "152/223, train_loss: 0.0936, step time: 0.1180\n",
      "153/223, train_loss: 0.0985, step time: 0.1195\n",
      "154/223, train_loss: 0.1011, step time: 0.1188\n",
      "155/223, train_loss: 0.0983, step time: 0.1478\n",
      "156/223, train_loss: 0.1024, step time: 0.1168\n",
      "157/223, train_loss: 0.0932, step time: 0.0997\n",
      "158/223, train_loss: 0.0923, step time: 0.1091\n",
      "159/223, train_loss: 0.0923, step time: 0.1120\n",
      "160/223, train_loss: 0.1088, step time: 0.1133\n",
      "161/223, train_loss: 0.0940, step time: 0.1072\n",
      "162/223, train_loss: 0.0971, step time: 0.1023\n",
      "163/223, train_loss: 0.1097, step time: 0.1083\n",
      "164/223, train_loss: 0.1082, step time: 0.1145\n",
      "165/223, train_loss: 0.1041, step time: 0.1162\n",
      "166/223, train_loss: 0.1036, step time: 0.1016\n",
      "167/223, train_loss: 0.0997, step time: 0.1134\n",
      "168/223, train_loss: 0.1087, step time: 0.1133\n",
      "169/223, train_loss: 0.0978, step time: 0.1230\n",
      "170/223, train_loss: 0.0960, step time: 0.1102\n",
      "171/223, train_loss: 0.0953, step time: 0.1183\n",
      "172/223, train_loss: 0.0994, step time: 0.1130\n",
      "173/223, train_loss: 0.0921, step time: 0.1369\n",
      "174/223, train_loss: 0.0964, step time: 0.1366\n",
      "175/223, train_loss: 0.0967, step time: 0.1176\n",
      "176/223, train_loss: 0.1060, step time: 0.1174\n",
      "177/223, train_loss: 0.0855, step time: 0.1069\n",
      "178/223, train_loss: 0.1023, step time: 0.1299\n",
      "179/223, train_loss: 0.1134, step time: 0.1300\n",
      "180/223, train_loss: 0.1050, step time: 0.1009\n",
      "181/223, train_loss: 0.0880, step time: 0.0994\n",
      "182/223, train_loss: 0.0928, step time: 0.0989\n",
      "183/223, train_loss: 0.1024, step time: 0.0991\n",
      "184/223, train_loss: 0.0911, step time: 0.1010\n",
      "185/223, train_loss: 0.0922, step time: 0.0998\n",
      "186/223, train_loss: 0.1114, step time: 0.0998\n",
      "187/223, train_loss: 0.1017, step time: 0.1000\n",
      "188/223, train_loss: 0.1020, step time: 0.1016\n",
      "189/223, train_loss: 0.0950, step time: 0.1005\n",
      "190/223, train_loss: 0.0989, step time: 0.0993\n",
      "191/223, train_loss: 0.0915, step time: 0.0993\n",
      "192/223, train_loss: 0.1101, step time: 0.1004\n",
      "193/223, train_loss: 0.1048, step time: 0.1007\n",
      "194/223, train_loss: 0.0993, step time: 0.1007\n",
      "195/223, train_loss: 0.0972, step time: 0.1005\n",
      "196/223, train_loss: 0.0943, step time: 0.1043\n",
      "197/223, train_loss: 0.0996, step time: 0.1068\n",
      "198/223, train_loss: 0.0998, step time: 0.1182\n",
      "199/223, train_loss: 0.1028, step time: 0.1372\n",
      "200/223, train_loss: 0.0980, step time: 0.1110\n",
      "201/223, train_loss: 0.1021, step time: 0.1104\n",
      "202/223, train_loss: 0.0895, step time: 0.1007\n",
      "203/223, train_loss: 0.1023, step time: 0.1080\n",
      "204/223, train_loss: 0.1003, step time: 0.1215\n",
      "205/223, train_loss: 0.0865, step time: 0.1421\n",
      "206/223, train_loss: 0.1000, step time: 0.1080\n",
      "207/223, train_loss: 0.1067, step time: 0.1244\n",
      "208/223, train_loss: 0.0958, step time: 0.1110\n",
      "209/223, train_loss: 0.1011, step time: 0.0988\n",
      "210/223, train_loss: 0.0934, step time: 0.0985\n",
      "211/223, train_loss: 0.0944, step time: 0.0989\n",
      "212/223, train_loss: 0.1136, step time: 0.0991\n",
      "213/223, train_loss: 0.0883, step time: 0.0999\n",
      "214/223, train_loss: 0.0929, step time: 0.1264\n",
      "215/223, train_loss: 0.0920, step time: 0.1037\n",
      "216/223, train_loss: 0.0962, step time: 0.1119\n",
      "217/223, train_loss: 0.0958, step time: 0.1018\n",
      "218/223, train_loss: 0.0949, step time: 0.1032\n",
      "219/223, train_loss: 0.0989, step time: 0.1349\n",
      "220/223, train_loss: 0.0953, step time: 0.1070\n",
      "221/223, train_loss: 0.1018, step time: 0.0994\n",
      "222/223, train_loss: 0.1071, step time: 0.1001\n",
      "223/223, train_loss: 0.1077, step time: 0.1012\n",
      "epoch 279 average loss: 0.0995\n",
      "time consuming of epoch 279 is: 91.1440\n",
      "----------\n",
      "epoch 280/300\n",
      "1/223, train_loss: 0.0901, step time: 0.1157\n",
      "2/223, train_loss: 0.1170, step time: 0.1161\n",
      "3/223, train_loss: 0.1025, step time: 0.1589\n",
      "4/223, train_loss: 0.0894, step time: 0.1073\n",
      "5/223, train_loss: 0.0973, step time: 0.1005\n",
      "6/223, train_loss: 0.1013, step time: 0.1010\n",
      "7/223, train_loss: 0.1036, step time: 0.1120\n",
      "8/223, train_loss: 0.0982, step time: 0.1116\n",
      "9/223, train_loss: 0.0978, step time: 0.1005\n",
      "10/223, train_loss: 0.0939, step time: 0.1042\n",
      "11/223, train_loss: 0.0987, step time: 0.1235\n",
      "12/223, train_loss: 0.1040, step time: 0.1205\n",
      "13/223, train_loss: 0.1004, step time: 0.1182\n",
      "14/223, train_loss: 0.0948, step time: 0.1184\n",
      "15/223, train_loss: 0.0938, step time: 0.1166\n",
      "16/223, train_loss: 0.0927, step time: 0.1137\n",
      "17/223, train_loss: 0.0927, step time: 0.0999\n",
      "18/223, train_loss: 0.1004, step time: 0.1054\n",
      "19/223, train_loss: 0.0983, step time: 0.1224\n",
      "20/223, train_loss: 0.1010, step time: 0.1187\n",
      "21/223, train_loss: 0.1082, step time: 0.1079\n",
      "22/223, train_loss: 0.0928, step time: 0.1096\n",
      "23/223, train_loss: 0.0892, step time: 0.1206\n",
      "24/223, train_loss: 0.0944, step time: 0.1073\n",
      "25/223, train_loss: 0.1003, step time: 0.1076\n",
      "26/223, train_loss: 0.0951, step time: 0.1191\n",
      "27/223, train_loss: 0.1016, step time: 0.1254\n",
      "28/223, train_loss: 0.0977, step time: 0.1683\n",
      "29/223, train_loss: 0.1116, step time: 0.1141\n",
      "30/223, train_loss: 0.1048, step time: 0.1140\n",
      "31/223, train_loss: 0.0970, step time: 0.1197\n",
      "32/223, train_loss: 0.0941, step time: 0.1083\n",
      "33/223, train_loss: 0.0991, step time: 0.1020\n",
      "34/223, train_loss: 0.1087, step time: 0.1010\n",
      "35/223, train_loss: 0.1107, step time: 0.1275\n",
      "36/223, train_loss: 0.1011, step time: 0.1119\n",
      "37/223, train_loss: 0.1054, step time: 0.1158\n",
      "38/223, train_loss: 0.1057, step time: 0.1250\n",
      "39/223, train_loss: 0.0961, step time: 0.1435\n",
      "40/223, train_loss: 0.0926, step time: 0.1105\n",
      "41/223, train_loss: 0.1050, step time: 0.1003\n",
      "42/223, train_loss: 0.0960, step time: 0.1067\n",
      "43/223, train_loss: 0.0965, step time: 0.1204\n",
      "44/223, train_loss: 0.0911, step time: 0.1149\n",
      "45/223, train_loss: 0.0856, step time: 0.1003\n",
      "46/223, train_loss: 0.0928, step time: 0.1130\n",
      "47/223, train_loss: 0.0949, step time: 0.1000\n",
      "48/223, train_loss: 0.0947, step time: 0.1063\n",
      "49/223, train_loss: 0.0937, step time: 0.1009\n",
      "50/223, train_loss: 0.1026, step time: 0.1004\n",
      "51/223, train_loss: 0.0841, step time: 0.1012\n",
      "52/223, train_loss: 0.0997, step time: 0.1125\n",
      "53/223, train_loss: 0.1092, step time: 0.1156\n",
      "54/223, train_loss: 0.0919, step time: 0.1023\n",
      "55/223, train_loss: 0.1032, step time: 0.1007\n",
      "56/223, train_loss: 0.0888, step time: 0.1018\n",
      "57/223, train_loss: 0.1041, step time: 0.1099\n",
      "58/223, train_loss: 0.1126, step time: 0.1280\n",
      "59/223, train_loss: 0.0972, step time: 0.1008\n",
      "60/223, train_loss: 0.0910, step time: 0.1132\n",
      "61/223, train_loss: 0.0965, step time: 0.1121\n",
      "62/223, train_loss: 0.0948, step time: 0.1026\n",
      "63/223, train_loss: 0.1053, step time: 0.1007\n",
      "64/223, train_loss: 0.0918, step time: 0.1004\n",
      "65/223, train_loss: 0.0897, step time: 0.1117\n",
      "66/223, train_loss: 0.0903, step time: 0.1018\n",
      "67/223, train_loss: 0.0904, step time: 0.1180\n",
      "68/223, train_loss: 0.0925, step time: 0.1121\n",
      "69/223, train_loss: 0.0877, step time: 0.1109\n",
      "70/223, train_loss: 0.0927, step time: 0.1122\n",
      "71/223, train_loss: 0.0969, step time: 0.1154\n",
      "72/223, train_loss: 0.1082, step time: 0.1064\n",
      "73/223, train_loss: 0.0970, step time: 0.1104\n",
      "74/223, train_loss: 0.0949, step time: 0.1044\n",
      "75/223, train_loss: 0.0915, step time: 0.1056\n",
      "76/223, train_loss: 0.0995, step time: 0.1109\n",
      "77/223, train_loss: 0.1063, step time: 0.1149\n",
      "78/223, train_loss: 0.0954, step time: 0.1128\n",
      "79/223, train_loss: 0.1015, step time: 0.1088\n",
      "80/223, train_loss: 0.1046, step time: 0.1040\n",
      "81/223, train_loss: 0.1034, step time: 0.1156\n",
      "82/223, train_loss: 0.0985, step time: 0.1121\n",
      "83/223, train_loss: 0.0998, step time: 0.1235\n",
      "84/223, train_loss: 0.1022, step time: 0.1144\n",
      "85/223, train_loss: 0.0921, step time: 0.1090\n",
      "86/223, train_loss: 0.1044, step time: 0.1059\n",
      "87/223, train_loss: 0.0939, step time: 0.1023\n",
      "88/223, train_loss: 0.1027, step time: 0.1005\n",
      "89/223, train_loss: 0.0869, step time: 0.1060\n",
      "90/223, train_loss: 0.0897, step time: 0.1353\n",
      "91/223, train_loss: 0.1014, step time: 0.1061\n",
      "92/223, train_loss: 0.1080, step time: 0.1006\n",
      "93/223, train_loss: 0.0896, step time: 0.1137\n",
      "94/223, train_loss: 0.1031, step time: 0.1022\n",
      "95/223, train_loss: 0.0992, step time: 0.1006\n",
      "96/223, train_loss: 0.0967, step time: 0.1006\n",
      "97/223, train_loss: 0.0937, step time: 0.1010\n",
      "98/223, train_loss: 0.0913, step time: 0.1242\n",
      "99/223, train_loss: 0.0948, step time: 0.1344\n",
      "100/223, train_loss: 0.0884, step time: 0.1199\n",
      "101/223, train_loss: 0.1168, step time: 0.1061\n",
      "102/223, train_loss: 0.0984, step time: 0.1208\n",
      "103/223, train_loss: 0.1089, step time: 0.1064\n",
      "104/223, train_loss: 0.0978, step time: 0.1297\n",
      "105/223, train_loss: 0.0955, step time: 0.1084\n",
      "106/223, train_loss: 0.0923, step time: 0.1090\n",
      "107/223, train_loss: 0.1044, step time: 0.1159\n",
      "108/223, train_loss: 0.0893, step time: 0.1210\n",
      "109/223, train_loss: 0.0979, step time: 0.1101\n",
      "110/223, train_loss: 0.0995, step time: 0.1051\n",
      "111/223, train_loss: 0.1038, step time: 0.1020\n",
      "112/223, train_loss: 0.0945, step time: 0.1189\n",
      "113/223, train_loss: 0.0918, step time: 0.1170\n",
      "114/223, train_loss: 0.1144, step time: 0.1103\n",
      "115/223, train_loss: 0.1015, step time: 0.1038\n",
      "116/223, train_loss: 0.0882, step time: 0.1106\n",
      "117/223, train_loss: 0.0967, step time: 0.1155\n",
      "118/223, train_loss: 0.1158, step time: 0.1011\n",
      "119/223, train_loss: 0.1035, step time: 0.0996\n",
      "120/223, train_loss: 0.0964, step time: 0.1053\n",
      "121/223, train_loss: 0.0920, step time: 0.1183\n",
      "122/223, train_loss: 0.0971, step time: 0.1012\n",
      "123/223, train_loss: 0.1051, step time: 0.1106\n",
      "124/223, train_loss: 0.0987, step time: 0.1007\n",
      "125/223, train_loss: 0.0894, step time: 0.1013\n",
      "126/223, train_loss: 0.1001, step time: 0.1266\n",
      "127/223, train_loss: 0.1035, step time: 0.1024\n",
      "128/223, train_loss: 0.1178, step time: 0.1358\n",
      "129/223, train_loss: 0.0991, step time: 0.1149\n",
      "130/223, train_loss: 0.0977, step time: 0.1168\n",
      "131/223, train_loss: 0.0937, step time: 0.1061\n",
      "132/223, train_loss: 0.0942, step time: 0.1005\n",
      "133/223, train_loss: 0.0912, step time: 0.1197\n",
      "134/223, train_loss: 0.0890, step time: 0.1090\n",
      "135/223, train_loss: 0.0888, step time: 0.1085\n",
      "136/223, train_loss: 0.1078, step time: 0.1644\n",
      "137/223, train_loss: 0.0927, step time: 0.1007\n",
      "138/223, train_loss: 0.1098, step time: 0.1025\n",
      "139/223, train_loss: 0.1044, step time: 0.1261\n",
      "140/223, train_loss: 0.0922, step time: 0.1312\n",
      "141/223, train_loss: 0.1045, step time: 0.1272\n",
      "142/223, train_loss: 0.0981, step time: 0.1008\n",
      "143/223, train_loss: 0.0915, step time: 0.1287\n",
      "144/223, train_loss: 0.0902, step time: 0.1023\n",
      "145/223, train_loss: 0.0995, step time: 0.1009\n",
      "146/223, train_loss: 0.0959, step time: 0.1084\n",
      "147/223, train_loss: 0.1156, step time: 0.1124\n",
      "148/223, train_loss: 0.1022, step time: 0.1298\n",
      "149/223, train_loss: 0.0989, step time: 0.1070\n",
      "150/223, train_loss: 0.1103, step time: 0.1096\n",
      "151/223, train_loss: 0.0977, step time: 0.1069\n",
      "152/223, train_loss: 0.1033, step time: 0.1073\n",
      "153/223, train_loss: 0.0993, step time: 0.1161\n",
      "154/223, train_loss: 0.1145, step time: 0.1099\n",
      "155/223, train_loss: 0.1004, step time: 0.1144\n",
      "156/223, train_loss: 0.0982, step time: 0.1104\n",
      "157/223, train_loss: 0.0991, step time: 0.1007\n",
      "158/223, train_loss: 0.1049, step time: 0.1051\n",
      "159/223, train_loss: 0.0905, step time: 0.1009\n",
      "160/223, train_loss: 0.0949, step time: 0.0996\n",
      "161/223, train_loss: 0.1155, step time: 0.1105\n",
      "162/223, train_loss: 0.0977, step time: 0.1120\n",
      "163/223, train_loss: 0.0913, step time: 0.1327\n",
      "164/223, train_loss: 0.0971, step time: 0.1253\n",
      "165/223, train_loss: 0.1033, step time: 0.1217\n",
      "166/223, train_loss: 0.0937, step time: 0.1062\n",
      "167/223, train_loss: 0.1054, step time: 0.1141\n",
      "168/223, train_loss: 0.0936, step time: 0.1006\n",
      "169/223, train_loss: 0.1124, step time: 0.1085\n",
      "170/223, train_loss: 0.0991, step time: 0.1061\n",
      "171/223, train_loss: 0.1097, step time: 0.1047\n",
      "172/223, train_loss: 0.1083, step time: 0.1001\n",
      "173/223, train_loss: 0.0911, step time: 0.1042\n",
      "174/223, train_loss: 0.1083, step time: 0.1066\n",
      "175/223, train_loss: 0.1022, step time: 0.1006\n",
      "176/223, train_loss: 0.1071, step time: 0.1009\n",
      "177/223, train_loss: 0.0917, step time: 0.1030\n",
      "178/223, train_loss: 0.0981, step time: 0.1219\n",
      "179/223, train_loss: 0.1006, step time: 0.1054\n",
      "180/223, train_loss: 0.0854, step time: 0.1003\n",
      "181/223, train_loss: 0.0912, step time: 0.1031\n",
      "182/223, train_loss: 0.0861, step time: 0.1341\n",
      "183/223, train_loss: 0.0867, step time: 0.1004\n",
      "184/223, train_loss: 0.1032, step time: 0.1041\n",
      "185/223, train_loss: 0.0988, step time: 0.1226\n",
      "186/223, train_loss: 0.1183, step time: 0.1154\n",
      "187/223, train_loss: 0.0992, step time: 0.1088\n",
      "188/223, train_loss: 0.0999, step time: 0.1012\n",
      "189/223, train_loss: 0.1096, step time: 0.1007\n",
      "190/223, train_loss: 0.0975, step time: 0.1292\n",
      "191/223, train_loss: 0.1023, step time: 0.1003\n",
      "192/223, train_loss: 0.0924, step time: 0.1097\n",
      "193/223, train_loss: 0.0883, step time: 0.1442\n",
      "194/223, train_loss: 0.0958, step time: 0.1333\n",
      "195/223, train_loss: 0.1061, step time: 0.1123\n",
      "196/223, train_loss: 0.0945, step time: 0.1196\n",
      "197/223, train_loss: 0.1019, step time: 0.1009\n",
      "198/223, train_loss: 0.1028, step time: 0.1136\n",
      "199/223, train_loss: 0.0905, step time: 0.1481\n",
      "200/223, train_loss: 0.0896, step time: 0.1493\n",
      "201/223, train_loss: 0.0903, step time: 0.1120\n",
      "202/223, train_loss: 0.0970, step time: 0.1013\n",
      "203/223, train_loss: 0.0939, step time: 0.1100\n",
      "204/223, train_loss: 0.1014, step time: 0.1186\n",
      "205/223, train_loss: 0.0952, step time: 0.1229\n",
      "206/223, train_loss: 0.0940, step time: 0.1089\n",
      "207/223, train_loss: 0.1041, step time: 0.1035\n",
      "208/223, train_loss: 0.1106, step time: 0.1284\n",
      "209/223, train_loss: 0.1100, step time: 0.1257\n",
      "210/223, train_loss: 0.0956, step time: 0.1005\n",
      "211/223, train_loss: 0.1135, step time: 0.1034\n",
      "212/223, train_loss: 0.0863, step time: 0.1030\n",
      "213/223, train_loss: 0.1127, step time: 0.1061\n",
      "214/223, train_loss: 0.0977, step time: 0.1090\n",
      "215/223, train_loss: 0.1038, step time: 0.1003\n",
      "216/223, train_loss: 0.0897, step time: 0.1017\n",
      "217/223, train_loss: 0.2924, step time: 0.1003\n",
      "218/223, train_loss: 0.0925, step time: 0.1248\n",
      "219/223, train_loss: 0.0973, step time: 0.0996\n",
      "220/223, train_loss: 0.0944, step time: 0.1003\n",
      "221/223, train_loss: 0.0995, step time: 0.1005\n",
      "222/223, train_loss: 0.1016, step time: 0.1005\n",
      "223/223, train_loss: 0.0994, step time: 0.0991\n",
      "epoch 280 average loss: 0.0995\n",
      "current epoch: 280 current mean dice: 0.8621 tc: 0.9225 wt: 0.8717 et: 0.7921\n",
      "best mean dice: 0.8622 at epoch: 260\n",
      "time consuming of epoch 280 is: 91.5543\n",
      "----------\n",
      "epoch 281/300\n",
      "1/223, train_loss: 0.1142, step time: 0.1075\n",
      "2/223, train_loss: 0.0956, step time: 0.0998\n",
      "3/223, train_loss: 0.0993, step time: 0.1003\n",
      "4/223, train_loss: 0.0933, step time: 0.1218\n",
      "5/223, train_loss: 0.0915, step time: 0.1155\n",
      "6/223, train_loss: 0.0953, step time: 0.1094\n",
      "7/223, train_loss: 0.0929, step time: 0.1067\n",
      "8/223, train_loss: 0.1034, step time: 0.1327\n",
      "9/223, train_loss: 0.0963, step time: 0.1200\n",
      "10/223, train_loss: 0.0950, step time: 0.1285\n",
      "11/223, train_loss: 0.1010, step time: 0.1195\n",
      "12/223, train_loss: 0.0919, step time: 0.1307\n",
      "13/223, train_loss: 0.1020, step time: 0.1093\n",
      "14/223, train_loss: 0.0934, step time: 0.1091\n",
      "15/223, train_loss: 0.1045, step time: 0.2020\n",
      "16/223, train_loss: 0.0964, step time: 0.1363\n",
      "17/223, train_loss: 0.1027, step time: 0.1246\n",
      "18/223, train_loss: 0.0957, step time: 0.1019\n",
      "19/223, train_loss: 0.1039, step time: 0.1004\n",
      "20/223, train_loss: 0.0886, step time: 0.1073\n",
      "21/223, train_loss: 0.1120, step time: 0.1054\n",
      "22/223, train_loss: 0.0913, step time: 0.1007\n",
      "23/223, train_loss: 0.0946, step time: 0.1001\n",
      "24/223, train_loss: 0.0968, step time: 0.1236\n",
      "25/223, train_loss: 0.0890, step time: 0.1079\n",
      "26/223, train_loss: 0.0923, step time: 0.1405\n",
      "27/223, train_loss: 0.1085, step time: 0.1250\n",
      "28/223, train_loss: 0.1007, step time: 0.1036\n",
      "29/223, train_loss: 0.0970, step time: 0.1124\n",
      "30/223, train_loss: 0.1032, step time: 0.1088\n",
      "31/223, train_loss: 0.1110, step time: 0.1157\n",
      "32/223, train_loss: 0.0964, step time: 0.1009\n",
      "33/223, train_loss: 0.1011, step time: 0.1153\n",
      "34/223, train_loss: 0.1020, step time: 0.1024\n",
      "35/223, train_loss: 0.1017, step time: 0.1313\n",
      "36/223, train_loss: 0.0888, step time: 0.1125\n",
      "37/223, train_loss: 0.1104, step time: 0.1050\n",
      "38/223, train_loss: 0.1142, step time: 0.1189\n",
      "39/223, train_loss: 0.0998, step time: 0.1011\n",
      "40/223, train_loss: 0.1041, step time: 0.1115\n",
      "41/223, train_loss: 0.1050, step time: 0.1043\n",
      "42/223, train_loss: 0.0880, step time: 0.1163\n",
      "43/223, train_loss: 0.1054, step time: 0.1229\n",
      "44/223, train_loss: 0.1019, step time: 0.1006\n",
      "45/223, train_loss: 0.0970, step time: 0.1131\n",
      "46/223, train_loss: 0.0890, step time: 0.1019\n",
      "47/223, train_loss: 0.1122, step time: 0.1051\n",
      "48/223, train_loss: 0.1037, step time: 0.1185\n",
      "49/223, train_loss: 0.1088, step time: 0.1021\n",
      "50/223, train_loss: 0.1054, step time: 0.1225\n",
      "51/223, train_loss: 0.0862, step time: 0.1012\n",
      "52/223, train_loss: 0.1065, step time: 0.1007\n",
      "53/223, train_loss: 0.0915, step time: 0.1013\n",
      "54/223, train_loss: 0.0919, step time: 0.1006\n",
      "55/223, train_loss: 0.1037, step time: 0.1062\n",
      "56/223, train_loss: 0.0923, step time: 0.1196\n",
      "57/223, train_loss: 0.0905, step time: 0.1008\n",
      "58/223, train_loss: 0.0983, step time: 0.1136\n",
      "59/223, train_loss: 0.0961, step time: 0.1003\n",
      "60/223, train_loss: 0.0927, step time: 0.1027\n",
      "61/223, train_loss: 0.0922, step time: 0.1187\n",
      "62/223, train_loss: 0.1101, step time: 0.1074\n",
      "63/223, train_loss: 0.0880, step time: 0.1283\n",
      "64/223, train_loss: 0.0917, step time: 0.1003\n",
      "65/223, train_loss: 0.0983, step time: 0.1113\n",
      "66/223, train_loss: 0.0877, step time: 0.1167\n",
      "67/223, train_loss: 0.0859, step time: 0.0998\n",
      "68/223, train_loss: 0.0987, step time: 0.1125\n",
      "69/223, train_loss: 0.1023, step time: 0.1022\n",
      "70/223, train_loss: 0.1055, step time: 0.1200\n",
      "71/223, train_loss: 0.0936, step time: 0.1301\n",
      "72/223, train_loss: 0.0954, step time: 0.1320\n",
      "73/223, train_loss: 0.0894, step time: 0.1061\n",
      "74/223, train_loss: 0.1050, step time: 0.1063\n",
      "75/223, train_loss: 0.1006, step time: 0.1256\n",
      "76/223, train_loss: 0.0952, step time: 0.1022\n",
      "77/223, train_loss: 0.0945, step time: 0.0998\n",
      "78/223, train_loss: 0.1056, step time: 0.1071\n",
      "79/223, train_loss: 0.1187, step time: 0.1235\n",
      "80/223, train_loss: 0.0942, step time: 0.1044\n",
      "81/223, train_loss: 0.1008, step time: 0.1065\n",
      "82/223, train_loss: 0.0988, step time: 0.1095\n",
      "83/223, train_loss: 0.0916, step time: 0.1131\n",
      "84/223, train_loss: 0.1004, step time: 0.1146\n",
      "85/223, train_loss: 0.0923, step time: 0.1075\n",
      "86/223, train_loss: 0.0970, step time: 0.1122\n",
      "87/223, train_loss: 0.1061, step time: 0.1224\n",
      "88/223, train_loss: 0.0891, step time: 0.1098\n",
      "89/223, train_loss: 0.1058, step time: 0.1157\n",
      "90/223, train_loss: 0.0964, step time: 0.1130\n",
      "91/223, train_loss: 0.0920, step time: 0.1233\n",
      "92/223, train_loss: 0.1128, step time: 0.1122\n",
      "93/223, train_loss: 0.0954, step time: 0.1080\n",
      "94/223, train_loss: 0.0981, step time: 0.1004\n",
      "95/223, train_loss: 0.0938, step time: 0.1268\n",
      "96/223, train_loss: 0.0974, step time: 0.1045\n",
      "97/223, train_loss: 0.1037, step time: 0.1166\n",
      "98/223, train_loss: 0.0979, step time: 0.1124\n",
      "99/223, train_loss: 0.0966, step time: 0.1081\n",
      "100/223, train_loss: 0.1012, step time: 0.1059\n",
      "101/223, train_loss: 0.1013, step time: 0.1017\n",
      "102/223, train_loss: 0.0991, step time: 0.1142\n",
      "103/223, train_loss: 0.0923, step time: 0.1308\n",
      "104/223, train_loss: 0.0966, step time: 0.1180\n",
      "105/223, train_loss: 0.1165, step time: 0.0998\n",
      "106/223, train_loss: 0.0956, step time: 0.1294\n",
      "107/223, train_loss: 0.0971, step time: 0.1000\n",
      "108/223, train_loss: 0.0947, step time: 0.1124\n",
      "109/223, train_loss: 0.1048, step time: 0.1402\n",
      "110/223, train_loss: 0.1036, step time: 0.1327\n",
      "111/223, train_loss: 0.0924, step time: 0.1008\n",
      "112/223, train_loss: 0.0982, step time: 0.1185\n",
      "113/223, train_loss: 0.1023, step time: 0.1139\n",
      "114/223, train_loss: 0.1056, step time: 0.1056\n",
      "115/223, train_loss: 0.0871, step time: 0.1213\n",
      "116/223, train_loss: 0.0908, step time: 0.1712\n",
      "117/223, train_loss: 0.1060, step time: 0.1008\n",
      "118/223, train_loss: 0.1003, step time: 0.1097\n",
      "119/223, train_loss: 0.0926, step time: 0.1289\n",
      "120/223, train_loss: 0.1045, step time: 0.0998\n",
      "121/223, train_loss: 0.0937, step time: 0.0999\n",
      "122/223, train_loss: 0.0993, step time: 0.1020\n",
      "123/223, train_loss: 0.1013, step time: 0.1361\n",
      "124/223, train_loss: 0.1035, step time: 0.0991\n",
      "125/223, train_loss: 0.0902, step time: 0.1125\n",
      "126/223, train_loss: 0.0986, step time: 0.1024\n",
      "127/223, train_loss: 0.0970, step time: 0.1006\n",
      "128/223, train_loss: 0.0860, step time: 0.1012\n",
      "129/223, train_loss: 0.0976, step time: 0.1244\n",
      "130/223, train_loss: 0.0934, step time: 0.1211\n",
      "131/223, train_loss: 0.0920, step time: 0.1554\n",
      "132/223, train_loss: 0.0869, step time: 0.1678\n",
      "133/223, train_loss: 0.0957, step time: 0.1154\n",
      "134/223, train_loss: 0.0970, step time: 0.1005\n",
      "135/223, train_loss: 0.0995, step time: 0.1001\n",
      "136/223, train_loss: 0.0974, step time: 0.1257\n",
      "137/223, train_loss: 0.0983, step time: 0.1069\n",
      "138/223, train_loss: 0.1095, step time: 0.1161\n",
      "139/223, train_loss: 0.1067, step time: 0.1365\n",
      "140/223, train_loss: 0.0987, step time: 0.1011\n",
      "141/223, train_loss: 0.0989, step time: 0.1216\n",
      "142/223, train_loss: 0.0894, step time: 0.1403\n",
      "143/223, train_loss: 0.1066, step time: 0.1359\n",
      "144/223, train_loss: 0.0974, step time: 0.1021\n",
      "145/223, train_loss: 0.0925, step time: 0.1010\n",
      "146/223, train_loss: 0.0918, step time: 0.0999\n",
      "147/223, train_loss: 0.0929, step time: 0.0999\n",
      "148/223, train_loss: 0.0921, step time: 0.1196\n",
      "149/223, train_loss: 0.3004, step time: 0.1077\n",
      "150/223, train_loss: 0.0978, step time: 0.1122\n",
      "151/223, train_loss: 0.1001, step time: 0.1032\n",
      "152/223, train_loss: 0.1035, step time: 0.1166\n",
      "153/223, train_loss: 0.1042, step time: 0.1013\n",
      "154/223, train_loss: 0.1055, step time: 0.1148\n",
      "155/223, train_loss: 0.1164, step time: 0.1012\n",
      "156/223, train_loss: 0.0986, step time: 0.1041\n",
      "157/223, train_loss: 0.1074, step time: 0.1137\n",
      "158/223, train_loss: 0.1039, step time: 0.1003\n",
      "159/223, train_loss: 0.1019, step time: 0.1008\n",
      "160/223, train_loss: 0.1088, step time: 0.1039\n",
      "161/223, train_loss: 0.1135, step time: 0.1076\n",
      "162/223, train_loss: 0.0937, step time: 0.1129\n",
      "163/223, train_loss: 0.1089, step time: 0.1068\n",
      "164/223, train_loss: 0.0865, step time: 0.1088\n",
      "165/223, train_loss: 0.0926, step time: 0.1089\n",
      "166/223, train_loss: 0.0889, step time: 0.1240\n",
      "167/223, train_loss: 0.1001, step time: 0.1362\n",
      "168/223, train_loss: 0.1051, step time: 0.1373\n",
      "169/223, train_loss: 0.0960, step time: 0.1132\n",
      "170/223, train_loss: 0.0942, step time: 0.1337\n",
      "171/223, train_loss: 0.1040, step time: 0.1199\n",
      "172/223, train_loss: 0.1108, step time: 0.1221\n",
      "173/223, train_loss: 0.0913, step time: 0.1175\n",
      "174/223, train_loss: 0.0970, step time: 0.1125\n",
      "175/223, train_loss: 0.0946, step time: 0.1271\n",
      "176/223, train_loss: 0.0982, step time: 0.1102\n",
      "177/223, train_loss: 0.0961, step time: 0.1147\n",
      "178/223, train_loss: 0.0925, step time: 0.1249\n",
      "179/223, train_loss: 0.0960, step time: 0.1081\n",
      "180/223, train_loss: 0.0944, step time: 0.1057\n",
      "181/223, train_loss: 0.0951, step time: 0.1158\n",
      "182/223, train_loss: 0.0991, step time: 0.1080\n",
      "183/223, train_loss: 0.1036, step time: 0.1371\n",
      "184/223, train_loss: 0.1010, step time: 0.1225\n",
      "185/223, train_loss: 0.1028, step time: 0.1114\n",
      "186/223, train_loss: 0.0990, step time: 0.1114\n",
      "187/223, train_loss: 0.0991, step time: 0.1927\n",
      "188/223, train_loss: 0.0936, step time: 0.0993\n",
      "189/223, train_loss: 0.1014, step time: 0.1084\n",
      "190/223, train_loss: 0.0916, step time: 0.1005\n",
      "191/223, train_loss: 0.0927, step time: 0.1006\n",
      "192/223, train_loss: 0.0971, step time: 0.1033\n",
      "193/223, train_loss: 0.0976, step time: 0.1120\n",
      "194/223, train_loss: 0.1049, step time: 0.0996\n",
      "195/223, train_loss: 0.0971, step time: 0.1142\n",
      "196/223, train_loss: 0.1034, step time: 0.1092\n",
      "197/223, train_loss: 0.0958, step time: 0.1050\n",
      "198/223, train_loss: 0.0994, step time: 0.1495\n",
      "199/223, train_loss: 0.0844, step time: 0.0999\n",
      "200/223, train_loss: 0.1004, step time: 0.1044\n",
      "201/223, train_loss: 0.1095, step time: 0.1079\n",
      "202/223, train_loss: 0.0891, step time: 0.1169\n",
      "203/223, train_loss: 0.1106, step time: 0.1027\n",
      "204/223, train_loss: 0.1070, step time: 0.1293\n",
      "205/223, train_loss: 0.1030, step time: 0.1199\n",
      "206/223, train_loss: 0.0980, step time: 0.1076\n",
      "207/223, train_loss: 0.1009, step time: 0.1181\n",
      "208/223, train_loss: 0.0881, step time: 0.1415\n",
      "209/223, train_loss: 0.1068, step time: 0.1125\n",
      "210/223, train_loss: 0.0998, step time: 0.1087\n",
      "211/223, train_loss: 0.0895, step time: 0.1120\n",
      "212/223, train_loss: 0.0899, step time: 0.1020\n",
      "213/223, train_loss: 0.0975, step time: 0.1030\n",
      "214/223, train_loss: 0.0945, step time: 0.1114\n",
      "215/223, train_loss: 0.0937, step time: 0.0988\n",
      "216/223, train_loss: 0.0946, step time: 0.1189\n",
      "217/223, train_loss: 0.0898, step time: 0.0992\n",
      "218/223, train_loss: 0.0903, step time: 0.0987\n",
      "219/223, train_loss: 0.1062, step time: 0.0996\n",
      "220/223, train_loss: 0.1091, step time: 0.1443\n",
      "221/223, train_loss: 0.0929, step time: 0.1000\n",
      "222/223, train_loss: 0.1018, step time: 0.0994\n",
      "223/223, train_loss: 0.1008, step time: 0.0997\n",
      "epoch 281 average loss: 0.0994\n",
      "time consuming of epoch 281 is: 89.4638\n",
      "----------\n",
      "epoch 282/300\n",
      "1/223, train_loss: 0.1003, step time: 0.1133\n",
      "2/223, train_loss: 0.1049, step time: 0.1056\n",
      "3/223, train_loss: 0.1135, step time: 0.0995\n",
      "4/223, train_loss: 0.1025, step time: 0.1202\n",
      "5/223, train_loss: 0.0963, step time: 0.1083\n",
      "6/223, train_loss: 0.0936, step time: 0.1377\n",
      "7/223, train_loss: 0.1007, step time: 0.1001\n",
      "8/223, train_loss: 0.0976, step time: 0.1364\n",
      "9/223, train_loss: 0.1048, step time: 0.1206\n",
      "10/223, train_loss: 0.0980, step time: 0.1096\n",
      "11/223, train_loss: 0.1021, step time: 0.1099\n",
      "12/223, train_loss: 0.0990, step time: 0.1138\n",
      "13/223, train_loss: 0.1041, step time: 0.1106\n",
      "14/223, train_loss: 0.0965, step time: 0.1065\n",
      "15/223, train_loss: 0.1021, step time: 0.1137\n",
      "16/223, train_loss: 0.1067, step time: 0.1275\n",
      "17/223, train_loss: 0.0961, step time: 0.1137\n",
      "18/223, train_loss: 0.1063, step time: 0.1063\n",
      "19/223, train_loss: 0.1012, step time: 0.1557\n",
      "20/223, train_loss: 0.0951, step time: 0.1002\n",
      "21/223, train_loss: 0.0935, step time: 0.1016\n",
      "22/223, train_loss: 0.1028, step time: 0.1020\n",
      "23/223, train_loss: 0.0976, step time: 0.1008\n",
      "24/223, train_loss: 0.0925, step time: 0.1016\n",
      "25/223, train_loss: 0.1001, step time: 0.1008\n",
      "26/223, train_loss: 0.0924, step time: 0.1207\n",
      "27/223, train_loss: 0.0989, step time: 0.1000\n",
      "28/223, train_loss: 0.1051, step time: 0.1007\n",
      "29/223, train_loss: 0.0986, step time: 0.1010\n",
      "30/223, train_loss: 0.0968, step time: 0.1024\n",
      "31/223, train_loss: 0.1025, step time: 0.1197\n",
      "32/223, train_loss: 0.0932, step time: 0.1081\n",
      "33/223, train_loss: 0.0874, step time: 0.1009\n",
      "34/223, train_loss: 0.0987, step time: 0.1100\n",
      "35/223, train_loss: 0.0957, step time: 0.1240\n",
      "36/223, train_loss: 0.1033, step time: 0.1138\n",
      "37/223, train_loss: 0.1019, step time: 0.1011\n",
      "38/223, train_loss: 0.0939, step time: 0.1065\n",
      "39/223, train_loss: 0.0925, step time: 0.1190\n",
      "40/223, train_loss: 0.0973, step time: 0.1000\n",
      "41/223, train_loss: 0.0929, step time: 0.1084\n",
      "42/223, train_loss: 0.0981, step time: 0.1006\n",
      "43/223, train_loss: 0.1046, step time: 0.1199\n",
      "44/223, train_loss: 0.0958, step time: 0.1189\n",
      "45/223, train_loss: 0.0863, step time: 0.1199\n",
      "46/223, train_loss: 0.0993, step time: 0.1115\n",
      "47/223, train_loss: 0.0944, step time: 0.1268\n",
      "48/223, train_loss: 0.0933, step time: 0.0988\n",
      "49/223, train_loss: 0.0947, step time: 0.1037\n",
      "50/223, train_loss: 0.0946, step time: 0.1031\n",
      "51/223, train_loss: 0.1048, step time: 0.1052\n",
      "52/223, train_loss: 0.0975, step time: 0.1023\n",
      "53/223, train_loss: 0.1063, step time: 0.1067\n",
      "54/223, train_loss: 0.1031, step time: 0.1089\n",
      "55/223, train_loss: 0.0955, step time: 0.1059\n",
      "56/223, train_loss: 0.0871, step time: 0.1181\n",
      "57/223, train_loss: 0.1105, step time: 0.1083\n",
      "58/223, train_loss: 0.0982, step time: 0.1450\n",
      "59/223, train_loss: 0.0963, step time: 0.1117\n",
      "60/223, train_loss: 0.1067, step time: 0.1169\n",
      "61/223, train_loss: 0.1085, step time: 0.1002\n",
      "62/223, train_loss: 0.1066, step time: 0.1021\n",
      "63/223, train_loss: 0.1019, step time: 0.1205\n",
      "64/223, train_loss: 0.0921, step time: 0.1058\n",
      "65/223, train_loss: 0.0949, step time: 0.1071\n",
      "66/223, train_loss: 0.0996, step time: 0.1006\n",
      "67/223, train_loss: 0.0922, step time: 0.1054\n",
      "68/223, train_loss: 0.0950, step time: 0.1139\n",
      "69/223, train_loss: 0.0931, step time: 0.1127\n",
      "70/223, train_loss: 0.0910, step time: 0.1003\n",
      "71/223, train_loss: 0.1138, step time: 0.1257\n",
      "72/223, train_loss: 0.0979, step time: 0.1206\n",
      "73/223, train_loss: 0.1031, step time: 0.1504\n",
      "74/223, train_loss: 0.0998, step time: 0.1302\n",
      "75/223, train_loss: 0.0909, step time: 0.1002\n",
      "76/223, train_loss: 0.0976, step time: 0.1009\n",
      "77/223, train_loss: 0.1039, step time: 0.1110\n",
      "78/223, train_loss: 0.1027, step time: 0.1161\n",
      "79/223, train_loss: 0.0872, step time: 0.0999\n",
      "80/223, train_loss: 0.1029, step time: 0.1119\n",
      "81/223, train_loss: 0.0936, step time: 0.1199\n",
      "82/223, train_loss: 0.0877, step time: 0.1110\n",
      "83/223, train_loss: 0.0898, step time: 0.1166\n",
      "84/223, train_loss: 0.0964, step time: 0.1021\n",
      "85/223, train_loss: 0.0881, step time: 0.1184\n",
      "86/223, train_loss: 0.0888, step time: 0.0995\n",
      "87/223, train_loss: 0.0884, step time: 0.1158\n",
      "88/223, train_loss: 0.0961, step time: 0.1247\n",
      "89/223, train_loss: 0.0997, step time: 0.1045\n",
      "90/223, train_loss: 0.0949, step time: 0.1045\n",
      "91/223, train_loss: 0.0997, step time: 0.1132\n",
      "92/223, train_loss: 0.0928, step time: 0.1219\n",
      "93/223, train_loss: 0.0962, step time: 0.1259\n",
      "94/223, train_loss: 0.0957, step time: 0.1228\n",
      "95/223, train_loss: 0.1015, step time: 0.1091\n",
      "96/223, train_loss: 0.0878, step time: 0.0991\n",
      "97/223, train_loss: 0.0942, step time: 0.0992\n",
      "98/223, train_loss: 0.1060, step time: 0.1088\n",
      "99/223, train_loss: 0.1068, step time: 0.1194\n",
      "100/223, train_loss: 0.1031, step time: 0.1047\n",
      "101/223, train_loss: 0.0935, step time: 0.1090\n",
      "102/223, train_loss: 0.0977, step time: 0.1137\n",
      "103/223, train_loss: 0.1009, step time: 0.1014\n",
      "104/223, train_loss: 0.0958, step time: 0.1165\n",
      "105/223, train_loss: 0.0892, step time: 0.1211\n",
      "106/223, train_loss: 0.0945, step time: 0.1125\n",
      "107/223, train_loss: 0.0914, step time: 0.1128\n",
      "108/223, train_loss: 0.1106, step time: 0.1090\n",
      "109/223, train_loss: 0.0883, step time: 0.1023\n",
      "110/223, train_loss: 0.0992, step time: 0.1010\n",
      "111/223, train_loss: 0.1094, step time: 0.1156\n",
      "112/223, train_loss: 0.0921, step time: 0.1120\n",
      "113/223, train_loss: 0.0912, step time: 0.1023\n",
      "114/223, train_loss: 0.0988, step time: 0.1014\n",
      "115/223, train_loss: 0.0965, step time: 0.1086\n",
      "116/223, train_loss: 0.1083, step time: 0.1056\n",
      "117/223, train_loss: 0.0895, step time: 0.1176\n",
      "118/223, train_loss: 0.1045, step time: 0.0998\n",
      "119/223, train_loss: 0.0901, step time: 0.1040\n",
      "120/223, train_loss: 0.0999, step time: 0.1100\n",
      "121/223, train_loss: 0.0946, step time: 0.1051\n",
      "122/223, train_loss: 0.0884, step time: 0.1008\n",
      "123/223, train_loss: 0.1069, step time: 0.1037\n",
      "124/223, train_loss: 0.0925, step time: 0.0993\n",
      "125/223, train_loss: 0.0953, step time: 0.0993\n",
      "126/223, train_loss: 0.0963, step time: 0.1025\n",
      "127/223, train_loss: 0.0940, step time: 0.1091\n",
      "128/223, train_loss: 0.1057, step time: 0.1100\n",
      "129/223, train_loss: 0.0962, step time: 0.1021\n",
      "130/223, train_loss: 0.0937, step time: 0.1022\n",
      "131/223, train_loss: 0.0986, step time: 0.1093\n",
      "132/223, train_loss: 0.1045, step time: 0.1016\n",
      "133/223, train_loss: 0.2917, step time: 0.1013\n",
      "134/223, train_loss: 0.1019, step time: 0.1005\n",
      "135/223, train_loss: 0.0975, step time: 0.1053\n",
      "136/223, train_loss: 0.1116, step time: 0.1110\n",
      "137/223, train_loss: 0.0994, step time: 0.0988\n",
      "138/223, train_loss: 0.0908, step time: 0.1167\n",
      "139/223, train_loss: 0.0912, step time: 0.1097\n",
      "140/223, train_loss: 0.0996, step time: 0.1244\n",
      "141/223, train_loss: 0.0991, step time: 0.1026\n",
      "142/223, train_loss: 0.1126, step time: 0.1141\n",
      "143/223, train_loss: 0.0936, step time: 0.1313\n",
      "144/223, train_loss: 0.1006, step time: 0.1079\n",
      "145/223, train_loss: 0.0972, step time: 0.1020\n",
      "146/223, train_loss: 0.1106, step time: 0.1150\n",
      "147/223, train_loss: 0.0876, step time: 0.1251\n",
      "148/223, train_loss: 0.1017, step time: 0.1421\n",
      "149/223, train_loss: 0.1038, step time: 0.1224\n",
      "150/223, train_loss: 0.1046, step time: 0.1013\n",
      "151/223, train_loss: 0.0984, step time: 0.1005\n",
      "152/223, train_loss: 0.0979, step time: 0.0999\n",
      "153/223, train_loss: 0.0944, step time: 0.1006\n",
      "154/223, train_loss: 0.0980, step time: 0.1016\n",
      "155/223, train_loss: 0.0984, step time: 0.1039\n",
      "156/223, train_loss: 0.0965, step time: 0.1001\n",
      "157/223, train_loss: 0.1006, step time: 0.1015\n",
      "158/223, train_loss: 0.0925, step time: 0.1012\n",
      "159/223, train_loss: 0.0942, step time: 0.1011\n",
      "160/223, train_loss: 0.1134, step time: 0.1295\n",
      "161/223, train_loss: 0.0992, step time: 0.1212\n",
      "162/223, train_loss: 0.0985, step time: 0.1020\n",
      "163/223, train_loss: 0.0985, step time: 0.1069\n",
      "164/223, train_loss: 0.1083, step time: 0.1004\n",
      "165/223, train_loss: 0.1025, step time: 0.1073\n",
      "166/223, train_loss: 0.0915, step time: 0.1321\n",
      "167/223, train_loss: 0.1002, step time: 0.1078\n",
      "168/223, train_loss: 0.1083, step time: 0.1138\n",
      "169/223, train_loss: 0.0999, step time: 0.1129\n",
      "170/223, train_loss: 0.1045, step time: 0.1096\n",
      "171/223, train_loss: 0.1024, step time: 0.1133\n",
      "172/223, train_loss: 0.0868, step time: 0.1257\n",
      "173/223, train_loss: 0.1045, step time: 0.1277\n",
      "174/223, train_loss: 0.1088, step time: 0.2007\n",
      "175/223, train_loss: 0.1040, step time: 0.1012\n",
      "176/223, train_loss: 0.0973, step time: 0.1127\n",
      "177/223, train_loss: 0.0997, step time: 0.1201\n",
      "178/223, train_loss: 0.0939, step time: 0.1338\n",
      "179/223, train_loss: 0.1051, step time: 0.1335\n",
      "180/223, train_loss: 0.1043, step time: 0.1100\n",
      "181/223, train_loss: 0.0965, step time: 0.1243\n",
      "182/223, train_loss: 0.1004, step time: 0.1177\n",
      "183/223, train_loss: 0.1046, step time: 0.1013\n",
      "184/223, train_loss: 0.1037, step time: 0.1202\n",
      "185/223, train_loss: 0.0917, step time: 0.1162\n",
      "186/223, train_loss: 0.0914, step time: 0.1136\n",
      "187/223, train_loss: 0.1101, step time: 0.1116\n",
      "188/223, train_loss: 0.0938, step time: 0.1050\n",
      "189/223, train_loss: 0.0988, step time: 0.1213\n",
      "190/223, train_loss: 0.0875, step time: 0.1096\n",
      "191/223, train_loss: 0.0947, step time: 0.1052\n",
      "192/223, train_loss: 0.1133, step time: 0.1038\n",
      "193/223, train_loss: 0.0929, step time: 0.1051\n",
      "194/223, train_loss: 0.0939, step time: 0.1153\n",
      "195/223, train_loss: 0.1115, step time: 0.1188\n",
      "196/223, train_loss: 0.1107, step time: 0.1442\n",
      "197/223, train_loss: 0.0983, step time: 0.1194\n",
      "198/223, train_loss: 0.0905, step time: 0.1158\n",
      "199/223, train_loss: 0.0923, step time: 0.1104\n",
      "200/223, train_loss: 0.1007, step time: 0.1150\n",
      "201/223, train_loss: 0.0995, step time: 0.1126\n",
      "202/223, train_loss: 0.0925, step time: 0.1095\n",
      "203/223, train_loss: 0.0986, step time: 0.0997\n",
      "204/223, train_loss: 0.1056, step time: 0.1006\n",
      "205/223, train_loss: 0.1098, step time: 0.1066\n",
      "206/223, train_loss: 0.1051, step time: 0.1045\n",
      "207/223, train_loss: 0.0953, step time: 0.1008\n",
      "208/223, train_loss: 0.0914, step time: 0.1146\n",
      "209/223, train_loss: 0.1053, step time: 0.1000\n",
      "210/223, train_loss: 0.1070, step time: 0.1078\n",
      "211/223, train_loss: 0.0997, step time: 0.1001\n",
      "212/223, train_loss: 0.1129, step time: 0.1121\n",
      "213/223, train_loss: 0.0948, step time: 0.1065\n",
      "214/223, train_loss: 0.0985, step time: 0.1084\n",
      "215/223, train_loss: 0.1024, step time: 0.1057\n",
      "216/223, train_loss: 0.0975, step time: 0.1097\n",
      "217/223, train_loss: 0.1053, step time: 0.1170\n",
      "218/223, train_loss: 0.0878, step time: 0.1000\n",
      "219/223, train_loss: 0.0950, step time: 0.0999\n",
      "220/223, train_loss: 0.0924, step time: 0.0997\n",
      "221/223, train_loss: 0.1055, step time: 0.1003\n",
      "222/223, train_loss: 0.0962, step time: 0.0986\n",
      "223/223, train_loss: 0.0981, step time: 0.0986\n",
      "epoch 282 average loss: 0.0995\n",
      "time consuming of epoch 282 is: 91.6350\n",
      "----------\n",
      "epoch 283/300\n",
      "1/223, train_loss: 0.0998, step time: 0.1044\n",
      "2/223, train_loss: 0.1020, step time: 0.1086\n",
      "3/223, train_loss: 0.1011, step time: 0.1182\n",
      "4/223, train_loss: 0.0973, step time: 0.1420\n",
      "5/223, train_loss: 0.0984, step time: 0.1133\n",
      "6/223, train_loss: 0.0953, step time: 0.1099\n",
      "7/223, train_loss: 0.0922, step time: 0.1091\n",
      "8/223, train_loss: 0.0996, step time: 0.1241\n",
      "9/223, train_loss: 0.1050, step time: 0.1261\n",
      "10/223, train_loss: 0.1054, step time: 0.1201\n",
      "11/223, train_loss: 0.0954, step time: 0.1064\n",
      "12/223, train_loss: 0.1006, step time: 0.1071\n",
      "13/223, train_loss: 0.1061, step time: 0.1088\n",
      "14/223, train_loss: 0.0936, step time: 0.1131\n",
      "15/223, train_loss: 0.1027, step time: 0.1275\n",
      "16/223, train_loss: 0.1048, step time: 0.1050\n",
      "17/223, train_loss: 0.1192, step time: 0.1173\n",
      "18/223, train_loss: 0.0918, step time: 0.1161\n",
      "19/223, train_loss: 0.0951, step time: 0.1348\n",
      "20/223, train_loss: 0.0983, step time: 0.1234\n",
      "21/223, train_loss: 0.0927, step time: 0.1162\n",
      "22/223, train_loss: 0.0906, step time: 0.1095\n",
      "23/223, train_loss: 0.1166, step time: 0.1087\n",
      "24/223, train_loss: 0.1092, step time: 0.0996\n",
      "25/223, train_loss: 0.0939, step time: 0.1178\n",
      "26/223, train_loss: 0.0954, step time: 0.1285\n",
      "27/223, train_loss: 0.1027, step time: 0.1025\n",
      "28/223, train_loss: 0.1070, step time: 0.1085\n",
      "29/223, train_loss: 0.1035, step time: 0.1150\n",
      "30/223, train_loss: 0.0990, step time: 0.1261\n",
      "31/223, train_loss: 0.0929, step time: 0.1098\n",
      "32/223, train_loss: 0.1004, step time: 0.1004\n",
      "33/223, train_loss: 0.0996, step time: 0.1082\n",
      "34/223, train_loss: 0.0906, step time: 0.1109\n",
      "35/223, train_loss: 0.0960, step time: 0.1087\n",
      "36/223, train_loss: 0.0906, step time: 0.1052\n",
      "37/223, train_loss: 0.1022, step time: 0.1059\n",
      "38/223, train_loss: 0.0984, step time: 0.0998\n",
      "39/223, train_loss: 0.0929, step time: 0.1163\n",
      "40/223, train_loss: 0.0980, step time: 0.1002\n",
      "41/223, train_loss: 0.0951, step time: 0.1006\n",
      "42/223, train_loss: 0.0881, step time: 0.0999\n",
      "43/223, train_loss: 0.0912, step time: 0.1216\n",
      "44/223, train_loss: 0.0995, step time: 0.1089\n",
      "45/223, train_loss: 0.1083, step time: 0.1005\n",
      "46/223, train_loss: 0.1053, step time: 0.1009\n",
      "47/223, train_loss: 0.0995, step time: 0.1097\n",
      "48/223, train_loss: 0.0975, step time: 0.1003\n",
      "49/223, train_loss: 0.1027, step time: 0.1010\n",
      "50/223, train_loss: 0.1018, step time: 0.1436\n",
      "51/223, train_loss: 0.0903, step time: 0.1201\n",
      "52/223, train_loss: 0.2936, step time: 0.1168\n",
      "53/223, train_loss: 0.1046, step time: 0.1174\n",
      "54/223, train_loss: 0.0927, step time: 0.1003\n",
      "55/223, train_loss: 0.0978, step time: 0.1161\n",
      "56/223, train_loss: 0.0937, step time: 0.1008\n",
      "57/223, train_loss: 0.1007, step time: 0.1246\n",
      "58/223, train_loss: 0.0967, step time: 0.1094\n",
      "59/223, train_loss: 0.1086, step time: 0.1182\n",
      "60/223, train_loss: 0.0906, step time: 0.1589\n",
      "61/223, train_loss: 0.0907, step time: 0.1269\n",
      "62/223, train_loss: 0.1097, step time: 0.1043\n",
      "63/223, train_loss: 0.1058, step time: 0.1034\n",
      "64/223, train_loss: 0.0918, step time: 0.1004\n",
      "65/223, train_loss: 0.0891, step time: 0.1145\n",
      "66/223, train_loss: 0.0956, step time: 0.1108\n",
      "67/223, train_loss: 0.0961, step time: 0.1102\n",
      "68/223, train_loss: 0.0958, step time: 0.1022\n",
      "69/223, train_loss: 0.1037, step time: 0.1048\n",
      "70/223, train_loss: 0.0989, step time: 0.1052\n",
      "71/223, train_loss: 0.0879, step time: 0.1237\n",
      "72/223, train_loss: 0.1063, step time: 0.1131\n",
      "73/223, train_loss: 0.1034, step time: 0.1195\n",
      "74/223, train_loss: 0.0985, step time: 0.1004\n",
      "75/223, train_loss: 0.1077, step time: 0.1132\n",
      "76/223, train_loss: 0.1072, step time: 0.1049\n",
      "77/223, train_loss: 0.1187, step time: 0.1169\n",
      "78/223, train_loss: 0.0957, step time: 0.1021\n",
      "79/223, train_loss: 0.0959, step time: 0.1143\n",
      "80/223, train_loss: 0.0999, step time: 0.1138\n",
      "81/223, train_loss: 0.0971, step time: 0.1029\n",
      "82/223, train_loss: 0.0905, step time: 0.1023\n",
      "83/223, train_loss: 0.0996, step time: 0.1234\n",
      "84/223, train_loss: 0.1088, step time: 0.1119\n",
      "85/223, train_loss: 0.0970, step time: 0.1174\n",
      "86/223, train_loss: 0.0918, step time: 0.1102\n",
      "87/223, train_loss: 0.1010, step time: 0.1125\n",
      "88/223, train_loss: 0.1096, step time: 0.1012\n",
      "89/223, train_loss: 0.0981, step time: 0.1326\n",
      "90/223, train_loss: 0.1056, step time: 0.1143\n",
      "91/223, train_loss: 0.1054, step time: 0.1029\n",
      "92/223, train_loss: 0.0957, step time: 0.1042\n",
      "93/223, train_loss: 0.0924, step time: 0.1155\n",
      "94/223, train_loss: 0.0923, step time: 0.1141\n",
      "95/223, train_loss: 0.1192, step time: 0.1081\n",
      "96/223, train_loss: 0.0913, step time: 0.1378\n",
      "97/223, train_loss: 0.0865, step time: 0.1116\n",
      "98/223, train_loss: 0.0963, step time: 0.1151\n",
      "99/223, train_loss: 0.0954, step time: 0.1137\n",
      "100/223, train_loss: 0.1206, step time: 0.1205\n",
      "101/223, train_loss: 0.1001, step time: 0.1266\n",
      "102/223, train_loss: 0.0929, step time: 0.1189\n",
      "103/223, train_loss: 0.0889, step time: 0.1073\n",
      "104/223, train_loss: 0.0969, step time: 0.1055\n",
      "105/223, train_loss: 0.1130, step time: 0.1003\n",
      "106/223, train_loss: 0.0921, step time: 0.1098\n",
      "107/223, train_loss: 0.0921, step time: 0.1093\n",
      "108/223, train_loss: 0.0985, step time: 0.1035\n",
      "109/223, train_loss: 0.0843, step time: 0.1135\n",
      "110/223, train_loss: 0.0998, step time: 0.1095\n",
      "111/223, train_loss: 0.0949, step time: 0.1014\n",
      "112/223, train_loss: 0.1071, step time: 0.1237\n",
      "113/223, train_loss: 0.1039, step time: 0.1124\n",
      "114/223, train_loss: 0.1015, step time: 0.1002\n",
      "115/223, train_loss: 0.1072, step time: 0.1009\n",
      "116/223, train_loss: 0.0917, step time: 0.1432\n",
      "117/223, train_loss: 0.1041, step time: 0.1104\n",
      "118/223, train_loss: 0.0987, step time: 0.1171\n",
      "119/223, train_loss: 0.1059, step time: 0.1188\n",
      "120/223, train_loss: 0.1020, step time: 0.1066\n",
      "121/223, train_loss: 0.1016, step time: 0.1078\n",
      "122/223, train_loss: 0.0896, step time: 0.1397\n",
      "123/223, train_loss: 0.0880, step time: 0.1006\n",
      "124/223, train_loss: 0.0976, step time: 0.1046\n",
      "125/223, train_loss: 0.0994, step time: 0.1479\n",
      "126/223, train_loss: 0.0990, step time: 0.1000\n",
      "127/223, train_loss: 0.0998, step time: 0.1018\n",
      "128/223, train_loss: 0.0970, step time: 0.1004\n",
      "129/223, train_loss: 0.0924, step time: 0.1169\n",
      "130/223, train_loss: 0.0967, step time: 0.1155\n",
      "131/223, train_loss: 0.0949, step time: 0.1053\n",
      "132/223, train_loss: 0.0967, step time: 0.1200\n",
      "133/223, train_loss: 0.1073, step time: 0.1027\n",
      "134/223, train_loss: 0.1030, step time: 0.1150\n",
      "135/223, train_loss: 0.1069, step time: 0.1151\n",
      "136/223, train_loss: 0.1091, step time: 0.1011\n",
      "137/223, train_loss: 0.1043, step time: 0.1450\n",
      "138/223, train_loss: 0.1153, step time: 0.1145\n",
      "139/223, train_loss: 0.1010, step time: 0.1141\n",
      "140/223, train_loss: 0.0981, step time: 0.1143\n",
      "141/223, train_loss: 0.1089, step time: 0.1174\n",
      "142/223, train_loss: 0.0943, step time: 0.1147\n",
      "143/223, train_loss: 0.0926, step time: 0.1005\n",
      "144/223, train_loss: 0.0975, step time: 0.1223\n",
      "145/223, train_loss: 0.1012, step time: 0.1108\n",
      "146/223, train_loss: 0.0933, step time: 0.1024\n",
      "147/223, train_loss: 0.0940, step time: 0.1044\n",
      "148/223, train_loss: 0.0932, step time: 0.1113\n",
      "149/223, train_loss: 0.0929, step time: 0.1114\n",
      "150/223, train_loss: 0.0937, step time: 0.1010\n",
      "151/223, train_loss: 0.1029, step time: 0.1009\n",
      "152/223, train_loss: 0.1067, step time: 0.1018\n",
      "153/223, train_loss: 0.0987, step time: 0.1085\n",
      "154/223, train_loss: 0.1039, step time: 0.1155\n",
      "155/223, train_loss: 0.0976, step time: 0.1111\n",
      "156/223, train_loss: 0.0996, step time: 0.1358\n",
      "157/223, train_loss: 0.0985, step time: 0.1051\n",
      "158/223, train_loss: 0.0928, step time: 0.1108\n",
      "159/223, train_loss: 0.0988, step time: 0.1265\n",
      "160/223, train_loss: 0.0877, step time: 0.1000\n",
      "161/223, train_loss: 0.1078, step time: 0.1108\n",
      "162/223, train_loss: 0.0898, step time: 0.1338\n",
      "163/223, train_loss: 0.1006, step time: 0.1170\n",
      "164/223, train_loss: 0.1090, step time: 0.1123\n",
      "165/223, train_loss: 0.1037, step time: 0.1150\n",
      "166/223, train_loss: 0.0912, step time: 0.1004\n",
      "167/223, train_loss: 0.0959, step time: 0.1126\n",
      "168/223, train_loss: 0.0962, step time: 0.1123\n",
      "169/223, train_loss: 0.1067, step time: 0.1233\n",
      "170/223, train_loss: 0.0952, step time: 0.1098\n",
      "171/223, train_loss: 0.0943, step time: 0.1364\n",
      "172/223, train_loss: 0.1006, step time: 0.1080\n",
      "173/223, train_loss: 0.0924, step time: 0.1089\n",
      "174/223, train_loss: 0.0875, step time: 0.1427\n",
      "175/223, train_loss: 0.0990, step time: 0.1268\n",
      "176/223, train_loss: 0.0995, step time: 0.1252\n",
      "177/223, train_loss: 0.1039, step time: 0.1089\n",
      "178/223, train_loss: 0.1001, step time: 0.1073\n",
      "179/223, train_loss: 0.1023, step time: 0.1180\n",
      "180/223, train_loss: 0.1030, step time: 0.1024\n",
      "181/223, train_loss: 0.0909, step time: 0.1300\n",
      "182/223, train_loss: 0.0872, step time: 0.1094\n",
      "183/223, train_loss: 0.0957, step time: 0.1215\n",
      "184/223, train_loss: 0.1040, step time: 0.1077\n",
      "185/223, train_loss: 0.0910, step time: 0.1114\n",
      "186/223, train_loss: 0.0897, step time: 0.1117\n",
      "187/223, train_loss: 0.1035, step time: 0.1129\n",
      "188/223, train_loss: 0.0883, step time: 0.1043\n",
      "189/223, train_loss: 0.0992, step time: 0.1087\n",
      "190/223, train_loss: 0.0944, step time: 0.1148\n",
      "191/223, train_loss: 0.0984, step time: 0.1136\n",
      "192/223, train_loss: 0.0881, step time: 0.1151\n",
      "193/223, train_loss: 0.0964, step time: 0.1395\n",
      "194/223, train_loss: 0.1056, step time: 0.1132\n",
      "195/223, train_loss: 0.0929, step time: 0.1263\n",
      "196/223, train_loss: 0.0905, step time: 0.1235\n",
      "197/223, train_loss: 0.1042, step time: 0.1135\n",
      "198/223, train_loss: 0.1050, step time: 0.1066\n",
      "199/223, train_loss: 0.0984, step time: 0.1064\n",
      "200/223, train_loss: 0.1015, step time: 0.1033\n",
      "201/223, train_loss: 0.0897, step time: 0.0997\n",
      "202/223, train_loss: 0.1033, step time: 0.0986\n",
      "203/223, train_loss: 0.0912, step time: 0.0994\n",
      "204/223, train_loss: 0.1043, step time: 0.0994\n",
      "205/223, train_loss: 0.1032, step time: 0.0991\n",
      "206/223, train_loss: 0.0965, step time: 0.0999\n",
      "207/223, train_loss: 0.0984, step time: 0.1009\n",
      "208/223, train_loss: 0.0965, step time: 0.1039\n",
      "209/223, train_loss: 0.1022, step time: 0.0998\n",
      "210/223, train_loss: 0.0950, step time: 0.0998\n",
      "211/223, train_loss: 0.0957, step time: 0.0997\n",
      "212/223, train_loss: 0.1012, step time: 0.0997\n",
      "213/223, train_loss: 0.0879, step time: 0.1089\n",
      "214/223, train_loss: 0.0870, step time: 0.1047\n",
      "215/223, train_loss: 0.1023, step time: 0.1101\n",
      "216/223, train_loss: 0.1037, step time: 0.1127\n",
      "217/223, train_loss: 0.0960, step time: 0.1003\n",
      "218/223, train_loss: 0.1018, step time: 0.1315\n",
      "219/223, train_loss: 0.1016, step time: 0.1003\n",
      "220/223, train_loss: 0.1060, step time: 0.1114\n",
      "221/223, train_loss: 0.0974, step time: 0.1002\n",
      "222/223, train_loss: 0.0976, step time: 0.0997\n",
      "223/223, train_loss: 0.0870, step time: 0.1003\n",
      "epoch 283 average loss: 0.0995\n",
      "time consuming of epoch 283 is: 92.0138\n",
      "----------\n",
      "epoch 284/300\n",
      "1/223, train_loss: 0.1015, step time: 0.1208\n",
      "2/223, train_loss: 0.0886, step time: 0.1196\n",
      "3/223, train_loss: 0.1100, step time: 0.1246\n",
      "4/223, train_loss: 0.0954, step time: 0.1123\n",
      "5/223, train_loss: 0.1021, step time: 0.1126\n",
      "6/223, train_loss: 0.0999, step time: 0.1054\n",
      "7/223, train_loss: 0.1114, step time: 0.1284\n",
      "8/223, train_loss: 0.0898, step time: 0.1257\n",
      "9/223, train_loss: 0.1006, step time: 0.0996\n",
      "10/223, train_loss: 0.0944, step time: 0.1164\n",
      "11/223, train_loss: 0.0987, step time: 0.1164\n",
      "12/223, train_loss: 0.0938, step time: 0.1260\n",
      "13/223, train_loss: 0.1005, step time: 0.1156\n",
      "14/223, train_loss: 0.1009, step time: 0.1380\n",
      "15/223, train_loss: 0.1020, step time: 0.1355\n",
      "16/223, train_loss: 0.0851, step time: 0.1218\n",
      "17/223, train_loss: 0.0996, step time: 0.1007\n",
      "18/223, train_loss: 0.1030, step time: 0.1362\n",
      "19/223, train_loss: 0.1020, step time: 0.1380\n",
      "20/223, train_loss: 0.1135, step time: 0.1364\n",
      "21/223, train_loss: 0.1017, step time: 0.1352\n",
      "22/223, train_loss: 0.1011, step time: 0.1103\n",
      "23/223, train_loss: 0.1046, step time: 0.0999\n",
      "24/223, train_loss: 0.0996, step time: 0.1007\n",
      "25/223, train_loss: 0.1010, step time: 0.1008\n",
      "26/223, train_loss: 0.1030, step time: 0.1293\n",
      "27/223, train_loss: 0.0916, step time: 0.1252\n",
      "28/223, train_loss: 0.1045, step time: 0.1111\n",
      "29/223, train_loss: 0.0873, step time: 0.1168\n",
      "30/223, train_loss: 0.1048, step time: 0.1061\n",
      "31/223, train_loss: 0.1056, step time: 0.1174\n",
      "32/223, train_loss: 0.0910, step time: 0.1006\n",
      "33/223, train_loss: 0.0973, step time: 0.1090\n",
      "34/223, train_loss: 0.0938, step time: 0.1165\n",
      "35/223, train_loss: 0.1096, step time: 0.1044\n",
      "36/223, train_loss: 0.1000, step time: 0.1271\n",
      "37/223, train_loss: 0.0966, step time: 0.1000\n",
      "38/223, train_loss: 0.0968, step time: 0.1259\n",
      "39/223, train_loss: 0.1035, step time: 0.1118\n",
      "40/223, train_loss: 0.1013, step time: 0.1114\n",
      "41/223, train_loss: 0.0903, step time: 0.1077\n",
      "42/223, train_loss: 0.0931, step time: 0.1105\n",
      "43/223, train_loss: 0.1098, step time: 0.1081\n",
      "44/223, train_loss: 0.1050, step time: 0.1114\n",
      "45/223, train_loss: 0.1007, step time: 0.1209\n",
      "46/223, train_loss: 0.1083, step time: 0.1133\n",
      "47/223, train_loss: 0.0992, step time: 0.0993\n",
      "48/223, train_loss: 0.0901, step time: 0.1060\n",
      "49/223, train_loss: 0.1080, step time: 0.1202\n",
      "50/223, train_loss: 0.0906, step time: 0.1204\n",
      "51/223, train_loss: 0.0954, step time: 0.1095\n",
      "52/223, train_loss: 0.1019, step time: 0.1322\n",
      "53/223, train_loss: 0.0976, step time: 0.1126\n",
      "54/223, train_loss: 0.0987, step time: 0.1134\n",
      "55/223, train_loss: 0.1005, step time: 0.1439\n",
      "56/223, train_loss: 0.0899, step time: 0.1260\n",
      "57/223, train_loss: 0.0930, step time: 0.1065\n",
      "58/223, train_loss: 0.1049, step time: 0.1109\n",
      "59/223, train_loss: 0.0874, step time: 0.1159\n",
      "60/223, train_loss: 0.0970, step time: 0.1118\n",
      "61/223, train_loss: 0.1043, step time: 0.1072\n",
      "62/223, train_loss: 0.1069, step time: 0.1166\n",
      "63/223, train_loss: 0.0951, step time: 0.1004\n",
      "64/223, train_loss: 0.0917, step time: 0.1205\n",
      "65/223, train_loss: 0.0904, step time: 0.1256\n",
      "66/223, train_loss: 0.0932, step time: 0.1147\n",
      "67/223, train_loss: 0.1018, step time: 0.1128\n",
      "68/223, train_loss: 0.0954, step time: 0.1349\n",
      "69/223, train_loss: 0.1029, step time: 0.1018\n",
      "70/223, train_loss: 0.0939, step time: 0.1177\n",
      "71/223, train_loss: 0.0925, step time: 0.1097\n",
      "72/223, train_loss: 0.1009, step time: 0.1220\n",
      "73/223, train_loss: 0.0929, step time: 0.1221\n",
      "74/223, train_loss: 0.0988, step time: 0.0993\n",
      "75/223, train_loss: 0.0975, step time: 0.1000\n",
      "76/223, train_loss: 0.0939, step time: 0.1003\n",
      "77/223, train_loss: 0.1025, step time: 0.1003\n",
      "78/223, train_loss: 0.0914, step time: 0.1007\n",
      "79/223, train_loss: 0.0996, step time: 0.1032\n",
      "80/223, train_loss: 0.0871, step time: 0.1001\n",
      "81/223, train_loss: 0.1067, step time: 0.1128\n",
      "82/223, train_loss: 0.1060, step time: 0.1222\n",
      "83/223, train_loss: 0.1118, step time: 0.1303\n",
      "84/223, train_loss: 0.1003, step time: 0.1121\n",
      "85/223, train_loss: 0.0949, step time: 0.1172\n",
      "86/223, train_loss: 0.1042, step time: 0.1114\n",
      "87/223, train_loss: 0.0927, step time: 0.1249\n",
      "88/223, train_loss: 0.1046, step time: 0.1258\n",
      "89/223, train_loss: 0.0988, step time: 0.1346\n",
      "90/223, train_loss: 0.0986, step time: 0.1087\n",
      "91/223, train_loss: 0.0946, step time: 0.1013\n",
      "92/223, train_loss: 0.0952, step time: 0.1041\n",
      "93/223, train_loss: 0.0993, step time: 0.1529\n",
      "94/223, train_loss: 0.1078, step time: 0.1053\n",
      "95/223, train_loss: 0.0994, step time: 0.1002\n",
      "96/223, train_loss: 0.1072, step time: 0.1010\n",
      "97/223, train_loss: 0.0960, step time: 0.1268\n",
      "98/223, train_loss: 0.0918, step time: 0.1100\n",
      "99/223, train_loss: 0.0990, step time: 0.1003\n",
      "100/223, train_loss: 0.0959, step time: 0.1009\n",
      "101/223, train_loss: 0.0931, step time: 0.1058\n",
      "102/223, train_loss: 0.1067, step time: 0.1112\n",
      "103/223, train_loss: 0.0894, step time: 0.1098\n",
      "104/223, train_loss: 0.0980, step time: 0.1073\n",
      "105/223, train_loss: 0.1106, step time: 0.1007\n",
      "106/223, train_loss: 0.1061, step time: 0.1118\n",
      "107/223, train_loss: 0.0872, step time: 0.1004\n",
      "108/223, train_loss: 0.0888, step time: 0.1062\n",
      "109/223, train_loss: 0.1026, step time: 0.1078\n",
      "110/223, train_loss: 0.1070, step time: 0.1086\n",
      "111/223, train_loss: 0.0858, step time: 0.0994\n",
      "112/223, train_loss: 0.0925, step time: 0.1145\n",
      "113/223, train_loss: 0.0937, step time: 0.1439\n",
      "114/223, train_loss: 0.0966, step time: 0.1118\n",
      "115/223, train_loss: 0.0995, step time: 0.1182\n",
      "116/223, train_loss: 0.0937, step time: 0.1107\n",
      "117/223, train_loss: 0.0953, step time: 0.1106\n",
      "118/223, train_loss: 0.0945, step time: 0.1006\n",
      "119/223, train_loss: 0.0982, step time: 0.1006\n",
      "120/223, train_loss: 0.1018, step time: 0.1013\n",
      "121/223, train_loss: 0.0905, step time: 0.1361\n",
      "122/223, train_loss: 0.1099, step time: 0.1096\n",
      "123/223, train_loss: 0.1052, step time: 0.0999\n",
      "124/223, train_loss: 0.0910, step time: 0.1033\n",
      "125/223, train_loss: 0.0914, step time: 0.1235\n",
      "126/223, train_loss: 0.0983, step time: 0.1174\n",
      "127/223, train_loss: 0.1053, step time: 0.0994\n",
      "128/223, train_loss: 0.0971, step time: 0.1009\n",
      "129/223, train_loss: 0.0975, step time: 0.1025\n",
      "130/223, train_loss: 0.1093, step time: 0.1152\n",
      "131/223, train_loss: 0.1136, step time: 0.1171\n",
      "132/223, train_loss: 0.0913, step time: 0.1092\n",
      "133/223, train_loss: 0.0927, step time: 0.1009\n",
      "134/223, train_loss: 0.0901, step time: 0.1075\n",
      "135/223, train_loss: 0.1012, step time: 0.1066\n",
      "136/223, train_loss: 0.0976, step time: 0.1008\n",
      "137/223, train_loss: 0.1053, step time: 0.1001\n",
      "138/223, train_loss: 0.0933, step time: 0.1182\n",
      "139/223, train_loss: 0.1042, step time: 0.1280\n",
      "140/223, train_loss: 0.0911, step time: 0.1345\n",
      "141/223, train_loss: 0.1034, step time: 0.1208\n",
      "142/223, train_loss: 0.0984, step time: 0.1232\n",
      "143/223, train_loss: 0.0964, step time: 0.1053\n",
      "144/223, train_loss: 0.0981, step time: 0.1004\n",
      "145/223, train_loss: 0.0935, step time: 0.1018\n",
      "146/223, train_loss: 0.0995, step time: 0.1054\n",
      "147/223, train_loss: 0.0966, step time: 0.1165\n",
      "148/223, train_loss: 0.1016, step time: 0.1028\n",
      "149/223, train_loss: 0.0880, step time: 0.1003\n",
      "150/223, train_loss: 0.1105, step time: 0.1140\n",
      "151/223, train_loss: 0.0954, step time: 0.1007\n",
      "152/223, train_loss: 0.0968, step time: 0.1005\n",
      "153/223, train_loss: 0.1078, step time: 0.1034\n",
      "154/223, train_loss: 0.1057, step time: 0.1369\n",
      "155/223, train_loss: 0.0854, step time: 0.1023\n",
      "156/223, train_loss: 0.1091, step time: 0.1175\n",
      "157/223, train_loss: 0.1097, step time: 0.1363\n",
      "158/223, train_loss: 0.1024, step time: 0.1168\n",
      "159/223, train_loss: 0.0942, step time: 0.1093\n",
      "160/223, train_loss: 0.0967, step time: 0.1096\n",
      "161/223, train_loss: 0.0901, step time: 0.1272\n",
      "162/223, train_loss: 0.1146, step time: 0.1106\n",
      "163/223, train_loss: 0.1048, step time: 0.1077\n",
      "164/223, train_loss: 0.0955, step time: 0.1216\n",
      "165/223, train_loss: 0.1045, step time: 0.1093\n",
      "166/223, train_loss: 0.1009, step time: 0.1168\n",
      "167/223, train_loss: 0.0902, step time: 0.1072\n",
      "168/223, train_loss: 0.1028, step time: 0.1084\n",
      "169/223, train_loss: 0.0941, step time: 0.1074\n",
      "170/223, train_loss: 0.1108, step time: 0.1003\n",
      "171/223, train_loss: 0.0961, step time: 0.1176\n",
      "172/223, train_loss: 0.1002, step time: 0.1343\n",
      "173/223, train_loss: 0.0970, step time: 0.1200\n",
      "174/223, train_loss: 0.1078, step time: 0.1214\n",
      "175/223, train_loss: 0.0958, step time: 0.1073\n",
      "176/223, train_loss: 0.1057, step time: 0.1099\n",
      "177/223, train_loss: 0.0909, step time: 0.1094\n",
      "178/223, train_loss: 0.1088, step time: 0.1159\n",
      "179/223, train_loss: 0.0948, step time: 0.1118\n",
      "180/223, train_loss: 0.0965, step time: 0.1146\n",
      "181/223, train_loss: 0.0951, step time: 0.1192\n",
      "182/223, train_loss: 0.0981, step time: 0.1195\n",
      "183/223, train_loss: 0.1002, step time: 0.1384\n",
      "184/223, train_loss: 0.0900, step time: 0.1019\n",
      "185/223, train_loss: 0.0967, step time: 0.1213\n",
      "186/223, train_loss: 0.1070, step time: 0.1274\n",
      "187/223, train_loss: 0.0924, step time: 0.1199\n",
      "188/223, train_loss: 0.0905, step time: 0.1457\n",
      "189/223, train_loss: 0.0987, step time: 0.1390\n",
      "190/223, train_loss: 0.1054, step time: 0.1060\n",
      "191/223, train_loss: 0.1047, step time: 0.1004\n",
      "192/223, train_loss: 0.0969, step time: 0.1276\n",
      "193/223, train_loss: 0.0885, step time: 0.1046\n",
      "194/223, train_loss: 0.1048, step time: 0.1059\n",
      "195/223, train_loss: 0.0909, step time: 0.1058\n",
      "196/223, train_loss: 0.2910, step time: 0.1037\n",
      "197/223, train_loss: 0.0918, step time: 0.0999\n",
      "198/223, train_loss: 0.0968, step time: 0.1029\n",
      "199/223, train_loss: 0.1004, step time: 0.1101\n",
      "200/223, train_loss: 0.0986, step time: 0.1422\n",
      "201/223, train_loss: 0.1019, step time: 0.1115\n",
      "202/223, train_loss: 0.0958, step time: 0.1180\n",
      "203/223, train_loss: 0.0967, step time: 0.1069\n",
      "204/223, train_loss: 0.1069, step time: 0.1155\n",
      "205/223, train_loss: 0.1081, step time: 0.1035\n",
      "206/223, train_loss: 0.0878, step time: 0.1095\n",
      "207/223, train_loss: 0.0946, step time: 0.1167\n",
      "208/223, train_loss: 0.1045, step time: 0.1154\n",
      "209/223, train_loss: 0.1043, step time: 0.1003\n",
      "210/223, train_loss: 0.1036, step time: 0.1015\n",
      "211/223, train_loss: 0.0886, step time: 0.1274\n",
      "212/223, train_loss: 0.0871, step time: 0.1121\n",
      "213/223, train_loss: 0.0905, step time: 0.1161\n",
      "214/223, train_loss: 0.1036, step time: 0.1143\n",
      "215/223, train_loss: 0.0886, step time: 0.1058\n",
      "216/223, train_loss: 0.1097, step time: 0.1365\n",
      "217/223, train_loss: 0.1001, step time: 0.1132\n",
      "218/223, train_loss: 0.1092, step time: 0.1006\n",
      "219/223, train_loss: 0.0992, step time: 0.1134\n",
      "220/223, train_loss: 0.0949, step time: 0.1193\n",
      "221/223, train_loss: 0.0853, step time: 0.1255\n",
      "222/223, train_loss: 0.0926, step time: 0.0999\n",
      "223/223, train_loss: 0.0866, step time: 0.1001\n",
      "epoch 284 average loss: 0.0994\n",
      "time consuming of epoch 284 is: 90.6308\n",
      "----------\n",
      "epoch 285/300\n",
      "1/223, train_loss: 0.0964, step time: 0.1165\n",
      "2/223, train_loss: 0.0977, step time: 0.1051\n",
      "3/223, train_loss: 0.0899, step time: 0.1040\n",
      "4/223, train_loss: 0.2945, step time: 0.1267\n",
      "5/223, train_loss: 0.0932, step time: 0.1082\n",
      "6/223, train_loss: 0.1043, step time: 0.1230\n",
      "7/223, train_loss: 0.1081, step time: 0.1145\n",
      "8/223, train_loss: 0.1034, step time: 0.1103\n",
      "9/223, train_loss: 0.1065, step time: 0.1016\n",
      "10/223, train_loss: 0.0853, step time: 0.1201\n",
      "11/223, train_loss: 0.1097, step time: 0.1341\n",
      "12/223, train_loss: 0.0907, step time: 0.1089\n",
      "13/223, train_loss: 0.1078, step time: 0.1046\n",
      "14/223, train_loss: 0.0962, step time: 0.1131\n",
      "15/223, train_loss: 0.1134, step time: 0.1316\n",
      "16/223, train_loss: 0.0961, step time: 0.1020\n",
      "17/223, train_loss: 0.1009, step time: 0.1039\n",
      "18/223, train_loss: 0.0934, step time: 0.1138\n",
      "19/223, train_loss: 0.0904, step time: 0.1134\n",
      "20/223, train_loss: 0.1024, step time: 0.1167\n",
      "21/223, train_loss: 0.0946, step time: 0.1134\n",
      "22/223, train_loss: 0.0846, step time: 0.1137\n",
      "23/223, train_loss: 0.0900, step time: 0.1132\n",
      "24/223, train_loss: 0.1080, step time: 0.1140\n",
      "25/223, train_loss: 0.1125, step time: 0.1017\n",
      "26/223, train_loss: 0.0961, step time: 0.1221\n",
      "27/223, train_loss: 0.1025, step time: 0.1696\n",
      "28/223, train_loss: 0.1039, step time: 0.1008\n",
      "29/223, train_loss: 0.1104, step time: 0.1233\n",
      "30/223, train_loss: 0.0970, step time: 0.1087\n",
      "31/223, train_loss: 0.1051, step time: 0.1228\n",
      "32/223, train_loss: 0.0976, step time: 0.1128\n",
      "33/223, train_loss: 0.0894, step time: 0.1049\n",
      "34/223, train_loss: 0.0909, step time: 0.1068\n",
      "35/223, train_loss: 0.1040, step time: 0.1161\n",
      "36/223, train_loss: 0.0923, step time: 0.1008\n",
      "37/223, train_loss: 0.0902, step time: 0.1143\n",
      "38/223, train_loss: 0.1040, step time: 0.1066\n",
      "39/223, train_loss: 0.1050, step time: 0.1027\n",
      "40/223, train_loss: 0.1015, step time: 0.1046\n",
      "41/223, train_loss: 0.0981, step time: 0.1036\n",
      "42/223, train_loss: 0.0989, step time: 0.1094\n",
      "43/223, train_loss: 0.0976, step time: 0.1315\n",
      "44/223, train_loss: 0.1125, step time: 0.1003\n",
      "45/223, train_loss: 0.1061, step time: 0.1084\n",
      "46/223, train_loss: 0.0965, step time: 0.1124\n",
      "47/223, train_loss: 0.1019, step time: 0.1114\n",
      "48/223, train_loss: 0.0902, step time: 0.1137\n",
      "49/223, train_loss: 0.1033, step time: 0.1122\n",
      "50/223, train_loss: 0.0949, step time: 0.1059\n",
      "51/223, train_loss: 0.1014, step time: 0.1137\n",
      "52/223, train_loss: 0.0998, step time: 0.1154\n",
      "53/223, train_loss: 0.0956, step time: 0.1267\n",
      "54/223, train_loss: 0.1078, step time: 0.1038\n",
      "55/223, train_loss: 0.1048, step time: 0.1113\n",
      "56/223, train_loss: 0.1021, step time: 0.1082\n",
      "57/223, train_loss: 0.1011, step time: 0.1003\n",
      "58/223, train_loss: 0.1092, step time: 0.1003\n",
      "59/223, train_loss: 0.0933, step time: 0.1001\n",
      "60/223, train_loss: 0.0884, step time: 0.1087\n",
      "61/223, train_loss: 0.0967, step time: 0.1036\n",
      "62/223, train_loss: 0.0916, step time: 0.1262\n",
      "63/223, train_loss: 0.0959, step time: 0.1172\n",
      "64/223, train_loss: 0.0984, step time: 0.1036\n",
      "65/223, train_loss: 0.1000, step time: 0.1138\n",
      "66/223, train_loss: 0.1052, step time: 0.1230\n",
      "67/223, train_loss: 0.0942, step time: 0.1258\n",
      "68/223, train_loss: 0.1162, step time: 0.1324\n",
      "69/223, train_loss: 0.1011, step time: 0.0996\n",
      "70/223, train_loss: 0.1063, step time: 0.1002\n",
      "71/223, train_loss: 0.1015, step time: 0.1011\n",
      "72/223, train_loss: 0.0992, step time: 0.1004\n",
      "73/223, train_loss: 0.0944, step time: 0.0998\n",
      "74/223, train_loss: 0.1198, step time: 0.1007\n",
      "75/223, train_loss: 0.0923, step time: 0.1004\n",
      "76/223, train_loss: 0.0865, step time: 0.1005\n",
      "77/223, train_loss: 0.0944, step time: 0.0992\n",
      "78/223, train_loss: 0.1001, step time: 0.0997\n",
      "79/223, train_loss: 0.0952, step time: 0.1007\n",
      "80/223, train_loss: 0.1072, step time: 0.1009\n",
      "81/223, train_loss: 0.0885, step time: 0.1000\n",
      "82/223, train_loss: 0.1062, step time: 0.0998\n",
      "83/223, train_loss: 0.0915, step time: 0.1023\n",
      "84/223, train_loss: 0.1046, step time: 0.1052\n",
      "85/223, train_loss: 0.0912, step time: 0.1002\n",
      "86/223, train_loss: 0.1081, step time: 0.0998\n",
      "87/223, train_loss: 0.1031, step time: 0.0987\n",
      "88/223, train_loss: 0.1014, step time: 0.1117\n",
      "89/223, train_loss: 0.0933, step time: 0.1103\n",
      "90/223, train_loss: 0.0997, step time: 0.1068\n",
      "91/223, train_loss: 0.1014, step time: 0.1050\n",
      "92/223, train_loss: 0.0942, step time: 0.1007\n",
      "93/223, train_loss: 0.0975, step time: 0.1194\n",
      "94/223, train_loss: 0.0917, step time: 0.1054\n",
      "95/223, train_loss: 0.1000, step time: 0.1002\n",
      "96/223, train_loss: 0.0923, step time: 0.0994\n",
      "97/223, train_loss: 0.0962, step time: 0.1028\n",
      "98/223, train_loss: 0.0887, step time: 0.1013\n",
      "99/223, train_loss: 0.0913, step time: 0.1231\n",
      "100/223, train_loss: 0.0955, step time: 0.1068\n",
      "101/223, train_loss: 0.1023, step time: 0.1141\n",
      "102/223, train_loss: 0.1000, step time: 0.1004\n",
      "103/223, train_loss: 0.1011, step time: 0.1242\n",
      "104/223, train_loss: 0.0904, step time: 0.0990\n",
      "105/223, train_loss: 0.1003, step time: 0.1010\n",
      "106/223, train_loss: 0.0947, step time: 0.0985\n",
      "107/223, train_loss: 0.0922, step time: 0.1111\n",
      "108/223, train_loss: 0.0990, step time: 0.1211\n",
      "109/223, train_loss: 0.1035, step time: 0.1014\n",
      "110/223, train_loss: 0.0977, step time: 0.1020\n",
      "111/223, train_loss: 0.1014, step time: 0.1115\n",
      "112/223, train_loss: 0.0962, step time: 0.1119\n",
      "113/223, train_loss: 0.0948, step time: 0.1125\n",
      "114/223, train_loss: 0.1006, step time: 0.1043\n",
      "115/223, train_loss: 0.0938, step time: 0.1112\n",
      "116/223, train_loss: 0.0889, step time: 0.1111\n",
      "117/223, train_loss: 0.1009, step time: 0.1048\n",
      "118/223, train_loss: 0.0845, step time: 0.1025\n",
      "119/223, train_loss: 0.0947, step time: 0.1074\n",
      "120/223, train_loss: 0.1082, step time: 0.1258\n",
      "121/223, train_loss: 0.0902, step time: 0.1016\n",
      "122/223, train_loss: 0.1029, step time: 0.1018\n",
      "123/223, train_loss: 0.0903, step time: 0.1217\n",
      "124/223, train_loss: 0.0969, step time: 0.1001\n",
      "125/223, train_loss: 0.0977, step time: 0.1000\n",
      "126/223, train_loss: 0.0846, step time: 0.1006\n",
      "127/223, train_loss: 0.0963, step time: 0.1014\n",
      "128/223, train_loss: 0.1020, step time: 0.1023\n",
      "129/223, train_loss: 0.1070, step time: 0.1000\n",
      "130/223, train_loss: 0.0933, step time: 0.0996\n",
      "131/223, train_loss: 0.0986, step time: 0.1004\n",
      "132/223, train_loss: 0.0965, step time: 0.1070\n",
      "133/223, train_loss: 0.1002, step time: 0.1006\n",
      "134/223, train_loss: 0.0902, step time: 0.0999\n",
      "135/223, train_loss: 0.0952, step time: 0.1006\n",
      "136/223, train_loss: 0.1040, step time: 0.1323\n",
      "137/223, train_loss: 0.0969, step time: 0.1075\n",
      "138/223, train_loss: 0.1109, step time: 0.1195\n",
      "139/223, train_loss: 0.0938, step time: 0.1251\n",
      "140/223, train_loss: 0.0938, step time: 0.1046\n",
      "141/223, train_loss: 0.0942, step time: 0.1232\n",
      "142/223, train_loss: 0.1055, step time: 0.1108\n",
      "143/223, train_loss: 0.0907, step time: 0.1109\n",
      "144/223, train_loss: 0.1083, step time: 0.1116\n",
      "145/223, train_loss: 0.1106, step time: 0.1076\n",
      "146/223, train_loss: 0.0977, step time: 0.1107\n",
      "147/223, train_loss: 0.0967, step time: 0.1197\n",
      "148/223, train_loss: 0.0948, step time: 0.1250\n",
      "149/223, train_loss: 0.1101, step time: 0.1142\n",
      "150/223, train_loss: 0.0973, step time: 0.1157\n",
      "151/223, train_loss: 0.1024, step time: 0.1100\n",
      "152/223, train_loss: 0.0961, step time: 0.1260\n",
      "153/223, train_loss: 0.0947, step time: 0.1125\n",
      "154/223, train_loss: 0.1039, step time: 0.1083\n",
      "155/223, train_loss: 0.0992, step time: 0.1119\n",
      "156/223, train_loss: 0.1041, step time: 0.1046\n",
      "157/223, train_loss: 0.0957, step time: 0.1137\n",
      "158/223, train_loss: 0.1043, step time: 0.1136\n",
      "159/223, train_loss: 0.0916, step time: 0.1223\n",
      "160/223, train_loss: 0.0924, step time: 0.1530\n",
      "161/223, train_loss: 0.0926, step time: 0.1139\n",
      "162/223, train_loss: 0.0965, step time: 0.1247\n",
      "163/223, train_loss: 0.1074, step time: 0.1049\n",
      "164/223, train_loss: 0.0922, step time: 0.1000\n",
      "165/223, train_loss: 0.0917, step time: 0.1162\n",
      "166/223, train_loss: 0.1091, step time: 0.1095\n",
      "167/223, train_loss: 0.1032, step time: 0.1070\n",
      "168/223, train_loss: 0.1033, step time: 0.1117\n",
      "169/223, train_loss: 0.0956, step time: 0.0997\n",
      "170/223, train_loss: 0.0936, step time: 0.1186\n",
      "171/223, train_loss: 0.1055, step time: 0.1137\n",
      "172/223, train_loss: 0.0986, step time: 0.1137\n",
      "173/223, train_loss: 0.0983, step time: 0.1035\n",
      "174/223, train_loss: 0.1011, step time: 0.1040\n",
      "175/223, train_loss: 0.0972, step time: 0.1081\n",
      "176/223, train_loss: 0.1029, step time: 0.1046\n",
      "177/223, train_loss: 0.0975, step time: 0.1000\n",
      "178/223, train_loss: 0.0962, step time: 0.1055\n",
      "179/223, train_loss: 0.1170, step time: 0.1058\n",
      "180/223, train_loss: 0.0978, step time: 0.0995\n",
      "181/223, train_loss: 0.0950, step time: 0.1156\n",
      "182/223, train_loss: 0.1039, step time: 0.0991\n",
      "183/223, train_loss: 0.1004, step time: 0.0992\n",
      "184/223, train_loss: 0.1082, step time: 0.1001\n",
      "185/223, train_loss: 0.0957, step time: 0.1067\n",
      "186/223, train_loss: 0.0962, step time: 0.0990\n",
      "187/223, train_loss: 0.0861, step time: 0.1082\n",
      "188/223, train_loss: 0.0973, step time: 0.1065\n",
      "189/223, train_loss: 0.0927, step time: 0.1470\n",
      "190/223, train_loss: 0.0959, step time: 0.1264\n",
      "191/223, train_loss: 0.0908, step time: 0.1127\n",
      "192/223, train_loss: 0.1001, step time: 0.1116\n",
      "193/223, train_loss: 0.1067, step time: 0.1302\n",
      "194/223, train_loss: 0.0886, step time: 0.1334\n",
      "195/223, train_loss: 0.1055, step time: 0.1348\n",
      "196/223, train_loss: 0.0882, step time: 0.1221\n",
      "197/223, train_loss: 0.0967, step time: 0.0999\n",
      "198/223, train_loss: 0.1033, step time: 0.1001\n",
      "199/223, train_loss: 0.1117, step time: 0.1006\n",
      "200/223, train_loss: 0.1019, step time: 0.1019\n",
      "201/223, train_loss: 0.0975, step time: 0.0998\n",
      "202/223, train_loss: 0.0962, step time: 0.1004\n",
      "203/223, train_loss: 0.0916, step time: 0.1013\n",
      "204/223, train_loss: 0.0956, step time: 0.1043\n",
      "205/223, train_loss: 0.0925, step time: 0.1004\n",
      "206/223, train_loss: 0.0942, step time: 0.1007\n",
      "207/223, train_loss: 0.1054, step time: 0.1160\n",
      "208/223, train_loss: 0.0903, step time: 0.1365\n",
      "209/223, train_loss: 0.1065, step time: 0.1154\n",
      "210/223, train_loss: 0.1003, step time: 0.1011\n",
      "211/223, train_loss: 0.0971, step time: 0.1230\n",
      "212/223, train_loss: 0.0943, step time: 0.1027\n",
      "213/223, train_loss: 0.0936, step time: 0.0994\n",
      "214/223, train_loss: 0.0906, step time: 0.1000\n",
      "215/223, train_loss: 0.1118, step time: 0.1004\n",
      "216/223, train_loss: 0.0884, step time: 0.1002\n",
      "217/223, train_loss: 0.1019, step time: 0.1150\n",
      "218/223, train_loss: 0.0889, step time: 0.1183\n",
      "219/223, train_loss: 0.1132, step time: 0.1125\n",
      "220/223, train_loss: 0.0985, step time: 0.1214\n",
      "221/223, train_loss: 0.0992, step time: 0.0993\n",
      "222/223, train_loss: 0.0897, step time: 0.0990\n",
      "223/223, train_loss: 0.1025, step time: 0.0995\n",
      "epoch 285 average loss: 0.0994\n",
      "current epoch: 285 current mean dice: 0.8622 tc: 0.9225 wt: 0.8718 et: 0.7921\n",
      "best mean dice: 0.8622 at epoch: 260\n",
      "time consuming of epoch 285 is: 100.5855\n",
      "----------\n",
      "epoch 286/300\n",
      "1/223, train_loss: 0.0989, step time: 0.1015\n",
      "2/223, train_loss: 0.1076, step time: 0.1272\n",
      "3/223, train_loss: 0.0995, step time: 0.1137\n",
      "4/223, train_loss: 0.0887, step time: 0.1008\n",
      "5/223, train_loss: 0.0935, step time: 0.1078\n",
      "6/223, train_loss: 0.0904, step time: 0.1071\n",
      "7/223, train_loss: 0.1123, step time: 0.1104\n",
      "8/223, train_loss: 0.0957, step time: 0.1101\n",
      "9/223, train_loss: 0.1020, step time: 0.1070\n",
      "10/223, train_loss: 0.1066, step time: 0.1113\n",
      "11/223, train_loss: 0.1084, step time: 0.1190\n",
      "12/223, train_loss: 0.0882, step time: 0.1043\n",
      "13/223, train_loss: 0.0938, step time: 0.1174\n",
      "14/223, train_loss: 0.0897, step time: 0.1071\n",
      "15/223, train_loss: 0.0945, step time: 0.1269\n",
      "16/223, train_loss: 0.0992, step time: 0.1176\n",
      "17/223, train_loss: 0.1046, step time: 0.1008\n",
      "18/223, train_loss: 0.1069, step time: 0.1059\n",
      "19/223, train_loss: 0.1071, step time: 0.1241\n",
      "20/223, train_loss: 0.0946, step time: 0.1262\n",
      "21/223, train_loss: 0.0914, step time: 0.1027\n",
      "22/223, train_loss: 0.0899, step time: 0.1006\n",
      "23/223, train_loss: 0.1034, step time: 0.1023\n",
      "24/223, train_loss: 0.0963, step time: 0.1004\n",
      "25/223, train_loss: 0.0906, step time: 0.1139\n",
      "26/223, train_loss: 0.0947, step time: 0.1175\n",
      "27/223, train_loss: 0.1088, step time: 0.1103\n",
      "28/223, train_loss: 0.0886, step time: 0.1075\n",
      "29/223, train_loss: 0.0916, step time: 0.1029\n",
      "30/223, train_loss: 0.0981, step time: 0.1072\n",
      "31/223, train_loss: 0.0974, step time: 0.1174\n",
      "32/223, train_loss: 0.1075, step time: 0.1220\n",
      "33/223, train_loss: 0.1045, step time: 0.1166\n",
      "34/223, train_loss: 0.0975, step time: 0.1272\n",
      "35/223, train_loss: 0.1019, step time: 0.1250\n",
      "36/223, train_loss: 0.0989, step time: 0.1066\n",
      "37/223, train_loss: 0.1097, step time: 0.1055\n",
      "38/223, train_loss: 0.1031, step time: 0.1007\n",
      "39/223, train_loss: 0.0894, step time: 0.1036\n",
      "40/223, train_loss: 0.0964, step time: 0.1141\n",
      "41/223, train_loss: 0.0979, step time: 0.1526\n",
      "42/223, train_loss: 0.0937, step time: 0.1053\n",
      "43/223, train_loss: 0.1048, step time: 0.1253\n",
      "44/223, train_loss: 0.0979, step time: 0.1123\n",
      "45/223, train_loss: 0.0948, step time: 0.1087\n",
      "46/223, train_loss: 0.1013, step time: 0.1110\n",
      "47/223, train_loss: 0.0910, step time: 0.1110\n",
      "48/223, train_loss: 0.0863, step time: 0.1309\n",
      "49/223, train_loss: 0.1088, step time: 0.1061\n",
      "50/223, train_loss: 0.0923, step time: 0.1230\n",
      "51/223, train_loss: 0.0952, step time: 0.1004\n",
      "52/223, train_loss: 0.0943, step time: 0.1208\n",
      "53/223, train_loss: 0.0949, step time: 0.1088\n",
      "54/223, train_loss: 0.1122, step time: 0.1072\n",
      "55/223, train_loss: 0.0969, step time: 0.1271\n",
      "56/223, train_loss: 0.1077, step time: 0.1224\n",
      "57/223, train_loss: 0.1088, step time: 0.1124\n",
      "58/223, train_loss: 0.1042, step time: 0.1205\n",
      "59/223, train_loss: 0.0928, step time: 0.1072\n",
      "60/223, train_loss: 0.1078, step time: 0.1234\n",
      "61/223, train_loss: 0.1114, step time: 0.1167\n",
      "62/223, train_loss: 0.1094, step time: 0.1011\n",
      "63/223, train_loss: 0.1012, step time: 0.1141\n",
      "64/223, train_loss: 0.0975, step time: 0.1277\n",
      "65/223, train_loss: 0.1085, step time: 0.1163\n",
      "66/223, train_loss: 0.0993, step time: 0.1119\n",
      "67/223, train_loss: 0.0904, step time: 0.1133\n",
      "68/223, train_loss: 0.0939, step time: 0.1143\n",
      "69/223, train_loss: 0.0824, step time: 0.1148\n",
      "70/223, train_loss: 0.1034, step time: 0.1020\n",
      "71/223, train_loss: 0.1080, step time: 0.1203\n",
      "72/223, train_loss: 0.0858, step time: 0.1103\n",
      "73/223, train_loss: 0.0988, step time: 0.1123\n",
      "74/223, train_loss: 0.0940, step time: 0.1049\n",
      "75/223, train_loss: 0.1026, step time: 0.1247\n",
      "76/223, train_loss: 0.1101, step time: 0.1153\n",
      "77/223, train_loss: 0.0935, step time: 0.1328\n",
      "78/223, train_loss: 0.1138, step time: 0.1042\n",
      "79/223, train_loss: 0.1004, step time: 0.1142\n",
      "80/223, train_loss: 0.1237, step time: 0.1117\n",
      "81/223, train_loss: 0.0914, step time: 0.1008\n",
      "82/223, train_loss: 0.0913, step time: 0.1005\n",
      "83/223, train_loss: 0.1069, step time: 0.1291\n",
      "84/223, train_loss: 0.0959, step time: 0.1069\n",
      "85/223, train_loss: 0.0946, step time: 0.1074\n",
      "86/223, train_loss: 0.0942, step time: 0.1198\n",
      "87/223, train_loss: 0.1046, step time: 0.1135\n",
      "88/223, train_loss: 0.1035, step time: 0.1173\n",
      "89/223, train_loss: 0.0883, step time: 0.1104\n",
      "90/223, train_loss: 0.0993, step time: 0.1305\n",
      "91/223, train_loss: 0.1059, step time: 0.1114\n",
      "92/223, train_loss: 0.0981, step time: 0.1099\n",
      "93/223, train_loss: 0.0980, step time: 0.1093\n",
      "94/223, train_loss: 0.1023, step time: 0.1057\n",
      "95/223, train_loss: 0.1075, step time: 0.1020\n",
      "96/223, train_loss: 0.1031, step time: 0.1009\n",
      "97/223, train_loss: 0.0890, step time: 0.1160\n",
      "98/223, train_loss: 0.1031, step time: 0.1193\n",
      "99/223, train_loss: 0.0962, step time: 0.1196\n",
      "100/223, train_loss: 0.0974, step time: 0.1261\n",
      "101/223, train_loss: 0.0976, step time: 0.1124\n",
      "102/223, train_loss: 0.0892, step time: 0.1165\n",
      "103/223, train_loss: 0.1009, step time: 0.1187\n",
      "104/223, train_loss: 0.0964, step time: 0.0996\n",
      "105/223, train_loss: 0.1043, step time: 0.1365\n",
      "106/223, train_loss: 0.0942, step time: 0.1113\n",
      "107/223, train_loss: 0.0980, step time: 0.1079\n",
      "108/223, train_loss: 0.0994, step time: 0.1071\n",
      "109/223, train_loss: 0.1025, step time: 0.1029\n",
      "110/223, train_loss: 0.0868, step time: 0.1120\n",
      "111/223, train_loss: 0.0995, step time: 0.1045\n",
      "112/223, train_loss: 0.0947, step time: 0.1012\n",
      "113/223, train_loss: 0.0978, step time: 0.1095\n",
      "114/223, train_loss: 0.1049, step time: 0.1205\n",
      "115/223, train_loss: 0.0936, step time: 0.1125\n",
      "116/223, train_loss: 0.1012, step time: 0.1094\n",
      "117/223, train_loss: 0.1020, step time: 0.1087\n",
      "118/223, train_loss: 0.0927, step time: 0.1063\n",
      "119/223, train_loss: 0.0979, step time: 0.1403\n",
      "120/223, train_loss: 0.1172, step time: 0.1179\n",
      "121/223, train_loss: 0.0928, step time: 0.1193\n",
      "122/223, train_loss: 0.0933, step time: 0.1023\n",
      "123/223, train_loss: 0.0972, step time: 0.1422\n",
      "124/223, train_loss: 0.1061, step time: 0.1475\n",
      "125/223, train_loss: 0.0982, step time: 0.1103\n",
      "126/223, train_loss: 0.1050, step time: 0.1060\n",
      "127/223, train_loss: 0.0977, step time: 0.1044\n",
      "128/223, train_loss: 0.1056, step time: 0.1080\n",
      "129/223, train_loss: 0.1115, step time: 0.1107\n",
      "130/223, train_loss: 0.0976, step time: 0.1603\n",
      "131/223, train_loss: 0.0986, step time: 0.1070\n",
      "132/223, train_loss: 0.0858, step time: 0.1153\n",
      "133/223, train_loss: 0.0931, step time: 0.1152\n",
      "134/223, train_loss: 0.0969, step time: 0.1013\n",
      "135/223, train_loss: 0.0925, step time: 0.1079\n",
      "136/223, train_loss: 0.0950, step time: 0.1204\n",
      "137/223, train_loss: 0.1006, step time: 0.1180\n",
      "138/223, train_loss: 0.1165, step time: 0.1044\n",
      "139/223, train_loss: 0.1095, step time: 0.1094\n",
      "140/223, train_loss: 0.0988, step time: 0.1054\n",
      "141/223, train_loss: 0.0960, step time: 0.1010\n",
      "142/223, train_loss: 0.1013, step time: 0.1031\n",
      "143/223, train_loss: 0.1046, step time: 0.1617\n",
      "144/223, train_loss: 0.1007, step time: 0.1142\n",
      "145/223, train_loss: 0.1078, step time: 0.1177\n",
      "146/223, train_loss: 0.1077, step time: 0.1050\n",
      "147/223, train_loss: 0.1019, step time: 0.1156\n",
      "148/223, train_loss: 0.0942, step time: 0.1006\n",
      "149/223, train_loss: 0.0999, step time: 0.1124\n",
      "150/223, train_loss: 0.0928, step time: 0.1078\n",
      "151/223, train_loss: 0.0888, step time: 0.1095\n",
      "152/223, train_loss: 0.0918, step time: 0.1160\n",
      "153/223, train_loss: 0.0899, step time: 0.1059\n",
      "154/223, train_loss: 0.0936, step time: 0.1138\n",
      "155/223, train_loss: 0.0956, step time: 0.1130\n",
      "156/223, train_loss: 0.0888, step time: 0.1024\n",
      "157/223, train_loss: 0.1041, step time: 0.1006\n",
      "158/223, train_loss: 0.1069, step time: 0.1119\n",
      "159/223, train_loss: 0.1015, step time: 0.1028\n",
      "160/223, train_loss: 0.1041, step time: 0.1153\n",
      "161/223, train_loss: 0.0987, step time: 0.1114\n",
      "162/223, train_loss: 0.0893, step time: 0.0993\n",
      "163/223, train_loss: 0.1057, step time: 0.1081\n",
      "164/223, train_loss: 0.0964, step time: 0.1009\n",
      "165/223, train_loss: 0.0943, step time: 0.1067\n",
      "166/223, train_loss: 0.0866, step time: 0.1168\n",
      "167/223, train_loss: 0.0924, step time: 0.1062\n",
      "168/223, train_loss: 0.1058, step time: 0.1263\n",
      "169/223, train_loss: 0.0915, step time: 0.1192\n",
      "170/223, train_loss: 0.0945, step time: 0.1038\n",
      "171/223, train_loss: 0.0833, step time: 0.1057\n",
      "172/223, train_loss: 0.0992, step time: 0.1047\n",
      "173/223, train_loss: 0.0989, step time: 0.0993\n",
      "174/223, train_loss: 0.0913, step time: 0.1119\n",
      "175/223, train_loss: 0.0889, step time: 0.1243\n",
      "176/223, train_loss: 0.1057, step time: 0.1237\n",
      "177/223, train_loss: 0.2951, step time: 0.1083\n",
      "178/223, train_loss: 0.1012, step time: 0.1036\n",
      "179/223, train_loss: 0.0990, step time: 0.1264\n",
      "180/223, train_loss: 0.0928, step time: 0.1336\n",
      "181/223, train_loss: 0.1010, step time: 0.1040\n",
      "182/223, train_loss: 0.0968, step time: 0.1116\n",
      "183/223, train_loss: 0.0911, step time: 0.1033\n",
      "184/223, train_loss: 0.1102, step time: 0.1071\n",
      "185/223, train_loss: 0.0978, step time: 0.1058\n",
      "186/223, train_loss: 0.1030, step time: 0.1083\n",
      "187/223, train_loss: 0.0937, step time: 0.1386\n",
      "188/223, train_loss: 0.0982, step time: 0.1034\n",
      "189/223, train_loss: 0.0901, step time: 0.1040\n",
      "190/223, train_loss: 0.1059, step time: 0.1650\n",
      "191/223, train_loss: 0.1074, step time: 0.1191\n",
      "192/223, train_loss: 0.1106, step time: 0.1323\n",
      "193/223, train_loss: 0.0846, step time: 0.1034\n",
      "194/223, train_loss: 0.0954, step time: 0.1101\n",
      "195/223, train_loss: 0.0975, step time: 0.1279\n",
      "196/223, train_loss: 0.0980, step time: 0.1150\n",
      "197/223, train_loss: 0.1004, step time: 0.1096\n",
      "198/223, train_loss: 0.0910, step time: 0.1160\n",
      "199/223, train_loss: 0.0924, step time: 0.1120\n",
      "200/223, train_loss: 0.0920, step time: 0.1254\n",
      "201/223, train_loss: 0.1113, step time: 0.1057\n",
      "202/223, train_loss: 0.0969, step time: 0.1156\n",
      "203/223, train_loss: 0.0920, step time: 0.1010\n",
      "204/223, train_loss: 0.0984, step time: 0.1133\n",
      "205/223, train_loss: 0.1016, step time: 0.1045\n",
      "206/223, train_loss: 0.0985, step time: 0.1152\n",
      "207/223, train_loss: 0.0912, step time: 0.1246\n",
      "208/223, train_loss: 0.1028, step time: 0.1061\n",
      "209/223, train_loss: 0.0943, step time: 0.1100\n",
      "210/223, train_loss: 0.1026, step time: 0.1064\n",
      "211/223, train_loss: 0.0939, step time: 0.1149\n",
      "212/223, train_loss: 0.0958, step time: 0.1449\n",
      "213/223, train_loss: 0.0900, step time: 0.1305\n",
      "214/223, train_loss: 0.0969, step time: 0.1010\n",
      "215/223, train_loss: 0.1056, step time: 0.1012\n",
      "216/223, train_loss: 0.1003, step time: 0.1050\n",
      "217/223, train_loss: 0.1099, step time: 0.1047\n",
      "218/223, train_loss: 0.0929, step time: 0.1003\n",
      "219/223, train_loss: 0.1091, step time: 0.0998\n",
      "220/223, train_loss: 0.0966, step time: 0.1005\n",
      "221/223, train_loss: 0.0956, step time: 0.1019\n",
      "222/223, train_loss: 0.0981, step time: 0.0999\n",
      "223/223, train_loss: 0.0868, step time: 0.0997\n",
      "epoch 286 average loss: 0.0995\n",
      "time consuming of epoch 286 is: 89.1961\n",
      "----------\n",
      "epoch 287/300\n",
      "1/223, train_loss: 0.0941, step time: 0.1166\n",
      "2/223, train_loss: 0.0978, step time: 0.1068\n",
      "3/223, train_loss: 0.1030, step time: 0.1198\n",
      "4/223, train_loss: 0.0872, step time: 0.1170\n",
      "5/223, train_loss: 0.0878, step time: 0.1199\n",
      "6/223, train_loss: 0.0879, step time: 0.1135\n",
      "7/223, train_loss: 0.1018, step time: 0.1376\n",
      "8/223, train_loss: 0.0894, step time: 0.1056\n",
      "9/223, train_loss: 0.1060, step time: 0.1015\n",
      "10/223, train_loss: 0.0929, step time: 0.1482\n",
      "11/223, train_loss: 0.1025, step time: 0.1395\n",
      "12/223, train_loss: 0.0925, step time: 0.1173\n",
      "13/223, train_loss: 0.0928, step time: 0.1001\n",
      "14/223, train_loss: 0.0890, step time: 0.1003\n",
      "15/223, train_loss: 0.0988, step time: 0.1026\n",
      "16/223, train_loss: 0.1037, step time: 0.1003\n",
      "17/223, train_loss: 0.0927, step time: 0.1007\n",
      "18/223, train_loss: 0.1062, step time: 0.1007\n",
      "19/223, train_loss: 0.0935, step time: 0.1011\n",
      "20/223, train_loss: 0.1050, step time: 0.1008\n",
      "21/223, train_loss: 0.1034, step time: 0.1042\n",
      "22/223, train_loss: 0.1010, step time: 0.1062\n",
      "23/223, train_loss: 0.1045, step time: 0.0998\n",
      "24/223, train_loss: 0.0917, step time: 0.1001\n",
      "25/223, train_loss: 0.1150, step time: 0.1264\n",
      "26/223, train_loss: 0.0978, step time: 0.1329\n",
      "27/223, train_loss: 0.1077, step time: 0.1008\n",
      "28/223, train_loss: 0.0884, step time: 0.1083\n",
      "29/223, train_loss: 0.0946, step time: 0.1014\n",
      "30/223, train_loss: 0.0976, step time: 0.1290\n",
      "31/223, train_loss: 0.0915, step time: 0.1160\n",
      "32/223, train_loss: 0.0945, step time: 0.1011\n",
      "33/223, train_loss: 0.0897, step time: 0.1071\n",
      "34/223, train_loss: 0.1002, step time: 0.1180\n",
      "35/223, train_loss: 0.1088, step time: 0.1112\n",
      "36/223, train_loss: 0.0965, step time: 0.1172\n",
      "37/223, train_loss: 0.0934, step time: 0.1094\n",
      "38/223, train_loss: 0.0965, step time: 0.1510\n",
      "39/223, train_loss: 0.0898, step time: 0.1061\n",
      "40/223, train_loss: 0.0886, step time: 0.1004\n",
      "41/223, train_loss: 0.1081, step time: 0.1107\n",
      "42/223, train_loss: 0.0999, step time: 0.1162\n",
      "43/223, train_loss: 0.0867, step time: 0.1082\n",
      "44/223, train_loss: 0.0908, step time: 0.1222\n",
      "45/223, train_loss: 0.0930, step time: 0.1150\n",
      "46/223, train_loss: 0.3013, step time: 0.1126\n",
      "47/223, train_loss: 0.0911, step time: 0.1017\n",
      "48/223, train_loss: 0.0911, step time: 0.1067\n",
      "49/223, train_loss: 0.1010, step time: 0.1136\n",
      "50/223, train_loss: 0.0955, step time: 0.1086\n",
      "51/223, train_loss: 0.1014, step time: 0.0994\n",
      "52/223, train_loss: 0.1009, step time: 0.1066\n",
      "53/223, train_loss: 0.0901, step time: 0.1086\n",
      "54/223, train_loss: 0.1041, step time: 0.1021\n",
      "55/223, train_loss: 0.0980, step time: 0.1010\n",
      "56/223, train_loss: 0.0935, step time: 0.1090\n",
      "57/223, train_loss: 0.1030, step time: 0.1056\n",
      "58/223, train_loss: 0.0896, step time: 0.1169\n",
      "59/223, train_loss: 0.0983, step time: 0.1022\n",
      "60/223, train_loss: 0.0954, step time: 0.1101\n",
      "61/223, train_loss: 0.1105, step time: 0.1085\n",
      "62/223, train_loss: 0.0945, step time: 0.1097\n",
      "63/223, train_loss: 0.0935, step time: 0.1197\n",
      "64/223, train_loss: 0.1035, step time: 0.1009\n",
      "65/223, train_loss: 0.1031, step time: 0.1224\n",
      "66/223, train_loss: 0.0984, step time: 0.1106\n",
      "67/223, train_loss: 0.1017, step time: 0.1101\n",
      "68/223, train_loss: 0.0896, step time: 0.1134\n",
      "69/223, train_loss: 0.0980, step time: 0.1199\n",
      "70/223, train_loss: 0.1039, step time: 0.1080\n",
      "71/223, train_loss: 0.0943, step time: 0.1018\n",
      "72/223, train_loss: 0.1093, step time: 0.1037\n",
      "73/223, train_loss: 0.0933, step time: 0.1130\n",
      "74/223, train_loss: 0.1161, step time: 0.1092\n",
      "75/223, train_loss: 0.0916, step time: 0.1135\n",
      "76/223, train_loss: 0.1127, step time: 0.1163\n",
      "77/223, train_loss: 0.1084, step time: 0.1122\n",
      "78/223, train_loss: 0.0973, step time: 0.1216\n",
      "79/223, train_loss: 0.1031, step time: 0.1163\n",
      "80/223, train_loss: 0.0954, step time: 0.1103\n",
      "81/223, train_loss: 0.0996, step time: 0.1141\n",
      "82/223, train_loss: 0.1019, step time: 0.1020\n",
      "83/223, train_loss: 0.0957, step time: 0.1067\n",
      "84/223, train_loss: 0.0842, step time: 0.1078\n",
      "85/223, train_loss: 0.0976, step time: 0.1003\n",
      "86/223, train_loss: 0.0891, step time: 0.1007\n",
      "87/223, train_loss: 0.1061, step time: 0.1152\n",
      "88/223, train_loss: 0.0939, step time: 0.1006\n",
      "89/223, train_loss: 0.1099, step time: 0.1008\n",
      "90/223, train_loss: 0.1051, step time: 0.1001\n",
      "91/223, train_loss: 0.0970, step time: 0.1160\n",
      "92/223, train_loss: 0.0917, step time: 0.1002\n",
      "93/223, train_loss: 0.0992, step time: 0.1046\n",
      "94/223, train_loss: 0.1011, step time: 0.1085\n",
      "95/223, train_loss: 0.1006, step time: 0.1007\n",
      "96/223, train_loss: 0.0913, step time: 0.1102\n",
      "97/223, train_loss: 0.0952, step time: 0.1084\n",
      "98/223, train_loss: 0.0992, step time: 0.1118\n",
      "99/223, train_loss: 0.1039, step time: 0.1221\n",
      "100/223, train_loss: 0.0982, step time: 0.1092\n",
      "101/223, train_loss: 0.1087, step time: 0.1109\n",
      "102/223, train_loss: 0.1010, step time: 0.1091\n",
      "103/223, train_loss: 0.1106, step time: 0.1212\n",
      "104/223, train_loss: 0.1119, step time: 0.1102\n",
      "105/223, train_loss: 0.1008, step time: 0.1136\n",
      "106/223, train_loss: 0.0884, step time: 0.1196\n",
      "107/223, train_loss: 0.0921, step time: 0.1104\n",
      "108/223, train_loss: 0.0922, step time: 0.1003\n",
      "109/223, train_loss: 0.1133, step time: 0.1049\n",
      "110/223, train_loss: 0.1080, step time: 0.1222\n",
      "111/223, train_loss: 0.0955, step time: 0.1723\n",
      "112/223, train_loss: 0.0892, step time: 0.1053\n",
      "113/223, train_loss: 0.1089, step time: 0.1095\n",
      "114/223, train_loss: 0.0926, step time: 0.1252\n",
      "115/223, train_loss: 0.0944, step time: 0.1339\n",
      "116/223, train_loss: 0.1079, step time: 0.1203\n",
      "117/223, train_loss: 0.1071, step time: 0.1141\n",
      "118/223, train_loss: 0.0950, step time: 0.1058\n",
      "119/223, train_loss: 0.0952, step time: 0.1112\n",
      "120/223, train_loss: 0.1047, step time: 0.1144\n",
      "121/223, train_loss: 0.0895, step time: 0.1149\n",
      "122/223, train_loss: 0.1049, step time: 0.1009\n",
      "123/223, train_loss: 0.0908, step time: 0.1183\n",
      "124/223, train_loss: 0.0934, step time: 0.1117\n",
      "125/223, train_loss: 0.1010, step time: 0.1001\n",
      "126/223, train_loss: 0.0926, step time: 0.1261\n",
      "127/223, train_loss: 0.0978, step time: 0.1142\n",
      "128/223, train_loss: 0.0932, step time: 0.1190\n",
      "129/223, train_loss: 0.0937, step time: 0.1066\n",
      "130/223, train_loss: 0.1040, step time: 0.1151\n",
      "131/223, train_loss: 0.0935, step time: 0.1215\n",
      "132/223, train_loss: 0.0921, step time: 0.1136\n",
      "133/223, train_loss: 0.0942, step time: 0.1173\n",
      "134/223, train_loss: 0.1046, step time: 0.1049\n",
      "135/223, train_loss: 0.1034, step time: 0.1044\n",
      "136/223, train_loss: 0.0996, step time: 0.1307\n",
      "137/223, train_loss: 0.0952, step time: 0.1465\n",
      "138/223, train_loss: 0.0940, step time: 0.1049\n",
      "139/223, train_loss: 0.1017, step time: 0.1153\n",
      "140/223, train_loss: 0.1055, step time: 0.1125\n",
      "141/223, train_loss: 0.0947, step time: 0.1137\n",
      "142/223, train_loss: 0.0927, step time: 0.1013\n",
      "143/223, train_loss: 0.0970, step time: 0.1041\n",
      "144/223, train_loss: 0.0872, step time: 0.1036\n",
      "145/223, train_loss: 0.0996, step time: 0.1119\n",
      "146/223, train_loss: 0.0941, step time: 0.1239\n",
      "147/223, train_loss: 0.1035, step time: 0.1362\n",
      "148/223, train_loss: 0.0844, step time: 0.1155\n",
      "149/223, train_loss: 0.0960, step time: 0.1022\n",
      "150/223, train_loss: 0.1055, step time: 0.1032\n",
      "151/223, train_loss: 0.0909, step time: 0.1091\n",
      "152/223, train_loss: 0.1050, step time: 0.1010\n",
      "153/223, train_loss: 0.1003, step time: 0.1085\n",
      "154/223, train_loss: 0.1041, step time: 0.1136\n",
      "155/223, train_loss: 0.1006, step time: 0.1134\n",
      "156/223, train_loss: 0.1086, step time: 0.1122\n",
      "157/223, train_loss: 0.0935, step time: 0.1160\n",
      "158/223, train_loss: 0.1068, step time: 0.1084\n",
      "159/223, train_loss: 0.1155, step time: 0.1149\n",
      "160/223, train_loss: 0.1063, step time: 0.1009\n",
      "161/223, train_loss: 0.0960, step time: 0.1174\n",
      "162/223, train_loss: 0.1062, step time: 0.1177\n",
      "163/223, train_loss: 0.0898, step time: 0.1008\n",
      "164/223, train_loss: 0.0912, step time: 0.1093\n",
      "165/223, train_loss: 0.1053, step time: 0.1141\n",
      "166/223, train_loss: 0.0968, step time: 0.1339\n",
      "167/223, train_loss: 0.1163, step time: 0.0991\n",
      "168/223, train_loss: 0.0889, step time: 0.1289\n",
      "169/223, train_loss: 0.1054, step time: 0.1227\n",
      "170/223, train_loss: 0.0964, step time: 0.1192\n",
      "171/223, train_loss: 0.1041, step time: 0.1107\n",
      "172/223, train_loss: 0.0984, step time: 0.1115\n",
      "173/223, train_loss: 0.0902, step time: 0.1135\n",
      "174/223, train_loss: 0.1053, step time: 0.1006\n",
      "175/223, train_loss: 0.0994, step time: 0.1018\n",
      "176/223, train_loss: 0.0992, step time: 0.1062\n",
      "177/223, train_loss: 0.0950, step time: 0.1121\n",
      "178/223, train_loss: 0.1000, step time: 0.1340\n",
      "179/223, train_loss: 0.1050, step time: 0.1304\n",
      "180/223, train_loss: 0.1070, step time: 0.1297\n",
      "181/223, train_loss: 0.1031, step time: 0.1142\n",
      "182/223, train_loss: 0.1036, step time: 0.1002\n",
      "183/223, train_loss: 0.0975, step time: 0.1119\n",
      "184/223, train_loss: 0.1032, step time: 0.1005\n",
      "185/223, train_loss: 0.1043, step time: 0.1200\n",
      "186/223, train_loss: 0.1017, step time: 0.1095\n",
      "187/223, train_loss: 0.0972, step time: 0.1043\n",
      "188/223, train_loss: 0.0884, step time: 0.1207\n",
      "189/223, train_loss: 0.0952, step time: 0.1235\n",
      "190/223, train_loss: 0.1090, step time: 0.1058\n",
      "191/223, train_loss: 0.0924, step time: 0.1002\n",
      "192/223, train_loss: 0.0858, step time: 0.1341\n",
      "193/223, train_loss: 0.0998, step time: 0.1072\n",
      "194/223, train_loss: 0.1030, step time: 0.1193\n",
      "195/223, train_loss: 0.0974, step time: 0.1197\n",
      "196/223, train_loss: 0.0962, step time: 0.1130\n",
      "197/223, train_loss: 0.1068, step time: 0.1047\n",
      "198/223, train_loss: 0.0976, step time: 0.1208\n",
      "199/223, train_loss: 0.1014, step time: 0.1097\n",
      "200/223, train_loss: 0.1021, step time: 0.1029\n",
      "201/223, train_loss: 0.0947, step time: 0.1199\n",
      "202/223, train_loss: 0.1063, step time: 0.1473\n",
      "203/223, train_loss: 0.0948, step time: 0.1122\n",
      "204/223, train_loss: 0.0914, step time: 0.1142\n",
      "205/223, train_loss: 0.0989, step time: 0.1158\n",
      "206/223, train_loss: 0.1022, step time: 0.0999\n",
      "207/223, train_loss: 0.1044, step time: 0.1063\n",
      "208/223, train_loss: 0.1097, step time: 0.1022\n",
      "209/223, train_loss: 0.0981, step time: 0.1145\n",
      "210/223, train_loss: 0.0966, step time: 0.1100\n",
      "211/223, train_loss: 0.0893, step time: 0.1053\n",
      "212/223, train_loss: 0.0975, step time: 0.1166\n",
      "213/223, train_loss: 0.1054, step time: 0.1184\n",
      "214/223, train_loss: 0.1002, step time: 0.1194\n",
      "215/223, train_loss: 0.0947, step time: 0.1305\n",
      "216/223, train_loss: 0.0984, step time: 0.1299\n",
      "217/223, train_loss: 0.0884, step time: 0.0998\n",
      "218/223, train_loss: 0.0895, step time: 0.1001\n",
      "219/223, train_loss: 0.0862, step time: 0.1005\n",
      "220/223, train_loss: 0.0917, step time: 0.1009\n",
      "221/223, train_loss: 0.0973, step time: 0.0985\n",
      "222/223, train_loss: 0.0961, step time: 0.0989\n",
      "223/223, train_loss: 0.1031, step time: 0.0992\n",
      "epoch 287 average loss: 0.0993\n",
      "time consuming of epoch 287 is: 87.9498\n",
      "----------\n",
      "epoch 288/300\n",
      "1/223, train_loss: 0.0862, step time: 0.1021\n",
      "2/223, train_loss: 0.1039, step time: 0.1007\n",
      "3/223, train_loss: 0.1033, step time: 0.1016\n",
      "4/223, train_loss: 0.0986, step time: 0.1324\n",
      "5/223, train_loss: 0.0947, step time: 0.1148\n",
      "6/223, train_loss: 0.0960, step time: 0.1157\n",
      "7/223, train_loss: 0.1054, step time: 0.1172\n",
      "8/223, train_loss: 0.0938, step time: 0.1353\n",
      "9/223, train_loss: 0.0889, step time: 0.1229\n",
      "10/223, train_loss: 0.0989, step time: 0.1145\n",
      "11/223, train_loss: 0.0886, step time: 0.1129\n",
      "12/223, train_loss: 0.1049, step time: 0.1196\n",
      "13/223, train_loss: 0.0923, step time: 0.1104\n",
      "14/223, train_loss: 0.0959, step time: 0.1108\n",
      "15/223, train_loss: 0.1034, step time: 0.1240\n",
      "16/223, train_loss: 0.0947, step time: 0.1109\n",
      "17/223, train_loss: 0.0931, step time: 0.1069\n",
      "18/223, train_loss: 0.0989, step time: 0.1076\n",
      "19/223, train_loss: 0.1025, step time: 0.1326\n",
      "20/223, train_loss: 0.1016, step time: 0.1226\n",
      "21/223, train_loss: 0.1012, step time: 0.1165\n",
      "22/223, train_loss: 0.0938, step time: 0.1019\n",
      "23/223, train_loss: 0.1066, step time: 0.1077\n",
      "24/223, train_loss: 0.1081, step time: 0.1220\n",
      "25/223, train_loss: 0.0898, step time: 0.1198\n",
      "26/223, train_loss: 0.0998, step time: 0.1083\n",
      "27/223, train_loss: 0.1017, step time: 0.1378\n",
      "28/223, train_loss: 0.1093, step time: 0.1210\n",
      "29/223, train_loss: 0.0932, step time: 0.1116\n",
      "30/223, train_loss: 0.0888, step time: 0.1106\n",
      "31/223, train_loss: 0.0940, step time: 0.1174\n",
      "32/223, train_loss: 0.2983, step time: 0.1279\n",
      "33/223, train_loss: 0.0946, step time: 0.1171\n",
      "34/223, train_loss: 0.1059, step time: 0.1073\n",
      "35/223, train_loss: 0.1080, step time: 0.1098\n",
      "36/223, train_loss: 0.0869, step time: 0.1127\n",
      "37/223, train_loss: 0.1070, step time: 0.1060\n",
      "38/223, train_loss: 0.1014, step time: 0.1156\n",
      "39/223, train_loss: 0.1006, step time: 0.0999\n",
      "40/223, train_loss: 0.0911, step time: 0.1165\n",
      "41/223, train_loss: 0.1065, step time: 0.1224\n",
      "42/223, train_loss: 0.1042, step time: 0.1096\n",
      "43/223, train_loss: 0.0924, step time: 0.1124\n",
      "44/223, train_loss: 0.1094, step time: 0.1009\n",
      "45/223, train_loss: 0.1059, step time: 0.1131\n",
      "46/223, train_loss: 0.0996, step time: 0.1123\n",
      "47/223, train_loss: 0.0945, step time: 0.1313\n",
      "48/223, train_loss: 0.0911, step time: 0.1101\n",
      "49/223, train_loss: 0.0880, step time: 0.1054\n",
      "50/223, train_loss: 0.0933, step time: 0.1250\n",
      "51/223, train_loss: 0.0931, step time: 0.1102\n",
      "52/223, train_loss: 0.0897, step time: 0.1102\n",
      "53/223, train_loss: 0.0915, step time: 0.1056\n",
      "54/223, train_loss: 0.0950, step time: 0.1060\n",
      "55/223, train_loss: 0.0918, step time: 0.1054\n",
      "56/223, train_loss: 0.0913, step time: 0.1042\n",
      "57/223, train_loss: 0.0920, step time: 0.1197\n",
      "58/223, train_loss: 0.0969, step time: 0.1318\n",
      "59/223, train_loss: 0.0936, step time: 0.1322\n",
      "60/223, train_loss: 0.0918, step time: 0.1309\n",
      "61/223, train_loss: 0.0955, step time: 0.1005\n",
      "62/223, train_loss: 0.0986, step time: 0.1078\n",
      "63/223, train_loss: 0.0850, step time: 0.1048\n",
      "64/223, train_loss: 0.1000, step time: 0.1088\n",
      "65/223, train_loss: 0.1041, step time: 0.1108\n",
      "66/223, train_loss: 0.0924, step time: 0.2024\n",
      "67/223, train_loss: 0.0951, step time: 0.1006\n",
      "68/223, train_loss: 0.0895, step time: 0.1425\n",
      "69/223, train_loss: 0.0998, step time: 0.1100\n",
      "70/223, train_loss: 0.1051, step time: 0.1022\n",
      "71/223, train_loss: 0.0904, step time: 0.1256\n",
      "72/223, train_loss: 0.1126, step time: 0.1268\n",
      "73/223, train_loss: 0.0943, step time: 0.1112\n",
      "74/223, train_loss: 0.0990, step time: 0.1001\n",
      "75/223, train_loss: 0.0924, step time: 0.1027\n",
      "76/223, train_loss: 0.1131, step time: 0.1324\n",
      "77/223, train_loss: 0.0926, step time: 0.1074\n",
      "78/223, train_loss: 0.0946, step time: 0.1228\n",
      "79/223, train_loss: 0.1089, step time: 0.1173\n",
      "80/223, train_loss: 0.0966, step time: 0.1257\n",
      "81/223, train_loss: 0.0922, step time: 0.1110\n",
      "82/223, train_loss: 0.1071, step time: 0.1144\n",
      "83/223, train_loss: 0.1132, step time: 0.1226\n",
      "84/223, train_loss: 0.0898, step time: 0.1004\n",
      "85/223, train_loss: 0.1050, step time: 0.1179\n",
      "86/223, train_loss: 0.0943, step time: 0.1057\n",
      "87/223, train_loss: 0.0964, step time: 0.1137\n",
      "88/223, train_loss: 0.1016, step time: 0.0998\n",
      "89/223, train_loss: 0.1014, step time: 0.1037\n",
      "90/223, train_loss: 0.1028, step time: 0.1159\n",
      "91/223, train_loss: 0.0872, step time: 0.1160\n",
      "92/223, train_loss: 0.1068, step time: 0.0998\n",
      "93/223, train_loss: 0.1003, step time: 0.1061\n",
      "94/223, train_loss: 0.0827, step time: 0.1053\n",
      "95/223, train_loss: 0.0899, step time: 0.1141\n",
      "96/223, train_loss: 0.0991, step time: 0.1202\n",
      "97/223, train_loss: 0.0981, step time: 0.1007\n",
      "98/223, train_loss: 0.0998, step time: 0.1113\n",
      "99/223, train_loss: 0.1019, step time: 0.1008\n",
      "100/223, train_loss: 0.0931, step time: 0.1418\n",
      "101/223, train_loss: 0.0984, step time: 0.1152\n",
      "102/223, train_loss: 0.0968, step time: 0.1114\n",
      "103/223, train_loss: 0.1033, step time: 0.0992\n",
      "104/223, train_loss: 0.1071, step time: 0.1005\n",
      "105/223, train_loss: 0.0893, step time: 0.1014\n",
      "106/223, train_loss: 0.1011, step time: 0.1005\n",
      "107/223, train_loss: 0.1013, step time: 0.1005\n",
      "108/223, train_loss: 0.0969, step time: 0.2085\n",
      "109/223, train_loss: 0.1206, step time: 0.1264\n",
      "110/223, train_loss: 0.0962, step time: 0.1056\n",
      "111/223, train_loss: 0.0997, step time: 0.1004\n",
      "112/223, train_loss: 0.0934, step time: 0.1569\n",
      "113/223, train_loss: 0.0890, step time: 0.1161\n",
      "114/223, train_loss: 0.0926, step time: 0.1112\n",
      "115/223, train_loss: 0.1015, step time: 0.1147\n",
      "116/223, train_loss: 0.1003, step time: 0.1076\n",
      "117/223, train_loss: 0.0902, step time: 0.1018\n",
      "118/223, train_loss: 0.1009, step time: 0.1352\n",
      "119/223, train_loss: 0.1061, step time: 0.1242\n",
      "120/223, train_loss: 0.0961, step time: 0.1014\n",
      "121/223, train_loss: 0.0948, step time: 0.1188\n",
      "122/223, train_loss: 0.0914, step time: 0.1210\n",
      "123/223, train_loss: 0.0930, step time: 0.0997\n",
      "124/223, train_loss: 0.1112, step time: 0.1006\n",
      "125/223, train_loss: 0.1076, step time: 0.1164\n",
      "126/223, train_loss: 0.1062, step time: 0.1262\n",
      "127/223, train_loss: 0.1012, step time: 0.1321\n",
      "128/223, train_loss: 0.1007, step time: 0.1114\n",
      "129/223, train_loss: 0.1017, step time: 0.1011\n",
      "130/223, train_loss: 0.0983, step time: 0.1325\n",
      "131/223, train_loss: 0.1008, step time: 0.1516\n",
      "132/223, train_loss: 0.0821, step time: 0.1165\n",
      "133/223, train_loss: 0.0876, step time: 0.1171\n",
      "134/223, train_loss: 0.0922, step time: 0.1151\n",
      "135/223, train_loss: 0.1047, step time: 0.1074\n",
      "136/223, train_loss: 0.0955, step time: 0.1022\n",
      "137/223, train_loss: 0.0963, step time: 0.1178\n",
      "138/223, train_loss: 0.1010, step time: 0.1133\n",
      "139/223, train_loss: 0.1025, step time: 0.1193\n",
      "140/223, train_loss: 0.0897, step time: 0.1169\n",
      "141/223, train_loss: 0.0922, step time: 0.1187\n",
      "142/223, train_loss: 0.0971, step time: 0.1221\n",
      "143/223, train_loss: 0.0947, step time: 0.1093\n",
      "144/223, train_loss: 0.1038, step time: 0.1301\n",
      "145/223, train_loss: 0.1022, step time: 0.1157\n",
      "146/223, train_loss: 0.0957, step time: 0.1228\n",
      "147/223, train_loss: 0.0978, step time: 0.1231\n",
      "148/223, train_loss: 0.0885, step time: 0.1147\n",
      "149/223, train_loss: 0.0925, step time: 0.1057\n",
      "150/223, train_loss: 0.1034, step time: 0.1068\n",
      "151/223, train_loss: 0.0945, step time: 0.1609\n",
      "152/223, train_loss: 0.1128, step time: 0.1102\n",
      "153/223, train_loss: 0.0986, step time: 0.1149\n",
      "154/223, train_loss: 0.1038, step time: 0.1029\n",
      "155/223, train_loss: 0.0944, step time: 0.1216\n",
      "156/223, train_loss: 0.0995, step time: 0.1177\n",
      "157/223, train_loss: 0.1022, step time: 0.1128\n",
      "158/223, train_loss: 0.0977, step time: 0.1142\n",
      "159/223, train_loss: 0.1035, step time: 0.1091\n",
      "160/223, train_loss: 0.1033, step time: 0.1242\n",
      "161/223, train_loss: 0.0935, step time: 0.1201\n",
      "162/223, train_loss: 0.0966, step time: 0.1299\n",
      "163/223, train_loss: 0.1027, step time: 0.1202\n",
      "164/223, train_loss: 0.0970, step time: 0.1060\n",
      "165/223, train_loss: 0.0877, step time: 0.1197\n",
      "166/223, train_loss: 0.0944, step time: 0.1304\n",
      "167/223, train_loss: 0.1024, step time: 0.1204\n",
      "168/223, train_loss: 0.1042, step time: 0.1063\n",
      "169/223, train_loss: 0.0914, step time: 0.1128\n",
      "170/223, train_loss: 0.0965, step time: 0.1065\n",
      "171/223, train_loss: 0.1000, step time: 0.1106\n",
      "172/223, train_loss: 0.1036, step time: 0.1095\n",
      "173/223, train_loss: 0.1012, step time: 0.1441\n",
      "174/223, train_loss: 0.1083, step time: 0.1194\n",
      "175/223, train_loss: 0.0944, step time: 0.1175\n",
      "176/223, train_loss: 0.0963, step time: 0.1046\n",
      "177/223, train_loss: 0.0978, step time: 0.1151\n",
      "178/223, train_loss: 0.0931, step time: 0.1027\n",
      "179/223, train_loss: 0.0996, step time: 0.1193\n",
      "180/223, train_loss: 0.0990, step time: 0.1139\n",
      "181/223, train_loss: 0.0995, step time: 0.1318\n",
      "182/223, train_loss: 0.0959, step time: 0.1417\n",
      "183/223, train_loss: 0.1096, step time: 0.1201\n",
      "184/223, train_loss: 0.0986, step time: 0.1063\n",
      "185/223, train_loss: 0.1031, step time: 0.1188\n",
      "186/223, train_loss: 0.0939, step time: 0.1362\n",
      "187/223, train_loss: 0.1003, step time: 0.1301\n",
      "188/223, train_loss: 0.1059, step time: 0.1289\n",
      "189/223, train_loss: 0.1163, step time: 0.1186\n",
      "190/223, train_loss: 0.1073, step time: 0.1016\n",
      "191/223, train_loss: 0.0951, step time: 0.1000\n",
      "192/223, train_loss: 0.1001, step time: 0.1039\n",
      "193/223, train_loss: 0.1030, step time: 0.1228\n",
      "194/223, train_loss: 0.1042, step time: 0.1246\n",
      "195/223, train_loss: 0.0936, step time: 0.1361\n",
      "196/223, train_loss: 0.1002, step time: 0.1014\n",
      "197/223, train_loss: 0.0944, step time: 0.1060\n",
      "198/223, train_loss: 0.0953, step time: 0.1161\n",
      "199/223, train_loss: 0.0970, step time: 0.1210\n",
      "200/223, train_loss: 0.0971, step time: 0.0999\n",
      "201/223, train_loss: 0.1173, step time: 0.1147\n",
      "202/223, train_loss: 0.1081, step time: 0.1239\n",
      "203/223, train_loss: 0.0931, step time: 0.1138\n",
      "204/223, train_loss: 0.0949, step time: 0.1145\n",
      "205/223, train_loss: 0.0972, step time: 0.1141\n",
      "206/223, train_loss: 0.1022, step time: 0.1057\n",
      "207/223, train_loss: 0.1049, step time: 0.1140\n",
      "208/223, train_loss: 0.1014, step time: 0.1206\n",
      "209/223, train_loss: 0.1023, step time: 0.1063\n",
      "210/223, train_loss: 0.0946, step time: 0.0995\n",
      "211/223, train_loss: 0.0997, step time: 0.1059\n",
      "212/223, train_loss: 0.0961, step time: 0.1202\n",
      "213/223, train_loss: 0.0979, step time: 0.1053\n",
      "214/223, train_loss: 0.0927, step time: 0.1057\n",
      "215/223, train_loss: 0.1045, step time: 0.1312\n",
      "216/223, train_loss: 0.0968, step time: 0.1129\n",
      "217/223, train_loss: 0.1068, step time: 0.1155\n",
      "218/223, train_loss: 0.1041, step time: 0.0995\n",
      "219/223, train_loss: 0.1018, step time: 0.0993\n",
      "220/223, train_loss: 0.1049, step time: 0.0999\n",
      "221/223, train_loss: 0.0991, step time: 0.0997\n",
      "222/223, train_loss: 0.0994, step time: 0.0997\n",
      "223/223, train_loss: 0.0911, step time: 0.0996\n",
      "epoch 288 average loss: 0.0994\n",
      "time consuming of epoch 288 is: 89.4783\n",
      "----------\n",
      "epoch 289/300\n",
      "1/223, train_loss: 0.0874, step time: 0.1015\n",
      "2/223, train_loss: 0.0901, step time: 0.1010\n",
      "3/223, train_loss: 0.0919, step time: 0.1023\n",
      "4/223, train_loss: 0.0987, step time: 0.1155\n",
      "5/223, train_loss: 0.0916, step time: 0.1384\n",
      "6/223, train_loss: 0.0902, step time: 0.1011\n",
      "7/223, train_loss: 0.1037, step time: 0.1011\n",
      "8/223, train_loss: 0.1010, step time: 0.1109\n",
      "9/223, train_loss: 0.1085, step time: 0.1035\n",
      "10/223, train_loss: 0.0881, step time: 0.1004\n",
      "11/223, train_loss: 0.1264, step time: 0.0984\n",
      "12/223, train_loss: 0.0928, step time: 0.1003\n",
      "13/223, train_loss: 0.1065, step time: 0.1108\n",
      "14/223, train_loss: 0.0939, step time: 0.0994\n",
      "15/223, train_loss: 0.0972, step time: 0.0991\n",
      "16/223, train_loss: 0.0956, step time: 0.1002\n",
      "17/223, train_loss: 0.0954, step time: 0.1011\n",
      "18/223, train_loss: 0.0982, step time: 0.1000\n",
      "19/223, train_loss: 0.1092, step time: 0.0993\n",
      "20/223, train_loss: 0.0897, step time: 0.0993\n",
      "21/223, train_loss: 0.1044, step time: 0.1002\n",
      "22/223, train_loss: 0.1026, step time: 0.0999\n",
      "23/223, train_loss: 0.1079, step time: 0.1004\n",
      "24/223, train_loss: 0.0903, step time: 0.1012\n",
      "25/223, train_loss: 0.2923, step time: 0.1178\n",
      "26/223, train_loss: 0.0938, step time: 0.1000\n",
      "27/223, train_loss: 0.0992, step time: 0.1132\n",
      "28/223, train_loss: 0.0924, step time: 0.0990\n",
      "29/223, train_loss: 0.1017, step time: 0.1172\n",
      "30/223, train_loss: 0.0948, step time: 0.1158\n",
      "31/223, train_loss: 0.1033, step time: 0.1332\n",
      "32/223, train_loss: 0.0977, step time: 0.1192\n",
      "33/223, train_loss: 0.0941, step time: 0.2085\n",
      "34/223, train_loss: 0.1090, step time: 0.1117\n",
      "35/223, train_loss: 0.0947, step time: 0.1003\n",
      "36/223, train_loss: 0.0921, step time: 0.1206\n",
      "37/223, train_loss: 0.0936, step time: 0.1152\n",
      "38/223, train_loss: 0.0946, step time: 0.1321\n",
      "39/223, train_loss: 0.1133, step time: 0.1408\n",
      "40/223, train_loss: 0.1068, step time: 0.1010\n",
      "41/223, train_loss: 0.1116, step time: 0.1390\n",
      "42/223, train_loss: 0.0935, step time: 0.1025\n",
      "43/223, train_loss: 0.0967, step time: 0.1135\n",
      "44/223, train_loss: 0.1046, step time: 0.1027\n",
      "45/223, train_loss: 0.1002, step time: 0.1015\n",
      "46/223, train_loss: 0.0957, step time: 0.1122\n",
      "47/223, train_loss: 0.0965, step time: 0.1049\n",
      "48/223, train_loss: 0.0919, step time: 0.1175\n",
      "49/223, train_loss: 0.0938, step time: 0.0996\n",
      "50/223, train_loss: 0.0974, step time: 0.1180\n",
      "51/223, train_loss: 0.1039, step time: 0.1143\n",
      "52/223, train_loss: 0.0964, step time: 0.0998\n",
      "53/223, train_loss: 0.0872, step time: 0.0991\n",
      "54/223, train_loss: 0.0877, step time: 0.1104\n",
      "55/223, train_loss: 0.1009, step time: 0.1036\n",
      "56/223, train_loss: 0.0987, step time: 0.1137\n",
      "57/223, train_loss: 0.1063, step time: 0.1088\n",
      "58/223, train_loss: 0.1019, step time: 0.1152\n",
      "59/223, train_loss: 0.1002, step time: 0.1054\n",
      "60/223, train_loss: 0.0930, step time: 0.1006\n",
      "61/223, train_loss: 0.0934, step time: 0.1138\n",
      "62/223, train_loss: 0.0953, step time: 0.1097\n",
      "63/223, train_loss: 0.0970, step time: 0.1034\n",
      "64/223, train_loss: 0.0943, step time: 0.1103\n",
      "65/223, train_loss: 0.0956, step time: 0.1006\n",
      "66/223, train_loss: 0.1062, step time: 0.1120\n",
      "67/223, train_loss: 0.1071, step time: 0.1010\n",
      "68/223, train_loss: 0.0912, step time: 0.1010\n",
      "69/223, train_loss: 0.0910, step time: 0.1005\n",
      "70/223, train_loss: 0.1106, step time: 0.1156\n",
      "71/223, train_loss: 0.1045, step time: 0.1008\n",
      "72/223, train_loss: 0.1020, step time: 0.1035\n",
      "73/223, train_loss: 0.0922, step time: 0.1028\n",
      "74/223, train_loss: 0.1014, step time: 0.1080\n",
      "75/223, train_loss: 0.0992, step time: 0.1212\n",
      "76/223, train_loss: 0.0983, step time: 0.1007\n",
      "77/223, train_loss: 0.0977, step time: 0.1038\n",
      "78/223, train_loss: 0.1039, step time: 0.1469\n",
      "79/223, train_loss: 0.0966, step time: 0.1228\n",
      "80/223, train_loss: 0.1003, step time: 0.1102\n",
      "81/223, train_loss: 0.0985, step time: 0.1005\n",
      "82/223, train_loss: 0.0956, step time: 0.1174\n",
      "83/223, train_loss: 0.0994, step time: 0.1194\n",
      "84/223, train_loss: 0.0853, step time: 0.1008\n",
      "85/223, train_loss: 0.0893, step time: 0.1179\n",
      "86/223, train_loss: 0.0955, step time: 0.1142\n",
      "87/223, train_loss: 0.1066, step time: 0.1001\n",
      "88/223, train_loss: 0.1009, step time: 0.0999\n",
      "89/223, train_loss: 0.0944, step time: 0.1013\n",
      "90/223, train_loss: 0.0991, step time: 0.1068\n",
      "91/223, train_loss: 0.0916, step time: 0.1151\n",
      "92/223, train_loss: 0.1142, step time: 0.1118\n",
      "93/223, train_loss: 0.1030, step time: 0.1334\n",
      "94/223, train_loss: 0.0982, step time: 0.1146\n",
      "95/223, train_loss: 0.0955, step time: 0.1008\n",
      "96/223, train_loss: 0.0901, step time: 0.1140\n",
      "97/223, train_loss: 0.1084, step time: 0.1246\n",
      "98/223, train_loss: 0.0990, step time: 0.1103\n",
      "99/223, train_loss: 0.1043, step time: 0.1202\n",
      "100/223, train_loss: 0.0973, step time: 0.1209\n",
      "101/223, train_loss: 0.1089, step time: 0.1002\n",
      "102/223, train_loss: 0.0969, step time: 0.1245\n",
      "103/223, train_loss: 0.1129, step time: 0.1063\n",
      "104/223, train_loss: 0.1045, step time: 0.1113\n",
      "105/223, train_loss: 0.0976, step time: 0.1127\n",
      "106/223, train_loss: 0.0989, step time: 0.1230\n",
      "107/223, train_loss: 0.0947, step time: 0.1129\n",
      "108/223, train_loss: 0.1195, step time: 0.1360\n",
      "109/223, train_loss: 0.1104, step time: 0.1151\n",
      "110/223, train_loss: 0.0980, step time: 0.1117\n",
      "111/223, train_loss: 0.0915, step time: 0.1157\n",
      "112/223, train_loss: 0.0912, step time: 0.1161\n",
      "113/223, train_loss: 0.0962, step time: 0.1102\n",
      "114/223, train_loss: 0.0962, step time: 0.1165\n",
      "115/223, train_loss: 0.1096, step time: 0.1123\n",
      "116/223, train_loss: 0.1067, step time: 0.1245\n",
      "117/223, train_loss: 0.1004, step time: 0.1201\n",
      "118/223, train_loss: 0.0873, step time: 0.1178\n",
      "119/223, train_loss: 0.0892, step time: 0.1353\n",
      "120/223, train_loss: 0.1026, step time: 0.1094\n",
      "121/223, train_loss: 0.0934, step time: 0.1159\n",
      "122/223, train_loss: 0.0960, step time: 0.1150\n",
      "123/223, train_loss: 0.1073, step time: 0.1245\n",
      "124/223, train_loss: 0.1016, step time: 0.1247\n",
      "125/223, train_loss: 0.0985, step time: 0.0999\n",
      "126/223, train_loss: 0.1048, step time: 0.1325\n",
      "127/223, train_loss: 0.0973, step time: 0.1117\n",
      "128/223, train_loss: 0.1008, step time: 0.1188\n",
      "129/223, train_loss: 0.0934, step time: 0.1057\n",
      "130/223, train_loss: 0.1068, step time: 0.1088\n",
      "131/223, train_loss: 0.1014, step time: 0.0992\n",
      "132/223, train_loss: 0.0984, step time: 0.1001\n",
      "133/223, train_loss: 0.0985, step time: 0.1237\n",
      "134/223, train_loss: 0.0945, step time: 0.1180\n",
      "135/223, train_loss: 0.0881, step time: 0.1185\n",
      "136/223, train_loss: 0.1027, step time: 0.1000\n",
      "137/223, train_loss: 0.0975, step time: 0.1065\n",
      "138/223, train_loss: 0.0980, step time: 0.1043\n",
      "139/223, train_loss: 0.0997, step time: 0.1199\n",
      "140/223, train_loss: 0.0979, step time: 0.1334\n",
      "141/223, train_loss: 0.1091, step time: 0.1466\n",
      "142/223, train_loss: 0.0938, step time: 0.1167\n",
      "143/223, train_loss: 0.0974, step time: 0.0997\n",
      "144/223, train_loss: 0.1007, step time: 0.1295\n",
      "145/223, train_loss: 0.0974, step time: 0.1271\n",
      "146/223, train_loss: 0.0948, step time: 0.1088\n",
      "147/223, train_loss: 0.0975, step time: 0.1253\n",
      "148/223, train_loss: 0.0924, step time: 0.1092\n",
      "149/223, train_loss: 0.0976, step time: 0.1007\n",
      "150/223, train_loss: 0.0923, step time: 0.1004\n",
      "151/223, train_loss: 0.0946, step time: 0.1041\n",
      "152/223, train_loss: 0.1011, step time: 0.0997\n",
      "153/223, train_loss: 0.1014, step time: 0.1031\n",
      "154/223, train_loss: 0.1135, step time: 0.1005\n",
      "155/223, train_loss: 0.0963, step time: 0.1267\n",
      "156/223, train_loss: 0.1091, step time: 0.1168\n",
      "157/223, train_loss: 0.1076, step time: 0.1592\n",
      "158/223, train_loss: 0.0927, step time: 0.0997\n",
      "159/223, train_loss: 0.1112, step time: 0.1085\n",
      "160/223, train_loss: 0.1004, step time: 0.1246\n",
      "161/223, train_loss: 0.1062, step time: 0.1481\n",
      "162/223, train_loss: 0.0899, step time: 0.1106\n",
      "163/223, train_loss: 0.0889, step time: 0.1136\n",
      "164/223, train_loss: 0.0983, step time: 0.1277\n",
      "165/223, train_loss: 0.0938, step time: 0.1289\n",
      "166/223, train_loss: 0.1033, step time: 0.1425\n",
      "167/223, train_loss: 0.0963, step time: 0.1267\n",
      "168/223, train_loss: 0.1081, step time: 0.1211\n",
      "169/223, train_loss: 0.0954, step time: 0.1107\n",
      "170/223, train_loss: 0.0985, step time: 0.1106\n",
      "171/223, train_loss: 0.0911, step time: 0.1286\n",
      "172/223, train_loss: 0.1073, step time: 0.1124\n",
      "173/223, train_loss: 0.1116, step time: 0.1246\n",
      "174/223, train_loss: 0.0900, step time: 0.1095\n",
      "175/223, train_loss: 0.1028, step time: 0.1051\n",
      "176/223, train_loss: 0.0945, step time: 0.1283\n",
      "177/223, train_loss: 0.1033, step time: 0.1042\n",
      "178/223, train_loss: 0.0907, step time: 0.1131\n",
      "179/223, train_loss: 0.1049, step time: 0.1372\n",
      "180/223, train_loss: 0.1061, step time: 0.1052\n",
      "181/223, train_loss: 0.0965, step time: 0.1048\n",
      "182/223, train_loss: 0.0969, step time: 0.1172\n",
      "183/223, train_loss: 0.1090, step time: 0.1077\n",
      "184/223, train_loss: 0.0921, step time: 0.1031\n",
      "185/223, train_loss: 0.1007, step time: 0.1376\n",
      "186/223, train_loss: 0.0953, step time: 0.1001\n",
      "187/223, train_loss: 0.0939, step time: 0.0999\n",
      "188/223, train_loss: 0.1049, step time: 0.1002\n",
      "189/223, train_loss: 0.0988, step time: 0.1114\n",
      "190/223, train_loss: 0.0957, step time: 0.1151\n",
      "191/223, train_loss: 0.0897, step time: 0.1157\n",
      "192/223, train_loss: 0.1067, step time: 0.1533\n",
      "193/223, train_loss: 0.1073, step time: 0.1027\n",
      "194/223, train_loss: 0.1010, step time: 0.1169\n",
      "195/223, train_loss: 0.0937, step time: 0.1118\n",
      "196/223, train_loss: 0.0929, step time: 0.1044\n",
      "197/223, train_loss: 0.0979, step time: 0.1083\n",
      "198/223, train_loss: 0.1125, step time: 0.1127\n",
      "199/223, train_loss: 0.1007, step time: 0.1107\n",
      "200/223, train_loss: 0.1055, step time: 0.1095\n",
      "201/223, train_loss: 0.0965, step time: 0.0998\n",
      "202/223, train_loss: 0.0969, step time: 0.0988\n",
      "203/223, train_loss: 0.0895, step time: 0.1146\n",
      "204/223, train_loss: 0.0872, step time: 0.1003\n",
      "205/223, train_loss: 0.0961, step time: 0.1153\n",
      "206/223, train_loss: 0.0954, step time: 0.1120\n",
      "207/223, train_loss: 0.0973, step time: 0.1150\n",
      "208/223, train_loss: 0.1110, step time: 0.1039\n",
      "209/223, train_loss: 0.0862, step time: 0.1152\n",
      "210/223, train_loss: 0.0937, step time: 0.1189\n",
      "211/223, train_loss: 0.1110, step time: 0.1009\n",
      "212/223, train_loss: 0.0995, step time: 0.1052\n",
      "213/223, train_loss: 0.0961, step time: 0.1265\n",
      "214/223, train_loss: 0.0937, step time: 0.1185\n",
      "215/223, train_loss: 0.0855, step time: 0.1086\n",
      "216/223, train_loss: 0.0867, step time: 0.1165\n",
      "217/223, train_loss: 0.1032, step time: 0.1023\n",
      "218/223, train_loss: 0.1025, step time: 0.0989\n",
      "219/223, train_loss: 0.0992, step time: 0.1000\n",
      "220/223, train_loss: 0.1031, step time: 0.1010\n",
      "221/223, train_loss: 0.0905, step time: 0.1009\n",
      "222/223, train_loss: 0.0955, step time: 0.1000\n",
      "223/223, train_loss: 0.1051, step time: 0.1012\n",
      "epoch 289 average loss: 0.0996\n",
      "time consuming of epoch 289 is: 93.3256\n",
      "----------\n",
      "epoch 290/300\n",
      "1/223, train_loss: 0.1077, step time: 0.1145\n",
      "2/223, train_loss: 0.1062, step time: 0.1006\n",
      "3/223, train_loss: 0.1044, step time: 0.1005\n",
      "4/223, train_loss: 0.0912, step time: 0.1015\n",
      "5/223, train_loss: 0.1050, step time: 0.1044\n",
      "6/223, train_loss: 0.0994, step time: 0.1312\n",
      "7/223, train_loss: 0.0913, step time: 0.1209\n",
      "8/223, train_loss: 0.0988, step time: 0.1107\n",
      "9/223, train_loss: 0.1059, step time: 0.1145\n",
      "10/223, train_loss: 0.1017, step time: 0.1003\n",
      "11/223, train_loss: 0.0896, step time: 0.1156\n",
      "12/223, train_loss: 0.0935, step time: 0.1056\n",
      "13/223, train_loss: 0.0906, step time: 0.1084\n",
      "14/223, train_loss: 0.1034, step time: 0.1252\n",
      "15/223, train_loss: 0.0999, step time: 0.1293\n",
      "16/223, train_loss: 0.0974, step time: 0.1009\n",
      "17/223, train_loss: 0.1094, step time: 0.1005\n",
      "18/223, train_loss: 0.1042, step time: 0.1195\n",
      "19/223, train_loss: 0.1027, step time: 0.1111\n",
      "20/223, train_loss: 0.1091, step time: 0.1158\n",
      "21/223, train_loss: 0.1039, step time: 0.1073\n",
      "22/223, train_loss: 0.0985, step time: 0.1291\n",
      "23/223, train_loss: 0.1046, step time: 0.1782\n",
      "24/223, train_loss: 0.0943, step time: 0.1182\n",
      "25/223, train_loss: 0.0933, step time: 0.1204\n",
      "26/223, train_loss: 0.1040, step time: 0.1263\n",
      "27/223, train_loss: 0.0917, step time: 0.1158\n",
      "28/223, train_loss: 0.0971, step time: 0.1006\n",
      "29/223, train_loss: 0.0903, step time: 0.1038\n",
      "30/223, train_loss: 0.0968, step time: 0.1174\n",
      "31/223, train_loss: 0.0967, step time: 0.1264\n",
      "32/223, train_loss: 0.1086, step time: 0.1002\n",
      "33/223, train_loss: 0.0999, step time: 0.0997\n",
      "34/223, train_loss: 0.1056, step time: 0.1058\n",
      "35/223, train_loss: 0.1036, step time: 0.1270\n",
      "36/223, train_loss: 0.1102, step time: 0.1112\n",
      "37/223, train_loss: 0.0933, step time: 0.1037\n",
      "38/223, train_loss: 0.0930, step time: 0.1010\n",
      "39/223, train_loss: 0.1056, step time: 0.1172\n",
      "40/223, train_loss: 0.0924, step time: 0.1355\n",
      "41/223, train_loss: 0.0983, step time: 0.1013\n",
      "42/223, train_loss: 0.1016, step time: 0.1000\n",
      "43/223, train_loss: 0.0883, step time: 0.1377\n",
      "44/223, train_loss: 0.1010, step time: 0.1144\n",
      "45/223, train_loss: 0.0925, step time: 0.1084\n",
      "46/223, train_loss: 0.0889, step time: 0.1010\n",
      "47/223, train_loss: 0.0997, step time: 0.1010\n",
      "48/223, train_loss: 0.1022, step time: 0.1000\n",
      "49/223, train_loss: 0.0991, step time: 0.1253\n",
      "50/223, train_loss: 0.0990, step time: 0.1062\n",
      "51/223, train_loss: 0.0852, step time: 0.1003\n",
      "52/223, train_loss: 0.0893, step time: 0.1000\n",
      "53/223, train_loss: 0.0952, step time: 0.1042\n",
      "54/223, train_loss: 0.0963, step time: 0.1102\n",
      "55/223, train_loss: 0.1018, step time: 0.1087\n",
      "56/223, train_loss: 0.0888, step time: 0.1074\n",
      "57/223, train_loss: 0.0908, step time: 0.1001\n",
      "58/223, train_loss: 0.0934, step time: 0.1152\n",
      "59/223, train_loss: 0.1093, step time: 0.1004\n",
      "60/223, train_loss: 0.1171, step time: 0.1280\n",
      "61/223, train_loss: 0.1004, step time: 0.1173\n",
      "62/223, train_loss: 0.0957, step time: 0.1210\n",
      "63/223, train_loss: 0.0959, step time: 0.1101\n",
      "64/223, train_loss: 0.1125, step time: 0.1008\n",
      "65/223, train_loss: 0.0856, step time: 0.1124\n",
      "66/223, train_loss: 0.0905, step time: 0.1030\n",
      "67/223, train_loss: 0.1045, step time: 0.1186\n",
      "68/223, train_loss: 0.1078, step time: 0.1256\n",
      "69/223, train_loss: 0.0969, step time: 0.1134\n",
      "70/223, train_loss: 0.0959, step time: 0.1061\n",
      "71/223, train_loss: 0.0952, step time: 0.1100\n",
      "72/223, train_loss: 0.0940, step time: 0.1074\n",
      "73/223, train_loss: 0.0939, step time: 0.1244\n",
      "74/223, train_loss: 0.0942, step time: 0.1099\n",
      "75/223, train_loss: 0.1124, step time: 0.1002\n",
      "76/223, train_loss: 0.0904, step time: 0.1001\n",
      "77/223, train_loss: 0.0974, step time: 0.1116\n",
      "78/223, train_loss: 0.0951, step time: 0.1390\n",
      "79/223, train_loss: 0.1053, step time: 0.1059\n",
      "80/223, train_loss: 0.0871, step time: 0.1001\n",
      "81/223, train_loss: 0.0852, step time: 0.1054\n",
      "82/223, train_loss: 0.0950, step time: 0.1079\n",
      "83/223, train_loss: 0.0907, step time: 0.1115\n",
      "84/223, train_loss: 0.1035, step time: 0.1066\n",
      "85/223, train_loss: 0.1019, step time: 0.1062\n",
      "86/223, train_loss: 0.0991, step time: 0.1163\n",
      "87/223, train_loss: 0.0996, step time: 0.1096\n",
      "88/223, train_loss: 0.1122, step time: 0.1193\n",
      "89/223, train_loss: 0.0998, step time: 0.1127\n",
      "90/223, train_loss: 0.0991, step time: 0.1117\n",
      "91/223, train_loss: 0.1088, step time: 0.1150\n",
      "92/223, train_loss: 0.0833, step time: 0.1616\n",
      "93/223, train_loss: 0.0974, step time: 0.1192\n",
      "94/223, train_loss: 0.0977, step time: 0.1232\n",
      "95/223, train_loss: 0.1055, step time: 0.1098\n",
      "96/223, train_loss: 0.0934, step time: 0.1006\n",
      "97/223, train_loss: 0.0864, step time: 0.1098\n",
      "98/223, train_loss: 0.1056, step time: 0.1230\n",
      "99/223, train_loss: 0.0941, step time: 0.1052\n",
      "100/223, train_loss: 0.0985, step time: 0.1007\n",
      "101/223, train_loss: 0.1041, step time: 0.1149\n",
      "102/223, train_loss: 0.0885, step time: 0.1403\n",
      "103/223, train_loss: 0.0913, step time: 0.1042\n",
      "104/223, train_loss: 0.0907, step time: 0.1309\n",
      "105/223, train_loss: 0.0991, step time: 0.1153\n",
      "106/223, train_loss: 0.0957, step time: 0.0999\n",
      "107/223, train_loss: 0.0953, step time: 0.1009\n",
      "108/223, train_loss: 0.0994, step time: 0.1212\n",
      "109/223, train_loss: 0.0995, step time: 0.1141\n",
      "110/223, train_loss: 0.1026, step time: 0.1002\n",
      "111/223, train_loss: 0.1165, step time: 0.1042\n",
      "112/223, train_loss: 0.0942, step time: 0.1056\n",
      "113/223, train_loss: 0.1153, step time: 0.1265\n",
      "114/223, train_loss: 0.1002, step time: 0.1084\n",
      "115/223, train_loss: 0.0915, step time: 0.1095\n",
      "116/223, train_loss: 0.0992, step time: 0.1224\n",
      "117/223, train_loss: 0.1020, step time: 0.1151\n",
      "118/223, train_loss: 0.1014, step time: 0.1150\n",
      "119/223, train_loss: 0.0971, step time: 0.1112\n",
      "120/223, train_loss: 0.1033, step time: 0.1183\n",
      "121/223, train_loss: 0.0971, step time: 0.1335\n",
      "122/223, train_loss: 0.0992, step time: 0.1072\n",
      "123/223, train_loss: 0.0969, step time: 0.1012\n",
      "124/223, train_loss: 0.1047, step time: 0.1084\n",
      "125/223, train_loss: 0.0975, step time: 0.1345\n",
      "126/223, train_loss: 0.0977, step time: 0.1231\n",
      "127/223, train_loss: 0.0957, step time: 0.1161\n",
      "128/223, train_loss: 0.0992, step time: 0.1061\n",
      "129/223, train_loss: 0.1078, step time: 0.1032\n",
      "130/223, train_loss: 0.0912, step time: 0.1005\n",
      "131/223, train_loss: 0.1062, step time: 0.1021\n",
      "132/223, train_loss: 0.0967, step time: 0.1361\n",
      "133/223, train_loss: 0.0946, step time: 0.1147\n",
      "134/223, train_loss: 0.1045, step time: 0.1551\n",
      "135/223, train_loss: 0.0876, step time: 0.1699\n",
      "136/223, train_loss: 0.1002, step time: 0.1001\n",
      "137/223, train_loss: 0.0982, step time: 0.1100\n",
      "138/223, train_loss: 0.0959, step time: 0.1019\n",
      "139/223, train_loss: 0.0962, step time: 0.1069\n",
      "140/223, train_loss: 0.0997, step time: 0.1098\n",
      "141/223, train_loss: 0.1032, step time: 0.1045\n",
      "142/223, train_loss: 0.2932, step time: 0.0992\n",
      "143/223, train_loss: 0.0891, step time: 0.1088\n",
      "144/223, train_loss: 0.0991, step time: 0.1043\n",
      "145/223, train_loss: 0.0934, step time: 0.1116\n",
      "146/223, train_loss: 0.0923, step time: 0.1239\n",
      "147/223, train_loss: 0.1206, step time: 0.1261\n",
      "148/223, train_loss: 0.0999, step time: 0.1041\n",
      "149/223, train_loss: 0.1027, step time: 0.1191\n",
      "150/223, train_loss: 0.1008, step time: 0.1154\n",
      "151/223, train_loss: 0.1022, step time: 0.1116\n",
      "152/223, train_loss: 0.0887, step time: 0.1025\n",
      "153/223, train_loss: 0.0920, step time: 0.1127\n",
      "154/223, train_loss: 0.0981, step time: 0.1052\n",
      "155/223, train_loss: 0.0929, step time: 0.1367\n",
      "156/223, train_loss: 0.1100, step time: 0.1214\n",
      "157/223, train_loss: 0.1024, step time: 0.1090\n",
      "158/223, train_loss: 0.1054, step time: 0.1136\n",
      "159/223, train_loss: 0.0917, step time: 0.1101\n",
      "160/223, train_loss: 0.0951, step time: 0.1062\n",
      "161/223, train_loss: 0.1069, step time: 0.1202\n",
      "162/223, train_loss: 0.0932, step time: 0.1171\n",
      "163/223, train_loss: 0.0943, step time: 0.1109\n",
      "164/223, train_loss: 0.1050, step time: 0.1119\n",
      "165/223, train_loss: 0.0986, step time: 0.1104\n",
      "166/223, train_loss: 0.0889, step time: 0.1198\n",
      "167/223, train_loss: 0.0970, step time: 0.1499\n",
      "168/223, train_loss: 0.0999, step time: 0.1005\n",
      "169/223, train_loss: 0.0921, step time: 0.1170\n",
      "170/223, train_loss: 0.1045, step time: 0.1009\n",
      "171/223, train_loss: 0.0948, step time: 0.1204\n",
      "172/223, train_loss: 0.1130, step time: 0.1066\n",
      "173/223, train_loss: 0.1020, step time: 0.1007\n",
      "174/223, train_loss: 0.0974, step time: 0.1112\n",
      "175/223, train_loss: 0.1073, step time: 0.1096\n",
      "176/223, train_loss: 0.0982, step time: 0.1109\n",
      "177/223, train_loss: 0.1022, step time: 0.1050\n",
      "178/223, train_loss: 0.0990, step time: 0.1068\n",
      "179/223, train_loss: 0.0887, step time: 0.1574\n",
      "180/223, train_loss: 0.0950, step time: 0.1277\n",
      "181/223, train_loss: 0.0970, step time: 0.1000\n",
      "182/223, train_loss: 0.0929, step time: 0.1105\n",
      "183/223, train_loss: 0.0953, step time: 0.1052\n",
      "184/223, train_loss: 0.0986, step time: 0.1488\n",
      "185/223, train_loss: 0.0947, step time: 0.1074\n",
      "186/223, train_loss: 0.0935, step time: 0.1323\n",
      "187/223, train_loss: 0.0919, step time: 0.1318\n",
      "188/223, train_loss: 0.1022, step time: 0.1321\n",
      "189/223, train_loss: 0.0896, step time: 0.1006\n",
      "190/223, train_loss: 0.0922, step time: 0.1034\n",
      "191/223, train_loss: 0.1010, step time: 0.1006\n",
      "192/223, train_loss: 0.0908, step time: 0.1032\n",
      "193/223, train_loss: 0.1057, step time: 0.1006\n",
      "194/223, train_loss: 0.1050, step time: 0.1004\n",
      "195/223, train_loss: 0.1044, step time: 0.1008\n",
      "196/223, train_loss: 0.0979, step time: 0.1000\n",
      "197/223, train_loss: 0.1028, step time: 0.0999\n",
      "198/223, train_loss: 0.0981, step time: 0.0998\n",
      "199/223, train_loss: 0.0984, step time: 0.0988\n",
      "200/223, train_loss: 0.1014, step time: 0.0995\n",
      "201/223, train_loss: 0.0889, step time: 0.1002\n",
      "202/223, train_loss: 0.1110, step time: 0.1004\n",
      "203/223, train_loss: 0.1037, step time: 0.0997\n",
      "204/223, train_loss: 0.0958, step time: 0.1005\n",
      "205/223, train_loss: 0.0984, step time: 0.1017\n",
      "206/223, train_loss: 0.1069, step time: 0.1018\n",
      "207/223, train_loss: 0.1076, step time: 0.1115\n",
      "208/223, train_loss: 0.0929, step time: 0.1053\n",
      "209/223, train_loss: 0.0944, step time: 0.1120\n",
      "210/223, train_loss: 0.0917, step time: 0.1037\n",
      "211/223, train_loss: 0.1117, step time: 0.1006\n",
      "212/223, train_loss: 0.1034, step time: 0.1006\n",
      "213/223, train_loss: 0.0906, step time: 0.1012\n",
      "214/223, train_loss: 0.1028, step time: 0.1056\n",
      "215/223, train_loss: 0.0935, step time: 0.1117\n",
      "216/223, train_loss: 0.1069, step time: 0.1086\n",
      "217/223, train_loss: 0.0894, step time: 0.1024\n",
      "218/223, train_loss: 0.0921, step time: 0.1015\n",
      "219/223, train_loss: 0.1011, step time: 0.1003\n",
      "220/223, train_loss: 0.1031, step time: 0.1003\n",
      "221/223, train_loss: 0.1066, step time: 0.0998\n",
      "222/223, train_loss: 0.0969, step time: 0.1019\n",
      "223/223, train_loss: 0.0956, step time: 0.0986\n",
      "epoch 290 average loss: 0.0995\n",
      "current epoch: 290 current mean dice: 0.8621 tc: 0.9225 wt: 0.8718 et: 0.7920\n",
      "best mean dice: 0.8622 at epoch: 260\n",
      "time consuming of epoch 290 is: 94.9621\n",
      "----------\n",
      "epoch 291/300\n",
      "1/223, train_loss: 0.0995, step time: 0.1015\n",
      "2/223, train_loss: 0.0939, step time: 0.1135\n",
      "3/223, train_loss: 0.1071, step time: 0.1020\n",
      "4/223, train_loss: 0.0919, step time: 0.1001\n",
      "5/223, train_loss: 0.1056, step time: 0.1093\n",
      "6/223, train_loss: 0.0937, step time: 0.1122\n",
      "7/223, train_loss: 0.0913, step time: 0.1103\n",
      "8/223, train_loss: 0.0988, step time: 0.1001\n",
      "9/223, train_loss: 0.0830, step time: 0.1131\n",
      "10/223, train_loss: 0.1060, step time: 0.1124\n",
      "11/223, train_loss: 0.1062, step time: 0.1347\n",
      "12/223, train_loss: 0.0874, step time: 0.1188\n",
      "13/223, train_loss: 0.0948, step time: 0.1071\n",
      "14/223, train_loss: 0.0931, step time: 0.1192\n",
      "15/223, train_loss: 0.1047, step time: 0.1346\n",
      "16/223, train_loss: 0.1007, step time: 0.1143\n",
      "17/223, train_loss: 0.0954, step time: 0.1001\n",
      "18/223, train_loss: 0.0963, step time: 0.1069\n",
      "19/223, train_loss: 0.0974, step time: 0.1086\n",
      "20/223, train_loss: 0.1010, step time: 0.1208\n",
      "21/223, train_loss: 0.0972, step time: 0.1086\n",
      "22/223, train_loss: 0.1022, step time: 0.1472\n",
      "23/223, train_loss: 0.0915, step time: 0.1113\n",
      "24/223, train_loss: 0.1055, step time: 0.1010\n",
      "25/223, train_loss: 0.0999, step time: 0.1061\n",
      "26/223, train_loss: 0.0914, step time: 0.1009\n",
      "27/223, train_loss: 0.1099, step time: 0.1087\n",
      "28/223, train_loss: 0.0959, step time: 0.1182\n",
      "29/223, train_loss: 0.0927, step time: 0.1058\n",
      "30/223, train_loss: 0.0952, step time: 0.1113\n",
      "31/223, train_loss: 0.1006, step time: 0.1467\n",
      "32/223, train_loss: 0.1045, step time: 0.1010\n",
      "33/223, train_loss: 0.0970, step time: 0.1178\n",
      "34/223, train_loss: 0.1152, step time: 0.1144\n",
      "35/223, train_loss: 0.0959, step time: 0.1002\n",
      "36/223, train_loss: 0.0901, step time: 0.1005\n",
      "37/223, train_loss: 0.0940, step time: 0.1073\n",
      "38/223, train_loss: 0.0959, step time: 0.1000\n",
      "39/223, train_loss: 0.0919, step time: 0.1164\n",
      "40/223, train_loss: 0.1034, step time: 0.1176\n",
      "41/223, train_loss: 0.1024, step time: 0.1001\n",
      "42/223, train_loss: 0.0925, step time: 0.1000\n",
      "43/223, train_loss: 0.1035, step time: 0.1003\n",
      "44/223, train_loss: 0.0984, step time: 0.1006\n",
      "45/223, train_loss: 0.1071, step time: 0.1023\n",
      "46/223, train_loss: 0.1025, step time: 0.1132\n",
      "47/223, train_loss: 0.0992, step time: 0.1001\n",
      "48/223, train_loss: 0.1030, step time: 0.1008\n",
      "49/223, train_loss: 0.0897, step time: 0.1096\n",
      "50/223, train_loss: 0.1006, step time: 0.0997\n",
      "51/223, train_loss: 0.0966, step time: 0.1016\n",
      "52/223, train_loss: 0.0908, step time: 0.1034\n",
      "53/223, train_loss: 0.0964, step time: 0.1089\n",
      "54/223, train_loss: 0.0999, step time: 0.1456\n",
      "55/223, train_loss: 0.1086, step time: 0.1385\n",
      "56/223, train_loss: 0.1059, step time: 0.1287\n",
      "57/223, train_loss: 0.1078, step time: 0.1080\n",
      "58/223, train_loss: 0.0967, step time: 0.1106\n",
      "59/223, train_loss: 0.0956, step time: 0.1005\n",
      "60/223, train_loss: 0.1008, step time: 0.1412\n",
      "61/223, train_loss: 0.1046, step time: 0.1027\n",
      "62/223, train_loss: 0.1011, step time: 0.1134\n",
      "63/223, train_loss: 0.0966, step time: 0.1201\n",
      "64/223, train_loss: 0.1009, step time: 0.1073\n",
      "65/223, train_loss: 0.1190, step time: 0.1011\n",
      "66/223, train_loss: 0.0960, step time: 0.1002\n",
      "67/223, train_loss: 0.1032, step time: 0.1012\n",
      "68/223, train_loss: 0.0969, step time: 0.1002\n",
      "69/223, train_loss: 0.0941, step time: 0.1106\n",
      "70/223, train_loss: 0.0911, step time: 0.1123\n",
      "71/223, train_loss: 0.0924, step time: 0.1156\n",
      "72/223, train_loss: 0.0997, step time: 0.1148\n",
      "73/223, train_loss: 0.3086, step time: 0.1095\n",
      "74/223, train_loss: 0.0996, step time: 0.1137\n",
      "75/223, train_loss: 0.1133, step time: 0.1008\n",
      "76/223, train_loss: 0.0944, step time: 0.1117\n",
      "77/223, train_loss: 0.0962, step time: 0.1207\n",
      "78/223, train_loss: 0.1006, step time: 0.1123\n",
      "79/223, train_loss: 0.1035, step time: 0.1559\n",
      "80/223, train_loss: 0.0967, step time: 0.1112\n",
      "81/223, train_loss: 0.1047, step time: 0.1067\n",
      "82/223, train_loss: 0.0950, step time: 0.1150\n",
      "83/223, train_loss: 0.0973, step time: 0.1166\n",
      "84/223, train_loss: 0.0944, step time: 0.1086\n",
      "85/223, train_loss: 0.0942, step time: 0.1111\n",
      "86/223, train_loss: 0.1009, step time: 0.1167\n",
      "87/223, train_loss: 0.0942, step time: 0.1013\n",
      "88/223, train_loss: 0.0983, step time: 0.1613\n",
      "89/223, train_loss: 0.1021, step time: 0.1161\n",
      "90/223, train_loss: 0.0890, step time: 0.1115\n",
      "91/223, train_loss: 0.0897, step time: 0.1415\n",
      "92/223, train_loss: 0.1063, step time: 0.1228\n",
      "93/223, train_loss: 0.0963, step time: 0.1165\n",
      "94/223, train_loss: 0.0956, step time: 0.1004\n",
      "95/223, train_loss: 0.0949, step time: 0.1195\n",
      "96/223, train_loss: 0.1060, step time: 0.1114\n",
      "97/223, train_loss: 0.0954, step time: 0.1008\n",
      "98/223, train_loss: 0.1048, step time: 0.1261\n",
      "99/223, train_loss: 0.0891, step time: 0.1108\n",
      "100/223, train_loss: 0.0949, step time: 0.1003\n",
      "101/223, train_loss: 0.1074, step time: 0.1086\n",
      "102/223, train_loss: 0.0874, step time: 0.1116\n",
      "103/223, train_loss: 0.1049, step time: 0.1103\n",
      "104/223, train_loss: 0.0924, step time: 0.1290\n",
      "105/223, train_loss: 0.0916, step time: 0.1070\n",
      "106/223, train_loss: 0.1040, step time: 0.1006\n",
      "107/223, train_loss: 0.0940, step time: 0.1158\n",
      "108/223, train_loss: 0.1087, step time: 0.1316\n",
      "109/223, train_loss: 0.1070, step time: 0.1143\n",
      "110/223, train_loss: 0.0892, step time: 0.1080\n",
      "111/223, train_loss: 0.1076, step time: 0.1166\n",
      "112/223, train_loss: 0.1129, step time: 0.1164\n",
      "113/223, train_loss: 0.0982, step time: 0.1008\n",
      "114/223, train_loss: 0.0968, step time: 0.1000\n",
      "115/223, train_loss: 0.0935, step time: 0.1118\n",
      "116/223, train_loss: 0.0938, step time: 0.1146\n",
      "117/223, train_loss: 0.0902, step time: 0.1551\n",
      "118/223, train_loss: 0.1040, step time: 0.1006\n",
      "119/223, train_loss: 0.0891, step time: 0.1099\n",
      "120/223, train_loss: 0.0933, step time: 0.1041\n",
      "121/223, train_loss: 0.0992, step time: 0.1070\n",
      "122/223, train_loss: 0.1005, step time: 0.1120\n",
      "123/223, train_loss: 0.0965, step time: 0.1144\n",
      "124/223, train_loss: 0.0963, step time: 0.1088\n",
      "125/223, train_loss: 0.1167, step time: 0.1288\n",
      "126/223, train_loss: 0.0950, step time: 0.1207\n",
      "127/223, train_loss: 0.1030, step time: 0.1033\n",
      "128/223, train_loss: 0.0941, step time: 0.1517\n",
      "129/223, train_loss: 0.1048, step time: 0.1028\n",
      "130/223, train_loss: 0.1030, step time: 0.1001\n",
      "131/223, train_loss: 0.0976, step time: 0.1011\n",
      "132/223, train_loss: 0.1012, step time: 0.1046\n",
      "133/223, train_loss: 0.0904, step time: 0.1093\n",
      "134/223, train_loss: 0.0889, step time: 0.1071\n",
      "135/223, train_loss: 0.1030, step time: 0.0997\n",
      "136/223, train_loss: 0.1019, step time: 0.1000\n",
      "137/223, train_loss: 0.0941, step time: 0.1102\n",
      "138/223, train_loss: 0.0942, step time: 0.1526\n",
      "139/223, train_loss: 0.1175, step time: 0.1089\n",
      "140/223, train_loss: 0.0954, step time: 0.0999\n",
      "141/223, train_loss: 0.1033, step time: 0.1241\n",
      "142/223, train_loss: 0.0949, step time: 0.1051\n",
      "143/223, train_loss: 0.0920, step time: 0.1414\n",
      "144/223, train_loss: 0.1038, step time: 0.1115\n",
      "145/223, train_loss: 0.0969, step time: 0.1143\n",
      "146/223, train_loss: 0.0983, step time: 0.1060\n",
      "147/223, train_loss: 0.1008, step time: 0.1051\n",
      "148/223, train_loss: 0.0946, step time: 0.1005\n",
      "149/223, train_loss: 0.1034, step time: 0.1249\n",
      "150/223, train_loss: 0.1003, step time: 0.1161\n",
      "151/223, train_loss: 0.0882, step time: 0.1046\n",
      "152/223, train_loss: 0.1048, step time: 0.1036\n",
      "153/223, train_loss: 0.0846, step time: 0.0996\n",
      "154/223, train_loss: 0.1026, step time: 0.1001\n",
      "155/223, train_loss: 0.0949, step time: 0.1006\n",
      "156/223, train_loss: 0.0995, step time: 0.1019\n",
      "157/223, train_loss: 0.1003, step time: 0.1018\n",
      "158/223, train_loss: 0.0953, step time: 0.1147\n",
      "159/223, train_loss: 0.0996, step time: 0.1235\n",
      "160/223, train_loss: 0.1002, step time: 0.1403\n",
      "161/223, train_loss: 0.0972, step time: 0.1006\n",
      "162/223, train_loss: 0.0920, step time: 0.1000\n",
      "163/223, train_loss: 0.1063, step time: 0.1008\n",
      "164/223, train_loss: 0.0990, step time: 0.1002\n",
      "165/223, train_loss: 0.0943, step time: 0.1074\n",
      "166/223, train_loss: 0.0969, step time: 0.1125\n",
      "167/223, train_loss: 0.0980, step time: 0.1283\n",
      "168/223, train_loss: 0.0875, step time: 0.1391\n",
      "169/223, train_loss: 0.1125, step time: 0.1198\n",
      "170/223, train_loss: 0.0978, step time: 0.1132\n",
      "171/223, train_loss: 0.1017, step time: 0.1133\n",
      "172/223, train_loss: 0.0972, step time: 0.1027\n",
      "173/223, train_loss: 0.1004, step time: 0.1263\n",
      "174/223, train_loss: 0.1078, step time: 0.1116\n",
      "175/223, train_loss: 0.0974, step time: 0.1053\n",
      "176/223, train_loss: 0.0916, step time: 0.1233\n",
      "177/223, train_loss: 0.0922, step time: 0.1079\n",
      "178/223, train_loss: 0.0880, step time: 0.1266\n",
      "179/223, train_loss: 0.1049, step time: 0.1181\n",
      "180/223, train_loss: 0.0948, step time: 0.1290\n",
      "181/223, train_loss: 0.1035, step time: 0.1028\n",
      "182/223, train_loss: 0.0876, step time: 0.1285\n",
      "183/223, train_loss: 0.0924, step time: 0.1181\n",
      "184/223, train_loss: 0.1154, step time: 0.1096\n",
      "185/223, train_loss: 0.1025, step time: 0.1127\n",
      "186/223, train_loss: 0.0903, step time: 0.1241\n",
      "187/223, train_loss: 0.0916, step time: 0.1067\n",
      "188/223, train_loss: 0.0914, step time: 0.1098\n",
      "189/223, train_loss: 0.0901, step time: 0.1024\n",
      "190/223, train_loss: 0.0996, step time: 0.1057\n",
      "191/223, train_loss: 0.0964, step time: 0.1111\n",
      "192/223, train_loss: 0.0960, step time: 0.0997\n",
      "193/223, train_loss: 0.0969, step time: 0.1006\n",
      "194/223, train_loss: 0.1038, step time: 0.1281\n",
      "195/223, train_loss: 0.0854, step time: 0.1208\n",
      "196/223, train_loss: 0.1181, step time: 0.1135\n",
      "197/223, train_loss: 0.0898, step time: 0.1168\n",
      "198/223, train_loss: 0.0914, step time: 0.1373\n",
      "199/223, train_loss: 0.1064, step time: 0.1205\n",
      "200/223, train_loss: 0.0942, step time: 0.1002\n",
      "201/223, train_loss: 0.0972, step time: 0.1002\n",
      "202/223, train_loss: 0.1043, step time: 0.1133\n",
      "203/223, train_loss: 0.0978, step time: 0.1240\n",
      "204/223, train_loss: 0.1015, step time: 0.1028\n",
      "205/223, train_loss: 0.1014, step time: 0.1018\n",
      "206/223, train_loss: 0.0999, step time: 0.1011\n",
      "207/223, train_loss: 0.0954, step time: 0.1162\n",
      "208/223, train_loss: 0.1038, step time: 0.1241\n",
      "209/223, train_loss: 0.1062, step time: 0.1142\n",
      "210/223, train_loss: 0.0956, step time: 0.1254\n",
      "211/223, train_loss: 0.0918, step time: 0.1051\n",
      "212/223, train_loss: 0.0918, step time: 0.1150\n",
      "213/223, train_loss: 0.0979, step time: 0.1103\n",
      "214/223, train_loss: 0.1084, step time: 0.1696\n",
      "215/223, train_loss: 0.0911, step time: 0.1504\n",
      "216/223, train_loss: 0.0908, step time: 0.1366\n",
      "217/223, train_loss: 0.1006, step time: 0.1014\n",
      "218/223, train_loss: 0.1021, step time: 0.1004\n",
      "219/223, train_loss: 0.1022, step time: 0.1115\n",
      "220/223, train_loss: 0.1035, step time: 0.1262\n",
      "221/223, train_loss: 0.0990, step time: 0.1000\n",
      "222/223, train_loss: 0.0979, step time: 0.1003\n",
      "223/223, train_loss: 0.0896, step time: 0.1000\n",
      "epoch 291 average loss: 0.0993\n",
      "time consuming of epoch 291 is: 89.3364\n",
      "----------\n",
      "epoch 292/300\n",
      "1/223, train_loss: 0.1031, step time: 0.1017\n",
      "2/223, train_loss: 0.1044, step time: 0.1012\n",
      "3/223, train_loss: 0.0881, step time: 0.1006\n",
      "4/223, train_loss: 0.1040, step time: 0.1030\n",
      "5/223, train_loss: 0.1005, step time: 0.1300\n",
      "6/223, train_loss: 0.0929, step time: 0.1205\n",
      "7/223, train_loss: 0.0886, step time: 0.1022\n",
      "8/223, train_loss: 0.0958, step time: 0.1157\n",
      "9/223, train_loss: 0.0998, step time: 0.1155\n",
      "10/223, train_loss: 0.0969, step time: 0.1345\n",
      "11/223, train_loss: 0.0961, step time: 0.1681\n",
      "12/223, train_loss: 0.1037, step time: 0.1027\n",
      "13/223, train_loss: 0.0974, step time: 0.1006\n",
      "14/223, train_loss: 0.1047, step time: 0.1009\n",
      "15/223, train_loss: 0.1016, step time: 0.1002\n",
      "16/223, train_loss: 0.0922, step time: 0.1137\n",
      "17/223, train_loss: 0.0985, step time: 0.1039\n",
      "18/223, train_loss: 0.0955, step time: 0.1052\n",
      "19/223, train_loss: 0.0914, step time: 0.1037\n",
      "20/223, train_loss: 0.0994, step time: 0.1325\n",
      "21/223, train_loss: 0.0939, step time: 0.1579\n",
      "22/223, train_loss: 0.1001, step time: 0.1431\n",
      "23/223, train_loss: 0.0978, step time: 0.1039\n",
      "24/223, train_loss: 0.0963, step time: 0.1043\n",
      "25/223, train_loss: 0.0981, step time: 0.1185\n",
      "26/223, train_loss: 0.2987, step time: 0.1300\n",
      "27/223, train_loss: 0.0882, step time: 0.1049\n",
      "28/223, train_loss: 0.1141, step time: 0.1141\n",
      "29/223, train_loss: 0.1098, step time: 0.1111\n",
      "30/223, train_loss: 0.0971, step time: 0.1123\n",
      "31/223, train_loss: 0.0992, step time: 0.1093\n",
      "32/223, train_loss: 0.0928, step time: 0.1189\n",
      "33/223, train_loss: 0.0939, step time: 0.1428\n",
      "34/223, train_loss: 0.0947, step time: 0.1295\n",
      "35/223, train_loss: 0.1018, step time: 0.1114\n",
      "36/223, train_loss: 0.0993, step time: 0.1195\n",
      "37/223, train_loss: 0.1045, step time: 0.1156\n",
      "38/223, train_loss: 0.0943, step time: 0.1170\n",
      "39/223, train_loss: 0.1020, step time: 0.1346\n",
      "40/223, train_loss: 0.0906, step time: 0.1016\n",
      "41/223, train_loss: 0.1004, step time: 0.1116\n",
      "42/223, train_loss: 0.1060, step time: 0.1292\n",
      "43/223, train_loss: 0.0969, step time: 0.1235\n",
      "44/223, train_loss: 0.0986, step time: 0.1216\n",
      "45/223, train_loss: 0.0969, step time: 0.1378\n",
      "46/223, train_loss: 0.1010, step time: 0.1057\n",
      "47/223, train_loss: 0.1005, step time: 0.1155\n",
      "48/223, train_loss: 0.1230, step time: 0.1073\n",
      "49/223, train_loss: 0.1009, step time: 0.1871\n",
      "50/223, train_loss: 0.0918, step time: 0.1112\n",
      "51/223, train_loss: 0.1028, step time: 0.1122\n",
      "52/223, train_loss: 0.0982, step time: 0.1119\n",
      "53/223, train_loss: 0.1035, step time: 0.1351\n",
      "54/223, train_loss: 0.1053, step time: 0.1084\n",
      "55/223, train_loss: 0.0958, step time: 0.1113\n",
      "56/223, train_loss: 0.0941, step time: 0.1045\n",
      "57/223, train_loss: 0.1086, step time: 0.1148\n",
      "58/223, train_loss: 0.1029, step time: 0.1439\n",
      "59/223, train_loss: 0.0912, step time: 0.1247\n",
      "60/223, train_loss: 0.1119, step time: 0.1128\n",
      "61/223, train_loss: 0.0851, step time: 0.1352\n",
      "62/223, train_loss: 0.1061, step time: 0.1271\n",
      "63/223, train_loss: 0.1104, step time: 0.1072\n",
      "64/223, train_loss: 0.1108, step time: 0.1210\n",
      "65/223, train_loss: 0.0962, step time: 0.1007\n",
      "66/223, train_loss: 0.0864, step time: 0.1008\n",
      "67/223, train_loss: 0.1032, step time: 0.1005\n",
      "68/223, train_loss: 0.0967, step time: 0.1056\n",
      "69/223, train_loss: 0.0999, step time: 0.0990\n",
      "70/223, train_loss: 0.0942, step time: 0.1047\n",
      "71/223, train_loss: 0.0978, step time: 0.0997\n",
      "72/223, train_loss: 0.0987, step time: 0.1125\n",
      "73/223, train_loss: 0.0989, step time: 0.1077\n",
      "74/223, train_loss: 0.1013, step time: 0.1094\n",
      "75/223, train_loss: 0.1078, step time: 0.1288\n",
      "76/223, train_loss: 0.0865, step time: 0.1019\n",
      "77/223, train_loss: 0.0948, step time: 0.1011\n",
      "78/223, train_loss: 0.0953, step time: 0.1219\n",
      "79/223, train_loss: 0.1153, step time: 0.1103\n",
      "80/223, train_loss: 0.1080, step time: 0.1251\n",
      "81/223, train_loss: 0.0938, step time: 0.1021\n",
      "82/223, train_loss: 0.1143, step time: 0.1018\n",
      "83/223, train_loss: 0.0979, step time: 0.1007\n",
      "84/223, train_loss: 0.0982, step time: 0.1204\n",
      "85/223, train_loss: 0.0938, step time: 0.1000\n",
      "86/223, train_loss: 0.0931, step time: 0.1322\n",
      "87/223, train_loss: 0.0925, step time: 0.1202\n",
      "88/223, train_loss: 0.0982, step time: 0.1145\n",
      "89/223, train_loss: 0.0990, step time: 0.1049\n",
      "90/223, train_loss: 0.0948, step time: 0.1148\n",
      "91/223, train_loss: 0.1035, step time: 0.1227\n",
      "92/223, train_loss: 0.1017, step time: 0.1074\n",
      "93/223, train_loss: 0.1010, step time: 0.1005\n",
      "94/223, train_loss: 0.1026, step time: 0.0989\n",
      "95/223, train_loss: 0.0962, step time: 0.0991\n",
      "96/223, train_loss: 0.1000, step time: 0.0986\n",
      "97/223, train_loss: 0.0892, step time: 0.1118\n",
      "98/223, train_loss: 0.0909, step time: 0.1578\n",
      "99/223, train_loss: 0.1005, step time: 0.1214\n",
      "100/223, train_loss: 0.0894, step time: 0.1042\n",
      "101/223, train_loss: 0.1047, step time: 0.1241\n",
      "102/223, train_loss: 0.1080, step time: 0.1204\n",
      "103/223, train_loss: 0.1016, step time: 0.1419\n",
      "104/223, train_loss: 0.0876, step time: 0.1290\n",
      "105/223, train_loss: 0.0995, step time: 0.1210\n",
      "106/223, train_loss: 0.0937, step time: 0.1330\n",
      "107/223, train_loss: 0.0964, step time: 0.1186\n",
      "108/223, train_loss: 0.0962, step time: 0.1104\n",
      "109/223, train_loss: 0.1081, step time: 0.1008\n",
      "110/223, train_loss: 0.1031, step time: 0.1245\n",
      "111/223, train_loss: 0.0929, step time: 0.1062\n",
      "112/223, train_loss: 0.1067, step time: 0.1138\n",
      "113/223, train_loss: 0.1064, step time: 0.1120\n",
      "114/223, train_loss: 0.1003, step time: 0.1137\n",
      "115/223, train_loss: 0.1036, step time: 0.1032\n",
      "116/223, train_loss: 0.1030, step time: 0.1102\n",
      "117/223, train_loss: 0.0923, step time: 0.1140\n",
      "118/223, train_loss: 0.1035, step time: 0.1133\n",
      "119/223, train_loss: 0.0958, step time: 0.1004\n",
      "120/223, train_loss: 0.1126, step time: 0.1185\n",
      "121/223, train_loss: 0.0973, step time: 0.1343\n",
      "122/223, train_loss: 0.1023, step time: 0.1048\n",
      "123/223, train_loss: 0.0952, step time: 0.1165\n",
      "124/223, train_loss: 0.0894, step time: 0.1013\n",
      "125/223, train_loss: 0.1013, step time: 0.1008\n",
      "126/223, train_loss: 0.1066, step time: 0.1140\n",
      "127/223, train_loss: 0.1123, step time: 0.1019\n",
      "128/223, train_loss: 0.0985, step time: 0.1178\n",
      "129/223, train_loss: 0.0944, step time: 0.1014\n",
      "130/223, train_loss: 0.0993, step time: 0.1011\n",
      "131/223, train_loss: 0.0993, step time: 0.1570\n",
      "132/223, train_loss: 0.1070, step time: 0.1193\n",
      "133/223, train_loss: 0.0981, step time: 0.1007\n",
      "134/223, train_loss: 0.0900, step time: 0.0995\n",
      "135/223, train_loss: 0.0921, step time: 0.1005\n",
      "136/223, train_loss: 0.0974, step time: 0.1251\n",
      "137/223, train_loss: 0.0910, step time: 0.1005\n",
      "138/223, train_loss: 0.0951, step time: 0.1084\n",
      "139/223, train_loss: 0.0940, step time: 0.1002\n",
      "140/223, train_loss: 0.0950, step time: 0.1185\n",
      "141/223, train_loss: 0.0923, step time: 0.1101\n",
      "142/223, train_loss: 0.1063, step time: 0.1613\n",
      "143/223, train_loss: 0.0895, step time: 0.1139\n",
      "144/223, train_loss: 0.1016, step time: 0.1099\n",
      "145/223, train_loss: 0.0883, step time: 0.1180\n",
      "146/223, train_loss: 0.0938, step time: 0.1435\n",
      "147/223, train_loss: 0.0907, step time: 0.1188\n",
      "148/223, train_loss: 0.0938, step time: 0.1151\n",
      "149/223, train_loss: 0.1061, step time: 0.1058\n",
      "150/223, train_loss: 0.0933, step time: 0.1077\n",
      "151/223, train_loss: 0.0943, step time: 0.1301\n",
      "152/223, train_loss: 0.0904, step time: 0.1205\n",
      "153/223, train_loss: 0.0942, step time: 0.1005\n",
      "154/223, train_loss: 0.0848, step time: 0.1077\n",
      "155/223, train_loss: 0.0956, step time: 0.0997\n",
      "156/223, train_loss: 0.1004, step time: 0.1095\n",
      "157/223, train_loss: 0.1084, step time: 0.1078\n",
      "158/223, train_loss: 0.1017, step time: 0.1167\n",
      "159/223, train_loss: 0.0993, step time: 0.1008\n",
      "160/223, train_loss: 0.1032, step time: 0.1137\n",
      "161/223, train_loss: 0.0973, step time: 0.1003\n",
      "162/223, train_loss: 0.0990, step time: 0.1285\n",
      "163/223, train_loss: 0.1097, step time: 0.1251\n",
      "164/223, train_loss: 0.1038, step time: 0.1445\n",
      "165/223, train_loss: 0.0967, step time: 0.1006\n",
      "166/223, train_loss: 0.0941, step time: 0.1050\n",
      "167/223, train_loss: 0.1060, step time: 0.1000\n",
      "168/223, train_loss: 0.0893, step time: 0.1171\n",
      "169/223, train_loss: 0.1048, step time: 0.1245\n",
      "170/223, train_loss: 0.0934, step time: 0.1176\n",
      "171/223, train_loss: 0.0997, step time: 0.1065\n",
      "172/223, train_loss: 0.0944, step time: 0.1428\n",
      "173/223, train_loss: 0.0871, step time: 0.1092\n",
      "174/223, train_loss: 0.0908, step time: 0.1137\n",
      "175/223, train_loss: 0.1074, step time: 0.1096\n",
      "176/223, train_loss: 0.0964, step time: 0.1336\n",
      "177/223, train_loss: 0.0968, step time: 0.1132\n",
      "178/223, train_loss: 0.0908, step time: 0.1163\n",
      "179/223, train_loss: 0.1146, step time: 0.1041\n",
      "180/223, train_loss: 0.0991, step time: 0.1254\n",
      "181/223, train_loss: 0.0853, step time: 0.1136\n",
      "182/223, train_loss: 0.1005, step time: 0.1033\n",
      "183/223, train_loss: 0.1031, step time: 0.1172\n",
      "184/223, train_loss: 0.0998, step time: 0.1136\n",
      "185/223, train_loss: 0.0935, step time: 0.1091\n",
      "186/223, train_loss: 0.0958, step time: 0.1127\n",
      "187/223, train_loss: 0.0953, step time: 0.1112\n",
      "188/223, train_loss: 0.0957, step time: 0.1099\n",
      "189/223, train_loss: 0.1062, step time: 0.1074\n",
      "190/223, train_loss: 0.0978, step time: 0.1200\n",
      "191/223, train_loss: 0.1035, step time: 0.1028\n",
      "192/223, train_loss: 0.1044, step time: 0.1174\n",
      "193/223, train_loss: 0.0994, step time: 0.1176\n",
      "194/223, train_loss: 0.0928, step time: 0.1195\n",
      "195/223, train_loss: 0.0954, step time: 0.1206\n",
      "196/223, train_loss: 0.0930, step time: 0.1440\n",
      "197/223, train_loss: 0.0960, step time: 0.1314\n",
      "198/223, train_loss: 0.0986, step time: 0.0993\n",
      "199/223, train_loss: 0.1001, step time: 0.0997\n",
      "200/223, train_loss: 0.0916, step time: 0.1004\n",
      "201/223, train_loss: 0.1003, step time: 0.1006\n",
      "202/223, train_loss: 0.0961, step time: 0.0993\n",
      "203/223, train_loss: 0.1035, step time: 0.0991\n",
      "204/223, train_loss: 0.0955, step time: 0.0999\n",
      "205/223, train_loss: 0.0909, step time: 0.1013\n",
      "206/223, train_loss: 0.1034, step time: 0.1041\n",
      "207/223, train_loss: 0.0911, step time: 0.0997\n",
      "208/223, train_loss: 0.1080, step time: 0.0990\n",
      "209/223, train_loss: 0.1059, step time: 0.1192\n",
      "210/223, train_loss: 0.0881, step time: 0.1129\n",
      "211/223, train_loss: 0.1081, step time: 0.1189\n",
      "212/223, train_loss: 0.1117, step time: 0.1444\n",
      "213/223, train_loss: 0.0925, step time: 0.1656\n",
      "214/223, train_loss: 0.1030, step time: 0.1089\n",
      "215/223, train_loss: 0.0962, step time: 0.1051\n",
      "216/223, train_loss: 0.1021, step time: 0.1004\n",
      "217/223, train_loss: 0.1005, step time: 0.1347\n",
      "218/223, train_loss: 0.0971, step time: 0.0989\n",
      "219/223, train_loss: 0.0948, step time: 0.1009\n",
      "220/223, train_loss: 0.1086, step time: 0.1012\n",
      "221/223, train_loss: 0.1072, step time: 0.1057\n",
      "222/223, train_loss: 0.0873, step time: 0.0994\n",
      "223/223, train_loss: 0.0849, step time: 0.0994\n",
      "epoch 292 average loss: 0.0995\n",
      "time consuming of epoch 292 is: 92.3850\n",
      "----------\n",
      "epoch 293/300\n",
      "1/223, train_loss: 0.1054, step time: 0.1103\n",
      "2/223, train_loss: 0.1024, step time: 0.1006\n",
      "3/223, train_loss: 0.0889, step time: 0.1000\n",
      "4/223, train_loss: 0.1029, step time: 0.1039\n",
      "5/223, train_loss: 0.0905, step time: 0.1196\n",
      "6/223, train_loss: 0.1053, step time: 0.1055\n",
      "7/223, train_loss: 0.0943, step time: 0.1130\n",
      "8/223, train_loss: 0.0968, step time: 0.1195\n",
      "9/223, train_loss: 0.0988, step time: 0.1028\n",
      "10/223, train_loss: 0.1039, step time: 0.1121\n",
      "11/223, train_loss: 0.1085, step time: 0.1101\n",
      "12/223, train_loss: 0.0922, step time: 0.1221\n",
      "13/223, train_loss: 0.0994, step time: 0.1281\n",
      "14/223, train_loss: 0.0961, step time: 0.1075\n",
      "15/223, train_loss: 0.1001, step time: 0.1152\n",
      "16/223, train_loss: 0.0942, step time: 0.1003\n",
      "17/223, train_loss: 0.0894, step time: 0.1011\n",
      "18/223, train_loss: 0.0898, step time: 0.1204\n",
      "19/223, train_loss: 0.0952, step time: 0.1071\n",
      "20/223, train_loss: 0.1063, step time: 0.1097\n",
      "21/223, train_loss: 0.0931, step time: 0.1120\n",
      "22/223, train_loss: 0.0984, step time: 0.1321\n",
      "23/223, train_loss: 0.1001, step time: 0.1107\n",
      "24/223, train_loss: 0.0901, step time: 0.1302\n",
      "25/223, train_loss: 0.1027, step time: 0.1052\n",
      "26/223, train_loss: 0.0957, step time: 0.1006\n",
      "27/223, train_loss: 0.1007, step time: 0.1022\n",
      "28/223, train_loss: 0.0989, step time: 0.1001\n",
      "29/223, train_loss: 0.0973, step time: 0.1061\n",
      "30/223, train_loss: 0.0953, step time: 0.1011\n",
      "31/223, train_loss: 0.0950, step time: 0.1037\n",
      "32/223, train_loss: 0.0953, step time: 0.1313\n",
      "33/223, train_loss: 0.0877, step time: 0.1073\n",
      "34/223, train_loss: 0.1039, step time: 0.1134\n",
      "35/223, train_loss: 0.0951, step time: 0.1290\n",
      "36/223, train_loss: 0.0929, step time: 0.1073\n",
      "37/223, train_loss: 0.0876, step time: 0.1105\n",
      "38/223, train_loss: 0.0916, step time: 0.1151\n",
      "39/223, train_loss: 0.0935, step time: 0.1060\n",
      "40/223, train_loss: 0.0967, step time: 0.1005\n",
      "41/223, train_loss: 0.1074, step time: 0.1188\n",
      "42/223, train_loss: 0.1071, step time: 0.1154\n",
      "43/223, train_loss: 0.0969, step time: 0.1180\n",
      "44/223, train_loss: 0.0974, step time: 0.1242\n",
      "45/223, train_loss: 0.0978, step time: 0.1183\n",
      "46/223, train_loss: 0.1058, step time: 0.1048\n",
      "47/223, train_loss: 0.1085, step time: 0.1036\n",
      "48/223, train_loss: 0.1000, step time: 0.1140\n",
      "49/223, train_loss: 0.0955, step time: 0.1087\n",
      "50/223, train_loss: 0.0980, step time: 0.1122\n",
      "51/223, train_loss: 0.0923, step time: 0.1023\n",
      "52/223, train_loss: 0.1008, step time: 0.1271\n",
      "53/223, train_loss: 0.0965, step time: 0.1244\n",
      "54/223, train_loss: 0.0895, step time: 0.1191\n",
      "55/223, train_loss: 0.1236, step time: 0.1131\n",
      "56/223, train_loss: 0.1052, step time: 0.1010\n",
      "57/223, train_loss: 0.0906, step time: 0.1196\n",
      "58/223, train_loss: 0.0984, step time: 0.1136\n",
      "59/223, train_loss: 0.1044, step time: 0.1141\n",
      "60/223, train_loss: 0.1011, step time: 0.1012\n",
      "61/223, train_loss: 0.0977, step time: 0.1183\n",
      "62/223, train_loss: 0.0832, step time: 0.1149\n",
      "63/223, train_loss: 0.1010, step time: 0.1116\n",
      "64/223, train_loss: 0.1043, step time: 0.1193\n",
      "65/223, train_loss: 0.0953, step time: 0.1145\n",
      "66/223, train_loss: 0.0953, step time: 0.1112\n",
      "67/223, train_loss: 0.1018, step time: 0.1002\n",
      "68/223, train_loss: 0.1056, step time: 0.1029\n",
      "69/223, train_loss: 0.0900, step time: 0.1095\n",
      "70/223, train_loss: 0.1120, step time: 0.1001\n",
      "71/223, train_loss: 0.1075, step time: 0.1004\n",
      "72/223, train_loss: 0.0934, step time: 0.1082\n",
      "73/223, train_loss: 0.0866, step time: 0.1468\n",
      "74/223, train_loss: 0.1095, step time: 0.1007\n",
      "75/223, train_loss: 0.0962, step time: 0.1002\n",
      "76/223, train_loss: 0.0944, step time: 0.1012\n",
      "77/223, train_loss: 0.0889, step time: 0.1350\n",
      "78/223, train_loss: 0.1072, step time: 0.1066\n",
      "79/223, train_loss: 0.1037, step time: 0.1012\n",
      "80/223, train_loss: 0.1000, step time: 0.1297\n",
      "81/223, train_loss: 0.1125, step time: 0.1457\n",
      "82/223, train_loss: 0.0943, step time: 0.1059\n",
      "83/223, train_loss: 0.1142, step time: 0.0996\n",
      "84/223, train_loss: 0.0987, step time: 0.1004\n",
      "85/223, train_loss: 0.0998, step time: 0.1148\n",
      "86/223, train_loss: 0.1034, step time: 0.1083\n",
      "87/223, train_loss: 0.0927, step time: 0.0998\n",
      "88/223, train_loss: 0.0925, step time: 0.1115\n",
      "89/223, train_loss: 0.1144, step time: 0.1198\n",
      "90/223, train_loss: 0.0988, step time: 0.1004\n",
      "91/223, train_loss: 0.0947, step time: 0.1010\n",
      "92/223, train_loss: 0.0927, step time: 0.1009\n",
      "93/223, train_loss: 0.0998, step time: 0.1139\n",
      "94/223, train_loss: 0.1093, step time: 0.1051\n",
      "95/223, train_loss: 0.1013, step time: 0.1092\n",
      "96/223, train_loss: 0.1049, step time: 0.1007\n",
      "97/223, train_loss: 0.0921, step time: 0.1123\n",
      "98/223, train_loss: 0.0862, step time: 0.1065\n",
      "99/223, train_loss: 0.0921, step time: 0.1045\n",
      "100/223, train_loss: 0.1021, step time: 0.1004\n",
      "101/223, train_loss: 0.0894, step time: 0.1050\n",
      "102/223, train_loss: 0.1021, step time: 0.1070\n",
      "103/223, train_loss: 0.1003, step time: 0.1068\n",
      "104/223, train_loss: 0.0922, step time: 0.1010\n",
      "105/223, train_loss: 0.0937, step time: 0.1119\n",
      "106/223, train_loss: 0.1053, step time: 0.1123\n",
      "107/223, train_loss: 0.1043, step time: 0.1131\n",
      "108/223, train_loss: 0.0968, step time: 0.1059\n",
      "109/223, train_loss: 0.1157, step time: 0.1149\n",
      "110/223, train_loss: 0.0945, step time: 0.1648\n",
      "111/223, train_loss: 0.0850, step time: 0.1328\n",
      "112/223, train_loss: 0.1165, step time: 0.1138\n",
      "113/223, train_loss: 0.0990, step time: 0.1064\n",
      "114/223, train_loss: 0.0898, step time: 0.1188\n",
      "115/223, train_loss: 0.0976, step time: 0.1009\n",
      "116/223, train_loss: 0.0938, step time: 0.0998\n",
      "117/223, train_loss: 0.1059, step time: 0.1013\n",
      "118/223, train_loss: 0.0973, step time: 0.1159\n",
      "119/223, train_loss: 0.0967, step time: 0.1053\n",
      "120/223, train_loss: 0.1091, step time: 0.1005\n",
      "121/223, train_loss: 0.1012, step time: 0.1038\n",
      "122/223, train_loss: 0.0877, step time: 0.1153\n",
      "123/223, train_loss: 0.0990, step time: 0.1064\n",
      "124/223, train_loss: 0.0981, step time: 0.1275\n",
      "125/223, train_loss: 0.0993, step time: 0.1159\n",
      "126/223, train_loss: 0.1024, step time: 0.1039\n",
      "127/223, train_loss: 0.0936, step time: 0.1001\n",
      "128/223, train_loss: 0.0911, step time: 0.1353\n",
      "129/223, train_loss: 0.0857, step time: 0.1006\n",
      "130/223, train_loss: 0.0931, step time: 0.1009\n",
      "131/223, train_loss: 0.0897, step time: 0.1016\n",
      "132/223, train_loss: 0.0987, step time: 0.1004\n",
      "133/223, train_loss: 0.1044, step time: 0.1321\n",
      "134/223, train_loss: 0.0925, step time: 0.1319\n",
      "135/223, train_loss: 0.0907, step time: 0.1069\n",
      "136/223, train_loss: 0.0934, step time: 0.1002\n",
      "137/223, train_loss: 0.0948, step time: 0.0999\n",
      "138/223, train_loss: 0.0931, step time: 0.1118\n",
      "139/223, train_loss: 0.0947, step time: 0.1094\n",
      "140/223, train_loss: 0.1060, step time: 0.1478\n",
      "141/223, train_loss: 0.1022, step time: 0.1571\n",
      "142/223, train_loss: 0.0943, step time: 0.1310\n",
      "143/223, train_loss: 0.1052, step time: 0.1300\n",
      "144/223, train_loss: 0.1141, step time: 0.1224\n",
      "145/223, train_loss: 0.0911, step time: 0.1203\n",
      "146/223, train_loss: 0.0973, step time: 0.1381\n",
      "147/223, train_loss: 0.0990, step time: 0.1043\n",
      "148/223, train_loss: 0.1024, step time: 0.1140\n",
      "149/223, train_loss: 0.1010, step time: 0.1111\n",
      "150/223, train_loss: 0.1013, step time: 0.1164\n",
      "151/223, train_loss: 0.1057, step time: 0.1084\n",
      "152/223, train_loss: 0.0999, step time: 0.1002\n",
      "153/223, train_loss: 0.0970, step time: 0.0999\n",
      "154/223, train_loss: 0.0993, step time: 0.1228\n",
      "155/223, train_loss: 0.1062, step time: 0.0998\n",
      "156/223, train_loss: 0.0993, step time: 0.1085\n",
      "157/223, train_loss: 0.1022, step time: 0.1050\n",
      "158/223, train_loss: 0.0983, step time: 0.1129\n",
      "159/223, train_loss: 0.1005, step time: 0.1045\n",
      "160/223, train_loss: 0.0888, step time: 0.1018\n",
      "161/223, train_loss: 0.0912, step time: 0.1143\n",
      "162/223, train_loss: 0.1087, step time: 0.1102\n",
      "163/223, train_loss: 0.0892, step time: 0.1097\n",
      "164/223, train_loss: 0.0959, step time: 0.1239\n",
      "165/223, train_loss: 0.0927, step time: 0.1057\n",
      "166/223, train_loss: 0.0946, step time: 0.1095\n",
      "167/223, train_loss: 0.0909, step time: 0.1039\n",
      "168/223, train_loss: 0.1028, step time: 0.1137\n",
      "169/223, train_loss: 0.0985, step time: 0.1070\n",
      "170/223, train_loss: 0.0893, step time: 0.1195\n",
      "171/223, train_loss: 0.0966, step time: 0.1112\n",
      "172/223, train_loss: 0.0971, step time: 0.1196\n",
      "173/223, train_loss: 0.0944, step time: 0.1178\n",
      "174/223, train_loss: 0.1055, step time: 0.1066\n",
      "175/223, train_loss: 0.0866, step time: 0.1600\n",
      "176/223, train_loss: 0.0991, step time: 0.1162\n",
      "177/223, train_loss: 0.1054, step time: 0.1250\n",
      "178/223, train_loss: 0.0977, step time: 0.1120\n",
      "179/223, train_loss: 0.0917, step time: 0.1154\n",
      "180/223, train_loss: 0.0900, step time: 0.1105\n",
      "181/223, train_loss: 0.2990, step time: 0.1108\n",
      "182/223, train_loss: 0.0934, step time: 0.1276\n",
      "183/223, train_loss: 0.0948, step time: 0.1106\n",
      "184/223, train_loss: 0.0943, step time: 0.1268\n",
      "185/223, train_loss: 0.0922, step time: 0.1000\n",
      "186/223, train_loss: 0.0987, step time: 0.1507\n",
      "187/223, train_loss: 0.1088, step time: 0.1005\n",
      "188/223, train_loss: 0.0934, step time: 0.1104\n",
      "189/223, train_loss: 0.0872, step time: 0.1098\n",
      "190/223, train_loss: 0.0906, step time: 0.1174\n",
      "191/223, train_loss: 0.1026, step time: 0.1247\n",
      "192/223, train_loss: 0.0995, step time: 0.1295\n",
      "193/223, train_loss: 0.0999, step time: 0.1006\n",
      "194/223, train_loss: 0.0980, step time: 0.1014\n",
      "195/223, train_loss: 0.1120, step time: 0.1002\n",
      "196/223, train_loss: 0.1030, step time: 0.0998\n",
      "197/223, train_loss: 0.1036, step time: 0.1101\n",
      "198/223, train_loss: 0.0947, step time: 0.1006\n",
      "199/223, train_loss: 0.1047, step time: 0.0996\n",
      "200/223, train_loss: 0.0942, step time: 0.0998\n",
      "201/223, train_loss: 0.0984, step time: 0.1053\n",
      "202/223, train_loss: 0.1068, step time: 0.1043\n",
      "203/223, train_loss: 0.0949, step time: 0.1106\n",
      "204/223, train_loss: 0.0961, step time: 0.1078\n",
      "205/223, train_loss: 0.1079, step time: 0.1218\n",
      "206/223, train_loss: 0.1016, step time: 0.1360\n",
      "207/223, train_loss: 0.0968, step time: 0.1184\n",
      "208/223, train_loss: 0.1079, step time: 0.1125\n",
      "209/223, train_loss: 0.1060, step time: 0.1371\n",
      "210/223, train_loss: 0.1034, step time: 0.1110\n",
      "211/223, train_loss: 0.1027, step time: 0.1052\n",
      "212/223, train_loss: 0.1024, step time: 0.1017\n",
      "213/223, train_loss: 0.1060, step time: 0.1017\n",
      "214/223, train_loss: 0.0907, step time: 0.1110\n",
      "215/223, train_loss: 0.0997, step time: 0.1000\n",
      "216/223, train_loss: 0.1068, step time: 0.1000\n",
      "217/223, train_loss: 0.0945, step time: 0.1006\n",
      "218/223, train_loss: 0.1044, step time: 0.1001\n",
      "219/223, train_loss: 0.0908, step time: 0.1153\n",
      "220/223, train_loss: 0.1083, step time: 0.0999\n",
      "221/223, train_loss: 0.0935, step time: 0.1004\n",
      "222/223, train_loss: 0.1116, step time: 0.0999\n",
      "223/223, train_loss: 0.0986, step time: 0.0989\n",
      "epoch 293 average loss: 0.0994\n",
      "time consuming of epoch 293 is: 90.3683\n",
      "----------\n",
      "epoch 294/300\n",
      "1/223, train_loss: 0.1087, step time: 0.1103\n",
      "2/223, train_loss: 0.0969, step time: 0.0999\n",
      "3/223, train_loss: 0.0863, step time: 0.1007\n",
      "4/223, train_loss: 0.0950, step time: 0.1088\n",
      "5/223, train_loss: 0.1009, step time: 0.1175\n",
      "6/223, train_loss: 0.1000, step time: 0.1068\n",
      "7/223, train_loss: 0.0984, step time: 0.1218\n",
      "8/223, train_loss: 0.1042, step time: 0.1165\n",
      "9/223, train_loss: 0.1070, step time: 0.1012\n",
      "10/223, train_loss: 0.0881, step time: 0.1208\n",
      "11/223, train_loss: 0.1005, step time: 0.1072\n",
      "12/223, train_loss: 0.1086, step time: 0.1103\n",
      "13/223, train_loss: 0.0937, step time: 0.1010\n",
      "14/223, train_loss: 0.1056, step time: 0.1002\n",
      "15/223, train_loss: 0.1052, step time: 0.1007\n",
      "16/223, train_loss: 0.0967, step time: 0.1177\n",
      "17/223, train_loss: 0.1113, step time: 0.1143\n",
      "18/223, train_loss: 0.1132, step time: 0.1008\n",
      "19/223, train_loss: 0.0975, step time: 0.1007\n",
      "20/223, train_loss: 0.1041, step time: 0.1211\n",
      "21/223, train_loss: 0.0987, step time: 0.1061\n",
      "22/223, train_loss: 0.0998, step time: 0.1248\n",
      "23/223, train_loss: 0.0940, step time: 0.1194\n",
      "24/223, train_loss: 0.0922, step time: 0.1002\n",
      "25/223, train_loss: 0.0967, step time: 0.1040\n",
      "26/223, train_loss: 0.0925, step time: 0.1169\n",
      "27/223, train_loss: 0.0939, step time: 0.1021\n",
      "28/223, train_loss: 0.0904, step time: 0.1096\n",
      "29/223, train_loss: 0.1039, step time: 0.0999\n",
      "30/223, train_loss: 0.0920, step time: 0.1257\n",
      "31/223, train_loss: 0.1042, step time: 0.1006\n",
      "32/223, train_loss: 0.0964, step time: 0.1303\n",
      "33/223, train_loss: 0.0912, step time: 0.1000\n",
      "34/223, train_loss: 0.0853, step time: 0.0999\n",
      "35/223, train_loss: 0.0964, step time: 0.1060\n",
      "36/223, train_loss: 0.1069, step time: 0.1006\n",
      "37/223, train_loss: 0.1015, step time: 0.1349\n",
      "38/223, train_loss: 0.0957, step time: 0.1149\n",
      "39/223, train_loss: 0.1069, step time: 0.1004\n",
      "40/223, train_loss: 0.0969, step time: 0.1064\n",
      "41/223, train_loss: 0.0980, step time: 0.1284\n",
      "42/223, train_loss: 0.0985, step time: 0.1172\n",
      "43/223, train_loss: 0.1009, step time: 0.1033\n",
      "44/223, train_loss: 0.0987, step time: 0.1143\n",
      "45/223, train_loss: 0.0878, step time: 0.1108\n",
      "46/223, train_loss: 0.1040, step time: 0.1027\n",
      "47/223, train_loss: 0.1208, step time: 0.1025\n",
      "48/223, train_loss: 0.0930, step time: 0.1372\n",
      "49/223, train_loss: 0.0947, step time: 0.1107\n",
      "50/223, train_loss: 0.1093, step time: 0.1213\n",
      "51/223, train_loss: 0.1110, step time: 0.1001\n",
      "52/223, train_loss: 0.0912, step time: 0.1147\n",
      "53/223, train_loss: 0.0956, step time: 0.1167\n",
      "54/223, train_loss: 0.0944, step time: 0.1134\n",
      "55/223, train_loss: 0.1105, step time: 0.1000\n",
      "56/223, train_loss: 0.0989, step time: 0.1021\n",
      "57/223, train_loss: 0.1050, step time: 0.1003\n",
      "58/223, train_loss: 0.1066, step time: 0.1126\n",
      "59/223, train_loss: 0.0878, step time: 0.1227\n",
      "60/223, train_loss: 0.0968, step time: 0.1313\n",
      "61/223, train_loss: 0.1020, step time: 0.1007\n",
      "62/223, train_loss: 0.0983, step time: 0.1005\n",
      "63/223, train_loss: 0.0951, step time: 0.1392\n",
      "64/223, train_loss: 0.1028, step time: 0.1222\n",
      "65/223, train_loss: 0.0918, step time: 0.1243\n",
      "66/223, train_loss: 0.1062, step time: 0.1016\n",
      "67/223, train_loss: 0.1049, step time: 0.1035\n",
      "68/223, train_loss: 0.1060, step time: 0.1300\n",
      "69/223, train_loss: 0.0964, step time: 0.1243\n",
      "70/223, train_loss: 0.1027, step time: 0.1000\n",
      "71/223, train_loss: 0.1103, step time: 0.1225\n",
      "72/223, train_loss: 0.0882, step time: 0.1060\n",
      "73/223, train_loss: 0.0960, step time: 0.1241\n",
      "74/223, train_loss: 0.1017, step time: 0.1073\n",
      "75/223, train_loss: 0.1051, step time: 0.0999\n",
      "76/223, train_loss: 0.1044, step time: 0.1184\n",
      "77/223, train_loss: 0.1029, step time: 0.1473\n",
      "78/223, train_loss: 0.0945, step time: 0.1016\n",
      "79/223, train_loss: 0.0995, step time: 0.1101\n",
      "80/223, train_loss: 0.0942, step time: 0.1174\n",
      "81/223, train_loss: 0.0940, step time: 0.1313\n",
      "82/223, train_loss: 0.0958, step time: 0.1058\n",
      "83/223, train_loss: 0.0934, step time: 0.1015\n",
      "84/223, train_loss: 0.1041, step time: 0.1005\n",
      "85/223, train_loss: 0.0998, step time: 0.1557\n",
      "86/223, train_loss: 0.0972, step time: 0.1136\n",
      "87/223, train_loss: 0.1004, step time: 0.1003\n",
      "88/223, train_loss: 0.0950, step time: 0.1126\n",
      "89/223, train_loss: 0.0927, step time: 0.1003\n",
      "90/223, train_loss: 0.0992, step time: 0.0997\n",
      "91/223, train_loss: 0.0955, step time: 0.1124\n",
      "92/223, train_loss: 0.1008, step time: 0.1186\n",
      "93/223, train_loss: 0.0910, step time: 0.1466\n",
      "94/223, train_loss: 0.0887, step time: 0.1205\n",
      "95/223, train_loss: 0.1036, step time: 0.1153\n",
      "96/223, train_loss: 0.0962, step time: 0.1036\n",
      "97/223, train_loss: 0.1029, step time: 0.1006\n",
      "98/223, train_loss: 0.0890, step time: 0.1148\n",
      "99/223, train_loss: 0.0922, step time: 0.1031\n",
      "100/223, train_loss: 0.0956, step time: 0.1080\n",
      "101/223, train_loss: 0.0993, step time: 0.1261\n",
      "102/223, train_loss: 0.0915, step time: 0.1247\n",
      "103/223, train_loss: 0.0931, step time: 0.1010\n",
      "104/223, train_loss: 0.0989, step time: 0.1097\n",
      "105/223, train_loss: 0.0948, step time: 0.1355\n",
      "106/223, train_loss: 0.0983, step time: 0.1411\n",
      "107/223, train_loss: 0.1092, step time: 0.1134\n",
      "108/223, train_loss: 0.0958, step time: 0.1329\n",
      "109/223, train_loss: 0.1024, step time: 0.1137\n",
      "110/223, train_loss: 0.0988, step time: 0.1004\n",
      "111/223, train_loss: 0.1028, step time: 0.1162\n",
      "112/223, train_loss: 0.1086, step time: 0.1032\n",
      "113/223, train_loss: 0.0912, step time: 0.1215\n",
      "114/223, train_loss: 0.0908, step time: 0.1091\n",
      "115/223, train_loss: 0.1030, step time: 0.1204\n",
      "116/223, train_loss: 0.0936, step time: 0.1008\n",
      "117/223, train_loss: 0.1006, step time: 0.1195\n",
      "118/223, train_loss: 0.1100, step time: 0.1191\n",
      "119/223, train_loss: 0.0954, step time: 0.1134\n",
      "120/223, train_loss: 0.0933, step time: 0.1242\n",
      "121/223, train_loss: 0.0895, step time: 0.1140\n",
      "122/223, train_loss: 0.0942, step time: 0.0992\n",
      "123/223, train_loss: 0.1035, step time: 0.1283\n",
      "124/223, train_loss: 0.0968, step time: 0.1331\n",
      "125/223, train_loss: 0.1044, step time: 0.1448\n",
      "126/223, train_loss: 0.0917, step time: 0.1001\n",
      "127/223, train_loss: 0.1013, step time: 0.1185\n",
      "128/223, train_loss: 0.0953, step time: 0.1056\n",
      "129/223, train_loss: 0.0937, step time: 0.1033\n",
      "130/223, train_loss: 0.1029, step time: 0.0999\n",
      "131/223, train_loss: 0.0872, step time: 0.1209\n",
      "132/223, train_loss: 0.1017, step time: 0.1019\n",
      "133/223, train_loss: 0.0937, step time: 0.1006\n",
      "134/223, train_loss: 0.0986, step time: 0.1006\n",
      "135/223, train_loss: 0.1004, step time: 0.1217\n",
      "136/223, train_loss: 0.0911, step time: 0.1056\n",
      "137/223, train_loss: 0.1002, step time: 0.1075\n",
      "138/223, train_loss: 0.0897, step time: 0.1012\n",
      "139/223, train_loss: 0.1066, step time: 0.1027\n",
      "140/223, train_loss: 0.1057, step time: 0.1366\n",
      "141/223, train_loss: 0.0953, step time: 0.1497\n",
      "142/223, train_loss: 0.1079, step time: 0.1121\n",
      "143/223, train_loss: 0.1006, step time: 0.1154\n",
      "144/223, train_loss: 0.0877, step time: 0.1228\n",
      "145/223, train_loss: 0.0958, step time: 0.1200\n",
      "146/223, train_loss: 0.0967, step time: 0.1157\n",
      "147/223, train_loss: 0.0997, step time: 0.1027\n",
      "148/223, train_loss: 0.1082, step time: 0.1116\n",
      "149/223, train_loss: 0.0930, step time: 0.1048\n",
      "150/223, train_loss: 0.0948, step time: 0.1111\n",
      "151/223, train_loss: 0.2913, step time: 0.1091\n",
      "152/223, train_loss: 0.1071, step time: 0.1352\n",
      "153/223, train_loss: 0.1120, step time: 0.1400\n",
      "154/223, train_loss: 0.1013, step time: 0.1012\n",
      "155/223, train_loss: 0.0900, step time: 0.1013\n",
      "156/223, train_loss: 0.1055, step time: 0.1043\n",
      "157/223, train_loss: 0.1008, step time: 0.1007\n",
      "158/223, train_loss: 0.0980, step time: 0.1042\n",
      "159/223, train_loss: 0.0982, step time: 0.1009\n",
      "160/223, train_loss: 0.0939, step time: 0.1198\n",
      "161/223, train_loss: 0.0917, step time: 0.1194\n",
      "162/223, train_loss: 0.0936, step time: 0.1095\n",
      "163/223, train_loss: 0.1088, step time: 0.1004\n",
      "164/223, train_loss: 0.0972, step time: 0.1002\n",
      "165/223, train_loss: 0.1048, step time: 0.1182\n",
      "166/223, train_loss: 0.1034, step time: 0.1007\n",
      "167/223, train_loss: 0.0965, step time: 0.1044\n",
      "168/223, train_loss: 0.1016, step time: 0.1746\n",
      "169/223, train_loss: 0.0989, step time: 0.1006\n",
      "170/223, train_loss: 0.0852, step time: 0.1013\n",
      "171/223, train_loss: 0.1087, step time: 0.1113\n",
      "172/223, train_loss: 0.0948, step time: 0.1098\n",
      "173/223, train_loss: 0.0958, step time: 0.1208\n",
      "174/223, train_loss: 0.0941, step time: 0.1277\n",
      "175/223, train_loss: 0.0926, step time: 0.1044\n",
      "176/223, train_loss: 0.0966, step time: 0.1010\n",
      "177/223, train_loss: 0.0956, step time: 0.1125\n",
      "178/223, train_loss: 0.1091, step time: 0.1017\n",
      "179/223, train_loss: 0.0870, step time: 0.1061\n",
      "180/223, train_loss: 0.0990, step time: 0.1120\n",
      "181/223, train_loss: 0.0915, step time: 0.1180\n",
      "182/223, train_loss: 0.0932, step time: 0.1194\n",
      "183/223, train_loss: 0.1044, step time: 0.1025\n",
      "184/223, train_loss: 0.0919, step time: 0.1008\n",
      "185/223, train_loss: 0.0885, step time: 0.1138\n",
      "186/223, train_loss: 0.0964, step time: 0.1062\n",
      "187/223, train_loss: 0.0963, step time: 0.1096\n",
      "188/223, train_loss: 0.0963, step time: 0.1093\n",
      "189/223, train_loss: 0.0942, step time: 0.1104\n",
      "190/223, train_loss: 0.1086, step time: 0.1445\n",
      "191/223, train_loss: 0.1017, step time: 0.1385\n",
      "192/223, train_loss: 0.0880, step time: 0.1227\n",
      "193/223, train_loss: 0.0991, step time: 0.1413\n",
      "194/223, train_loss: 0.0962, step time: 0.1066\n",
      "195/223, train_loss: 0.0954, step time: 0.1056\n",
      "196/223, train_loss: 0.0972, step time: 0.1142\n",
      "197/223, train_loss: 0.1004, step time: 0.1232\n",
      "198/223, train_loss: 0.1115, step time: 0.1045\n",
      "199/223, train_loss: 0.1050, step time: 0.1292\n",
      "200/223, train_loss: 0.1067, step time: 0.1355\n",
      "201/223, train_loss: 0.1105, step time: 0.1189\n",
      "202/223, train_loss: 0.1077, step time: 0.1364\n",
      "203/223, train_loss: 0.0900, step time: 0.0994\n",
      "204/223, train_loss: 0.1040, step time: 0.0992\n",
      "205/223, train_loss: 0.1117, step time: 0.1008\n",
      "206/223, train_loss: 0.0992, step time: 0.1215\n",
      "207/223, train_loss: 0.1022, step time: 0.1237\n",
      "208/223, train_loss: 0.1040, step time: 0.1039\n",
      "209/223, train_loss: 0.0947, step time: 0.1190\n",
      "210/223, train_loss: 0.1062, step time: 0.1313\n",
      "211/223, train_loss: 0.1076, step time: 0.1017\n",
      "212/223, train_loss: 0.1010, step time: 0.1010\n",
      "213/223, train_loss: 0.0947, step time: 0.1207\n",
      "214/223, train_loss: 0.0963, step time: 0.1414\n",
      "215/223, train_loss: 0.1009, step time: 0.1143\n",
      "216/223, train_loss: 0.0997, step time: 0.1061\n",
      "217/223, train_loss: 0.0889, step time: 0.1188\n",
      "218/223, train_loss: 0.1003, step time: 0.1128\n",
      "219/223, train_loss: 0.0955, step time: 0.1001\n",
      "220/223, train_loss: 0.0976, step time: 0.1005\n",
      "221/223, train_loss: 0.0918, step time: 0.0996\n",
      "222/223, train_loss: 0.0915, step time: 0.1001\n",
      "223/223, train_loss: 0.1097, step time: 0.0997\n",
      "epoch 294 average loss: 0.0996\n",
      "time consuming of epoch 294 is: 93.3614\n",
      "----------\n",
      "epoch 295/300\n",
      "1/223, train_loss: 0.1077, step time: 0.1101\n",
      "2/223, train_loss: 0.0892, step time: 0.1006\n",
      "3/223, train_loss: 0.0942, step time: 0.1019\n",
      "4/223, train_loss: 0.0988, step time: 0.1058\n",
      "5/223, train_loss: 0.0938, step time: 0.1075\n",
      "6/223, train_loss: 0.1066, step time: 0.1060\n",
      "7/223, train_loss: 0.1073, step time: 0.1402\n",
      "8/223, train_loss: 0.0916, step time: 0.1347\n",
      "9/223, train_loss: 0.1052, step time: 0.1134\n",
      "10/223, train_loss: 0.1019, step time: 0.1519\n",
      "11/223, train_loss: 0.1002, step time: 0.1150\n",
      "12/223, train_loss: 0.0927, step time: 0.1094\n",
      "13/223, train_loss: 0.0986, step time: 0.1092\n",
      "14/223, train_loss: 0.1134, step time: 0.1204\n",
      "15/223, train_loss: 0.0938, step time: 0.1098\n",
      "16/223, train_loss: 0.1011, step time: 0.1348\n",
      "17/223, train_loss: 0.1032, step time: 0.1184\n",
      "18/223, train_loss: 0.0957, step time: 0.0997\n",
      "19/223, train_loss: 0.0909, step time: 0.1090\n",
      "20/223, train_loss: 0.1018, step time: 0.1131\n",
      "21/223, train_loss: 0.0874, step time: 0.1006\n",
      "22/223, train_loss: 0.1075, step time: 0.1044\n",
      "23/223, train_loss: 0.1001, step time: 0.1053\n",
      "24/223, train_loss: 0.1021, step time: 0.1007\n",
      "25/223, train_loss: 0.0939, step time: 0.1105\n",
      "26/223, train_loss: 0.0927, step time: 0.1043\n",
      "27/223, train_loss: 0.0993, step time: 0.1003\n",
      "28/223, train_loss: 0.0901, step time: 0.1003\n",
      "29/223, train_loss: 0.1064, step time: 0.1013\n",
      "30/223, train_loss: 0.1039, step time: 0.1053\n",
      "31/223, train_loss: 0.1046, step time: 0.1057\n",
      "32/223, train_loss: 0.1036, step time: 0.1123\n",
      "33/223, train_loss: 0.0889, step time: 0.1107\n",
      "34/223, train_loss: 0.1038, step time: 0.1065\n",
      "35/223, train_loss: 0.0989, step time: 0.1104\n",
      "36/223, train_loss: 0.1030, step time: 0.1295\n",
      "37/223, train_loss: 0.0926, step time: 0.1075\n",
      "38/223, train_loss: 0.1014, step time: 0.1008\n",
      "39/223, train_loss: 0.1111, step time: 0.1000\n",
      "40/223, train_loss: 0.0994, step time: 0.1227\n",
      "41/223, train_loss: 0.1043, step time: 0.1404\n",
      "42/223, train_loss: 0.1054, step time: 0.1145\n",
      "43/223, train_loss: 0.0926, step time: 0.1003\n",
      "44/223, train_loss: 0.0947, step time: 0.1004\n",
      "45/223, train_loss: 0.1071, step time: 0.1126\n",
      "46/223, train_loss: 0.0961, step time: 0.1006\n",
      "47/223, train_loss: 0.1032, step time: 0.1300\n",
      "48/223, train_loss: 0.0976, step time: 0.1064\n",
      "49/223, train_loss: 0.1046, step time: 0.1111\n",
      "50/223, train_loss: 0.1030, step time: 0.1194\n",
      "51/223, train_loss: 0.0961, step time: 0.1264\n",
      "52/223, train_loss: 0.1035, step time: 0.1147\n",
      "53/223, train_loss: 0.0995, step time: 0.1005\n",
      "54/223, train_loss: 0.0974, step time: 0.1210\n",
      "55/223, train_loss: 0.0999, step time: 0.1027\n",
      "56/223, train_loss: 0.0967, step time: 0.1195\n",
      "57/223, train_loss: 0.0878, step time: 0.1190\n",
      "58/223, train_loss: 0.0893, step time: 0.1074\n",
      "59/223, train_loss: 0.0927, step time: 0.1010\n",
      "60/223, train_loss: 0.0909, step time: 0.1008\n",
      "61/223, train_loss: 0.0968, step time: 0.1004\n",
      "62/223, train_loss: 0.0960, step time: 0.1007\n",
      "63/223, train_loss: 0.0911, step time: 0.1006\n",
      "64/223, train_loss: 0.0890, step time: 0.1010\n",
      "65/223, train_loss: 0.0910, step time: 0.1078\n",
      "66/223, train_loss: 0.0966, step time: 0.1458\n",
      "67/223, train_loss: 0.0885, step time: 0.1063\n",
      "68/223, train_loss: 0.0908, step time: 0.1107\n",
      "69/223, train_loss: 0.0983, step time: 0.1038\n",
      "70/223, train_loss: 0.0970, step time: 0.1167\n",
      "71/223, train_loss: 0.0971, step time: 0.1004\n",
      "72/223, train_loss: 0.0900, step time: 0.1141\n",
      "73/223, train_loss: 0.0898, step time: 0.1127\n",
      "74/223, train_loss: 0.0876, step time: 0.1004\n",
      "75/223, train_loss: 0.0983, step time: 0.1005\n",
      "76/223, train_loss: 0.0981, step time: 0.1068\n",
      "77/223, train_loss: 0.1018, step time: 0.0992\n",
      "78/223, train_loss: 0.1003, step time: 0.1091\n",
      "79/223, train_loss: 0.0912, step time: 0.1062\n",
      "80/223, train_loss: 0.0870, step time: 0.1028\n",
      "81/223, train_loss: 0.0977, step time: 0.1331\n",
      "82/223, train_loss: 0.1021, step time: 0.1157\n",
      "83/223, train_loss: 0.0920, step time: 0.1071\n",
      "84/223, train_loss: 0.0899, step time: 0.0995\n",
      "85/223, train_loss: 0.0885, step time: 0.1227\n",
      "86/223, train_loss: 0.0980, step time: 0.1264\n",
      "87/223, train_loss: 0.1006, step time: 0.0993\n",
      "88/223, train_loss: 0.0943, step time: 0.1042\n",
      "89/223, train_loss: 0.0929, step time: 0.1041\n",
      "90/223, train_loss: 0.1115, step time: 0.1163\n",
      "91/223, train_loss: 0.1010, step time: 0.1283\n",
      "92/223, train_loss: 0.1085, step time: 0.1221\n",
      "93/223, train_loss: 0.1135, step time: 0.1047\n",
      "94/223, train_loss: 0.0825, step time: 0.1037\n",
      "95/223, train_loss: 0.0877, step time: 0.1075\n",
      "96/223, train_loss: 0.0972, step time: 0.1007\n",
      "97/223, train_loss: 0.0948, step time: 0.1199\n",
      "98/223, train_loss: 0.1063, step time: 0.1099\n",
      "99/223, train_loss: 0.0885, step time: 0.0996\n",
      "100/223, train_loss: 0.1032, step time: 0.1058\n",
      "101/223, train_loss: 0.0947, step time: 0.1418\n",
      "102/223, train_loss: 0.1018, step time: 0.1036\n",
      "103/223, train_loss: 0.0899, step time: 0.1340\n",
      "104/223, train_loss: 0.0944, step time: 0.1040\n",
      "105/223, train_loss: 0.1069, step time: 0.1246\n",
      "106/223, train_loss: 0.1073, step time: 0.1079\n",
      "107/223, train_loss: 0.1021, step time: 0.1336\n",
      "108/223, train_loss: 0.0889, step time: 0.1005\n",
      "109/223, train_loss: 0.0964, step time: 0.1009\n",
      "110/223, train_loss: 0.0931, step time: 0.1051\n",
      "111/223, train_loss: 0.0905, step time: 0.1001\n",
      "112/223, train_loss: 0.1018, step time: 0.1037\n",
      "113/223, train_loss: 0.0951, step time: 0.1007\n",
      "114/223, train_loss: 0.1023, step time: 0.1067\n",
      "115/223, train_loss: 0.0971, step time: 0.1049\n",
      "116/223, train_loss: 0.1026, step time: 0.1094\n",
      "117/223, train_loss: 0.1005, step time: 0.1112\n",
      "118/223, train_loss: 0.1066, step time: 0.1017\n",
      "119/223, train_loss: 0.0934, step time: 0.1370\n",
      "120/223, train_loss: 0.1109, step time: 0.1068\n",
      "121/223, train_loss: 0.1037, step time: 0.1029\n",
      "122/223, train_loss: 0.0950, step time: 0.1056\n",
      "123/223, train_loss: 0.1073, step time: 0.1053\n",
      "124/223, train_loss: 0.1079, step time: 0.1023\n",
      "125/223, train_loss: 0.0923, step time: 0.1295\n",
      "126/223, train_loss: 0.0968, step time: 0.1084\n",
      "127/223, train_loss: 0.1046, step time: 0.1179\n",
      "128/223, train_loss: 0.0926, step time: 0.1297\n",
      "129/223, train_loss: 0.0999, step time: 0.1009\n",
      "130/223, train_loss: 0.2952, step time: 0.1273\n",
      "131/223, train_loss: 0.0988, step time: 0.1006\n",
      "132/223, train_loss: 0.1015, step time: 0.0999\n",
      "133/223, train_loss: 0.1006, step time: 0.1026\n",
      "134/223, train_loss: 0.0893, step time: 0.1096\n",
      "135/223, train_loss: 0.1047, step time: 0.1248\n",
      "136/223, train_loss: 0.0924, step time: 0.1248\n",
      "137/223, train_loss: 0.0979, step time: 0.1040\n",
      "138/223, train_loss: 0.1029, step time: 0.1176\n",
      "139/223, train_loss: 0.0947, step time: 0.1005\n",
      "140/223, train_loss: 0.1035, step time: 0.1016\n",
      "141/223, train_loss: 0.0990, step time: 0.1018\n",
      "142/223, train_loss: 0.0967, step time: 0.1088\n",
      "143/223, train_loss: 0.1079, step time: 0.1002\n",
      "144/223, train_loss: 0.1075, step time: 0.1005\n",
      "145/223, train_loss: 0.1015, step time: 0.1000\n",
      "146/223, train_loss: 0.0998, step time: 0.1104\n",
      "147/223, train_loss: 0.0951, step time: 0.1009\n",
      "148/223, train_loss: 0.0966, step time: 0.1110\n",
      "149/223, train_loss: 0.1024, step time: 0.1006\n",
      "150/223, train_loss: 0.0918, step time: 0.1133\n",
      "151/223, train_loss: 0.0927, step time: 0.1009\n",
      "152/223, train_loss: 0.0913, step time: 0.1008\n",
      "153/223, train_loss: 0.0927, step time: 0.1858\n",
      "154/223, train_loss: 0.0929, step time: 0.1004\n",
      "155/223, train_loss: 0.1037, step time: 0.1005\n",
      "156/223, train_loss: 0.1024, step time: 0.1010\n",
      "157/223, train_loss: 0.0981, step time: 0.1044\n",
      "158/223, train_loss: 0.1019, step time: 0.1094\n",
      "159/223, train_loss: 0.0963, step time: 0.1008\n",
      "160/223, train_loss: 0.1066, step time: 0.1036\n",
      "161/223, train_loss: 0.0926, step time: 0.1156\n",
      "162/223, train_loss: 0.1017, step time: 0.1167\n",
      "163/223, train_loss: 0.1016, step time: 0.0999\n",
      "164/223, train_loss: 0.0952, step time: 0.1011\n",
      "165/223, train_loss: 0.1077, step time: 0.1010\n",
      "166/223, train_loss: 0.0967, step time: 0.1088\n",
      "167/223, train_loss: 0.1017, step time: 0.1001\n",
      "168/223, train_loss: 0.1081, step time: 0.1494\n",
      "169/223, train_loss: 0.1112, step time: 0.1400\n",
      "170/223, train_loss: 0.0851, step time: 0.1017\n",
      "171/223, train_loss: 0.0996, step time: 0.1047\n",
      "172/223, train_loss: 0.0954, step time: 0.1013\n",
      "173/223, train_loss: 0.0975, step time: 0.1056\n",
      "174/223, train_loss: 0.1069, step time: 0.1254\n",
      "175/223, train_loss: 0.0965, step time: 0.1001\n",
      "176/223, train_loss: 0.1091, step time: 0.1169\n",
      "177/223, train_loss: 0.1102, step time: 0.1034\n",
      "178/223, train_loss: 0.1075, step time: 0.1276\n",
      "179/223, train_loss: 0.0919, step time: 0.1144\n",
      "180/223, train_loss: 0.0998, step time: 0.1235\n",
      "181/223, train_loss: 0.1026, step time: 0.1050\n",
      "182/223, train_loss: 0.0893, step time: 0.1044\n",
      "183/223, train_loss: 0.0908, step time: 0.1322\n",
      "184/223, train_loss: 0.0899, step time: 0.1233\n",
      "185/223, train_loss: 0.0997, step time: 0.1030\n",
      "186/223, train_loss: 0.1095, step time: 0.1109\n",
      "187/223, train_loss: 0.0963, step time: 0.1134\n",
      "188/223, train_loss: 0.1015, step time: 0.1000\n",
      "189/223, train_loss: 0.0978, step time: 0.1165\n",
      "190/223, train_loss: 0.0966, step time: 0.1083\n",
      "191/223, train_loss: 0.0929, step time: 0.1017\n",
      "192/223, train_loss: 0.0863, step time: 0.1009\n",
      "193/223, train_loss: 0.1130, step time: 0.1113\n",
      "194/223, train_loss: 0.0888, step time: 0.1114\n",
      "195/223, train_loss: 0.0967, step time: 0.1008\n",
      "196/223, train_loss: 0.0903, step time: 0.1134\n",
      "197/223, train_loss: 0.0878, step time: 0.1111\n",
      "198/223, train_loss: 0.1026, step time: 0.1206\n",
      "199/223, train_loss: 0.1086, step time: 0.1154\n",
      "200/223, train_loss: 0.0999, step time: 0.1440\n",
      "201/223, train_loss: 0.0944, step time: 0.1375\n",
      "202/223, train_loss: 0.0932, step time: 0.1205\n",
      "203/223, train_loss: 0.0997, step time: 0.1062\n",
      "204/223, train_loss: 0.1054, step time: 0.1009\n",
      "205/223, train_loss: 0.0961, step time: 0.1046\n",
      "206/223, train_loss: 0.1045, step time: 0.0999\n",
      "207/223, train_loss: 0.0951, step time: 0.1141\n",
      "208/223, train_loss: 0.0987, step time: 0.1102\n",
      "209/223, train_loss: 0.0969, step time: 0.1226\n",
      "210/223, train_loss: 0.1032, step time: 0.1069\n",
      "211/223, train_loss: 0.0952, step time: 0.1221\n",
      "212/223, train_loss: 0.0964, step time: 0.1141\n",
      "213/223, train_loss: 0.1098, step time: 0.0996\n",
      "214/223, train_loss: 0.1057, step time: 0.1048\n",
      "215/223, train_loss: 0.0924, step time: 0.1000\n",
      "216/223, train_loss: 0.0906, step time: 0.1092\n",
      "217/223, train_loss: 0.0978, step time: 0.1028\n",
      "218/223, train_loss: 0.1074, step time: 0.1003\n",
      "219/223, train_loss: 0.0972, step time: 0.1004\n",
      "220/223, train_loss: 0.0941, step time: 0.1000\n",
      "221/223, train_loss: 0.1119, step time: 0.1007\n",
      "222/223, train_loss: 0.1045, step time: 0.0993\n",
      "223/223, train_loss: 0.1007, step time: 0.1001\n",
      "epoch 295 average loss: 0.0993\n",
      "saved new best metric model\n",
      "current epoch: 295 current mean dice: 0.8623 tc: 0.9225 wt: 0.8719 et: 0.7923\n",
      "best mean dice: 0.8623 at epoch: 295\n",
      "time consuming of epoch 295 is: 93.6162\n",
      "----------\n",
      "epoch 296/300\n",
      "1/223, train_loss: 0.0941, step time: 0.1038\n",
      "2/223, train_loss: 0.1050, step time: 0.1162\n",
      "3/223, train_loss: 0.1036, step time: 0.1271\n",
      "4/223, train_loss: 0.0921, step time: 0.1028\n",
      "5/223, train_loss: 0.0973, step time: 0.1002\n",
      "6/223, train_loss: 0.0926, step time: 0.1002\n",
      "7/223, train_loss: 0.0972, step time: 0.1160\n",
      "8/223, train_loss: 0.0948, step time: 0.1057\n",
      "9/223, train_loss: 0.0884, step time: 0.1217\n",
      "10/223, train_loss: 0.0990, step time: 0.1473\n",
      "11/223, train_loss: 0.0972, step time: 0.1110\n",
      "12/223, train_loss: 0.0981, step time: 0.1003\n",
      "13/223, train_loss: 0.0914, step time: 0.1063\n",
      "14/223, train_loss: 0.0915, step time: 0.1004\n",
      "15/223, train_loss: 0.1010, step time: 0.1010\n",
      "16/223, train_loss: 0.0901, step time: 0.1233\n",
      "17/223, train_loss: 0.1010, step time: 0.1155\n",
      "18/223, train_loss: 0.1033, step time: 0.1080\n",
      "19/223, train_loss: 0.0977, step time: 0.1363\n",
      "20/223, train_loss: 0.0949, step time: 0.1133\n",
      "21/223, train_loss: 0.1046, step time: 0.1229\n",
      "22/223, train_loss: 0.1037, step time: 0.1001\n",
      "23/223, train_loss: 0.0937, step time: 0.0999\n",
      "24/223, train_loss: 0.1007, step time: 0.1005\n",
      "25/223, train_loss: 0.0995, step time: 0.0993\n",
      "26/223, train_loss: 0.0979, step time: 0.0992\n",
      "27/223, train_loss: 0.0998, step time: 0.0993\n",
      "28/223, train_loss: 0.0901, step time: 0.1057\n",
      "29/223, train_loss: 0.0970, step time: 0.1002\n",
      "30/223, train_loss: 0.0950, step time: 0.0990\n",
      "31/223, train_loss: 0.1082, step time: 0.0992\n",
      "32/223, train_loss: 0.0913, step time: 0.1001\n",
      "33/223, train_loss: 0.1067, step time: 0.0993\n",
      "34/223, train_loss: 0.0993, step time: 0.0992\n",
      "35/223, train_loss: 0.1122, step time: 0.1000\n",
      "36/223, train_loss: 0.0931, step time: 0.1137\n",
      "37/223, train_loss: 0.0899, step time: 0.0992\n",
      "38/223, train_loss: 0.1093, step time: 0.0991\n",
      "39/223, train_loss: 0.0936, step time: 0.1002\n",
      "40/223, train_loss: 0.0896, step time: 0.1000\n",
      "41/223, train_loss: 0.1036, step time: 0.0998\n",
      "42/223, train_loss: 0.0997, step time: 0.0994\n",
      "43/223, train_loss: 0.0947, step time: 0.0998\n",
      "44/223, train_loss: 0.0915, step time: 0.1236\n",
      "45/223, train_loss: 0.1074, step time: 0.1028\n",
      "46/223, train_loss: 0.0921, step time: 0.1119\n",
      "47/223, train_loss: 0.1136, step time: 0.1065\n",
      "48/223, train_loss: 0.0993, step time: 0.0996\n",
      "49/223, train_loss: 0.0981, step time: 0.1256\n",
      "50/223, train_loss: 0.0865, step time: 0.1125\n",
      "51/223, train_loss: 0.0943, step time: 0.1767\n",
      "52/223, train_loss: 0.1051, step time: 0.1077\n",
      "53/223, train_loss: 0.0901, step time: 0.1064\n",
      "54/223, train_loss: 0.0960, step time: 0.1149\n",
      "55/223, train_loss: 0.0973, step time: 0.1041\n",
      "56/223, train_loss: 0.0896, step time: 0.1293\n",
      "57/223, train_loss: 0.0961, step time: 0.1385\n",
      "58/223, train_loss: 0.0965, step time: 0.1079\n",
      "59/223, train_loss: 0.0907, step time: 0.1342\n",
      "60/223, train_loss: 0.1145, step time: 0.1004\n",
      "61/223, train_loss: 0.1060, step time: 0.1088\n",
      "62/223, train_loss: 0.0893, step time: 0.1185\n",
      "63/223, train_loss: 0.1000, step time: 0.1105\n",
      "64/223, train_loss: 0.0988, step time: 0.1004\n",
      "65/223, train_loss: 0.0884, step time: 0.1091\n",
      "66/223, train_loss: 0.1107, step time: 0.1144\n",
      "67/223, train_loss: 0.0913, step time: 0.1088\n",
      "68/223, train_loss: 0.1107, step time: 0.1054\n",
      "69/223, train_loss: 0.0967, step time: 0.1164\n",
      "70/223, train_loss: 0.0950, step time: 0.1060\n",
      "71/223, train_loss: 0.0956, step time: 0.1220\n",
      "72/223, train_loss: 0.0956, step time: 0.1316\n",
      "73/223, train_loss: 0.1073, step time: 0.0999\n",
      "74/223, train_loss: 0.1065, step time: 0.1098\n",
      "75/223, train_loss: 0.0991, step time: 0.1002\n",
      "76/223, train_loss: 0.0941, step time: 0.1073\n",
      "77/223, train_loss: 0.1093, step time: 0.1107\n",
      "78/223, train_loss: 0.1062, step time: 0.1235\n",
      "79/223, train_loss: 0.3035, step time: 0.1069\n",
      "80/223, train_loss: 0.1000, step time: 0.1135\n",
      "81/223, train_loss: 0.1018, step time: 0.1142\n",
      "82/223, train_loss: 0.1035, step time: 0.1090\n",
      "83/223, train_loss: 0.0918, step time: 0.1188\n",
      "84/223, train_loss: 0.1016, step time: 0.1129\n",
      "85/223, train_loss: 0.0995, step time: 0.1089\n",
      "86/223, train_loss: 0.0870, step time: 0.1009\n",
      "87/223, train_loss: 0.0946, step time: 0.1314\n",
      "88/223, train_loss: 0.1001, step time: 0.1111\n",
      "89/223, train_loss: 0.0892, step time: 0.1150\n",
      "90/223, train_loss: 0.0922, step time: 0.1115\n",
      "91/223, train_loss: 0.1042, step time: 0.1112\n",
      "92/223, train_loss: 0.0883, step time: 0.1360\n",
      "93/223, train_loss: 0.0957, step time: 0.1022\n",
      "94/223, train_loss: 0.0915, step time: 0.1234\n",
      "95/223, train_loss: 0.0923, step time: 0.1257\n",
      "96/223, train_loss: 0.1124, step time: 0.1009\n",
      "97/223, train_loss: 0.1059, step time: 0.1125\n",
      "98/223, train_loss: 0.0942, step time: 0.1224\n",
      "99/223, train_loss: 0.1061, step time: 0.1251\n",
      "100/223, train_loss: 0.0982, step time: 0.1080\n",
      "101/223, train_loss: 0.1039, step time: 0.1174\n",
      "102/223, train_loss: 0.1006, step time: 0.1031\n",
      "103/223, train_loss: 0.0976, step time: 0.1253\n",
      "104/223, train_loss: 0.0927, step time: 0.1006\n",
      "105/223, train_loss: 0.0977, step time: 0.1301\n",
      "106/223, train_loss: 0.1003, step time: 0.1005\n",
      "107/223, train_loss: 0.0910, step time: 0.1542\n",
      "108/223, train_loss: 0.0962, step time: 0.1181\n",
      "109/223, train_loss: 0.1000, step time: 0.1056\n",
      "110/223, train_loss: 0.0930, step time: 0.0997\n",
      "111/223, train_loss: 0.1013, step time: 0.1014\n",
      "112/223, train_loss: 0.1011, step time: 0.1232\n",
      "113/223, train_loss: 0.1015, step time: 0.1092\n",
      "114/223, train_loss: 0.0965, step time: 0.1002\n",
      "115/223, train_loss: 0.0897, step time: 0.1001\n",
      "116/223, train_loss: 0.0911, step time: 0.1525\n",
      "117/223, train_loss: 0.1042, step time: 0.1115\n",
      "118/223, train_loss: 0.1136, step time: 0.1457\n",
      "119/223, train_loss: 0.1028, step time: 0.1495\n",
      "120/223, train_loss: 0.1022, step time: 0.0996\n",
      "121/223, train_loss: 0.1039, step time: 0.1001\n",
      "122/223, train_loss: 0.0963, step time: 0.0998\n",
      "123/223, train_loss: 0.0955, step time: 0.1077\n",
      "124/223, train_loss: 0.0919, step time: 0.1147\n",
      "125/223, train_loss: 0.0980, step time: 0.0997\n",
      "126/223, train_loss: 0.1036, step time: 0.1184\n",
      "127/223, train_loss: 0.0908, step time: 0.1090\n",
      "128/223, train_loss: 0.1233, step time: 0.1003\n",
      "129/223, train_loss: 0.0948, step time: 0.1003\n",
      "130/223, train_loss: 0.0999, step time: 0.1201\n",
      "131/223, train_loss: 0.0871, step time: 0.1065\n",
      "132/223, train_loss: 0.0926, step time: 0.1345\n",
      "133/223, train_loss: 0.0867, step time: 0.1112\n",
      "134/223, train_loss: 0.0865, step time: 0.1289\n",
      "135/223, train_loss: 0.0983, step time: 0.1443\n",
      "136/223, train_loss: 0.1080, step time: 0.1408\n",
      "137/223, train_loss: 0.1069, step time: 0.1012\n",
      "138/223, train_loss: 0.0913, step time: 0.1165\n",
      "139/223, train_loss: 0.0873, step time: 0.1183\n",
      "140/223, train_loss: 0.1009, step time: 0.1103\n",
      "141/223, train_loss: 0.1073, step time: 0.1014\n",
      "142/223, train_loss: 0.0987, step time: 0.1444\n",
      "143/223, train_loss: 0.0910, step time: 0.1212\n",
      "144/223, train_loss: 0.1142, step time: 0.1178\n",
      "145/223, train_loss: 0.0985, step time: 0.1108\n",
      "146/223, train_loss: 0.1054, step time: 0.1096\n",
      "147/223, train_loss: 0.0958, step time: 0.1039\n",
      "148/223, train_loss: 0.1059, step time: 0.1269\n",
      "149/223, train_loss: 0.0911, step time: 0.0995\n",
      "150/223, train_loss: 0.0990, step time: 0.1164\n",
      "151/223, train_loss: 0.0973, step time: 0.1203\n",
      "152/223, train_loss: 0.0872, step time: 0.1260\n",
      "153/223, train_loss: 0.0855, step time: 0.1125\n",
      "154/223, train_loss: 0.1002, step time: 0.1344\n",
      "155/223, train_loss: 0.1018, step time: 0.1085\n",
      "156/223, train_loss: 0.0918, step time: 0.1196\n",
      "157/223, train_loss: 0.1058, step time: 0.1005\n",
      "158/223, train_loss: 0.1037, step time: 0.1007\n",
      "159/223, train_loss: 0.1054, step time: 0.1006\n",
      "160/223, train_loss: 0.0982, step time: 0.1036\n",
      "161/223, train_loss: 0.0918, step time: 0.1015\n",
      "162/223, train_loss: 0.0970, step time: 0.1060\n",
      "163/223, train_loss: 0.1072, step time: 0.1057\n",
      "164/223, train_loss: 0.1036, step time: 0.1007\n",
      "165/223, train_loss: 0.1087, step time: 0.1002\n",
      "166/223, train_loss: 0.1054, step time: 0.1190\n",
      "167/223, train_loss: 0.1111, step time: 0.1010\n",
      "168/223, train_loss: 0.0939, step time: 0.1012\n",
      "169/223, train_loss: 0.0968, step time: 0.1006\n",
      "170/223, train_loss: 0.0999, step time: 0.0999\n",
      "171/223, train_loss: 0.1011, step time: 0.1009\n",
      "172/223, train_loss: 0.0951, step time: 0.1008\n",
      "173/223, train_loss: 0.0959, step time: 0.1004\n",
      "174/223, train_loss: 0.1088, step time: 0.1068\n",
      "175/223, train_loss: 0.1077, step time: 0.1046\n",
      "176/223, train_loss: 0.0989, step time: 0.1291\n",
      "177/223, train_loss: 0.0918, step time: 0.1333\n",
      "178/223, train_loss: 0.0920, step time: 0.1153\n",
      "179/223, train_loss: 0.0919, step time: 0.0996\n",
      "180/223, train_loss: 0.1025, step time: 0.1009\n",
      "181/223, train_loss: 0.0902, step time: 0.1300\n",
      "182/223, train_loss: 0.1054, step time: 0.1111\n",
      "183/223, train_loss: 0.0950, step time: 0.1034\n",
      "184/223, train_loss: 0.1074, step time: 0.1014\n",
      "185/223, train_loss: 0.1017, step time: 0.1001\n",
      "186/223, train_loss: 0.0971, step time: 0.1022\n",
      "187/223, train_loss: 0.0893, step time: 0.1090\n",
      "188/223, train_loss: 0.0996, step time: 0.1062\n",
      "189/223, train_loss: 0.0867, step time: 0.1010\n",
      "190/223, train_loss: 0.1024, step time: 0.1071\n",
      "191/223, train_loss: 0.1013, step time: 0.1047\n",
      "192/223, train_loss: 0.0864, step time: 0.1130\n",
      "193/223, train_loss: 0.1055, step time: 0.1249\n",
      "194/223, train_loss: 0.1054, step time: 0.1121\n",
      "195/223, train_loss: 0.1004, step time: 0.1092\n",
      "196/223, train_loss: 0.1143, step time: 0.1138\n",
      "197/223, train_loss: 0.1098, step time: 0.1003\n",
      "198/223, train_loss: 0.1007, step time: 0.1172\n",
      "199/223, train_loss: 0.0955, step time: 0.1200\n",
      "200/223, train_loss: 0.1093, step time: 0.1071\n",
      "201/223, train_loss: 0.1085, step time: 0.1231\n",
      "202/223, train_loss: 0.0929, step time: 0.1099\n",
      "203/223, train_loss: 0.0948, step time: 0.1105\n",
      "204/223, train_loss: 0.1020, step time: 0.1144\n",
      "205/223, train_loss: 0.0979, step time: 0.1140\n",
      "206/223, train_loss: 0.0930, step time: 0.1126\n",
      "207/223, train_loss: 0.1093, step time: 0.1086\n",
      "208/223, train_loss: 0.1006, step time: 0.1008\n",
      "209/223, train_loss: 0.0921, step time: 0.1189\n",
      "210/223, train_loss: 0.0907, step time: 0.1016\n",
      "211/223, train_loss: 0.0962, step time: 0.1170\n",
      "212/223, train_loss: 0.0920, step time: 0.1019\n",
      "213/223, train_loss: 0.0948, step time: 0.0997\n",
      "214/223, train_loss: 0.0983, step time: 0.1003\n",
      "215/223, train_loss: 0.0942, step time: 0.1007\n",
      "216/223, train_loss: 0.0999, step time: 0.1008\n",
      "217/223, train_loss: 0.1169, step time: 0.1006\n",
      "218/223, train_loss: 0.0932, step time: 0.1004\n",
      "219/223, train_loss: 0.0887, step time: 0.0995\n",
      "220/223, train_loss: 0.1039, step time: 0.0992\n",
      "221/223, train_loss: 0.1132, step time: 0.0994\n",
      "222/223, train_loss: 0.0941, step time: 0.1077\n",
      "223/223, train_loss: 0.0958, step time: 0.1009\n",
      "epoch 296 average loss: 0.0995\n",
      "time consuming of epoch 296 is: 94.8586\n",
      "----------\n",
      "epoch 297/300\n",
      "1/223, train_loss: 0.0993, step time: 0.1187\n",
      "2/223, train_loss: 0.0885, step time: 0.1005\n",
      "3/223, train_loss: 0.0942, step time: 0.1011\n",
      "4/223, train_loss: 0.1039, step time: 0.1063\n",
      "5/223, train_loss: 0.1017, step time: 0.0999\n",
      "6/223, train_loss: 0.0978, step time: 0.1273\n",
      "7/223, train_loss: 0.0874, step time: 0.1106\n",
      "8/223, train_loss: 0.0930, step time: 0.1056\n",
      "9/223, train_loss: 0.0975, step time: 0.1035\n",
      "10/223, train_loss: 0.0910, step time: 0.0997\n",
      "11/223, train_loss: 0.0970, step time: 0.1073\n",
      "12/223, train_loss: 0.1066, step time: 0.1010\n",
      "13/223, train_loss: 0.0940, step time: 0.1071\n",
      "14/223, train_loss: 0.0958, step time: 0.1077\n",
      "15/223, train_loss: 0.1078, step time: 0.1183\n",
      "16/223, train_loss: 0.0942, step time: 0.1181\n",
      "17/223, train_loss: 0.1084, step time: 0.1124\n",
      "18/223, train_loss: 0.0972, step time: 0.1096\n",
      "19/223, train_loss: 0.1119, step time: 0.1130\n",
      "20/223, train_loss: 0.1060, step time: 0.1242\n",
      "21/223, train_loss: 0.1022, step time: 0.1053\n",
      "22/223, train_loss: 0.0952, step time: 0.0999\n",
      "23/223, train_loss: 0.1117, step time: 0.1054\n",
      "24/223, train_loss: 0.0888, step time: 0.1122\n",
      "25/223, train_loss: 0.1065, step time: 0.0999\n",
      "26/223, train_loss: 0.1079, step time: 0.1141\n",
      "27/223, train_loss: 0.0922, step time: 0.1199\n",
      "28/223, train_loss: 0.0951, step time: 0.1001\n",
      "29/223, train_loss: 0.1021, step time: 0.1154\n",
      "30/223, train_loss: 0.0982, step time: 0.1029\n",
      "31/223, train_loss: 0.0970, step time: 0.1275\n",
      "32/223, train_loss: 0.0983, step time: 0.1258\n",
      "33/223, train_loss: 0.1005, step time: 0.1188\n",
      "34/223, train_loss: 0.0961, step time: 0.1029\n",
      "35/223, train_loss: 0.1127, step time: 0.1004\n",
      "36/223, train_loss: 0.1058, step time: 0.0995\n",
      "37/223, train_loss: 0.0965, step time: 0.1144\n",
      "38/223, train_loss: 0.1011, step time: 0.1486\n",
      "39/223, train_loss: 0.0994, step time: 0.1163\n",
      "40/223, train_loss: 0.1067, step time: 0.1056\n",
      "41/223, train_loss: 0.1042, step time: 0.1004\n",
      "42/223, train_loss: 0.1029, step time: 0.1145\n",
      "43/223, train_loss: 0.1086, step time: 0.1293\n",
      "44/223, train_loss: 0.0905, step time: 0.1188\n",
      "45/223, train_loss: 0.0958, step time: 0.1066\n",
      "46/223, train_loss: 0.0942, step time: 0.1195\n",
      "47/223, train_loss: 0.1068, step time: 0.1229\n",
      "48/223, train_loss: 0.1082, step time: 0.1262\n",
      "49/223, train_loss: 0.1104, step time: 0.1220\n",
      "50/223, train_loss: 0.1024, step time: 0.1134\n",
      "51/223, train_loss: 0.0939, step time: 0.1234\n",
      "52/223, train_loss: 0.0946, step time: 0.1208\n",
      "53/223, train_loss: 0.0869, step time: 0.1131\n",
      "54/223, train_loss: 0.0887, step time: 0.1006\n",
      "55/223, train_loss: 0.0902, step time: 0.1017\n",
      "56/223, train_loss: 0.1035, step time: 0.1075\n",
      "57/223, train_loss: 0.0969, step time: 0.1001\n",
      "58/223, train_loss: 0.0978, step time: 0.0988\n",
      "59/223, train_loss: 0.1129, step time: 0.0991\n",
      "60/223, train_loss: 0.1045, step time: 0.1109\n",
      "61/223, train_loss: 0.0968, step time: 0.1033\n",
      "62/223, train_loss: 0.0990, step time: 0.1392\n",
      "63/223, train_loss: 0.1139, step time: 0.0994\n",
      "64/223, train_loss: 0.0980, step time: 0.1163\n",
      "65/223, train_loss: 0.0934, step time: 0.1027\n",
      "66/223, train_loss: 0.0965, step time: 0.1124\n",
      "67/223, train_loss: 0.0993, step time: 0.1126\n",
      "68/223, train_loss: 0.1038, step time: 0.1105\n",
      "69/223, train_loss: 0.1000, step time: 0.1178\n",
      "70/223, train_loss: 0.0937, step time: 0.0999\n",
      "71/223, train_loss: 0.0892, step time: 0.1053\n",
      "72/223, train_loss: 0.1058, step time: 0.1196\n",
      "73/223, train_loss: 0.0985, step time: 0.0999\n",
      "74/223, train_loss: 0.0970, step time: 0.1205\n",
      "75/223, train_loss: 0.1031, step time: 0.1313\n",
      "76/223, train_loss: 0.1008, step time: 0.1139\n",
      "77/223, train_loss: 0.0964, step time: 0.1210\n",
      "78/223, train_loss: 0.1016, step time: 0.1225\n",
      "79/223, train_loss: 0.0937, step time: 0.1130\n",
      "80/223, train_loss: 0.0924, step time: 0.1152\n",
      "81/223, train_loss: 0.1080, step time: 0.1189\n",
      "82/223, train_loss: 0.0949, step time: 0.1122\n",
      "83/223, train_loss: 0.0973, step time: 0.1374\n",
      "84/223, train_loss: 0.0910, step time: 0.1077\n",
      "85/223, train_loss: 0.0926, step time: 0.1023\n",
      "86/223, train_loss: 0.0953, step time: 0.1190\n",
      "87/223, train_loss: 0.1041, step time: 0.1141\n",
      "88/223, train_loss: 0.0918, step time: 0.1152\n",
      "89/223, train_loss: 0.0986, step time: 0.1249\n",
      "90/223, train_loss: 0.0976, step time: 0.1038\n",
      "91/223, train_loss: 0.0944, step time: 0.1094\n",
      "92/223, train_loss: 0.0964, step time: 0.1310\n",
      "93/223, train_loss: 0.1064, step time: 0.1169\n",
      "94/223, train_loss: 0.0931, step time: 0.1092\n",
      "95/223, train_loss: 0.0979, step time: 0.1226\n",
      "96/223, train_loss: 0.1029, step time: 0.1374\n",
      "97/223, train_loss: 0.0885, step time: 0.1032\n",
      "98/223, train_loss: 0.1056, step time: 0.1007\n",
      "99/223, train_loss: 0.1074, step time: 0.1114\n",
      "100/223, train_loss: 0.0955, step time: 0.1041\n",
      "101/223, train_loss: 0.1047, step time: 0.1250\n",
      "102/223, train_loss: 0.0951, step time: 0.1351\n",
      "103/223, train_loss: 0.0885, step time: 0.1011\n",
      "104/223, train_loss: 0.0973, step time: 0.1159\n",
      "105/223, train_loss: 0.0995, step time: 0.1272\n",
      "106/223, train_loss: 0.1161, step time: 0.1077\n",
      "107/223, train_loss: 0.0950, step time: 0.1222\n",
      "108/223, train_loss: 0.1055, step time: 0.1256\n",
      "109/223, train_loss: 0.0984, step time: 0.1040\n",
      "110/223, train_loss: 0.0890, step time: 0.1169\n",
      "111/223, train_loss: 0.0978, step time: 0.1218\n",
      "112/223, train_loss: 0.0933, step time: 0.1289\n",
      "113/223, train_loss: 0.1098, step time: 0.1160\n",
      "114/223, train_loss: 0.1006, step time: 0.1303\n",
      "115/223, train_loss: 0.1009, step time: 0.1167\n",
      "116/223, train_loss: 0.1075, step time: 0.1142\n",
      "117/223, train_loss: 0.0998, step time: 0.1133\n",
      "118/223, train_loss: 0.0899, step time: 0.1009\n",
      "119/223, train_loss: 0.1015, step time: 0.1008\n",
      "120/223, train_loss: 0.0970, step time: 0.1010\n",
      "121/223, train_loss: 0.1108, step time: 0.1244\n",
      "122/223, train_loss: 0.1006, step time: 0.1273\n",
      "123/223, train_loss: 0.0884, step time: 0.1099\n",
      "124/223, train_loss: 0.1034, step time: 0.1163\n",
      "125/223, train_loss: 0.1014, step time: 0.1216\n",
      "126/223, train_loss: 0.0859, step time: 0.1100\n",
      "127/223, train_loss: 0.1021, step time: 0.1306\n",
      "128/223, train_loss: 0.0958, step time: 0.1168\n",
      "129/223, train_loss: 0.1042, step time: 0.1107\n",
      "130/223, train_loss: 0.0939, step time: 0.1043\n",
      "131/223, train_loss: 0.0996, step time: 0.1202\n",
      "132/223, train_loss: 0.1011, step time: 0.1525\n",
      "133/223, train_loss: 0.0908, step time: 0.1107\n",
      "134/223, train_loss: 0.0966, step time: 0.1251\n",
      "135/223, train_loss: 0.1067, step time: 0.1071\n",
      "136/223, train_loss: 0.0951, step time: 0.0995\n",
      "137/223, train_loss: 0.1072, step time: 0.1158\n",
      "138/223, train_loss: 0.0871, step time: 0.1186\n",
      "139/223, train_loss: 0.0954, step time: 0.1045\n",
      "140/223, train_loss: 0.0975, step time: 0.1049\n",
      "141/223, train_loss: 0.0919, step time: 0.1142\n",
      "142/223, train_loss: 0.0936, step time: 0.1156\n",
      "143/223, train_loss: 0.0969, step time: 0.1273\n",
      "144/223, train_loss: 0.0912, step time: 0.1058\n",
      "145/223, train_loss: 0.0947, step time: 0.1165\n",
      "146/223, train_loss: 0.0920, step time: 0.1264\n",
      "147/223, train_loss: 0.1006, step time: 0.1311\n",
      "148/223, train_loss: 0.1051, step time: 0.1100\n",
      "149/223, train_loss: 0.1002, step time: 0.1003\n",
      "150/223, train_loss: 0.0944, step time: 0.1050\n",
      "151/223, train_loss: 0.1033, step time: 0.1160\n",
      "152/223, train_loss: 0.0895, step time: 0.1224\n",
      "153/223, train_loss: 0.1123, step time: 0.1030\n",
      "154/223, train_loss: 0.0906, step time: 0.1160\n",
      "155/223, train_loss: 0.1008, step time: 0.1188\n",
      "156/223, train_loss: 0.0970, step time: 0.1005\n",
      "157/223, train_loss: 0.0870, step time: 0.1049\n",
      "158/223, train_loss: 0.0944, step time: 0.1352\n",
      "159/223, train_loss: 0.1025, step time: 0.1191\n",
      "160/223, train_loss: 0.0943, step time: 0.1124\n",
      "161/223, train_loss: 0.0962, step time: 0.0996\n",
      "162/223, train_loss: 0.0964, step time: 0.1021\n",
      "163/223, train_loss: 0.1001, step time: 0.1146\n",
      "164/223, train_loss: 0.0948, step time: 0.1210\n",
      "165/223, train_loss: 0.0976, step time: 0.1232\n",
      "166/223, train_loss: 0.0973, step time: 0.1157\n",
      "167/223, train_loss: 0.0950, step time: 0.1263\n",
      "168/223, train_loss: 0.0926, step time: 0.1288\n",
      "169/223, train_loss: 0.0902, step time: 0.1031\n",
      "170/223, train_loss: 0.0973, step time: 0.1152\n",
      "171/223, train_loss: 0.0975, step time: 0.1006\n",
      "172/223, train_loss: 0.0979, step time: 0.1049\n",
      "173/223, train_loss: 0.0875, step time: 0.1313\n",
      "174/223, train_loss: 0.0952, step time: 0.1278\n",
      "175/223, train_loss: 0.2932, step time: 0.1002\n",
      "176/223, train_loss: 0.1106, step time: 0.1024\n",
      "177/223, train_loss: 0.0961, step time: 0.1136\n",
      "178/223, train_loss: 0.0937, step time: 0.1112\n",
      "179/223, train_loss: 0.1050, step time: 0.1008\n",
      "180/223, train_loss: 0.0983, step time: 0.1115\n",
      "181/223, train_loss: 0.1031, step time: 0.1093\n",
      "182/223, train_loss: 0.1066, step time: 0.1112\n",
      "183/223, train_loss: 0.0973, step time: 0.1049\n",
      "184/223, train_loss: 0.0999, step time: 0.1013\n",
      "185/223, train_loss: 0.1040, step time: 0.1022\n",
      "186/223, train_loss: 0.0936, step time: 0.1121\n",
      "187/223, train_loss: 0.1008, step time: 0.1007\n",
      "188/223, train_loss: 0.0910, step time: 0.1029\n",
      "189/223, train_loss: 0.0833, step time: 0.1083\n",
      "190/223, train_loss: 0.0880, step time: 0.1144\n",
      "191/223, train_loss: 0.1078, step time: 0.1255\n",
      "192/223, train_loss: 0.1090, step time: 0.1130\n",
      "193/223, train_loss: 0.1000, step time: 0.1075\n",
      "194/223, train_loss: 0.1149, step time: 0.1002\n",
      "195/223, train_loss: 0.0963, step time: 0.1107\n",
      "196/223, train_loss: 0.0988, step time: 0.1133\n",
      "197/223, train_loss: 0.0888, step time: 0.1081\n",
      "198/223, train_loss: 0.0896, step time: 0.1068\n",
      "199/223, train_loss: 0.1111, step time: 0.1003\n",
      "200/223, train_loss: 0.0959, step time: 0.1004\n",
      "201/223, train_loss: 0.1000, step time: 0.1003\n",
      "202/223, train_loss: 0.0973, step time: 0.0997\n",
      "203/223, train_loss: 0.0891, step time: 0.0993\n",
      "204/223, train_loss: 0.0869, step time: 0.1209\n",
      "205/223, train_loss: 0.1035, step time: 0.1187\n",
      "206/223, train_loss: 0.0887, step time: 0.1081\n",
      "207/223, train_loss: 0.1055, step time: 0.1175\n",
      "208/223, train_loss: 0.0941, step time: 0.1369\n",
      "209/223, train_loss: 0.0978, step time: 0.1302\n",
      "210/223, train_loss: 0.0917, step time: 0.1181\n",
      "211/223, train_loss: 0.1028, step time: 0.1033\n",
      "212/223, train_loss: 0.1042, step time: 0.1255\n",
      "213/223, train_loss: 0.1033, step time: 0.1250\n",
      "214/223, train_loss: 0.1067, step time: 0.1145\n",
      "215/223, train_loss: 0.0885, step time: 0.1136\n",
      "216/223, train_loss: 0.1005, step time: 0.1119\n",
      "217/223, train_loss: 0.1002, step time: 0.1082\n",
      "218/223, train_loss: 0.1050, step time: 0.1005\n",
      "219/223, train_loss: 0.0933, step time: 0.0996\n",
      "220/223, train_loss: 0.0968, step time: 0.1007\n",
      "221/223, train_loss: 0.1060, step time: 0.0993\n",
      "222/223, train_loss: 0.0855, step time: 0.0998\n",
      "223/223, train_loss: 0.1047, step time: 0.1015\n",
      "epoch 297 average loss: 0.0994\n",
      "time consuming of epoch 297 is: 90.0429\n",
      "----------\n",
      "epoch 298/300\n",
      "1/223, train_loss: 0.0915, step time: 0.1098\n",
      "2/223, train_loss: 0.0970, step time: 0.1023\n",
      "3/223, train_loss: 0.0868, step time: 0.1007\n",
      "4/223, train_loss: 0.1015, step time: 0.1015\n",
      "5/223, train_loss: 0.1028, step time: 0.1128\n",
      "6/223, train_loss: 0.0953, step time: 0.1053\n",
      "7/223, train_loss: 0.0959, step time: 0.1150\n",
      "8/223, train_loss: 0.0859, step time: 0.1137\n",
      "9/223, train_loss: 0.0963, step time: 0.1200\n",
      "10/223, train_loss: 0.1101, step time: 0.1079\n",
      "11/223, train_loss: 0.1101, step time: 0.1268\n",
      "12/223, train_loss: 0.0961, step time: 0.1256\n",
      "13/223, train_loss: 0.0991, step time: 0.1122\n",
      "14/223, train_loss: 0.0996, step time: 0.1189\n",
      "15/223, train_loss: 0.0994, step time: 0.1136\n",
      "16/223, train_loss: 0.0969, step time: 0.1077\n",
      "17/223, train_loss: 0.1017, step time: 0.0999\n",
      "18/223, train_loss: 0.0900, step time: 0.1183\n",
      "19/223, train_loss: 0.1124, step time: 0.1122\n",
      "20/223, train_loss: 0.1014, step time: 0.1054\n",
      "21/223, train_loss: 0.0944, step time: 0.1163\n",
      "22/223, train_loss: 0.0901, step time: 0.0996\n",
      "23/223, train_loss: 0.0880, step time: 0.1213\n",
      "24/223, train_loss: 0.1017, step time: 0.1159\n",
      "25/223, train_loss: 0.0941, step time: 0.1112\n",
      "26/223, train_loss: 0.0983, step time: 0.1096\n",
      "27/223, train_loss: 0.1095, step time: 0.1022\n",
      "28/223, train_loss: 0.0920, step time: 0.1095\n",
      "29/223, train_loss: 0.1022, step time: 0.1174\n",
      "30/223, train_loss: 0.0996, step time: 0.1122\n",
      "31/223, train_loss: 0.0924, step time: 0.1409\n",
      "32/223, train_loss: 0.1012, step time: 0.1159\n",
      "33/223, train_loss: 0.1033, step time: 0.1017\n",
      "34/223, train_loss: 0.0983, step time: 0.1092\n",
      "35/223, train_loss: 0.0879, step time: 0.1194\n",
      "36/223, train_loss: 0.0895, step time: 0.1080\n",
      "37/223, train_loss: 0.0984, step time: 0.0995\n",
      "38/223, train_loss: 0.0889, step time: 0.1093\n",
      "39/223, train_loss: 0.1070, step time: 0.1138\n",
      "40/223, train_loss: 0.0988, step time: 0.1184\n",
      "41/223, train_loss: 0.1000, step time: 0.1032\n",
      "42/223, train_loss: 0.1026, step time: 0.0995\n",
      "43/223, train_loss: 0.1027, step time: 0.1141\n",
      "44/223, train_loss: 0.1032, step time: 0.1000\n",
      "45/223, train_loss: 0.0998, step time: 0.1230\n",
      "46/223, train_loss: 0.1046, step time: 0.1169\n",
      "47/223, train_loss: 0.0909, step time: 0.0994\n",
      "48/223, train_loss: 0.0899, step time: 0.0991\n",
      "49/223, train_loss: 0.0968, step time: 0.0989\n",
      "50/223, train_loss: 0.1064, step time: 0.1262\n",
      "51/223, train_loss: 0.1093, step time: 0.1157\n",
      "52/223, train_loss: 0.1076, step time: 0.1139\n",
      "53/223, train_loss: 0.1039, step time: 0.1203\n",
      "54/223, train_loss: 0.0903, step time: 0.1143\n",
      "55/223, train_loss: 0.0885, step time: 0.1054\n",
      "56/223, train_loss: 0.0986, step time: 0.1055\n",
      "57/223, train_loss: 0.1071, step time: 0.1123\n",
      "58/223, train_loss: 0.0982, step time: 0.1125\n",
      "59/223, train_loss: 0.0955, step time: 0.1083\n",
      "60/223, train_loss: 0.0973, step time: 0.1164\n",
      "61/223, train_loss: 0.0956, step time: 0.1107\n",
      "62/223, train_loss: 0.0940, step time: 0.1107\n",
      "63/223, train_loss: 0.0872, step time: 0.1166\n",
      "64/223, train_loss: 0.1050, step time: 0.1131\n",
      "65/223, train_loss: 0.1059, step time: 0.0994\n",
      "66/223, train_loss: 0.1054, step time: 0.1092\n",
      "67/223, train_loss: 0.1060, step time: 0.1030\n",
      "68/223, train_loss: 0.0847, step time: 0.1219\n",
      "69/223, train_loss: 0.0934, step time: 0.0993\n",
      "70/223, train_loss: 0.1004, step time: 0.1151\n",
      "71/223, train_loss: 0.1018, step time: 0.1129\n",
      "72/223, train_loss: 0.0962, step time: 0.0989\n",
      "73/223, train_loss: 0.1002, step time: 0.0992\n",
      "74/223, train_loss: 0.1083, step time: 0.1148\n",
      "75/223, train_loss: 0.1175, step time: 0.1110\n",
      "76/223, train_loss: 0.0925, step time: 0.1081\n",
      "77/223, train_loss: 0.0981, step time: 0.1090\n",
      "78/223, train_loss: 0.1009, step time: 0.1117\n",
      "79/223, train_loss: 0.0880, step time: 0.1184\n",
      "80/223, train_loss: 0.1014, step time: 0.1154\n",
      "81/223, train_loss: 0.0900, step time: 0.1100\n",
      "82/223, train_loss: 0.0951, step time: 0.1154\n",
      "83/223, train_loss: 0.1045, step time: 0.1140\n",
      "84/223, train_loss: 0.1002, step time: 0.1150\n",
      "85/223, train_loss: 0.0930, step time: 0.1103\n",
      "86/223, train_loss: 0.0879, step time: 0.1133\n",
      "87/223, train_loss: 0.1037, step time: 0.1179\n",
      "88/223, train_loss: 0.1002, step time: 0.1172\n",
      "89/223, train_loss: 0.0924, step time: 0.1077\n",
      "90/223, train_loss: 0.0983, step time: 0.1049\n",
      "91/223, train_loss: 0.1009, step time: 0.1156\n",
      "92/223, train_loss: 0.0926, step time: 0.1082\n",
      "93/223, train_loss: 0.0963, step time: 0.1041\n",
      "94/223, train_loss: 0.0995, step time: 0.1106\n",
      "95/223, train_loss: 0.0984, step time: 0.1080\n",
      "96/223, train_loss: 0.0971, step time: 0.1024\n",
      "97/223, train_loss: 0.0952, step time: 0.1208\n",
      "98/223, train_loss: 0.1096, step time: 0.1031\n",
      "99/223, train_loss: 0.0959, step time: 0.1053\n",
      "100/223, train_loss: 0.0948, step time: 0.1074\n",
      "101/223, train_loss: 0.0978, step time: 0.1142\n",
      "102/223, train_loss: 0.1113, step time: 0.1275\n",
      "103/223, train_loss: 0.0924, step time: 0.1052\n",
      "104/223, train_loss: 0.1118, step time: 0.1075\n",
      "105/223, train_loss: 0.0890, step time: 0.1273\n",
      "106/223, train_loss: 0.0962, step time: 0.1180\n",
      "107/223, train_loss: 0.0989, step time: 0.1022\n",
      "108/223, train_loss: 0.1021, step time: 0.1163\n",
      "109/223, train_loss: 0.0973, step time: 0.1129\n",
      "110/223, train_loss: 0.0926, step time: 0.1198\n",
      "111/223, train_loss: 0.1018, step time: 0.1004\n",
      "112/223, train_loss: 0.1131, step time: 0.1280\n",
      "113/223, train_loss: 0.1020, step time: 0.1114\n",
      "114/223, train_loss: 0.0883, step time: 0.1171\n",
      "115/223, train_loss: 0.0899, step time: 0.1118\n",
      "116/223, train_loss: 0.1006, step time: 0.1105\n",
      "117/223, train_loss: 0.0983, step time: 0.1272\n",
      "118/223, train_loss: 0.1035, step time: 0.1076\n",
      "119/223, train_loss: 0.0920, step time: 0.1024\n",
      "120/223, train_loss: 0.0992, step time: 0.1074\n",
      "121/223, train_loss: 0.0911, step time: 0.1105\n",
      "122/223, train_loss: 0.0964, step time: 0.1188\n",
      "123/223, train_loss: 0.0913, step time: 0.1325\n",
      "124/223, train_loss: 0.1101, step time: 0.1117\n",
      "125/223, train_loss: 0.1021, step time: 0.1120\n",
      "126/223, train_loss: 0.0933, step time: 0.1162\n",
      "127/223, train_loss: 0.1083, step time: 0.1026\n",
      "128/223, train_loss: 0.0975, step time: 0.1089\n",
      "129/223, train_loss: 0.0991, step time: 0.1060\n",
      "130/223, train_loss: 0.1049, step time: 0.1213\n",
      "131/223, train_loss: 0.0876, step time: 0.1348\n",
      "132/223, train_loss: 0.0919, step time: 0.1313\n",
      "133/223, train_loss: 0.1000, step time: 0.1001\n",
      "134/223, train_loss: 0.0990, step time: 0.1160\n",
      "135/223, train_loss: 0.0968, step time: 0.1111\n",
      "136/223, train_loss: 0.1068, step time: 0.1004\n",
      "137/223, train_loss: 0.1126, step time: 0.1009\n",
      "138/223, train_loss: 0.0961, step time: 0.1189\n",
      "139/223, train_loss: 0.0965, step time: 0.1131\n",
      "140/223, train_loss: 0.1017, step time: 0.1130\n",
      "141/223, train_loss: 0.1053, step time: 0.1014\n",
      "142/223, train_loss: 0.1037, step time: 0.1546\n",
      "143/223, train_loss: 0.1095, step time: 0.1363\n",
      "144/223, train_loss: 0.0956, step time: 0.1050\n",
      "145/223, train_loss: 0.0966, step time: 0.1056\n",
      "146/223, train_loss: 0.0935, step time: 0.1000\n",
      "147/223, train_loss: 0.0938, step time: 0.1309\n",
      "148/223, train_loss: 0.0996, step time: 0.1070\n",
      "149/223, train_loss: 0.1118, step time: 0.1010\n",
      "150/223, train_loss: 0.0949, step time: 0.1042\n",
      "151/223, train_loss: 0.1081, step time: 0.0997\n",
      "152/223, train_loss: 0.1015, step time: 0.1189\n",
      "153/223, train_loss: 0.0902, step time: 0.1177\n",
      "154/223, train_loss: 0.1093, step time: 0.0998\n",
      "155/223, train_loss: 0.2987, step time: 0.1066\n",
      "156/223, train_loss: 0.0946, step time: 0.1057\n",
      "157/223, train_loss: 0.1074, step time: 0.1025\n",
      "158/223, train_loss: 0.0930, step time: 0.1137\n",
      "159/223, train_loss: 0.1037, step time: 0.1118\n",
      "160/223, train_loss: 0.1047, step time: 0.1036\n",
      "161/223, train_loss: 0.1086, step time: 0.1025\n",
      "162/223, train_loss: 0.0922, step time: 0.1000\n",
      "163/223, train_loss: 0.1036, step time: 0.1328\n",
      "164/223, train_loss: 0.0926, step time: 0.1170\n",
      "165/223, train_loss: 0.1035, step time: 0.1005\n",
      "166/223, train_loss: 0.1033, step time: 0.1005\n",
      "167/223, train_loss: 0.1016, step time: 0.1116\n",
      "168/223, train_loss: 0.1031, step time: 0.1449\n",
      "169/223, train_loss: 0.1036, step time: 0.1038\n",
      "170/223, train_loss: 0.0957, step time: 0.1131\n",
      "171/223, train_loss: 0.0915, step time: 0.1156\n",
      "172/223, train_loss: 0.0959, step time: 0.1164\n",
      "173/223, train_loss: 0.0961, step time: 0.1006\n",
      "174/223, train_loss: 0.1044, step time: 0.1081\n",
      "175/223, train_loss: 0.0918, step time: 0.1007\n",
      "176/223, train_loss: 0.0879, step time: 0.0996\n",
      "177/223, train_loss: 0.0994, step time: 0.1007\n",
      "178/223, train_loss: 0.0917, step time: 0.1167\n",
      "179/223, train_loss: 0.0938, step time: 0.1008\n",
      "180/223, train_loss: 0.1169, step time: 0.1069\n",
      "181/223, train_loss: 0.0941, step time: 0.1012\n",
      "182/223, train_loss: 0.0998, step time: 0.1111\n",
      "183/223, train_loss: 0.0999, step time: 0.1061\n",
      "184/223, train_loss: 0.1007, step time: 0.1284\n",
      "185/223, train_loss: 0.1029, step time: 0.1066\n",
      "186/223, train_loss: 0.1007, step time: 0.1055\n",
      "187/223, train_loss: 0.0949, step time: 0.1054\n",
      "188/223, train_loss: 0.0881, step time: 0.1201\n",
      "189/223, train_loss: 0.0999, step time: 0.1003\n",
      "190/223, train_loss: 0.0918, step time: 0.1127\n",
      "191/223, train_loss: 0.1005, step time: 0.1059\n",
      "192/223, train_loss: 0.0954, step time: 0.1216\n",
      "193/223, train_loss: 0.0987, step time: 0.1054\n",
      "194/223, train_loss: 0.1039, step time: 0.1016\n",
      "195/223, train_loss: 0.0908, step time: 0.1108\n",
      "196/223, train_loss: 0.0934, step time: 0.1301\n",
      "197/223, train_loss: 0.0913, step time: 0.1131\n",
      "198/223, train_loss: 0.1013, step time: 0.1070\n",
      "199/223, train_loss: 0.0971, step time: 0.1037\n",
      "200/223, train_loss: 0.0996, step time: 0.1373\n",
      "201/223, train_loss: 0.0927, step time: 0.1635\n",
      "202/223, train_loss: 0.1060, step time: 0.1120\n",
      "203/223, train_loss: 0.1075, step time: 0.1189\n",
      "204/223, train_loss: 0.0908, step time: 0.1170\n",
      "205/223, train_loss: 0.0883, step time: 0.1089\n",
      "206/223, train_loss: 0.0888, step time: 0.1028\n",
      "207/223, train_loss: 0.0903, step time: 0.1014\n",
      "208/223, train_loss: 0.1018, step time: 0.1248\n",
      "209/223, train_loss: 0.0950, step time: 0.1015\n",
      "210/223, train_loss: 0.1208, step time: 0.1107\n",
      "211/223, train_loss: 0.1008, step time: 0.1068\n",
      "212/223, train_loss: 0.1030, step time: 0.1072\n",
      "213/223, train_loss: 0.1033, step time: 0.1056\n",
      "214/223, train_loss: 0.0997, step time: 0.1013\n",
      "215/223, train_loss: 0.0886, step time: 0.1000\n",
      "216/223, train_loss: 0.1030, step time: 0.1043\n",
      "217/223, train_loss: 0.1000, step time: 0.1000\n",
      "218/223, train_loss: 0.0891, step time: 0.1005\n",
      "219/223, train_loss: 0.0916, step time: 0.1083\n",
      "220/223, train_loss: 0.0962, step time: 0.1004\n",
      "221/223, train_loss: 0.1091, step time: 0.1010\n",
      "222/223, train_loss: 0.0975, step time: 0.0994\n",
      "223/223, train_loss: 0.1057, step time: 0.1000\n",
      "epoch 298 average loss: 0.0995\n",
      "time consuming of epoch 298 is: 91.4084\n",
      "----------\n",
      "epoch 299/300\n",
      "1/223, train_loss: 0.0902, step time: 0.1054\n",
      "2/223, train_loss: 0.0916, step time: 0.1001\n",
      "3/223, train_loss: 0.1024, step time: 0.1269\n",
      "4/223, train_loss: 0.0954, step time: 0.1137\n",
      "5/223, train_loss: 0.0964, step time: 0.1150\n",
      "6/223, train_loss: 0.0999, step time: 0.1009\n",
      "7/223, train_loss: 0.1032, step time: 0.1142\n",
      "8/223, train_loss: 0.1032, step time: 0.1158\n",
      "9/223, train_loss: 0.0910, step time: 0.1054\n",
      "10/223, train_loss: 0.0985, step time: 0.1002\n",
      "11/223, train_loss: 0.0926, step time: 0.1017\n",
      "12/223, train_loss: 0.0860, step time: 0.1170\n",
      "13/223, train_loss: 0.0946, step time: 0.1053\n",
      "14/223, train_loss: 0.0927, step time: 0.1094\n",
      "15/223, train_loss: 0.0952, step time: 0.1229\n",
      "16/223, train_loss: 0.0932, step time: 0.1114\n",
      "17/223, train_loss: 0.1075, step time: 0.1147\n",
      "18/223, train_loss: 0.1047, step time: 0.1170\n",
      "19/223, train_loss: 0.0960, step time: 0.1290\n",
      "20/223, train_loss: 0.1050, step time: 0.1108\n",
      "21/223, train_loss: 0.0960, step time: 0.1071\n",
      "22/223, train_loss: 0.0947, step time: 0.1170\n",
      "23/223, train_loss: 0.0995, step time: 0.1005\n",
      "24/223, train_loss: 0.1051, step time: 0.1006\n",
      "25/223, train_loss: 0.0942, step time: 0.1033\n",
      "26/223, train_loss: 0.0996, step time: 0.1104\n",
      "27/223, train_loss: 0.1162, step time: 0.1372\n",
      "28/223, train_loss: 0.1031, step time: 0.1185\n",
      "29/223, train_loss: 0.0900, step time: 0.1070\n",
      "30/223, train_loss: 0.0891, step time: 0.1129\n",
      "31/223, train_loss: 0.1004, step time: 0.1061\n",
      "32/223, train_loss: 0.1042, step time: 0.1163\n",
      "33/223, train_loss: 0.0920, step time: 0.1577\n",
      "34/223, train_loss: 0.1009, step time: 0.1085\n",
      "35/223, train_loss: 0.0897, step time: 0.1184\n",
      "36/223, train_loss: 0.1175, step time: 0.1442\n",
      "37/223, train_loss: 0.1074, step time: 0.1259\n",
      "38/223, train_loss: 0.1033, step time: 0.1025\n",
      "39/223, train_loss: 0.0888, step time: 0.0998\n",
      "40/223, train_loss: 0.0889, step time: 0.1152\n",
      "41/223, train_loss: 0.1033, step time: 0.1256\n",
      "42/223, train_loss: 0.0980, step time: 0.1004\n",
      "43/223, train_loss: 0.1131, step time: 0.1005\n",
      "44/223, train_loss: 0.1005, step time: 0.1014\n",
      "45/223, train_loss: 0.1065, step time: 0.1006\n",
      "46/223, train_loss: 0.1150, step time: 0.1151\n",
      "47/223, train_loss: 0.1027, step time: 0.1016\n",
      "48/223, train_loss: 0.0872, step time: 0.1018\n",
      "49/223, train_loss: 0.0930, step time: 0.1031\n",
      "50/223, train_loss: 0.0959, step time: 0.1009\n",
      "51/223, train_loss: 0.0871, step time: 0.1602\n",
      "52/223, train_loss: 0.0971, step time: 0.1305\n",
      "53/223, train_loss: 0.0884, step time: 0.1009\n",
      "54/223, train_loss: 0.0951, step time: 0.1210\n",
      "55/223, train_loss: 0.1080, step time: 0.1131\n",
      "56/223, train_loss: 0.0969, step time: 0.1082\n",
      "57/223, train_loss: 0.0977, step time: 0.1134\n",
      "58/223, train_loss: 0.0954, step time: 0.1021\n",
      "59/223, train_loss: 0.0867, step time: 0.1139\n",
      "60/223, train_loss: 0.1026, step time: 0.1097\n",
      "61/223, train_loss: 0.0942, step time: 0.1153\n",
      "62/223, train_loss: 0.1006, step time: 0.1000\n",
      "63/223, train_loss: 0.1062, step time: 0.1296\n",
      "64/223, train_loss: 0.1011, step time: 0.1273\n",
      "65/223, train_loss: 0.1023, step time: 0.1051\n",
      "66/223, train_loss: 0.0949, step time: 0.1226\n",
      "67/223, train_loss: 0.1041, step time: 0.1079\n",
      "68/223, train_loss: 0.2990, step time: 0.1105\n",
      "69/223, train_loss: 0.0962, step time: 0.1349\n",
      "70/223, train_loss: 0.0825, step time: 0.1087\n",
      "71/223, train_loss: 0.0864, step time: 0.1015\n",
      "72/223, train_loss: 0.0994, step time: 0.1282\n",
      "73/223, train_loss: 0.0978, step time: 0.1227\n",
      "74/223, train_loss: 0.0928, step time: 0.0993\n",
      "75/223, train_loss: 0.0894, step time: 0.1077\n",
      "76/223, train_loss: 0.1069, step time: 0.1241\n",
      "77/223, train_loss: 0.0964, step time: 0.1012\n",
      "78/223, train_loss: 0.0964, step time: 0.1009\n",
      "79/223, train_loss: 0.0986, step time: 0.1350\n",
      "80/223, train_loss: 0.0903, step time: 0.1242\n",
      "81/223, train_loss: 0.0918, step time: 0.1195\n",
      "82/223, train_loss: 0.0982, step time: 0.1077\n",
      "83/223, train_loss: 0.0983, step time: 0.1086\n",
      "84/223, train_loss: 0.0862, step time: 0.1285\n",
      "85/223, train_loss: 0.0943, step time: 0.1188\n",
      "86/223, train_loss: 0.1004, step time: 0.1147\n",
      "87/223, train_loss: 0.1088, step time: 0.1017\n",
      "88/223, train_loss: 0.0916, step time: 0.1127\n",
      "89/223, train_loss: 0.1128, step time: 0.1100\n",
      "90/223, train_loss: 0.0884, step time: 0.1202\n",
      "91/223, train_loss: 0.0916, step time: 0.1050\n",
      "92/223, train_loss: 0.0994, step time: 0.1009\n",
      "93/223, train_loss: 0.0991, step time: 0.1004\n",
      "94/223, train_loss: 0.0926, step time: 0.1093\n",
      "95/223, train_loss: 0.0953, step time: 0.1217\n",
      "96/223, train_loss: 0.0959, step time: 0.1045\n",
      "97/223, train_loss: 0.1023, step time: 0.1010\n",
      "98/223, train_loss: 0.0962, step time: 0.1179\n",
      "99/223, train_loss: 0.1079, step time: 0.1016\n",
      "100/223, train_loss: 0.0969, step time: 0.1111\n",
      "101/223, train_loss: 0.1053, step time: 0.1045\n",
      "102/223, train_loss: 0.1009, step time: 0.1138\n",
      "103/223, train_loss: 0.0968, step time: 0.1100\n",
      "104/223, train_loss: 0.0992, step time: 0.1092\n",
      "105/223, train_loss: 0.0915, step time: 0.1021\n",
      "106/223, train_loss: 0.0948, step time: 0.1133\n",
      "107/223, train_loss: 0.0950, step time: 0.1063\n",
      "108/223, train_loss: 0.0957, step time: 0.1076\n",
      "109/223, train_loss: 0.1124, step time: 0.1132\n",
      "110/223, train_loss: 0.0986, step time: 0.1161\n",
      "111/223, train_loss: 0.0986, step time: 0.1257\n",
      "112/223, train_loss: 0.0959, step time: 0.1218\n",
      "113/223, train_loss: 0.0975, step time: 0.1076\n",
      "114/223, train_loss: 0.0986, step time: 0.1173\n",
      "115/223, train_loss: 0.0884, step time: 0.1248\n",
      "116/223, train_loss: 0.1002, step time: 0.1310\n",
      "117/223, train_loss: 0.1035, step time: 0.1005\n",
      "118/223, train_loss: 0.1053, step time: 0.1010\n",
      "119/223, train_loss: 0.1017, step time: 0.1077\n",
      "120/223, train_loss: 0.1024, step time: 0.1666\n",
      "121/223, train_loss: 0.1020, step time: 0.1147\n",
      "122/223, train_loss: 0.0970, step time: 0.1005\n",
      "123/223, train_loss: 0.1019, step time: 0.1008\n",
      "124/223, train_loss: 0.0970, step time: 0.1010\n",
      "125/223, train_loss: 0.0956, step time: 0.1008\n",
      "126/223, train_loss: 0.0945, step time: 0.1185\n",
      "127/223, train_loss: 0.0963, step time: 0.1002\n",
      "128/223, train_loss: 0.1020, step time: 0.1318\n",
      "129/223, train_loss: 0.0897, step time: 0.1105\n",
      "130/223, train_loss: 0.0918, step time: 0.1104\n",
      "131/223, train_loss: 0.1000, step time: 0.1358\n",
      "132/223, train_loss: 0.1054, step time: 0.1073\n",
      "133/223, train_loss: 0.0894, step time: 0.1009\n",
      "134/223, train_loss: 0.1097, step time: 0.1301\n",
      "135/223, train_loss: 0.1012, step time: 0.1173\n",
      "136/223, train_loss: 0.1043, step time: 0.1003\n",
      "137/223, train_loss: 0.1021, step time: 0.1058\n",
      "138/223, train_loss: 0.1013, step time: 0.1078\n",
      "139/223, train_loss: 0.1010, step time: 0.1188\n",
      "140/223, train_loss: 0.1069, step time: 0.1314\n",
      "141/223, train_loss: 0.1037, step time: 0.1002\n",
      "142/223, train_loss: 0.0937, step time: 0.1052\n",
      "143/223, train_loss: 0.1002, step time: 0.1248\n",
      "144/223, train_loss: 0.0911, step time: 0.1053\n",
      "145/223, train_loss: 0.1052, step time: 0.1006\n",
      "146/223, train_loss: 0.0997, step time: 0.1496\n",
      "147/223, train_loss: 0.1200, step time: 0.1668\n",
      "148/223, train_loss: 0.0930, step time: 0.1287\n",
      "149/223, train_loss: 0.0986, step time: 0.0997\n",
      "150/223, train_loss: 0.1011, step time: 0.1004\n",
      "151/223, train_loss: 0.1034, step time: 0.1005\n",
      "152/223, train_loss: 0.0949, step time: 0.1035\n",
      "153/223, train_loss: 0.1034, step time: 0.0998\n",
      "154/223, train_loss: 0.0983, step time: 0.1004\n",
      "155/223, train_loss: 0.0955, step time: 0.1001\n",
      "156/223, train_loss: 0.0956, step time: 0.1038\n",
      "157/223, train_loss: 0.0959, step time: 0.0995\n",
      "158/223, train_loss: 0.0921, step time: 0.1003\n",
      "159/223, train_loss: 0.0966, step time: 0.1009\n",
      "160/223, train_loss: 0.1095, step time: 0.1139\n",
      "161/223, train_loss: 0.0902, step time: 0.0993\n",
      "162/223, train_loss: 0.1043, step time: 0.1002\n",
      "163/223, train_loss: 0.0938, step time: 0.1005\n",
      "164/223, train_loss: 0.0974, step time: 0.1068\n",
      "165/223, train_loss: 0.0940, step time: 0.0990\n",
      "166/223, train_loss: 0.0950, step time: 0.0996\n",
      "167/223, train_loss: 0.1035, step time: 0.0999\n",
      "168/223, train_loss: 0.0972, step time: 0.0997\n",
      "169/223, train_loss: 0.1025, step time: 0.1002\n",
      "170/223, train_loss: 0.0884, step time: 0.1003\n",
      "171/223, train_loss: 0.1084, step time: 0.1001\n",
      "172/223, train_loss: 0.0888, step time: 0.1086\n",
      "173/223, train_loss: 0.0974, step time: 0.1142\n",
      "174/223, train_loss: 0.0998, step time: 0.1246\n",
      "175/223, train_loss: 0.0900, step time: 0.1033\n",
      "176/223, train_loss: 0.1002, step time: 0.1205\n",
      "177/223, train_loss: 0.1084, step time: 0.1193\n",
      "178/223, train_loss: 0.1023, step time: 0.1087\n",
      "179/223, train_loss: 0.1191, step time: 0.1202\n",
      "180/223, train_loss: 0.1079, step time: 0.1140\n",
      "181/223, train_loss: 0.1007, step time: 0.1152\n",
      "182/223, train_loss: 0.0993, step time: 0.1171\n",
      "183/223, train_loss: 0.1048, step time: 0.1002\n",
      "184/223, train_loss: 0.0973, step time: 0.1104\n",
      "185/223, train_loss: 0.0975, step time: 0.1010\n",
      "186/223, train_loss: 0.1033, step time: 0.1126\n",
      "187/223, train_loss: 0.0924, step time: 0.1109\n",
      "188/223, train_loss: 0.1083, step time: 0.1351\n",
      "189/223, train_loss: 0.0904, step time: 0.1273\n",
      "190/223, train_loss: 0.1022, step time: 0.1130\n",
      "191/223, train_loss: 0.0868, step time: 0.1236\n",
      "192/223, train_loss: 0.0998, step time: 0.1057\n",
      "193/223, train_loss: 0.0921, step time: 0.1001\n",
      "194/223, train_loss: 0.0950, step time: 0.1159\n",
      "195/223, train_loss: 0.1046, step time: 0.1362\n",
      "196/223, train_loss: 0.1072, step time: 0.1081\n",
      "197/223, train_loss: 0.0963, step time: 0.1263\n",
      "198/223, train_loss: 0.0975, step time: 0.1211\n",
      "199/223, train_loss: 0.0868, step time: 0.1067\n",
      "200/223, train_loss: 0.1000, step time: 0.1172\n",
      "201/223, train_loss: 0.1110, step time: 0.1016\n",
      "202/223, train_loss: 0.0953, step time: 0.1142\n",
      "203/223, train_loss: 0.0985, step time: 0.1002\n",
      "204/223, train_loss: 0.0898, step time: 0.1159\n",
      "205/223, train_loss: 0.0932, step time: 0.1152\n",
      "206/223, train_loss: 0.1128, step time: 0.1207\n",
      "207/223, train_loss: 0.1029, step time: 0.1173\n",
      "208/223, train_loss: 0.1025, step time: 0.1298\n",
      "209/223, train_loss: 0.0967, step time: 0.1109\n",
      "210/223, train_loss: 0.0889, step time: 0.1133\n",
      "211/223, train_loss: 0.1029, step time: 0.1261\n",
      "212/223, train_loss: 0.1082, step time: 0.1263\n",
      "213/223, train_loss: 0.1046, step time: 0.1054\n",
      "214/223, train_loss: 0.0959, step time: 0.1395\n",
      "215/223, train_loss: 0.0980, step time: 0.1324\n",
      "216/223, train_loss: 0.0947, step time: 0.1181\n",
      "217/223, train_loss: 0.0965, step time: 0.1073\n",
      "218/223, train_loss: 0.0961, step time: 0.1139\n",
      "219/223, train_loss: 0.0988, step time: 0.1156\n",
      "220/223, train_loss: 0.1022, step time: 0.1052\n",
      "221/223, train_loss: 0.0905, step time: 0.0996\n",
      "222/223, train_loss: 0.0951, step time: 0.0998\n",
      "223/223, train_loss: 0.1079, step time: 0.1000\n",
      "epoch 299 average loss: 0.0994\n",
      "time consuming of epoch 299 is: 93.7347\n",
      "----------\n",
      "epoch 300/300\n",
      "1/223, train_loss: 0.0909, step time: 0.1024\n",
      "2/223, train_loss: 0.1074, step time: 0.1121\n",
      "3/223, train_loss: 0.0922, step time: 0.1204\n",
      "4/223, train_loss: 0.1129, step time: 0.1188\n",
      "5/223, train_loss: 0.1091, step time: 0.1056\n",
      "6/223, train_loss: 0.0861, step time: 0.1118\n",
      "7/223, train_loss: 0.1005, step time: 0.1431\n",
      "8/223, train_loss: 0.1029, step time: 0.1359\n",
      "9/223, train_loss: 0.3049, step time: 0.1149\n",
      "10/223, train_loss: 0.0971, step time: 0.1158\n",
      "11/223, train_loss: 0.0934, step time: 0.1416\n",
      "12/223, train_loss: 0.0990, step time: 0.1094\n",
      "13/223, train_loss: 0.1044, step time: 0.1156\n",
      "14/223, train_loss: 0.0973, step time: 0.1001\n",
      "15/223, train_loss: 0.0897, step time: 0.1002\n",
      "16/223, train_loss: 0.1104, step time: 0.1192\n",
      "17/223, train_loss: 0.0988, step time: 0.1383\n",
      "18/223, train_loss: 0.0930, step time: 0.1491\n",
      "19/223, train_loss: 0.1034, step time: 0.1005\n",
      "20/223, train_loss: 0.0963, step time: 0.1284\n",
      "21/223, train_loss: 0.0895, step time: 0.1276\n",
      "22/223, train_loss: 0.0953, step time: 0.1264\n",
      "23/223, train_loss: 0.1054, step time: 0.1075\n",
      "24/223, train_loss: 0.0994, step time: 0.1164\n",
      "25/223, train_loss: 0.0903, step time: 0.1033\n",
      "26/223, train_loss: 0.0975, step time: 0.1029\n",
      "27/223, train_loss: 0.1091, step time: 0.1002\n",
      "28/223, train_loss: 0.1017, step time: 0.1067\n",
      "29/223, train_loss: 0.1029, step time: 0.1220\n",
      "30/223, train_loss: 0.1004, step time: 0.0995\n",
      "31/223, train_loss: 0.0937, step time: 0.1000\n",
      "32/223, train_loss: 0.1061, step time: 0.1179\n",
      "33/223, train_loss: 0.0966, step time: 0.1208\n",
      "34/223, train_loss: 0.0955, step time: 0.1132\n",
      "35/223, train_loss: 0.0862, step time: 0.1006\n",
      "36/223, train_loss: 0.0944, step time: 0.1188\n",
      "37/223, train_loss: 0.0909, step time: 0.1086\n",
      "38/223, train_loss: 0.1099, step time: 0.1167\n",
      "39/223, train_loss: 0.0908, step time: 0.1177\n",
      "40/223, train_loss: 0.1030, step time: 0.1150\n",
      "41/223, train_loss: 0.1050, step time: 0.1102\n",
      "42/223, train_loss: 0.1031, step time: 0.1127\n",
      "43/223, train_loss: 0.0961, step time: 0.1112\n",
      "44/223, train_loss: 0.0976, step time: 0.1190\n",
      "45/223, train_loss: 0.1089, step time: 0.1127\n",
      "46/223, train_loss: 0.0931, step time: 0.1101\n",
      "47/223, train_loss: 0.1010, step time: 0.0994\n",
      "48/223, train_loss: 0.0949, step time: 0.1198\n",
      "49/223, train_loss: 0.1016, step time: 0.1236\n",
      "50/223, train_loss: 0.1017, step time: 0.1160\n",
      "51/223, train_loss: 0.1083, step time: 0.1118\n",
      "52/223, train_loss: 0.1107, step time: 0.1157\n",
      "53/223, train_loss: 0.0947, step time: 0.1088\n",
      "54/223, train_loss: 0.1012, step time: 0.0986\n",
      "55/223, train_loss: 0.1086, step time: 0.0991\n",
      "56/223, train_loss: 0.0927, step time: 0.0987\n",
      "57/223, train_loss: 0.0913, step time: 0.1063\n",
      "58/223, train_loss: 0.1013, step time: 0.1107\n",
      "59/223, train_loss: 0.0911, step time: 0.0998\n",
      "60/223, train_loss: 0.0875, step time: 0.1092\n",
      "61/223, train_loss: 0.0994, step time: 0.1001\n",
      "62/223, train_loss: 0.0901, step time: 0.0997\n",
      "63/223, train_loss: 0.0983, step time: 0.1033\n",
      "64/223, train_loss: 0.0992, step time: 0.1307\n",
      "65/223, train_loss: 0.0954, step time: 0.1059\n",
      "66/223, train_loss: 0.0911, step time: 0.1007\n",
      "67/223, train_loss: 0.1020, step time: 0.1016\n",
      "68/223, train_loss: 0.1071, step time: 0.1056\n",
      "69/223, train_loss: 0.1058, step time: 0.1100\n",
      "70/223, train_loss: 0.1030, step time: 0.0999\n",
      "71/223, train_loss: 0.1062, step time: 0.1020\n",
      "72/223, train_loss: 0.0960, step time: 0.1005\n",
      "73/223, train_loss: 0.0920, step time: 0.1105\n",
      "74/223, train_loss: 0.0978, step time: 0.1032\n",
      "75/223, train_loss: 0.1000, step time: 0.1056\n",
      "76/223, train_loss: 0.1017, step time: 0.1558\n",
      "77/223, train_loss: 0.1072, step time: 0.1256\n",
      "78/223, train_loss: 0.0949, step time: 0.1135\n",
      "79/223, train_loss: 0.1059, step time: 0.1033\n",
      "80/223, train_loss: 0.0890, step time: 0.1119\n",
      "81/223, train_loss: 0.1053, step time: 0.0999\n",
      "82/223, train_loss: 0.0930, step time: 0.1004\n",
      "83/223, train_loss: 0.0989, step time: 0.1055\n",
      "84/223, train_loss: 0.0996, step time: 0.1009\n",
      "85/223, train_loss: 0.0993, step time: 0.1235\n",
      "86/223, train_loss: 0.0959, step time: 0.1079\n",
      "87/223, train_loss: 0.1064, step time: 0.1132\n",
      "88/223, train_loss: 0.0916, step time: 0.1015\n",
      "89/223, train_loss: 0.0982, step time: 0.1159\n",
      "90/223, train_loss: 0.0956, step time: 0.1441\n",
      "91/223, train_loss: 0.0967, step time: 0.1330\n",
      "92/223, train_loss: 0.0975, step time: 0.1173\n",
      "93/223, train_loss: 0.0945, step time: 0.1158\n",
      "94/223, train_loss: 0.1075, step time: 0.1067\n",
      "95/223, train_loss: 0.0920, step time: 0.1014\n",
      "96/223, train_loss: 0.0976, step time: 0.1033\n",
      "97/223, train_loss: 0.0987, step time: 0.1191\n",
      "98/223, train_loss: 0.0972, step time: 0.1016\n",
      "99/223, train_loss: 0.0903, step time: 0.1335\n",
      "100/223, train_loss: 0.1118, step time: 0.1045\n",
      "101/223, train_loss: 0.1123, step time: 0.1101\n",
      "102/223, train_loss: 0.1036, step time: 0.1216\n",
      "103/223, train_loss: 0.0987, step time: 0.1068\n",
      "104/223, train_loss: 0.0873, step time: 0.1043\n",
      "105/223, train_loss: 0.1039, step time: 0.1187\n",
      "106/223, train_loss: 0.0978, step time: 0.1115\n",
      "107/223, train_loss: 0.1096, step time: 0.1042\n",
      "108/223, train_loss: 0.0948, step time: 0.1155\n",
      "109/223, train_loss: 0.0998, step time: 0.1047\n",
      "110/223, train_loss: 0.1013, step time: 0.1191\n",
      "111/223, train_loss: 0.0921, step time: 0.1426\n",
      "112/223, train_loss: 0.0913, step time: 0.1041\n",
      "113/223, train_loss: 0.0940, step time: 0.1001\n",
      "114/223, train_loss: 0.1056, step time: 0.1206\n",
      "115/223, train_loss: 0.1103, step time: 0.1022\n",
      "116/223, train_loss: 0.1072, step time: 0.1165\n",
      "117/223, train_loss: 0.1002, step time: 0.1197\n",
      "118/223, train_loss: 0.0922, step time: 0.1009\n",
      "119/223, train_loss: 0.1027, step time: 0.1067\n",
      "120/223, train_loss: 0.1030, step time: 0.1354\n",
      "121/223, train_loss: 0.0912, step time: 0.1164\n",
      "122/223, train_loss: 0.0986, step time: 0.1093\n",
      "123/223, train_loss: 0.1010, step time: 0.1318\n",
      "124/223, train_loss: 0.0988, step time: 0.1014\n",
      "125/223, train_loss: 0.1018, step time: 0.1029\n",
      "126/223, train_loss: 0.1104, step time: 0.1226\n",
      "127/223, train_loss: 0.0927, step time: 0.1274\n",
      "128/223, train_loss: 0.0980, step time: 0.1002\n",
      "129/223, train_loss: 0.1011, step time: 0.1453\n",
      "130/223, train_loss: 0.0978, step time: 0.1000\n",
      "131/223, train_loss: 0.0950, step time: 0.1049\n",
      "132/223, train_loss: 0.0870, step time: 0.1135\n",
      "133/223, train_loss: 0.0973, step time: 0.1164\n",
      "134/223, train_loss: 0.0983, step time: 0.1214\n",
      "135/223, train_loss: 0.1039, step time: 0.1002\n",
      "136/223, train_loss: 0.0889, step time: 0.1092\n",
      "137/223, train_loss: 0.0983, step time: 0.1272\n",
      "138/223, train_loss: 0.0947, step time: 0.1555\n",
      "139/223, train_loss: 0.0906, step time: 0.1208\n",
      "140/223, train_loss: 0.0985, step time: 0.1006\n",
      "141/223, train_loss: 0.0981, step time: 0.1148\n",
      "142/223, train_loss: 0.1173, step time: 0.1073\n",
      "143/223, train_loss: 0.1029, step time: 0.1098\n",
      "144/223, train_loss: 0.0886, step time: 0.1108\n",
      "145/223, train_loss: 0.0895, step time: 0.1059\n",
      "146/223, train_loss: 0.0880, step time: 0.1176\n",
      "147/223, train_loss: 0.1025, step time: 0.1008\n",
      "148/223, train_loss: 0.1024, step time: 0.1003\n",
      "149/223, train_loss: 0.0998, step time: 0.0995\n",
      "150/223, train_loss: 0.0938, step time: 0.0998\n",
      "151/223, train_loss: 0.1053, step time: 0.1004\n",
      "152/223, train_loss: 0.1009, step time: 0.1009\n",
      "153/223, train_loss: 0.0961, step time: 0.0994\n",
      "154/223, train_loss: 0.0955, step time: 0.0999\n",
      "155/223, train_loss: 0.0918, step time: 0.0998\n",
      "156/223, train_loss: 0.1011, step time: 0.1004\n",
      "157/223, train_loss: 0.0929, step time: 0.0995\n",
      "158/223, train_loss: 0.1011, step time: 0.0991\n",
      "159/223, train_loss: 0.0964, step time: 0.1004\n",
      "160/223, train_loss: 0.1065, step time: 0.1006\n",
      "161/223, train_loss: 0.1028, step time: 0.0999\n",
      "162/223, train_loss: 0.0948, step time: 0.1010\n",
      "163/223, train_loss: 0.1100, step time: 0.1003\n",
      "164/223, train_loss: 0.0913, step time: 0.1215\n",
      "165/223, train_loss: 0.0963, step time: 0.1057\n",
      "166/223, train_loss: 0.0996, step time: 0.1016\n",
      "167/223, train_loss: 0.1082, step time: 0.1045\n",
      "168/223, train_loss: 0.1127, step time: 0.1206\n",
      "169/223, train_loss: 0.1016, step time: 0.1380\n",
      "170/223, train_loss: 0.0940, step time: 0.1216\n",
      "171/223, train_loss: 0.0940, step time: 0.1236\n",
      "172/223, train_loss: 0.0985, step time: 0.1062\n",
      "173/223, train_loss: 0.1024, step time: 0.1247\n",
      "174/223, train_loss: 0.0953, step time: 0.1009\n",
      "175/223, train_loss: 0.0958, step time: 0.1010\n",
      "176/223, train_loss: 0.1132, step time: 0.1114\n",
      "177/223, train_loss: 0.1052, step time: 0.1108\n",
      "178/223, train_loss: 0.1039, step time: 0.1151\n",
      "179/223, train_loss: 0.0924, step time: 0.1004\n",
      "180/223, train_loss: 0.0942, step time: 0.1329\n",
      "181/223, train_loss: 0.0972, step time: 0.1267\n",
      "182/223, train_loss: 0.0951, step time: 0.1131\n",
      "183/223, train_loss: 0.0923, step time: 0.1325\n",
      "184/223, train_loss: 0.0992, step time: 0.1039\n",
      "185/223, train_loss: 0.0996, step time: 0.1172\n",
      "186/223, train_loss: 0.0946, step time: 0.1046\n",
      "187/223, train_loss: 0.1018, step time: 0.1062\n",
      "188/223, train_loss: 0.1028, step time: 0.1054\n",
      "189/223, train_loss: 0.1095, step time: 0.1064\n",
      "190/223, train_loss: 0.0976, step time: 0.1177\n",
      "191/223, train_loss: 0.0965, step time: 0.1208\n",
      "192/223, train_loss: 0.0910, step time: 0.1764\n",
      "193/223, train_loss: 0.0909, step time: 0.1000\n",
      "194/223, train_loss: 0.1022, step time: 0.0999\n",
      "195/223, train_loss: 0.0942, step time: 0.1005\n",
      "196/223, train_loss: 0.1006, step time: 0.1144\n",
      "197/223, train_loss: 0.0963, step time: 0.1083\n",
      "198/223, train_loss: 0.0889, step time: 0.1304\n",
      "199/223, train_loss: 0.1024, step time: 0.1379\n",
      "200/223, train_loss: 0.0919, step time: 0.1007\n",
      "201/223, train_loss: 0.0996, step time: 0.1199\n",
      "202/223, train_loss: 0.0905, step time: 0.1164\n",
      "203/223, train_loss: 0.0974, step time: 0.1118\n",
      "204/223, train_loss: 0.0993, step time: 0.1291\n",
      "205/223, train_loss: 0.0984, step time: 0.1149\n",
      "206/223, train_loss: 0.1007, step time: 0.1189\n",
      "207/223, train_loss: 0.0948, step time: 0.1107\n",
      "208/223, train_loss: 0.1030, step time: 0.1305\n",
      "209/223, train_loss: 0.0973, step time: 0.1145\n",
      "210/223, train_loss: 0.1008, step time: 0.1091\n",
      "211/223, train_loss: 0.0962, step time: 0.1208\n",
      "212/223, train_loss: 0.0956, step time: 0.1249\n",
      "213/223, train_loss: 0.1009, step time: 0.1082\n",
      "214/223, train_loss: 0.0888, step time: 0.1102\n",
      "215/223, train_loss: 0.0962, step time: 0.1330\n",
      "216/223, train_loss: 0.0899, step time: 0.1487\n",
      "217/223, train_loss: 0.0956, step time: 0.1143\n",
      "218/223, train_loss: 0.1082, step time: 0.1007\n",
      "219/223, train_loss: 0.0975, step time: 0.1104\n",
      "220/223, train_loss: 0.0894, step time: 0.1005\n",
      "221/223, train_loss: 0.0928, step time: 0.0984\n",
      "222/223, train_loss: 0.0929, step time: 0.0993\n",
      "223/223, train_loss: 0.0920, step time: 0.0998\n",
      "epoch 300 average loss: 0.0995\n",
      "current epoch: 300 current mean dice: 0.8622 tc: 0.9225 wt: 0.8718 et: 0.7923\n",
      "best mean dice: 0.8623 at epoch: 295\n",
      "time consuming of epoch 300 is: 97.0586\n",
      "train completed, best_metric: 0.8623 at epoch: 295\n"
     ]
    }
   ],
   "source": [
    "best_metric = -1\n",
    "best_metric_epoch = -1\n",
    "best_metrics_epochs_and_time = [[], [], []]\n",
    "epoch_loss_values = []\n",
    "metric_values = []\n",
    "metric_values_tc = []\n",
    "metric_values_wt = []\n",
    "metric_values_et = []\n",
    "\n",
    "total_start = time.time()\n",
    "for epoch in range(max_epochs):\n",
    "    epoch_start = time.time()\n",
    "    print(\"-\" * 10)\n",
    "    print(f\"epoch {epoch + 1}/{max_epochs}\")\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    step = 0\n",
    "    for batch_data in train_loader:\n",
    "        step_start = time.time()\n",
    "        step += 1\n",
    "        inputs, labels = (\n",
    "            batch_data[\"image\"].to(device),\n",
    "            batch_data[\"label\"].to(device),\n",
    "        )\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_function(outputs, labels)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        epoch_loss += loss.item()\n",
    "        print(\n",
    "            f\"{step}/{len(train_ds) // train_loader.batch_size}\"\n",
    "            f\", train_loss: {loss.item():.4f}\"\n",
    "            f\", step time: {(time.time() - step_start):.4f}\"\n",
    "        )\n",
    "    lr_scheduler.step()\n",
    "    epoch_loss /= step\n",
    "    epoch_loss_values.append(epoch_loss)\n",
    "    print(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    if (epoch + 1) % val_interval == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for val_data in val_loader:\n",
    "                val_inputs, val_labels = (\n",
    "                    val_data[\"image\"].to(device),\n",
    "                    val_data[\"label\"].to(device),\n",
    "                )\n",
    "                val_outputs = inference(val_inputs)\n",
    "                val_outputs = [post_trans(i) for i in decollate_batch(val_outputs)]\n",
    "                dice_metric(y_pred=val_outputs, y=val_labels)\n",
    "                dice_metric_batch(y_pred=val_outputs, y=val_labels)\n",
    "\n",
    "            metric = dice_metric.aggregate().item()\n",
    "            metric_values.append(metric)\n",
    "            metric_batch = dice_metric_batch.aggregate()\n",
    "            metric_tc = metric_batch[0].item()\n",
    "            metric_values_tc.append(metric_tc)\n",
    "            metric_wt = metric_batch[1].item()\n",
    "            metric_values_wt.append(metric_wt)\n",
    "            metric_et = metric_batch[2].item()\n",
    "            metric_values_et.append(metric_et)\n",
    "            dice_metric.reset()\n",
    "            dice_metric_batch.reset()\n",
    "\n",
    "            if metric > best_metric:\n",
    "                best_metric = metric\n",
    "                best_metric_epoch = epoch + 1\n",
    "                best_metrics_epochs_and_time[0].append(best_metric)\n",
    "                best_metrics_epochs_and_time[1].append(best_metric_epoch)\n",
    "                best_metrics_epochs_and_time[2].append(time.time() - total_start)\n",
    "                torch.save(\n",
    "                    model.state_dict(),\n",
    "                    os.path.join(root_dir, \"best_metric_model.pth\"),\n",
    "                )\n",
    "                print(\"saved new best metric model\")\n",
    "            print(\n",
    "                f\"current epoch: {epoch + 1} current mean dice: {metric:.4f}\"\n",
    "                f\" tc: {metric_tc:.4f} wt: {metric_wt:.4f} et: {metric_et:.4f}\"\n",
    "                f\"\\nbest mean dice: {best_metric:.4f}\"\n",
    "                f\" at epoch: {best_metric_epoch}\"\n",
    "            )\n",
    "    print(f\"time consuming of epoch {epoch + 1} is: {(time.time() - epoch_start):.4f}\")\n",
    "total_time = time.time() - total_start\n",
    "print(f\"train completed, best_metric: {best_metric:.4f} at epoch: {best_metric_epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train completed, best_metric: 0.8623 at epoch: 295, total time: 27041.378520965576.\n"
     ]
    }
   ],
   "source": [
    "print(f\"train completed, best_metric: {best_metric:.4f} at epoch: {best_metric_epoch}, total time: {total_time}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the loss and metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsIAAAGDCAYAAAAh/naNAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAAsTAAALEwEAmpwYAABENUlEQVR4nO3deZhcZZn38e/d3el09p0tK4GwhF0jqCiC7C6EcQ2CghvjKPqioyM4rqgz4ow6zgyjgDCiIogsTtRIZBRwQSBhIECCQAiQhS2BrCS95nn/OKdD0XSSTujuOlX1/VxXXVV1zqmquyrJ6V/ufp6nIqWEJEmSVGvqyl2AJEmSVA4GYUmSJNUkg7AkSZJqkkFYkiRJNckgLEmSpJpkEJYkSVJNMgirT0VEioi9y12HJGnHVeM5PCIWRsTR5a5DxWAQriER8VhEbIqIDSWX/yx3XV1FxFn5yffd5a7l5YqIKfl7aSh3LZJqT0TcGBEXdLN9ZkQ89XLOTRFxS35+O6TL9hvy7Ufv7HPvZD2d59vOn29PR8SvIuL40uNSSgeklG7pz9pUXAbh2vPWlNLQkss55S6oG2cCzwHv64snN5RKqiFXAGdERHTZ/l7gypRS+8t8/ocoOVdHxBjgNcDKl/m8L8fIlNJQ4BDgJuCGiDirjPWowAzCArZ0Yf8cEf8ZEWsj4q8RcWzJ/j0iYnZEPBcRiyPiwyX76iPicxHxSESsj4i7ImJiydMfFxEPR8SaiLiomxNyaR2TgTcAZwMnRsRu+fbvRcS/djn2fyLiUyX1XRcRKyPi0Yj4RMlxX46IayPiJxGxDjgrIg6PiL/kNT2Zv+/GksecEBEP5p/Ff0XErRHxoZL9H4iIByJidUTMzeve0c98W5/p4RExPyLW5V2Nb+fbm/L38Wxe+7yI2HVHX1tSzfgFMAZ4feeGiBgFvAX40fbOhT1wJfDuiKjP758G3AC0lrxeXUScl/+MeDYiromI0SX7f553p9dGxB8i4oCSfT/Mf278Ov/5ckdE7NWTwlJKT6WUvgt8GbgwIury53wsIo7Lb2/151dE7BcRN+Xn6Acj4l078LmoQhiEVeoI4BFgLPAl4PqSk9XVwHJgD+AdwD9FxBvzfZ8iO/m9CRgOfADYWPK8bwFeBRwMvAs4cRs1vA+Yn1K6DngAOD3ffhXZyTZgy4n8BODq/OT2S2ABMB44Fjg3IkpfZyZwLTCS7MTdAXwyf6+vyR/z0fy5x+bHnk/2A+RB4LWdTxQRM4HPAW8DxgF/zOvbUdv6TL8LfDelNBzYC7gm334mMAKYmNf2EWDTTry2pBqQUtpEdv4o/Q3bu4C/ppQWsI1zYQ89ASwiOx+Tv86PuhzzceBUsibHHsBq4KKS/b8BpgG7AP9Hdo4uNQv4CjAKWAx8fQfqA7g+f+59u9nX7c+viBhC1k3+af7YWcB/RcT0HXxtFV1KyUuNXIDHgA3AmpLLh/N9Z5Gd0KLk+DvJfn02kexkOaxk3z8DP8xvPwjM3MprJuB1JfevAc7bRo0PA+fmt88HFuS3A1gKHJXf/zDw+/z2EcDSLs9zPvDf+e0vA3/YzmdzLnBDfvt9wF9K9gWwDPhQfv83wAdL9teRBf/J3TzvlPwzaOiyfXuf6R/ITvxjuzzuA8BtwMHl/vvkxYuXyrgAr8vP9035/T8Dn9zKsVvOhfn9BOy9lWNvAT4EnEHWDNgPeCjftxw4Or/9AHBsyeN2B9q6nhfzfSPz1xyR3/8h8IOS/W8iC/Hd1bO1821Tvv3I/P5jwHH57W5/fgHvBv7YZdvFwJfK/efppXcvdoRrz6kppZEll0tL9q1I+b/23ONk/3vfA3gupbS+y77x+e2JZJ3krXmq5PZGYGh3B0XEkcCeZJ1SyP4nflBEHJrXdTXZ/9wB3sMLXYPJwB75r/bWRMQaso5t6ZCBZV1ea5/IJlE8lQ+X+Ceyjgj5+91yfP7ay0sePhn4bslrPUcWlsfTc9v7TD8I7AP8NR/+8JZ8+4+BuWSd8Cci4psRMWAHXldSjUkp/QlYBZyaDys4nOz8ur1zYU9dD7wROIfsHNXVZLJxup3nzAfIGgG75kMTvpEPTVhHFlLpUkOPfoZsQ+d59blu9m3t59dk4IguP1dOB3bbwddWwRmEVWp859CD3CSyLvETwOiIGNZl34r89jKyX9+/XGeSBcp7IuIp4I6S7ZB1HN6Rj8c9Ariu5PUf7RLwh6WU3lTy3KUBH+B7wF+BaSkbfvC5/LUBngQmdB6YfyYTSh67DPjbLq83KKV02w68121+pimlh1NKp5H9Su5C4NqIGJJSakspfSWlNJ1suMZb6KNJhZKqyo/IzhVnAHNTSk/n27d1LuyRlNJGst+U/R3dB+FlwMldzplNKaUVZE2NmcBxZMO+puSP2aEatuNvgGfIur/d1dbdz69lwK1dah6aUvq7XqxLBWAQVqldgE9ExICIeCewPzAnpbSM7Nfx/5xP1jqYrGP5k/xxPwC+GhHTInNwZDOHeywimsjGrZ0NHFpy+TjwnohoSCndTdbV+AHZiXxN/vA7gfUR8dmIGJR3GA6MiFdt4yWHAeuADRGxH9kJvNOvyTrRp0a2wsTHeHEX4PvA+Z0TOiJiRP55bcvA/LNryt/rCrbxmUbEGRExLqW0mexXmgCbI+KYiDgon5iyjuzXi5u389qS9COysPlhspUkOm3rXLgjPge8IaX0WDf7vg98PW9iEBHj8rkWna/fAjwLDCbrSPeKiNg1Is4hm/Nyfn4+7WprP79+BewTEe/NfyYOiIhXRcT+vVWfisEgXHt+GS9eR/iGkn13kE1YWEU2GeEdKaVn832nkf1P/QmyGcFfSin9b77v22Rjf39LdkK9DBi0g3WdSjbp60cpm+n7VErpKeByoAE4KT/up2Qn8592PjCl1EHWGT0UeJQXwvKIbbzep8k6EeuBS4GflTzfKuCdwDfJTs7TgflkJ2tSSjeQdWmvzn+Vdz9w8nbe34b8/XVe3si2P9OTgIURsYFs4tyslE162Y1sIt86sl8v3kr3HRhJ2iIPqLcBQ4DZJbu2ei7cwed/Ih+C0Z3v5q/524hYD9xO9ls9yAL642TNgUX5vpdrTUQ8D9xHNqb4nSmly7dybLc/v/JhayeQTZJ7gmx4xoXAwF6oTwUSLx4SqloV2RqLH0opva7ctRRNvirFcuD0lNLN5a5HkiT1DjvCUjci4sSIGBkRA3lhzFxvdCokSVJBGISl7r2GbCbxKuCtZKttuF6vJElVxKERkiRJqkl2hCVJklSTDMKSJEmqSQ3leuGxY8emKVOmlOvlJWmn3XXXXatSSuPKXUd/8pwtqZJt7bxdtiA8ZcoU5s+fX66Xl6SdFhGPl7uG/uY5W1Il29p526ERkiRJqkkGYUmSJNUkg7AkSZJqkkFYkiRJNckgLEmSpJpkEJYkSVJNMghLkiSpJhmEJUmSVJMMwpIkSapJBmFJkiTVJIOwJEmSalJDuQvYIX/8IwwdCocdVu5KJEmSKlb75naeb32eDa0b2NC6gfbN7XSkDjanzXRs7iCRqI966uvqt1wPqBvAgPoBL7renDbTtrmN9s3ttHW00ZE6qIs66qKOIKiLrOfakTqy19jcQUfq2LJvy7ERbGrbxKb2TVuu2zraXnRMXdQxfvh49hmzT699DpUVhD/8YTjkEPjZz8pdiSRJUq/anDazbO0yHnr2IZauXcqydctYtnYZS9ctZdXGVbR2tNLS3kJrRyutHa0ADKgfQENdAw11DQyoG0BTQxODBgxiUMMgBg0YRH3Us65lHWtb1rK2eS1rW9ayvmU9LR0tZX63O+cjr/wI33vL93rt+SorCDc2QmtruauQJEnaIqXE6ubVtHW0vaiDujlt5qkNT7Fi3QpWrF/BinUrWNuydstjIA+/65bx4LMP8uCqB9nUvmnL8wbBbkN3Y+KIiUwYPoGB9QMZ2DCQxvpGGusagayz29mRbe1opbm9mU3tm9jYtpHnNj1HR+pg+MDh7D50d/Ybux8jBo5g+MDhDG0cuuUyeMBgBtQNoL6u/kXd3I7UsaWD27G5g7bNbbR1tL3ouj7qsxCeB/L6qCeR2Jw2szltJqVEIm3Z1/n5dL730ktpgB/UMIgB9QNIKb3omN2G7tarf3Y9CsIRcRLwXaAe+EFK6Rtd9n8HOCa/OxjYJaU0shfrzAwcaBCWJEkvMnfxXCKC46Yet+VX8V1tatvEktVLmDJyCkMah7xkf8fmDh5d8ygPrnqQxvpGxgwew5hBYxgzeAxNDU08sf4Jlq5duuXy+JrHeXxtflnzOM+3Pd+jWhvrGwkCgIjseo9he7DvmH05Zsox7Dd2P/YZsw+TR0xm/PDxNNY37uSnop7YbhCOiHrgIuB4YDkwLyJmp5QWdR6TUvpkyfEfB/pmEK8dYUmSlEsp8bU/fI0v3vJFAKaNnsbHXvUxzjr0LEY0jSClxJ+X/Zkr7rmCaxZdw7qWdUAWPKeNnsY+Y/ahtaOV+5+5n0UrF72oG7s9oweNZvKIyewzZh+On3o8k0ZMoqmh6UUd1Iiso7vHsD0YP2w8ewzbo9sQrvLpSUf4cGBxSmkJQERcDcwEFm3l+NOAL/VOeV00NkJLZY5pkSRJvaeto42P/OojXH7P5Zxx8BmctNdJXDTvIs6dey7/+Pt/5JR9T+GOFXewZPUShgwYwtunv51j9zyW5euW89CzD/Hwcw9zw19voKGugYN2OYiPzPgIB+5yIPuP3Z+O1MGzG5/l2U3PsmrjKja2bWT8sPFMGjFpy8VAWx16EoTHA8tK7i8HjujuwIiYDOwJ/H4r+88GzgaYNGnSDhUKZEF47dodf5wkSSq09s3tXHrXpazauOpFgXPiiIkvGR6wrmUd77jmHdy05Ca+cNQX+MrRXyEiOP3g07nribu4aN5FXPfAdbxqj1fxpTd8ibft/zaGNg4t0ztTkfX2ZLlZwLUppY7udqaULgEuAZgxY0ba4Wd3jLAkSVXnsTWPcfr1p3Pbsttesi8Ixg8fz16j9mLqqKlMHTWVaxZewwOrHuCyUy7jA4d94EXHv3KPV3L5zMu5fObl/VW+KlhPgvAKYGLJ/Qn5tu7MAj72covaKscIS5JUVa6890o+Ouej2e23Xcnb9n8by9ct3zIp7bE1j/HomkdZsnoJcx+ZyxPrn2D4wOHMec8cjt/r+DJXr0rXkyA8D5gWEXuSBeBZwHu6HhQR+wGjgL/0aoWlHCMsSVK/WdO8hqGNQ2mo6/3VVp9Y/wT/cNM/cOV9V3LkxCP5ydt+wpSRUwDYe/Te7D16724ft7FtI0EwaMCgXq9JtWe7f7NTSu0RcQ4wl2z5tMtTSgsj4gJgfkppdn7oLODq1LkwXl+wIyxJUp9btXEVX7r5S1x818UMHzicN+/zZk7Z5xRO2vskhg0ctkPPlVLimeef4a+r/sqdK+7kzifu5I7ld7Bs3TLqo56vHP0VPvf6z/U4bA8eMHhn3pLUrR79rUspzQHmdNn2xS73v9x7ZW2FY4QlSeozrR2tXHTnRXzl1q+woXUDHzzsgzR3NPOrh37FT+79CY31jRw58Ugmj5zMrkN2zS5Dd6WpoYl1LeuybzDLv73s8bWPs/i5xSx+bjEbWjdseY2po6Zy5KQjOXyPwzlhrxM4YJcDyviOVev8ZjlJkmrQ6k2reXLDkzy94Wmefv5pnlj/BN+f/30efu5hTtzrRL594reZPm46kK3o8Jdlf2H2g7O59fFb+d2S3/HUhqdo29zW7XM3NTQxcfhE9h69N0dNOoq9R+/NtDHTeOXur2TckHH9+TalbTIIS5JUA1JK3Pv0vVz/wPVc/9fruf+Z+19yzP5j92fOe+Zw8rSTX7S9oa6B109+Pa+f/PoXPd+a5jU8teEpWjpatnx177CBw/w2NFWMygvCTpaTJAAi4iTgu2TzN36QUvpGl/2TgCuAkfkx56WU5kTEFOAB4MH80NtTSh/pr7rVv1ZvWs2/3PYvXLPwGh5Z/Qh1UcfrJ72ebxz7DSaNmMSuQ7MhDrsN3Y3Rg0Zv+drf7YkIRg0axahBo/r4HUh9p/KCcFsbpAQ9/IcqSdUoIuqBi4Djyb7oaF5EzE4plX7r5+eBa1JK34uI6WRzPabk+x5JKR3ajyWrl6WUuOWxW1i0chFvn/52dhu620v2X3X/VXxy7idZtXEVJ+x1Ap898rPM3G8muwzZpUxVS8VSWUF44MDsuq0tC8WSVLsOBxanlJYARMTVwEygNAgnYHh+ewTwRL9WqD7R2tHKNQuv4dt/+TZ3P3U3AOfOPZdT9zuVj7zyIxyz5zEsWb2Ej/76o9y05CYOH384c8+Yy6G7HVrewqUCqqwg3Bl+W1sNwpJq3XhgWcn95cARXY75MvDbiPg4MAQ4rmTfnhFxN7AO+HxK6Y9dXyAizgbOBpg0aVLvVa4dsjltZunapTz07EPcsfwOvn/X93li/RPsP3Z/Ln3rpRwx/gh+eM8P+eGCH3LtomuZOmoqK9atoLG+kf88+T/5yIyPUF9XX+63IRVSZQbhlhYY6neGS9J2nAb8MKX0rYh4DfDjiDgQeBKYlFJ6NiJeCfwiIg5IKa0rfXBK6RLgEoAZM2b03RrxeomW9hY+97vPcdOSm3j4uYdpbm/esu+4qcfxg7f+gBP3PpG6qAPgWyd+i68f+3WuXXQtP7znh7x24mu58LgL2WPYHuV6C1JFqMwg7MoRkrQCmFhyf0K+rdQHgZMAUkp/iYgmYGxK6RmgJd9+V0Q8AuwDzO/zqrVdqzet5m3XvI1bHruFk/c+mRP2OoF9x+zLPmP2Yb+x+7Hr0F27fVxTQxNnHHwGZxx8Rj9XLFWuygrCnWOEDcKSNA+YFhF7kgXgWcB7uhyzFDgW+GFE7A80ASsjYhzwXEqpIyKmAtOAJf1Xurbm0dWP8qafvoklq5fwk7/5CacffHq5S5KqWmUFYTvCkgRASqk9Is4B5pItjXZ5SmlhRFwAzE8pzQb+Hrg0Ij5JNnHurJRSioijgAsiog3YDHwkpfRcmd6KcneuuJO3XvVW2jra+O0Zv+UNU95Q7pKkqleZQdi1hCWJlNIcsiXRSrd9seT2IuDIbh53HXBdnxeoHvvVQ7/iXT9/F7sO3ZXfnPUb9hu7X7lLkmpCXbkL2CF2hCVJFaS1o5Uv3fwlZj84e6vHXLfoOv7mZ3/D9HHTuf2DtxuCpX5UWR1hxwhLkirEmuY1vO1nb+Pmx24G4JOv/iQXHnchA+oHbDnm6vuv5ozrz+CICUcw5z1zGNE0olzlSjXJjrAkSb1s6dqlvO7y1/GnpX/islMu45xXncN3bv8OR19xNMvXLQfgRwt+xOnXn86Rk47kxtNvNARLZVBZHWHHCEuSCu7uJ+/mzT99MxvbNnLjGTfyxj3fCIfB6ya9jg/98kMcdvFhnHXIWXzrL9/imD2PYfas2QxpHFLusqWaVJlB2I6wJKlAnt34LPc8dQ/znpjH1//4dUY1jeJPH/gTB+5y4JZj3n3guzl0t0N5+zVv51//8q+ctPdJXP+u6xk0YFAZK5dqW2UFYccIS5IKoK2jjRv+egM/ve+n/N+T/8eydS982/WrJ7ya6951Xbff6rbv2H2540N38OuHf83MfWcysGFgf5YtqYvKCsJ2hCVJZfTM889w6V2X8r3532PF+hVMGjGJ109+PYfueiiH7X4Yh+x6COOGjNvmcwxpHMK7DnhXP1UsaVsqMwg7RliS1I/aN7fzid98gsvuvozWjlZO2OsELn7LxZw87WTqorLmnUt6QWUGYTvCkqR+9LU/fI3vzf8eH37Fh/nUaz7lWr9SlaisIOwYYUlSP7vlsVv46h++yvsOeR+XvPWScpcjqRdV1u9z7AhLkvrRqo2rOP3609l79N5c9KaLyl2OpF5WWR1hxwhLkvpJSomzfnEWqzau4tfv+TVDG4eWuyRJvawyg7AdYUlSH/u32/+NXz/8a/7j5P/g0N0OLXc5kvpAZQXh+nqIMAhLkvpMSolbH7+Vz/7vZzl1v1P52Ks+Vu6SJPWRygrCEdmEOYOwJKkXrWtZx++W/I4bF9/IjY/cyNK1S5k0YhKXnXIZEVHu8iT1kcoKwpANjzAIS5J20nObnuOuJ+5iwdMLsstTC3hg1QO0b25nWOMwjpt6HP/4+n/kb/b7G0YPGl3uciX1ocoMws3N5a5CklSB7nv6Pl57+WvZ0LoBgD2G7cEhux7CKfuewvFTj+e1E1/LgPoBZa5SUn+pvCDc1OSqEZKkHdbS3sJ7b3gvgwcM5vp3Xc9hux/G2MFjy12WpDKqvCA8aBBs2lTuKiRJFeYrt36FBU8v4H9m/Q/H73V8ucuRVACV9YUakHWEHRohSdoBty27jQv/fCEfOPQDnLLvKeUuR1JBVF4QHjTIICxJ6rENrRt43w3vY9KISXznpO+UuxxJBVJ5QyOamhwaIUnqsU//9tMsWb2EW866heEDh5e7HEkFYkdYklS1fvPwb7j4rov5+9f8PUdNPqrc5UgqmMoLwnaEJUk9sHTtUs78xZkcuMuBfPWNXy13OZIKqDKDsB1hSdI2bGzbyKlXn0pLRwvXvvNamhqayl2SpAKqvDHCDo2QJG1DSokPzf4Q9zx1D7887ZfsO3bfcpckqaAqLwg7NEKStA3/etu/ctX9V/H1N36dN+/z5nKXI6nAKm9ohB1hSdJWzF08l/N+dx7vnP5Ozn/d+eUuR1LB9SgIR8RJEfFgRCyOiPO2csy7ImJRRCyMiJ/2bpkl7AhLkrrxyHOPMOu6WRy4y4H898z/JiLKXZKkgtvu0IiIqAcuAo4HlgPzImJ2SmlRyTHTgPOBI1NKqyNil74qmKYmaG/PLg2VN7JDktT7NqfNvP9/3g/AL979C4Y0DilzRZIqQU86wocDi1NKS1JKrcDVwMwux3wYuCiltBogpfRM75ZZYtCg7Lqlpc9eQpJUWS77v8v449I/8q0TvsWeo/YsdzmSKkRPgvB4YFnJ/eX5tlL7APtExJ8j4vaIOKm3CnyJpnwJHIdHSJKAJ9c/yWdu+gxHTzma9x/6/nKXI6mC9NbYggZgGnA0MAH4Q0QclFJaU3pQRJwNnA0wadKknXulzo6wE+YkScC5c8+lub2Zi99yseOCJe2QnnSEVwATS+5PyLeVWg7MTim1pZQeBR4iC8YvklK6JKU0I6U0Y9y4cTtXsR1hSVLuVw/9imsWXsMXjvoC+4zZp9zlSKowPQnC84BpEbFnRDQCs4DZXY75BVk3mIgYSzZUYknvlVnCjrAkCdjQuoGP/vqjHDDuAD5z5GfKXY6kCrTdoREppfaIOAeYC9QDl6eUFkbEBcD8lNLsfN8JEbEI6AA+k1J6tk8q7uwIG4QlqaZ94fdfYPm65fz5A3+msb6x3OVIqkA9GiOcUpoDzOmy7YsltxPwqfzStxwaIUk17+4n7+bf7/x3/m7G3/Gaia8pdzmSKlRlfrMc2BGWpBqVUuITN36CMYPG8PVjv17uciRVsMr7Rgo7wpJU03628Gf8aemfuOQtlzCyaWS5y5FUwewIS5IqxvOtz/OZmz7DYbsdxgcO+0C5y5FU4Sq3I2wQlqSa880/f5Pl65Zz1duvor6uvtzlSKpwldcRdmiEJNWkx9c8zjdv+yanHXgar5v0unKXI6kKVF4QdmiEJNWkT9/0aeqijm8e/81ylyKpSlReELYjLEk15+ZHb+baRddy3pHnMWH4hHKXI6lKVF4QHjgwu7YjLEk14/M3f57JIybz6dd+utylSKoilReEI7KusEFYkmpCa0cr81bM4z0HvYdBAwaVuxxJVaTygjBk44QdGiFJNeGBlQ/QtrmNQ3Y9pNylSKoylRmE7QhLUs1Y8PQCAA7ZzSAsqXdVbhC2IyxJNWHBUwsY1DCIaaOnlbsUSVWmMoPwoEF2hCWpRix4egEH7nKgX6AhqddVZhB2aIQk1YSUEgueXuD4YEl9ojKDsJPlJKkmPLH+CVZtXOX4YEl9ojKDsB1hSaoJnRPlDt714DJXIqkaVW4QtiMsSVVvwVMGYUl9pzKDsJPlJImIOCkiHoyIxRFxXjf7J0XEzRFxd0TcGxFvKtl3fv64ByPixP6tvOcWPL2AySMmM7JpZLlLkVSFGspdwE5xaISkGhcR9cBFwPHAcmBeRMxOKS0qOezzwDUppe9FxHRgDjAlvz0LOADYA/jfiNgnpdTRv+9i+xY8vcDxwZL6TOV2hB0aIam2HQ4sTiktSSm1AlcDM7sck4Dh+e0RwBP57ZnA1SmllpTSo8Di/PkKZVPbJh569iFXjJDUZyozCNsRlqTxwLKS+8vzbaW+DJwREcvJusEf34HHlt39z9zP5rTZICypz1RmELYjLEk9cRrww5TSBOBNwI8josfn/Yg4OyLmR8T8lStX9lmRW+NXK0vqa5UZhDs7wimVuxJJKpcVwMSS+xPybaU+CFwDkFL6C9AEjO3hY0kpXZJSmpFSmjFu3LheLL1nFjy1gKGNQ5k6amq/v7ak2lC5QRigtbW8dUhS+cwDpkXEnhHRSDb5bXaXY5YCxwJExP5kQXhlftysiBgYEXsC04A7+63yHlrw9AIO2uUg6nrexJakHVKZZ5dBg7Jrh0dIqlEppXbgHGAu8ADZ6hALI+KCiDglP+zvgQ9HxALgKuCslFlI1ileBNwIfKxoK0aklLj36XsdHyypT1Xu8mnghDlJNS2lNIdsElzpti+W3F4EHLmVx34d+HqfFvgyPL72cda2rHV8sKQ+ZUdYklQ4nd8oZ0dYUl+qzCBsR1iSqtqCpxcQBAftelC5S5FUxQzCkqTCWfD0AvYavRdDG4eWuxRJVawyg7BDIySpqi14aoHDIiT1ucoMwnaEJalqrW9ZzyOrHzEIS+pzlRmE7QhLUtW675n7AL9RTlLfq8wgbEdYkqqWK0ZI6i+VGYQ7O8IGYUmqOvOemMfoQaOZNGJSuUuRVOUqMwh3doQdGiFJVeeWx27hqMlHERHlLkVSlTMIS5IKY+napTy65lGOnnx0uUuRVAMqMwg7NEKSqtKtj90KwNFTji5vIZJqQmUHYTvCklRVbnnsFkY1jfIb5ST1i8oMwnV10NhoEJakKnPL47fwhilvoC4q88eTpMpSuWeaQYMMwpJURZauXcqS1UscHyyp3/QoCEfESRHxYEQsjojzutl/VkSsjIh78suHer/ULpqaDMKSVEUcHyypvzVs74CIqAcuAo4HlgPzImJ2SmlRl0N/llI6pw9q7J4dYUmqKo4PltTfetIRPhxYnFJaklJqBa4GZvZtWT1gEJakqnLL49n6wY4PltRfenK2GQ8sK7m/PN/W1dsj4t6IuDYiJnb3RBFxdkTMj4j5K1eu3IlySwwa5PJpklQltowPdliEpH7UW//t/iUwJaV0MHATcEV3B6WULkkpzUgpzRg3btzLe0U7wpJUNRwfLKkcehKEVwClHd4J+bYtUkrPppRa8rs/AF7ZO+Vtg0FYkqpG5/jgg3c9uNylSKohPQnC84BpEbFnRDQCs4DZpQdExO4ld08BHui9ErfCICxJVcPxwZLKYbtnnJRSO3AOMJcs4F6TUloYERdExCn5YZ+IiIURsQD4BHBWXxW8hUFYkqrCsrXLHB8sqSy2u3waQEppDjCny7Yvltw+Hzi/d0vbDoOwJFWFWx93fLCk8qjc30H5hRqSVBUcHyypXCo3CNsRlqSqcMtjjg+WVB6Ve9YxCEtSxVu+bjmPrH7EYRGSyqKyg3B7e3aRJFWk+U/MB+DVE15d5kok1aLKDsLgt8tJUgVbtHIRAAeMO6DMlUiqRZUfhB0eIUkVa9HKRUwcPpFhA4eVuxRJNcggLEkqm4UrF3LALnaDJZWHQViSVBYdmzv466q/Mn3s9HKXIqlGGYQlSWXx2JrHaG5vZvo4g7Ck8qjcINzUlF0bhCWpIi1cuRDAoRGSyqZyg7AdYUmqaJ0rRuw/dv8yVyKpVhmEJUllsWjlIiYMn8CIphHlLkVSjar8IOw6wpJUkRauXOj4YEllVflB2I6wJFWczWkzD6x8wC/SkFRWBmFJUr97bM1jbGrfZEdYUlkZhCVJ/a5zopxBWFI5GYQlSf3OICypCCo3CLuOsCRVrIUrF7LHsD0Y2TSy3KVIqmGVG4QjYOBAg7AkVaBFKxc5UU5S2VVuEIZseIRBWJIqSueKEQ6LkFRuBmFJUr9aunYpz7c9b0dYUtlVfhD2CzUkqaI4UU5SUVR+ELYjLEkVZeEzCwGDsKTyMwhLkvrVolWL2H3o7owaNKrcpUiqcQZhSVK/WrRykd1gSYVgEJYk9ZuUkkunSSqMyg7CTU0GYUmqIMvWLWND6wY7wpIKobKDsB1hSaoonStGHLCLHWFJ5WcQliT1m84VI/Yfu3+ZK5Ekg7AkqR8tWrmIXYfsypjBY8pdiiRVQRD2CzUkqWIsXLnQYRGSCqPyg7AdYUmqGEtWL2Ha6GnlLkOSgGoIwh0d0NZW7kokqd9FxEkR8WBELI6I87rZ/52IuCe/PBQRa0r2dZTsm91fNW9q38SQAUP66+UkaZsayl3AyzJoUHa9aRMMGFDeWiSpH0VEPXARcDywHJgXEbNTSos6j0kpfbLk+I8Dh5U8xaaU0qH9VO4Wze3NNDU09ffLSlK3Kr8jDA6PkFSLDgcWp5SWpJRagauBmds4/jTgqn6pbCs6NnfQvrndICypMCo7CDflJ1ODsKTaMx5YVnJ/eb7tJSJiMrAn8PuSzU0RMT8ibo+IU/usyhItHS0ADGwY2B8vJ0nbVT1DIyRJWzMLuDal1FGybXJKaUVETAV+HxH3pZQeKX1QRJwNnA0wadKkl11Ec3u2yo8dYUlFUdkdYYOwpNq1AphYcn9Cvq07s+gyLCKltCK/XgLcwovHD3cec0lKaUZKaca4ceNedsEt7XlHuN6OsKRiMAhLUmWaB0yLiD0jopEs7L5k9YeI2A8YBfylZNuoiBiY3x4LHAks6vrY3mZHWFLR9CgIb2+JnpLj3h4RKSJm9F6J29AZhP1SDUk1JqXUDpwDzAUeAK5JKS2MiAsi4pSSQ2cBV6eUUsm2/YH5EbEAuBn4RulqE33FICypaLY7RrgnS/Tkxw0D/h9wR18U2q3Bg7PrjRv77SUlqShSSnOAOV22fbHL/S9387jbgIP6tLhuOFlOUtH0pCPc0yV6vgpcCPRfe9YgLEkVw46wpKLpSRDe7hI9EfEKYGJK6de9WNv2GYQlqWJ0BmEny0kqipc9WS4i6oBvA3/fg2PPztetnL9y5cqX+9IGYUmqIJ2rRtgRllQUPQnC21uiZxhwIHBLRDwGvBqY3d2Eud5eioch+ffVP//8y38uSVKfcmiEpKLpSRDe5hI9KaW1KaWxKaUpKaUpwO3AKSml+X1ScanOb5azIyxJhedkOUlFs90gvANL9PS/urpsCTWDsCQVnh1hSUXTo69Y7skSPSXbj375Ze2AwYMdGiFJFcDJcpKKprK/WQ6yccJ2hCWp8JwsJ6loKj8IDx5sEJakCuDQCElFYxCWJPULJ8tJKprqCMKOEZakwmtub6Y+6mmo69H0FEnqc5UfhB0jLEkVobm92WERkgql8oOwQyMkqSK0tLc4LEJSoVRHEHZohCQVnh1hSUVTHUHYjrAkFV5zR7NrCEsqlMoPwo4RlqSK0NLeYkdYUqFUfhC2IyxJFcGhEZKKpjqCcHs7tLaWuxJJ0ja0dDhZTlKxVEcQBrvCklRwdoQlFY1BWJLUL5rbnSwnqVgqPwgPGZJdu4SaJBWak+UkFU3lB2E7wpJUERwaIaloKj8Id3aEN2wobx2SpG1qbm92spykQqn8IDx0aHZtEJakQmvpaKGp3o6wpOKo/CA8bFh2bRCWpEKzIyypaCo/CHd2hNevL28dkqRtcrKcpKKp/CDc2RE2CEtSYaWUnCwnqXCqJwg7NEKSCqttcxuJ5DrCkgql8oPwwIFQX29HWJIKrKW9BcCOsKRCqfwgHJF1he0IS1JhNbc3AwZhScVS+UEYsiBsR1iSCqulI+sIu2qEpCKpjiA8dKhBWJIKzI6wpCKqjiDs0AhJKrTOIOxkOUlFUh1B2I6wJBWak+UkFVF1BGHHCEtSoTk0QlIRVU8QdmiEJBXWlqERTpaTVCDVEYQdGiFJhda5aoQdYUlFUh1B2I6wJBWak+UkFVF1BOGhQ6G5Gdrby12JJKkbTpaTVETVEYSHDcuuHR4hSYXkZDlJRWQQliT1OSfLSSqi6gjCw4dn1wZhSSokJ8tJKqLqCMIjR2bXa9aUswpJ0lY4NEJSEVVHEB4xIrs2CEtSIXVOlmusbyxzJZL0guoIwnaEJanQmtubaaxvpC6q48eOpOpQHWekziC8dm1Zy5Akda+5vdk1hCUVTnUEYYdGSFKhtXS0OD5YUuH0KAhHxEkR8WBELI6I87rZ/5GIuC8i7omIP0XE9N4vdRuammDgQIOwJBVUc3uzQVhS4Ww3CEdEPXARcDIwHTitm6D705TSQSmlQ4FvAt/u7UK3a+RIh0ZIUkE1tze7hrCkwulJR/hwYHFKaUlKqRW4GphZekBKaV3J3SFA6r0Se2jkSDvCklRQDo2QVEQNPThmPLCs5P5y4IiuB0XEx4BPAY3AG7t7oog4GzgbYNKkSTta67YZhCWpsJwsJ6mIem2yXErpopTSXsBngc9v5ZhLUkozUkozxo0b11svnRkxwqERklRQLe12hCUVT0+C8ApgYsn9Cfm2rbkaOPVl1LRz7AhLUmE5WU5SEfUkCM8DpkXEnhHRCMwCZpceEBHTSu6+GXi490rsIYOwJBWWk+UkFdF2xwinlNoj4hxgLlAPXJ5SWhgRFwDzU0qzgXMi4jigDVgNnNmXRXfLoRGSVFhOlpNURD2ZLEdKaQ4wp8u2L5bc/n+9XNeOGzkSmpuzS5MnW0nVLyJOAr5L1qT4QUrpG132fwc4Jr87GNglpTQy33cmL8zn+FpK6Yq+rNXJcpKKqEdBuCKMGpVdr1kDu+1W1lIkqa+VrPF+PNlqPvMiYnZKaVHnMSmlT5Yc/3HgsPz2aOBLwAyy5S7vyh+7uq/qdbKcpCKqjq9YBhg9Orte3WfncUkqku2u8d7FacBV+e0TgZtSSs/l4fcm4KS+LNbJcpKKqPqC8HPPlbcOSeof3a3xPr67AyNiMrAn8PsdeWxEnB0R8yNi/sqVK19WsQ6NkFRE1ROEO4dGGIQlqatZwLUppY4deVBvrv3uZDlJRVQ9QdiOsKTasiNrvM/ihWERO/rYl21z2kxrR6tBWFLhGIQlqTJtd413gIjYDxgF/KVk81zghIgYFRGjgBPybX2ipb0FwHWEJRVO9awaMXw41NUZhCXVhB6u8Q5ZQL46pZRKHvtcRHyVLEwDXJBS6rOTZ0tHFoTtCEsqmuoJwnV12ThhV42QVCO2t8Z7fv/LW3ns5cDlfVZcieb2ZgAny0kqnOoZGgHZ8Ag7wpJUKJ1DI+wISyoag7AkqU91doQNwpKKprqC8KhRBmFJKpgtQyOcLCepYKorCNsRlqTCcbKcpKIyCEuS+pST5SQVVXUF4TFjYM0aaG8vdyWSpJyT5SQVVXUF4V12gZRg1apyVyJJyjlZTlJRVVcQ3nXX7Prpp8tbhyRpCyfLSSoqg7AkqU85WU5SURmEJUl9yslykorKICxJ6lOOEZZUVNUVhIcNg6Ymg7AkFYirRkgqquoKwhFZV9ggLEmF4WQ5SUVVXUEYDMKSVDAtHS0EwYC6AeUuRZJexCAsSepTze3NNDU0ERHlLkWSXqT6gvBuu8FTT5W7CklSrrm92WERkgqp+oLw+PHwzDPQ2lruSiRJZJPlnCgnqYiqLwhPmJB9zfKTT5a7EkkS0NzR7BrCkgqpOoMwwPLl5a1DkgTYEZZUXNUbhJctK28dkiTghclyklQ01ReEJ07Mru0IS1IhOFlOUlFVXxAePjz7hjmDsCQVQkuHQyMkFVP1BWHIhkcYhCWpEJrbnSwnqZiqNwg7RliSCsExwpKKqjqD8NSpsHhxtoyaJKmsXDVCUlFVZxCePh2ee86vWpakAnCynKSiqs4gfMAB2fWiReWtQ5KUTZartyMsqXiqMwhPn55dL1xY3jokSY4RllRY1RmEd9sNRo0yCEtSATg0QlJRVWcQjsiGR9x3X7krkaSa52Q5SUVVnUEYYMYMuPtuaG8vdyWSVLPaN7fTkTpcR1hSIVVvED78cNi0yeERklRGLe0tAHaEJRVSj4JwRJwUEQ9GxOKIOK+b/Z+KiEURcW9E/C4iJvd+qTvo8MOz6zvuKG8dklTDmtubAYOwpGLabhCOiHrgIuBkYDpwWkRM73LY3cCMlNLBwLXAN3u70B02dSqMGQN33lnuSiSpZnUGYSfLSSqinnSEDwcWp5SWpJRagauBmaUHpJRuTiltzO/eDkzo3TJ3QkTWFbYjLEll09Lh0AhJxdWTIDweWFZyf3m+bWs+CPymux0RcXZEzI+I+StXrux5lTvr8MOzMcLr1/f9a0mSXmJLR9jJcpIKqFcny0XEGcAM4F+6259SuiSlNCOlNGPcuHG9+dLdO+IISAnuuqvvX0uS9BKOEZZUZD0JwiuAiSX3J+TbXiQijgP+ETglpdTSO+W9TK96VXbtOGFJKgtXjZBUZD0JwvOAaRGxZ0Q0ArOA2aUHRMRhwMVkIfiZ3i9zJ40dC9OmwR//WO5KJKkmOVlOUpFtNwinlNqBc4C5wAPANSmlhRFxQUSckh/2L8BQ4OcRcU9EzN7K0/W/44+Hm2+GlmI0qSWpljhZTlKRNfTkoJTSHGBOl21fLLl9XC/X1XtOPBH+67/gz3+GN76x3NVIUk1xspykIqveb5brdMwxMGAA/Pzn5a5EkmqOk+UkFVn1B+Fhw+Css+Cyy+Dxx8tdjSTVFCfLSSqy6g/CAF/4Qnb9ne+Utw5JqjFOlpNUZLURhCdOhJkz4corobW13NVIUs1wspykIquNIAzwgQ/AqlXwq1+VuxJJqhmOEZZUZLUThE84AfbYAy6/vNyVSFLNcNUISUVWO0G4vh7OPBN+8xtY8ZIvxpMk9YGW9hYa6hqor6svdymS9BK1E4QBPvhBaGjIrjs6yl2NJFW95vZmu8GSCqu2gvBee8G//zvMnQs/+lG5q5Gkqtfc3uz4YEmFVVtBGODss+GVr4SvfhWeeqrc1UhSVWvpaDEISyqs2gvCEXDhhbB0Key7Lzz0ULkrkqSq1dze7BrCkgqr9oIwwLHHwv33ZxPo3vteaGkpd0WSVJXsCEsqstoMwgD77QeXXgp33gnvfz80N5e7IkmqOk6Wk1RktRuEAd7+dvinf4KrroJjjoG2tnJXJElVxclykoqstoMwwPnnw49/DLffDhdcUO5qJKnHIuKkiHgwIhZHxHlbOeZdEbEoIhZGxE9LtndExD35ZXZf1djS7tAIScXVUO4CCuGMM+C3v4WvfS3rCn/jG+WuSJK2KSLqgYuA44HlwLyImJ1SWlRyzDTgfODIlNLqiNil5Ck2pZQO7es6m9ubGTN4TF+/jCTtFINwp//+bxg8OFtRYsQI+Oxnoc6GuaTCOhxYnFJaAhARVwMzgUUlx3wYuCiltBogpfRMfxfp0AhJRWbS61RfD//5n9m44c99Dv7+78tdkSRty3hgWcn95fm2UvsA+0TEnyPi9og4qWRfU0TMz7ef2t0LRMTZ+THzV65cuVNFumqEpCIzCJdqaICf/xw+/GH4j/+AhQvLXZEkvRwNwDTgaOA04NKIGJnvm5xSmgG8B/i3iNir64NTSpeklGaklGaMGzdupwpw1QhJRWYQ7ioiW0lixAg4+WSYPh1+//tyVyVJXa0AJpbcn5BvK7UcmJ1SakspPQo8RBaMSSmtyK+XALcAh/VFkU6Wk1RkBuHujB0Lc+bA889n30D3oQ9lS6x1dJS7MknqNA+YFhF7RkQjMAvouvrDL8i6wUTEWLKhEksiYlREDCzZfiQvHlvca+wISyoyg/DWHHEErFoFv/41PP00vOc98OUvw8aN5a5MkkgptQPnAHOBB4BrUkoLI+KCiDglP2wu8GxELAJuBj6TUnoW2B+YHxEL8u3fKF1tojc5WU5SkblqxLZEwBveACtXwtlnZ8urfetb8A//kE2oa2wsd4WSalhKaQ4wp8u2L5bcTsCn8kvpMbcBB/VDfU6Wk1RodoR7YvBg+MEP4Mor4ZRT4CtfgZEjs2+je6bfVyOSpIrQ2tEKwMAGh0ZIKiaDcE81NWXDI66+Gn71KzjzzOzb6PbYIxtD3DmEQpIEZEunAXaEJRWWQXhnvPnN8L3vwR13wN/+LVx2GbzlLXDkkXDjjbBhQ7krlKSya25vBnCynKTCMgi/HAcfDBddBL/8JVx6KTz3XLbk2vDhsPvucNpp8PnPO3xCUk3qDMJ2hCUVlZPlesNb3pJdn3Ya3HZbdnnoIfjjH7Mv6Pj612G33eDEE+FNb4Ljj8/WKfYrnCVVsZZ2h0ZIKjaDcG8aMiQLuccf/8K2Bx6AG26A++/POsdXXJEF4IjsuI9/HAYMgKOOgoHd/Prwyiuz5z311H57G5LUG7YMjXCynKSCMgj3tf33zy6QfSHHHXdk44jXrMnGFt94Y7Zv6lQ4/PBsuMUuu2TrGF93XbZ2cVMT/PWvMHlyud6FJO0wh0ZIKjqDcH+qr4fXvja7AHz1q1mn+Iknssl3t92WrUpR6pRT4Kab4H3vy77tbsiQ/q9bknZC56oRTpaTVFQG4XIaMSJbaQLgne/MrjdsgMceg/vug732yrrEV10FZ5yRdYpHjcrGGz/5ZDac4t3vhunTYdKkrHMsSQVhR1hS0RmEi2boUDjwwOzS6bTTYMIEuP76bEjFsmVZSL7pphd3kPfYIxtnPGVKFqj32w8uvDB7zqFDs3HJktRPnCwnqegMwpXi9a/PLqVaW+HOO+HRR1+4tLTA73+fTcC7+2748Y+zYwcPzsLwu94F++yTfW30jBnZMI0xY7IhGnvskd3uTTffnHW43//+3n1eSYXnZDlJRWcQrmSNjfC612WX7jz0ULZiBWTferd8OVx8MbS1Zd3hlLJ9TU3Q3Axjx2ZjkidPztZBbmvLVriYPBmWLMmGZ4wYsWM1/sM/wIIF2fP2dsiWVGgOjZBUdAbharbPPvDZz754W3MzrFqVhdK77oI//zm7P3Uq/OQn2YS8p57q/vk+9zl4zWtg82ZoaMiGXhx3XHZ76tQsJDc0ZOOYIXue+fOz21ddBeec03fvVVLh+BXLkorOIFxrmpqy8cbw0m7y3/1ddv3887B6dTa8Yu3abNjE7rvDJZdkk/gaG18YgvGd77z0NV71qqzb3BmCd9klWyFj48asu3zQQVmIbml5oRs9aFDfvm9J/c6vWJZUdAZhvdSQIS8s07brrllnGbJucKlNm+Avf8mGTyxenAXblSvhd7/LxiMfdlg2ee8HP8hWxSjtTg8bBuvXZ/tbWl5YEWPKlOyx06Zl45oHD84CeUND9nwjR2Yhfc89s+dpacnCdXcTATs64JZbspU5XFFD6ndOlpNUdAZh7bxBg+CNb8xuH330C9u//OWXHnv//Vl3eelSuOeebM3k3XfPwvDIkdlKGKtWZeOYH34Y/ud/oL29Z3WMHp0tH9feDhMnZuG6uTkL5XfdBa9+NZx7bhbu9903C8iDBmXDQxobs4ukXudkOUlF16MgHBEnAd8F6oEfpJS+0WX/UcC/AQcDs1JK1/Zynap0dXVZx3fUKDjkEDjzzO0/pq0t6zq3t2fDKu68Mwu4I0ZkK1F0Tvi7774sQEfAvfdmHeTRo7PHfepT8N//DbNmbf11RozIOsYNDVknevTo7PUefxyOOSYL0MOGZUvWtbVlx9fVZcfvthuMG5fV2dKSdbSHDeutT02qaA6NkFR02w3CEVEPXAQcDywH5kXE7JTSopLDlgJnAZ/uiyJVowYMyC6QhdPOsc076p//OetIb9oEjzySdYDXr8+GWLS2wjPPZCG2pQUefDC71NVlHesrrsg6yM3NPX+9sWNh+PAsEHdehgzJOuK77579Z6ChIbsMHJjta23NAnXnhMPO68bGbKhIQ0M2fKQzsDc2Zt9UKBVYS0cLA+sHEq5hLqmgetIRPhxYnFJaAhARVwMzgS1BOKX0WL5vcx/UKL08jY3wildktzu/yW9HbdqUrdNcX58NuWjI/+k8+WR2f8iQbNuSJVknef36Fy4rV2bbhw+HRYuyyYhtbVnHuqVl59/XyJFZyG5oeGEcded/HjqHfAwc+NLbXa+3ta/zesCA7L03NmZhvDOQl146X7vz0tCQPaaubuffoypac3uzwyIkFVpPgvB4YFnJ/eXAEX1TjlRQgwZlX2UN2Tjj3tLRkYXsAQOyMdLr1mUBefXqbJm61tbsWwI7O9etrdn+TZuyY9avz+53Buu2thcuLS3ZNxG2tGSPa2194XbpdUdH772f7kRkgbgzFEdk111v99a+trZs3eyRI7M/t87hNXV1WbCH7L13LgP4L/8Cb31r334GNaq5vdmJcpIKrV8ny0XE2cDZAJMmTerPl5aKqb4+G/oAMH58dulvHR0vBOWtheW2theOa27OLu3t2bbuQnjn8aXHpJSFz82bt367N/bV1WUTJNety8Z6Dx6cbeusMSILxPX12f3Oda/V6/Ybux/H7nlsucuQpK3qSRBeAUwsuT8h37bDUkqXAJcAzJgxI+3Mc0jqZfX1WefUtZzVy8599bnlLkGStqkng/fmAdMiYs+IaARmAbP7tixJkiSpb203CKeU2oFzgLnAA8A1KaWFEXFBRJwCEBGviojlwDuBiyNiYV8WLUmSJL1cPRojnFKaA8zpsu2LJbfnkQ2ZkCRJkiqC6xpJkiSpJhmEJUmSVJMMwpIkSapJBmFJkiTVJIOwJEmSapJBWJIkSTXJICxJkqSaZBCWJElSTTIIS5IkqSYZhCVJklSTIqVUnheOWAk8voMPGwus6oNy+lql1g2VW7t1969KrRt2rvbJKaVxfVFMUfXwnF1rfw+KwtrLw9rLY2dr7/a8XbYgvDMiYn5KaUa569hRlVo3VG7t1t2/KrVuqOzai6aSP0trLw9rLw9rf4FDIyRJklSTDMKSJEmqSZUWhC8pdwE7qVLrhsqt3br7V6XWDZVde9FU8mdp7eVh7eVh7bmKGiMsSZIk9ZZK6whLkiRJvaIignBEnBQRD0bE4og4r9z1bE9EPBYR90XEPRExP982OiJuioiH8+tRBajz8oh4JiLuL9nWbZ2R+ff8z+DeiHhFwer+ckSsyD/zeyLiTSX7zs/rfjAiTixP1RAREyPi5ohYFBELI+L/5dsr4TPfWu2F/twjoiki7oyIBXndX8m37xkRd+T1/SwiGvPtA/P7i/P9U8pRdyXyPN13KvVcndfj+bo4tVfC597/5+yUUqEvQD3wCDAVaAQWANPLXdd2an4MGNtl2zeB8/Lb5wEXFqDOo4BXAPdvr07gTcBvgABeDdxRsLq/DHy6m2On539nBgJ75n+X6stU9+7AK/Lbw4CH8voq4TPfWu2F/tzzz25ofnsAcEf+WV4DzMq3fx/4u/z2R4Hv57dnAT8r12deSRfP031ea0Weq7dRe6HPG3ktnq/LU3u/n7MroSN8OLA4pbQkpdQKXA3MLHNNO2MmcEV++wrg1PKVkkkp/QF4rsvmrdU5E/hRytwOjIyI3ful0C62UvfWzASuTim1pJQeBRaT/Z3qdymlJ1NK/5ffXg88AIynMj7zrdW+NYX43PPPbkN+d0B+ScAbgWvz7V0/884/i2uBYyMi+qfaiuZ5ug9V6rkaPF/3b9WZSj1fQ3nO2ZUQhMcDy0ruL2fbf6BFkIDfRsRdEXF2vm3XlNKT+e2ngF3LU9p2ba3OSvhzOCf/ldTlJb/SLGTd+a9vDiP7325FfeZdaoeCf+4RUR8R9wDPADeRdTvWpJTau6ltS935/rXAmH4tuDIV5s97B1TyeRoq7LzRjUKfN0p5vu5f/X3OroQgXIlel1J6BXAy8LGIOKp0Z8p6+IVfrqNS6sx9D9gLOBR4EvhWWavZhogYClwHnJtSWle6r+ifeTe1F/5zTyl1pJQOBSaQdTn2K29FKoiqOE9DZdWaK/x5o5Pn6/7X3+fsSgjCK4CJJfcn5NsKK6W0Ir9+BriB7A/y6c5fk+TXz5Svwm3aWp2F/nNIKT2d/+PZDFzKC7/WKVTdETGA7MR0ZUrp+nxzRXzm3dVeKZ87QEppDXAz8BqyX1s25LtKa9tSd75/BPBs/1ZakQr35709FX6ehgo5b3SnUs4bnq/Lq7/O2ZUQhOcB0/IZg41kg6Fnl7mmrYqIIRExrPM2cAJwP1nNZ+aHnQn8T3kq3K6t1TkbeF8+M/bVwNqSXw+VXZexWH9D9plDVvesfGbpnsA04M7+rg+yWcXAZcADKaVvl+wq/Ge+tdqL/rlHxLiIGJnfHgQcTzZe7mbgHflhXT/zzj+LdwC/z7s+2jbP0/2v8OeNrSn6eQM8X1O+z73/z9ldZ88V8UI2G/MhsnEi/1juerZT61Sy2ZcLgIWd9ZKNWfkd8DDwv8DoAtR6FdmvR9rIxtx8cGt1ks3kvCj/M7gPmFGwun+c13Vv/g9j95Lj/zGv+0Hg5DLW/TqyX6PdC9yTX95UIZ/51mov9OcOHAzcndd3P/DFfPtUshP9YuDnwMB8e1N+f3G+f2q5PvNKu3ie7tN6K/JcvY3aC33eyOvwfF2e2vv9nO03y0mSJKkmVcLQCEmSJKnXGYQlSZJUkwzCkiRJqkkGYUmSJNUkg7AkSZJqkkFYNSsijo6IX5W7DknS9nnOVl8wCEuSJKkmGYRVeBFxRkTcGRH3RMTFEVEfERsi4jsRsTAifhcR4/JjD42I2yPi3oi4ISJG5dv3joj/jYgFEfF/EbFX/vRDI+LaiPhrRFyZfyOPJGknec5WJTEIq9AiYn/g3cCRKaVDgQ7gdGAIMD+ldABwK/Cl/CE/Aj6bUjqY7Bt0OrdfCVyUUjoEeC3ZNx0BHAacC0wn++aaI/v4LUlS1fKcrUrTUO4CpO04FnglMC//j/8g4BlgM/Cz/JifANdHxAhgZErp1nz7FcDPI2IYMD6ldANASqkZIH++O1NKy/P79wBTgD/1+buSpOrkOVsVxSCsogvgipTS+S/aGPGFLsft7HeFt5Tc7sB/E5L0cnjOVkVxaISK7nfAOyJiF4CIGB0Rk8n+7r4jP+Y9wJ9SSmuB1RHx+nz7e4FbU0rrgeURcWr+HAMjYnB/vglJqhGes1VR/J+UCi2ltCgiPg/8NiLqgDbgY8DzwOH5vmfIxqQBnAl8Pz9pLgHen29/L3BxRFyQP8c7+/FtSFJN8JytShMp7exvJ6TyiYgNKaWh5a5DkrR9nrNVVA6NkCRJUk2yIyxJkqSaZEdYkiRJNckgLEmSpJpkEJYkSVJNMghLkiSpJhmEJUmSVJMMwpIkSapJ/x+wZ6WpRy0mvAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 864x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1296x432 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(\"train\", (12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Epoch Average Loss\")\n",
    "x = [i + 1 for i in range(len(epoch_loss_values))]\n",
    "y = epoch_loss_values\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(x, y, color=\"red\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Val Mean Dice\")\n",
    "x = [val_interval * (i + 1) for i in range(len(metric_values))]\n",
    "y = metric_values\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(x, y, color=\"green\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(\"train\", (18, 6))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title(\"Val Mean Dice TC\")\n",
    "x = [val_interval * (i + 1) for i in range(len(metric_values_tc))]\n",
    "y = metric_values_tc\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(x, y, color=\"blue\")\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title(\"Val Mean Dice WT\")\n",
    "x = [val_interval * (i + 1) for i in range(len(metric_values_wt))]\n",
    "y = metric_values_wt\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(x, y, color=\"brown\")\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title(\"Val Mean Dice ET\")\n",
    "x = [val_interval * (i + 1) for i in range(len(metric_values_et))]\n",
    "y = metric_values_et\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(x, y, color=\"purple\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check best pytorch model output with the input image and label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of labels  torch.Size([3, 96, 96, 96])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUUAAAFSCAYAAABlp81+AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAAsTAAALEwEAmpwYAAA/eElEQVR4nO2de7RlVXXmvymICD4AQSgKhIIC5CFgUTwKEBUQERDwbRSlHRjy0hgNQzDdJrat6UgcIbTJME1rDGlto4IJiKjBCvIwoQREKKgSqiigquRRgOA7Irr6j3vu4nf3OPPefe45595Tt77fGAzm3Wfvtddae7PY35pzzRWlFBljjBnjabNdAWOMGSU8KBpjDPCgaIwxwIOiMcYAD4rGGAM8KBpjDPCguAkREXdExMtmux7TJSLujYjjZ7seJCJ2j4gSEZvPdl3MYPCguAlRStm/lPKt2a7HpkREbBcR/xwRP4uI+yLiLbNdJzM5/r+bMcPlbyU9IWlHSQdL+mpE3FpKuWNWa2VS/KW4CUH5GREfiogvRcRnI+InEbE8IvaOiA9ExIaIWBcRJ+Dad0TEys65ayLidxplvz8iHoiI+yPinR1JubDz2zMi4uMRsTYiHoqIv4uIZ05Sz9/GvVZExCL8fHBE3BYRP4qIL0TElp1rto2IKyLi4Yh4rGPvgjK/FRH/IyK+3Sn3XyNi+85v4xL4zE4dH4mI/4prnxYR50XE3RHxaER8MSK2a9HfW0t6naQPllJ+Wkq5XtLlkt421bVm9vCguGnzakn/V9K2km6R9A2NvRPzJX1Y0v/GuRsknSLpOZLeIemC8cEqIk6U9D5Jx0taKOlljfv8haS9NfaltLBT/p92q1BEvEHShyS9vXOvUyU9ilPeKOlESQskHSjpv3SOP03SZyTtJukFkn4h6W8axb+lU/fnS9pC0jmN34+WtI+k4yT9aUTs2zn+bkmnS3qppJ0lPaaxL8Cp2FvSk6WUu3DsVkn7t7jWzBalFP+zifwj6V5Jx3fsD0m6Cr+9WtJPJW3W+fvZkoqkbZKy/kXSezr230v6n/htYefahZJC0s8k7Ynfl0i6Jyn3G+PlJvU/A3+fL+nvknMPlvQY/v6WpP+Gv39f0tc79u6d+u6C378j6c0de6Wk4/DbPEm/0tj00/i1m3epw0skPdg49tuSvjXb74L/yf/xnOKmzUOwfyHpkVLKr/G3JD1L0uMR8SpJf6axr5+nSdpK0vLOOTtLugllrYO9Q+fcmyNi/FhI2iyp066S7p6kzg/C/nnn3oqIrSRdoLGvyG07vz87IjZDm5rXPmuKssd/303SP0fEb/D7rzU2TzgZP9XY1y55jqSfTHGdmUUsn82URMQzJF0q6eOSdiylbCPpSo0NbpL0gKRdcMmusB/R2AC7fyllm84/zy2lNAekcdZJ2nMa1fxjjUnfw0spz5F0zHj1p1FWtzq9CvXfppSyZSnlB1Ncd5ekzSNiLxw7SJKdLCOMB0XThi0kPUPSw5Ke7Hw1noDfvyjpHRGxb+eL7YPjP5RSfiPp/2hsDvL5khQR8yPilcm9PiXpnIg4JMZYGBG7tajjszU2+D7ecYL8WY9tnIy/k/TR8XpExA4RcdpUF5VSfibpy5I+HBFbR8RRkk7T2DyuGVE8KJopKaX8RNIfamzwe0xjDovL8fvXJP0vSVdLWi3phs5Pv+z8+9zx4xHxY0nf1NhXXbd7fUnSRyX9P43JzH+RNKWnV9JfS3qmxr5Mb5D09ZbNa8OFGmvvv0bETzrlH97y2t/v1GuDpM9L+r3icJyRJjqTv8YMjI7X9nZJzyilPDnb9TGmF/ylaAZCRLymE4+4raSPSfqKB0SzMeJB0QyK39GYRLxbY57Z35vd6hgzPSyfjTEG9PWlGBEnRsSdEbE6Is4bVKWMMWa2mPaXYkRsprE4rFdIWi/pRkm/VUpZMbjqGWPMzNLPipbDJK0upayRpIj4J43FYKWDYkRYqxtjRoJSStfA/n4GxfmauJxrvbrEbkXE2ZLO7uM+IwuWrWnU5mbb1G069ec1ZNTab8x0Gfra51LKRZIukubel+IoDwRt6jad+vOabIAko/w/DmO60Y+j5QeauMZ1l84xY4zZaOlnULxR0l4RsSAitpD0ZmHplzHGbIxMWz6XUp6MiHdpLP/dZpL+3ms6Ny0yKc3jvUrmUZDbo1AHM3vMaPD2XJtTNE8xqIFkFAakUaiDGT6Z99nL/IwxBjjz9kZOP6E3g/wianPvNuf7y8zMNv5SNMYY4EHRGGOA5fNGTj9B2v1K6V6v6WdlzUyupGlTpp0xGyfz58+XJG3YsCE9x1+KxhgDPCgaYwxwnGIf9CqhhuEFHraMa8rWfjzcGf30hWWsmS6OUzTGmBZ4UDTGGGDvcx/0KtcGtfxtUGVm5fezdrnJsL3Jlsxm0PhL0RhjgAdFY4wBls8jwLCyZHejjTd4svMHtWZ5lDPpDLJ8e8c3PvylaIwxwIOiMcYAy+cZZCaDvQcVEN6r3G4yqPXOg/SIT8Ugg8aHlarNDA9/KRpjDPCgaIwxwGufWzDs/Uf6SZ3VD5tttlnX+7aVg70y7EDzUWHU62fG8NpnY4xpgQdFY4wBls+zRJu1v228wL3K7ac97an/D26++eZdbZ7zn//5n63ux+O8/le/+lXXOvUasN5rCrJBYSk8d7F8NsaYFnhQNMYYYPk8BHrNQp3JZF779Kc/vdpbbrlltbfYYotqP/OZz6z285///GrvsMMOXa9l+Sxnq622qvYvfvGLCXXlNY8++mi1H3nkkWpTij/00EPV/uEPf1jtX/7yl9WmF/zJJ59UNzLv+GzJ6rZYfo8uls/GGNMCD4rGGAPm7NrnmZAtbaQbPbGsByXjc57znGpT6u6+++7VpjTeeuutp7R32WWXaj/xxBNdy6FMZlt+9KMfVft5z3vehPZQJv/mN7+pNtvG448//ni1161bV+3HHnusa5kPP/xwtX/6059W+yc/+Um1f/3rX3etd3Z8UO/CdK4dBck8k3tmzwX8pWiMMcCDojHGAA+KxhgDRiIkp9fVHb0yE23M6sfwlO22267anPPbe++9q805xec+97nV5vzfbrvtVu0dd9yx2gy3YQgPV6Vwjo9w3nHevHnV5vygJG2//fbV5hzm8uXLq33ooYdWm/OCDMm57777qv2DH/yg2j//+c+r/eCDD3Y9h3Oe69ev71o+28NVNT/72c+qnc07ch6YZKtzmn00k3kzs3LIMJKMzIX5SIfkGGNMCzwoGmMMGAn53GeZA7u2TbKDXlel7LPPPtWmrKTU3XbbbatNaXz44YdXm2ErXCVy1113Vfv222/veg4lbFPqjUMpTKn+7Gc/e8J5rDflPVeiMIyH59x7773VPv3006u90047db0f5SrDbRieQ8nMqYEHHnig2pThrAPDf7jChjB0iPdiHZp9yr97Xd00F2TpOG3l/2yt+rF8NsaYFnhQNMYYMBLyeRjp/vs9v40XkF5KnrNo0aJqUzJTJi5cuLDae+21V7WZgOGyyy6r9vXXX19telzpWaYEpNc7W+lBmcfzMy+rNDFxBKUuJTe91zxnm2226VoOV/fQa0xJzmuf8YxnVJveenrGeV96/dlfWcIN9uOdd95Z7VtvvbXaN954Y7VXr14tkiWvaLPlw6B2YdyYZPigttrotc2Wz8YY0wIPisYYA2ZNPvcasDoMb12zzDb3oGRmcoUlS5ZUm8HYRxxxRLX322+/atMjfPnll1f70ksvrTaDlClpKT2f9axnda0b5TDJ8ilSMlK2sg5NKMspeylv6VlnnbJzOMXAutLmFEMme1k3ThPQy86ckwyCz3JRsv6U0t/+9rdFvvvd71abUx3Z1EWvWzNkzAX53M+CDctnY4wZAh4UjTEGbDTyeSZoUyfKqZe//OXVXrBgQddzKKuvvfbaan/mM5+pNgOz6YnlfelZZZ5FSmmyatWqatNzm0lmSkN6aCmLJWmPPfaoNiUwg8XvueeealPqclqB/cX1zgyW/vGPf1xt5pxkcDi93pSkrBunALJ1zTzOZ5CtM2fEQJNrrrmm2jfccEO177jjjmpn+SHbMCr/vUzFKP+3XEqxfDbGmDZ4UDTGGDASwdstr632THioKKfoHT3hhBOqfeCBB1Z78eLF1d6wYUO1P/axj1WbspJykDKWxylV6WWm15teWUoyrvGl3Ob6YK4J3nnnnavNwGeWI0kHHHBAtSlXudZ47dq11WY/8t5sDyU6px4o45trsLvB+lCes3x6qylbKZl5nJ549jvXdHN6QpKOPvroavNd+PKXv1xtBn9zTXXmlZ7L66aHLbO79ZflszHGtMSDojHGgBmXz1PJ3WFkCc7Knyx1GCXzSSedVO0999yz2i972cuqTc/ihz/84WpTrlHG3n333V3vte+++3a9Nks1RnlGOcxrufY3C2qmF5ueaKbXkiZ6fil1eW9OB9ALnnl+2R5m5CbMmE2PM+vAPmJwPOXz/Pnzq82pCsrnTMLyfaGs5n0l6aCDDupaV06HXHTRRdW+6aabqs3n2SaVXZvjpjvTls8RsWtEXB0RKyLijoh4T+f4dhFxVUSs6vx726nKMsaYUaeNfH5S0h+XUvaTdISkP4iI/SSdJ2lpKWUvSUs7fxtjzEZNz/I5Ii6T9Dedf15WSnkgIuZJ+lYpZZ8prh3qxlWTbSbU7ZzJ0mKdcsop1WYA72tf+9pqUzK/733vqzZlIr2slK70jhLeiwHblMNZNmh6gLmGmHXg2l96d9l2BlNTJkoTpS49sCyL19MjTknLvmA5lLr3339/13pk9WOgOIO3H3300WrzmWepzNhHLIfHWWc+G2niOvVDDjmk2tys7AUveEG1L7zwwmrfdttt1ebzZJltvNK9/rczbK/vKEr7gXifI2J3SS+WtEzSjqWU8af2oKQds+uMMWZjoXsqlS5ExLMkXSrpj0opP278X6BM8hV4tqSz+62oMcbMBK3kc0Q8XdIVkr5RSvmrzrE7NSD53A/9BLI2Zc+xxx5bba5tfc1rXlNtSrqzz35qrKf8ouRiUDTlE6VYlvKKsnfXXXetdrZncrZ5VBbsTblJqc7y2RZpoieb3ltOV3DNMttDO1trTJuSnPtKcw0yZSWhl5mea3r9KU9ZnyyNGt8pPhv2nTRRGvM5Z3t9M9j73HPPrTafLevNKYZBeaJHMRfBsOnH+xySPi1p5fiA2OFySWd27DMlXda81hhjNjbayOejJL1N0vKI+F7n2J9I+gtJX4yIsyTdJ+mNQ6mhMcbMIFMOiqWU6yVl7qvjpnvjYazfbHMt5RmDr6WJ0vD444+vNgNwzznnnGpTMlM+UhpRPlOKUtJmqa0oHzPvI2Uy03fRQ8u6ZeuAWSblL7NQN+vEwGlKziwzNqU7y2GwOCU5pSunD+j5Zb0ZDcB11lmQNgPl+T5m+02zT2k3s5MzXRijAFhXPgfe773vfW+1zz///GrzfVm5cmW12ddZxEUbet3zfK5JaeJlfsYYAzwoGmMMaB2SM2gG9fnd64Y+9CpTekrSC1/4wmpTNr773e+uNuUKpRjlHSUzvY+U55SGlGiU5Ax2psRk3RiYzPOz/Y25/pbtzyR50/vMcunVzfbAzjzrPE6JzX7JJDalJJ8H5TmfB4/T49ycGhiHMpR15rVcA88AdWniOnBew3IZxUD5zftxQcA//MM/VJue75tvvrlrPeh97xV6vUfRKz3se/tL0RhjgAdFY4wBsyaf2zAoDzUl6cEHH1ztefPmTTjvyCOPrPZVV11VbXo16clkNmhKY8qkbE0wPdosh23OArwpHynhs/RiTP/F+2b7RNNurn0mWeoxymF6gSnFabPvuIkX13Wz33kt+zTbG5vXZuu9uQ6aa8s5JUG5zX5nXzdh/SiZKVGZ3Zx9yvafd95T+VY+8pGPVHvRokXVXrZsWVqP6dJPlvthMWy57i9FY4wBHhSNMQaMtHzu5zOZn/rMhMzj3LdZmujJ/fM///Nq05PJOlFmZh5LQlmW7ddMuZltoMSAYMq+hQsXdj2fNu9LSU6aGzERykm2mV5g1ptBypTJXC+cZbSmV5aB4tnmU3w2mTecnvVsj23KcHqSWX/2I73B0kQvMNvJtc+rV6/uej/uGc3pnb/8y7+s9qmnnlpt7h++//77d633oOgnld9kZY0a/lI0xhjgQdEYY8BIy+d+oIeSgbb77PNUdrNm8O473/nOanOTKcpMSihKQ3oZMwnMa7m+mMcpOVg+y6Hcojc5C+qm3OT5PIf3ojeV50gTZSm94AxS5uZLlOKUmZm3m5I2yzzO81kftoGwT1lnwufHcthflMVsV3PtMz3cDOQnDKL/3ve+V22u62bA9j/+4z9Wm5L5TW96U7U/97nPVZtTNZN5x6dLG/k7ip7rNvhL0RhjgAdFY4wBc0o+8xP9mGOO6XrOoYceWu0rrrhiwm/MOE2PMzeTYoA0g6LpNaXMytJrMTA3S8fFe1GSc80x5SAlJr2j2YZelKGU89n+zE0odQmlPgPT22yaxfZn19IDnqVUI2wPg8NJttczYd3oxWZW7Ga9KV2zdGbMJM53ise5Dvr9739/13vznWebs720e6WfNcej7G1u4i9FY4wBHhSNMQbMKflMjzO9eNxIiLLnE5/4xITrKZOzoOMscJjyk2mbeD7lZpZ5mV5DejgppbN9f7PNoJrBxeOwvzL53Fz7TI8wZTylVTPd2DgMKOZUAmU8nw/bnPVRJjfZL5zOoIRlO/mcGIieZQXns6eHWpo4pcH+YxtoU25zWiXbP3vt2rXV5rp8SnW2h32dPZs2bEwSuB/8pWiMMcCDojHGgI1GPmeeL0ogpgXj8SVLllT74osv7lqONDEI94ADDqg2JQelCKUYg3QpuQjlIKUxpVGWqiqTblmaL9aZ8pmyivKXUo2e4QULFkxoA6Vh5hFneziNwUBwSlRK5mzNMtvJ8jNvMteHs22sJ6cMsimGbJ9r1o33kia+R6wf6813gbKf7VyzZk21KbcvuOCCajON2NKlS6vNqQouRGCQvemOvxSNMQZ4UDTGGLDRyOcMykHu40tPMr2DlM9NaUj5STlFmU3PJwN+KfsodVi/NgHLPJ8SjXIrWwdN6cVgZ96LspUecNY/y6LdbAM9v5TSvPf3v//9atMrnwW1U9JyCoDedEpABtkTZrPm1Ab7jvXJ0sBlGdVJ8zinCSiZKb+zNdh8znxWDMBmG7797W93Pc5pmOYGbVPBfmGG8Iy5th+0vxSNMQZ4UDTGGDBy8jn7FM/SEHEfZ0q7ww8/vNpc40wZ1tyvl2nFKN0oY3icXkOWRTmV7aHcDPgdhx7nbD0upS7PydY1Z4HM69evrzYD3LOgbGmiTKbU5Z7ZvB/lN6Uon0O2tzJlOKV+m720H3rooWqvW7eu672Y2ToLsqfMzaZI6N1u1oPTBJySaRPRkE2lsB7f+MY3qv27v/u71eZ+0PRiZ+9FPwxSMo+CFPeXojHGAA+KxhgDRk4+Z5/M/KymvHnRi15UbUpVbhJ05ZVXVnuyjZu4dpTyiJKLcoj1oGRiXbmuNfNoZym4MonZRlawPtma60yeZeuApYkeZ2Y055poBiyzT7MAafY1A41ZpywzNuUg+yhbB51tsMXz+Y6wbtl9m1MhLIvyOwsiz9a1E96P0pty+9Zbb632fvvtV21uksUIjRUrVnS9F5lpT/QoeK/9pWiMMcCDojHGgJGTz20+v+l9o6xYtGhRtZmRmF5WyrOmNOT9KKEoV7hmmfXIrm2zNzLlV7ZuOrtXdjzzRFPm0ltPry890c1UU5TM/I3Pje2hNKTXmOdTVjc9ueNk6cwoTzkdQtnOvuC9+Ax4Le/F41lQftNDz3vw+mxNON8XtofTQQxG51prBmnfcsst1T7jjDOqTfn87//+79VmX2cbbGWMgpd4WPhL0RhjgAdFY4wBIyef2+wVS3lHzyrXwZ5//vnVphzK9i1u3psSiPImSz3Fda20s4zRlHT0ajJ4O/OAZ3sg8xzWOQt8poeeUo3XNtfNZlKJkpbtz4KFKd0J28ZnxX5hOVkG6+w9oreWWa45LcI68DlRCrMfmxt9sb8pmbNphSz6IItooOxlNAQjAxi8zvrw2fJ9aUMbT/RckNL+UjTGGOBB0RhjwMjJ5wxKFAamUuZSVjOQlbKKEojHm78xezIlIKUI5XMmbyjFKIcoXSif6E3M9nTmtVk5lH2UapmUzrzqTYnVJhCc7aTHPZvG4PmUX7x3lnaN9aFHl9dy2iLbrIrlsL94PqdO2L9N+ctr2MeUz9m6dh5n/Th9cP/991f7wAMPrDbbyf7af//9q71q1apqU3qzbW3oNah7Y8JfisYYAzwoGmMMGDn5nHmcCaUeg07pcaO0ZcA2vY+U29JE6UqbcoqyhNKQEpDnNzNXj5OlkaLc4r1YJvuIHtS77rqr2pRwbD/7K/NEsvzMS9y8B6FHODsnI8tmnqVL473YR7Q5DcHnmu2rnU1JcLolq5uUr4vOgu7ZBtaV7zCjAOhlZnu45nr58uXVPvLII6tN+UxJfs8992hTYPzdnsxL7i9FY4wBHhSNMQZ4UDTGGDASc4rZ4vJsM3TOx3DXvssvv7zrOVxhwcQHnFNp3oNzj9n2Alm9OUfI+SKG/PD8bKVEFnrCeb5rrrmm2pzXZPuZTGDvvfeuNrdsYJm8VzYnKuW723FeLEuvn63iYN+xTnw2WTvZj3w2nDvMclRmyS1YPo/zXs0+yuarsv5iuXzmfD83bNhQbbaHxxmSxDCZZlKPcfiuDZumr2C2Vr60ua+/FI0xBnhQNMYYMBLyOSNLkU/5xN3c7rzzzmpnSRyYZ68ZSkFpzJCZNvUgPIcSkMkBKKspqyjpsmQPlEZZHbIVE8wzyZAP5qKkDGuG5FB+ZuE2bRI5sNws6ULWfraZzymT5Ox3htVk4T8sPzuH79dkkiybYslWNzHEKkvwwTAfPkOeQ2nMNmRJMwZFm904h3WPQeEvRWOMAR4UjTEGjJx8zla0UJJmaedXrlxZba5QYNR/tgG6lEvubPsD3iPzjlKKZkkHeN/M487kC9nOhpRb2YoLXsuVEUxlf+yxx1a76bnMJF22yiSLGqAEpLeasprPgPVg32XJPihDs9ySXAHFd4HRCplMnixpRpbUoo2cZL9Q9mfbS2TlsN7sC0YcjEoih17lcBahMigp3fpLMSI2i4hbIuKKzt8LImJZRKyOiC9ExBZTlWGMMaNOL/L5PZJW4u+PSbqglLJQ0mOSzhpkxYwxZjZoJZ8jYhdJJ0v6qKT3xdg367GS3tI55WJJH5L0yRZlScqlRPY5TC9ztkj/0Ucf7Xqc0oNSjRJDmpjggfkUKeMorQi9rJR3meeP7aQ0pGTKgogXLlxYbcpekkn+bEN3SqwHH3yw2uz3Zp2yHfCyqYTMc02bfZcFwVNiZgkx+DzY5kceeaTad9xxR9c6vOpVr+p6nM8ys/uFuTiz/I2cDqI8Z/QFt5pYs2ZNtQ877LBqZ976XmFuReZ6bCtt+5G9vV47XlcGvTdp+6X415LeL2n8yTxP0uOllPE3fr2k+V2uU0ScHRE3RcRNLe9ljDGzxpSDYkScImlDKeXm6dyglHJRKWVxKWXxdK43xpiZpI18PkrSqRFxkqQtJT1H0oWStomIzTtfi7tIGqorK8t9R1lFuUH5RIk1Wa5Dyj4Gv65bt67a/OxmWfSCZx5e3o+f/ZSDlNiTbbg+zuLFT/2/5qabnvoYz3YLzLymmQx7+ctfPuF+lPdZcDWfQzaVkOWHzGz2F+Ugy2FkQeYBzjyunBbhM6A3PNtdj++gNHH6IduOgNezfuxfBmZTMreBU0HsC04ZNKdGBkEmZ0dl7XMbj/uUX4qllA+UUnYppewu6c2S/q2U8lZJV0t6fee0MyVdNv2qGmPMaNBP8Pa5GnO6rNbYHOOnB1MlY4yZPXoK3i6lfEvStzr2GkmHTXZ+Ukbr3ynPuC6ZAdGUNPQm08vMciZLi5V5wSmNeQ3X3dLmrmr0iFNKZddSxmVyjTAVGNu8bNmyatOjmcnTTAo3A5OztcA8j9KN8pa02bKAMjwLGufzYDuzelKekpe+9KXVZr/Tw86pE0ps9p00UXJTAvO95XQQ+yLb2TBLHZedzzqxH++7775qZ5EUw2C25PJ08DI/Y4wBHhSNMQZsNGufKT0Z1EtpQCmRSYxsLbI0UX7Ra5x5EOnh5j3o7aWkpbcv83ZmG9qz/GztK3cnZDuvuuqqamepuSjhOF3AYGdpYl9kdc1kGfuXz5B9SnlHqbv99tt3LTMLRufxW2+9tWudDznkkK51oPynZGZ92I8MXpYm9hHbzL7kdADfkWxXRb4jrAfbw3eBUzJ8d1gftqFXBrluetipwHrFX4rGGAM8KBpjDBg5+ZxBjyOlBINR6Yls402dbM0q5Ucm0SjdGVxL7zO95pSllI+sE2UsPdc8nzKc0iNbZ33iiSdW+7LLngonpaxi33Fzr6YUplzL1mazj1gntp/H+QwZmE2ZuMMOO1Sb0wTsF5bJzd35bNieffbZp9rsi8wbnG2SxfW+krTTTjtVm1KcdeL7xakOtpPt53RQljqOfZGVw+fM93pQTEcKDzsVWK/4S9EYY4AHRWOMASMnnzM5mMlHetMoQykrKL0pQ5rygdKKZdFrmGWG5nF6MumJzNZs83yum2VmaLYhy9qdeVB5PtcyX3311dWmTKTcbEpDwj7ivdkXWTZz7gdN+Zztp8xoAE4rZFI9C+o/+uiju57Dd4H1z1LQsX+bKej4vvAaSlpKcbanzfpqBoSzfkx3l8n+U089tdqrV6/WIBik5LX32RhjRgwPisYYA0ZOPmdkwcGUd22yP9MTR2krTZRNXJtKSUcZx099ehwp75trh8fhdEAm6XkOJQrPoXSj15tylv2y8847V5sB0byW7W2uFabkYnZnlkXZm0lOPjd6gVesWFHtLGs3+519kUnmAw88sNrNZ96tPpw+yDy9bEsTTldkezqTbM0220NpzOfDd4fPuY23elBSdRQk7yDxl6IxxgAPisYYA0ZaPmf7FVMyUcJRtlHOMhiZ0oMeUCnPes3gWtaJMotyrbleeBx6ECmTKD8y7yOnA1hvyiTaXI+bZfA+4ogjqn3dddd1Pb/poacspxTnOnD2C4OUKRMPOOCAamepwzhVQY9rlo6M8HksWLCg2mzb3XffXW3uGU7oMWb9s8UB0sSpiCwFHddUs4/YNvYLZT+fCd8FPhsGwXMahrK/mTF80IxCIHaTQW5cZYwxmwQeFI0xBoycfM4+s7M0R5k0+u53v1ttyl+W35S5lDGUt5RumWeWci1L7URZmXloWT/KNXofd9xxx673ZRAwJdaee+5ZbUoyyq099tija/15jpRvykW5xmkIyrXddtut2pzeoPzMrs3eC7aZfZ1t+sT2MIph//3373otN/FiVAHXtDfJ1spnKd+awd/jMKUYnwkjI3g8e78Ip5u4OCAjSxHWTJfW7fxRkcxkIBtXGWPMpoQHRWOMASMnnzMo2ygTKGcpwyg3GbxLydD0YtIjlW18xHrwekqFTN7QO0ypTtnH+u26667VpnSjNKLEyuRwVn8GE7NMejqbMozX06tPych+oVea1zJYmt5aZsnOvKNZQHy21zM38TrhhBOqne0lzggATj1Q/maB9VIun/ms2gTjc9on22+b7ynbTE83++W2226rNtuf0asHeRQ9zr3iL0VjjAEeFI0xBmw08plkabcoQyhJeZxrdClnpYlyheVSBlBmZVmlWb8sMDsL/qXUZfn0GvJaSmB6U9n+bE0sve9cc8wpCcpESTr99NOrzWkM7j/NNmdreXnOTTfdVO0s/VsmxXgObT4DTosw+zUzePO58tnTE035y3elmV4tSyuWSW7Wm+9nNn3Q3Gd6HPZRNlWTbR6W0UYC06Ob7Su+MUlpfykaYwzwoGiMMWCk5TM/uSkBGQS8fv36alOqMaibXtJ999232k3vGwNyKaGy1GMk2zQqSx1Fry7bxvLpcabUYb9QejGglhKO5dO7fcstt1Q7S9PV9D5Tcu++++7VzrJBZ+uus82aKPsozylv99prr2pnG1dR0lHe0hPN6QbWh/3OfuT7wn5sesOzzN2U0tm7xrKyNHXNaZ9xMrnK5880bW0CmflOZedvrDI5w1+KxhgDPCgaYwyYNfnc6yc3JR0lXJZtm/Jk1apV1WaqJco/aaK8yYKReW/KLF7L9tCbSBiwzGsZOJ15H3k+10HTa8x+4YZR11xzTbX5DCif582bV20GX0sT+7iN55fylnViH73iFa+oNr3glN5ZJvFs3TinTyiTb7/99movXbq02ieffHK1s/eI7aKEbcpnRiJQcrO/+e6wXE718JnwHpyqyPbezoLAyaBSh43avs2T4dRhxhjTIx4UjTEGzJp8zj6ts0/xbIOiQw45pNqUDzyfQbeUMwxqlfK0YMwwTbIs3Kwf78F0WZTklPSsHz2XzfW13erAcyi3rrrqqq7nZzKf3uBmH/Ee2Z7OlJ9ZdnJKY9aD0QGUxpR6zYzp3c5hHdg2Skmuj+ce2K985Su71jNb997cxIrvG/sve+fpZea7Q5uRC2wnpwb4znMahoH/nJ7oR95mnuhRl9JOHWaMMT3iQdEYY8CMy+duMrCNlKY8W7t2bbWPOuqoalNuUGJRtmVrl5v3yFJ7ZYHZmUykl5kyi15dSh2Wma2VbTPF8JWvfKXabdJOUZJRzjFQWpq4dpb9wn6lZzbbZCrzIGcebfYRPdF8F7L9ljktwjXaN9xwQ7X5vtADTC8x68OpEPaJNPH5Z9MeJFsTzmdFjzZlP+vBc7g/Nz3xzFTOd7YNbd5BHp9MMo+itB7HX4rGGAM8KBpjDNho1j7Ta0TvKL1vlGHZhkHZRlLN6ynXsjpRlrIemZSkBzFbp01ZSVnF8+kRpez92te+Vm1K9SzAl2Vy6iBL3yVNlIYMImadsimKLB1btpabcGogS6lGCUxJS28w209PN+/LaYssrRttPnspf25Zlmxen+2BzWv5TvH5833OFgfw2uamZFPBKIx+N6jKrulVog8DfykaYwzwoGiMMWDkgrcJpQQ9opStlEyZVOE5lBjNDMaUzLSzzaeyTNc8nq1HztYdZ1I/y2ZNDyq9kiwzS6+VyRMGOzefU+ZNzTJ9U7pm5bBO7C/alImUxpnnntKQUwkM3ua68SxDONvCOmTPr1kW60o7C7Tm/dh+Tm9kwdtZWjt6nLkf+kte8pKu57chew+yd2qy8/o5Pgz8pWiMMcCDojHGgBmXz90+g9t8imeeXsq2bM0u031RSjEwV8o9s5TMlOL0uFL2ZhtXUQIx0JZlUrpm3krei8HLrD/7JdsnmeewzIMOOqhr/aV87TPvnWUJz7zgJJsOyaYA+Dz5jlC2Uz5yL+1MJmfSmP04mUxknThdw/plHu4sCzdhnXivLEKDMpxkcrsNbWTyqARl9+q59peiMcYAD4rGGANGIni7jceJEpbyifKRacRuvvnmajOTNGkGr2bBxZQZXPtMCUS5wuDlLMiXEjvz0PJaSvUHH3ywazkkkwzNjajGYd9x6qG5PjyD9aAsz9Zmk2wjpiyNFmU765dNf3DzJd6LAc5sc7aGmuVn0lbKpwn4ftHjnEnsLOKAsB60Dz744GpzyuA//uM/qs3290MbSdqU2L1K634keq/38peiMcYAD4rGGANmLXVYr6mHKJ/oTbvnnnuqfeyxx1b7m9/8ZrUpHygZmhm1s82neH22BplkxynLKL0J28lyeLyZqmqcbG1uVv7hhx9eba4hzmTuZPdgv2SB2SyX59Dz3SZwOtski/2bed/p3ea7wD5lUHeWXZz1bz5L/p09Q9YpW79MuG4+28SNUwPccKsZZTHOunXruh7vlTbe3X490Q7eNsaYWaLVoBgR20TEJRHx/YhYGRFLImK7iLgqIlZ1/r3t1CUZY8xo01Y+Xyjp66WU10fEFpK2kvQnkpaWUv4iIs6TdJ6kc6cqaPwzuNdPbkqp9evXV5vrfbl/cLZHL73V9EpKEz3LlH2ZpMk8i1mmb8on1o/34rWZh5PyjvfN5Ck9nfQyZ2t/s9RnUr42N9s0K/PeNtcLd7tfto8x78u2sUyulc+eaxsJy43E+K7x3WQgvpRvdkWb7WT0Ac+hJ5r3ZpuZvo31YMA6p5WydfyMnmgD/9tpsxnUZLQZC0YqdVhEPFfSMZI+3anQE6WUxyWdJunizmkXSzp9OFU0xpiZo418XiDpYUmfiYhbIuJTEbG1pB1LKeObWTwoacduF0fE2RFxU0TcNJgqG2PM8GgzKG4uaZGkT5ZSXizpZxqTypUy9j3b9Zu2lHJRKWVxKWVxv5U1xphh02ZOcb2k9aWUZZ2/L9HYoPhQRMwrpTwQEfMkbejlxr1GwfP8u+66q9rMCcd5jmzT88m2I+C8EkMgsnT5WXgK57A4b8OQDs6LMSSF4RNZ+znnx43b2R7OU2X5+rJVErxXM9wke24MdcnCe7JwK87tce6Q12bztJwjy/IYZrC/OKfG94DPjGTPUpq4w14WGsVQmmwHP86Rsp2c5+Sc4sknn1xtPo9LLrmk2sccc0y1b7nllq5165XpzPf1es1IheSUUh6UtC4i9ukcOk7SCkmXSzqzc+xMSZcNpYbGGDODtPU+v1vS5zqe5zWS3qGxAfWLEXGWpPskvXE4VTTGmJmj1aBYSvmepG5zgsdN98Ztdu3Kzqc0oIxhuM2JJ55Y7SuvvLLau+22W7VXrFgx4R7coJ6Sm/emdMlWa3ClRJZqn3nwKJOYNCLbVD3bIY8ymTKf5WflcLqA0rO5GoKyPAtDoqTLtlfIVpxkfco6sa6EdaA0zhJU8DllOTqz3JBZEovmvflMeH2WyIPJPlg/9jWlNKcS9t5772rzmd9xxx3Vvu+++9J6T5dRyZs4KLyixRhjgAdFY4wBI7GbX5tcaZRVlDeUucuXL6/2Bz/4wWp/6Utf6lp+U8Lceeed1eZqD65QoDSkXKMczuQjpRTvzeO8Fz2XlGtZkgGen3l3CVcGcVUC5emLXvSiCddQTrNtlHrsC+YppETNds/bsOGpIIZsZcxDDz1UbcpKRhxkkQTZapMsCUb2nEjTi59NUWRe5my7C04Z7L///tWm1/iMM86oNnfqu/baa6u9aNGiaq9atarafB4zzShLbn8pGmMM8KBojDFgpLcjyKCUomeNkpcSe9999602g2abnlXKOP5GD18mS2lTPmblU+pRlmVTCVmOxsxzzTrwWkpJeu4pWyntmK9SmtivTEBAmfjAAw9Um95+SsM2HvF777236704TcCpiixonrkFKb0XLlxY7cwznE3hZFsiNMlygvJZZYsD+N7xefJ94XvK6Am2+Xvf+17X+xJOnzQTpWxq+EvRGGOAB0VjjAEjIZ8z2nil16xZU+2jjz662gzkPvfcp9I8vv3tb6/2PvvsI0JpyXsz6JprZCmbKA2z9a4skzKJAdH0gmaBz5knnt7KLHib0pDSi+uVKT2bQcqU3NlUAj3OlLeZ951Q6nOKgUHzlMmEcptB16wP20PPdSaNWQ6fH/uh2UdsQ3bvbE01A78pgTklcdZZZ3Utk/L5a1/7WrUZZZBtg2Gewl+KxhgDPCgaYwwYafncxitNuUFJcuONN1b7pJNOqjYlRjOomddT3jG1O2Upy8q8elmaf0pDlsl7ZZvY81oez1L583iWyp87FlKGMg2WNHEHOEpurqml7KeHlx5OnkPPNyUt+4v92CYInlD2skz2EacS6Oll+dzigOc0vc98nmxnFq3Atcx8PrT5nDnFMm/evGpffvnl1W6zcyTZ1D3OxF+KxhgDPCgaYwwYCfncz05dlDfXXXddtSlpTjjhhGq/4x3vqPb5558/oaw999yz2lwHTSnC1E6UNJTS9CaTzNtJrzfLp8eVHmT2UbZGO0vBRS8520WPK+V583lQin3/+9/vWhY9rplXms989913rzalPtvMtrFf2F/ZVEW2Vjxbf02b0xmUwtma5ub1jAjI1ldz6iGTyccff3y1uRjhiiuu6Fq/ZcuWVZvvppkafykaYwzwoGiMMWAk5HOWRqzXza24Tveoo46q9tKlS6v91re+tdoMcJUmylgGvDJtE6HXkFC6bb/99l3rSs8iy8k2xso8l5w+oPRi+dmGTlkGb64zpse1WT/eg95rXs++oDTOvMCsB89nwHbWBtZ1wYIF1c489JSnjDxgfTitkG3o1dwAjWRear5rTH/3ghe8oNqUyQceeGC1mTqM7ymnfCyZp4+/FI0xBnhQNMYYMBLymfSzByy9ftdcc021KYEo8z7ykY9MuP60006r9s4771xtyrIXvvCF1c4CcLONkiifmMKMHs5msHS3ayndspRi7CPKRN6XUi3z3DY96ZSWTAtG+ck2sH7sF9aPZXI6gFMPfIb3339/tdk2ltNmoy/WmXXLUnNRevN5871rnpelbbv77rurzTX4DHbncUrsG264odqrV6/uWlczffylaIwxwIOiMcaAkZPPpI0nOsuMzIBYegdvu+22ar/lLW+ZUBZTj1GiZN5LphGj7GPKp8zjmGWGpkzM9hZmEDRlH6VkJvUoH0nmGW7KZ0pFrrulR5SymoHghP3F55ytO6Y3NfNoZ577LOM5N3HilETm0c/28G56n7mJF73DrAcjGvj86XHmu/DVr3612lm2eTMY/KVojDHAg6IxxoCRk8/9rIMmlDo333xztSm9KLEl6Q1veEO1KVEoj1jWkiVLqs3UYWwDPdfZGmR6gSnX2H5KZkpaSjLKR9pZluvs/CxDuJSn58rWe1PGUrpyKoFefJbDvqOU5NrnLHUcvbUZbCdlLsvPAu6zQH9pohxmIDulLp/n4sWLq8114BdffHG1V6xYUe2mt3ucbPMpHjdT4y9FY4wBHhSNMQZEPxK155tFlG6bUQ2qDpn0pvR69atfXW1KaUl697vfXW16Js8+++xqU94y0JbeREpMSkMGThNKJq7xzTyflIyUYUwdRYnFTY8YNLz33ntXmzJvsk2ZeA/Kcu4tfMABB1SbkpnPgVMJLCfLeE7pSk80ZTIlJr3YlOTZRlLM/s3jWXQDJTafn5RnuuZ7wSkTyucrr7yy2t/85jernb07/eQN2NQppXTdGc9fisYYAzwoGmMMmDXvc6+f9232gG6zJvraa6+t9ute97oJ53GzqyOPPLLa55xzTrWZrZupyii5XvziF1eb+x5zbSqDqLNNrCi9mWGa8nmPPfaoNiUjZW62/jhbKzyZ55oSkG2mRGXwMsti+7PgbUpsXtvGs5zVh/3Fa+klpqzmtAqjCrgWO4sAkCY+N/bXokWLqs3Af0rm66+/vtr0cGdYMg8efykaYwzwoGiMMWDW5HM/n/29ZuTO1h/TYypNlJ+UQG9729uqzX2PP//5z1ebAbLMDM2AXe6TnG3ulAUj8zi9m5TJlNiUxpR6lNKUqpS8lI/ZtIWUPwf2Bb3plJlcj01PNL3ghPVmnR566KFqs18ok/kMeF/Wn+u12V+Z9zmzpYnTBIcddli1OU1yySWXVJsLAvh+ZptjTfZMujFsiT3XJLy/FI0xBnhQNMYYMOPyeRCf19nnehtZQUnyne98Z8Jv9DRSfnITpA984APVZhDxhRde2PVa7o1MCcgM3ln2bMpwSkMGDmfe2iwTOL3elLPZfsuU59JEqcu+pze5ec049MSzTmwn28PpA96Xgdb33ntv1zowwzozdTNomh5gwumMLFs4z2EKNWnis+Vzu+yyy6pNycwg9cn2k57qeIYlc2/4S9EYY4AHRWOMASOROqyfwOzsfJbZVm5fd9111T7hhBOqfcUVV1SbwbhcK02PLaU0pdVdd91Vba5HptyifKScp2Sm55rlEHqT6QFmPSlbKbcp55uZsylRKfUovymfs8zg2dpslklvOutK7zbrx+fJOhxyyCHqRvZOURqzPpwW4bp37sksTVxfzr3FuTabzyerR5v/LoZN9t/OXJPMxF+KxhgDPCgaYwyYcfk8lYd4GKmQsmubQbeUgJQ9r33ta6tNKc1g73e9613VpsyilM4k/fLly6tN6XbMMcdUm/KRXlnKUN6XQc30uNIbyszW9OhSwjczarPevDf7glMM9KzyWgajs96UnryW7ef5lORMhcbIAEp4BkdTYmcymdMNL33pS6vNPrr00ktF6BFnezLPfcYoSNRRqMNM4y9FY4wBHhSNMQaMdObtNsHYg0pB1iyLEoreXmbupkR7yUteUu0jjjii2kxVdu6551abmZTpoc4kHTN7U+ZTqrLOXPvL9GKE8peeUXqim9fSk03JTalLsj2aKZ8pjWkT9gWfx1577VVtpvaiVKV8ztKLsfxjjz222twAinuGM8XXypUrJ9Q1iwjg1MimKEtHDWfeNsaYFnhQNMYYMOPyGXY93kY+91PPtqmW2gR/U3Kdcsop1aaHlxsZveY1r6k25eYnP/nJan/961+vNoN6KY25cRPTa7E+lN6UrfTKZpm36a1eu3ZttZsbJvHe9GRTujLAm15pykq2k/VmnRhEztReTMGW7ZPN6QOew/XOhx9+eLU5hcGpBK5d517gd955Z7U53dCsR8aw/7ubrf++NqZpgb7kc0S8NyLuiIjbI+LzEbFlRCyIiGURsToivhARW0xdkjHGjDZTDooRMV/SH0paXEo5QNJmkt4s6WOSLiilLJT0mKSzhllRY4yZCaaUz51B8QZJB0n6saR/kfQJSZ+TtFMp5cmIWCLpQ6WUV05R1kC+rXvNPNwvmaym7DvqqKOqzY2r6L2lV5rrndevX1/tz372s9VeunRptdesWVNtBm+zPtyIid5gZoKmrKZUpdd35513rjbTfUkT5TSvZx9R6vMcenhZD0pvtmGnnXaqNqU6vbiUxlwfzr2nWT77jkHWt99+e7WZXZ3y+fHHH+/alulIxmFL2oyNVeoOg2nL51LKDyR9XNJaSQ9I+pGkmyU9XkoZj+lYL2l+9xKMMWbjoY183lbSaZIWSNpZ0taSTmx7g4g4OyJuioibpl1LY4yZIdrI5zdIOrGUclbn77dLWiLpDRqyfO4nw3Ybmm3vp1xKaXpKjz766GrTK02Jetxxx3U9h1KVMu6rX/1qtVetWlVtyj5eS2nMoGZ6hilzs9Rf0kTpymsob5nyjJtGZanQKKUZUM4+YsA2pyTooWc7ueaY3vT77ruv2pySYH81pwzGmU4m7Dbv8EzK5FFIRzYq9ON9XivpiIjYKsZ69DhJKyRdLen1nXPOlHRZcr0xxmw0tJlTXCbpEknflbS8c81Fks6V9L6IWC3peZI+PcR6GmPMjDBrwdt9llPtYWUt7vV6nk/JSZvBwvRQ0zvKLM6LFy+u9vnnn1/tJUuWVPvkk0+uNuUgs3zTs8oAcq4Dpp1t3NT8mzI7W0dM2ZsFmh988MHVpqwm9KZz3TFlL4PD2X56nOlBpvzPsm0Pi009cHoU8NpnY4xpgQdFY4wBI7H2uVfa1LmN/G2e06uHu809KNEoPSkrDz300GovWrSo2vTWMkUWvan0VtPrzQ2dmO6L6c4oPbl+d7IUVwzMpveamajpiaY3mRtOUcZec8011aYnmunFuDab0pjH6TVuM83Rq0xuO1XTTzq7Qb3b/Vy7qXioLZ+NMaYFHhSNMQaMtPe5nwDU6WTt7ifdUna/7PxMVjM114IFC6pN+UwPbZZ2i3KWsEyuG6ZUpTe82Q+U/VyzTe81vb30djNwnOdQ9nJqgN5kbjjFOrWRwP14k2cr+Ho69CPD+/lvZxT7og2Wz8YY0wIPisYYAzwoGmMMGLk5xY0pn9ygwoqyrQ8418gwGYatEM4R8hyG5HC+kPOXnKfkdgIsU8pzMBLOL3JekKtSOI/IcCDOkbIeWSjNphI+YgaP5xSNMaYFHhSNMQaMnHwe4L2q3W/4wKBCgHqlTThPdk4WhkJ5ynNoZ9sMNO+X3SPbMZBTAKTXPIVzIRzEzD6Wz8YY0wIPisYYAzZ6+TzT8qlXWT6dlTW91KFNmZPJ4emW3zxvUFJ32MkU+sFS/Snmz39qnzom+hh1xuu9YcMGPfHEE5bPxhgzFR4UjTEGbD71KaNBJl0G6QHuRxINO9C8H9rUrW39M1meBVRnz4pkweuDrPdUtIkw2NQlMxkVyTwMGe8vRWOMAR4UjTEGbPTe50b51Z6OZ3iG+6Kn8we1a2G/Qe3DyGXpne3MbODgbWOMaYEHRWOMARuN95n06t0cpFQbRvD2oOqQnTOM8pu/DWpHun6wZDbj9OOV9peiMcYAD4rGGAPmlPd5pulHxg7q/Db1GaQ3uJ8dD9tgD7IZJl77bIwxPeJB0RhjgOXziNFPUHc/MrnN+U16leuWw7PPXJ6e6NXj7OBtY4xpgQdFY4wBG6V8nk0J0E+Q8mzJlUFK8mF4wQfloR6Fvh6lepgxKKvHsffZGGNa4kHRGGPARimfB8VksnLYwdht6CfL9bCDrI0ZZdp4ou19NsaYFnhQNMYYsEnL5yYzGYw8jHsNqszZ3ADMmOni4G1jjBkCHhSNMQbMtHx+WNLPJD0yYzcdDbaX2zzX2dTaK23cbd6tlLJDtx9mdFCUpIi4qZSyeEZvOsu4zXOfTa290txts+WzMcYAD4rGGANmY1C8aBbuOdu4zXOfTa290hxt84zPKRpjzChj+WyMMWBGB8WIODEi7oyI1RFx3kzeeyaIiF0j4uqIWBERd0TEezrHt4uIqyJiVeff2852XQdNRGwWEbdExBWdvxdExLLOs/5CRGwx23UcJBGxTURcEhHfj4iVEbFkLj/niHhv552+PSI+HxFbztVnPGODYkRsJulvJb1K0n6Sfisi9pup+88QT0r641LKfpKOkPQHnTaeJ2lpKWUvSUs7f8813iNpJf7+mKQLSikLJT0m6axZqdXwuFDS10spL5R0kMbaPiefc0TMl/SHkhaXUg6QtJmkN2uOPuOZ/FI8TNLqUsqaUsoTkv5J0mkzeP+hU0p5oJTy3Y79E439hzJfY+28uHPaxZJOn5UKDomI2EXSyZI+1fk7JB0r6ZLOKXOqzRHxXEnHSPq0JJVSniilPK65/Zw3l/TMiNhc0laSHtAcfcYzOSjOl7QOf6/vHJuTRMTukl4saZmkHUspD3R+elDSjrNVryHx15LeL2k80ePzJD1eSnmy8/dce9YLJD0s6TOdKYNPRcTWmqPPuZTyA0kfl7RWY4PhjyTdrDn6jO1oGQIR8SxJl0r6o1LKj/lbGXP3zxmXf0ScImlDKeXm2a7LDLK5pEWSPllKebHGlq5OkMpz6Tl35kZP09j/DHaWtLWkE2e1UkNkJgfFH0jaFX/v0jk2p4iIp2tsQPxcKeXLncMPRcS8zu/zJG2YrfoNgaMknRoR92psSuRYjc23bdORWtLce9brJa0vpSzr/H2JxgbJufqcj5d0Tynl4VLKryR9WWPPfU4+45kcFG+UtFfHY7WFxiZqL5/B+w+dzlzapyWtLKX8FX66XNKZHftMSZfNdN2GRSnlA6WUXUopu2vsmf5bKeWtkq6W9PrOaXOtzQ9KWhcR+3QOHSdphebuc14r6YiI2Krzjo+3d04+45nOknOSxuafNpP096WUj87YzWeAiDha0nWSluup+bU/0di84hclvUDSfZLeWEr54axUcohExMsknVNKOSUi9tDYl+N2km6RdEYp5ZezWL2BEhEHa8yxtIWkNZLeobGPjDn5nCPiv0t6k8YiLG6R9E6NzSHOuWfsFS3GGAPsaDHGGOBB0RhjgAdFY4wBHhSNMQZ4UDTGGOBB0RhjgAdFY4wBHhSNMQb8f1mmEAUvTDPmAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1728x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1296x432 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1296x432 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(os.path.join(root_dir, \"best_metric_model.pth\")))\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # select one image to evaluate and visualize the model output\n",
    "    val_input = val_ds[6][\"image\"].unsqueeze(0).to(device)\n",
    "    roi_size = (96, 96, 96)\n",
    "    sw_batch_size = 4\n",
    "    val_output = inference(val_input)\n",
    "    print('shape of labels ',val_ds[6][\"label\"].shape)\n",
    "\n",
    "\n",
    "    val_output = post_trans(val_output[0])\n",
    "    plt.figure(\"image\", (24, 6))\n",
    "    for i in range(1):\n",
    "        plt.subplot(1, 4, i + 1)\n",
    "        plt.title(f\"image channel {i}\")\n",
    "        plt.imshow(val_ds[6][\"image\"][i, :, :, 48].detach().cpu(), cmap=\"gray\")\n",
    "    plt.show()\n",
    "    \n",
    "    # visualize the 3 channels label corresponding to this image\n",
    "    plt.figure(\"label\", (18, 6))\n",
    "    for i in range(3):\n",
    "        plt.subplot(1, 3, i + 1)\n",
    "        plt.title(f\"label channel {i}\")\n",
    "        plt.imshow(val_ds[6][\"label\"][i, :, :, 48].detach().cpu())\n",
    "    plt.show()\n",
    "    # visualize the 3 channels model output corresponding to this image\n",
    "    plt.figure(\"output\", (18, 6))\n",
    "    for i in range(3):\n",
    "        plt.subplot(1, 3, i + 1)\n",
    "        plt.title(f\"output channel {i}\")\n",
    "        plt.imshow(val_output[i, :, :, 48].detach().cpu())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation on original image spacings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_transform = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"image\", \"label\"]),\n",
    "        ConvertToMultiChannelHeadRecod(keys=\"label\"),\n",
    "        EnsureChannelFirstd(keys=[\"image\"]),\n",
    "        EnsureTyped(keys=[\"image\", \"label\"]),\n",
    "        Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
    "        #Spacingd(\n",
    "        #    keys=[\"image\", \"label\"],\n",
    "        #    pixdim=(2.0, 2.0, 2.0),\n",
    "        #    mode=(\"bilinear\", \"nearest\"),\n",
    "        #),\n",
    "        Resized(keys=[\"image\", \"label\"], spatial_size=[96, 96, 96], mode=(\"trilinear\", \"nearest\")),\n",
    "        NormalizeIntensityd(keys=\"image\", nonzero=True, channel_wise=True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "val_org_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"image\", \"label\"]),\n",
    "        ConvertToMultiChannelHeadRecod(keys=\"label\"),\n",
    "        #EnsureChannelFirstd(keys=[\"image\"]),\n",
    "        Orientationd(keys=[\"image\"], axcodes=\"RAS\"),\n",
    "        Resized(keys=[\"image\"], spatial_size=[96, 96, 96], mode=(\"trilinear\")),\n",
    "        NormalizeIntensityd(keys=\"image\", nonzero=True, channel_wise=True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# val_ds = CacheDataset(data=val_files, transform=val_transform, cache_rate=1.0, num_workers=4)\n",
    "val_org_ds = Dataset(data=val_files, transform=val_transform)\n",
    "val_org_loader = DataLoader(val_org_ds, batch_size=1, shuffle=False, num_workers=4)\n",
    "\n",
    "post_transforms = Compose(\n",
    "    [\n",
    "        Invertd(\n",
    "            keys=\"pred\",\n",
    "            transform=val_org_transforms,\n",
    "            orig_keys=\"image1\",\n",
    "            meta_keys=\"pred_meta_dict\",\n",
    "            orig_meta_keys=\"image_meta_dict\",\n",
    "            meta_key_postfix=\"meta_dict\",\n",
    "            nearest_interp=False,\n",
    "            to_tensor=True,\n",
    "            device=\"cpu\",\n",
    "        ),\n",
    "        Activationsd(keys=\"pred\", sigmoid=True),\n",
    "        AsDiscreted(keys=\"pred\", threshold=0.5),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of labels  torch.Size([1, 3, 96, 96, 96])\n",
      "shape of labels  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/ajoshi/.local/lib/python3.9/site-packages/monai/transforms/post/dictionary.py:680: UserWarning: transform info of `image1` is not available in MetaTensor pred.\n",
      "  warnings.warn(f\"transform info of `{orig_key}` is not available in MetaTensor {key}.\")\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [47]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     val_data \u001b[38;5;241m=\u001b[39m [post_transforms(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m val_data]\n\u001b[1;32m     12\u001b[0m     val_outputs, val_labels \u001b[38;5;241m=\u001b[39m from_engine([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpred\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m])(val_data)\n\u001b[0;32m---> 13\u001b[0m     dice_metric(y_pred\u001b[38;5;241m=\u001b[39m\u001b[43mval_outputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m), y\u001b[38;5;241m=\u001b[39mval_labels\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     14\u001b[0m     dice_metric_batch(y_pred\u001b[38;5;241m=\u001b[39mval_outputs, y\u001b[38;5;241m=\u001b[39mval_labels)\n\u001b[1;32m     16\u001b[0m metric_org \u001b[38;5;241m=\u001b[39m dice_metric\u001b[38;5;241m.\u001b[39maggregate()\u001b[38;5;241m.\u001b[39mitem()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(os.path.join(root_dir, \"best_metric_model.pth\")))\n",
    "model.eval()\n",
    "device = 'cuda:0'\n",
    "with torch.no_grad():\n",
    "    for val_data in val_org_loader:\n",
    "        val_inputs = val_data[\"image\"].to(device)\n",
    "        val_data[\"pred\"] = inference(val_inputs)\n",
    "        print('shape of labels ',val_data[\"label\"].shape)\n",
    "        val_data = decollate_batch(val_data)\n",
    "        print('shape of labels ',len(val_data))\n",
    "        val_data = [post_transforms(i) for i in val_data]\n",
    "        val_outputs, val_labels = from_engine([\"pred\", \"label\"])(val_data)\n",
    "        dice_metric(y_pred=val_outputs, y=val_labels)\n",
    "        dice_metric_batch(y_pred=val_outputs, y=val_labels)\n",
    "\n",
    "    metric_org = dice_metric.aggregate().item()\n",
    "    metric_batch_org = dice_metric_batch.aggregate()\n",
    "\n",
    "    dice_metric.reset()\n",
    "    dice_metric_batch.reset()\n",
    "\n",
    "metric_tc, metric_wt, metric_et = metric_batch_org[0].item(), metric_batch_org[1].item(), metric_batch_org[2].item()\n",
    "\n",
    "print(\"Metric on original image spacing: \", metric_org)\n",
    "print(f\"metric_tc: {metric_tc:.4f}\")\n",
    "print(f\"metric_wt: {metric_wt:.4f}\")\n",
    "print(f\"metric_et: {metric_et:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert torch to onnx model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = torch.randn(1, 4, 240, 240, 160).to(device)\n",
    "onnx_path = os.path.join(root_dir, \"best_metric_model.onnx\")\n",
    "torch.onnx.export(model, dummy_input, onnx_path, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference onnx model\n",
    "Here we change the model used by predictor to onnx_infer, both of which are used to obtain a tensor after the input has been reasoned by the neural network.\n",
    "\n",
    "Note: If the warning `pthread_setaffinity_np failed` appears when executing this cell, this is a known problem with the onnxruntime and does not affect the execution result. If you want to disable the warning, you can cancel the following comment to solve the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the following program snippet will not affect the execution time.\n",
    "# options = ort.SessionOptions()\n",
    "# options.intra_op_num_threads = 1\n",
    "# options.inter_op_num_threads = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onnx_infer(inputs):\n",
    "    ort_inputs = {ort_session.get_inputs()[0].name: inputs.cpu().numpy()}\n",
    "    ort_outs = ort_session.run(None, ort_inputs)\n",
    "    return torch.Tensor(ort_outs[0]).to(inputs.device)\n",
    "\n",
    "\n",
    "def predict(input):\n",
    "    def _compute(input):\n",
    "        return sliding_window_inference(\n",
    "            inputs=input,\n",
    "            roi_size=(96, 96, 96),\n",
    "            sw_batch_size=1,\n",
    "            predictor=onnx_infer,\n",
    "            overlap=0.5,\n",
    "        )\n",
    "\n",
    "    if VAL_AMP:\n",
    "        with torch.cuda.amp.autocast():\n",
    "            return _compute(input)\n",
    "    else:\n",
    "        return _compute(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model_path = os.path.join(root_dir, \"best_metric_model.onnx\")\n",
    "ort_session = onnxruntime.InferenceSession(onnx_model_path)\n",
    "\n",
    "for val_data in tqdm(val_loader, desc=\"Onnxruntime Inference Progress\"):\n",
    "    val_inputs, val_labels = (\n",
    "        val_data[\"image\"].to(device),\n",
    "        val_data[\"label\"].to(device),\n",
    "    )\n",
    "\n",
    "    ort_outs = predict(val_inputs)\n",
    "    val_outputs = post_trans(torch.Tensor(ort_outs[0]).to(device)).unsqueeze(0)\n",
    "\n",
    "    dice_metric(y_pred=val_outputs, y=val_labels)\n",
    "    dice_metric_batch(y_pred=val_outputs, y=val_labels)\n",
    "onnx_metric = dice_metric.aggregate().item()\n",
    "onnx_metric_batch = dice_metric_batch.aggregate()\n",
    "onnx_metric_tc = onnx_metric_batch[0].item()\n",
    "onnx_metric_wt = onnx_metric_batch[1].item()\n",
    "onnx_metric_et = onnx_metric_batch[2].item()\n",
    "\n",
    "print(f\"onnx metric: {onnx_metric}\")\n",
    "print(f\"onnx_metric_tc: {onnx_metric_tc:.4f}\")\n",
    "print(f\"onnx_metric_wt: {onnx_metric_wt:.4f}\")\n",
    "print(f\"onnx_metric_et: {onnx_metric_et:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check best onnx model output with the input image and label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model_path = os.path.join(root_dir, \"best_metric_model.onnx\")\n",
    "ort_session = onnxruntime.InferenceSession(onnx_model_path)\n",
    "model.load_state_dict(torch.load(os.path.join(root_dir, \"best_metric_model.pth\")))\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # select one image to evaluate and visualize the model output\n",
    "    val_input = val_ds[6][\"image\"].unsqueeze(0).to(device)\n",
    "    val_output = inference(val_input)\n",
    "    val_output = post_trans(val_output[0])\n",
    "    ort_output = predict(val_input)\n",
    "    ort_output = post_trans(torch.Tensor(ort_output[0]).to(device)).unsqueeze(0)\n",
    "    plt.figure(\"image\", (24, 6))\n",
    "    for i in range(4):\n",
    "        plt.subplot(1, 4, i + 1)\n",
    "        plt.title(f\"image channel {i}\")\n",
    "        plt.imshow(val_ds[6][\"image\"][i, :, :, 70].detach().cpu(), cmap=\"gray\")\n",
    "    plt.show()\n",
    "    # visualize the 3 channels label corresponding to this image\n",
    "    plt.figure(\"label\", (18, 6))\n",
    "    for i in range(3):\n",
    "        plt.subplot(1, 3, i + 1)\n",
    "        plt.title(f\"label channel {i}\")\n",
    "        plt.imshow(val_ds[6][\"label\"][i, :, :, 70].detach().cpu())\n",
    "    plt.show()\n",
    "    # visualize the 3 channels model output corresponding to this image\n",
    "    plt.figure(\"output\", (18, 6))\n",
    "    for i in range(3):\n",
    "        plt.subplot(1, 3, i + 1)\n",
    "        plt.title(f\"pth output channel {i}\")\n",
    "        plt.imshow(val_output[i, :, :, 70].detach().cpu())\n",
    "    plt.show()\n",
    "    plt.figure(\"output\", (18, 6))\n",
    "    for i in range(3):\n",
    "        plt.subplot(1, 3, i + 1)\n",
    "        plt.title(f\"onnx output channel {i}\")\n",
    "        plt.imshow(ort_output[0, i, :, :, 70].detach().cpu())\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
